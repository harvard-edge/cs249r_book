<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/ops/ops.html" rel="next">
<link href="../../../contents/core/hw_acceleration/hw_acceleration.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-208a45d5e3541500f9f69911177a25d6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script><script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script><script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script><script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script><script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script><script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script><script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script><script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script><style>
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
</style>
<style>
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-md " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo"></a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">
<li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
</li>
</ul>
</div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/efficient_ai/efficient_ai.html">Performance Engineering</a></li><li class="breadcrumb-item"><a href="../../../contents/core/benchmarking/benchmarking.html">Benchmarking AI</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav><div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden">
<i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div>
<i class="bi bi-x-lg quarto-announcement-action"></i>
</div>
</header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99"><h2 id="toc-title">On this page</h2>
   
  <ul>
<li>
<a href="#benchmarking-ai" id="toc-benchmarking-ai" class="nav-link active" data-scroll-target="#benchmarking-ai">Benchmarking AI</a>
  <ul>
<li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-benchmarking-ai-overview-1430" id="toc-sec-benchmarking-ai-overview-1430" class="nav-link" data-scroll-target="#sec-benchmarking-ai-overview-1430">Overview</a></li>
  <li>
<a href="#sec-benchmarking-ai-historical-context-65b3" id="toc-sec-benchmarking-ai-historical-context-65b3" class="nav-link" data-scroll-target="#sec-benchmarking-ai-historical-context-65b3">Historical Context</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-performance-benchmarks-6623" id="toc-sec-benchmarking-ai-performance-benchmarks-6623" class="nav-link" data-scroll-target="#sec-benchmarking-ai-performance-benchmarks-6623">Performance Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-energy-benchmarks-3d37" id="toc-sec-benchmarking-ai-energy-benchmarks-3d37" class="nav-link" data-scroll-target="#sec-benchmarking-ai-energy-benchmarks-3d37">Energy Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-domainspecific-benchmarks-6269" id="toc-sec-benchmarking-ai-domainspecific-benchmarks-6269" class="nav-link" data-scroll-target="#sec-benchmarking-ai-domainspecific-benchmarks-6269">Domain-Specific Benchmarks</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-ai-benchmarks-d3e3" id="toc-sec-benchmarking-ai-ai-benchmarks-d3e3" class="nav-link" data-scroll-target="#sec-benchmarking-ai-ai-benchmarks-d3e3">AI Benchmarks</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-algorithmic-benchmarks-839e" id="toc-sec-benchmarking-ai-algorithmic-benchmarks-839e" class="nav-link" data-scroll-target="#sec-benchmarking-ai-algorithmic-benchmarks-839e">Algorithmic Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-system-benchmarks-a3ce" id="toc-sec-benchmarking-ai-system-benchmarks-a3ce" class="nav-link" data-scroll-target="#sec-benchmarking-ai-system-benchmarks-a3ce">System Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-data-benchmarks-2dd8" id="toc-sec-benchmarking-ai-data-benchmarks-2dd8" class="nav-link" data-scroll-target="#sec-benchmarking-ai-data-benchmarks-2dd8">Data Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-community-consensus-fbb9" id="toc-sec-benchmarking-ai-community-consensus-fbb9" class="nav-link" data-scroll-target="#sec-benchmarking-ai-community-consensus-fbb9">Community Consensus</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-benchmark-components-77e9" id="toc-sec-benchmarking-ai-benchmark-components-77e9" class="nav-link" data-scroll-target="#sec-benchmarking-ai-benchmark-components-77e9">Benchmark Components</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-problem-definition-ed9a" id="toc-sec-benchmarking-ai-problem-definition-ed9a" class="nav-link" data-scroll-target="#sec-benchmarking-ai-problem-definition-ed9a">Problem Definition</a></li>
  <li><a href="#sec-benchmarking-ai-standardized-datasets-a1d6" id="toc-sec-benchmarking-ai-standardized-datasets-a1d6" class="nav-link" data-scroll-target="#sec-benchmarking-ai-standardized-datasets-a1d6">Standardized Datasets</a></li>
  <li><a href="#sec-benchmarking-ai-model-selection-3b14" id="toc-sec-benchmarking-ai-model-selection-3b14" class="nav-link" data-scroll-target="#sec-benchmarking-ai-model-selection-3b14">Model Selection</a></li>
  <li><a href="#sec-benchmarking-ai-evaluation-metrics-7a69" id="toc-sec-benchmarking-ai-evaluation-metrics-7a69" class="nav-link" data-scroll-target="#sec-benchmarking-ai-evaluation-metrics-7a69">Evaluation Metrics</a></li>
  <li><a href="#sec-benchmarking-ai-benchmark-harness-bc2f" id="toc-sec-benchmarking-ai-benchmark-harness-bc2f" class="nav-link" data-scroll-target="#sec-benchmarking-ai-benchmark-harness-bc2f">Benchmark Harness</a></li>
  <li><a href="#sec-benchmarking-ai-system-specifications-1e97" id="toc-sec-benchmarking-ai-system-specifications-1e97" class="nav-link" data-scroll-target="#sec-benchmarking-ai-system-specifications-1e97">System Specifications</a></li>
  <li><a href="#sec-benchmarking-ai-run-rules-4c81" id="toc-sec-benchmarking-ai-run-rules-4c81" class="nav-link" data-scroll-target="#sec-benchmarking-ai-run-rules-4c81">Run Rules</a></li>
  <li><a href="#sec-benchmarking-ai-result-interpretation-62db" id="toc-sec-benchmarking-ai-result-interpretation-62db" class="nav-link" data-scroll-target="#sec-benchmarking-ai-result-interpretation-62db">Result Interpretation</a></li>
  <li><a href="#sec-benchmarking-ai-example-benchmark-5ed2" id="toc-sec-benchmarking-ai-example-benchmark-5ed2" class="nav-link" data-scroll-target="#sec-benchmarking-ai-example-benchmark-5ed2">Example Benchmark</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-benchmarking-granularity-8676" id="toc-sec-benchmarking-ai-benchmarking-granularity-8676" class="nav-link" data-scroll-target="#sec-benchmarking-ai-benchmarking-granularity-8676">Benchmarking Granularity</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-micro-benchmarks-cd5e" id="toc-sec-benchmarking-ai-micro-benchmarks-cd5e" class="nav-link" data-scroll-target="#sec-benchmarking-ai-micro-benchmarks-cd5e">Micro Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-macro-benchmarks-eb7e" id="toc-sec-benchmarking-ai-macro-benchmarks-eb7e" class="nav-link" data-scroll-target="#sec-benchmarking-ai-macro-benchmarks-eb7e">Macro Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-endtoend-benchmarks-cb75" id="toc-sec-benchmarking-ai-endtoend-benchmarks-cb75" class="nav-link" data-scroll-target="#sec-benchmarking-ai-endtoend-benchmarks-cb75">End-to-End Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-tradeoffs-6217" id="toc-sec-benchmarking-ai-tradeoffs-6217" class="nav-link" data-scroll-target="#sec-benchmarking-ai-tradeoffs-6217">Trade-offs</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-training-benchmarks-c516" id="toc-sec-benchmarking-ai-training-benchmarks-c516" class="nav-link" data-scroll-target="#sec-benchmarking-ai-training-benchmarks-c516">Training Benchmarks</a>
  <ul class="collapse">
<li>
<a href="#sec-benchmarking-ai-motivation-af6b" id="toc-sec-benchmarking-ai-motivation-af6b" class="nav-link" data-scroll-target="#sec-benchmarking-ai-motivation-af6b">Motivation</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-importance-training-benchmarks-14e6" id="toc-sec-benchmarking-ai-importance-training-benchmarks-14e6" class="nav-link" data-scroll-target="#sec-benchmarking-ai-importance-training-benchmarks-14e6">Importance of Training Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-hardware-software-optimization-b3c5" id="toc-sec-benchmarking-ai-hardware-software-optimization-b3c5" class="nav-link" data-scroll-target="#sec-benchmarking-ai-hardware-software-optimization-b3c5">Hardware &amp; Software Optimization</a></li>
  <li><a href="#sec-benchmarking-ai-scalability-efficiency-77df" id="toc-sec-benchmarking-ai-scalability-efficiency-77df" class="nav-link" data-scroll-target="#sec-benchmarking-ai-scalability-efficiency-77df">Scalability &amp; Efficiency</a></li>
  <li><a href="#sec-benchmarking-ai-cost-energy-factors-82fd" id="toc-sec-benchmarking-ai-cost-energy-factors-82fd" class="nav-link" data-scroll-target="#sec-benchmarking-ai-cost-energy-factors-82fd">Cost &amp; Energy Factors</a></li>
  <li><a href="#sec-benchmarking-ai-fair-ml-systems-comparison-fa96" id="toc-sec-benchmarking-ai-fair-ml-systems-comparison-fa96" class="nav-link" data-scroll-target="#sec-benchmarking-ai-fair-ml-systems-comparison-fa96">Fair ML Systems Comparison</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-metrics-fab9" id="toc-sec-benchmarking-ai-metrics-fab9" class="nav-link" data-scroll-target="#sec-benchmarking-ai-metrics-fab9">Metrics</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-time-throughput-31dc" id="toc-sec-benchmarking-ai-time-throughput-31dc" class="nav-link" data-scroll-target="#sec-benchmarking-ai-time-throughput-31dc">Time and Throughput</a></li>
  <li><a href="#sec-benchmarking-ai-scalability-parallelism-ef6d" id="toc-sec-benchmarking-ai-scalability-parallelism-ef6d" class="nav-link" data-scroll-target="#sec-benchmarking-ai-scalability-parallelism-ef6d">Scalability &amp; Parallelism</a></li>
  <li><a href="#sec-benchmarking-ai-resource-utilization-918d" id="toc-sec-benchmarking-ai-resource-utilization-918d" class="nav-link" data-scroll-target="#sec-benchmarking-ai-resource-utilization-918d">Resource Utilization</a></li>
  <li><a href="#sec-benchmarking-ai-energy-efficiency-cost-960b" id="toc-sec-benchmarking-ai-energy-efficiency-cost-960b" class="nav-link" data-scroll-target="#sec-benchmarking-ai-energy-efficiency-cost-960b">Energy Efficiency &amp; Cost</a></li>
  <li><a href="#sec-benchmarking-ai-fault-tolerance-robustness-707f" id="toc-sec-benchmarking-ai-fault-tolerance-robustness-707f" class="nav-link" data-scroll-target="#sec-benchmarking-ai-fault-tolerance-robustness-707f">Fault Tolerance &amp; Robustness</a></li>
  <li><a href="#sec-benchmarking-ai-reproducibility-standardization-79ac" id="toc-sec-benchmarking-ai-reproducibility-standardization-79ac" class="nav-link" data-scroll-target="#sec-benchmarking-ai-reproducibility-standardization-79ac">Reproducibility &amp; Standardization</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-training-performance-evaluation-8754" id="toc-sec-benchmarking-ai-training-performance-evaluation-8754" class="nav-link" data-scroll-target="#sec-benchmarking-ai-training-performance-evaluation-8754">Training Performance Evaluation</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-training-benchmark-pitfalls-7552" id="toc-sec-benchmarking-ai-training-benchmark-pitfalls-7552" class="nav-link" data-scroll-target="#sec-benchmarking-ai-training-benchmark-pitfalls-7552">Training Benchmark Pitfalls</a></li>
  <li><a href="#sec-benchmarking-ai-final-thoughts-586d" id="toc-sec-benchmarking-ai-final-thoughts-586d" class="nav-link" data-scroll-target="#sec-benchmarking-ai-final-thoughts-586d">Final Thoughts</a></li>
  </ul>
</li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-inference-benchmarks-5e47" id="toc-sec-benchmarking-ai-inference-benchmarks-5e47" class="nav-link" data-scroll-target="#sec-benchmarking-ai-inference-benchmarks-5e47">Inference Benchmarks</a>
  <ul class="collapse">
<li>
<a href="#sec-benchmarking-ai-motivation-fa6c" id="toc-sec-benchmarking-ai-motivation-fa6c" class="nav-link" data-scroll-target="#sec-benchmarking-ai-motivation-fa6c">Motivation</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-importance-inference-benchmarks-3710" id="toc-sec-benchmarking-ai-importance-inference-benchmarks-3710" class="nav-link" data-scroll-target="#sec-benchmarking-ai-importance-inference-benchmarks-3710">Importance of Inference Benchmarks</a></li>
  <li><a href="#sec-benchmarking-ai-hardware-software-optimization-6999" id="toc-sec-benchmarking-ai-hardware-software-optimization-6999" class="nav-link" data-scroll-target="#sec-benchmarking-ai-hardware-software-optimization-6999">Hardware &amp; Software Optimization</a></li>
  <li><a href="#sec-benchmarking-ai-scalability-efficiency-be05" id="toc-sec-benchmarking-ai-scalability-efficiency-be05" class="nav-link" data-scroll-target="#sec-benchmarking-ai-scalability-efficiency-be05">Scalability &amp; Efficiency</a></li>
  <li><a href="#sec-benchmarking-ai-cost-energy-factors-c9a6" id="toc-sec-benchmarking-ai-cost-energy-factors-c9a6" class="nav-link" data-scroll-target="#sec-benchmarking-ai-cost-energy-factors-c9a6">Cost &amp; Energy Factors</a></li>
  <li><a href="#sec-benchmarking-ai-fair-ml-systems-comparison-4f8b" id="toc-sec-benchmarking-ai-fair-ml-systems-comparison-4f8b" class="nav-link" data-scroll-target="#sec-benchmarking-ai-fair-ml-systems-comparison-4f8b">Fair ML Systems Comparison</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-metrics-3374" id="toc-sec-benchmarking-ai-metrics-3374" class="nav-link" data-scroll-target="#sec-benchmarking-ai-metrics-3374">Metrics</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-latency-tail-latency-33f0" id="toc-sec-benchmarking-ai-latency-tail-latency-33f0" class="nav-link" data-scroll-target="#sec-benchmarking-ai-latency-tail-latency-33f0">Latency &amp; Tail Latency</a></li>
  <li><a href="#sec-benchmarking-ai-throughput-batch-processing-efficiency-6338" id="toc-sec-benchmarking-ai-throughput-batch-processing-efficiency-6338" class="nav-link" data-scroll-target="#sec-benchmarking-ai-throughput-batch-processing-efficiency-6338">Throughput &amp; Batch Processing Efficiency</a></li>
  <li><a href="#sec-benchmarking-ai-precision-accuracy-tradeoffs-952d" id="toc-sec-benchmarking-ai-precision-accuracy-tradeoffs-952d" class="nav-link" data-scroll-target="#sec-benchmarking-ai-precision-accuracy-tradeoffs-952d">Precision &amp; Accuracy Trade-offs</a></li>
  <li><a href="#sec-benchmarking-ai-memory-footprint-model-size-8fe0" id="toc-sec-benchmarking-ai-memory-footprint-model-size-8fe0" class="nav-link" data-scroll-target="#sec-benchmarking-ai-memory-footprint-model-size-8fe0">Memory Footprint &amp; Model Size</a></li>
  <li><a href="#sec-benchmarking-ai-coldstart-model-load-time-d303" id="toc-sec-benchmarking-ai-coldstart-model-load-time-d303" class="nav-link" data-scroll-target="#sec-benchmarking-ai-coldstart-model-load-time-d303">Cold-Start &amp; Model Load Time</a></li>
  <li><a href="#sec-benchmarking-ai-scalability-dynamic-workload-handling-8d16" id="toc-sec-benchmarking-ai-scalability-dynamic-workload-handling-8d16" class="nav-link" data-scroll-target="#sec-benchmarking-ai-scalability-dynamic-workload-handling-8d16">Scalability &amp; Dynamic Workload Handling</a></li>
  <li><a href="#sec-benchmarking-ai-power-consumption-energy-efficiency-ede1" id="toc-sec-benchmarking-ai-power-consumption-energy-efficiency-ede1" class="nav-link" data-scroll-target="#sec-benchmarking-ai-power-consumption-energy-efficiency-ede1">Power Consumption &amp; Energy Efficiency</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-inference-performance-evaluation-7640" id="toc-sec-benchmarking-ai-inference-performance-evaluation-7640" class="nav-link" data-scroll-target="#sec-benchmarking-ai-inference-performance-evaluation-7640">Inference Performance Evaluation</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-inference-systems-considerations-8198" id="toc-sec-benchmarking-ai-inference-systems-considerations-8198" class="nav-link" data-scroll-target="#sec-benchmarking-ai-inference-systems-considerations-8198">Inference Systems Considerations</a></li>
  <li><a href="#sec-benchmarking-ai-inference-benchmark-pitfalls-0451" id="toc-sec-benchmarking-ai-inference-benchmark-pitfalls-0451" class="nav-link" data-scroll-target="#sec-benchmarking-ai-inference-benchmark-pitfalls-0451">Inference Benchmark Pitfalls</a></li>
  <li><a href="#sec-benchmarking-ai-final-thoughts-4738" id="toc-sec-benchmarking-ai-final-thoughts-4738" class="nav-link" data-scroll-target="#sec-benchmarking-ai-final-thoughts-4738">Final Thoughts</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-mlperf-inference-benchmarks-85e7" id="toc-sec-benchmarking-ai-mlperf-inference-benchmarks-85e7" class="nav-link" data-scroll-target="#sec-benchmarking-ai-mlperf-inference-benchmarks-85e7">MLPerf Inference Benchmarks</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-mlperf-inference-7257" id="toc-sec-benchmarking-ai-mlperf-inference-7257" class="nav-link" data-scroll-target="#sec-benchmarking-ai-mlperf-inference-7257">MLPerf Inference</a></li>
  <li><a href="#sec-benchmarking-ai-mlperf-mobile-b4b5" id="toc-sec-benchmarking-ai-mlperf-mobile-b4b5" class="nav-link" data-scroll-target="#sec-benchmarking-ai-mlperf-mobile-b4b5">MLPerf Mobile</a></li>
  <li><a href="#sec-benchmarking-ai-mlperf-client-1f92" id="toc-sec-benchmarking-ai-mlperf-client-1f92" class="nav-link" data-scroll-target="#sec-benchmarking-ai-mlperf-client-1f92">MLPerf Client</a></li>
  <li><a href="#sec-benchmarking-ai-mlperf-tiny-7941" id="toc-sec-benchmarking-ai-mlperf-tiny-7941" class="nav-link" data-scroll-target="#sec-benchmarking-ai-mlperf-tiny-7941">MLPerf Tiny</a></li>
  <li><a href="#sec-benchmarking-ai-continued-expansion-b07e" id="toc-sec-benchmarking-ai-continued-expansion-b07e" class="nav-link" data-scroll-target="#sec-benchmarking-ai-continued-expansion-b07e">Continued Expansion</a></li>
  </ul>
</li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-energy-efficiency-measurement-a669" id="toc-sec-benchmarking-ai-energy-efficiency-measurement-a669" class="nav-link" data-scroll-target="#sec-benchmarking-ai-energy-efficiency-measurement-a669">Energy Efficiency Measurement</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-power-measurement-boundaries-5d17" id="toc-sec-benchmarking-ai-power-measurement-boundaries-5d17" class="nav-link" data-scroll-target="#sec-benchmarking-ai-power-measurement-boundaries-5d17">Power Measurement Boundaries</a></li>
  <li><a href="#sec-benchmarking-ai-performance-vs-energy-efficiency-826e" id="toc-sec-benchmarking-ai-performance-vs-energy-efficiency-826e" class="nav-link" data-scroll-target="#sec-benchmarking-ai-performance-vs-energy-efficiency-826e">Performance vs Energy Efficiency</a></li>
  <li><a href="#sec-benchmarking-ai-standardized-power-measurement-67d6" id="toc-sec-benchmarking-ai-standardized-power-measurement-67d6" class="nav-link" data-scroll-target="#sec-benchmarking-ai-standardized-power-measurement-67d6">Standardized Power Measurement</a></li>
  <li><a href="#sec-benchmarking-ai-mlperf-power-case-study-81eb" id="toc-sec-benchmarking-ai-mlperf-power-case-study-81eb" class="nav-link" data-scroll-target="#sec-benchmarking-ai-mlperf-power-case-study-81eb">MLPerf Power Case Study</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-challenges-limitations-5fd3" id="toc-sec-benchmarking-ai-challenges-limitations-5fd3" class="nav-link" data-scroll-target="#sec-benchmarking-ai-challenges-limitations-5fd3">Challenges &amp; Limitations</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-environmental-conditions-d6ae" id="toc-sec-benchmarking-ai-environmental-conditions-d6ae" class="nav-link" data-scroll-target="#sec-benchmarking-ai-environmental-conditions-d6ae">Environmental Conditions</a></li>
  <li><a href="#sec-benchmarking-ai-hardware-lottery-8141" id="toc-sec-benchmarking-ai-hardware-lottery-8141" class="nav-link" data-scroll-target="#sec-benchmarking-ai-hardware-lottery-8141">Hardware Lottery</a></li>
  <li><a href="#sec-benchmarking-ai-benchmark-engineering-d8cc" id="toc-sec-benchmarking-ai-benchmark-engineering-d8cc" class="nav-link" data-scroll-target="#sec-benchmarking-ai-benchmark-engineering-d8cc">Benchmark Engineering</a></li>
  <li><a href="#sec-benchmarking-ai-bias-overoptimization-9bbf" id="toc-sec-benchmarking-ai-bias-overoptimization-9bbf" class="nav-link" data-scroll-target="#sec-benchmarking-ai-bias-overoptimization-9bbf">Bias &amp; Over-Optimization</a></li>
  <li><a href="#sec-benchmarking-ai-benchmark-evolution-69c1" id="toc-sec-benchmarking-ai-benchmark-evolution-69c1" class="nav-link" data-scroll-target="#sec-benchmarking-ai-benchmark-evolution-69c1">Benchmark Evolution</a></li>
  <li><a href="#sec-benchmarking-ai-mlperfs-role-5183" id="toc-sec-benchmarking-ai-mlperfs-role-5183" class="nav-link" data-scroll-target="#sec-benchmarking-ai-mlperfs-role-5183">MLPerf’s Role</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking-ai-beyond-system-benchmarking-e69a" id="toc-sec-benchmarking-ai-beyond-system-benchmarking-e69a" class="nav-link" data-scroll-target="#sec-benchmarking-ai-beyond-system-benchmarking-e69a">Beyond System Benchmarking</a>
  <ul class="collapse">
<li><a href="#sec-benchmarking-ai-model-benchmarking-e9fb" id="toc-sec-benchmarking-ai-model-benchmarking-e9fb" class="nav-link" data-scroll-target="#sec-benchmarking-ai-model-benchmarking-e9fb">Model Benchmarking</a></li>
  <li><a href="#sec-benchmarking-ai-data-benchmarking-855c" id="toc-sec-benchmarking-ai-data-benchmarking-855c" class="nav-link" data-scroll-target="#sec-benchmarking-ai-data-benchmarking-855c">Data Benchmarking</a></li>
  <li><a href="#sec-benchmarking-ai-benchmarking-trifecta-7da9" id="toc-sec-benchmarking-ai-benchmarking-trifecta-7da9" class="nav-link" data-scroll-target="#sec-benchmarking-ai-benchmarking-trifecta-7da9">Benchmarking Trifecta</a></li>
  </ul>
</li>
  <li><a href="#sec-benchmarking-ai-summary-3acb" id="toc-sec-benchmarking-ai-summary-3acb" class="nav-link" data-scroll-target="#sec-benchmarking-ai-summary-3acb">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/efficient_ai/efficient_ai.html">Performance Engineering</a></li><li class="breadcrumb-item"><a href="../../../contents/core/benchmarking/benchmarking.html">Benchmarking AI</a></li></ol></nav></header><section id="benchmarking-ai" class="level1 page-columns page-full"><h1>Benchmarking AI</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: Photo of a podium set against a tech-themed backdrop. On each tier of the podium, there are AI chips with intricate designs. The top chip has a gold medal hanging from it, the second one has a silver medal, and the third has a bronze medal. Banners with ‘AI Olympics’ are displayed prominently in the background.</em></p>
</div></div><p> <img src="images/png/cover_ai_benchmarking.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>How can quantitative evaluation reshape the development of machine learning systems, and what metrics reveal true system capabilities?</em></p>
<p>The measurement and analysis of AI system performance represent a critical element in bridging theoretical capabilities with practical outcomes. Systematic evaluation approaches reveal fundamental relationships between model behavior, resource utilization, and operational reliability. These measurements draw out the essential trade-offs across accuracy, efficiency, and scalability, providing insights that guide architectural decisions throughout the development lifecycle. These evaluation frameworks establish core principles for assessing and validating system design choices and enable the creation of robust solutions that meet increasingly complex performance requirements across diverse deployment scenarios.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand the objectives of AI benchmarking, including performance evaluation, resource assessment, and validation.</p></li>
<li><p>Differentiate between training and inference benchmarking and their respective evaluation methodologies.</p></li>
<li><p>Identify key benchmarking metrics and trends, including accuracy, fairness, complexity, and efficiency.</p></li>
<li><p>Recognize system benchmarking concepts, including throughput, latency, power consumption, and computational efficiency.</p></li>
<li><p>Understand the limitations of isolated evaluations and the necessity of integrated benchmarking frameworks.</p></li>
</ul>
</div>
</div>
</section><section id="sec-benchmarking-ai-overview-1430" class="level2"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-overview-1430">Overview</h2>
<p>Computing systems continue to evolve and grow in complexity. Understanding their performance becomes essential to engineer them better. System evaluation measures how computing systems perform relative to specified requirements and goals. Engineers and researchers examine metrics like processing speed, resource usage, and reliability to understand system behavior under different conditions and workloads. These measurements help teams identify bottlenecks, optimize performance, and verify that systems meet design specifications.</p>
<p>Standardized measurement forms the backbone of scientific and engineering progress. The metric system enables precise communication of physical quantities. Organizations like the National Institute of Standards and Technology maintain fundamental measures from the kilogram to the second. This standardization extends to computing, where benchmarks provide uniform methods to quantify system performance. Standard performance tests measure processor operations, memory bandwidth, network throughput, and other computing capabilities. These benchmarks allow meaningful comparison between different hardware and software configurations.</p>
<p>Machine learning systems present distinct measurement challenges. Unlike traditional computing tasks, ML systems integrate hardware performance, algorithmic behavior, and data characteristics. Performance evaluation must account for computational efficiency and statistical effectiveness. Training time, model accuracy, and generalization capabilities all factor into system assessment. The interdependence between computing resources, algorithmic choices, and dataset properties creates new dimensions for measurement and comparison.</p>
<p>These considerations lead us to define machine learning benchmarking as follows:</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition of ML Benchmarking">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of ML Benchmarking
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Machine Learning Benchmarking (ML Benchmarking)</strong> is the <em>systematic evaluation</em> of <em>compute performance, algorithmic effectiveness, and data quality</em> in machine learning systems. It assesses <em>system capabilities</em>, <em>model accuracy and convergence</em>, and <em>data scalability and representativeness</em> to optimize system performance across diverse workloads. ML benchmarking enables engineers and researchers to <em>quantify trade-offs</em>, <em>improve deployment efficiency</em>, and <em>ensure reproducibility</em> in both research and production settings. As ML systems evolve, benchmarks also incorporate <em>fairness, robustness, and energy efficiency</em>, reflecting the increasing complexity of AI evaluation.</p>
</div>
</div>
<p>This chapter focuses primarily on benchmarking machine learning systems, examining how computational resources affect training and inference performance. While the main emphasis remains on system-level evaluation, understanding the role of algorithms and data proves essential for comprehensive ML benchmarking.</p>
</section><section id="sec-benchmarking-ai-historical-context-65b3" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-historical-context-65b3">Historical Context</h2>
<p>The evolution of computing benchmarks mirrors the development of computer systems themselves, progressing from simple performance metrics to increasingly specialized evaluation frameworks. As computing expanded beyond scientific calculations into diverse applications, benchmarks evolved to measure new capabilities, constraints, and use cases. This progression reflects three major shifts in computing: the transition from mainframes to personal computers, the rise of energy efficiency as a critical concern, and the emergence of specialized computing domains such as machine learning.</p>
<p>Early benchmarks focused primarily on raw computational power, measuring basic operations like floating-point calculations. As computing applications diversified, benchmark development branched into distinct specialized categories, each designed to evaluate specific aspects of system performance. This specialization accelerated with the emergence of graphics processing, mobile computing, and eventually, cloud services and machine learning.</p>
<section id="sec-benchmarking-ai-performance-benchmarks-6623" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-performance-benchmarks-6623">Performance Benchmarks</h3>
<p>The evolution of benchmarks in computing illustrates how systematic performance measurement has shaped technological progress. During the 1960s and 1970s, when mainframe computers dominated the computing landscape, performance benchmarks focused primarily on fundamental computational tasks. The <a href="https://en.wikipedia.org/wiki/Whetstone_(benchmark)">Whetstone benchmark</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, introduced in 1964 to measure floating-point arithmetic performance, became a definitive standard that demonstrated how systematic testing could drive improvements in computer architecture <span class="citation" data-cites="curnow1976synthetic">(<a href="#ref-curnow1976synthetic" role="doc-biblioref">Curnow 1976</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Introduced in 1964, the Whetstone benchmark was one of the first synthetic benchmarks designed to measure floating-point arithmetic performance, influencing early computer architecture improvements.</p></div><div id="ref-curnow1976synthetic" class="csl-entry" role="listitem">
Curnow, H. J. 1976. <span>“A Synthetic Benchmark.”</span> <em>The Computer Journal</em> 19 (1): 43–49. <a href="https://doi.org/10.1093/comjnl/19.1.43">https://doi.org/10.1093/comjnl/19.1.43</a>.
</div><div id="ref-Weicker1984" class="csl-entry" role="listitem">
Weicker, Reinhold P. 1984. <span>“Dhrystone: A Synthetic Systems Programming Benchmark.”</span> <em>Communications of the ACM</em> 27 (10): 1013–30. <a href="https://doi.org/10.1145/358274.358283">https://doi.org/10.1145/358274.358283</a>.
</div></div><p>The introduction of the <a href="https://en.wikipedia.org/wiki/LINPACK_benchmark">LINPACK benchmark</a> in 1979 expanded the focus of performance evaluation, offering a means to assess how efficiently systems solved linear equations. As computing shifted toward personal computers in the 1980s, the need for standardized performance measurement grew. The <a href="https://en.wikipedia.org/wiki/Dhrystone">Dhrystone benchmark</a>, introduced in 1984, provided one of the first integer-based benchmarks, complementing floating-point evaluations <span class="citation" data-cites="Weicker1984">(<a href="#ref-Weicker1984" role="doc-biblioref">Weicker 1984</a>)</span>.</p>
<p>The late 1980s and early 1990s saw the emergence of systematic benchmarking frameworks that emphasized real-world workloads. The <a href="https://www.spec.org/cpu/">SPEC CPU benchmarks</a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, introduced in 1989 by the <a href="https://www.spec.org/">System Performance Evaluation Cooperative (SPEC)</a>, fundamentally changed hardware evaluation by shifting the focus from synthetic tests to a standardized suite designed to measure performance using practical computing workloads. This approach enabled manufacturers to optimize their systems for real applications, accelerating advances in processor design and software optimization.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Launched in 1989, the SPEC CPU benchmark suite shifted performance evaluation towards real-world workloads, significantly influencing processor design and optimization.</p></div></div><p>The increasing demand for graphics-intensive applications and mobile computing in the 1990s and early 2000s presented new benchmarking challenges. The introduction of <a href="https://www.3dmark.com/">3DMark</a> in 1998 established an industry standard for evaluating graphics performance, shaping the development of programmable shaders and modern GPU architectures. Mobile computing introduced an additional constraint, namely, power efficiency, necessitating benchmarks that assessed both computational performance and energy consumption. The release of <a href="https://bapco.com/products/mobilemark-2014/">MobileMark</a> by <a href="https://bapco.com/">BAPCo</a> provided a means to evaluate power efficiency in laptops and mobile devices, influencing the development of energy-efficient architectures such as <a href="https://www.arm.com/">ARM</a>.</p>
<p>The focus of benchmarking in the past decade has shifted toward cloud computing, big data, and artificial intelligence. Cloud service providers such as Amazon Web Services and Google Cloud optimize their platforms based on performance, scalability, and cost-effectiveness <span class="citation" data-cites="ranganathan2024twenty">(<a href="#ref-ranganathan2024twenty" role="doc-biblioref">Ranganathan and Hölzle 2024</a>)</span>. Benchmarks like <a href="http://cloudsuite.ch/">CloudSuite</a> have become critical for evaluating cloud infrastructure, measuring how well systems handle distributed workloads. Machine learning has introduced another dimension of performance evaluation. The introduction of <a href="https://mlcommons.org/">MLPerf</a> in 2018 established a widely accepted standard for measuring machine learning training and inference efficiency across different hardware architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ranganathan2024twenty" class="csl-entry" role="listitem">
Ranganathan, Parthasarathy, and Urs Hölzle. 2024. <span>“Twenty Five Years of Warehouse-Scale Computing.”</span> <em>IEEE Micro</em> 44 (5): 11–22. <a href="https://doi.org/10.1109/mm.2024.3409469">https://doi.org/10.1109/mm.2024.3409469</a>.
</div></div></section><section id="sec-benchmarking-ai-energy-benchmarks-3d37" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-energy-benchmarks-3d37">Energy Benchmarks</h3>
<p>As computing scaled from personal devices to massive data centers, energy efficiency emerged as a critical dimension of performance evaluation. The mid-2000s marked a shift in benchmarking methodologies, moving beyond raw computational speed to assess power efficiency across diverse computing platforms. The increasing thermal constraints in processor design, coupled with the scaling demands of large-scale internet services, underscored energy consumption as a fundamental consideration in system evaluation <span class="citation" data-cites="Barroso2003">(<a href="#ref-Barroso2003" role="doc-biblioref">Barroso and Hölzle 2007</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Barroso2003" class="csl-entry" role="listitem">
Barroso, Luiz André, and Urs Hölzle. 2007. <span>“The Case for Energy-Proportional Computing.”</span> <em>Computer</em> 40 (12): 33–37. <a href="https://doi.org/10.1109/mc.2007.443">https://doi.org/10.1109/mc.2007.443</a>.
</div></div><p>Power benchmarking addresses three interconnected challenges: environmental sustainability, operational efficiency, and device usability. The growing energy demands of the technology sector have intensified concerns about sustainability, while energy costs continue to shape the economics of data center operations. In mobile computing, power efficiency directly determines battery life and user experience, reinforcing the importance of energy-aware performance measurement.</p>
<p>The industry has responded with several standardized benchmarks that quantify energy efficiency. <a href="https://www.spec.org/power/">SPEC Power</a> provides a widely accepted methodology for measuring server efficiency across varying workload levels, allowing for direct comparisons of power-performance trade-offs. The <a href="https://top500.org/lists/green500/">Green500</a> ranking<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> applies similar principles to high-performance computing, ranking the world’s most powerful supercomputers based on their energy efficiency rather than their raw performance. The <a href="https://www.energystar.gov/products/computers">ENERGY STAR</a> certification program has also established foundational energy standards that have shaped the design of consumer and enterprise computing systems.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Established in 2007, the Green500 ranks supercomputers based on energy efficiency, highlighting advances in power-efficient high-performance computing.</p></div></div><p>Power benchmarking faces distinct challenges, particularly in accounting for the diverse workload patterns and system configurations encountered across different computing environments. Recent advancements, such as the <a href="https://mlcommons.org/">MLPerf Power</a> benchmark, have introduced specialized methodologies for measuring the energy impact of machine learning workloads, addressing the growing importance of energy efficiency in AI-driven computing. As artificial intelligence and edge computing continue to evolve, power benchmarking will play an increasingly crucial role in driving energy-efficient hardware and software innovations.</p>
</section><section id="sec-benchmarking-ai-domainspecific-benchmarks-6269" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-domainspecific-benchmarks-6269">Domain-Specific Benchmarks</h3>
<p>The evolution of computing applications, particularly in artificial intelligence, has highlighted the limitations of general-purpose benchmarks and led to the development of domain-specific evaluation frameworks. Standardized benchmarks, while effective for assessing broad system performance, often fail to capture the unique constraints and operational requirements of specialized workloads. This gap has resulted in the emergence of tailored benchmarking methodologies designed to evaluate performance in specific computing domains <span class="citation" data-cites="Hennessy2003">(<a href="#ref-Hennessy2003" role="doc-biblioref">Hennessy and Patterson 2003</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Hennessy2003" class="csl-entry" role="listitem">
Hennessy, John L, and David A Patterson. 2003. <span>“Computer Architecture: A Quantitative Approach.”</span> <em>Morgan Kaufmann</em>.
</div></div><p>Machine learning presents one of the most prominent examples of this transition. Traditional CPU and GPU benchmarks are insufficient for assessing workloads, which involve complex interactions between computation, memory bandwidth, and data movement. The introduction of MLPerf has standardized performance measurement for machine learning models, providing detailed insights into training and inference efficiency.</p>
<p>Beyond AI, domain-specific benchmarks have been adopted across various industries. Healthcare organizations have developed benchmarking frameworks to evaluate machine learning models used in medical diagnostics, ensuring that performance assessments align with real-world patient data. In financial computing, specialized benchmarking methodologies assess transaction latency and fraud detection accuracy, ensuring that high-frequency trading systems meet stringent timing requirements. Autonomous vehicle developers implement evaluation frameworks that test AI models under varying environmental conditions and traffic scenarios, ensuring the reliability of self-driving systems.</p>
<p>The strength of domain-specific benchmarks lies in their ability to capture workload-specific performance characteristics that general benchmarks may overlook. By tailoring performance evaluation to sector-specific requirements, these benchmarks provide insights that drive targeted optimizations in both hardware and software. As computing continues to expand into new domains, specialized benchmarking will remain a key tool for assessing and improving performance in emerging fields.</p>
</section></section><section id="sec-benchmarking-ai-ai-benchmarks-d3e3" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-ai-benchmarks-d3e3">AI Benchmarks</h2>
<p>The evolution of benchmarks reaches its apex in machine learning, reflecting a journey that parallels the field’s development towards domain-specific applications. Early machine learning benchmarks focused primarily on algorithmic performance, measuring how well models could perform specific tasks <span class="citation" data-cites="lecun1998gradient">(<a href="#ref-lecun1998gradient" role="doc-biblioref">Lecun et al. 1998</a>)</span>. As machine learning applications scaled and computational demands grew, the focus expanded to include system performance and hardware efficiency <span class="citation" data-cites="jouppi2017datacenter">(<a href="#ref-jouppi2017datacenter" role="doc-biblioref">Jouppi et al. 2017</a>)</span>. Most recently, the critical role of data quality has emerged as the third essential dimension of evaluation <span class="citation" data-cites="gebru2021datasheets">(<a href="#ref-gebru2021datasheets" role="doc-biblioref">Gebru et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jouppi2017datacenter" class="csl-entry" role="listitem">
Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. <span>“In-Datacenter Performance Analysis of a Tensor Processing Unit.”</span> In <em>Proceedings of the 44th Annual International Symposium on Computer Architecture</em>, 45:1–12. 2. ACM. <a href="https://doi.org/10.1145/3079856.3080246">https://doi.org/10.1145/3079856.3080246</a>.
</div></div><p>What sets AI benchmarks apart from traditional performance metrics is their inherent variability, introducing accuracy as a fundamental dimension of evaluation. Unlike conventional benchmarks, which measure fixed, deterministic characteristics like computational speed or energy consumption, AI benchmarks must account for the probabilistic nature of machine learning models. The same system can produce different results depending on the data it encounters, making accuracy a defining factor in performance assessment. This distinction adds complexity, as benchmarking AI systems requires not only measuring raw computational efficiency but also understanding trade-offs between accuracy, generalization, and resource constraints.</p>
<p>The growing complexity and ubiquity of machine learning systems demand comprehensive benchmarking across all three dimensions: algorithmic models, hardware systems, and training data. This multifaceted evaluation approach represents a significant departure from earlier benchmarks that could focus on isolated aspects like computational speed or energy efficiency <span class="citation" data-cites="hernandez2020measuring">(<a href="#ref-hernandez2020measuring" role="doc-biblioref">Hernandez and Brown 2020</a>)</span>. Modern machine learning benchmarks must address the sophisticated interplay between these dimensions, as limitations in any one area can fundamentally constrain overall system performance.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hernandez2020measuring" class="csl-entry" role="listitem">
Hernandez, Danny, and Tom B. Brown. 2020. <span>“Measuring the Algorithmic Efficiency of Neural Networks.”</span> <em>arXiv Preprint arXiv:2007.03051</em>, May. <a href="https://doi.org/10.48550/arxiv.2005.04305">https://doi.org/10.48550/arxiv.2005.04305</a>.
</div><div id="ref-jouppi2021ten" class="csl-entry" role="listitem">
Jouppi, Norman P., Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, et al. 2021. <span>“Ten Lessons from Three Generations Shaped Google’s TPUv4i : Industrial Product.”</span> In <em>2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</em>, 64:1–14. 5. IEEE. <a href="https://doi.org/10.1109/isca52012.2021.00010">https://doi.org/10.1109/isca52012.2021.00010</a>.
</div><div id="ref-bender2021stochastic" class="csl-entry" role="listitem">
Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. <span>“On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.”</span> In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 610–23. ACM. <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>.
</div></div><p>This evolution in benchmark complexity mirrors the field’s deepening understanding of what drives machine learning system success. While algorithmic innovations initially dominated progress metrics, the challenges of deploying models at scale revealed the critical importance of hardware efficiency <span class="citation" data-cites="jouppi2021ten">(<a href="#ref-jouppi2021ten" role="doc-biblioref">Jouppi et al. 2021</a>)</span>. Subsequently, high-profile failures of machine learning systems in real-world deployments highlighted how data quality and representation fundamentally determine system reliability and fairness <span class="citation" data-cites="bender2021stochastic">(<a href="#ref-bender2021stochastic" role="doc-biblioref">Bender et al. 2021</a>)</span>. Understanding how these dimensions interact has become essential for accurately assessing machine learning system performance, informing development decisions, and measuring technological progress in the field.</p>
<section id="sec-benchmarking-ai-algorithmic-benchmarks-839e" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-algorithmic-benchmarks-839e">Algorithmic Benchmarks</h3>
<p>AI algorithms must balance multiple interconnected performance objectives, including accuracy, speed, resource efficiency, and generalization capability. As machine learning applications span diverse domains, including computer vision, natural language processing, speech recognition, and reinforcement learning, evaluating these objectives requires standardized methodologies tailored to each domain’s unique challenges. Algorithmic benchmarks, such as ImageNet <span class="citation" data-cites="deng2009imagenet">(<a href="#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>)</span>, establish these evaluation frameworks, providing a consistent basis for comparing different machine learning approaches.</p>
<div class="no-row-height column-margin column-container"></div><div class="callout callout-style-default callout-note callout-titled" title="Definition of Machine Learning  Algorithmic Benchmarks">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of Machine Learning Algorithmic Benchmarks
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>ML Algorithmic benchmarks</strong> refer to the evaluation of machine learning models on <em>standardized tasks</em> using <em>predefined datasets and metrics</em>. These benchmarks measure <em>accuracy, efficiency, and generalization</em> to ensure <em>objective comparisons</em> across different models. Algorithmic benchmarks provide <em>performance baselines</em>, enabling systematic assessment of <em>trade-offs between model complexity and computational cost</em>. They drive <em>technological progress</em> by tracking improvements over time and identifying <em>limitations</em> in existing approaches.</p>
</div>
</div>
<p>Algorithmic benchmarks serve several critical functions in advancing AI. They establish clear performance baselines, enabling objective comparisons between competing approaches. By systematically evaluating trade-offs between model complexity, computational requirements, and task performance, they help researchers and practitioners identify optimal design choices. Moreover, they track technological progress by documenting improvements over time, guiding the development of new techniques while exposing limitations in existing methodologies.</p>
<p>For instance, the graph in <a href="#fig-imagenet-challenge" class="quarto-xref">Figure&nbsp;1</a> illustrates the significant reduction in error rates on the <a href="https://www.image-net.org/challenges/LSVRC/">ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</a> classification task over the years. Starting from the baseline models in 2010 and 2011, the introduction of AlexNet in 2012 marked a substantial improvement, reducing the error rate from 25.8% to 16.4%. Subsequent models like ZFNet, VGGNet, GoogleNet, and ResNet continued this trend, with ResNet achieving a remarkable error rate of 3.57% by 2015. This progression highlights how algorithmic benchmarks not only measure current capabilities but also drive continuous advancements in AI performance.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-imagenet-challenge" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-imagenet-challenge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="benchmarking_files/figure-html/fig-imagenet-challenge-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: ImageNet Challenge Progression: Neural networks have significantly reduced error rates over time, from 25.8% in 2010 to 3.57% by 2015, highlighting the impact of architectural advancements on classification accuracy."><img src="benchmarking_files/figure-html/fig-imagenet-challenge-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-imagenet-challenge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>ImageNet Challenge Progression</strong>: Neural networks have significantly reduced error rates over time, from 25.8% in 2010 to 3.57% by 2015, highlighting the impact of architectural advancements on classification accuracy.
</figcaption></figure>
</div>
</div>
</div>
</section><section id="sec-benchmarking-ai-system-benchmarks-a3ce" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-system-benchmarks-a3ce">System Benchmarks</h3>
<p>AI computations, particularly in machine learning, place extraordinary demands on computational resources. The underlying hardware infrastructure, encompassing general-purpose CPUs, graphics processing units (GPUs), tensor processing units (TPUs), and application-specific integrated circuits (ASICs), fundamentally determines the speed, efficiency, and scalability of AI solutions. System benchmarks establish standardized methodologies for evaluating hardware performance across diverse AI workloads, measuring critical metrics including computational throughput, memory bandwidth, power efficiency, and scaling characteristics <span class="citation" data-cites="reddi2020mlperf mattson2020mlperf">(<a href="#ref-reddi2020mlperf" role="doc-biblioref">Reddi et al. 2019</a>; <a href="#ref-mattson2020mlperf" role="doc-biblioref">Mattson et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mattson2020mlperf" class="csl-entry" role="listitem">
Mattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020. <span>“MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance.”</span> <em>IEEE Micro</em> 40 (2): 8–16. <a href="https://doi.org/10.1109/mm.2020.2974843">https://doi.org/10.1109/mm.2020.2974843</a>.
</div></div><div class="callout callout-style-default callout-note callout-titled" title="Definition of Machine Learning System Benchmarks">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of Machine Learning System Benchmarks
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>ML System benchmarks</strong> refer to the evaluation of <em>computational infrastructure</em> used to execute AI workloads, assessing <em>performance, efficiency, and scalability</em> under standardized conditions. These benchmarks measure <em>throughput, latency, and resource utilization</em> to ensure <em>objective comparisons</em> across different system configurations. System benchmarks provide <em>insights into workload efficiency</em>, guiding <em>infrastructure selection, system optimization,</em> and <em>advancements in computational architectures</em>.</p>
</div>
</div>
<p>These benchmarks fulfill two essential functions in the AI ecosystem. First, they enable developers and organizations to make informed decisions when selecting hardware platforms for their AI applications by providing comprehensive comparative performance data across system configurations. Critical evaluation factors include training speed, inference latency, energy efficiency, and cost-effectiveness. Second, hardware manufacturers rely on these benchmarks to quantify generational improvements and guide the development of specialized AI accelerators, driving continuous advancement in computational capabilities.</p>
<p>System benchmarks evaluate performance across multiple scales, ranging from single-chip configurations to large distributed systems, and diverse AI workloads including both training and inference tasks. This comprehensive evaluation approach ensures that benchmarks accurately reflect real-world deployment scenarios and deliver actionable insights that inform both hardware selection decisions and system architecture design. For example, <a href="#fig-imagenet-gpus" class="quarto-xref">Figure&nbsp;2</a> illustrates the correlation between ImageNet classification error rates and GPU adoption from 2010 to 2014. These results clearly highlight how improved hardware capabilities, combined with algorithmic advances, drove significant progress in computer vision performance.</p>
<div id="fig-imagenet-gpus" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-imagenet-gpus-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="511cedd345ba6d417355c7cc4a89ab4230ceebb3.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: ImageNet Benchmark: Advancements in GPU technology have driven improvements in ImageNet classification accuracy since 2012, showcasing the interplay between hardware and algorithmic progress."><img src="benchmarking_files/mediabag/511cedd345ba6d417355c7cc4a89ab4230ceebb3.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-imagenet-gpus-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>ImageNet Benchmark</strong>: Advancements in GPU technology have driven improvements in ImageNet classification accuracy since 2012, showcasing the interplay between hardware and algorithmic progress.
</figcaption></figure>
</div>
</section><section id="sec-benchmarking-ai-data-benchmarks-2dd8" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-data-benchmarks-2dd8">Data Benchmarks</h3>
<p>Data quality, scale, and diversity fundamentally shape machine learning system performance, directly influencing how effectively algorithms learn and generalize to new situations. Data benchmarks establish standardized datasets and evaluation methodologies that enable consistent comparison of different approaches. These frameworks assess critical aspects of data quality, including domain coverage, potential biases, and resilience to real-world variations in input data <span class="citation" data-cites="gebru2021datasheets">(<a href="#ref-gebru2021datasheets" role="doc-biblioref">Gebru et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gebru2021datasheets" class="csl-entry" role="listitem">
Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. <span>“Datasheets for Datasets.”</span> <em>Communications of the ACM</em> 64 (12): 86–92. <a href="https://doi.org/10.1145/3458723">https://doi.org/10.1145/3458723</a>.
</div></div><div class="callout callout-style-default callout-note callout-titled" title="Definition of Machine Learning Data Benchmarks">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of Machine Learning Data Benchmarks
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>ML Data benchmarks</strong> refer to the evaluation of <em>datasets and data quality</em> in machine learning, assessing <em>coverage, bias, and robustness</em> under standardized conditions. These benchmarks measure <em>data representativeness, consistency, and impact on model performance</em> to ensure <em>objective comparisons</em> across different AI approaches. Data benchmarks provide <em>insights into data reliability</em>, guiding <em>dataset selection, bias mitigation,</em> and <em>improvements in data-driven AI systems</em>.</p>
</div>
</div>
<p>Data benchmarks serve an essential function in understanding AI system behavior under diverse data conditions. Through systematic evaluation, they help identify common failure modes, expose gaps in data coverage, and reveal underlying biases that could impact model behavior in deployment. By providing common frameworks for data evaluation, these benchmarks enable the AI community to systematically improve data quality and address potential issues before deploying systems in production environments. This proactive approach to data quality assessment has become increasingly critical as AI systems take on more complex and consequential tasks across different domains.</p>
</section><section id="sec-benchmarking-ai-community-consensus-fbb9" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-community-consensus-fbb9">Community Consensus</h3>
<p>The proliferation of benchmarks spanning performance, energy efficiency, and domain-specific applications creates a fundamental challenge: establishing industry-wide standards. While early computing benchmarks primarily measured processor speed and memory bandwidth, modern benchmarks evaluate sophisticated aspects of system performance, from power consumption profiles to application-specific capabilities. This evolution in scope and complexity necessitates comprehensive validation and consensus from the computing community, particularly in rapidly evolving fields like machine learning where performance must be evaluated across multiple interdependent dimensions.</p>
<p>The lasting impact of a benchmark depends fundamentally on its acceptance by the research community, where technical excellence alone proves insufficient. Benchmarks developed without broad community input often fail to gain traction, frequently missing metrics that leading research groups consider essential. Successful benchmarks emerge through collaborative development involving academic institutions, industry partners, and domain experts. This inclusive approach ensures benchmarks evaluate capabilities most crucial for advancing the field, while balancing theoretical and practical considerations.</p>
<p>Benchmarks developed through extensive collaboration among respected institutions carry the authority necessary to drive widespread adoption, while those perceived as advancing particular corporate interests face skepticism and limited acceptance. The success of ImageNet demonstrates how sustained community engagement through workshops and challenges establishes long-term viability. This community-driven development creates a foundation for formal standardization, where organizations like IEEE and ISO transform these benchmarks into official standards.</p>
<p>The standardization process provides crucial infrastructure for benchmark formalization and adoption. <a href="https://standards.ieee.org/develop/wg/">IEEE working groups</a> transform community-developed benchmarking methodologies into formal industry standards, establishing precise specifications for measurement and reporting. The <a href="https://standards.ieee.org/ieee/2416/7065/">IEEE 2416-2019</a> standard for system power modeling<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> exemplifies this process, codifying best practices developed through community consensus. Similarly, <a href="https://www.iso.org/committee/45020.html">ISO/IEC technical committees</a> develop international standards for benchmark validation and certification, ensuring consistent evaluation across global research and industry communities. These organizations bridge the gap between community-driven innovation and formal standardization, providing frameworks that enable reliable comparison of results across different institutions and geographic regions.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<strong>IEEE 2416-2019</strong>: A standard defining methodologies for parameterized power modeling, enabling system-level power analysis and optimization in electronic design, including AI hardware.</p></div></div><p>Successful community benchmarks establish clear governance structures for managing their evolution. Through rigorous version control systems and detailed change documentation, benchmarks maintain backward compatibility while incorporating new advances. This governance includes formal processes for proposing, reviewing, and implementing changes, ensuring that benchmarks remain relevant while maintaining stability. Modern benchmarks increasingly emphasize reproducibility requirements, incorporating automated verification systems and standardized evaluation environments.</p>
<p>Open access accelerates benchmark adoption and ensures consistent implementation. Projects that provide open-source reference implementations, comprehensive documentation, validation suites, and containerized evaluation environments reduce barriers to entry. This standardization enables research groups to evaluate solutions using uniform methods and metrics. Without such coordinated implementation frameworks, organizations might interpret benchmarks inconsistently, compromising result reproducibility and meaningful comparison across studies.</p>
<p>The most successful benchmarks strike a careful balance between academic rigor and industry practicality. Academic involvement ensures theoretical soundness and comprehensive evaluation methodology, while industry participation grounds benchmarks in practical constraints and real-world applications. This balance proves particularly crucial in machine learning benchmarks, where theoretical advances must translate to practical improvements in deployed systems <span class="citation" data-cites="patterson2021carbon">(<a href="#ref-patterson2021carbon" role="doc-biblioref">Patterson et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-patterson2021carbon" class="csl-entry" role="listitem">
Patterson, David, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. <span>“Carbon Emissions and Large Neural Network Training.”</span> <em>arXiv Preprint arXiv:2104.10350</em>, April. <a href="http://arxiv.org/abs/2104.10350v3">http://arxiv.org/abs/2104.10350v3</a>.
</div></div><p>Community consensus establishes enduring benchmark relevance, while fragmentation impedes scientific progress. Through collaborative development and transparent operation, benchmarks evolve into authoritative standards for measuring advancement. The most successful benchmarks in energy efficiency and domain-specific applications share this foundation of community development and governance, demonstrating how collective expertise and shared purpose create lasting impact in rapidly advancing fields.</p>
<div id="quiz-question-sec-benchmarking-ai-ai-benchmarks-d3e3" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li>
<p>What sets AI benchmarks apart from traditional performance metrics?</p>
<ol type="a">
<li>They focus solely on computational speed.</li>
<li>They account for the probabilistic nature of machine learning models.</li>
<li>They measure only energy consumption.</li>
<li>They are fixed and deterministic.</li>
</ol>
</li>
<li><p>Explain why system benchmarks are crucial for AI computations.</p></li>
<li><p>True or False: Data benchmarks only assess the computational speed of machine learning models.</p></li>
</ol>
<p><a href="#quiz-answer-sec-benchmarking-ai-ai-benchmarks-d3e3" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section></section><section id="sec-benchmarking-ai-benchmark-components-77e9" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-benchmark-components-77e9">Benchmark Components</h2>
<p>An AI benchmark provides a structured framework for evaluating artificial intelligence systems. While individual benchmarks vary in their specific focus, they share common components that enable systematic evaluation and comparison of AI models.</p>
<p><a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a> illustrates the structured workflow of a benchmark implementation, showcasing how components like task definition, dataset selection, model selection, and evaluation interconnect to form a complete evaluation pipeline. This visualization highlights how each phase builds upon the previous one, ensuring systematic and reproducible AI performance assessment.</p>
<div id="fig-benchmark-components" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-benchmark-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/benchmarking_components.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Benchmark Workflow: AI benchmarks standardize evaluation through a structured pipeline, enabling reproducible performance comparisons across different models and systems. This workflow systematically assesses AI capabilities by defining tasks, selecting datasets, training models, and rigorously evaluating results."><img src="images/png/benchmarking_components.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-benchmark-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Benchmark Workflow</strong>: AI benchmarks standardize evaluation through a structured pipeline, enabling reproducible performance comparisons across different models and systems. This workflow systematically assesses AI capabilities by defining tasks, selecting datasets, training models, and rigorously evaluating results.
</figcaption></figure>
</div>
<section id="sec-benchmarking-ai-problem-definition-ed9a" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-problem-definition-ed9a">Problem Definition</h3>
<p>A benchmark implementation begins with a formal specification of the machine learning task and its evaluation criteria. In machine learning, tasks represent well-defined problems that AI systems must solve. Consider an anomaly detection system that processes audio signals to identify deviations from normal operation patterns, as shown in <a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a>. This industrial monitoring application exemplifies how formal task specifications translate into practical implementations.</p>
<p>The formal definition of a benchmark task encompasses both the computational problem and its evaluation framework. While the specific tasks vary by domain, well-established categories have emerged across fields. Natural language processing tasks, for example, include machine translation, question answering <span class="citation" data-cites="hirschberg2015advances">(<a href="#ref-hirschberg2015advances" role="doc-biblioref">Hirschberg and Manning 2015</a>)</span>, and text classification. Computer vision similarly employs standardized tasks such as object detection, image segmentation, and facial recognition <span class="citation" data-cites="everingham2010pascal">(<a href="#ref-everingham2010pascal" role="doc-biblioref">Everingham et al. 2009</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hirschberg2015advances" class="csl-entry" role="listitem">
Hirschberg, Julia, and Christopher D. Manning. 2015. <span>“Advances in Natural Language Processing.”</span> <em>Science</em> 349 (6245): 261–66. <a href="https://doi.org/10.1126/science.aaa8685">https://doi.org/10.1126/science.aaa8685</a>.
</div><div id="ref-everingham2010pascal" class="csl-entry" role="listitem">
Everingham, Mark, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. 2009. <span>“The Pascal Visual Object Classes (VOC) Challenge.”</span> <em>International Journal of Computer Vision</em> 88 (2): 303–38. <a href="https://doi.org/10.1007/s11263-009-0275-4">https://doi.org/10.1007/s11263-009-0275-4</a>.
</div></div><p>Every benchmark task specification must define three fundamental elements. The input specification determines what data the system processes. In <a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a>, this consists of audio waveform data. The output specification describes the required system response, such as the binary classification of normal versus anomalous patterns. The performance specification establishes quantitative requirements for accuracy, processing speed, and resource utilization.</p>
<p>Task design directly impacts the benchmark’s ability to evaluate AI systems effectively. The audio anomaly detection example illustrates this relationship through its specific requirements: processing continuous signal data, adapting to varying noise conditions, and operating within strict time constraints. These practical constraints create a detailed framework for assessing model performance, ensuring evaluations reflect real-world operational demands.</p>
<p>The implementation of a benchmark proceeds systematically from this task definition. Each subsequent phase - from dataset selection through deployment - builds upon these initial specifications, ensuring that evaluations maintain consistency while addressing the defined requirements across different approaches and implementations.</p>
</section><section id="sec-benchmarking-ai-standardized-datasets-a1d6" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-standardized-datasets-a1d6">Standardized Datasets</h3>
<p>Building upon the problem definition, standardized datasets provide the foundation for training and evaluating models. These carefully curated collections ensure all models undergo testing under identical conditions, enabling direct comparisons across different approaches and architectures. <a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a> demonstrates this through an audio anomaly detection example, where waveform data serves as the standardized input for evaluating detection performance.</p>
<p>In computer vision, datasets such as <a href="http://www.image-net.org/">ImageNet</a> <span class="citation" data-cites="deng2009imagenet">(<a href="#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>)</span>, <a href="https://cocodataset.org/">COCO</a> <span class="citation" data-cites="lin2014microsoft">(<a href="#ref-lin2014microsoft" role="doc-biblioref">Lin et al. 2014</a>)</span>, and <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> <span class="citation" data-cites="krizhevsky2009learning">(<a href="#ref-krizhevsky2009learning" role="doc-biblioref">Krizhevsky, Hinton, et al. 2009</a>)</span> serve as reference standards. For natural language processing, collections such as <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a> <span class="citation" data-cites="rajpurkar2016squad">(<a href="#ref-rajpurkar2016squad" role="doc-biblioref">Rajpurkar et al. 2016</a>)</span>, <a href="https://gluebenchmark.com/">GLUE</a> <span class="citation" data-cites="wang2018glue">(<a href="#ref-wang2018glue" role="doc-biblioref">Wang et al. 2018</a>)</span>, and <a href="https://www.salesforce.com/blog/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText</a> <span class="citation" data-cites="merity2016pointer">(<a href="#ref-merity2016pointer" role="doc-biblioref">Merity et al. 2016</a>)</span> fulfill similar functions. These datasets encompass a range of complexities and edge cases to thoroughly evaluate machine learning systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-krizhevsky2009learning" class="csl-entry" role="listitem">
Krizhevsky, Alex, Geoffrey Hinton, et al. 2009. <span>“Learning Multiple Layers of Features from Tiny Images.”</span>
</div><div id="ref-rajpurkar2016squad" class="csl-entry" role="listitem">
Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. <span>“SQuAD: 100,000+ Questions for Machine Comprehension of Text.”</span> <em>arXiv Preprint arXiv:1606.05250</em>, June, 2383–92. <a href="https://doi.org/10.18653/v1/d16-1264">https://doi.org/10.18653/v1/d16-1264</a>.
</div><div id="ref-merity2016pointer" class="csl-entry" role="listitem">
Merity, Stephen, Caiming Xiong, James Bradbury, and Richard Socher. 2016. <span>“Pointer Sentinel Mixture Models.”</span> <em>arXiv Preprint arXiv:1609.07843</em>, September. <a href="http://arxiv.org/abs/1609.07843v1">http://arxiv.org/abs/1609.07843v1</a>.
</div></div><p>The strategic selection of datasets, shown early in the workflow of <a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a>, shapes all subsequent implementation steps and determines the benchmark’s effectiveness. In the audio anomaly detection example, the dataset must include representative waveform samples of normal operation alongside examples of various anomalous conditions. Notable examples include datasets like ToyADMOS for industrial manufacturing anomalies and Google Speech Commands for general sound recognition. Regardless of the specific dataset chosen, the data volume must suffice for both model training and validation, while incorporating real-world signal characteristics and noise patterns that reflect deployment conditions.</p>
<p>The selection of benchmark datasets fundamentally shapes experimental outcomes and model evaluation. Effective datasets must balance two key requirements: accurately representing real-world challenges while maintaining sufficient complexity to differentiate model performance meaningfully. While research often utilizes simplified datasets like ToyADMOS <span class="citation" data-cites="koizumi2019toyadmos">(<a href="#ref-koizumi2019toyadmos" role="doc-biblioref">Koizumi et al. 2019</a>)</span>, these controlled environments, though valuable for methodological development, may not fully capture real-world deployment complexities. Benchmark development frequently necessitates combining multiple datasets due to access limitations on proprietary industrial data. As machine learning capabilities advance, benchmark datasets must similarly evolve to maintain their utility in evaluating contemporary systems and emerging challenges.</p>
<div class="no-row-height column-margin column-container"><div id="ref-koizumi2019toyadmos" class="csl-entry" role="listitem">
Koizumi, Yuma, Shoichiro Saito, Hisashi Uematsu, Noboru Harada, and Keisuke Imoto. 2019. <span>“ToyADMOS: A Dataset of Miniature-Machine Operating Sounds for Anomalous Sound Detection.”</span> In <em>2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em>, 313–17. IEEE; IEEE. <a href="https://doi.org/10.1109/waspaa.2019.8937164">https://doi.org/10.1109/waspaa.2019.8937164</a>.
</div></div></section><section id="sec-benchmarking-ai-model-selection-3b14" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-model-selection-3b14">Model Selection</h3>
<p>The benchmark process advances systematically from initial task definition to model architecture selection and implementation. This critical phase establishes performance baselines and determines the optimal modeling approach. <a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a> illustrates this progression through the model selection stage and subsequent training code development.</p>
<p>Baseline models serve as the reference points for evaluating novel approaches. These span from basic implementations, including linear regression for continuous predictions and logistic regression for classification tasks, to advanced architectures with proven success in comparable domains. In natural language processing applications, transformer-based models like BERT have emerged as standard benchmarks for comparative analysis.</p>
<p>Selecting the right baseline model requires careful evaluation of architectures against benchmark requirements. This selection process directly informs the development of training code, which forms the cornerstone of benchmark reproducibility. The training implementation must thoroughly document all aspects of the model pipeline, from data preprocessing through training procedures, enabling precise replication of model behavior across research teams.</p>
<p>Model development follows two primary optimization paths: training and inference. During training optimization, efforts concentrate on achieving target accuracy metrics while operating within computational constraints. The training implementation must demonstrate consistent achievement of performance thresholds under specified conditions.</p>
<p>The inference optimization path addresses deployment considerations, particularly the transition from development to production environments. A key example involves precision reduction through quantization, progressing from FP32 to INT8 representations to enhance deployment efficiency. This process demands careful calibration to maintain model accuracy while reducing resource requirements. The benchmark must detail both the quantization methodology and verification procedures that confirm preserved performance.</p>
<p>The intersection of these optimization paths with real-world constraints shapes deployment strategy. Comprehensive benchmarks must therefore specify requirements for both training and inference scenarios, ensuring models maintain consistent performance from development through deployment. This crucial connection between development and production metrics naturally leads to the establishment of evaluation criteria.</p>
<p>The optimization process must balance four key objectives: model accuracy, computational speed, memory utilization, and energy efficiency. This complex optimization landscape necessitates robust evaluation metrics that can effectively quantify performance across all dimensions. As models transition from development to deployment, these metrics serve as critical tools for guiding optimization decisions and validating performance enhancements.</p>
</section><section id="sec-benchmarking-ai-evaluation-metrics-7a69" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-evaluation-metrics-7a69">Evaluation Metrics</h3>
<p>While model selection establishes the architectural framework, evaluation metrics provide the quantitative measures needed to assess machine learning model performance. These metrics establish objective standards for comparing different approaches, enabling researchers and practitioners to gauge solution effectiveness. The selection of appropriate metrics represents a fundamental aspect of benchmark design, as they must align with task objectives while providing meaningful insights into model behavior across both training and deployment scenarios.</p>
<p>Task-specific metrics quantify a model’s performance on its intended function. Classification tasks employ metrics including accuracy (overall correct predictions), precision (positive prediction accuracy), recall (positive case detection rate), and F1 score (precision-recall harmonic mean) <span class="citation" data-cites="sokolova2009systematic">(<a href="#ref-sokolova2009systematic" role="doc-biblioref">Sokolova and Lapalme 2009</a>)</span>. Regression problems utilize error measurements like Mean Squared Error (MSE) and Mean Absolute Error (MAE) to assess prediction accuracy. Domain-specific applications often require specialized metrics - for example, machine translation uses the BLEU score to evaluate the semantic and syntactic similarity between machine-generated and human reference translations <span class="citation" data-cites="papineni2002bleu">(<a href="#ref-papineni2002bleu" role="doc-biblioref">Papineni et al. 2001</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sokolova2009systematic" class="csl-entry" role="listitem">
Sokolova, Marina, and Guy Lapalme. 2009. <span>“A Systematic Analysis of Performance Measures for Classification Tasks.”</span> <em>Information Processing &amp;Amp; Management</em> 45 (4): 427–37. <a href="https://doi.org/10.1016/j.ipm.2009.03.002">https://doi.org/10.1016/j.ipm.2009.03.002</a>.
</div><div id="ref-papineni2002bleu" class="csl-entry" role="listitem">
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. <span>“BLEU: A Method for Automatic Evaluation of Machine Translation.”</span> In <em>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL ’02</em>, 311. Association for Computational Linguistics. <a href="https://doi.org/10.3115/1073083.1073135">https://doi.org/10.3115/1073083.1073135</a>.
</div></div><p>As models transition from research to production deployment, implementation metrics become equally important. Model size, measured in parameters or memory footprint, affects deployment feasibility across different hardware platforms. Processing latency, typically measured in milliseconds per inference, determines whether the model meets real-time requirements. Energy consumption, measured in watts or joules per inference, indicates operational efficiency. These practical considerations reflect the growing need for solutions that balance accuracy with computational efficiency.</p>
<p>The selection of appropriate metrics requires careful consideration of task requirements and deployment constraints. A single metric rarely captures all relevant aspects of performance. For instance, in anomaly detection systems, high accuracy alone may not indicate good performance if the model generates frequent false alarms. Similarly, a fast model with poor accuracy fails to provide practical value.</p>
<p><a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a> demonstrates this multi-metric evaluation approach. The anomaly detection system reports performance across multiple dimensions: model size (270 Kparameters), processing speed (10.4 ms/inference), and detection accuracy (0.86 AUC). This combination of metrics ensures the model meets both technical and operational requirements in real-world deployment scenarios.</p>
</section><section id="sec-benchmarking-ai-benchmark-harness-bc2f" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-benchmark-harness-bc2f">Benchmark Harness</h3>
<p>Evaluation metrics provide the measurement framework, while a benchmark harness implements the systematic infrastructure for evaluating model performance under controlled conditions. This critical component ensures reproducible testing by managing how inputs are delivered to the system under test and how measurements are collected, effectively transforming theoretical metrics into quantifiable measurements.</p>
<p>The harness design should align with the intended deployment scenario and usage patterns. For server deployments, the harness implements request patterns that simulate real-world traffic, typically generating inputs using a Poisson distribution to model random but statistically consistent server workloads. The harness manages concurrent requests and varying load intensities to evaluate system behavior under different operational conditions.</p>
<p>For embedded and mobile applications, the harness generates input patterns that reflect actual deployment conditions. This might involve sequential image injection for mobile vision applications or synchronized multi-sensor streams for autonomous systems. Such precise input generation and timing control ensures the system experiences realistic operational patterns, revealing performance characteristics that would emerge in actual device deployment.</p>
<p>The harness must also accommodate different throughput models. Batch processing scenarios require the ability to evaluate system performance on large volumes of parallel inputs, while real-time applications need precise timing control for sequential processing. <a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a> illustrates this in the embedded implementation phase, where the harness must support precise measurement of inference time and energy consumption per operation.</p>
<p>Reproducibility demands that the harness maintain consistent testing conditions across different evaluation runs. This includes controlling environmental factors such as background processes, thermal conditions, and power states that might affect performance measurements. The harness must also provide mechanisms for collecting and logging performance metrics without significantly impacting the system under test.</p>
</section><section id="sec-benchmarking-ai-system-specifications-1e97" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-system-specifications-1e97">System Specifications</h3>
<p>Beyond the benchmark harness that controls test execution, system specifications are fundamental components of machine learning benchmarks that directly impact model performance, training time, and experimental reproducibility. These specifications encompass the complete computational environment, ensuring that benchmarking results can be properly contextualized, compared, and reproduced by other researchers.</p>
<p>Hardware specifications typically include:</p>
<ol type="1">
<li>Processor type and speed (e.g., CPU model, clock rate)</li>
<li>GPUs, or TPUs, including model, memory capacity, and quantity if used for distributed training</li>
<li>Memory capacity and type (e.g., RAM size, DDR4)</li>
<li>Storage type and capacity (e.g., SSD, HDD)</li>
<li>Network configuration, if relevant for distributed computing</li>
</ol>
<p>Software specifications generally include:</p>
<ol type="1">
<li>Operating system and version</li>
<li>Programming language and version</li>
<li>Machine learning frameworks and libraries (e.g., TensorFlow, PyTorch) with version numbers</li>
<li>Compiler information and optimization flags</li>
<li>Custom software or scripts used in the benchmark process</li>
<li>Environment management tools and configuration (e.g., Docker containers, virtual environments)</li>
</ol>
<p>The precise documentation of these specifications is essential for experimental validity and reproducibility. This documentation enables other researchers to replicate the benchmark environment with high fidelity, provides critical context for interpreting performance metrics, and facilitates understanding of resource requirements and scaling characteristics across different models and tasks.</p>
<p>In many cases, benchmarks may include results from multiple hardware configurations to provide a more comprehensive view of model performance across different computational environments. This approach is particularly valuable as it highlights the trade-offs between model complexity, computational resources, and performance.</p>
<p>As the field evolves, hardware and software specifications increasingly incorporate detailed energy consumption metrics and computational efficiency measures, such as FLOPS/watt and total power usage over training time. This expansion reflects growing concerns about the environmental impact of large-scale machine learning models and supports the development of more sustainable AI practices. Comprehensive specification documentation thus serves multiple purposes: enabling reproducibility, supporting fair comparisons, and advancing both the technical and environmental aspects of machine learning research.</p>
</section><section id="sec-benchmarking-ai-run-rules-4c81" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-run-rules-4c81">Run Rules</h3>
<p>Run rules establish the procedural framework that ensures benchmark results can be reliably replicated by researchers and practitioners, complementing the technical environment defined by system specifications. These guidelines are fundamental for validating research claims, building upon existing work, and advancing machine learning. Central to reproducibility in AI benchmarks is the management of controlled randomness—the systematic handling of stochastic processes such as weight initialization and data shuffling that ensures consistent, verifiable results.</p>
<p>Comprehensive documentation of hyperparameters forms a critical component of reproducibility. Hyperparameters are configuration settings that govern the learning process independently of the training data, including learning rates, batch sizes, and network architectures. Given that minor hyperparameter adjustments can significantly impact model performance, their precise documentation is essential. Additionally, benchmarks mandate the preservation and sharing of training and evaluation datasets. When direct data sharing is restricted by privacy or licensing constraints, benchmarks must provide detailed specifications for data preprocessing and selection criteria, enabling researchers to construct comparable datasets or understand the characteristics of the original experimental data.</p>
<p>Code provenance and availability constitute another vital aspect of reproducibility guidelines. Contemporary benchmarks typically require researchers to publish implementation code in version-controlled repositories, encompassing not only the model implementation but also comprehensive scripts for data preprocessing, training, and evaluation. Advanced benchmarks often provide containerized environments that encapsulate all dependencies and configurations. Furthermore, detailed experimental logging is mandatory, including systematic recording of training metrics, model checkpoints, and documentation of any experimental adjustments.</p>
<p>These reproducibility guidelines serve multiple crucial functions: they enhance transparency, enable rigorous peer review, and accelerate scientific progress in AI research. By following these protocols, the research community can effectively verify results, iterate on successful approaches, and identify methodological limitations. In the rapidly evolving landscape of machine learning, these robust reproducibility practices form the foundation for reliable and progressive research.</p>
</section><section id="sec-benchmarking-ai-result-interpretation-62db" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-result-interpretation-62db">Result Interpretation</h3>
<p>Building upon the foundation established by run rules, result interpretation guidelines provide the essential framework for understanding and contextualizing benchmark outcomes. These guidelines help researchers and practitioners draw meaningful conclusions from benchmark results, ensuring fair and informative comparisons between different models or approaches. A fundamental aspect is understanding the statistical significance of performance differences. Benchmarks typically specify protocols for conducting statistical tests and reporting confidence intervals, enabling practitioners to distinguish between meaningful improvements and variations attributable to random factors.</p>
<p>Result interpretation requires careful consideration of real-world applications. While a 1% improvement in accuracy might be crucial for medical diagnostics or financial systems, other applications might prioritize inference speed or model efficiency over marginal accuracy gains. Understanding these context-specific requirements is essential for meaningful interpretation of benchmark results. Users must also recognize inherent benchmark limitations, as no single evaluation framework can encompass all possible use cases. Common limitations include dataset biases, task-specific characteristics, and constraints of evaluation metrics.</p>
<p>Modern benchmarks often necessitate multi-dimensional analysis across various performance metrics. For instance, when a model demonstrates superior accuracy but requires substantially more computational resources, interpretation guidelines help practitioners evaluate these trade-offs based on their specific constraints and requirements. The guidelines also address the critical issue of benchmark overfitting, where models might be excessively optimized for specific benchmark tasks at the expense of real-world generalization. To mitigate this risk, guidelines often recommend evaluating model performance on related but distinct tasks and considering practical deployment scenarios.</p>
<p>These comprehensive interpretation frameworks ensure that benchmarks serve their intended purpose: providing standardized performance measurements while enabling nuanced understanding of model capabilities. This balanced approach supports evidence-based decision-making in both research contexts and practical machine learning applications.</p>
</section><section id="sec-benchmarking-ai-example-benchmark-5ed2" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-example-benchmark-5ed2">Example Benchmark</h3>
<p>A benchmark run evaluates system performance by synthesizing multiple components under controlled conditions to produce reproducible measurements. <a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a> illustrates this integration through an audio anomaly detection system. It shows how performance metrics are systematically measured and reported within a framework that encompasses problem definition, datasets, model selection, evaluation criteria, and standardized run rules.</p>
<p>The benchmark measures several key performance dimensions. For computational resources, the system reports a model size of 270 Kparameters and requires 10.4 milliseconds per inference. For task effectiveness, it achieves a detection accuracy of 0.86 AUC (Area Under Curve) in distinguishing normal from anomalous audio patterns. For operational efficiency, it consumes 516 µJ of energy per inference.</p>
<p>The relative importance of these metrics varies by deployment context. Energy consumption per inference is critical for battery-powered devices but less consequential for systems with constant power supply. Model size constraints differ significantly between cloud deployments with abundant resources and embedded devices with limited memory. Processing speed requirements depend on whether the system must operate in real-time or can process data in batches.</p>
<p>The benchmark reveals inherent trade-offs between performance metrics in machine learning systems. For instance, reducing the model size from 270 Kparameters might improve processing speed and energy efficiency but could decrease the 0.86 AUC detection accuracy. <a href="#fig-benchmark-components" class="quarto-xref">Figure&nbsp;3</a> illustrates how these interconnected metrics contribute to overall system performance in the deployment phase.</p>
<p>Whether these measurements constitute a “passing” benchmark depends on the specific requirements of the intended application. The benchmark framework provides the structure and methodology for consistent evaluation, while the acceptance criteria must align with deployment constraints and performance requirements.</p>
<div id="quiz-question-sec-benchmarking-ai-benchmark-components-77e9" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li>
<p>Which component of an AI benchmark ensures that models are evaluated under consistent conditions, allowing for reproducible results?</p>
<ol type="a">
<li>Problem Definition</li>
<li>Standardized Datasets</li>
<li>Benchmark Harness</li>
<li>System Specifications</li>
</ol>
</li>
<li><p>Explain how the selection of standardized datasets influences the effectiveness of an AI benchmark.</p></li>
<li><p>True or False: Evaluation metrics in AI benchmarks should focus solely on model accuracy to ensure effective performance assessment.</p></li>
<li><p>Order the following components in the sequence they typically occur in an AI benchmarking workflow: Model Selection, Problem Definition, Evaluation Metrics, Standardized Datasets.</p></li>
</ol>
<p><a href="#quiz-answer-sec-benchmarking-ai-benchmark-components-77e9" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section></section><section id="sec-benchmarking-ai-benchmarking-granularity-8676" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-benchmarking-granularity-8676">Benchmarking Granularity</h2>
<p>While benchmarking components individually provides detailed insights into model selection, dataset efficiency, and evaluation metrics, a complete assessment of machine learning systems requires analyzing performance across different levels of abstraction. Benchmarks can range from fine-grained evaluations of individual tensor operations to holistic end-to-end measurements of full AI pipelines.</p>
<p>System level benchmarking provides a structured and systematic approach to assessing a ML system’s performance across various dimensions. Given the complexity of ML systems, we can dissect their performance through different levels of granularity and obtain a comprehensive view of the system’s efficiency, identify potential bottlenecks, and pinpoint areas for improvement. To this end, various types of benchmarks have evolved over the years and continue to persist.</p>
<p><a href="#fig-granularity" class="quarto-xref">Figure&nbsp;4</a> shows the different layers of granularity of an ML system. At the application level, end-to-end benchmarks assess the overall system performance, considering factors like data preprocessing, model training, and inference. While at the model layer, benchmarks focus on assessing the efficiency and accuracy of specific models. This includes evaluating how well models generalize to new data and their computational efficiency during training and inference. Furthermore, benchmarking can extend to hardware and software infrastructure, examining the performance of individual components like GPUs or TPUs.</p>
<div id="fig-granularity" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="3d24215d5f76ade022dba23ee7788ce40988b8de.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Benchmarking Granularity: ML system performance assessment occurs at multiple levels, from end-to-end application metrics to individual model and hardware component efficiency, enabling targeted optimization and bottleneck identification. This hierarchical approach allows practitioners to systematically analyze system performance and prioritize improvements based on specific component limitations."><img src="benchmarking_files/mediabag/3d24215d5f76ade022dba23ee7788ce40988b8de.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Benchmarking Granularity</strong>: ML system performance assessment occurs at multiple levels, from end-to-end application metrics to individual model and hardware component efficiency, enabling targeted optimization and bottleneck identification. This hierarchical approach allows practitioners to systematically analyze system performance and prioritize improvements based on specific component limitations.
</figcaption></figure>
</div>
<section id="sec-benchmarking-ai-micro-benchmarks-cd5e" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-micro-benchmarks-cd5e">Micro Benchmarks</h3>
<p>Micro-benchmarks are specialized evaluation tools that assess distinct components or specific operations within a broader machine learning process. These benchmarks isolate individual tasks to provide detailed insights into the computational demands of particular system elements, from neural network layers to optimization techniques to activation functions. For example, micro-benchmarks might measure the time required to execute a convolutional layer in a deep learning model or evaluate the speed of data preprocessing operations that prepare training data.</p>
<p>A key area of micro-benchmarking focuses on tensor operations, which are the computational foundation of deep learning. Libraries like <a href="https://developer.nvidia.com/cudnn">cuDNN</a> by NVIDIA provide benchmarks for measuring fundamental computations such as convolutions and matrix multiplications across different hardware configurations. These measurements help developers understand how their hardware handles the core mathematical operations that dominate ML workloads.</p>
<p>Micro-benchmarks also examine activation functions and neural network layers in isolation. This includes measuring the performance of various activation functions like ReLU, Sigmoid, and Tanh under controlled conditions, as well as evaluating the computational efficiency of distinct neural network components such as LSTM cells or Transformer blocks when processing standardized inputs.</p>
<p><a href="https://github.com/baidu-research/DeepBench">DeepBench</a>, developed by Baidu, was one of the first to demonstrate the value of comprehensive micro-benchmarking. It evaluates these fundamental operations across different hardware platforms, providing detailed performance data that helps developers optimize their deep learning implementations. By isolating and measuring individual operations, DeepBench enables precise comparison of hardware platforms and identification of potential performance bottlenecks.</p>
</section><section id="sec-benchmarking-ai-macro-benchmarks-eb7e" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-macro-benchmarks-eb7e">Macro Benchmarks</h3>
<p>While micro-benchmarks examine individual operations like tensor computations and layer performance, macro benchmarks evaluate complete machine learning models. This shift from component-level to model-level assessment provides insights into how architectural choices and component interactions affect overall model behavior. For instance, while micro-benchmarks might show optimal performance for individual convolutional layers, macro-benchmarks reveal how these layers work together within a complete convolutional neural network.</p>
<p>Macro-benchmarks measure multiple performance dimensions that emerge only at the model level. These include prediction accuracy, which shows how well the model generalizes to new data; memory consumption patterns across different batch sizes and sequence lengths; throughput under varying computational loads; and latency across different hardware configurations. Understanding these metrics helps developers make informed decisions about model architecture, optimization strategies, and deployment configurations.</p>
<p>The assessment of complete models occurs under standardized conditions using established datasets and tasks. For example, computer vision models might be evaluated on <a href="https://www.image-net.org/">ImageNet</a>, measuring both computational efficiency and prediction accuracy. Natural language processing models might be assessed on translation tasks, examining how they balance quality and speed across different language pairs.</p>
<p>Several industry-standard benchmarks enable consistent model evaluation across platforms. <a href="https://github.com/mlcommons/inference">MLPerf Inference</a> provides comprehensive testing suites adapted for different computational environments <span class="citation" data-cites="reddi2020mlperf">(<a href="#ref-reddi2020mlperf" role="doc-biblioref">Reddi et al. 2019</a>)</span>. <a href="https://github.com/mlcommons/mobile_app_open">MLPerf Mobile</a> focuses on mobile device constraints <span class="citation" data-cites="janapa2022mlperf">(<a href="#ref-janapa2022mlperf" role="doc-biblioref">Janapa Reddi et al. 2022</a>)</span>, while <a href="https://github.com/mlcommons/tiny">MLPerf Tiny</a> addresses microcontroller deployments <span class="citation" data-cites="banbury2021mlperf">(<a href="#ref-banbury2021mlperf" role="doc-biblioref">Banbury et al. 2021</a>)</span>. For embedded systems, <a href="https://github.com/eembc/mlmark">EEMBC’s MLMark</a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> emphasizes both performance and power efficiency. The <a href="https://ai-benchmark.com/">AI-Benchmark</a> suite specializes in mobile platforms, evaluating models across diverse tasks from image recognition to face parsing.</p>
<div class="no-row-height column-margin column-container"><div id="ref-janapa2022mlperf" class="csl-entry" role="listitem">
Janapa Reddi, Vijay et al. 2022. <span>“MLPerf Mobile V2. 0: An Industry-Standard Benchmark Suite for Mobile Machine Learning.”</span> In <em>Proceedings of Machine Learning and Systems</em>, 4:806–23.
</div><div id="ref-banbury2021mlperf" class="csl-entry" role="listitem">
Banbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, et al. 2021. <span>“MLPerf Tiny Benchmark.”</span> <em>arXiv Preprint arXiv:2106.07597</em>, June. <a href="http://arxiv.org/abs/2106.07597v4">http://arxiv.org/abs/2106.07597v4</a>.
</div><div id="fn5"><p><sup>5</sup>&nbsp;<strong>EEMBC (Embedded Microprocessor Benchmark Consortium)</strong>: A nonprofit industry group that develops benchmarks for embedded systems, including MLMark for evaluating machine learning workloads.</p></div></div></section><section id="sec-benchmarking-ai-endtoend-benchmarks-cb75" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-endtoend-benchmarks-cb75">End-to-End Benchmarks</h3>
<p>End-to-end benchmarks provide an all-inclusive evaluation that extends beyond the boundaries of the ML model itself. Rather than focusing solely on a machine learning model’s computational efficiency or accuracy, these benchmarks encompass the entire pipeline of an AI system. This includes initial ETL (Extract-Transform-Load) or ELT (Extract-Load-Transform) data processing, the core model’s performance, post-processing of results, and critical infrastructure components like storage and network systems.</p>
<p>Data processing is the foundation of all AI systems, transforming raw data into a format suitable for model training or inference. In ETL pipelines, data undergoes extraction from source systems, transformation through cleaning and feature engineering, and loading into model-ready formats. These preprocessing steps’ efficiency, scalability, and accuracy significantly impact overall system performance. End-to-end benchmarks must assess standardized datasets through these pipelines to ensure data preparation doesn’t become a bottleneck.</p>
<p>The post-processing phase plays an equally important role. This involves interpreting the model’s raw outputs, converting scores into meaningful categories, filtering results based on predefined tasks, or integrating with other systems. For instance, a computer vision system might need to post-process detection boundaries, apply confidence thresholds, and format results for downstream applications. In real-world deployments, this phase proves crucial for delivering actionable insights.</p>
<p>Beyond core AI operations, infrastructure components heavily influence overall performance and user experience. Storage solutions, whether cloud-based, on-premises, or hybrid, can significantly impact data retrieval and storage times, especially with vast AI datasets. Network interactions, vital for distributed systems, can become performance bottlenecks if not optimized. End-to-end benchmarks must evaluate these components under specified environmental conditions to ensure reproducible measurements of the entire system.</p>
<p>To date, there are no public, end-to-end benchmarks that fully account for data storage, network, and compute performance. While MLPerf Training and Inference approach end-to-end evaluation, they primarily focus on model performance rather than real-world deployment scenarios. Nonetheless, they provide valuable baseline metrics for assessing AI system capabilities.</p>
<p>Given the inherent specificity of end-to-end benchmarking, organizations typically perform these evaluations internally by instrumenting production deployments. This allows engineers to develop result interpretation guidelines based on realistic workloads, but given the sensitivity and specificity of the information, these benchmarks rarely appear in public settings.</p>
</section><section id="sec-benchmarking-ai-tradeoffs-6217" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-tradeoffs-6217">Trade-offs</h3>
<p>As shown in <a href="#tbl-benchmark-comparison" class="quarto-xref">Table&nbsp;1</a>, different challenges emerge at different stages of an AI system’s lifecycle. Each benchmarking approach provides unique insights: micro-benchmarks help engineers optimize specific components like GPU kernel implementations or data loading operations, macro-benchmarks guide model architecture decisions and algorithm selection, while end-to-end benchmarks reveal system-level bottlenecks in production environments.</p>
<div id="tbl-benchmark-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-benchmark-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Benchmarking Granularity Levels</strong>: Different benchmark scopes—micro, macro, and end-to-end—target distinct stages of ML system development and reveal unique performance bottlenecks. Micro-benchmarks isolate individual operations for low-level optimization, macro-benchmarks evaluate complete models to guide architectural choices, and end-to-end benchmarks assess full system performance in production environments.
</figcaption><div aria-describedby="tbl-benchmark-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 30%">
<col style="width: 28%">
<col style="width: 28%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Component</th>
<th style="text-align: left;">Micro Benchmarks</th>
<th style="text-align: left;">Macro Benchmarks</th>
<th style="text-align: left;">End-to-End Benchmarks</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Focus</td>
<td style="text-align: left;">Individual operations</td>
<td style="text-align: left;">Complete models</td>
<td style="text-align: left;">Full system pipeline</td>
</tr>
<tr class="even">
<td style="text-align: left;">Scope</td>
<td style="text-align: left;">Tensor ops, layers, activations</td>
<td style="text-align: left;">Model architecture, training, inference</td>
<td style="text-align: left;">ETL, model, infrastructure</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Example</td>
<td style="text-align: left;">Conv layer performance on cuDNN</td>
<td style="text-align: left;">ResNet-50 on ImageNet</td>
<td style="text-align: left;">Production recommendation system</td>
</tr>
<tr class="even">
<td style="text-align: left;">Advantages</td>
<td style="text-align: left;">Precise bottleneck identification, Component optimization</td>
<td style="text-align: left;">Model architecture comparison, Standardized evaluation</td>
<td style="text-align: left;">Realistic performance assessment, System-wide insights</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Challenges</td>
<td style="text-align: left;">May miss interaction effects</td>
<td style="text-align: left;">Limited infrastructure insights</td>
<td style="text-align: left;">Complex to standardize, Often proprietary</td>
</tr>
<tr class="even">
<td style="text-align: left;">Typical Use</td>
<td style="text-align: left;">Hardware selection, Operation optimization</td>
<td style="text-align: left;">Model selection, Research comparison</td>
<td style="text-align: left;">Production system evaluation</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Component interaction often produces unexpected behaviors. For example, while micro-benchmarks might show excellent performance for individual convolutional layers, and macro-benchmarks might demonstrate strong accuracy for the complete model, end-to-end evaluation could reveal that data preprocessing creates unexpected bottlenecks during high-traffic periods. These system-level insights often remain hidden when components undergo isolated testing.</p>
<p>Component interaction often produces unexpected behaviors. For example, while micro-benchmarks might show excellent performance for individual convolutional layers, and macro-benchmarks might demonstrate strong accuracy for the complete model, end-to-end evaluation could reveal that data preprocessing creates unexpected bottlenecks during high-traffic periods. These system-level insights often remain hidden when components undergo isolated testing.</p>
<div id="quiz-question-sec-benchmarking-ai-benchmarking-granularity-8676" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li>
<p>Which type of benchmark provides a comprehensive evaluation of an entire AI system, including data processing, model performance, and infrastructure components?</p>
<ol type="a">
<li>Micro-benchmarks</li>
<li>Macro-benchmarks</li>
<li>End-to-end benchmarks</li>
<li>Component-level benchmarks</li>
</ol>
</li>
<li><p>True or False: Micro-benchmarks are sufficient for identifying system-level bottlenecks in production environments.</p></li>
<li><p>Explain why macro-benchmarks are important for model architecture decisions.</p></li>
<li><p>Micro-benchmarks focus on evaluating individual operations such as ____ and activation functions to provide detailed insights into computational demands.</p></li>
<li><p>Order the following benchmarking types from the most granular to the least granular: Macro-benchmarks, End-to-end benchmarks, Micro-benchmarks.</p></li>
</ol>
<p><a href="#quiz-answer-sec-benchmarking-ai-benchmarking-granularity-8676" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section></section><section id="sec-benchmarking-ai-training-benchmarks-c516" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-training-benchmarks-c516">Training Benchmarks</h2>
<p>Training benchmarks provide a systematic approach to evaluating the efficiency, scalability, and resource demands of the training phase. They allow practitioners to assess how different design choices, including model architectures, data loading mechanisms, hardware configurations, and distributed training strategies, impact performance. These benchmarks are particularly vital as machine learning systems grow in scale, requiring billions of parameters, terabytes of data, and distributed computing environments.</p>
<p>For instance, large-scale models like <a href="https://arxiv.org/abs/2005.14165">OpenAI’s GPT-3</a> <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>, which consists of 175 billion parameters trained on 45 terabytes of data, highlight the immense computational demands of training. Benchmarks enable systematic evaluation of the underlying systems to ensure that hardware and software configurations can meet these demands efficiently.</p>
<div class="no-row-height column-margin column-container"></div><div class="callout callout-style-default callout-note callout-titled" title="Definition of ML Training Benchmarks">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of ML Training Benchmarks
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>ML Training Benchmarks</strong> are standardized tools used to evaluate the <em>performance</em>, <em>efficiency</em>, and <em>scalability</em> of machine learning systems during the <em>training phase</em>. These benchmarks measure key <em>system-level metrics</em>, such as <em>time-to-accuracy</em>, <em>throughput</em>, <em>resource utilization</em>, and <em>energy consumption</em>. By providing a structured evaluation framework, training benchmarks enable <em>fair comparisons</em> across <em>hardware platforms</em>, <em>software frameworks</em>, and <em>distributed computing setups</em>. They help identify <em>bottlenecks</em> and optimize <em>training processes</em> for <em>large-scale machine learning models</em>, ensuring that computational resources are used effectively.</p>
</div>
</div>
<p>Efficient data storage and delivery during training also play a major role in the training process. For instance, in a machine learning model that predicts bounding boxes around objects in an image, thousands of images may be required. However, loading an entire image dataset into memory is typically infeasible, so practitioners rely on data loaders from ML frameworks. Successful model training depends on timely and efficient data delivery, making it essential to benchmark tools like data pipelines, preprocessing speed, and storage retrieval times to understand their impact on training performance.</p>
<p>Hardware selection is another key factor in training machine learning systems, as it can significantly impact training time. Training benchmarks evaluate CPU, GPU, memory, and network utilization during the training phase to guide system optimizations. Understanding how resources are used is essential: Are GPUs being fully leveraged? Is there unnecessary memory overhead? Benchmarks can uncover bottlenecks or inefficiencies in resource utilization, leading to cost savings and performance improvements.</p>
<p>In many cases, using a single hardware accelerator, such as a single GPU, is insufficient to meet the computational demands of large-scale model training. Machine learning models are often trained in data centers with multiple GPUs or TPUs, where distributed computing enables parallel processing across nodes. Training benchmarks assess how efficiently the system scales across multiple nodes, manages data sharding, and handles challenges like node failures or drop-offs during training.</p>
<p>To illustrate these benchmarking principles, we will reference <a href="https://mlcommons.org/benchmarks/training/">MLPerf Training</a> throughout this section. Briefly, MLPerf is an industry-standard benchmark suite designed to evaluate machine learning system performance. It provides standardized tests for training and inference across a range of deep learning workloads, including image classification, language modeling, object detection, and recommendation systems.</p>
<section id="sec-benchmarking-ai-motivation-af6b" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-motivation-af6b">Motivation</h3>
<p>From a systems perspective, training machine learning models is a computationally intensive process that requires careful optimization of resources. Training benchmarks serve as essential tools for evaluating system efficiency, identifying bottlenecks, and ensuring that machine learning systems can scale effectively. They provide a standardized approach to measuring how various system components, including hardware accelerators, memory, storage, and network infrastructure, affect training performance.</p>
<p>Training benchmarks enable researchers and engineers to push the state-of-the-art, optimize configurations, improve scalability, and reduce overall resource consumption by systematically evaluating these factors. As shown in <a href="#fig-mlperf-training-improve" class="quarto-xref">Figure&nbsp;5</a>, the performance improvements in progressive versions of MLPerf Training benchmarks have consistently outpaced Moore’s Law, which demonstrates that what gets measured gets improved. Using standardized benchmarking trends allows us to rigorously showcase the rapid evolution of ML computing.</p>
<div id="fig-mlperf-training-improve" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-mlperf-training-improve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="e1d6b0a658657fd9898f19b84595bffca981d789.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: MLPerf Training Progress: Standardized benchmarks reveal that machine learning training performance consistently surpasses moore’s law, indicating substantial gains from systems-level optimizations. These trends emphasize how focused measurement and iterative improvement drive rapid advancements in ML training efficiency and scalability. Source: [@tschand2024mlperf]."><img src="benchmarking_files/mediabag/e1d6b0a658657fd9898f19b84595bffca981d789.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlperf-training-improve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>MLPerf Training Progress</strong>: Standardized benchmarks reveal that machine learning training performance consistently surpasses moore’s law, indicating substantial gains from systems-level optimizations. These trends emphasize how focused measurement and iterative improvement drive rapid advancements in ML training efficiency and scalability. Source: <span class="citation" data-cites="tschand2024mlperf">(<a href="#ref-tschand2024mlperf" role="doc-biblioref">Tschand et al. 2024</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"></div></figure>
</div>
<section id="sec-benchmarking-ai-importance-training-benchmarks-14e6" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-importance-training-benchmarks-14e6">Importance of Training Benchmarks</h4>
<p>As machine learning models grow in complexity, training becomes increasingly demanding in terms of compute power, memory, and data storage. The ability to measure and compare training efficiency is critical to ensuring that systems can effectively handle large-scale workloads. Training benchmarks provide a structured methodology for assessing performance across different hardware platforms, software frameworks, and optimization techniques.</p>
<p>One of the fundamental challenges in training machine learning models is the efficient allocation of computational resources. Training a transformer-based model such as GPT-3, which consists of 175 billion parameters and requires processing terabytes of data, places an enormous burden on modern computing infrastructure. Without standardized benchmarks, it becomes difficult to determine whether a system is fully utilizing its resources or whether inefficiencies, including slow data loading, underutilized accelerators, and excessive memory overhead, are limiting performance.</p>
<p>Training benchmarks help uncover such inefficiencies by measuring key performance indicators, including system throughput, time-to-accuracy, and hardware utilization. These benchmarks allow practitioners to analyze whether GPUs, TPUs, and CPUs are being leveraged effectively or whether specific bottlenecks, such as memory bandwidth constraints or inefficient data pipelines, are reducing overall system performance. For example, a system using TF32<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> precision1 may achieve higher throughput than one using FP32, but if TF32 introduces numerical instability that increases the number of iterations required to reach the target accuracy, the overall training time may be longer. By providing insights into these factors, benchmarks support the design of more efficient training workflows that maximize hardware potential while minimizing unnecessary computation.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<strong>TensorFloat-32 (TF32)</strong>: Introduced in NVIDIA Ampere GPUs, provides higher throughput than FP32 but may introduce numerical stability issues affecting model convergence.</p></div></div></section><section id="sec-benchmarking-ai-hardware-software-optimization-b3c5" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-hardware-software-optimization-b3c5">Hardware &amp; Software Optimization</h4>
<p>The performance of machine learning training is heavily influenced by the choice of hardware and software. Training benchmarks guide system designers in selecting optimal configurations by measuring how different architectures, including GPUs, TPUs, and emerging AI accelerators, handle computational workloads. These benchmarks also evaluate how well deep learning frameworks, such as TensorFlow and PyTorch, optimize performance across different hardware setups.</p>
<p>For example, the MLPerf Training benchmark suite is widely used to compare the performance of different accelerator architectures on tasks such as image classification, natural language processing, and recommendation systems. By running standardized benchmarks across multiple hardware configurations, engineers can determine whether certain accelerators are better suited for specific training workloads. This information is particularly valuable in large-scale data centers and cloud computing environments, where selecting the right combination of hardware and software can lead to significant performance gains and cost savings.</p>
<p>Beyond hardware selection, training benchmarks also inform software optimizations. Machine learning frameworks implement various low-level optimizations, including mixed-precision training, memory-efficient data loading, and distributed training strategies, that can significantly impact system performance. Benchmarks help quantify the impact of these optimizations, ensuring that training systems are configured for maximum efficiency.</p>
</section><section id="sec-benchmarking-ai-scalability-efficiency-77df" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-scalability-efficiency-77df">Scalability &amp; Efficiency</h4>
<p>As machine learning workloads continue to grow, efficient scaling across distributed computing environments has become a key concern. Many modern deep learning models are trained across multiple GPUs or TPUs, requiring efficient parallelization strategies to ensure that additional computing resources lead to meaningful performance improvements. Training benchmarks measure how well a system scales by evaluating system throughput, memory efficiency, and overall training time as additional computational resources are introduced.</p>
<p>Effective scaling is not always guaranteed. While adding more GPUs or TPUs should, in theory, reduce training time, issues such as communication overhead, data synchronization latency, and memory bottlenecks can limit scaling efficiency. Training benchmarks help identify these challenges by quantifying how performance scales with increasing hardware resources. A well-designed system should exhibit near-linear scaling, where doubling the number of GPUs results in a near-halving of training time. However, real-world inefficiencies often prevent perfect scaling, and benchmarks provide the necessary insights to optimize system design accordingly.</p>
<p>Another crucial factor in training efficiency is time-to-accuracy, which measures how quickly a model reaches a target accuracy level. Achieving faster convergence with fewer computational resources is a key goal in training optimization, and benchmarks help compare different training methodologies to determine which approaches strike the best balance between speed and accuracy. By leveraging training benchmarks, system designers can assess whether their infrastructure is capable of handling large-scale workloads efficiently while maintaining training stability and accuracy.</p>
</section><section id="sec-benchmarking-ai-cost-energy-factors-82fd" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-cost-energy-factors-82fd">Cost &amp; Energy Factors</h4>
<p>The computational cost of training large-scale models has risen sharply in recent years, making cost-efficiency a critical consideration. Training a model such as GPT-3 can require millions of dollars in cloud computing resources, making it imperative to evaluate cost-effectiveness across different hardware and software configurations. Training benchmarks provide a means to quantify the cost per training run by analyzing computational expenses, cloud pricing models, and energy consumption.</p>
<p>Beyond financial cost, energy efficiency has become an increasingly important metric. Large-scale training runs consume vast amounts of electricity, contributing to significant carbon emissions. Benchmarks help evaluate energy efficiency by measuring power consumption per unit of training progress, allowing organizations to identify sustainable approaches to AI development.</p>
<p>For example, MLPerf includes an energy benchmarking component that tracks the power consumption of various hardware accelerators during training. This allows researchers to compare different computing platforms not only in terms of raw performance but also in terms of their environmental impact. By integrating energy efficiency metrics into benchmarking studies, organizations can design AI systems that balance computational power with sustainability goals.</p>
</section><section id="sec-benchmarking-ai-fair-ml-systems-comparison-fa96" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-fair-ml-systems-comparison-fa96">Fair ML Systems Comparison</h4>
<p>One of the primary functions of training benchmarks is to establish a standardized framework for comparing ML systems. Given the wide variety of hardware architectures, deep learning frameworks, and optimization techniques available today, ensuring fair and reproducible comparisons is essential.</p>
<p>Standardized benchmarks provide a common evaluation methodology, allowing researchers and practitioners to assess how different training systems perform under identical conditions. For example, MLPerf Training benchmarks enable vendor-neutral comparisons by defining strict evaluation criteria for deep learning tasks such as image classification, language modeling, and recommendation systems. This ensures that performance results are meaningful and not skewed by differences in dataset preprocessing, hyperparameter tuning, or implementation details.</p>
<p>Furthermore, reproducibility is a major concern in machine learning research. Training benchmarks help address this challenge by providing clearly defined methodologies for performance evaluation, ensuring that results can be consistently reproduced across different computing environments. By adhering to standardized benchmarks, researchers can make informed decisions when selecting hardware, software, and training methodologies, ultimately driving progress in AI systems development.</p>
</section></section><section id="sec-benchmarking-ai-metrics-fab9" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-metrics-fab9">Metrics</h3>
<p>Evaluating the performance of machine learning training requires a set of well-defined metrics that go beyond conventional algorithmic measures. From a systems perspective, training benchmarks assess how efficiently and effectively a machine learning model can be trained to a predefined accuracy threshold. Metrics such as throughput, scalability, and energy efficiency are only meaningful in relation to whether the model successfully reaches its target accuracy. Without this constraint, optimizing for raw speed or resource utilization may lead to misleading conclusions.</p>
<p>Training benchmarks, such as MLPerf Training, define specific accuracy targets for different machine learning tasks, ensuring that performance measurements are made in a fair and reproducible manner. A system that trains a model quickly but fails to reach the required accuracy is not considered a valid benchmark result. Conversely, a system that achieves the best possible accuracy but takes an excessive amount of time or resources may not be practically useful. Effective benchmarking requires balancing speed, efficiency, and accuracy convergence.</p>
<section id="sec-benchmarking-ai-time-throughput-31dc" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-time-throughput-31dc">Time and Throughput</h4>
<p>One of the fundamental metrics for evaluating training efficiency is the time required to reach a predefined accuracy threshold. Training time (<span class="math inline">\(T_{\text{train}}\)</span>) measures how long a model takes to converge to an acceptable performance level, reflecting the overall computational efficiency of the system. It is formally defined as: <span class="math display">\[
T_{\text{train}} = \arg\min_{t} \big\{ \text{accuracy}(t) \geq \text{target accuracy} \big\}
\]</span></p>
<p>This metric ensures that benchmarking focuses on how quickly and effectively a system can achieve meaningful results.</p>
<p>Throughput, often expressed as the number of training samples processed per second, provides an additional measure of system performance: <span class="math display">\[
T = \frac{N_{\text{samples}}}{T_{\text{train}}}
\]</span> where <span class="math inline">\(N_{\text{samples}}\)</span> is the total number of training samples processed. However, throughput alone does not guarantee meaningful results, as a model may process a large number of samples quickly without necessarily reaching the desired accuracy.</p>
<p>For example, in MLPerf Training, the benchmark for ResNet-50 may require reaching an accuracy target like 75.9% top-1 on the ImageNet dataset. A system that processes 10,000 images per second but fails to achieve this accuracy is not considered a valid benchmark result, while a system that processes fewer images per second but converges efficiently is preferable. This highlights why throughput must always be evaluated in relation to time-to-accuracy rather than as an independent performance measure.</p>
</section><section id="sec-benchmarking-ai-scalability-parallelism-ef6d" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-scalability-parallelism-ef6d">Scalability &amp; Parallelism</h4>
<p>As machine learning models increase in size, training workloads often require distributed computing across multiple processors or accelerators. Scalability measures how effectively training performance improves as more computational resources are added. An ideal system should exhibit near-linear scaling, where doubling the number of GPUs or TPUs leads to a proportional reduction in training time. However, real-world performance is often constrained by factors such as communication overhead, memory bandwidth limitations, and inefficiencies in parallelization strategies.</p>
<p>When training large-scale models such as GPT-3, OpenAI employed thousands of GPUs in a distributed training setup. While increasing the number of GPUs provided more raw computational power, the performance improvements were not perfectly linear due to network communication overhead between nodes. Benchmarks such as MLPerf quantify how well a system scales across multiple GPUs, providing insights into where inefficiencies arise in distributed training.</p>
<p>Parallelism in training is categorized into data parallelism, model parallelism, and pipeline parallelism, each presenting distinct challenges. Data parallelism, the most commonly used strategy, involves splitting the training dataset across multiple compute nodes. The efficiency of this approach depends on synchronization mechanisms and gradient communication overhead. In contrast, model parallelism partitions the neural network itself, requiring efficient coordination between processors. Benchmarks evaluate how well a system manages these parallelism strategies without degrading accuracy convergence.</p>
</section><section id="sec-benchmarking-ai-resource-utilization-918d" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-resource-utilization-918d">Resource Utilization</h4>
<p>The efficiency of machine learning training depends not only on speed and scalability but also on how well available hardware resources are utilized. Compute utilization measures the extent to which processing units, such as GPUs or TPUs, are actively engaged during training. Low utilization may indicate bottlenecks in data movement, memory access, or inefficient workload scheduling.</p>
<p>For instance, when training BERT on a TPU cluster, researchers observed that input pipeline inefficiencies were limiting overall throughput. Although the TPUs had high raw compute power, the system was not keeping them fully utilized due to slow data retrieval from storage. By profiling the resource utilization, engineers identified the bottleneck and optimized the input pipeline using TFRecord and data prefetching, leading to improved performance.</p>
<p>Memory bandwidth is another critical factor, as deep learning models require frequent access to large volumes of data during training. If memory bandwidth becomes a limiting factor, increasing compute power alone will not improve training speed. Benchmarks assess how well models leverage available memory, ensuring that data transfer rates between storage, main memory, and processing units do not become performance bottlenecks.</p>
<p>I/O performance also plays a significant role in training efficiency, particularly when working with large datasets that cannot fit entirely in memory. Benchmarks evaluate the efficiency of data loading pipelines, including preprocessing operations, caching mechanisms, and storage retrieval speeds. Systems that fail to optimize data loading can experience significant slowdowns, regardless of computational power.</p>
</section><section id="sec-benchmarking-ai-energy-efficiency-cost-960b" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-energy-efficiency-cost-960b">Energy Efficiency &amp; Cost</h4>
<p>Training large-scale machine learning models requires substantial computational resources, leading to significant energy consumption and financial costs. Energy efficiency metrics quantify the power usage of training workloads, helping identify systems that optimize computational efficiency while minimizing energy waste. The increasing focus on sustainability has led to the inclusion of energy-based benchmarks, such as those in MLPerf Training, which measure power consumption per training run.</p>
<p>Training GPT-3 was estimated to consume 1,287 MWh of electricity, which is comparable to the yearly energy usage of 100 US households. If a system can achieve the same accuracy with fewer training iterations, it directly reduces energy consumption. Energy-aware benchmarks help guide the development of hardware and training strategies that optimize power efficiency while maintaining accuracy targets.</p>
<p>Cost considerations extend beyond electricity usage to include hardware expenses, cloud computing costs, and infrastructure maintenance. Training benchmarks provide insights into the cost-effectiveness of different hardware and software configurations by measuring training time in relation to resource expenditure. Organizations can use these benchmarks to balance performance and budget constraints when selecting training infrastructure.</p>
</section><section id="sec-benchmarking-ai-fault-tolerance-robustness-707f" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-fault-tolerance-robustness-707f">Fault Tolerance &amp; Robustness</h4>
<p>Training workloads often run for extended periods, sometimes spanning days or weeks, making fault tolerance an essential consideration. A robust system must be capable of handling unexpected failures, including hardware malfunctions, network disruptions, and memory errors, without compromising accuracy convergence.</p>
<p>In large-scale cloud-based training, node failures are common due to hardware instability. If a GPU node in a distributed cluster fails, training must continue without corrupting the model. MLPerf Training includes evaluations of fault-tolerant training strategies, such as checkpointing, where models periodically save their progress. This ensures that failures do not require restarting the entire training process.</p>
</section><section id="sec-benchmarking-ai-reproducibility-standardization-79ac" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-reproducibility-standardization-79ac">Reproducibility &amp; Standardization</h4>
<p>For benchmarks to be meaningful, results must be reproducible across different runs, hardware platforms, and software frameworks. Variability in training results can arise due to stochastic processes, hardware differences, and software optimizations. Ensuring reproducibility requires standardizing evaluation protocols, controlling for randomness in model initialization, and enforcing consistency in dataset processing.</p>
<p>MLPerf Training enforces strict reproducibility requirements, ensuring that accuracy results remain stable across multiple training runs. When NVIDIA submitted benchmark results for MLPerf, they had to demonstrate that their ResNet-50 ImageNet training time remained consistent across different GPUs. This ensures that benchmarks measure true system performance rather than noise from randomness.</p>
</section></section><section id="sec-benchmarking-ai-training-performance-evaluation-8754" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-training-performance-evaluation-8754">Training Performance Evaluation</h3>
<p>Evaluating the performance of machine learning training systems involves more than just measuring how fast a model can be trained. A comprehensive benchmarking approach considers multiple dimensions—each capturing a different aspect of system behavior. The specific metrics used depend on the goals of the evaluation, whether those are optimizing speed, improving resource efficiency, reducing energy consumption, or ensuring robustness and reproducibility.</p>
<p><a href="#tbl-training-metrics" class="quarto-xref">Table&nbsp;2</a> provides an overview of the core categories and associated metrics commonly used to benchmark system-level training performance. These categories serve as a framework for understanding how training systems behave under different workloads and configurations.</p>
<div id="tbl-training-metrics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-training-metrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Training Benchmark Dimensions</strong>: Key categories and metrics for comprehensively evaluating machine learning training systems, moving beyond simple speed to assess resource efficiency, reproducibility, and overall performance tradeoffs. understanding these dimensions enables systematic comparison of different training approaches and infrastructure configurations.
</figcaption><div aria-describedby="tbl-training-metrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 54%">
<col style="width: 27%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Key Metrics</th>
<th style="text-align: left;">Example Benchmark Use</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Training Time and Throughput</td>
<td style="text-align: left;">Time-to-accuracy (seconds, minutes, hours); Throughput (samples/sec)</td>
<td style="text-align: left;">Comparing training speed across different GPU architectures</td>
</tr>
<tr class="even">
<td style="text-align: left;">Scalability and Parallelism</td>
<td style="text-align: left;">Scaling efficiency (% of ideal speedup); Communication overhead (latency, bandwidth)</td>
<td style="text-align: left;">Analyzing distributed training performance for large models</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Resource Utilization</td>
<td style="text-align: left;">Compute utilization (% GPU/TPU usage); Memory bandwidth (GB/s); I/O efficiency (data loading speed)</td>
<td style="text-align: left;">Optimizing data pipelines to improve GPU utilization</td>
</tr>
<tr class="even">
<td style="text-align: left;">Energy Efficiency and Cost</td>
<td style="text-align: left;">Energy consumption per run (MWh, kWh); Performance per watt (TOPS/W)</td>
<td style="text-align: left;">Evaluating energy-efficient training strategies</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Fault Tolerance and Robustness</td>
<td style="text-align: left;">Checkpoint overhead (time per save); Recovery success rate (%)</td>
<td style="text-align: left;">Assessing failure recovery in cloud-based training systems</td>
</tr>
<tr class="even">
<td style="text-align: left;">Reproducibility and Standardization</td>
<td style="text-align: left;">Variance across runs (% difference in accuracy, training time); Framework consistency (TensorFlow vs.&nbsp;PyTorch vs.&nbsp;JAX)</td>
<td style="text-align: left;">Ensuring consistency in benchmark results across hardware</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Training time and throughput are often the first metrics considered when evaluating system performance. Time-to-accuracy, the duration required for a model to achieve a specified accuracy level, is a practical and widely used benchmark. Throughput, typically measured in samples per second, provides insight into how efficiently data is processed during training. For example, when comparing a ResNet-50 model trained on NVIDIA A100 versus V100 GPUs, the A100 generally offers higher throughput and faster convergence. However, it is important to ensure that increased throughput does not come at the expense of convergence quality, especially when reduced numerical precision (e.g., TF32) is used to speed up computation.</p>
<p>As model sizes continue to grow, scalability becomes a critical performance dimension. Efficient use of multiple GPUs or TPUs is essential for training large models such as GPT-3 or T5. In this context, scaling efficiency and communication overhead are key metrics. A system might scale linearly up to 64 GPUs, but beyond that, performance gains may taper off due to increased synchronization and communication costs. Benchmarking tools that monitor interconnect bandwidth and gradient aggregation latency can reveal how well a system handles distributed training.</p>
<p>Resource utilization complements these measures by examining how effectively a system leverages its compute and memory resources. Metrics such as GPU utilization, memory bandwidth, and data loading efficiency help identify performance bottlenecks. For instance, a BERT pretraining task that exhibits only moderate GPU utilization may be constrained by an underperforming data pipeline. Optimizations like sharding input files or prefetching data into device memory can often resolve these inefficiencies.</p>
<p>In addition to raw performance, energy efficiency and cost have become increasingly important considerations. Training large models at scale can consume significant power, raising environmental and financial concerns. Metrics such as energy consumed per training run and performance per watt (e.g., TOPS/W) help evaluate the sustainability of different hardware and system configurations. For example, while two systems may reach the same accuracy in the same amount of time, the one that uses significantly less energy may be preferred for long-term deployment.</p>
<p>Fault tolerance and robustness address how well a system performs under non-ideal conditions, which are common in real-world deployments. Training jobs frequently encounter hardware failures, preemptions, or network instability. Metrics like checkpoint overhead and recovery success rate provide insight into the resilience of a training system. In practice, checkpointing can introduce non-trivial overhead—for example, pausing training every 30 minutes to write a full checkpoint may reduce overall throughput by 5-10%. Systems must strike a balance between failure recovery and performance impact.</p>
<p>Finally, reproducibility and standardization ensure that benchmark results are consistent, interpretable, and transferable. Even minor differences in software libraries, initialization seeds, or floating-point behavior can affect training outcomes. Comparing the same model across frameworks, such as comparing PyTorch with Automatic Mixed Precision to TensorFlow with XLA, can reveal variation in convergence rates or final accuracy. Reliable benchmarking requires careful control of these variables, along with repeated runs to assess statistical variance.</p>
<p>Together, these dimensions provide a holistic view of training performance. They help researchers, engineers, and system designers move beyond simplistic comparisons and toward a more nuanced understanding of how machine learning systems behave under realistic conditions. As training workloads continue to scale, such multidimensional evaluation will be essential for guiding hardware choices, software optimizations, and infrastructure design.</p>
<section id="sec-benchmarking-ai-training-benchmark-pitfalls-7552" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-training-benchmark-pitfalls-7552">Training Benchmark Pitfalls</h4>
<p>Despite the availability of well-defined benchmarking methodologies, certain misconceptions and flawed evaluation practices often lead to misleading conclusions. Understanding these pitfalls is important for interpreting benchmark results correctly.</p>
<section id="sec-benchmarking-ai-overemphasis-raw-throughput-0625" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-overemphasis-raw-throughput-0625">Overemphasis on Raw Throughput</h5>
<p>A common mistake in training benchmarks is assuming that higher throughput always translates to better training performance. It is possible to artificially increase throughput by using lower numerical precision, reducing synchronization, or even bypassing certain computations. However, these optimizations do not necessarily lead to faster convergence.</p>
<p>For example, a system using TF32 precision may achieve higher throughput than one using FP32, but if TF32 introduces numerical instability that increases the number of iterations required to reach the target accuracy, the overall training time may be longer. The correct way to evaluate throughput is in relation to time-to-accuracy, ensuring that speed optimizations do not come at the expense of convergence efficiency.</p>
</section><section id="sec-benchmarking-ai-isolated-singlenode-performance-f189" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-isolated-singlenode-performance-f189">Isolated Single-Node Performance</h5>
<p>Benchmarking training performance on a single node without considering how well it scales in a distributed setting can lead to misleading conclusions. A GPU may demonstrate excellent throughput when used independently, but when deployed across hundreds of nodes, communication overhead and synchronization constraints may diminish these efficiency gains.</p>
<p>For instance, a system optimized for single-node performance may employ memory optimizations that do not generalize well to multi-node environments. Large-scale models such as GPT-3 require efficient gradient synchronization across multiple nodes, making it essential to assess scalability rather than relying solely on single-node performance metrics.</p>
</section><section id="sec-benchmarking-ai-ignoring-failures-interference-24d6" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-ignoring-failures-interference-24d6">Ignoring Failures &amp; Interference</h5>
<p>Many benchmarks assume an idealized training environment where hardware failures, memory corruption, network instability, or interference from other processes do not occur. However, real-world training jobs often experience unexpected failures and workload interference that require checkpointing, recovery mechanisms, and resource management.</p>
<p>A system optimized for ideal-case performance but lacking fault tolerance and interference handling may achieve impressive benchmark results under controlled conditions, but frequent failures, inefficient recovery, and resource contention could make it impractical for large-scale deployment. Effective benchmarking should consider checkpointing overhead, failure recovery efficiency, and the impact of interference from other processes rather than assuming perfect execution conditions.</p>
</section><section id="sec-benchmarking-ai-linear-scaling-assumption-c423" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-linear-scaling-assumption-c423">Linear Scaling Assumption</h5>
<p>When evaluating distributed training, it is often assumed that increasing the number of GPUs or TPUs will result in proportional speedups. In practice, communication bottlenecks, memory contention, and synchronization overheads lead to diminishing returns as more compute nodes are added.</p>
<p>For example, training a model across 1,000 GPUs does not necessarily provide 100 times the speed of training on 10 GPUs. At a certain scale, gradient communication costs become a limiting factor, offsetting the benefits of additional parallelism. Proper benchmarking should assess scalability efficiency rather than assuming idealized linear improvements.</p>
</section><section id="sec-benchmarking-ai-ignoring-reproducibility-4e83" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-ignoring-reproducibility-4e83">Ignoring Reproducibility</h5>
<p>Benchmark results are often reported without verifying their reproducibility across different hardware and software frameworks. Even minor variations in floating-point arithmetic, memory layouts, or optimization strategies can introduce statistical differences in training time and accuracy.</p>
<p>For example, a benchmark run on TensorFlow with XLA optimizations may exhibit different convergence characteristics compared to the same model trained using PyTorch with Automatic Mixed Precision (AMP). Proper benchmarking requires evaluating results across multiple frameworks to ensure that software-specific optimizations do not distort performance comparisons.</p>
</section></section><section id="sec-benchmarking-ai-final-thoughts-586d" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-final-thoughts-586d">Final Thoughts</h4>
<p>Training benchmarks provide valuable insights into machine learning system performance, but their interpretation requires careful consideration of real-world constraints. High throughput does not necessarily mean faster training if it compromises accuracy convergence. Similarly, scaling efficiency must be evaluated holistically, taking into account both computational efficiency and communication overhead.</p>
<p>Avoiding common benchmarking pitfalls and employing structured evaluation methodologies allows machine learning practitioners to gain a deeper understanding of how to optimize training workflows, design efficient AI systems, and develop scalable machine learning infrastructure. As models continue to increase in complexity, benchmarking methodologies must evolve to reflect real-world challenges, ensuring that benchmarks remain meaningful and actionable in guiding AI system development.</p>
<div id="quiz-question-sec-benchmarking-ai-training-benchmarks-c516" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li>
<p>Which metric is most crucial for evaluating the efficiency of a training system in reaching a predefined accuracy threshold?</p>
<ol type="a">
<li>Throughput</li>
<li>Time-to-accuracy</li>
<li>Energy consumption</li>
<li>Memory bandwidth</li>
</ol>
</li>
<li><p>Explain why training benchmarks are essential for optimizing hardware and software configurations in large-scale ML systems.</p></li>
<li><p>True or False: Higher throughput always results in faster training times for machine learning models.</p></li>
<li><p>Training benchmarks help assess the scalability of a system by measuring how well it handles increased computational resources, such as additional ____ or TPUs.</p></li>
<li><p>Order the following steps in evaluating a training benchmark: Identify bottlenecks, Measure time-to-accuracy, Optimize configurations, Implement distributed training.</p></li>
</ol>
<p><a href="#quiz-answer-sec-benchmarking-ai-training-benchmarks-c516" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section></section></section><section id="sec-benchmarking-ai-inference-benchmarks-5e47" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-inference-benchmarks-5e47">Inference Benchmarks</h2>
<p>Inference benchmarks provide a systematic approach to evaluating the efficiency, latency, and resource demands of the inference phase in machine learning systems. Unlike training, where the focus is on optimizing large-scale computations over extensive datasets, inference involves deploying trained models to make real-time or batch predictions efficiently. These benchmarks help assess how various factors, including model architectures, hardware configurations, quantization techniques, and runtime optimizations, impact inference performance.</p>
<p>As deep learning models grow in complexity and size, efficient inference becomes a key challenge, particularly for applications requiring real-time decision-making, such as autonomous driving, healthcare diagnostics, and conversational AI. For example, serving large-scale models like <a href="https://arxiv.org/abs/2303.08774">OpenAI’s GPT-4</a> involves handling billions of parameters while maintaining low latency. Inference benchmarks enable systematic evaluation of the underlying hardware and software stacks to ensure that models can be deployed efficiently across different environments, from cloud data centers to edge devices.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition of ML Inference Benchmarks">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of ML Inference Benchmarks
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>ML Inference Benchmarks</strong> are standardized tools used to evaluate the <em>performance</em>, <em>efficiency</em>, and <em>scalability</em> of machine learning systems during the <em>inference phase</em>. These benchmarks measure key <em>system-level metrics</em>, such as <em>latency</em>, <em>throughput</em>, <em>energy consumption</em>, and <em>memory footprint</em>. By providing a structured evaluation framework, inference benchmarks enable <em>fair comparisons</em> across <em>hardware platforms</em>, <em>software runtimes</em>, and <em>deployment configurations</em>. They help identify <em>bottlenecks</em> and optimize <em>inference pipelines</em> for <em>real-time and large-scale machine learning applications</em>, ensuring that computational resources are utilized effectively.</p>
</div>
</div>
<p>Unlike training, which is often conducted in large-scale data centers with ample computational resources, inference must be optimized for diverse deployment scenarios, including mobile devices, IoT systems, and embedded processors. Efficient inference depends on multiple factors, such as optimized data pipelines, quantization, pruning, and hardware acceleration. Benchmarks help evaluate how well these optimizations improve real-world deployment performance.</p>
<p>Hardware selection plays an important role in inference efficiency. While GPUs and TPUs are widely used for training, inference workloads often require specialized accelerators like NPUs (Neural Processing Units), FPGAs, and dedicated inference chips such as Google’s Edge TPU. Inference benchmarks evaluate the utilization and performance of these hardware components, helping practitioners choose the right configurations for their deployment needs.</p>
<p>Scaling inference workloads across cloud servers, edge platforms, mobile devices, and tinyML systems introduces additional challenges. As illustrated in <a href="#fig-power-differentials" class="quarto-xref">Figure&nbsp;6</a>, there is a significant differential in power consumption among these systems, ranging from microwatts to megawatts. Inference benchmarks evaluate the trade-offs between latency, cost, and energy efficiency, thereby assisting organizations in making informed deployment decisions.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-power-differentials" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-power-differentials-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="benchmarking_files/figure-html/fig-power-differentials-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Energy Consumption: The figure emphasizes the significant differences in power usage across various system types, from microwatts to megawatts, emphasizing the trade-offs between latency, cost, and energy efficiency in inference benchmarks."><img src="benchmarking_files/figure-html/fig-power-differentials-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-power-differentials-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Energy Consumption</strong>: The figure emphasizes the significant differences in power usage across various system types, from microwatts to megawatts, emphasizing the trade-offs between latency, cost, and energy efficiency in inference benchmarks.
</figcaption></figure>
</div>
</div>
</div>
<p>As with training, we will reference MLPerf Inference throughout this section to illustrate benchmarking principles. MLPerf provides standardized inference tests across different workloads, including image classification, object detection, speech recognition, and language processing. A full discussion of MLPerf’s methodology and structure is presented later in this chapter.</p>
<section id="sec-benchmarking-ai-motivation-fa6c" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-motivation-fa6c">Motivation</h3>
<p>Deploying machine learning models for inference introduces a unique set of challenges distinct from training. While training optimizes large-scale computation over extensive datasets, inference must deliver predictions efficiently and at scale in real-world environments. Inference benchmarks provide a systematic approach to evaluating system performance, identifying bottlenecks, and ensuring that models can operate effectively across diverse deployment scenarios.</p>
<p>Unlike training, which typically runs on dedicated high-performance hardware, inference must adapt to varying constraints. A model deployed in a cloud server might prioritize high-throughput batch processing, while the same model running on a mobile device must operate under strict latency and power constraints. On edge devices with limited compute and memory, optimizations such as quantization and pruning become critical. Benchmarks help assess these trade-offs, ensuring that inference systems maintain the right balance between accuracy, speed, and efficiency across different platforms.</p>
<p>Inference benchmarks help answer fundamental questions about model deployment. How quickly can a model generate predictions in real-world conditions? What are the trade-offs between inference speed and accuracy? Can an inference system handle increasing demand while maintaining low latency? By evaluating these factors, benchmarks guide optimizations in both hardware and software to improve overall efficiency <span class="citation" data-cites="reddi2020mlperf">(<a href="#ref-reddi2020mlperf" role="doc-biblioref">Reddi et al. 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reddi2020mlperf" class="csl-entry" role="listitem">
Reddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2019. <span>“MLPerf Inference Benchmark.”</span> <em>arXiv Preprint arXiv:1911.02549</em>, November, 446–59. <a href="https://doi.org/10.1109/isca45697.2020.00045">https://doi.org/10.1109/isca45697.2020.00045</a>.
</div></div><section id="sec-benchmarking-ai-importance-inference-benchmarks-3710" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-importance-inference-benchmarks-3710">Importance of Inference Benchmarks</h4>
<p>Inference plays a critical role in AI applications, where performance directly affects usability and cost. Unlike training, which is often performed offline, inference typically operates in real-time or near real-time, making latency a primary concern. A self-driving car processing camera feeds must react within milliseconds, while a voice assistant generating responses should feel instantaneous to users.</p>
<p>Different applications impose varying constraints on inference. Some workloads require single-instance inference, where predictions must be made as quickly as possible for each individual input. This is crucial in real-time systems such as robotics, augmented reality, and conversational AI, where even small delays can impact responsiveness. Other workloads, such as large-scale recommendation systems or search engines, process massive batches of queries simultaneously, prioritizing throughput over per-query latency. Benchmarks allow engineers to evaluate both scenarios and ensure models are optimized for their intended use case.</p>
<p>A key difference between training and inference is that inference workloads often run continuously in production, meaning that small inefficiencies can compound over time. Unlike a training job that runs once and completes, an inference system deployed in the cloud may serve millions of queries daily, and a model running on a smartphone must manage battery consumption over extended use. Benchmarks provide a structured way to measure inference efficiency under these real-world constraints, helping developers make informed choices about model optimization, hardware selection, and deployment strategies.</p>
</section><section id="sec-benchmarking-ai-hardware-software-optimization-6999" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-hardware-software-optimization-6999">Hardware &amp; Software Optimization</h4>
<p>Efficient inference depends on both hardware acceleration and software optimizations. While GPUs and TPUs dominate training, inference is more diverse in its hardware needs. A cloud-based AI service might leverage powerful accelerators for large-scale workloads, whereas mobile devices rely on specialized inference chips like NPUs or optimized CPU execution. On embedded systems, where resources are constrained, achieving high performance requires careful memory and compute efficiency. Benchmarks help evaluate how well different hardware platforms handle inference workloads, guiding deployment decisions.</p>
<p>Software optimizations are just as important. Frameworks like TensorRT, ONNX Runtime, and TVM apply optimizations such as operator fusion, quantization, and kernel tuning to improve inference speed and reduce computational overhead. These optimizations can make a significant difference, especially in environments with limited resources. Benchmarks allow developers to measure the impact of such techniques on latency, throughput, and power efficiency, ensuring that optimizations translate into real-world improvements without degrading model accuracy.</p>
</section><section id="sec-benchmarking-ai-scalability-efficiency-be05" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-scalability-efficiency-be05">Scalability &amp; Efficiency</h4>
<p>Inference workloads vary significantly in their scaling requirements. A cloud-based AI system handling millions of queries per second must ensure that increasing demand does not cause delays, while a mobile application running a model locally must execute quickly even under power constraints. Unlike training, which is typically performed on a fixed set of high-performance machines, inference must scale dynamically based on usage patterns and available computational resources.</p>
<p>Benchmarks evaluate how inference systems scale under different conditions. They measure how well performance holds up under increasing query loads, whether additional compute resources improve inference speed, and how efficiently models run across different deployment environments. Large-scale inference deployments often involve distributed inference servers, where multiple copies of a model process incoming requests in parallel. Benchmarks assess how efficiently this scaling occurs and whether additional resources lead to meaningful improvements in latency and throughput.</p>
<p>Another key factor in inference efficiency is cold-start performance—the time it takes for a model to load and begin processing queries. This is especially relevant for applications that do not run inference continuously but instead load models on demand. Benchmarks help determine whether a system can quickly transition from idle to active execution without significant overhead.</p>
</section><section id="sec-benchmarking-ai-cost-energy-factors-c9a6" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-cost-energy-factors-c9a6">Cost &amp; Energy Factors</h4>
<p>Because inference workloads run continuously, operational cost and energy efficiency are critical factors. Unlike training, where compute costs are incurred once, inference costs accumulate over time as models are deployed in production. Running an inefficient model at scale can significantly increase cloud compute expenses, while an inefficient mobile inference system can drain battery life quickly. Benchmarks provide insights into cost per inference request, helping organizations optimize for both performance and affordability.</p>
<p>Energy efficiency is also a growing concern, particularly for mobile and edge AI applications. Many inference workloads run on battery-powered devices, where excessive computation can impact usability. A model running on a smartphone, for example, must be optimized to minimize power consumption while maintaining responsiveness. Benchmarks help evaluate inference efficiency per watt, ensuring that models can operate sustainably across different platforms.</p>
</section><section id="sec-benchmarking-ai-fair-ml-systems-comparison-4f8b" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-fair-ml-systems-comparison-4f8b">Fair ML Systems Comparison</h4>
<p>With many different hardware platforms and optimization techniques available, standardized benchmarking is essential for fair comparisons. Without well-defined benchmarks, it becomes difficult to determine whether performance gains come from genuine improvements or from optimizations that exploit specific hardware features. Inference benchmarks provide a consistent evaluation methodology, ensuring that comparisons are meaningful and reproducible.</p>
<p>For example, MLPerf Inference defines rigorous evaluation criteria for tasks such as image classification, object detection, and speech recognition, making it possible to compare different systems under controlled conditions. These standardized tests prevent misleading results caused by differences in dataset preprocessing, proprietary optimizations, or vendor-specific tuning. By enforcing reproducibility, benchmarks allow researchers and engineers to make informed decisions when selecting inference frameworks, hardware accelerators, and optimization techniques.</p>
</section></section><section id="sec-benchmarking-ai-metrics-3374" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-metrics-3374">Metrics</h3>
<p>Evaluating the performance of inference systems requires a distinct set of metrics from those used for training. While training benchmarks emphasize throughput, scalability, and time-to-accuracy, inference benchmarks must focus on latency, efficiency, and resource utilization in practical deployment settings. These metrics ensure that machine learning models perform well across different environments, from cloud data centers handling millions of requests to mobile and edge devices operating under strict power and memory constraints.</p>
<p>Unlike training, where the primary goal is to optimize learning speed, inference benchmarks evaluate how efficiently a trained model can process inputs and generate predictions at scale. The following sections describe the most important inference benchmarking metrics, explaining their relevance and how they are used to compare different systems.</p>
<section id="sec-benchmarking-ai-latency-tail-latency-33f0" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-latency-tail-latency-33f0">Latency &amp; Tail Latency</h4>
<p>Latency is one of the most critical performance metrics for inference, particularly in real-time applications where delays can negatively impact user experience or system safety. Latency refers to the time taken for an inference system to process an input and produce a prediction. While the average latency of a system is useful, it does not capture performance in high-demand scenarios where occasional delays can degrade reliability.</p>
<p>To account for this, benchmarks often measure tail latency, which reflects the worst-case delays in a system. These are typically reported as the 95th percentile (p95) or 99th percentile (p99) latency, meaning that 95% or 99% of inferences are completed within a given time. For applications such as autonomous driving or real-time trading, maintaining low tail latency is essential to avoid unpredictable delays that could lead to catastrophic outcomes.</p>
</section><section id="sec-benchmarking-ai-throughput-batch-processing-efficiency-6338" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-throughput-batch-processing-efficiency-6338">Throughput &amp; Batch Processing Efficiency</h4>
<p>While latency measures the speed of individual inference requests, throughput measures how many inference requests a system can process per second. It is typically expressed in queries per second (QPS) or frames per second (FPS) for vision tasks. Some inference systems operate on a single-instance basis, where each input is processed independently as soon as it arrives. Other systems process multiple inputs in parallel using batch inference, which can significantly improve efficiency by leveraging hardware optimizations.</p>
<p>For example, cloud-based services handling millions of queries per second benefit from batch inference, where large groups of inputs are processed together to maximize computational efficiency. In contrast, applications like robotics, interactive AI, and augmented reality require low-latency single-instance inference, where the system must respond immediately to each new input.</p>
<p>Benchmarks must consider both single-instance and batch throughput to provide a comprehensive understanding of inference performance across different deployment scenarios.</p>
</section><section id="sec-benchmarking-ai-precision-accuracy-tradeoffs-952d" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-precision-accuracy-tradeoffs-952d">Precision &amp; Accuracy Trade-offs</h4>
<p>Optimizing inference performance often involves reducing numerical precision, which can significantly accelerate computation while reducing memory and energy consumption. However, lower-precision calculations can introduce accuracy degradation, making it essential to benchmark the trade-offs between speed and predictive quality.</p>
<p>Inference benchmarks evaluate how well models perform under different numerical settings, such as FP32, FP16, and INT8. Many modern AI accelerators support mixed-precision inference, allowing systems to dynamically adjust numerical representation based on workload requirements. Quantization and pruning techniques further improve efficiency, but their impact on model accuracy varies depending on the task and dataset. Benchmarks help determine whether these optimizations are viable for deployment, ensuring that improvements in efficiency do not come at the cost of unacceptable accuracy loss.</p>
</section><section id="sec-benchmarking-ai-memory-footprint-model-size-8fe0" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-memory-footprint-model-size-8fe0">Memory Footprint &amp; Model Size</h4>
<p>Beyond computational optimizations, memory footprint is another critical consideration for inference systems, particularly for devices with limited resources. Efficient inference depends not only on speed but also on memory usage. Unlike training, where large models can be distributed across powerful GPUs or TPUs, inference often requires models to run within strict memory budgets. The total model size determines how much storage is required for deployment, while RAM usage reflects the working memory needed during execution. Some models require large memory bandwidth to efficiently transfer data between processing units, which can become a bottleneck if the hardware lacks sufficient capacity.</p>
<p>Inference benchmarks evaluate these factors to ensure that models can be deployed effectively across a range of devices. A model that achieves high accuracy but exceeds memory constraints may be impractical for real-world use. To address this, compression techniques such as quantization, pruning, and knowledge distillation are often applied to reduce model size while maintaining accuracy. Benchmarks help assess whether these optimizations strike the right balance between memory efficiency and predictive performance.</p>
</section><section id="sec-benchmarking-ai-coldstart-model-load-time-d303" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-coldstart-model-load-time-d303">Cold-Start &amp; Model Load Time</h4>
<p>Once memory requirements are optimized, cold-start performance<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> becomes critical for ensuring inference systems are ready to respond quickly upon deployment. In many deployment scenarios, models are not always kept in memory but instead loaded on demand when needed. This can introduce significant delays, particularly in serverless AI<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> environments, where resources are allocated dynamically based on incoming requests. Cold-start performance measures how quickly a system can transition from idle to active execution, ensuring that inference is available without excessive wait times.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Cold-Start Time</strong>: The time required for a model to initialize and become ready to process the first inference request after being loaded from disk or a low-power state.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Serverless AI</strong>: A deployment model where inference workloads are executed on demand, eliminating the need for dedicated compute resources but introducing cold-start latency challenges.</p></div></div><p>Model load time refers to the duration required to load a trained model into memory before it can process inputs. In some cases, particularly on resource-limited devices, models must be reloaded frequently to free up memory for other applications. The time taken for the first inference request is also an important consideration, as it reflects the total delay users experience when interacting with an AI-powered service. Benchmarks help quantify these delays, ensuring that inference systems can meet real-world responsiveness requirements.</p>
</section><section id="sec-benchmarking-ai-scalability-dynamic-workload-handling-8d16" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-scalability-dynamic-workload-handling-8d16">Scalability &amp; Dynamic Workload Handling</h4>
<p>While cold-start latency addresses initial responsiveness, scalability ensures that inference systems can handle fluctuating workloads and concurrent demands over time Inference workloads must scale effectively across different usage patterns. In cloud-based AI services, this means efficiently handling millions of concurrent users, while on mobile or embedded devices, it involves managing multiple AI models running simultaneously without overloading the system.</p>
<p>Scalability measures how well inference performance improves when additional computational resources are allocated. In some cases, adding more GPUs or TPUs increases throughput significantly, but in other scenarios, bottlenecks such as memory bandwidth limitations or network latency may limit scaling efficiency. Benchmarks also assess how well a system balances multiple concurrent models in real-world deployment, where different AI-powered features may need to run at the same time without interference.</p>
<p>For cloud-based AI, benchmarks evaluate how efficiently a system handles fluctuating demand, ensuring that inference servers can dynamically allocate resources without compromising latency. In mobile and embedded AI, efficient multi-model execution is essential for running multiple AI-powered features simultaneously without degrading system performance.</p>
</section><section id="sec-benchmarking-ai-power-consumption-energy-efficiency-ede1" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-power-consumption-energy-efficiency-ede1">Power Consumption &amp; Energy Efficiency</h4>
<p>Since inference workloads run continuously in production, power consumption and energy efficiency are critical considerations. This is particularly important for mobile and edge devices, where battery life and thermal constraints limit available computational resources. Even in large-scale cloud environments, power efficiency directly impacts operational costs and sustainability goals.</p>
<p>The energy required for a single inference is often measured in joules per inference, reflecting how efficiently a system processes inputs while minimizing power draw. In cloud-based inference, efficiency is commonly expressed as queries per second per watt (QPS/W) to quantify how well a system balances performance and energy consumption. For mobile AI applications, optimizing inference power consumption extends battery life and allows models to run efficiently on resource-constrained devices. Reducing energy use also plays a key role in making large-scale AI systems more environmentally sustainable, ensuring that computational advancements align with energy-conscious deployment strategies. By balancing power consumption with performance, energy-efficient inference systems enable AI to scale sustainably across diverse applications, from data centers to edge devices.</p>
</section></section><section id="sec-benchmarking-ai-inference-performance-evaluation-7640" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-inference-performance-evaluation-7640">Inference Performance Evaluation</h3>
<p>Evaluating inference performance is a critical step in understanding how well machine learning systems meet the demands of real-world applications. Unlike training, which is typically conducted offline, inference systems must process inputs and generate predictions efficiently across a wide range of deployment scenarios. Metrics such as latency, throughput, memory usage, and energy efficiency provide a structured way to measure system performance and identify areas for improvement.</p>
<p><a href="#tbl-inference-metrics" class="quarto-xref">Table&nbsp;3</a> below summarizes the key metrics used to evaluate inference systems, highlighting their relevance to different contexts. While each metric offers unique insights, it is important to approach inference benchmarking holistically. Trade-offs between metrics, including speed versus accuracy and throughput versus power consumption, are common, and understanding these trade-offs is essential for effective system design.</p>
<div id="tbl-inference-metrics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-inference-metrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: <strong>Inference Performance Metrics</strong>: Evaluating latency, throughput, and resource usage provides a quantitative basis for optimizing deployed machine learning systems and selecting appropriate hardware configurations. Understanding these metrics—and the trade-offs between them—is crucial for balancing speed, cost, and accuracy in real-world applications.
</figcaption><div aria-describedby="tbl-inference-metrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 44%">
<col style="width: 36%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Key Metrics</th>
<th style="text-align: left;">Example Benchmark Use</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Latency and Tail Latency</td>
<td style="text-align: left;">Mean latency (ms/request); Tail latency (p95, p99, p99.9)</td>
<td style="text-align: left;">Evaluating real-time performance for safety-critical AI</td>
</tr>
<tr class="even">
<td style="text-align: left;">Throughput and Efficiency</td>
<td style="text-align: left;">Queries per second (QPS); Frames per second (FPS); Batch throughput</td>
<td style="text-align: left;">Comparing large-scale cloud inference systems</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Numerical Precision Impact</td>
<td style="text-align: left;">Accuracy degradation (FP32 vs.&nbsp;INT8); Speedup from reduced precision</td>
<td style="text-align: left;">Balancing accuracy vs.&nbsp;efficiency in optimized inference</td>
</tr>
<tr class="even">
<td style="text-align: left;">Memory Footprint</td>
<td style="text-align: left;">Model size (MB/GB); RAM usage (MB); Memory bandwidth utilization</td>
<td style="text-align: left;">Assessing feasibility for edge and mobile deployments</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cold-Start and Load Time</td>
<td style="text-align: left;">Model load time (s); First inference latency (s)</td>
<td style="text-align: left;">Evaluating responsiveness in serverless AI</td>
</tr>
<tr class="even">
<td style="text-align: left;">Scalability</td>
<td style="text-align: left;">Efficiency under load; Multi-model serving performance</td>
<td style="text-align: left;">Measuring robustness for dynamic, high-demand systems</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Power and Energy Efficiency</td>
<td style="text-align: left;">Power consumption (Watts); Performance per Watt (QPS/W)</td>
<td style="text-align: left;">Optimizing energy use for mobile and sustainable AI</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="sec-benchmarking-ai-inference-systems-considerations-8198" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-inference-systems-considerations-8198">Inference Systems Considerations</h4>
<p>Inference systems face unique challenges depending on where and how they are deployed. Real-time applications, such as self-driving cars or voice assistants, require low latency to ensure timely responses, while large-scale cloud deployments focus on maximizing throughput to handle millions of queries. Edge devices, on the other hand, are constrained by memory and power, making efficiency critical.</p>
<p>One of the most important aspects of evaluating inference performance is understanding the trade-offs between metrics. For example, optimizing for high throughput might increase latency, making a system unsuitable for real-time applications. Similarly, reducing numerical precision improves power efficiency and speed but may lead to minor accuracy degradation. A thoughtful evaluation must balance these trade-offs to align with the intended application.</p>
<p>The deployment environment also plays a significant role in determining evaluation priorities. Cloud-based systems often prioritize scalability and adaptability to dynamic workloads, while mobile and edge systems require careful attention to memory usage and energy efficiency. These differing priorities mean that benchmarks must be tailored to the context of the system’s use, rather than relying on one-size-fits-all evaluations.</p>
<p>Ultimately, evaluating inference performance requires a holistic approach. Focusing on a single metric, such as latency or energy efficiency, provides an incomplete picture. Instead, all relevant dimensions must be considered together to ensure that the system meets its functional, resource, and performance goals in a balanced way.</p>
</section><section id="sec-benchmarking-ai-inference-benchmark-pitfalls-0451" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-inference-benchmark-pitfalls-0451">Inference Benchmark Pitfalls</h4>
<p>Even with well-defined metrics, benchmarking inference systems can be challenging. Missteps during the evaluation process often lead to misleading conclusions. Below are common pitfalls that students and practitioners should be aware of when analyzing inference performance.</p>
<section id="sec-benchmarking-ai-overemphasis-average-latency-d5e1" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-overemphasis-average-latency-d5e1">Overemphasis on Average Latency</h5>
<p>While average latency provides a baseline measure of response time, it fails to capture how a system performs under peak load. In real-world scenarios, worst-case latency, which is captured through metrics such as p95 or p99 tail latency, can significantly impact system reliability. For instance, a conversational AI system may fail to provide timely responses if occasional latency spikes exceed acceptable thresholds.</p>
</section><section id="sec-benchmarking-ai-ignoring-memory-energy-constraints-ea99" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-ignoring-memory-energy-constraints-ea99">Ignoring Memory &amp; Energy Constraints</h5>
<p>A model with excellent throughput or latency may be unsuitable for mobile or edge deployments if it requires excessive memory or power. For example, an inference system designed for cloud environments might fail to operate efficiently on a battery-powered device. Proper benchmarks must consider memory footprint and energy consumption to ensure practicality across deployment contexts.</p>
</section><section id="sec-benchmarking-ai-ignoring-coldstart-performance-a4ea" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-ignoring-coldstart-performance-a4ea">Ignoring Cold-Start Performance</h5>
<p>In serverless environments, where models are loaded on demand, cold-start latency is a critical factor. Ignoring the time it takes to initialize a model and process the first request can result in unrealistic expectations for responsiveness. Evaluating both model load time and first-inference latency ensures that systems are designed to meet real-world responsiveness requirements.</p>
</section><section id="sec-benchmarking-ai-isolated-metrics-evaluation-cf28" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-isolated-metrics-evaluation-cf28">Isolated Metrics Evaluation</h5>
<p>Benchmarking inference systems often involves balancing competing metrics. For example, maximizing batch throughput might degrade latency, while aggressive quantization could reduce accuracy. Focusing on a single metric without considering its impact on others can lead to incomplete or misleading evaluations. Comprehensive benchmarks must account for these interactions.</p>
</section><section id="sec-benchmarking-ai-linear-scaling-assumption-f9ac" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-linear-scaling-assumption-f9ac">Linear Scaling Assumption</h5>
<p>Inference performance does not always scale proportionally with additional resources. Bottlenecks such as memory bandwidth, thermal limits, or communication overhead can limit the benefits of adding more GPUs or TPUs. Benchmarks that assume linear scaling behavior may overestimate system performance, particularly in distributed deployments.</p>
</section><section id="sec-benchmarking-ai-ignoring-application-requirements-4ec2" class="level5"><h5 class="anchored" data-anchor-id="sec-benchmarking-ai-ignoring-application-requirements-4ec2">Ignoring Application Requirements</h5>
<p>Generic benchmarking results may fail to account for the specific needs of an application. For instance, a benchmark optimized for cloud inference might be irrelevant for edge devices, where energy and memory constraints dominate. Tailoring benchmarks to the deployment context ensures that results are meaningful and actionable.</p>
</section></section><section id="sec-benchmarking-ai-final-thoughts-4738" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-final-thoughts-4738">Final Thoughts</h4>
<p>Inference benchmarks are essential tools for understanding system performance, but their utility depends on careful and holistic evaluation. Metrics like latency, throughput, memory usage, and energy efficiency provide valuable insights, but their importance varies depending on the application and deployment context. Students should approach benchmarking as a process of balancing multiple priorities, rather than optimizing for a single metric.</p>
<p>Avoiding common pitfalls and considering the trade-offs between different metrics allows practitioners to design inference systems that are reliable, efficient, and suitable for real-world deployment. The ultimate goal of benchmarking is to guide system improvements that align with the demands of the intended application.</p>
</section></section><section id="sec-benchmarking-ai-mlperf-inference-benchmarks-85e7" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-mlperf-inference-benchmarks-85e7">MLPerf Inference Benchmarks</h3>
<p>The MLPerf Inference benchmark, developed by <a href="https://mlcommons.org/en/">MLCommons</a>, provides a standardized framework for evaluating machine learning inference performance across a range of deployment environments. Initially, MLPerf started with a single inference benchmark, but as machine learning systems expanded into diverse applications, it became clear that a one-size-fits-all benchmark was insufficient. Different inference scenarios, including cloud-based AI services and resource-constrained embedded devices, demanded tailored evaluations. This realization led to the development of a family of MLPerf inference benchmarks, each designed to assess performance within a specific deployment setting.</p>
<section id="sec-benchmarking-ai-mlperf-inference-7257" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-mlperf-inference-7257">MLPerf Inference</h4>
<p><a href="https://mlcommons.org/en/inference-datacenter/">MLPerf Inference</a> serves as the baseline benchmark, originally designed to evaluate large-scale inference systems. It primarily focuses on data center and cloud-based inference workloads, where high throughput, low latency, and efficient resource utilization are essential. The benchmark assesses performance across a range of deep learning models, including image classification, object detection, natural language processing, and recommendation systems. This version of MLPerf remains the gold standard for comparing AI accelerators, GPUs, TPUs, and CPUs in high-performance computing environments.</p>
</section><section id="sec-benchmarking-ai-mlperf-mobile-b4b5" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-mlperf-mobile-b4b5">MLPerf Mobile</h4>
<p><a href="https://mlcommons.org/en/mlperf-mobile/">MLPerf Mobile</a> extends MLPerf’s evaluation framework to smartphones and other mobile devices. Unlike cloud-based inference, mobile inference operates under strict power and memory constraints, requiring models to be optimized for efficiency without sacrificing responsiveness. The benchmark measures latency and responsiveness for real-time AI tasks, such as camera-based scene detection, speech recognition, and augmented reality applications. MLPerf Mobile has become an industry standard for assessing AI performance on flagship smartphones and mobile AI chips, helping developers optimize models for on-device AI workloads.</p>
</section><section id="sec-benchmarking-ai-mlperf-client-1f92" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-mlperf-client-1f92">MLPerf Client</h4>
<p><a href="https://mlcommons.org/en/inference-edge/">MLPerf Client</a> focuses on inference performance on consumer computing devices, such as laptops, desktops, and workstations. This benchmark addresses local AI workloads that run directly on personal devices, eliminating reliance on cloud inference. Tasks such as real-time video editing, speech-to-text transcription, and AI-enhanced productivity applications fall under this category. Unlike cloud-based benchmarks, MLPerf Client evaluates how AI workloads interact with general-purpose hardware, such as CPUs, discrete GPUs, and integrated Neural Processing Units (NPUs), making it relevant for consumer and enterprise AI applications.</p>
</section><section id="sec-benchmarking-ai-mlperf-tiny-7941" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-mlperf-tiny-7941">MLPerf Tiny</h4>
<p><a href="https://mlcommons.org/en/inference-tiny/">MLPerf Tiny</a> was created to benchmark embedded and ultra-low-power AI systems, such as IoT devices, wearables, and microcontrollers. Unlike other MLPerf benchmarks, which assess performance on powerful accelerators, MLPerf Tiny evaluates inference on devices with limited compute, memory, and power resources. This benchmark is particularly relevant for applications such as smart sensors, AI-driven automation, and real-time industrial monitoring, where models must run efficiently on hardware with minimal processing capabilities. MLPerf Tiny plays a crucial role in the advancement of AI at the edge, helping developers optimize models for constrained environments.</p>
</section><section id="sec-benchmarking-ai-continued-expansion-b07e" class="level4"><h4 class="anchored" data-anchor-id="sec-benchmarking-ai-continued-expansion-b07e">Continued Expansion</h4>
<p>The evolution of MLPerf Inference from a single benchmark to a spectrum of benchmarks reflects the diversity of AI deployment scenarios. Different environments, including cloud, mobile, desktop, and embedded environments, have unique constraints and requirements, and MLPerf provides a structured way to evaluate AI models accordingly.</p>
<p>MLPerf is an essential tool for:</p>
<ul>
<li><p>Understanding how inference performance varies across deployment settings.</p></li>
<li><p>Learning which performance metrics are most relevant for different AI applications.</p></li>
<li><p>Optimizing models and hardware choices based on real-world usage constraints.</p></li>
</ul>
<p>Recognizing the necessity of tailored inference benchmarks deepens our understanding of AI deployment challenges and highlights the importance of benchmarking in developing efficient, scalable, and practical machine learning systems.</p>
<div id="quiz-question-sec-benchmarking-ai-inference-benchmarks-5e47" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li>
<p>Which inference benchmark metric is most critical for ensuring real-time performance in safety-critical AI applications?</p>
<ol type="a">
<li>Throughput</li>
<li>Tail latency</li>
<li>Memory footprint</li>
<li>Power consumption</li>
</ol>
</li>
<li><p>Explain why inference benchmarks are essential for optimizing AI models on mobile devices.</p></li>
<li><p>True or False: Inference benchmarks typically focus more on throughput than on latency.</p></li>
<li><p>Inference benchmarks evaluate the impact of hardware accelerators like NPUs and FPGAs on ________ and energy efficiency in AI deployments.</p></li>
<li><p>Order the following inference benchmarking considerations from most to least critical for mobile AI applications: Power consumption, Memory footprint, Throughput, Latency.</p></li>
</ol>
<p><a href="#quiz-answer-sec-benchmarking-ai-inference-benchmarks-5e47" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section></section></section><section id="sec-benchmarking-ai-energy-efficiency-measurement-a669" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-energy-efficiency-measurement-a669">Energy Efficiency Measurement</h2>
<p>As machine learning expands into diverse applications, concerns about its growing power consumption and ecological footprint have intensified. While performance benchmarks help optimize speed and accuracy, they do not always account for energy efficiency, which is an increasingly critical factor in real-world deployment. Efficient inference is particularly important in scenarios where power is a limited resource, such as mobile devices, embedded AI, and cloud-scale inference workloads. The need to optimize both performance and power consumption has led to the development of standardized energy efficiency benchmarks.</p>
<p>However, measuring power consumption in machine learning systems presents unique challenges. The energy demands of ML models vary dramatically across deployment environments, as shown in <a href="#tbl-power" class="quarto-xref">Table&nbsp;4</a>. This wide spectrum, spanning from TinyML devices consuming mere microwatts to data center racks requiring kilowatts, illustrates the fundamental challenge in creating standardized benchmarking methodologies <span class="citation" data-cites="henderson2020towards">(<a href="#ref-henderson2020towards" role="doc-biblioref">Henderson et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-henderson2020towards" class="csl-entry" role="listitem">
Henderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. <span>“Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning.”</span> <em>CoRR</em> abs/2002.05651 (248): 1–43. <a href="https://doi.org/10.48550/arxiv.2002.05651">https://doi.org/10.48550/arxiv.2002.05651</a>.
</div></div><div id="tbl-power" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-power-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: <strong>Power Consumption Spectrum</strong>: Machine learning deployments exhibit a wide range of power demands, from microwatt-scale TinyML devices to milliwatt-scale microcontrollers; this variability challenges the development of standardized energy efficiency benchmarks. Understanding these differences is crucial for optimizing model deployment across resource-constrained and high-performance computing environments.
</figcaption><div aria-describedby="tbl-power-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 47%">
<col style="width: 27%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: center;">Device Type</th>
<th style="text-align: center;">Power Consumption</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Tiny</td>
<td style="text-align: center;">Neural Decision Processor (NDP)</td>
<td style="text-align: center;">150 µW</td>
</tr>
<tr class="even">
<td style="text-align: left;">Tiny</td>
<td style="text-align: center;">M7 Microcontroller</td>
<td style="text-align: center;">25 mW</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mobile</td>
<td style="text-align: center;">Raspberry Pi 4</td>
<td style="text-align: center;">3.5 W</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mobile</td>
<td style="text-align: center;">Smartphone</td>
<td style="text-align: center;">4 W</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Edge</td>
<td style="text-align: center;">Smart Camera</td>
<td style="text-align: center;">10-15 W</td>
</tr>
<tr class="even">
<td style="text-align: left;">Edge</td>
<td style="text-align: center;">Edge Server</td>
<td style="text-align: center;">65-95 W</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cloud</td>
<td style="text-align: center;">ML Server Node</td>
<td style="text-align: center;">300-500 W</td>
</tr>
<tr class="even">
<td style="text-align: left;">Cloud</td>
<td style="text-align: center;">ML Server Rack</td>
<td style="text-align: center;">4-10 kW</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>This dramatic range in power requirements, which spans over four orders of magnitude, presents significant challenges for measurement and benchmarking. Creating a unified methodology requires careful consideration of each scale’s unique characteristics. For example, accurately measuring microwatt-level consumption in TinyML devices demands different instrumentation and techniques than monitoring kilowatt-scale server racks. Any comprehensive benchmarking framework must accommodate these vastly different scales while ensuring measurements remain consistent, fair, and reproducible across diverse hardware configurations.</p>
<section id="sec-benchmarking-ai-power-measurement-boundaries-5d17" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-power-measurement-boundaries-5d17">Power Measurement Boundaries</h3>
<p><a href="#fig-power-diagram" class="quarto-xref">Figure&nbsp;7</a> illustrates how power consumption is measured at different system scales, from TinyML devices to full-scale data center inference nodes. Each scenario highlights distinct measurement boundaries, shown in green, which indicate the components included in energy accounting. Components outside these boundaries, shown with red dashed outlines, are excluded from power measurements.</p>
<div id="fig-power-diagram" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-power-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="5fc9fcd7ee5d5add57deb446f637cb84d8f9b7dc.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Power Measurement Boundaries: MLPerf defines system boundaries for power measurement, ranging from single-chip devices to full data center nodes, to enable fair comparisons of energy efficiency across diverse hardware platforms. these boundaries delineate which components’ power consumption is included in reported metrics, impacting the interpretation of performance results. Source: [@tschand2024mlperf]."><img src="benchmarking_files/mediabag/5fc9fcd7ee5d5add57deb446f637cb84d8f9b7dc.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-power-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Power Measurement Boundaries</strong>: MLPerf defines system boundaries for power measurement, ranging from single-chip devices to full data center nodes, to enable fair comparisons of energy efficiency across diverse hardware platforms. these boundaries delineate which components’ power consumption is included in reported metrics, impacting the interpretation of performance results. Source: <span class="citation" data-cites="tschand2024mlperf">(<a href="#ref-tschand2024mlperf" role="doc-biblioref">Tschand et al. 2024</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>The diagram is organized into three categories, Tiny, Inference, and Training examples, each reflecting different measurement scopes based on system architecture and deployment environment. In TinyML systems, the entire low-power SoC, including compute, memory, and basic interconnects, typically falls within the measurement boundary. Inference nodes introduce more complexity, incorporating multiple SoCs, local storage, accelerators, and memory, while often excluding remote storage and off-chip components. Training deployments span multiple racks, where only selected elements, including compute nodes and network switches, are measured, while storage systems, cooling infrastructure, and parts of the interconnect fabric are often excluded.</p>
<p>System-level power measurement offers a more holistic view than measuring individual components in isolation. While component-level metrics (e.g., accelerator or processor power) are valuable for performance tuning, real-world ML workloads involve intricate interactions between compute units, memory systems, and supporting infrastructure. For instance, memory-bound inference tasks can consume up to 60% of total system power on data movement alone.</p>
<p>Shared infrastructure presents additional challenges. In data centers, resources such as cooling systems and power delivery are shared across workloads, complicating attribution of energy use to specific ML tasks. Cooling alone can account for 20-30% of total facility power consumption, making it a major factor in energy efficiency assessments <span class="citation" data-cites="barroso2022datacenter">(<a href="#ref-barroso2022datacenter" role="doc-biblioref">Barroso, Clidaras, and Hölzle 2013</a>)</span>. Even at the edge, components like memory and I/O interfaces may serve both ML and non-ML functions, further blurring measurement boundaries.</p>
<div class="no-row-height column-margin column-container"><div id="ref-barroso2022datacenter" class="csl-entry" role="listitem">
Barroso, Luiz André, Jimmy Clidaras, and Urs Hölzle. 2013. <em>The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01741-4">https://doi.org/10.1007/978-3-031-01741-4</a>.
</div></div><p>Modern hardware also introduces variability through dynamic power management. Features like dynamic voltage and frequency scaling (DVFS) can cause power consumption to vary by 30-50% for the same ML model, depending on system load and concurrent activity.</p>
<p>Finally, support infrastructure, with a particular emphasis on cooling, has a significant impact on total energy use in large-scale deployments. Data centers must maintain operational temperatures, typically between 20-25°C, to ensure system reliability. Cooling overhead is captured in the Power Usage Effectiveness (PUE) metric, which ranges from 1.1 in highly efficient facilities to over 2.0 in less optimized ones <span class="citation" data-cites="barroso2019datacenter">(<a href="#ref-barroso2019datacenter" role="doc-biblioref">Barroso, Hölzle, and Ranganathan 2019</a>)</span>. Even edge devices require basic thermal management, with cooling accounting for 5-10% of overall power consumption.</p>
<div class="no-row-height column-margin column-container"><div id="ref-barroso2019datacenter" class="csl-entry" role="listitem">
Barroso, Luiz André, Urs Hölzle, and Parthasarathy Ranganathan. 2019. <em>The Datacenter as a Computer: Designing Warehouse-Scale Machines</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01761-2">https://doi.org/10.1007/978-3-031-01761-2</a>.
</div></div></section><section id="sec-benchmarking-ai-performance-vs-energy-efficiency-826e" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-performance-vs-energy-efficiency-826e">Performance vs Energy Efficiency</h3>
<p>A critical consideration in ML system design is the relationship between performance and energy efficiency. Maximizing raw performance often leads to diminishing returns in energy efficiency. For example, increasing processor frequency by 20% might yield only a 5% performance improvement while increasing power consumption by 50%. This non-linear relationship means that the most energy-efficient operating point is often not the highest performing one.</p>
<p>In many deployment scenarios, particularly in battery-powered devices, finding the optimal balance between performance and energy efficiency is crucial. For instance, reducing model precision from FP32 to INT8 might reduce accuracy by 1-2% but can improve energy efficiency by 3-4x. Similarly, batch processing can improve throughput efficiency at the cost of increased latency.</p>
<p>These tradeoffs span three key dimensions: accuracy, performance, and energy efficiency. Model quantization illustrates this relationship clearly, reducing numerical precision from FP32 to INT8 typically results in a small accuracy drop (1-2%), but it can improve both inference speed and energy efficiency by 3-4x. Similarly, techniques like pruning and model compression require carefully balancing accuracy losses against efficiency gains. Finding the optimal operating point among these three factors depends heavily on deployment requirements; mobile applications might prioritize energy efficiency, while cloud services might optimize for accuracy at the cost of higher power consumption.</p>
<p>As benchmarking methodologies continue to evolve, energy efficiency metrics will play an increasingly central role in AI optimization. Future advancements in sustainable AI benchmarking<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> will help researchers and engineers design systems that balance performance, power consumption, and environmental impact, ensuring that ML systems operate efficiently without unnecessary energy waste.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;Reducing the environmental impact of machine learning by improving energy efficiency, using renewable energy sources, and designing models that require fewer computational resources.</p></div></div></section><section id="sec-benchmarking-ai-standardized-power-measurement-67d6" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-standardized-power-measurement-67d6">Standardized Power Measurement</h3>
<p>While power measurement techniques, such as <a href="https://www.spec.org/power/">SPEC Power</a>, have long existed for general computing systems <span class="citation" data-cites="lange2009identifying">(<a href="#ref-lange2009identifying" role="doc-biblioref">Lange 2009</a>)</span>, machine learning workloads present unique challenges that require specialized measurement approaches. Machine learnign systems exhibit distinct power consumption patterns characterized by phases of intense computation interspersed with data movement and preprocessing operations. These patterns vary significantly across different types of models and tasks. A large language model’s power profile looks very different from that of a computer vision inference task.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lange2009identifying" class="csl-entry" role="listitem">
Lange, Klaus-Dieter. 2009. <span>“Identifying Shades of Green: The SPECpower Benchmarks.”</span> <em>Computer</em> 42 (3): 95–97. <a href="https://doi.org/10.1109/mc.2009.84">https://doi.org/10.1109/mc.2009.84</a>.
</div></div><p>Direct power measurement requires careful consideration of sampling rates and measurement windows. For example, transformer model inference creates short, intense power spikes during attention computations, requiring high-frequency sampling (&gt; 1 KHz) to capture accurately. In contrast, CNN inference tends to show more consistent power draw patterns that can be captured with lower sampling rates. The measurement duration must also account for ML-specific behaviors like warm-up periods, where initial inferences may consume more power due to cache population and pipeline initialization.</p>
<p>Memory access patterns in ML workloads significantly impact power consumption measurements. While traditional compute benchmarks might focus primarily on processor power, ML systems often spend substantial energy moving data between memory hierarchies. For example, recommendation models like DLRM can spend more energy on memory access than computation. This requires measurement approaches that can capture both compute and memory subsystem power consumption.</p>
<p>Accelerator-specific considerations further complicate power measurement. Many ML systems employ specialized hardware like GPUs, TPUs, or NPUs. These accelerators often have their own power management schemes and can operate independently of the main system processor. Accurate measurement requires capturing power consumption across all relevant compute units while maintaining proper time synchronization. This is particularly challenging in heterogeneous systems that may dynamically switch between different compute resources based on workload characteristics or power constraints.</p>
<p>The scale and distribution of ML workloads also influences measurement methodology. In distributed training scenarios, power measurement must account for both local compute power and the energy cost of gradient synchronization across nodes. Similarly, edge ML deployments must consider both active inference power and the energy cost of model updates or data preprocessing.</p>
<p>Batch size and throughput considerations add another layer of complexity. Unlike traditional computing workloads, ML systems often process inputs in batches to improve computational efficiency. However, the relationship between batch size and power consumption is non-linear. While larger batches generally improve compute efficiency, they also increase memory pressure and peak power requirements. Measurement methodologies must therefore capture power consumption across different batch sizes to provide a complete efficiency profile.</p>
<p>System idle states require special attention in ML workloads, particularly in edge scenarios where systems operate intermittently, actively processing when new data arrives, then entering low-power states between inferences. A wake-word detection Tiny ML system, for instance, might only actively process audio for a small fraction of its operating time, making idle power consumption a critical factor in overall efficiency.</p>
<p>Temperature effects play a crucial role in ML system power measurement. Sustained ML workloads can cause significant temperature increases, triggering thermal throttling and changing power consumption patterns. This is especially relevant in edge devices where thermal constraints may limit sustained performance. Measurement methodologies must account for these thermal effects and their impact on power consumption, particularly during extended benchmarking runs.</p>
</section><section id="sec-benchmarking-ai-mlperf-power-case-study-81eb" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-mlperf-power-case-study-81eb">MLPerf Power Case Study</h3>
<p>MLPerf Power <span class="citation" data-cites="tschand2024mlperf">(<a href="#ref-tschand2024mlperf" role="doc-biblioref">Tschand et al. 2024</a>)</span> is a standard methodolgy for measuring energy efficiency in machine learning systems. This comprehensive benchmarking framework provides accurate assessment of power consumption across diverse ML deployments. At the datacenter level, it measures power usage in large-scale AI workloads, where energy consumption optimization directly impacts operational costs. For edge computing, it evaluates power efficiency in consumer devices like smartphones and laptops, where battery life constraints are paramount. In tiny inference scenarios, it assesses energy consumption for ultra-low-power AI systems, particularly IoT sensors and microcontrollers operating with strict power budgets.</p>
<div class="no-row-height column-margin column-container"></div><p>The MLPerf Power methodology relies on standardized measurement protocols that adapt to various hardware architectures, ranging from general-purpose CPUs to specialized AI accelerators. This standardization ensures meaningful cross-platform comparisons while maintaining measurement integrity across different computing scales.</p>
<p>The benchmark has accumulated thousands of reproducible measurements submitted by industry organizations, which demonstrates their latest hardware capabilities and the sector-wide focus on energy-efficient AI technology. <a href="#fig-power-trends" class="quarto-xref">Figure&nbsp;8</a> illustrates the evolution of energy efficiency across system scales through successive MLPerf versions.</p>
<div id="fig-power-trends" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-power-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="abc1b67f602eab6020e4fe42578452660f91d630.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Energy Efficiency Gains: Successive MLPerf inference benchmark versions consistently improve energy efficiency (samples/watt) across diverse system scales—datacenter, edge, and tiny—reflecting ongoing advancements in both hardware and software optimization for AI workloads. Standardized measurement protocols enable meaningful comparisons of energy efficiency improvements across different AI systems and deployment scenarios, driving sector-wide progress toward sustainable AI technologies. Source: [@tschand2024mlperf]."><img src="benchmarking_files/mediabag/abc1b67f602eab6020e4fe42578452660f91d630.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-power-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>Energy Efficiency Gains</strong>: Successive MLPerf inference benchmark versions consistently improve energy efficiency (samples/watt) across diverse system scales—datacenter, edge, and tiny—reflecting ongoing advancements in both hardware and software optimization for AI workloads. Standardized measurement protocols enable meaningful comparisons of energy efficiency improvements across different AI systems and deployment scenarios, driving sector-wide progress toward sustainable AI technologies. Source: <span class="citation" data-cites="tschand2024mlperf">(<a href="#ref-tschand2024mlperf" role="doc-biblioref">Tschand et al. 2024</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-tschand2024mlperf" class="csl-entry" role="listitem">
Tschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024. <span>“MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI.”</span> <em>arXiv Preprint arXiv:2410.12032</em>, October. <a href="http://arxiv.org/abs/2410.12032v2">http://arxiv.org/abs/2410.12032v2</a>.
</div></div></figure>
</div>
<p>The MLPerf Power methodology adapts to different hardware architectures, ranging from general-purpose CPUs to specialized AI accelerators, while maintaining a uniform measurement standard. This ensures that comparisons across platforms are meaningful and unbiased.</p>
<p>Across the versions and ML deployment scales of the MLPerf benchmark suite, industry organizations have submitted reproducible measurements on their most recent hardware to observe and quantify the industry-wide emphasis on optimizing AI technology for energy efficiency. <a href="#fig-power-trends" class="quarto-xref">Figure&nbsp;8</a> shows the trends in energy efficiency from tiny to datacenter scale systems across MLPerf versions.</p>
<p>Analysis of these trends reveals two significant patterns: first, a plateauing of energy efficiency improvements across all three scales for traditional ML workloads, and second, a dramatic increase in energy efficiency specifically for generative AI applications. This dichotomy suggests both the maturation of optimization techniques for conventional ML tasks and the rapid innovation occurring in the generative AI space. These trends underscore the dual challenges facing the field: developing novel approaches to break through efficiency plateaus while ensuring sustainable scaling practices for increasingly powerful generative AI models.</p>
<div id="quiz-question-sec-benchmarking-ai-energy-efficiency-measurement-a669" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li>
<p>What is a significant challenge in creating standardized energy efficiency benchmarks for ML systems?</p>
<ol type="a">
<li>Consistent power consumption across all deployment environments</li>
<li>Diverse power requirements across different ML deployment scales</li>
<li>Uniform hardware architecture in ML systems</li>
<li>Lack of interest in energy efficiency from industry</li>
</ol>
</li>
<li><p>Explain why system-level power measurement offers a more holistic view than measuring individual components in isolation.</p></li>
<li><p>True or False: Increasing processor frequency always leads to proportional improvements in both performance and energy efficiency.</p></li>
<li><p>Reducing model precision from FP32 to INT8 might reduce accuracy by 1-2% but can improve energy efficiency by ____.</p></li>
<li><p>Order the following steps in assessing energy efficiency in ML systems: Measure power consumption, Analyze performance tradeoffs, Implement power management techniques, Evaluate system-level interactions.</p></li>
</ol>
<p><a href="#quiz-answer-sec-benchmarking-ai-energy-efficiency-measurement-a669" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section></section><section id="sec-benchmarking-ai-challenges-limitations-5fd3" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-challenges-limitations-5fd3">Challenges &amp; Limitations</h2>
<p>Benchmarking provides a structured framework for evaluating the performance of AI systems, but it comes with significant challenges. If these challenges are not properly addressed, they can undermine the credibility and usefulness of benchmarking results. One of the most fundamental issues is incomplete problem coverage. Many benchmarks, while useful for controlled comparisons, fail to capture the full diversity of real-world applications. For instance, common image classification datasets, such as <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>, contain a limited variety of images. As a result, models that perform well on these datasets may struggle when applied to more complex, real-world scenarios with greater variability in lighting, perspective, and object composition.</p>
<p>Another challenge is statistical insignificance, which arises when benchmark evaluations are conducted on too few data samples or trials. For example, testing an optical character recognition (OCR) system on a small dataset may not accurately reflect its performance on large-scale, noisy text documents. Without sufficient trials and diverse input distributions, benchmarking results may be misleading or fail to capture true system reliability.</p>
<p>Reproducibility is also a major concern. Benchmark results can vary significantly depending on factors such as hardware configurations, software versions, and system dependencies. Small differences in compilers, numerical precision, or library updates can lead to inconsistent performance measurements across different environments. To mitigate this issue, MLPerf addresses reproducibility by providing reference implementations, standardized test environments, and strict submission guidelines. Even with these efforts, achieving true consistency across diverse hardware platforms remains an ongoing challenge.</p>
<p>A more fundamental limitation of benchmarking is the risk of misalignment with real-world goals. Many benchmarks emphasize metrics such as speed, accuracy, and throughput, but practical AI deployments often require balancing multiple objectives, including power efficiency, cost, and robustness. A model that achieves state-of-the-art accuracy on a benchmark may be impractical for deployment if it consumes excessive energy or requires expensive hardware. Furthermore, benchmarks can quickly become outdated due to the rapid evolution of AI models and hardware. New techniques may emerge that render existing benchmarks less relevant, necessitating continuous updates to keep benchmarking methodologies aligned with state-of-the-art developments.</p>
<p>While these challenges affect all benchmarking efforts, the most pressing concern is the role of benchmark engineering, which introduces the risk of over-optimization for specific benchmark tasks rather than meaningful improvements in real-world performance.</p>
<section id="sec-benchmarking-ai-environmental-conditions-d6ae" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-environmental-conditions-d6ae">Environmental Conditions</h3>
<p>Environmental conditions in AI benchmarking refer to the physical and operational circumstances under which experiments are conducted. These conditions, while often overlooked, can significantly influence benchmark results and impact the reproducibility of experiments. Physical environmental factors include ambient temperature, humidity, air quality, and altitude. These elements can affect hardware performance in subtle but measurable ways. For instance, elevated temperatures may lead to thermal throttling<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> in processors, potentially reducing computational speed and affecting benchmark outcomes. Similarly, variations in altitude can impact cooling system efficiency and hard drive performance due to changes in air pressure.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<strong>Thermal Throttling</strong>: A mechanism in computer processors that reduces performance to prevent overheating, often triggered by excessive computational load or inadequate cooling.</p></div></div><p>Operational environmental factors encompass the broader system context in which benchmarks are executed. This includes background processes running on the system, network conditions, and power supply stability. The presence of other active programs or services can compete for computational resources, potentially altering the performance characteristics of the model under evaluation. To ensure the validity and reproducibility of benchmark results, it is essential to document and control these environmental conditions to the extent possible. This may involve conducting experiments in temperature-controlled environments, monitoring and reporting ambient conditions, standardizing the operational state of benchmark systems, and documenting any background processes or system loads.</p>
<p>In scenarios where controlling all environmental variables is impractical, such as in distributed or cloud-based benchmarking, it becomes essential to report these conditions in detail. This information allows other researchers to account for potential variations when interpreting or attempting to reproduce results. As machine learning models are increasingly deployed in diverse real-world environments, understanding the impact of environmental conditions on model performance becomes even more critical. This knowledge not only ensures more accurate benchmarking but also informs the development of robust models capable of consistent performance across varying operational conditions.</p>
</section><section id="sec-benchmarking-ai-hardware-lottery-8141" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-hardware-lottery-8141">Hardware Lottery</h3>
<p>A critical issue in benchmarking is what has been described as the hardware lottery, a concept introduced by <span class="citation" data-cites="hooker2021hardware">(<a href="#ref-hooker2021hardware" role="doc-biblioref">Ahmed et al. 2021</a>)</span>. The success of a machine learning model is often dictated not only by its architecture and training data but also by how well it aligns with the underlying hardware used for inference. Some models perform exceptionally well, not because they are inherently better, but because they are optimized for the parallel processing capabilities of GPUs or TPUs. Meanwhile, other promising architectures may be overlooked because they do not map efficiently to dominant hardware platforms.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hooker2021hardware" class="csl-entry" role="listitem">
Ahmed, Reyan, Greg Bodwin, Keaton Hamm, Stephen Kobourov, and Richard Spence. 2021. <span>“On Additive Spanners in Weighted Graphs with Local Error.”</span> <em>arXiv Preprint arXiv:2103.09731</em> 64 (12): 58–65. <a href="https://doi.org/10.1145/3467017">https://doi.org/10.1145/3467017</a>.
</div></div><p>This dependence on hardware compatibility introduces biases into benchmarking. A model that is highly efficient on a specific GPU may perform poorly on a CPU or a custom AI accelerator. For instance, <a href="#fig-hw-lottery" class="quarto-xref">Figure&nbsp;9</a> compares the performance of models across different hardware platforms. The multi-hardware models show comparable results to “MobileNetV3 Large min” on both the CPU <code>uint8</code> and GPU configurations. However, these multi-hardware models demonstrate significant performance improvements over the MobileNetV3 Large baseline when run on the EdgeTPU and DSP hardware. This emphasizes the variable efficiency of multi-hardware models in specialized computing environments.</p>
<div id="fig-hw-lottery" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-hw-lottery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/hardware_lottery.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Hardware-Dependent Accuracy: Model performance varies significantly across hardware platforms, indicating that architectural efficiency is not solely determined by design but also by hardware compatibility. Multi-hardware models exhibit comparable accuracy to mobilenetv3 large on CPU and GPU configurations, yet achieve substantial gains on EdgeTPU and DSP, emphasizing the importance of hardware-aware model optimization for specialized computing environments. Source: [@chu2021discovering]."><img src="images/png/hardware_lottery.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hw-lottery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Hardware-Dependent Accuracy</strong>: Model performance varies significantly across hardware platforms, indicating that architectural efficiency is not solely determined by design but also by hardware compatibility. Multi-hardware models exhibit comparable accuracy to mobilenetv3 large on CPU and GPU configurations, yet achieve substantial gains on EdgeTPU and DSP, emphasizing the importance of hardware-aware model optimization for specialized computing environments. Source: <span class="citation" data-cites="chu2021discovering">(<a href="#ref-chu2021discovering" role="doc-biblioref">Chu et al. 2021</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-chu2021discovering" class="csl-entry" role="listitem">
Chu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew Howard. 2021. <span>“Discovering Multi-Hardware Mobile Models via Architecture Search.”</span> In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 34:3016–25. IEEE. <a href="https://doi.org/10.1109/cvprw53098.2021.00337">https://doi.org/10.1109/cvprw53098.2021.00337</a>.
</div></div></figure>
</div>
<p>Without careful benchmarking across diverse hardware configurations, the field risks favoring architectures that “win” the hardware lottery rather than selecting models based on their intrinsic strengths. This bias can shape research directions, influence funding allocation, and impact the design of next-generation AI systems. In extreme cases, it may even stifle innovation by discouraging exploration of alternative architectures that do not align with current hardware trends.</p>
</section><section id="sec-benchmarking-ai-benchmark-engineering-d8cc" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-benchmark-engineering-d8cc">Benchmark Engineering</h3>
<p>While the hardware lottery is an unintended consequence of hardware trends, benchmark engineering is an intentional practice where models or systems are explicitly optimized to excel on specific benchmark tests. This practice can lead to misleading performance claims and results that do not generalize beyond the benchmarking environment.</p>
<p>Benchmark engineering occurs when AI developers fine-tune hyperparameters, preprocessing techniques, or model architectures specifically to maximize benchmark scores rather than improve real-world performance. For example, an object detection model might be carefully optimized to achieve record-low latency on a benchmark but fail when deployed in dynamic, real-world environments with varying lighting, motion blur, and occlusions. Similarly, a language model might be tuned to excel on benchmark datasets but struggle when processing conversational speech with informal phrasing and code-switching.</p>
<p>The pressure to achieve high benchmark scores is often driven by competition, marketing, and research recognition. Benchmarks are frequently used to rank AI models and systems, creating an incentive to optimize specifically for them. While this can drive technical advancements, it also risks prioritizing benchmark-specific optimizations at the expense of broader generalization.</p>
</section><section id="sec-benchmarking-ai-bias-overoptimization-9bbf" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-bias-overoptimization-9bbf">Bias &amp; Over-Optimization</h3>
<p>To ensure that benchmarks remain useful and fair, several strategies can be employed. Transparency is one of the most important factors in maintaining benchmarking integrity. Benchmark submissions should include detailed documentation on any optimizations applied, ensuring that improvements are clearly distinguished from benchmark-specific tuning. Researchers and developers should report both benchmark performance and real-world deployment results to provide a complete picture of a system’s capabilities.</p>
<p>Another approach is to diversify and evolve benchmarking methodologies. Instead of relying on a single static benchmark, AI systems should be evaluated across multiple, continuously updated benchmarks that reflect real-world complexity. This reduces the risk of models being overfitted to a single test set and encourages general-purpose improvements rather than narrow optimizations.</p>
<p>Standardization and third-party verification can also help mitigate bias. By establishing industry-wide benchmarking standards and requiring independent third-party audits of results, the AI community can improve the reliability and credibility of benchmarking outcomes. Third-party verification ensures that reported results are reproducible across different settings and helps prevent unintentional benchmark gaming.</p>
<p>Another important strategy is application-specific testing. While benchmarks provide controlled evaluations, real-world deployment testing remains essential. AI models should be assessed not only on benchmark datasets but also in practical deployment environments. For instance, an autonomous driving model should be tested in a variety of weather conditions and urban settings rather than being judged solely on controlled benchmark datasets.</p>
<p>Finally, fairness across hardware platforms must be considered. Benchmarks should test AI models on multiple hardware configurations to ensure that performance is not being driven solely by compatibility with a specific platform. This helps reduce the risk of the hardware lottery and provides a more balanced evaluation of AI system efficiency.</p>
</section><section id="sec-benchmarking-ai-benchmark-evolution-69c1" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-benchmark-evolution-69c1">Benchmark Evolution</h3>
<p>One of the greatest challenges in benchmarking is that benchmarks are never static. As AI systems evolve, so must the benchmarks that evaluate them. What defines “good performance” today may be irrelevant tomorrow as models, hardware, and application requirements change. While benchmarks are essential for tracking progress, they can also quickly become outdated, leading to over-optimization for old metrics rather than real-world performance improvements.</p>
<p>This evolution is evident in the history of AI benchmarks. Early model benchmarks, for instance, focused heavily on image classification and object detection, as these were some of the first widely studied deep learning tasks. However, as AI expanded into natural language processing, recommendation systems, and generative AI, it became clear that these early benchmarks no longer reflected the most important challenges in the field. In response, new benchmarks emerged to measure language understanding <span class="citation" data-cites="wang2018glue wang2019superglue">(<a href="#ref-wang2018glue" role="doc-biblioref">Wang et al. 2018</a>, <a href="#ref-wang2019superglue" role="doc-biblioref">2019</a>)</span> and generative AI <span class="citation" data-cites="liang2022helm">(<a href="#ref-liang2022helm" role="doc-biblioref">Liang et al. 2022</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wang2018glue" class="csl-entry" role="listitem">
Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. <span>“GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.”</span> <em>arXiv Preprint arXiv:1804.07461</em>, April. <a href="http://arxiv.org/abs/1804.07461v3">http://arxiv.org/abs/1804.07461v3</a>.
</div><div id="ref-wang2019superglue" class="csl-entry" role="listitem">
Wang, Alex, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. <span>“SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.”</span> <em>arXiv Preprint arXiv:1905.00537</em>, May. <a href="http://arxiv.org/abs/1905.00537v3">http://arxiv.org/abs/1905.00537v3</a>.
</div><div id="ref-liang2022helm" class="csl-entry" role="listitem">
Liang, Percy, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, et al. 2022. <span>“Holistic Evaluation of Language Models.”</span> <em>arXiv Preprint arXiv:2211.09110</em>, November. <a href="http://arxiv.org/abs/2211.09110v2">http://arxiv.org/abs/2211.09110v2</a>.
</div><div id="ref-duarte2022fastml" class="csl-entry" role="listitem">
Duarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. 2022a. <span>“FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning.”</span> <em>arXiv Preprint arXiv:2207.07958</em>, July. <a href="http://arxiv.org/abs/2207.07958v1">http://arxiv.org/abs/2207.07958v1</a>.
</div></div><p>Benchmark evolution extends beyond the addition of new tasks to encompass new dimensions of performance measurement. While traditional AI benchmarks emphasized accuracy and throughput, modern applications demand evaluation across multiple criteria: fairness, robustness, scalability, and energy efficiency. <a href="#fig-sciml-graph" class="quarto-xref">Figure&nbsp;10</a> illustrates this complexity through scientific applications, which span orders of magnitude in their performance requirements. For instance, Large Hadron Collider sensors must process data at rates approaching 10<span class="math inline">\(^{14}\)</span> bytes per second with nanosecond-scale computation times, while mobile applications operate at 10<span class="math inline">\(^{4}\)</span> bytes per second with longer computational windows. This range of requirements necessitates specialized benchmarks—for example, edge AI applications require benchmarks like MLPerf that specifically evaluate performance under resource constraints and scientific application domains need their own “Fast ML for Science” benchmarks <span class="citation" data-cites="duarte2022fastml">(<a href="#ref-duarte2022fastml" role="doc-biblioref">Duarte et al. 2022a</a>)</span>.</p>
<div id="fig-sciml-graph" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-sciml-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c4821deceb48b3e5290afa21d398b55a074b17ba.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Performance Spectrum: Scientific applications and edge devices demand vastly different computational resources, spanning multiple orders of magnitude in data rates and latency requirements. Consequently, traditional benchmarks focused solely on accuracy are insufficient; specialized evaluation metrics and benchmarks—like mlperf—become essential for optimizing AI systems across diverse deployment scenarios. Source: [@duarte2022fastmlsciencebenchmarksaccelerating]."><img src="benchmarking_files/mediabag/c4821deceb48b3e5290afa21d398b55a074b17ba.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sciml-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Performance Spectrum</strong>: Scientific applications and edge devices demand vastly different computational resources, spanning multiple orders of magnitude in data rates and latency requirements. Consequently, traditional benchmarks focused solely on accuracy are insufficient; specialized evaluation metrics and benchmarks—like mlperf—become essential for optimizing AI systems across diverse deployment scenarios. Source: <span class="citation" data-cites="duarte2022fastmlsciencebenchmarksaccelerating">(<a href="#ref-duarte2022fastmlsciencebenchmarksaccelerating" role="doc-biblioref">Duarte et al. 2022b</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-duarte2022fastmlsciencebenchmarksaccelerating" class="csl-entry" role="listitem">
———. 2022b. <span>“FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning,”</span> July. <a href="http://arxiv.org/abs/2207.07958v1">http://arxiv.org/abs/2207.07958v1</a>.
</div></div></figure>
</div>
<p>The need for evolving benchmarks also presents a challenge: stability versus adaptability. On the one hand, benchmarks must remain stable for long enough to allow meaningful comparisons over time. If benchmarks change too frequently, it becomes difficult to track long-term progress and compare new results with historical performance. On the other hand, failing to update benchmarks leads to stagnation, where models are optimized for outdated tasks rather than advancing the field. Striking the right balance between benchmark longevity and adaptation is an ongoing challenge for the AI community.</p>
<p>Despite these difficulties, evolving benchmarks is essential for ensuring that AI progress remains meaningful. Without updates, benchmarks risk becoming detached from real-world needs, leading researchers and engineers to focus on optimizing models for artificial test cases rather than solving practical challenges. As AI continues to expand into new domains, benchmarking must keep pace, ensuring that performance evaluations remain relevant, fair, and aligned with real-world deployment scenarios.</p>
</section><section id="sec-benchmarking-ai-mlperfs-role-5183" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-mlperfs-role-5183">MLPerf’s Role</h3>
<p>MLPerf has played a crucial role in improving benchmarking by reducing bias, increasing generalizability, and ensuring benchmarks evolve alongside AI advancements. One of its key contributions is the standardization of benchmarking environments. By providing reference implementations, clearly defined rules, and reproducible test environments, MLPerf ensures that performance results are consistent across different hardware and software platforms, reducing variability in benchmarking outcomes.</p>
<p>Recognizing that AI is deployed in a variety of real-world settings, MLPerf has also introduced different categories of inference benchmarks. The inclusion of MLPerf Inference, MLPerf Mobile, MLPerf Client, and MLPerf Tiny reflects an effort to evaluate models in the contexts where they will actually be deployed. This approach mitigates issues such as the hardware lottery by ensuring that AI systems are tested across diverse computational environments, rather than being over-optimized for a single hardware type.</p>
<p>Beyond providing a structured benchmarking framework, MLPerf is continuously evolving to keep pace with the rapid progress in AI. New tasks are incorporated into benchmarks to reflect emerging challenges, such as generative AI models and energy-efficient computing, ensuring that evaluations remain relevant and forward-looking. By regularly updating its benchmarking methodologies, MLPerf helps prevent benchmarks from becoming outdated or encouraging overfitting to legacy performance metrics.</p>
<p>By prioritizing fairness, transparency, and adaptability, MLPerf ensures that benchmarking remains a meaningful tool for guiding AI research and deployment. Instead of simply measuring raw speed or accuracy, MLPerf’s evolving benchmarks aim to capture the complexities of real-world AI performance, ultimately fostering more reliable, efficient, and impactful AI systems.</p>
<div id="quiz-question-sec-benchmarking-ai-challenges-limitations-5fd3" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li>
<p>What is a major limitation of current AI benchmarks in capturing real-world application diversity?</p>
<ol type="a">
<li>They focus too much on hardware compatibility.</li>
<li>They often have incomplete problem coverage.</li>
<li>They are too complex to implement.</li>
<li>They only measure power efficiency.</li>
</ol>
</li>
<li><p>True or False: Environmental conditions such as ambient temperature and air quality do not significantly affect AI benchmarking results.</p></li>
<li><p>Explain how the concept of the ‘hardware lottery’ can introduce bias into AI benchmarking.</p></li>
<li><p>Benchmark engineering can lead to misleading performance claims by optimizing models for specific tasks rather than improving ________ performance.</p></li>
<li><p>Order the following steps to ensure fair and effective AI benchmarking: Document environmental conditions, Use diverse benchmarks, Standardize test environments, Report real-world results.</p></li>
</ol>
<p><a href="#quiz-answer-sec-benchmarking-ai-challenges-limitations-5fd3" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section></section><section id="sec-benchmarking-ai-beyond-system-benchmarking-e69a" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-beyond-system-benchmarking-e69a">Beyond System Benchmarking</h2>
<p>While this chapter has primarily focused on system benchmarking, AI performance is not determined by system efficiency alone. Machine learning models and datasets play an equally crucial role in shaping AI capabilities. Model benchmarking evaluates algorithmic performance, while data benchmarking ensures that training datasets are high-quality, unbiased, and representative of real-world distributions. Understanding these aspects is vital because AI systems are not just computational pipelines—they are deeply dependent on the models they execute and the data they are trained on.</p>
<section id="sec-benchmarking-ai-model-benchmarking-e9fb" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-model-benchmarking-e9fb">Model Benchmarking</h3>
<p>Model benchmarks measure how well different machine learning algorithms perform on specific tasks. Historically, benchmarks focused almost exclusively on accuracy, but as models have grown more complex, additional factors, including fairness, robustness, efficiency, and generalizability, have become equally important.</p>
<p>The evolution of machine learning has been largely driven by benchmark datasets. The MNIST dataset <span class="citation" data-cites="lecun1998gradient">(<a href="#ref-lecun1998gradient" role="doc-biblioref">Lecun et al. 1998</a>)</span> was one of the earliest catalysts, advancing handwritten digit recognition, while the ImageNet dataset <span class="citation" data-cites="deng2009imagenet">(<a href="#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>)</span> sparked the deep learning revolution in image classification. More recently, datasets like COCO <span class="citation" data-cites="lin2014microsoft">(<a href="#ref-lin2014microsoft" role="doc-biblioref">Lin et al. 2014</a>)</span> for object detection and GPT-3’s training corpus <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> have pushed the boundaries of model capabilities even further.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun1998gradient" class="csl-entry" role="listitem">
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div><div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, R. Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>“ImageNet: A Large-Scale Hierarchical Image Database.”</span> In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. Ieee; IEEE. <a href="https://doi.org/10.1109/cvprw.2009.5206848">https://doi.org/10.1109/cvprw.2009.5206848</a>.
</div><div id="ref-lin2014microsoft" class="csl-entry" role="listitem">
Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. <span>“Microsoft COCO: Common Objects in Context.”</span> In <em>Computer Vision – ECCV 2014</em>, 740–55. Springer; Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-10602-1%5C_48">https://doi.org/10.1007/978-3-319-10602-1\_48</a>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> Edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. <em>Advances in Neural Information Processing Systems</em> 33 (May): 1877–1901. <a href="https://doi.org/10.48550/arxiv.2005.14165">https://doi.org/10.48550/arxiv.2005.14165</a>.
</div><div id="ref-xu2024benchmarking" class="csl-entry" role="listitem">
Xu, Ruijie, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. 2024. <span>“Benchmarking Benchmark Leakage in Large Language Models.”</span> <em>arXiv Preprint arXiv:2404.18824</em>, April. <a href="http://arxiv.org/abs/2404.18824v1">http://arxiv.org/abs/2404.18824v1</a>.
</div></div><p>However, model benchmarks face significant limitations, particularly in the era of Large Language Models (LLMs). Beyond the traditional challenge of models failing in real-world conditions, commonly referred to as the Sim2Real gap, a new form of benchmark optimization has emerged, analogous to but distinct from classical benchmark engineering in computer systems. In traditional systems evaluation, developers would explicitly optimize their code implementations to perform well on benchmark suites like SPEC or TPC, which we discussed earlier under “Benchmark Engineering”. In the case of LLMs, this phenomenon manifests through data rather than code: benchmark datasets may become embedded in training data, either inadvertently through web-scale training or deliberately through dataset curation <span class="citation" data-cites="xu2024benchmarking">(<a href="#ref-xu2024benchmarking" role="doc-biblioref">Xu et al. 2024</a>)</span>. This creates fundamental challenges for model evaluation, as high performance on benchmark tasks may reflect memorization rather than genuine capability. The key distinction lies in the mechanism: while systems benchmark engineering occurred through explicit code optimization, LLM benchmark adaptation can occur implicitly through data exposure during pre-training, raising new questions about the validity of current evaluation methodologies.</p>
<p>These challenges extend beyond just LLMs. Traditional machine learning systems continue to struggle with problems of overfitting and bias. The Gender Shades project <span class="citation" data-cites="buolamwini2018gender">(<a href="#ref-buolamwini2018gender" role="doc-biblioref">Buolamwini and Gebru 2018</a>)</span>, for instance, revealed that commercial facial recognition models performed significantly worse on darker-skinned individuals, highlighting the critical importance of fairness in model evaluation. Such findings underscore the limitations of focusing solely on aggregate accuracy metrics.</p>
<div class="no-row-height column-margin column-container"><div id="ref-buolamwini2018gender" class="csl-entry" role="listitem">
Buolamwini, Joy, and Timnit Gebru. 2018. <span>“Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.”</span> In <em>Conference on Fairness, Accountability and Transparency</em>, 77–91. PMLR. <a href="http://proceedings.mlr.press/v81/buolamwini18a.html">http://proceedings.mlr.press/v81/buolamwini18a.html</a>.
</div></div><p>Moving forward, we must fundamentally rethink its approach to benchmarking. This evolution requires developing evaluation frameworks that go beyond traditional metrics to assess multiple dimensions of model behavior—from generalization and robustness to fairness and efficiency. Key challenges include creating benchmarks that remain relevant as models advance, developing methodologies that can differentiate between genuine capabilities and artificial performance gains, and establishing standards for benchmark documentation and transparency. Success in these areas will help ensure that benchmark results provide meaningful insights about model capabilities rather than reflecting artifacts of training procedures or evaluation design.</p>
</section><section id="sec-benchmarking-ai-data-benchmarking-855c" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-data-benchmarking-855c">Data Benchmarking</h3>
<p>The evolution of artificial intelligence has traditionally focused on model-centric approaches, emphasizing architectural improvements and optimization techniques. However, contemporary AI development reveals that data quality, rather than model design alone, often determines performance boundaries. This recognition has elevated data benchmarking to a critical field that ensures AI models learn from datasets that are high-quality, diverse, and free from bias.</p>
<p>This evolution represents a fundamental shift from model-centric to data-centric AI approaches, as illustrated in <a href="#fig-model-vs-data" class="quarto-xref">Figure&nbsp;11</a>. The traditional model-centric paradigm focuses on enhancing model architectures, refining algorithms, and improving computational efficiency while treating datasets as fixed components. In contrast, the emerging data-centric approach systematically improves dataset quality through better annotations, increased diversity, and bias reduction, while maintaining consistent model architectures and system configurations. Research increasingly demonstrates that methodical dataset enhancement can yield superior performance gains compared to model refinements alone, challenging the conventional emphasis on architectural innovation.</p>
<div id="fig-model-vs-data" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-model-vs-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="e66c88ffc172d27f92b99b5454d93e446fa99392.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Development Paradigms: Model-centric AI prioritizes architectural innovation with fixed datasets, while data-centric AI systematically improves dataset quality—annotations, diversity, and bias—with consistent model architectures to achieve performance gains. Modern research indicates that strategic data enhancement often yields greater improvements than solely refining model complexity."><img src="benchmarking_files/mediabag/e66c88ffc172d27f92b99b5454d93e446fa99392.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-vs-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Development Paradigms</strong>: Model-centric AI prioritizes architectural innovation with fixed datasets, while data-centric AI systematically improves dataset quality—annotations, diversity, and bias—with consistent model architectures to achieve performance gains. Modern research indicates that strategic data enhancement often yields greater improvements than solely refining model complexity.
</figcaption></figure>
</div>
<p>Data quality’s primacy in AI development reflects a fundamental shift in understanding: superior datasets, not just sophisticated models, produce more reliable and robust AI systems. Initiatives like DataPerf and DataComp have emerged to systematically evaluate how dataset improvements affect model performance. For instance, DataComp <span class="citation" data-cites="gadre2024datacomp">(<a href="#ref-gadre2024datacomp" role="doc-biblioref">Nishigaki 2024</a>)</span> demonstrated that models trained on a carefully curated 30% subset of data achieved better results than those trained on the complete dataset, challenging the assumption that more data automatically leads to better performance <span class="citation" data-cites="northcutt2021pervasive">(<a href="#ref-northcutt2021pervasive" role="doc-biblioref">Northcutt, Athalye, and Mueller 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gadre2024datacomp" class="csl-entry" role="listitem">
Nishigaki, Shinsuke. 2024. <span>“Eigenphase Distributions of Unimodular Circular Ensembles.”</span> <em>arXiv Preprint arXiv:2401.09045</em> 36 (January). <a href="http://arxiv.org/abs/2401.09045v2">http://arxiv.org/abs/2401.09045v2</a>.
</div><div id="ref-northcutt2021pervasive" class="csl-entry" role="listitem">
Northcutt, Curtis G., Anish Athalye, and Jonas Mueller. 2021. <span>“Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.”</span> <em>arXiv Preprint arXiv:2103.14749</em> 34 (March): 19075–90. <a href="https://doi.org/10.48550/arxiv.2103.14749">https://doi.org/10.48550/arxiv.2103.14749</a>.
</div></div><p>A significant challenge in data benchmarking emerges from dataset saturation. When models achieve near-perfect accuracy on benchmarks like ImageNet, it becomes crucial to distinguish whether performance gains represent genuine advances in AI capability or merely optimization to existing test sets. <a href="#fig-dataset-saturation" class="quarto-xref">Figure&nbsp;12</a> illustrates this trend, showing AI systems surpassing human performance across various applications over the past decade.</p>
<div id="fig-dataset-saturation" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-dataset-saturation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c4c5d7a34a48dee9b6266164bfe2fd31f7080e10.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Dataset Saturation: AI systems surpass human performance on benchmark datasets, indicating that continued gains may not reflect genuine improvements in intelligence but rather optimization to fixed evaluation sets. This trend underscores the need for dynamic, challenging datasets that accurately assess AI capabilities and drive meaningful progress beyond simple pattern recognition. Source: [@kiela2021dynabench]."><img src="benchmarking_files/mediabag/c4c5d7a34a48dee9b6266164bfe2fd31f7080e10.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dataset-saturation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>Dataset Saturation</strong>: AI systems surpass human performance on benchmark datasets, indicating that continued gains may not reflect genuine improvements in intelligence but rather optimization to fixed evaluation sets. This trend underscores the need for dynamic, challenging datasets that accurately assess AI capabilities and drive meaningful progress beyond simple pattern recognition. Source: <span class="citation" data-cites="kiela2021dynabench">(<a href="#ref-kiela2021dynabench" role="doc-biblioref">Kiela et al. 2021</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>This saturation phenomenon raises fundamental methodological questions <span class="citation" data-cites="kiela2021dynabench">(<a href="#ref-kiela2021dynabench" role="doc-biblioref">Kiela et al. 2021</a>)</span>. The MNIST dataset provides an illustrative example: certain test images, though nearly illegible to humans, were assigned specific labels during the dataset’s creation in 1994. When models correctly predict these labels, their apparent superhuman performance may actually reflect memorization of dataset artifacts rather than true digit recognition capabilities.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kiela2021dynabench" class="csl-entry" role="listitem">
Kiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. <span>“Dynabench: Rethinking Benchmarking in NLP.”</span> In <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 9:418–34. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2021.naacl-main.324">https://doi.org/10.18653/v1/2021.naacl-main.324</a>.
</div><div id="ref-beyer2020we" class="csl-entry" role="listitem">
Beyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. 2020. <span>“Are We Done with ImageNet?”</span> <em>arXiv Preprint arXiv:2006.07159</em>, June. <a href="http://arxiv.org/abs/2006.07159v1">http://arxiv.org/abs/2006.07159v1</a>.
</div></div><p>These challenges extend beyond individual domains. The provocative question “Are we done with ImageNet?” <span class="citation" data-cites="beyer2020we">(<a href="#ref-beyer2020we" role="doc-biblioref">Beyer et al. 2020</a>)</span> highlights broader concerns about the limitations of static benchmarks. Models optimized for fixed datasets often struggle with distribution shifts—real-world changes that occur after training data collection. This limitation has driven the development of dynamic benchmarking approaches, such as Dynabench <span class="citation" data-cites="kiela2021dynabench">(<a href="#ref-kiela2021dynabench" role="doc-biblioref">Kiela et al. 2021</a>)</span>, which continuously evolves test data based on model performance to maintain benchmark relevance.</p>
<p>Current data benchmarking efforts encompass several critical dimensions. Label quality assessment remains a central focus, as explored in DataPerf’s debugging challenge. Initiatives like MSWC <span class="citation" data-cites="mazumder2021multilingual">(<a href="#ref-mazumder2021multilingual" role="doc-biblioref">Mazumder et al. 2021</a>)</span> for speech recognition address bias and representation in datasets. Out-of-distribution generalization receives particular attention through benchmarks like RxRx and WILDS <span class="citation" data-cites="koh2021wilds">(<a href="#ref-koh2021wilds" role="doc-biblioref">Koh et al. 2021</a>)</span>. These diverse efforts reflect a growing recognition that advancing AI capabilities requires not just better models and systems, but fundamentally better approaches to data quality assessment and benchmark design.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mazumder2021multilingual" class="csl-entry" role="listitem">
Mazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. <span>“Multilingual Spoken Words Corpus.”</span> In <em>Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>.
</div><div id="ref-koh2021wilds" class="csl-entry" role="listitem">
Koh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. <span>“WILDS: A Benchmark of in-the-Wild Distribution Shifts.”</span> In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, edited by Marina Meila and Tong Zhang, 139:5637–64. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/koh21a.html">http://proceedings.mlr.press/v139/koh21a.html</a>.
</div></div></section><section id="sec-benchmarking-ai-benchmarking-trifecta-7da9" class="level3"><h3 class="anchored" data-anchor-id="sec-benchmarking-ai-benchmarking-trifecta-7da9">Benchmarking Trifecta</h3>
<p>AI benchmarking has traditionally evaluated systems, models, and data as separate entities. However, real-world AI performance emerges from the interplay between these three components. A fast system cannot compensate for a poorly trained model, and even the most powerful model is constrained by the quality of the data it learns from. This interdependence necessitates a holistic benchmarking approach that considers all three dimensions together.</p>
<p>As illustrated in <a href="#fig-benchmarking-trifecta" class="quarto-xref">Figure&nbsp;13</a>, the future of benchmarking lies in an integrated framework that jointly evaluates system efficiency, model performance, and data quality. This approach enables researchers to identify optimization opportunities that remain invisible when these components are analyzed in isolation. For example, co-designing efficient AI models with hardware-aware optimizations and carefully curated datasets can lead to superior performance while reducing computational costs.</p>
<div id="fig-benchmarking-trifecta" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-benchmarking-trifecta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="a387e026dbe197680b0623041361ba4cc6dfba7a.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: AI System Interdependence: Highlights the critical interplay between infrastructure, models, and data in determining overall AI system performance, emphasizing that optimization requires a holistic approach rather than isolated improvements. this figure illustrates that gains in one component cannot fully compensate for limitations in others, necessitating co-design strategies for efficient and effective AI."><img src="benchmarking_files/mediabag/a387e026dbe197680b0623041361ba4cc6dfba7a.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-benchmarking-trifecta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: <strong>AI System Interdependence</strong>: Highlights the critical interplay between infrastructure, models, and data in determining overall AI system performance, emphasizing that optimization requires a holistic approach rather than isolated improvements. this figure illustrates that gains in one component cannot fully compensate for limitations in others, necessitating co-design strategies for efficient and effective AI.
</figcaption></figure>
</div>
<p>As AI continues to evolve, benchmarking methodologies must advance in tandem. Evaluating AI performance through the lens of systems, models, and data ensures that benchmarks drive improvements not just in accuracy, but also in efficiency, fairness, and robustness. This holistic perspective will be critical for developing AI that is not only powerful but also practical, scalable, and ethical.</p>
<div id="quiz-question-sec-benchmarking-ai-beyond-system-benchmarking-e69a" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li>
<p>Which of the following best describes the ‘benchmarking trifecta’ in AI systems?</p>
<ol type="a">
<li>A focus on model accuracy, system speed, and data volume independently.</li>
<li>An integrated evaluation of system efficiency, model performance, and data quality.</li>
<li>A methodology that emphasizes model architecture enhancements.</li>
<li>An approach that prioritizes data collection over model and system improvements.</li>
</ol>
</li>
<li><p>Explain why model benchmarks that focus solely on accuracy might be insufficient in evaluating AI capabilities.</p></li>
<li><p>True or False: Data-centric AI approaches can lead to more significant performance improvements than model-centric approaches by enhancing dataset quality.</p></li>
<li><p>The phenomenon where models perform well on benchmarks due to memorization of dataset artifacts rather than genuine understanding is known as ____.</p></li>
<li><p>Order the following steps to ensure a holistic AI benchmarking approach: Evaluate model performance, Assess data quality, Analyze system efficiency, Integrate findings.</p></li>
</ol>
<p><a href="#quiz-answer-sec-benchmarking-ai-beyond-system-benchmarking-e69a" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section></section><section id="sec-benchmarking-ai-summary-3acb" class="level2"><h2 class="anchored" data-anchor-id="sec-benchmarking-ai-summary-3acb">Summary</h2>
<p><em>“What gets measured gets improved.”</em> Benchmarking plays a foundational role in the advancement of AI, providing the essential measurements needed to track progress, identify limitations, and drive innovation. This chapter has explored the multifaceted nature of benchmarking, spanning systems, models, and data, and has highlighted its critical role in optimizing AI performance across different dimensions.</p>
<p>ML system benchmarks enable optimizations in speed, efficiency, and scalability, ensuring that hardware and infrastructure can support increasingly complex AI workloads. Model benchmarks provide standardized tasks and evaluation metrics beyond accuracy, driving progress in algorithmic innovation. Data benchmarks, meanwhile, reveal key issues related to data quality, bias, and representation, ensuring that AI models are built on fair and diverse datasets.</p>
<p>While these components, systems, models, and data, are often evaluated in isolation, future benchmarking efforts will likely adopt a more integrated approach. By measuring the interplay between system, model, and data benchmarks, AI researchers and engineers can uncover new insights into the co-design of data, algorithms, and infrastructure. This holistic perspective will be essential as AI applications grow more sophisticated and are deployed across increasingly diverse environments.</p>
<p>Benchmarking is not static—it must continuously evolve to capture new AI capabilities, address emerging challenges, and refine evaluation methodologies. As AI systems become more complex and influential, the need for rigorous, transparent, and socially beneficial benchmarking standards becomes even more pressing. Achieving this requires close collaboration between industry, academia, and standardization bodies to ensure that benchmarks remain relevant, unbiased, and aligned with real-world needs.</p>
<p>Ultimately, benchmarking serves as the compass that guides AI progress. By persistently measuring and openly sharing results, we can navigate toward AI systems that are performant, robust, and trustworthy. However, benchmarking must also be aligned with human-centered principles, ensuring that AI serves society in a fair and ethical manner. The future of benchmarking is already expanding into new frontiers, including the evaluation of AI safety, fairness, and generative AI models, which will shape the next generation of AI benchmarks. These topics, while beyond the scope of this chapter, will be explored further in the discussion on Generative AI.</p>
<p>For those interested in emerging trends in AI benchmarking, the article <em><a href="https://medium.com/towards-data-science/the-olympics-of-ai-benchmarking-machine-learning-systems-c4b2051fbd2b">The Olympics of AI: Benchmarking Machine Learning Systems</a></em> provides a broader look at benchmarking efforts in robotics, extended reality, and neuromorphic computing. As benchmarking continues to evolve, it remains an essential tool for understanding, improving, and shaping the future of AI.</p>
<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
</section><section id="self-check-answers" class="level2"><h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-benchmarking-ai-ai-benchmarks-d3e3" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li>
<p><strong>What sets AI benchmarks apart from traditional performance metrics?</strong></p>
<ol type="a">
<li>They focus solely on computational speed.</li>
<li>They account for the probabilistic nature of machine learning models.</li>
<li>They measure only energy consumption.</li>
<li>They are fixed and deterministic.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. AI benchmarks account for the probabilistic nature of machine learning models, which introduces variability in results depending on the data encountered, unlike traditional metrics that measure fixed characteristics.</p>
<p><em>Learning Objective</em>: Understand the unique characteristics of AI benchmarks compared to traditional performance metrics.</p>
</li>
<li>
<p><strong>Explain why system benchmarks are crucial for AI computations.</strong></p>
<p><em>Answer</em>: System benchmarks are crucial because they evaluate the performance, efficiency, and scalability of the computational infrastructure used for AI workloads. They provide insights into throughput, latency, and resource utilization, guiding hardware selection and system optimization.</p>
<p><em>Learning Objective</em>: Analyze the importance of system benchmarks in AI computations and their impact on hardware selection and system optimization.</p>
</li>
<li>
<p><strong>True or False: Data benchmarks only assess the computational speed of machine learning models.</strong></p>
<p><em>Answer</em>: False. Data benchmarks assess the quality, coverage, bias, and robustness of datasets, which directly influence model performance and generalization capabilities.</p>
<p><em>Learning Objective</em>: Dispel misconceptions about the scope of data benchmarks and highlight their role in assessing data quality and its impact on AI systems.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-benchmarking-ai-ai-benchmarks-d3e3" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-benchmarking-ai-benchmark-components-77e9" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which component of an AI benchmark ensures that models are evaluated under consistent conditions, allowing for reproducible results?</strong></p>
<ol type="a">
<li>Problem Definition</li>
<li>Standardized Datasets</li>
<li>Benchmark Harness</li>
<li>System Specifications</li>
</ol>
<p><em>Answer</em>: The correct answer is C. The benchmark harness ensures reproducible testing by managing how inputs are delivered to the system under test and how measurements are collected, allowing for consistent conditions across evaluations.</p>
<p><em>Learning Objective</em>: Understand the role of the benchmark harness in ensuring reproducibility in AI benchmarking.</p>
</li>
<li>
<p><strong>Explain how the selection of standardized datasets influences the effectiveness of an AI benchmark.</strong></p>
<p><em>Answer</em>: The selection of standardized datasets influences the effectiveness of an AI benchmark by ensuring that all models are tested under identical conditions, enabling direct comparisons. Effective datasets must accurately represent real-world challenges and maintain complexity to differentiate model performance meaningfully.</p>
<p><em>Learning Objective</em>: Analyze the impact of dataset selection on the benchmarking process and model evaluation.</p>
</li>
<li>
<p><strong>True or False: Evaluation metrics in AI benchmarks should focus solely on model accuracy to ensure effective performance assessment.</strong></p>
<p><em>Answer</em>: False. Evaluation metrics should not focus solely on model accuracy; they must also consider other dimensions such as computational speed, memory utilization, and energy efficiency to provide a comprehensive assessment of model performance.</p>
<p><em>Learning Objective</em>: Recognize the importance of multi-dimensional evaluation metrics in AI benchmarking.</p>
</li>
<li>
<p><strong>Order the following components in the sequence they typically occur in an AI benchmarking workflow: Model Selection, Problem Definition, Evaluation Metrics, Standardized Datasets.</strong></p>
<p><em>Answer</em>: 1. Problem Definition 2. Standardized Datasets 3. Model Selection 4. Evaluation Metrics. This sequence reflects the logical progression from defining the task to selecting datasets, choosing models, and finally determining how to evaluate model performance.</p>
<p><em>Learning Objective</em>: Comprehend the sequence of components in an AI benchmarking workflow and their interconnections.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-benchmarking-ai-benchmark-components-77e9" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-benchmarking-ai-benchmarking-granularity-8676" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which type of benchmark provides a comprehensive evaluation of an entire AI system, including data processing, model performance, and infrastructure components?</strong></p>
<ol type="a">
<li>Micro-benchmarks</li>
<li>Macro-benchmarks</li>
<li>End-to-end benchmarks</li>
<li>Component-level benchmarks</li>
</ol>
<p><em>Answer</em>: The correct answer is C. End-to-end benchmarks provide a comprehensive evaluation of an entire AI system, including data processing, model performance, and infrastructure components, offering system-wide insights.</p>
<p><em>Learning Objective</em>: Understand the scope and purpose of end-to-end benchmarks in ML systems.</p>
</li>
<li>
<p><strong>True or False: Micro-benchmarks are sufficient for identifying system-level bottlenecks in production environments.</strong></p>
<p><em>Answer</em>: False. Micro-benchmarks focus on individual operations and may miss interaction effects and system-level bottlenecks that are only visible in end-to-end evaluations.</p>
<p><em>Learning Objective</em>: Recognize the limitations of micro-benchmarks in identifying system-level bottlenecks.</p>
</li>
<li>
<p><strong>Explain why macro-benchmarks are important for model architecture decisions.</strong></p>
<p><em>Answer</em>: Macro-benchmarks evaluate complete models, providing insights into how architectural choices and component interactions affect overall model behavior. This helps in making informed decisions about model architecture, optimization strategies, and deployment configurations.</p>
<p><em>Learning Objective</em>: Analyze the role of macro-benchmarks in guiding model architecture decisions.</p>
</li>
<li>
<p><strong>Micro-benchmarks focus on evaluating individual operations such as ____ and activation functions to provide detailed insights into computational demands.</strong></p>
<p><em>Answer</em>: tensor operations. Micro-benchmarks focus on evaluating individual operations such as tensor operations and activation functions to provide detailed insights into computational demands.</p>
<p><em>Learning Objective</em>: Recall the focus of micro-benchmarks in ML systems.</p>
</li>
<li>
<p><strong>Order the following benchmarking types from the most granular to the least granular: Macro-benchmarks, End-to-end benchmarks, Micro-benchmarks.</strong></p>
<p><em>Answer</em>: Micro-benchmarks, Macro-benchmarks, End-to-end benchmarks. Micro-benchmarks focus on individual operations, macro-benchmarks evaluate complete models, and end-to-end benchmarks assess the entire system pipeline.</p>
<p><em>Learning Objective</em>: Understand the levels of granularity in ML benchmarking and their implications.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-benchmarking-ai-benchmarking-granularity-8676" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-benchmarking-ai-training-benchmarks-c516" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which metric is most crucial for evaluating the efficiency of a training system in reaching a predefined accuracy threshold?</strong></p>
<ol type="a">
<li>Throughput</li>
<li>Time-to-accuracy</li>
<li>Energy consumption</li>
<li>Memory bandwidth</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Time-to-accuracy is crucial as it measures how quickly a model reaches a target accuracy, reflecting the overall efficiency of the training process.</p>
<p><em>Learning Objective</em>: Understand the importance of time-to-accuracy in training benchmarks.</p>
</li>
<li>
<p><strong>Explain why training benchmarks are essential for optimizing hardware and software configurations in large-scale ML systems.</strong></p>
<p><em>Answer</em>: Training benchmarks provide a standardized framework for evaluating system performance, identifying bottlenecks, and guiding optimizations. They help ensure that hardware and software configurations are efficient, scalable, and cost-effective, particularly in large-scale environments.</p>
<p><em>Learning Objective</em>: Analyze the role of training benchmarks in system optimization.</p>
</li>
<li>
<p><strong>True or False: Higher throughput always results in faster training times for machine learning models.</strong></p>
<p><em>Answer</em>: False. While higher throughput can indicate faster data processing, it does not guarantee faster training times if it compromises accuracy convergence or introduces inefficiencies.</p>
<p><em>Learning Objective</em>: Challenge the misconception that throughput alone determines training efficiency.</p>
</li>
<li>
<p><strong>Training benchmarks help assess the scalability of a system by measuring how well it handles increased computational resources, such as additional ____ or TPUs.</strong></p>
<p><em>Answer</em>: GPUs. Scalability assessments focus on how effectively a system can utilize additional GPUs or TPUs to improve training performance.</p>
<p><em>Learning Objective</em>: Understand scalability considerations in training benchmarks.</p>
</li>
<li>
<p><strong>Order the following steps in evaluating a training benchmark: Identify bottlenecks, Measure time-to-accuracy, Optimize configurations, Implement distributed training.</strong></p>
<p><em>Answer</em>: Measure time-to-accuracy, Identify bottlenecks, Implement distributed training, Optimize configurations. This sequence ensures that performance is evaluated first, bottlenecks are identified, distributed strategies are applied, and configurations are optimized.</p>
<p><em>Learning Objective</em>: Reinforce the process of evaluating and optimizing training benchmarks.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-benchmarking-ai-training-benchmarks-c516" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-benchmarking-ai-inference-benchmarks-5e47" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which inference benchmark metric is most critical for ensuring real-time performance in safety-critical AI applications?</strong></p>
<ol type="a">
<li>Throughput</li>
<li>Tail latency</li>
<li>Memory footprint</li>
<li>Power consumption</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Tail latency is crucial for real-time performance in safety-critical applications because it measures the worst-case delays, ensuring the system can handle peak loads without compromising safety.</p>
<p><em>Learning Objective</em>: Understand the importance of tail latency in real-time AI applications and its role in inference benchmarking.</p>
</li>
<li>
<p><strong>Explain why inference benchmarks are essential for optimizing AI models on mobile devices.</strong></p>
<p><em>Answer</em>: Inference benchmarks are essential for optimizing AI models on mobile devices because they evaluate performance under strict power and memory constraints, ensuring models are efficient and responsive. They help identify trade-offs between latency, accuracy, and energy consumption, guiding optimizations for on-device AI workloads.</p>
<p><em>Learning Objective</em>: Analyze the role of inference benchmarks in optimizing AI models for mobile devices.</p>
</li>
<li>
<p><strong>True or False: Inference benchmarks typically focus more on throughput than on latency.</strong></p>
<p><em>Answer</em>: False. Inference benchmarks focus on both latency and throughput, but latency is often more critical for real-time applications where timely responses are essential.</p>
<p><em>Learning Objective</em>: Differentiate between the focus on latency and throughput in inference benchmarks.</p>
</li>
<li>
<p><strong>Inference benchmarks evaluate the impact of hardware accelerators like NPUs and FPGAs on ________ and energy efficiency in AI deployments.</strong></p>
<p><em>Answer</em>: latency. Inference benchmarks assess how hardware accelerators affect latency and energy efficiency, ensuring models run efficiently across different platforms.</p>
<p><em>Learning Objective</em>: Understand the role of hardware accelerators in improving latency and energy efficiency in AI deployments.</p>
</li>
<li>
<p><strong>Order the following inference benchmarking considerations from most to least critical for mobile AI applications: Power consumption, Memory footprint, Throughput, Latency.</strong></p>
<p><em>Answer</em>: Latency, Power consumption, Memory footprint, Throughput. Latency is most critical for responsiveness, followed by power consumption and memory footprint due to resource constraints, with throughput being less critical in mobile contexts.</p>
<p><em>Learning Objective</em>: Evaluate the relative importance of different inference benchmarking considerations for mobile AI applications.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-benchmarking-ai-inference-benchmarks-5e47" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-benchmarking-ai-energy-efficiency-measurement-a669" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li>
<p><strong>What is a significant challenge in creating standardized energy efficiency benchmarks for ML systems?</strong></p>
<ol type="a">
<li>Consistent power consumption across all deployment environments</li>
<li>Diverse power requirements across different ML deployment scales</li>
<li>Uniform hardware architecture in ML systems</li>
<li>Lack of interest in energy efficiency from industry</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Diverse power requirements across different ML deployment scales present a significant challenge in creating standardized energy efficiency benchmarks because they require accommodating vastly different scales and ensuring consistent, fair, and reproducible measurements.</p>
<p><em>Learning Objective</em>: Understand the challenges in creating standardized energy efficiency benchmarks for ML systems.</p>
</li>
<li>
<p><strong>Explain why system-level power measurement offers a more holistic view than measuring individual components in isolation.</strong></p>
<p><em>Answer</em>: System-level power measurement provides a more comprehensive understanding because it captures the interactions between compute units, memory systems, and supporting infrastructure, which are crucial for real-world ML workloads. This approach accounts for the total energy consumption, including shared resources like cooling, which individual component metrics might overlook.</p>
<p><em>Learning Objective</em>: Analyze the benefits of system-level power measurement in ML systems.</p>
</li>
<li>
<p><strong>True or False: Increasing processor frequency always leads to proportional improvements in both performance and energy efficiency.</strong></p>
<p><em>Answer</em>: False. Increasing processor frequency often results in diminishing returns for energy efficiency. For example, a 20% increase in frequency might only yield a 5% performance improvement while increasing power consumption by 50%, illustrating the non-linear relationship between performance and energy efficiency.</p>
<p><em>Learning Objective</em>: Evaluate the tradeoffs between performance and energy efficiency in ML systems.</p>
</li>
<li>
<p><strong>Reducing model precision from FP32 to INT8 might reduce accuracy by 1-2% but can improve energy efficiency by ____.</strong></p>
<p><em>Answer</em>: 3-4x. Reducing model precision from FP32 to INT8 can significantly enhance energy efficiency while maintaining acceptable accuracy levels, which is crucial for optimizing ML systems for power-constrained environments.</p>
<p><em>Learning Objective</em>: Understand the impact of model quantization on energy efficiency.</p>
</li>
<li>
<p><strong>Order the following steps in assessing energy efficiency in ML systems: Measure power consumption, Analyze performance tradeoffs, Implement power management techniques, Evaluate system-level interactions.</strong></p>
<p><em>Answer</em>: 1. Measure power consumption, 2. Evaluate system-level interactions, 3. Analyze performance tradeoffs, 4. Implement power management techniques. This sequence ensures a comprehensive assessment by first understanding the power usage, then considering the interactions and tradeoffs, and finally applying management techniques to optimize energy efficiency.</p>
<p><em>Learning Objective</em>: Apply a systematic approach to assessing energy efficiency in ML systems.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-benchmarking-ai-energy-efficiency-measurement-a669" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-benchmarking-ai-challenges-limitations-5fd3" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li>
<p><strong>What is a major limitation of current AI benchmarks in capturing real-world application diversity?</strong></p>
<ol type="a">
<li>They focus too much on hardware compatibility.</li>
<li>They often have incomplete problem coverage.</li>
<li>They are too complex to implement.</li>
<li>They only measure power efficiency.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Many AI benchmarks fail to capture the full diversity of real-world applications, as they often use limited datasets that do not reflect the complexity and variability encountered in practical scenarios.</p>
<p><em>Learning Objective</em>: Understand the limitations of AI benchmarks in representing real-world application scenarios.</p>
</li>
<li>
<p><strong>True or False: Environmental conditions such as ambient temperature and air quality do not significantly affect AI benchmarking results.</strong></p>
<p><em>Answer</em>: False. Environmental conditions can significantly influence benchmark results by affecting hardware performance, such as causing thermal throttling, which can alter computational speed and benchmark outcomes.</p>
<p><em>Learning Objective</em>: Recognize the impact of environmental conditions on the accuracy and reproducibility of AI benchmarking.</p>
</li>
<li>
<p><strong>Explain how the concept of the ‘hardware lottery’ can introduce bias into AI benchmarking.</strong></p>
<p><em>Answer</em>: The hardware lottery refers to the success of AI models being influenced by their compatibility with specific hardware platforms. This can introduce bias into benchmarking, as models optimized for popular hardware may perform well, while those not aligned with dominant platforms might be unfairly overlooked, skewing research and development priorities.</p>
<p><em>Learning Objective</em>: Analyze how hardware compatibility can bias AI benchmarking results and influence research directions.</p>
</li>
<li>
<p><strong>Benchmark engineering can lead to misleading performance claims by optimizing models for specific tasks rather than improving ________ performance.</strong></p>
<p><em>Answer</em>: real-world. Benchmark engineering focuses on optimizing models to excel in specific benchmark tests, which may not translate to improved performance in practical, real-world environments.</p>
<p><em>Learning Objective</em>: Understand the risks of benchmark engineering and its impact on real-world applicability.</p>
</li>
<li>
<p><strong>Order the following steps to ensure fair and effective AI benchmarking: Document environmental conditions, Use diverse benchmarks, Standardize test environments, Report real-world results.</strong></p>
<p><em>Answer</em>: Standardize test environments, Document environmental conditions, Use diverse benchmarks, Report real-world results. Standardizing test environments ensures consistent conditions, documenting environmental conditions provides context, using diverse benchmarks captures a wide range of scenarios, and reporting real-world results ensures transparency and applicability.</p>
<p><em>Learning Objective</em>: Apply a structured approach to conducting fair and comprehensive AI benchmarking.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-benchmarking-ai-challenges-limitations-5fd3" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-benchmarking-ai-beyond-system-benchmarking-e69a" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which of the following best describes the ‘benchmarking trifecta’ in AI systems?</strong></p>
<ol type="a">
<li>A focus on model accuracy, system speed, and data volume independently.</li>
<li>An integrated evaluation of system efficiency, model performance, and data quality.</li>
<li>A methodology that emphasizes model architecture enhancements.</li>
<li>An approach that prioritizes data collection over model and system improvements.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. The ‘benchmarking trifecta’ refers to the integrated evaluation of system efficiency, model performance, and data quality, highlighting the interdependence of these components in AI performance.</p>
<p><em>Learning Objective</em>: Understand the concept of the ‘benchmarking trifecta’ and its significance in AI performance evaluation.</p>
</li>
<li>
<p><strong>Explain why model benchmarks that focus solely on accuracy might be insufficient in evaluating AI capabilities.</strong></p>
<p><em>Answer</em>: Focusing solely on accuracy can overlook other critical factors like fairness, robustness, and generalizability. High accuracy might result from memorization rather than genuine understanding, especially if benchmark datasets are embedded in training data, leading to misleading evaluations of AI capabilities.</p>
<p><em>Learning Objective</em>: Analyze the limitations of accuracy-focused model benchmarks and understand the need for more comprehensive evaluation metrics.</p>
</li>
<li>
<p><strong>True or False: Data-centric AI approaches can lead to more significant performance improvements than model-centric approaches by enhancing dataset quality.</strong></p>
<p><em>Answer</em>: True. Data-centric AI approaches focus on improving dataset quality, which can yield superior performance gains compared to merely refining model architectures, as better data often leads to more reliable and robust AI systems.</p>
<p><em>Learning Objective</em>: Recognize the impact of data-centric approaches on AI performance and their potential advantages over model-centric methods.</p>
</li>
<li>
<p><strong>The phenomenon where models perform well on benchmarks due to memorization of dataset artifacts rather than genuine understanding is known as ____.</strong></p>
<p><em>Answer</em>: benchmark optimization. This occurs when models achieve high performance on benchmarks by memorizing specific data patterns rather than demonstrating true capability, often due to dataset artifacts.</p>
<p><em>Learning Objective</em>: Identify and understand the implications of benchmark optimization in evaluating AI models.</p>
</li>
<li>
<p><strong>Order the following steps to ensure a holistic AI benchmarking approach: Evaluate model performance, Assess data quality, Analyze system efficiency, Integrate findings.</strong></p>
<p><em>Answer</em>: 1. Assess data quality. 2. Evaluate model performance. 3. Analyze system efficiency. 4. Integrate findings. This sequence ensures that each component is evaluated comprehensively and the insights are combined to provide a holistic view of AI performance.</p>
<p><em>Learning Objective</em>: Understand the steps involved in a holistic AI benchmarking approach and their importance in evaluating overall AI performance.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-benchmarking-ai-beyond-system-benchmarking-e69a" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>



</section></section><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="pagination-link" aria-label="AI Acceleration">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">AI Acceleration</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/ops/ops.html" class="pagination-link" aria-label="ML Operations">
        <span class="nav-page-text">ML Operations</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
<li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
</div>
  </div>
</footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>


</body></html>