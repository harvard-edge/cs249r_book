<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; Security &amp; Privacy ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/responsible_ai/responsible_ai.html" rel="next">
<link href="../../../contents/core/ondevice_learning/ondevice_learning.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-46f4cc9626f044588a66931b604fc9c8.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-501830576c2f64b82eb8d4d98338213c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-23c578b0d34795b186010c7819385de6.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-501830576c2f64b82eb8d4d98338213c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../scripts/ai_menu/dist/worker-voUF5YDa.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/privacy_security/privacy_security.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="ab18bf679d12eff080697991ab97393a" class="alert alert-primary hidden"><i class="bi bi-book quarto-announcement-icon"></i><div class="quarto-announcement-content">
<div id="banner">
<p>üéâ <b>Just Announced:</b> <i>Introduction to Machine Learning Systems</i> will be published by <b>MIT Press</b> in 2026!<br> üíª Fully open source at <a href="https://mlsysbook.ai">mlsysbook.ai</a><br> üóíÔ∏è <a href="contents/frontmatter/changelog/changelog.html">View the full changelog</a><br> ‚≠ê Help grow the project: <a href="https://github.com/harvard-edge/cs249r_book">Star on GitHub</a></p>
</div>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">FRONTMATTER</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Author‚Äôs Note</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">MAIN</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ML Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">DNN Architectures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ML Operations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">LABS</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Grove Vision AI V2</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shared Labs</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">APPENDIX</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/appendix/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">REFERENCES</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link active" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview"><span class="header-section-number">15.1</span> Overview</a></li>
  <li><a href="#definitions-and-distinctions" id="toc-definitions-and-distinctions" class="nav-link" data-scroll-target="#definitions-and-distinctions"><span class="header-section-number">15.2</span> Definitions and Distinctions</a>
  <ul>
  <li><a href="#security-defined" id="toc-security-defined" class="nav-link" data-scroll-target="#security-defined"><span class="header-section-number">15.2.1</span> Security Defined</a></li>
  <li><a href="#privacy-defined" id="toc-privacy-defined" class="nav-link" data-scroll-target="#privacy-defined"><span class="header-section-number">15.2.2</span> Privacy Defined</a></li>
  <li><a href="#security-versus-privacy" id="toc-security-versus-privacy" class="nav-link" data-scroll-target="#security-versus-privacy"><span class="header-section-number">15.2.3</span> Security versus Privacy</a></li>
  <li><a href="#interactions-and-trade-offs" id="toc-interactions-and-trade-offs" class="nav-link" data-scroll-target="#interactions-and-trade-offs"><span class="header-section-number">15.2.4</span> Interactions and Trade-offs</a></li>
  </ul></li>
  <li><a href="#historical-incidents" id="toc-historical-incidents" class="nav-link" data-scroll-target="#historical-incidents"><span class="header-section-number">15.3</span> Historical Incidents</a>
  <ul>
  <li><a href="#stuxnet" id="toc-stuxnet" class="nav-link" data-scroll-target="#stuxnet"><span class="header-section-number">15.3.1</span> Stuxnet</a></li>
  <li><a href="#jeep-cherokee-hack" id="toc-jeep-cherokee-hack" class="nav-link" data-scroll-target="#jeep-cherokee-hack"><span class="header-section-number">15.3.2</span> Jeep Cherokee Hack</a></li>
  <li><a href="#mirai-botnet" id="toc-mirai-botnet" class="nav-link" data-scroll-target="#mirai-botnet"><span class="header-section-number">15.3.3</span> Mirai Botnet</a></li>
  </ul></li>
  <li><a href="#secure-design-priorities" id="toc-secure-design-priorities" class="nav-link" data-scroll-target="#secure-design-priorities"><span class="header-section-number">15.4</span> Secure Design Priorities</a>
  <ul>
  <li><a href="#device-level-security" id="toc-device-level-security" class="nav-link" data-scroll-target="#device-level-security"><span class="header-section-number">15.4.1</span> Device-Level Security</a></li>
  <li><a href="#system-level-isolation" id="toc-system-level-isolation" class="nav-link" data-scroll-target="#system-level-isolation"><span class="header-section-number">15.4.2</span> System-Level Isolation</a></li>
  <li><a href="#large-scale-network-exploitation" id="toc-large-scale-network-exploitation" class="nav-link" data-scroll-target="#large-scale-network-exploitation"><span class="header-section-number">15.4.3</span> Large-Scale Network Exploitation</a></li>
  <li><a href="#toward-secure-design" id="toc-toward-secure-design" class="nav-link" data-scroll-target="#toward-secure-design"><span class="header-section-number">15.4.4</span> Toward Secure Design</a></li>
  </ul></li>
  <li><a href="#threats-to-ml-models" id="toc-threats-to-ml-models" class="nav-link" data-scroll-target="#threats-to-ml-models"><span class="header-section-number">15.5</span> Threats to ML Models</a>
  <ul>
  <li><a href="#model-theft" id="toc-model-theft" class="nav-link" data-scroll-target="#model-theft"><span class="header-section-number">15.5.1</span> Model Theft</a>
  <ul class="collapse">
  <li><a href="#exact-model-theft" id="toc-exact-model-theft" class="nav-link" data-scroll-target="#exact-model-theft">Exact Model Theft</a></li>
  <li><a href="#approximate-model-theft" id="toc-approximate-model-theft" class="nav-link" data-scroll-target="#approximate-model-theft">Approximate Model Theft</a></li>
  <li><a href="#case-study-tesla-ip-theft" id="toc-case-study-tesla-ip-theft" class="nav-link" data-scroll-target="#case-study-tesla-ip-theft">Case Study: Tesla IP Theft</a></li>
  </ul></li>
  <li><a href="#data-poisoning" id="toc-data-poisoning" class="nav-link" data-scroll-target="#data-poisoning"><span class="header-section-number">15.5.2</span> Data Poisoning</a></li>
  <li><a href="#adversarial-attacks" id="toc-adversarial-attacks" class="nav-link" data-scroll-target="#adversarial-attacks"><span class="header-section-number">15.5.3</span> Adversarial Attacks</a></li>
  <li><a href="#case-study-traffic-sign-detection-model-trickery" id="toc-case-study-traffic-sign-detection-model-trickery" class="nav-link" data-scroll-target="#case-study-traffic-sign-detection-model-trickery"><span class="header-section-number">15.5.4</span> Case Study: Traffic Sign Detection Model Trickery</a></li>
  </ul></li>
  <li><a href="#threats-to-ml-hardware" id="toc-threats-to-ml-hardware" class="nav-link" data-scroll-target="#threats-to-ml-hardware"><span class="header-section-number">15.6</span> Threats to ML Hardware</a>
  <ul>
  <li><a href="#hardware-bugs" id="toc-hardware-bugs" class="nav-link" data-scroll-target="#hardware-bugs"><span class="header-section-number">15.6.1</span> Hardware Bugs</a></li>
  <li><a href="#physical-attacks" id="toc-physical-attacks" class="nav-link" data-scroll-target="#physical-attacks"><span class="header-section-number">15.6.2</span> Physical Attacks</a></li>
  <li><a href="#fault-injection-attacks" id="toc-fault-injection-attacks" class="nav-link" data-scroll-target="#fault-injection-attacks"><span class="header-section-number">15.6.3</span> Fault Injection Attacks</a></li>
  <li><a href="#side-channel-attacks" id="toc-side-channel-attacks" class="nav-link" data-scroll-target="#side-channel-attacks"><span class="header-section-number">15.6.4</span> Side-Channel Attacks</a></li>
  <li><a href="#leaky-interfaces" id="toc-leaky-interfaces" class="nav-link" data-scroll-target="#leaky-interfaces"><span class="header-section-number">15.6.5</span> Leaky Interfaces</a></li>
  <li><a href="#counterfeit-hardware" id="toc-counterfeit-hardware" class="nav-link" data-scroll-target="#counterfeit-hardware"><span class="header-section-number">15.6.6</span> Counterfeit Hardware</a></li>
  <li><a href="#supply-chain-risks" id="toc-supply-chain-risks" class="nav-link" data-scroll-target="#supply-chain-risks"><span class="header-section-number">15.6.7</span> Supply Chain Risks</a></li>
  <li><a href="#case-study-the-supermicro-hardware-security-controversy" id="toc-case-study-the-supermicro-hardware-security-controversy" class="nav-link" data-scroll-target="#case-study-the-supermicro-hardware-security-controversy"><span class="header-section-number">15.6.8</span> Case Study: The Supermicro Hardware Security Controversy</a></li>
  </ul></li>
  <li><a href="#defensive-strategies" id="toc-defensive-strategies" class="nav-link" data-scroll-target="#defensive-strategies"><span class="header-section-number">15.7</span> Defensive Strategies</a>
  <ul>
  <li><a href="#data-privacy-techniques" id="toc-data-privacy-techniques" class="nav-link" data-scroll-target="#data-privacy-techniques"><span class="header-section-number">15.7.1</span> Data Privacy Techniques</a>
  <ul class="collapse">
  <li><a href="#differential-privacy" id="toc-differential-privacy" class="nav-link" data-scroll-target="#differential-privacy">Differential Privacy</a></li>
  <li><a href="#federated-learning" id="toc-federated-learning" class="nav-link" data-scroll-target="#federated-learning">Federated Learning</a></li>
  <li><a href="#synthetic-data-generation" id="toc-synthetic-data-generation" class="nav-link" data-scroll-target="#synthetic-data-generation">Synthetic Data Generation</a></li>
  <li><a href="#comparative-properties" id="toc-comparative-properties" class="nav-link" data-scroll-target="#comparative-properties">Comparative Properties</a></li>
  </ul></li>
  <li><a href="#secure-model-design" id="toc-secure-model-design" class="nav-link" data-scroll-target="#secure-model-design"><span class="header-section-number">15.7.2</span> Secure Model Design</a></li>
  <li><a href="#secure-model-deployment" id="toc-secure-model-deployment" class="nav-link" data-scroll-target="#secure-model-deployment"><span class="header-section-number">15.7.3</span> Secure Model Deployment</a></li>
  <li><a href="#system-level-monitoring" id="toc-system-level-monitoring" class="nav-link" data-scroll-target="#system-level-monitoring"><span class="header-section-number">15.7.4</span> System-level Monitoring</a>
  <ul class="collapse">
  <li><a href="#input-validation" id="toc-input-validation" class="nav-link" data-scroll-target="#input-validation">Input Validation</a></li>
  <li><a href="#output-monitoring" id="toc-output-monitoring" class="nav-link" data-scroll-target="#output-monitoring">Output Monitoring</a></li>
  <li><a href="#integrity-checks" id="toc-integrity-checks" class="nav-link" data-scroll-target="#integrity-checks">Integrity Checks</a></li>
  <li><a href="#response-and-rollback" id="toc-response-and-rollback" class="nav-link" data-scroll-target="#response-and-rollback">Response and Rollback</a></li>
  </ul></li>
  <li><a href="#hardware-based-security" id="toc-hardware-based-security" class="nav-link" data-scroll-target="#hardware-based-security"><span class="header-section-number">15.7.5</span> Hardware-based Security</a>
  <ul class="collapse">
  <li><a href="#trusted-execution-environments" id="toc-trusted-execution-environments" class="nav-link" data-scroll-target="#trusted-execution-environments">Trusted Execution Environments</a></li>
  <li><a href="#secure-boot" id="toc-secure-boot" class="nav-link" data-scroll-target="#secure-boot">Secure Boot</a></li>
  <li><a href="#hardware-security-modules" id="toc-hardware-security-modules" class="nav-link" data-scroll-target="#hardware-security-modules">Hardware Security Modules</a></li>
  <li><a href="#physical-unclonable-functions" id="toc-physical-unclonable-functions" class="nav-link" data-scroll-target="#physical-unclonable-functions">Physical Unclonable Functions</a></li>
  <li><a href="#mechanisms-comparison" id="toc-mechanisms-comparison" class="nav-link" data-scroll-target="#mechanisms-comparison">Mechanisms Comparison</a></li>
  </ul></li>
  <li><a href="#toward-trustworthy-systems" id="toc-toward-trustworthy-systems" class="nav-link" data-scroll-target="#toward-trustworthy-systems"><span class="header-section-number">15.7.6</span> Toward Trustworthy Systems</a></li>
  </ul></li>
  <li><a href="#offensive-capabilities" id="toc-offensive-capabilities" class="nav-link" data-scroll-target="#offensive-capabilities"><span class="header-section-number">15.8</span> Offensive Capabilities</a>
  <ul>
  <li><a href="#case-study-deep-learning-for-sca" id="toc-case-study-deep-learning-for-sca" class="nav-link" data-scroll-target="#case-study-deep-learning-for-sca"><span class="header-section-number">15.8.1</span> Case Study: Deep Learning for SCA</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">15.9</span> Conclusion</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources"><span class="header-section-number">15.10</span> Resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/widget_quiz/contents/core/privacy_security/privacy_security.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/widget_quiz/contents/core/privacy_security/privacy_security.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/png/cover_security_privacy.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="DALL¬∑E 3 Prompt: An illustration on privacy and security in machine learning systems. The image shows a digital landscape with a network of interconnected nodes and data streams, symbolizing machine learning algorithms. In the foreground, there‚Äôs a large lock superimposed over the network, representing privacy and security. The lock is semi-transparent, allowing the underlying network to be partially visible. The background features binary code and digital encryption symbols, emphasizing the theme of cybersecurity. The color scheme is a mix of blues, greens, and grays, suggesting a high-tech, digital environment."><img src="images/png/cover_security_privacy.png" class="img-fluid figure-img" alt="DALL¬∑E 3 Prompt: An illustration on privacy and security in machine learning systems. The image shows a digital landscape with a network of interconnected nodes and data streams, symbolizing machine learning algorithms. In the foreground, there‚Äôs a large lock superimposed over the network, representing privacy and security. The lock is semi-transparent, allowing the underlying network to be partially visible. The background features binary code and digital encryption symbols, emphasizing the theme of cybersecurity. The color scheme is a mix of blues, greens, and grays, suggesting a high-tech, digital environment."></a></p>
<figcaption><em>DALL¬∑E 3 Prompt: An illustration on privacy and security in machine learning systems. The image shows a digital landscape with a network of interconnected nodes and data streams, symbolizing machine learning algorithms. In the foreground, there‚Äôs a large lock superimposed over the network, representing privacy and security. The lock is semi-transparent, allowing the underlying network to be partially visible. The background features binary code and digital encryption symbols, emphasizing the theme of cybersecurity. The color scheme is a mix of blues, greens, and grays, suggesting a high-tech, digital environment.</em></figcaption>
</figure>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>What principles guide the protection of machine learning systems, and how do security and privacy requirements shape system architecture?</em></p>
<p>Protection mechanisms are a fundamental dimension of modern AI system design. Security considerations expose critical patterns for safeguarding data, models, and infrastructure while sustaining operational effectiveness. Implementing defensive strategies reveals inherent trade-offs between protection, performance, and usability‚Äîtrade-offs that influence architectural decisions throughout the AI lifecycle. Understanding these dynamics is essential for creating trustworthy systems, grounding the principles needed to preserve privacy and defend against adversarial threats while maintaining functionality in production environments.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Identify key security and privacy risks in machine learning systems.</p></li>
<li><p>Understand how to design models with security and privacy in mind.</p></li>
<li><p>Describe methods for securing model deployment and access.</p></li>
<li><p>Explain strategies for monitoring and defending systems at runtime.</p></li>
<li><p>Recognize the role of hardware in building trusted ML infrastructure.</p></li>
<li><p>Apply a layered approach to defending machine learning systems.</p></li>
</ul>
</div>
</div>
</section>
<section id="overview" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">15.1</span> Overview</h2>
<p>Machine learning systems, like all computational systems, must be designed not only for performance and accuracy but also for security and privacy. These concerns shape the architecture and operation of ML systems across their lifecycle‚Äîfrom data collection and model training to deployment and user interaction. While traditional system security focuses on software vulnerabilities, network protocols, and hardware defenses, machine learning systems introduce additional and unique attack surfaces. These include threats to the data that fuels learning, the models that encode behavior, and the infrastructure that serves predictions.</p>
<p>Security and privacy mechanisms in ML systems serve roles analogous to trust and access control layers in classical computing. Just as operating systems enforce user permissions and protect resource boundaries, ML systems must implement controls that safeguard sensitive data, defend proprietary models, and mitigate adversarial manipulation. These mechanisms span software, hardware, and organizational layers, forming a critical foundation for system reliability and trustworthiness.</p>
<p>Although closely related, security and privacy address distinct aspects of protection. Security focuses on ensuring system integrity and availability in the presence of adversaries. Privacy, by contrast, emphasizes the control and protection of sensitive information, even in the absence of active attacks. These concepts often interact, but they are not interchangeable. To effectively design and evaluate defenses for ML systems, it is essential to understand how these goals differ, how they reinforce one another, and what distinct mechanisms they entail.</p>
<p>Security and privacy often function as complementary forces. Security prevents unauthorized access and protects system behavior, while privacy measures limit the exposure of sensitive information. Their synergy is essential: strong security supports privacy by preventing data breaches, while privacy-preserving techniques reduce the attack surface available to adversaries. However, achieving robust protection on both fronts often introduces trade-offs. Defensive mechanisms may incur computational overhead, increase system complexity, or impact usability. Designers must carefully balance these costs against protection goals, guided by an understanding of threats, system constraints, and risk tolerance.</p>
<p>The landscape of security and privacy challenges in ML systems continues to evolve. High-profile incidents such as model extraction attacks, data leakage from generative models, and hardware-level vulnerabilities have underscored the need for comprehensive and adaptive defenses. These solutions must address not only technical threats, but also regulatory, ethical, and operational requirements across cloud, edge, and embedded deployments.</p>
<p>As this chapter progresses, we will examine the threats facing machine learning systems, the defensive strategies available, and the trade-offs involved in deploying them in practice. A clear understanding of these principles is essential for building trustworthy systems that operate reliably in adversarial and privacy-sensitive environments.</p>
</section>
<section id="definitions-and-distinctions" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="definitions-and-distinctions"><span class="header-section-number">15.2</span> Definitions and Distinctions</h2>
<p>Security and privacy are core concerns in machine learning system design, but they are often misunderstood or conflated. While both aim to protect systems and data, they do so in different ways, address different threat models, and require distinct technical responses. For ML systems, clearly distinguishing between the two helps guide the design of robust and responsible infrastructure.</p>
<section id="security-defined" class="level3" data-number="15.2.1">
<h3 data-number="15.2.1" class="anchored" data-anchor-id="security-defined"><span class="header-section-number">15.2.1</span> Security Defined</h3>
<p>Security in machine learning focuses on defending systems from adversarial behavior. This includes protecting model parameters, training pipelines, deployment infrastructure, and data access pathways from manipulation or misuse.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Security Definition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Security Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Security</strong> in machine learning systems is the <em>protection of data, models, and infrastructure</em> from unauthorized access, manipulation, or disruption. It spans the <em>design and implementation</em> of defensive mechanisms that protect against data poisoning, model theft, adversarial manipulation, and system-level vulnerabilities. Security mechanisms ensure the <em>integrity</em>, <em>confidentiality</em>, and <em>availability</em> of machine learning services across development, deployment, and operational environments.</p>
</div>
</div>
<p><em>Example</em>: A facial recognition system deployed in public transit infrastructure may be targeted with adversarial inputs that cause it to misidentify individuals or fail entirely. This is a runtime security vulnerability that threatens both accuracy and system availability.</p>
</section>
<section id="privacy-defined" class="level3" data-number="15.2.2">
<h3 data-number="15.2.2" class="anchored" data-anchor-id="privacy-defined"><span class="header-section-number">15.2.2</span> Privacy Defined</h3>
<p>Privacy focuses on limiting the exposure and misuse of sensitive information within ML systems. This includes protecting training data, inference inputs, and model outputs from leaking personal or proprietary information‚Äîeven when systems operate correctly and no explicit attack is taking place.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Privacy Definition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Privacy Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Privacy</strong> in machine learning systems is the <em>protection of sensitive information</em> from unauthorized disclosure, inference, or misuse. It spans the <em>design and implementation</em> of methods that reduce the risk of exposing personal, proprietary, or regulated data while enabling machine learning systems to operate effectively. Privacy mechanisms help preserve <em>confidentiality</em> and <em>control</em> over data usage across development, deployment, and operational environments.</p>
</div>
</div>
<p><em>Example</em>: A language model trained on medical transcripts may inadvertently memorize snippets of patient conversations. If a user later triggers this content through a public-facing chatbot, it represents a privacy failure‚Äîeven in the absence of an attacker.</p>
</section>
<section id="security-versus-privacy" class="level3" data-number="15.2.3">
<h3 data-number="15.2.3" class="anchored" data-anchor-id="security-versus-privacy"><span class="header-section-number">15.2.3</span> Security versus Privacy</h3>
<p>Although they intersect in some areas (e.g., encrypted storage supports both), security and privacy differ in their objectives, threat models, and typical mitigation strategies. <a href="#tbl-security-privacy-comparison" class="quarto-xref">Table&nbsp;<span>15.1</span></a> below summarizes these distinctions in the context of machine learning systems.</p>
<div id="tbl-security-privacy-comparison" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-security-privacy-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.1: How security and privacy concerns manifest differently in machine learning systems. Security focuses on protecting against active threats that seek to manipulate or disrupt system behavior, while privacy emphasizes safeguarding sensitive information from exposure, even in benign operational contexts.
</figcaption>
<div aria-describedby="tbl-security-privacy-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 36%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Security</th>
<th style="text-align: left;">Privacy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Primary Goal</td>
<td style="text-align: left;">Prevent unauthorized access or disruption</td>
<td style="text-align: left;">Limit exposure of sensitive information</td>
</tr>
<tr class="even">
<td style="text-align: left;">Threat Model</td>
<td style="text-align: left;">Adversarial actors (external or internal)</td>
<td style="text-align: left;">Honest-but-curious observers or passive leaks</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Typical Concerns</td>
<td style="text-align: left;">Model theft, poisoning, evasion attacks</td>
<td style="text-align: left;">Data leakage, re-identification, memorization</td>
</tr>
<tr class="even">
<td style="text-align: left;">Example Attack</td>
<td style="text-align: left;">Adversarial inputs cause misclassification</td>
<td style="text-align: left;">Model inversion reveals training data</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Representative Defenses</td>
<td style="text-align: left;">Access control, adversarial training</td>
<td style="text-align: left;">Differential privacy, federated learning</td>
</tr>
<tr class="even">
<td style="text-align: left;">Relevance to Regulation</td>
<td style="text-align: left;">Emphasized in cybersecurity standards</td>
<td style="text-align: left;">Central to data protection laws (e.g., GDPR)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="interactions-and-trade-offs" class="level3" data-number="15.2.4">
<h3 data-number="15.2.4" class="anchored" data-anchor-id="interactions-and-trade-offs"><span class="header-section-number">15.2.4</span> Interactions and Trade-offs</h3>
<p>Security and privacy are deeply interrelated but not interchangeable. A secure system helps maintain privacy by restricting unauthorized access to models and data. At the same time, privacy-preserving designs can improve security by reducing the attack surface‚Äîe.g., minimizing the retention of sensitive data reduces the risk of exposure if a system is compromised.</p>
<p>However, they can also be in tension. Techniques like differential privacy reduce memorization risks but may lower model utility. Encryption enhances security but may obscure transparency and auditability, complicating privacy compliance.</p>
<p>In machine learning systems, designers must reason about these trade-offs holistically. Systems that serve sensitive domains, including healthcare, finance, and public safety, must simultaneously protect against both misuse (security) and overexposure (privacy). Understanding the boundaries between these concerns is key to building systems that are not only performant, but trustworthy and legally compliant.</p>
</section>
</section>
<section id="historical-incidents" class="level2 page-columns page-full" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="historical-incidents"><span class="header-section-number">15.3</span> Historical Incidents</h2>
<p>While the security of machine learning systems introduces new technical challenges, valuable lessons can be drawn from well-known security breaches across a range of computing systems. These incidents demonstrate how weaknesses in system design, in industrial control systems, connected vehicles, or consumer devices, can lead to widespread, and sometimes physical, consequences. Although the examples discussed in this section do not all involve machine learning directly, they provide critical insights into the importance of designing secure systems. These lessons apply broadly to machine learning applications deployed across cloud, edge, and embedded environments.</p>
<section id="stuxnet" class="level3 page-columns page-full" data-number="15.3.1">
<h3 data-number="15.3.1" class="anchored" data-anchor-id="stuxnet"><span class="header-section-number">15.3.1</span> Stuxnet</h3>
<p>In 2010, security researchers discovered a highly sophisticated computer worm later named <a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf">Stuxnet</a>, which targeted industrial control systems used in Iran‚Äôs Natanz nuclear facility <span class="citation" data-cites="farwell2011stuxnet">(<a href="../references.html#ref-farwell2011stuxnet" role="doc-biblioref">Farwell and Rohozinski 2011</a>)</span>. Stuxnet exploited four previously unknown ‚Äú<a href="https://en.wikipedia.org/wiki/Zero-day_%28computing%29">zero-day</a>‚Äù vulnerabilities in Microsoft Windows, allowing it to spread undetected through both networked and isolated systems.</p>
<div class="no-row-height column-margin column-container"></div><p>Unlike typical malware designed to steal information or perform espionage, Stuxnet was engineered to cause physical damage. Its objective was to disrupt uranium enrichment by sabotaging the centrifuges used in the process. Despite the facility being air-gapped from external networks, the malware is believed to have entered the system via an infected USB device, demonstrating how physical access can compromise even isolated environments.</p>
<p>Stuxnet represents a landmark in cybersecurity, revealing how malicious software can bridge the digital and physical worlds to manipulate industrial infrastructure. It specifically targeted programmable logic controllers (PLCs) responsible for automating electromechanical processes, such as controlling the speed of centrifuges. By exploiting vulnerabilities in the Windows operating system and the Siemens Step7 software used to program the PLCs, Stuxnet achieved highly targeted, real-world disruption.</p>
<p>While Stuxnet did not target machine learning systems directly, its relevance extends to any system where software interacts with physical processes. Machine learning is increasingly integrated into industrial control, robotics, and cyber-physical systems, making these lessons applicable to the security of modern ML deployments. <a href="#fig-stuxnet" class="quarto-xref">Figure&nbsp;<span>15.1</span></a> illustrates the operation of Stuxnet in greater detail.</p>
<div id="fig-stuxnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stuxnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/stuxnet.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;15.1: Stuxnet explained. Source: IEEE Spectrum"><img src="images/png/stuxnet.png" class="img-fluid figure-img" style="width:90.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stuxnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.1: Stuxnet explained. Source: IEEE Spectrum
</figcaption>
</figure>
</div>
</section>
<section id="jeep-cherokee-hack" class="level3 page-columns page-full" data-number="15.3.2">
<h3 data-number="15.3.2" class="anchored" data-anchor-id="jeep-cherokee-hack"><span class="header-section-number">15.3.2</span> Jeep Cherokee Hack</h3>
<p>In 2015, security researchers publicly demonstrated a remote cyberattack on a Jeep Cherokee that exposed critical vulnerabilities in automotive system design <span class="citation" data-cites="miller2015remote miller2019lessons">(<a href="../references.html#ref-miller2015remote" role="doc-biblioref">Miller and Valasek 2015</a>; <a href="../references.html#ref-miller2019lessons" role="doc-biblioref">Miller 2019</a>)</span>. Conducted as a controlled experiment, the researchers exploited a vulnerability in the vehicle‚Äôs Uconnect entertainment system, which was connected to the internet via a cellular network. By gaining remote access to this system, they were able to send commands that affected the vehicle‚Äôs engine, transmission, and braking systems‚Äîwithout physical access to the car.</p>
<div class="no-row-height column-margin column-container"><div id="ref-miller2015remote" class="csl-entry" role="listitem">
Miller, Charlie, and Chris Valasek. 2015. <span>‚ÄúThe Antivirus Hacker‚Äôs Handbook.‚Äù</span> <em>Black Hat USA</em>. Wiley. <a href="https://doi.org/10.1002/9781119183525.ch15">https://doi.org/10.1002/9781119183525.ch15</a>.
</div><div id="ref-miller2019lessons" class="csl-entry" role="listitem">
Miller, Charlie. 2019. <span>‚ÄúLessons Learned from Hacking a Car.‚Äù</span> <em>IEEE Design &amp;Amp; Test</em> 36 (6): 7‚Äì9. <a href="https://doi.org/10.1109/mdat.2018.2863106">https://doi.org/10.1109/mdat.2018.2863106</a>.
</div></div><p>This demonstration served as a wake-up call for the automotive industry. It highlighted the risks posed by the growing connectivity of modern vehicles. Traditionally isolated automotive control systems, such as those managing steering and braking, were shown to be vulnerable when exposed through externally accessible software interfaces. The ability to remotely manipulate safety-critical functions raised serious concerns about passenger safety, regulatory oversight, and industry best practices.</p>
<div id="vid-jeephack" class="callout callout-style-default callout-important callout-titled" title="Jeep Cherokee Hack">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important&nbsp;15.1: Jeep Cherokee Hack
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/MK0SrxBC1xs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
<p>The incident also led to a recall of over 1.4 million vehicles to patch the vulnerability, highlighting the need for manufacturers to prioritize cybersecurity in their designs. The National Highway Traffic Safety Administration (NHTSA) issued guidelines for automakers to improve vehicle cybersecurity, including recommendations for secure software development practices and incident response protocols.</p>
<p>The automotive industry has since made significant strides in addressing these vulnerabilities, but the incident serves as a cautionary tale for all sectors that rely on connected systems. As machine learning becomes more prevalent in safety-critical applications, the lessons learned from the Jeep Cherokee hack will be essential for ensuring the security and reliability of future ML deployments.</p>
<p>Although this incident did not involve machine learning, the architectural patterns it exposed are highly relevant to ML system security. Modern vehicles increasingly rely on machine learning for driver-assistance, navigation, and in-cabin intelligence, which include features that operate in conjunction with connected software services. This integration expands the potential attack surface if systems are not properly isolated or secured. The Jeep Cherokee hack highlights the need for defense-in-depth strategies, secure software updates, authenticated communications, and rigorous security testing‚Äîprinciples that apply broadly to machine learning systems deployed across automotive, industrial, and consumer environments.</p>
<p>As machine learning continues to be integrated into connected and safety-critical applications, the lessons from the Jeep Cherokee hack remain highly relevant. They emphasize that securing externally connected software is not just a best practice but a necessity for protecting the integrity and safety of machine learning-enabled systems.</p>
</section>
<section id="mirai-botnet" class="level3 page-columns page-full" data-number="15.3.3">
<h3 data-number="15.3.3" class="anchored" data-anchor-id="mirai-botnet"><span class="header-section-number">15.3.3</span> Mirai Botnet</h3>
<p>In 2016, the <a href="https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/">Mirai botnet</a> emerged as one of the most disruptive distributed denial-of-service (DDoS) attacks in internet history <span class="citation" data-cites="antonakakis2017understanding">(<a href="../references.html#ref-antonakakis2017understanding" role="doc-biblioref">Antonakakis et al. 2017</a>)</span>. The botnet infected thousands of networked devices, including digital cameras, DVRs, and other consumer electronics. These devices, often deployed with factory-default usernames and passwords, were easily compromised by the Mirai malware and enlisted into a large-scale attack network.</p>
<div class="no-row-height column-margin column-container"></div><p>The Mirai botnet was used to overwhelm major internet infrastructure providers, disrupting access to popular online services across the United States and beyond. The scale of the attack demonstrated how vulnerable consumer and industrial devices can become a platform for widespread disruption when security is not prioritized in their design and deployment.</p>
<div id="vid-mirai" class="callout callout-style-default callout-important callout-titled" title="Mirai Botnet">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important&nbsp;15.2: Mirai Botnet
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/1pywzRTJDaY" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
<p>While the devices exploited by Mirai did not include machine learning components, the architectural patterns exposed by this incident are increasingly relevant as machine learning expands into edge computing and Internet of Things (IoT) devices. Many ML-enabled products, such as smart cameras, voice assistants, and edge analytics platforms, share similar deployment characteristics‚Äîoperating on networked devices with limited hardware resources, often managed at scale.</p>
<p>The Mirai botnet highlights the critical importance of basic security hygiene, including secure credential management, authenticated software updates, and network access control. Without these protections, even powerful machine learning models can become part of larger attack infrastructures if deployed on insecure hardware.</p>
<p>As machine learning continues to move beyond centralized data centers into distributed and networked environments, the lessons from the Mirai botnet remain highly relevant. They emphasize the need for secure device provisioning, ongoing vulnerability management, and industry-wide coordination to prevent large-scale exploitation of ML-enabled systems.</p>
</section>
</section>
<section id="secure-design-priorities" class="level2 page-columns page-full" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="secure-design-priorities"><span class="header-section-number">15.4</span> Secure Design Priorities</h2>
<p>The historical breaches described earlier reveal how weaknesses in system design, whether in hardware, software, or network infrastructure, can lead to widespread and often physical consequences. While these incidents did not directly target machine learning systems, they offer valuable insights into architectural and operational patterns that increasingly characterize modern ML deployments. These lessons point to three overarching areas of concern: device-level security, system-level isolation and control, and protection against large-scale network exploitation.</p>
<section id="device-level-security" class="level3 page-columns page-full" data-number="15.4.1">
<h3 data-number="15.4.1" class="anchored" data-anchor-id="device-level-security"><span class="header-section-number">15.4.1</span> Device-Level Security</h3>
<p>The Mirai botnet exemplifies how large-scale exploitation of poorly secured devices can lead to significant disruption. This attack succeeded by exploiting common weaknesses such as default usernames and passwords, unsecured firmware update mechanisms, and unencrypted communications. While often associated with consumer-grade IoT products, these vulnerabilities are increasingly relevant to machine learning systems, particularly those deployed at the edge <span class="citation" data-cites="antonakakis2017understanding">(<a href="../references.html#ref-antonakakis2017understanding" role="doc-biblioref">Antonakakis et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-antonakakis2017understanding" class="csl-entry" role="listitem">
Antonakakis, Manos, Tim April, Michael Bailey, Matt Bernhard, Elie Bursztein, Jaime Cochran, Zakir Durumeric, et al. 2017. <span>‚ÄúUnderstanding the Mirai Botnet.‚Äù</span> In <em>26th USENIX Security Symposium (USENIX Security 17)</em>, 16:1093‚Äì1110.
</div></div><p>Edge ML devices, including smart cameras, industrial controllers, and wearable health monitors, typically rely on lightweight embedded hardware like ARM-based processors running minimal operating systems. These systems are designed for low-power, distributed operation but often lack the comprehensive security features found in larger computing platforms. As these devices take on more responsibility for local data processing and real-time decision-making, they become attractive targets for remote compromise.</p>
<p>A compromised population of such devices can be aggregated into a botnet, similar to Mirai, and leveraged for large-scale attacks. Beyond denial-of-service threats, attackers could use these ML-enabled devices to exfiltrate sensitive data, interfere with model execution, or manipulate system outputs. Without strong device-level protections, which include secure boot processes, authenticated firmware updates, and encrypted communications, edge ML deployments remain vulnerable to being turned into platforms for broader system disruption.</p>
</section>
<section id="system-level-isolation" class="level3" data-number="15.4.2">
<h3 data-number="15.4.2" class="anchored" data-anchor-id="system-level-isolation"><span class="header-section-number">15.4.2</span> System-Level Isolation</h3>
<p>The Jeep Cherokee hack highlighted the risks that arise when externally connected software services are insufficiently isolated from safety-critical system functions. By exploiting a vulnerability in the vehicle‚Äôs Uconnect entertainment system, researchers were able to remotely manipulate core control functions such as steering and braking. This incident demonstrated that network connectivity, if not carefully managed, can expose critical system pathways to external threats.</p>
<p>Machine learning systems increasingly operate in similar contexts, particularly in domains such as automotive safety, healthcare, and industrial automation. Modern vehicles, for example, integrate machine learning models for driver-assistance, autonomous navigation, and sensor fusion. These models run alongside connected software services that provide infotainment, navigation updates, and remote diagnostics. Without strong system-level isolation, attackers can exploit these externally facing services to gain access to safety-critical ML components, expanding the overall attack surface.</p>
<p>The automotive industry‚Äôs response to the Jeep Cherokee incident, which includes large-scale recalls, over-the-air software patches, and the development of industry-wide cybersecurity standards through organizations such as Auto-ISAC and the National Highway Traffic Safety Administration (NHTSA), provides a valuable example of how industries can address emerging ML security risks.</p>
<p>Similar isolation principles apply to other machine learning deployments, including medical devices that analyze patient data in real time, industrial controllers that optimize manufacturing processes, and infrastructure systems that manage power grids or water supplies. Securing these systems requires architectural compartmentalization of subsystems, authenticated communication channels, and validated update mechanisms. These measures help prevent external actors from escalating access or manipulating ML-driven decision-making in safety-critical environments.</p>
</section>
<section id="large-scale-network-exploitation" class="level3 page-columns page-full" data-number="15.4.3">
<h3 data-number="15.4.3" class="anchored" data-anchor-id="large-scale-network-exploitation"><span class="header-section-number">15.4.3</span> Large-Scale Network Exploitation</h3>
<p>The Stuxnet attack demonstrated the ability of targeted cyber operations to cross from digital systems into the physical world, resulting in real-world disruption and damage. By exploiting software vulnerabilities in industrial control systems, the attack caused mechanical failures in uranium enrichment equipment <span class="citation" data-cites="farwell2011stuxnet">(<a href="../references.html#ref-farwell2011stuxnet" role="doc-biblioref">Farwell and Rohozinski 2011</a>)</span>. While Stuxnet did not target machine learning systems directly, it revealed critical risks that apply broadly to cyber-physical systems‚Äîparticularly those involving supply chain vulnerabilities, undisclosed (zero-day) exploits, and techniques for bypassing network isolation, such as air gaps.</p>
<div class="no-row-height column-margin column-container"><div id="ref-farwell2011stuxnet" class="csl-entry" role="listitem">
Farwell, James P., and Rafal Rohozinski. 2011. <span>‚ÄúStuxnet and the Future of Cyber War.‚Äù</span> <em>Survival</em> 53 (1): 23‚Äì40. <a href="https://doi.org/10.1080/00396338.2011.555586">https://doi.org/10.1080/00396338.2011.555586</a>.
</div></div><p>As machine learning increasingly powers decision-making in manufacturing, energy management, robotics, and other operational technologies, similar risks emerge. ML-based controllers that influence physical processes, including adjusting production lines, managing industrial robots, and optimizing power distribution, represent new attack surfaces. Compromising these models or the systems that deploy them can result in physical harm, operational disruption, or strategic manipulation of critical infrastructure.</p>
<p>Stuxnet‚Äôs sophistication highlights the potential for state-sponsored or well-resourced adversaries to target ML-driven systems as part of larger geopolitical or economic campaigns. As machine learning takes on more influential roles in controlling real-world systems, securing these deployments against both cyber and physical threats becomes essential for ensuring operational resilience and public safety.</p>
</section>
<section id="toward-secure-design" class="level3" data-number="15.4.4">
<h3 data-number="15.4.4" class="anchored" data-anchor-id="toward-secure-design"><span class="header-section-number">15.4.4</span> Toward Secure Design</h3>
<p>Collectively, these incidents illustrate that security must be designed into machine learning systems from the outset. Protecting such systems requires attention to multiple layers of the stack, including model-level protections to defend against attacks such as model theft, adversarial manipulation, and data leakage; data pipeline security to ensure the confidentiality, integrity, and governance of training and inference data across cloud, edge, and embedded environments; system-level isolation and access control to prevent external interfaces from compromising model execution or manipulating safety-critical outputs; secure deployment and update mechanisms to safeguard runtime environments from tampering or exploitation; and continuous monitoring and incident response capabilities to detect and recover from breaches in dynamic, distributed deployments.</p>
<p>These priorities reflect the lessons drawn from past incidents‚Äîemphasizing the need to protect device-level resources, isolate critical system functions, and defend against large-scale exploitation. The remainder of this chapter builds on these principles, beginning with a closer examination of threats specific to machine learning models and data. It then expands the discussion to hardware-level vulnerabilities and the unique considerations of embedded ML systems. Finally, it explores defensive strategies, including privacy-preserving techniques, secure hardware mechanisms, and system-level design practices, forming a foundation for building trustworthy machine learning systems capable of withstanding both known and emerging threats.</p>
</section>
</section>
<section id="threats-to-ml-models" class="level2 page-columns page-full" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="threats-to-ml-models"><span class="header-section-number">15.5</span> Threats to ML Models</h2>
<p>Building on the lessons from historical security incidents, we now turn to threats that are specific to machine learning models. These threats span the entire ML lifecycle, ranging from training-time manipulations to inference-time evasion, and fall into three broad categories: threats to model confidentiality (e.g., model theft), threats to training integrity (e.g., data poisoning), and threats to inference robustness (e.g., adversarial examples). Each category targets different vulnerabilities and requires distinct defensive strategies.</p>
<p>Three primary threats stand out in this context: model theft, where adversaries steal proprietary models and the sensitive knowledge they encode; data poisoning, where attackers manipulate training data to corrupt model behavior; and adversarial attacks, where carefully crafted inputs deceive models into making incorrect predictions. Each of these threats exploits different stages of the machine learning lifecycle‚Äîfrom data ingestion and model training to deployment and inference.</p>
<p>We begin with model theft, examining how attackers extract or replicate models to undermine economic value and privacy. As shown in <a href="#fig-ml-lifecycle-threats" class="quarto-xref">Figure&nbsp;<span>15.2</span></a>, model theft typically targets the deployment stage of the machine learning lifecycle, where trained models are exposed through APIs, on-device engines, or serialized files. This threat sits alongside others, including data poisoning during training and adversarial attacks during inference, that together span the full pipeline from data collection to real-time prediction. Understanding the lifecycle positioning of each threat helps clarify their distinct attack surfaces and appropriate defenses.</p>
<div id="fig-ml-lifecycle-threats" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-lifecycle-threats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="dc7de53105e4ffea78ab15ce8f27f25206245923.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;15.2: Key security and privacy threats mapped to stages of the machine learning lifecycle."><img src="privacy_security_files/mediabag/dc7de53105e4ffea78ab15ce8f27f25206245923.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-lifecycle-threats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.2: Key security and privacy threats mapped to stages of the machine learning lifecycle.
</figcaption>
</figure>
</div>
<p>Machine learning models are not solely passive targets of attack; in some cases, they can themselves be employed as components of an attack strategy. Pretrained models, particularly large generative or discriminative networks, may be adapted to automate tasks such as adversarial example generation, phishing content synthesis, or protocol subversion. Furthermore, open-source or publicly accessible models can be fine-tuned for malicious purposes, including impersonation, surveillance, or reverse-engineering of secure systems. This dual-use potential necessitates a broader security perspective‚Äîone that considers models not only as assets to defend but also as possible instruments of attack.</p>
<section id="model-theft" class="level3 page-columns page-full" data-number="15.5.1">
<h3 data-number="15.5.1" class="anchored" data-anchor-id="model-theft"><span class="header-section-number">15.5.1</span> Model Theft</h3>
<p>Threats to model confidentiality arise when adversaries gain access to a trained model‚Äôs parameters, architecture, or output behavior. These attacks can undermine the economic value of machine learning systems, enable competitors to replicate proprietary functionality, or expose private information encoded in model weights.</p>
<p>Such threats arise across a range of deployment settings, including public APIs, cloud-hosted services, on-device inference engines, and shared model repositories. Machine learning models may be vulnerable due to exposed interfaces, insecure serialization formats, or insufficient access controls‚Äîfactors that create opportunities for unauthorized extraction or replication <span class="citation" data-cites="ateniese2015hacking">(<a href="../references.html#ref-ateniese2015hacking" role="doc-biblioref">Ateniese et al. 2015</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ateniese2015hacking" class="csl-entry" role="listitem">
Ateniese, Giuseppe, Luigi V. Mancini, Angelo Spognardi, Antonio Villani, Domenico Vitali, and Giovanni Felici. 2015. <span>‚ÄúHacking Smart Machines with Smarter Ones: How to Extract Meaningful Data from Machine Learning Classifiers.‚Äù</span> <em>International Journal of Security and Networks</em> 10 (3): 137. <a href="https://doi.org/10.1504/ijsn.2015.071829">https://doi.org/10.1504/ijsn.2015.071829</a>.
</div></div><p>High-profile legal cases have highlighted the strategic and economic value of machine learning models. For example, former Google engineer Anthony Levandowski was accused of <a href="https://www.nytimes.com/2017/02/23/technology/google-self-driving-waymo-uber-otto-lawsuit.html">stealing proprietary designs from Waymo</a>, including critical components of its autonomous vehicle technology, before founding a competing startup. Such cases illustrate the potential for insider threats to bypass technical protections and gain access to sensitive intellectual property.</p>
<p>The consequences of model theft extend beyond economic loss. Stolen models can be used to extract sensitive information, replicate proprietary algorithms, or enable further attacks. For instance, a competitor who obtains a stolen recommendation model from an e-commerce platform might gain insights into customer behavior, business analytics, and embedded trade secrets. This knowledge can also be used to conduct model inversion attacks, where an attacker attempts to infer private details about the model‚Äôs training data <span class="citation" data-cites="fredrikson2015model">(<a href="../references.html#ref-fredrikson2015model" role="doc-biblioref">Fredrikson, Jha, and Ristenpart 2015</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-fredrikson2015model" class="csl-entry" role="listitem">
Fredrikson, Matt, Somesh Jha, and Thomas Ristenpart. 2015. <span>‚ÄúModel Inversion Attacks That Exploit Confidence Information and Basic Countermeasures.‚Äù</span> In <em>Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security</em>, 1322‚Äì33. ACM. <a href="https://doi.org/10.1145/2810103.2813677">https://doi.org/10.1145/2810103.2813677</a>.
</div><div id="ref-narayanan2006break" class="csl-entry" role="listitem">
Narayanan, Arvind, and Vitaly Shmatikov. 2006. <span>‚ÄúHow to Break Anonymity of the Netflix Prize Dataset.‚Äù</span> <em>CoRR</em>. <a href="http://arxiv.org/abs/cs/0610105">http://arxiv.org/abs/cs/0610105</a>.
</div></div><p>In a model inversion attack, the adversary queries the model through a legitimate interface, such as a public API, and observes its outputs. By analyzing confidence scores or output probabilities, the attacker can optimize inputs to reconstruct data resembling the model‚Äôs training set. For example, a facial recognition model used for secure access could be manipulated to reveal statistical properties of the employee photos on which it was trained. Similar vulnerabilities have been demonstrated in studies on the Netflix Prize dataset, where researchers were able to infer individual movie preferences from anonymized data <span class="citation" data-cites="narayanan2006break">(<a href="../references.html#ref-narayanan2006break" role="doc-biblioref">Narayanan and Shmatikov 2006</a>)</span>.</p>
<p>Model theft can target two distinct objectives: extracting exact model properties, such as architecture and parameters, or replicating approximate model behavior to produce similar outputs without direct access to internal representations. Both forms of theft undermine the security and value of machine learning systems, as explored in the following subsections.</p>
<p>These two attack paths are illustrated in <a href="#fig-model-theft-types" class="quarto-xref">Figure&nbsp;<span>15.3</span></a>. In exact model theft, the attacker gains access to the model‚Äôs internal components, including serialized files, weights, and architecture definitions, and reproduces the model directly. In contrast, approximate model theft relies on observing the model‚Äôs input-output behavior, typically through a public API. By repeatedly querying the model and collecting responses, the attacker trains a surrogate that mimics the original model‚Äôs functionality. While the first approach compromises the model‚Äôs internal design and training investment, the second threatens its predictive value and can facilitate further attacks such as adversarial example transfer or model inversion.</p>
<div id="fig-model-theft-types" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model-theft-types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="035d7acaf39ba820999005867951492c47e3dd5c.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;15.3: Two primary model theft strategies: extracting internal components versus replicating external behavior."><img src="privacy_security_files/mediabag/035d7acaf39ba820999005867951492c47e3dd5c.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-theft-types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.3: Two primary model theft strategies: extracting internal components versus replicating external behavior.
</figcaption>
</figure>
</div>
<section id="exact-model-theft" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="exact-model-theft">Exact Model Theft</h4>
<p>Exact model property theft refers to attacks aimed at extracting the internal structure and learned parameters of a machine learning model. These attacks often target deployed models that are exposed through APIs, embedded in on-device inference engines, or shared as downloadable model files on collaboration platforms. Exploiting weak access control, insecure model packaging, or unprotected deployment interfaces, attackers can recover proprietary model assets without requiring full control of the underlying infrastructure.</p>
<p>These attacks typically seek three types of information. The first is the model‚Äôs learned parameters, such as weights and biases. By extracting these parameters, attackers can replicate the model‚Äôs functionality without incurring the cost of training. This replication allows them to benefit from the model‚Äôs performance while bypassing the original development effort.</p>
<p>The second target is the model‚Äôs fine-tuned hyperparameters, including training configurations such as learning rate, batch size, and regularization settings. These hyperparameters significantly influence model performance, and stealing them enables attackers to reproduce high-quality results with minimal additional experimentation.</p>
<p>Finally, attackers may seek to reconstruct the model‚Äôs architecture. This includes the sequence and types of layers, activation functions, and connectivity patterns that define the model‚Äôs behavior. Architecture theft may be accomplished through side-channel attacks, reverse engineering, or analysis of observable model behavior. Revealing the architecture not only compromises intellectual property but also gives competitors strategic insights into the design choices that provide competitive advantage.</p>
<p>System designers must account for these risks by securing model serialization formats, restricting access to runtime APIs, and hardening deployment pipelines. Protecting models requires a combination of software engineering practices, including access control, encryption, and obfuscation techniques, to reduce the risk of unauthorized extraction <span class="citation" data-cites="tramer2016stealing">(<a href="../references.html#ref-tramer2016stealing" role="doc-biblioref">Tram√®r et al. 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tramer2016stealing" class="csl-entry" role="listitem">
Tram√®r, Florian, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. 2016. <span>‚ÄúStealing Machine Learning Models via Prediction APIs.‚Äù</span> In <em>25th USENIX Security Symposium (USENIX Security 16)</em>, 601‚Äì18.
</div></div></section>
<section id="approximate-model-theft" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="approximate-model-theft">Approximate Model Theft</h4>
<p>While some attackers seek to extract a model‚Äôs exact internal properties, others focus on replicating its external behavior. Approximate model behavior theft refers to attacks that attempt to recreate a model‚Äôs decision-making capabilities without directly accessing its parameters or architecture. Instead, attackers observe the model‚Äôs inputs and outputs to build a substitute model that performs similarly on the same tasks.</p>
<p>This type of theft often targets models deployed as services, where the model is exposed through an API or embedded in a user-facing application. By repeatedly querying the model and recording its responses, an attacker can train their own model to mimic the behavior of the original. This process, often called model distillation or knockoff modeling, enables attackers to achieve comparable functionality without access to the original model‚Äôs proprietary internals <span class="citation" data-cites="orekondy2019knockoff">(<a href="../references.html#ref-orekondy2019knockoff" role="doc-biblioref">Orekondy, Schiele, and Fritz 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-orekondy2019knockoff" class="csl-entry" role="listitem">
Orekondy, Tribhuvanesh, Bernt Schiele, and Mario Fritz. 2019. <span>‚ÄúKnockoff Nets: Stealing Functionality of Black-Box Models.‚Äù</span> In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 4949‚Äì58. IEEE. <a href="https://doi.org/10.1109/cvpr.2019.00509">https://doi.org/10.1109/cvpr.2019.00509</a>.
</div></div><p>Attackers may evaluate the success of behavior replication in two ways. The first is by measuring the level of effectiveness of the substitute model. This involves assessing whether the cloned model achieves similar accuracy, precision, recall, or other performance metrics on benchmark tasks. By aligning the substitute‚Äôs performance with that of the original, attackers can build a model that is practically indistinguishable in effectiveness, even if its internal structure differs.</p>
<p>The second is by testing prediction consistency. This involves checking whether the substitute model produces the same outputs as the original model when presented with the same inputs. Matching not only correct predictions but also the original model‚Äôs mistakes can provide attackers with a high-fidelity reproduction of the target model‚Äôs behavior. This is particularly concerning in applications such as natural language processing, where attackers might replicate sentiment analysis models to gain competitive insights or bypass proprietary systems.</p>
<p>Approximate behavior theft is particularly challenging to defend against in open-access deployment settings, such as public APIs or consumer-facing applications. Limiting the rate of queries, detecting automated extraction patterns, and watermarking model outputs are among the techniques that can help mitigate this risk. However, these defenses must be balanced with usability and performance considerations, especially in production environments.</p>
<p>One notable demonstration of approximate model theft focuses on extracting internal components of black-box language models via public APIs. In their paper, <span class="citation" data-cites="carlini2024stealing">Carlini et al. (<a href="../references.html#ref-carlini2024stealing" role="doc-biblioref">2024</a>)</span>, researchers show how to reconstruct the final embedding projection matrix of several OpenAI models, including <code>ada</code>, <code>babbage</code>, and <code>gpt-3.5-turbo</code>, using only public API access. By exploiting the low-rank structure of the output projection layer and making carefully crafted queries, they recover the model‚Äôs hidden dimensionality and replicate the weight matrix up to affine transformations.</p>
<div class="no-row-height column-margin column-container"></div><p>While the attack does not reconstruct the full model, it reveals critical internal architecture parameters and sets a precedent for future, deeper extractions. This work demonstrated that even partial model theft poses risks to confidentiality and competitive advantage, especially when model behavior can be probed through rich API responses such as logit bias and log-probabilities.</p>
<div id="tbl-openai-theft" class="striped hover quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-tbl figure page-columns page-full">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-openai-theft-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.2: Model theft results from <span class="citation" data-cites="carlini2024stealing">Carlini et al. (<a href="../references.html#ref-carlini2024stealing" role="doc-biblioref">2024</a>)</span>. The table summarizes the model sizes, number of queries required for dimension extraction, root mean square errors (RMS) for weight matrix extraction, and estimated costs based on OpenAI‚Äôs API pricing.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-carlini2024stealing" class="csl-entry" role="listitem">
Carlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. <span>‚ÄúStealing Part of a Production Language Model.‚Äù</span> <em>arXiv Preprint arXiv:2403.06634</em>, March. <a href="http://arxiv.org/abs/2403.06634v2">http://arxiv.org/abs/2403.06634v2</a>.
</div></div><div aria-describedby="tbl-openai-theft-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 16%">
<col style="width: 17%">
<col style="width: 18%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Size (Dimension Extraction)</th>
<th style="text-align: left;">Number of Queries</th>
<th style="text-align: left;">RMS (Weight Matrix Extraction)</th>
<th style="text-align: left;">Cost (USD)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">OpenAI ada</td>
<td style="text-align: left;">1024 ‚úì</td>
<td style="text-align: left;">&lt; (2 ^6)</td>
<td style="text-align: left;">(5 ^{-4})</td>
<td style="text-align: left;">$1 / $4</td>
</tr>
<tr class="even">
<td style="text-align: left;">OpenAI babbage</td>
<td style="text-align: left;">2048 ‚úì</td>
<td style="text-align: left;">&lt; (4 ^6)</td>
<td style="text-align: left;">(7 ^{-4})</td>
<td style="text-align: left;">$2 / $12</td>
</tr>
<tr class="odd">
<td style="text-align: left;">OpenAI babbage-002</td>
<td style="text-align: left;">1536 ‚úì</td>
<td style="text-align: left;">&lt; (4 ^6)</td>
<td style="text-align: left;">Not implemented</td>
<td style="text-align: left;">$2 / $12</td>
</tr>
<tr class="even">
<td style="text-align: left;">OpenAI gpt-3.5-turbo-instruct</td>
<td style="text-align: left;">Not disclosed</td>
<td style="text-align: left;">&lt; (4 ^7)</td>
<td style="text-align: left;">Not implemented</td>
<td style="text-align: left;">$200 / ~$2,000 (estimated)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">OpenAI gpt-3.5-turbo-1106</td>
<td style="text-align: left;">Not disclosed</td>
<td style="text-align: left;">&lt; (4 ^7)</td>
<td style="text-align: left;">Not implemented</td>
<td style="text-align: left;">$800 / ~$8,000 (estimated)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As shown in their empirical evaluation, reproduced in <a href="#tbl-openai-theft" class="quarto-xref">Table&nbsp;<span>15.2</span></a>, model parameters could be extracted with root mean square errors as low as <span class="math inline">\(10^{-4}\)</span>, confirming that high-fidelity approximation is achievable at scale. These findings raise important implications for system design, suggesting that innocuous API features, like returning top-k logits, can serve as significant leakage vectors if not tightly controlled.</p>
</section>
<section id="case-study-tesla-ip-theft" class="level4">
<h4 class="anchored" data-anchor-id="case-study-tesla-ip-theft">Case Study: Tesla IP Theft</h4>
<p>In 2018, Tesla filed a <a href="https://storage.courtlistener.com/recap/gov.uscourts.nvd.131251/gov.uscourts.nvd.131251.1.0_1.pdf">lawsuit</a> against the self-driving car startup <a href="https://zoox.com/">Zoox</a>, alleging that former Tesla employees had stolen proprietary data and trade secrets related to Tesla‚Äôs autonomous driving technology. According to the lawsuit, several employees transferred over 10 gigabytes of confidential files, including machine learning models and source code, before leaving Tesla to join Zoox.</p>
<p>Among the stolen materials was a key image recognition model used for object detection in Tesla‚Äôs self-driving system. By obtaining this model, Zoox could have bypassed years of research and development, giving the company a competitive advantage. Beyond the economic implications, there were concerns that the stolen model could expose Tesla to further security risks, such as model inversion attacks aimed at extracting sensitive data from the model‚Äôs training set.</p>
<p>The Zoox employees denied any wrongdoing, and the case was ultimately settled out of court. Nevertheless, the incident highlights the real-world risks of model theft, particularly in industries where machine learning models represent significant intellectual property. The theft of models not only undermines competitive advantage but also raises broader concerns about privacy, safety, and the potential for downstream exploitation.</p>
<p>This case demonstrates that model theft is not limited to theoretical attacks conducted over APIs or public interfaces. Insider threats, supply chain vulnerabilities, and unauthorized access to development infrastructure pose equally serious risks to machine learning systems deployed in commercial environments.</p>
</section>
</section>
<section id="data-poisoning" class="level3 page-columns page-full" data-number="15.5.2">
<h3 data-number="15.5.2" class="anchored" data-anchor-id="data-poisoning"><span class="header-section-number">15.5.2</span> Data Poisoning</h3>
<p>Training integrity threats stem from the manipulation of data used to train machine learning models. These attacks aim to corrupt the learning process by introducing examples that appear benign but induce harmful or biased behavior in the final model.</p>
<p>Data poisoning attacks are a prominent example, in which adversaries inject carefully crafted data points into the training set to influence model behavior in targeted or systemic ways <span class="citation" data-cites="biggio2012poisoning">(<a href="../references.html#ref-biggio2012poisoning" role="doc-biblioref">Biggio, Nelson, and Laskov 2012</a>)</span>. Poisoned data may cause a model to make incorrect predictions, degrade its generalization ability, or embed failure modes that remain dormant until triggered post-deployment.</p>
<div class="no-row-height column-margin column-container"><div id="ref-biggio2012poisoning" class="csl-entry" role="listitem">
Biggio, Battista, Blaine Nelson, and Pavel Laskov. 2012. <span>‚ÄúPoisoning Attacks Against Support Vector Machines.‚Äù</span> In <em>Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012</em>. icml.cc / Omnipress. <a href="http://icml.cc/2012/papers/880.pdf">http://icml.cc/2012/papers/880.pdf</a>.
</div></div><p>Data poisoning is a security threat because it involves intentional manipulation of the training data by an adversary, with the goal of embedding vulnerabilities or subverting model behavior. These attacks are especially concerning in applications where models retrain on data collected from external sources, including user interactions, crowdsourced annotations, and online scraping, since attackers can inject poisoned data without direct access to the training pipeline. Even in more controlled settings, poisoning may occur through compromised data storage, insider manipulation, or insecure data transfer processes.</p>
<p>From a security perspective, poisoning attacks vary depending on the attacker‚Äôs level of access and knowledge. In white-box scenarios, the adversary may have detailed insight into the model architecture or training process, enabling more precise manipulation. In contrast, black-box or limited-access attacks exploit open data submission channels or indirect injection vectors. Poisoning can target different stages of the ML pipeline, ranging from data collection and preprocessing to labeling and storage, making the attack surface both broad and system-dependent.</p>
<p>Poisoning attacks typically follow a three-stage process. First, the attacker injects malicious data into the training set. These examples are often designed to appear legitimate but introduce subtle distortions that alter the model‚Äôs learning process. Second, the model trains on this compromised data, embedding the attacker‚Äôs intended behavior. Finally, once the model is deployed, the attacker may exploit the altered behavior to cause mispredictions, bypass safety checks, or degrade overall reliability.</p>
<p>Formally, data poisoning can be viewed as a bilevel optimization problem, where the attacker seeks to select poisoning data <span class="math inline">\(D_p\)</span> that maximizes the model‚Äôs loss on a validation or target dataset <span class="math inline">\(D_{\text{test}}\)</span>. Let <span class="math inline">\(D\)</span> represent the original training data. The attacker‚Äôs obj <span class="math display">\[
\max_{D_p} \ \mathcal{L}(f_{D \cup D_p}, D_{\text{test}})
\]</span> where <span class="math inline">\(f_{D \cup D_p}\)</span> is the model trained on the combined dataset. For targeted attacks, this objective may focus on specific inputs <span class="math inline">\(x_t\)</span> and target labels <span class="math inline">\(y_t\)</span>: <span class="math display">\[
\max_{D_p} \ \mathcal{L}(f_{D \cup D_p}, x_t, y_t)
\]</span></p>
<p>This formulation captures the adversary‚Äôs goal of introducing carefully crafted data points to manipulate the model‚Äôs decision boundaries.</p>
<p>For example, consider a traffic sign classification model trained to distinguish between stop signs and speed limit signs. An attacker might inject a small number of stop sign images labeled as speed limit signs into the training data. The attacker‚Äôs goal is to subtly shift the model‚Äôs decision boundary so that future stop signs are misclassified as speed limit signs. In this case, the poisoning data <span class="math inline">\(D_p\)</span> consists of mislabeled stop sign images, and the attacker‚Äôs objective is to maximize the misclassification of legitimate stop signs <span class="math inline">\(x_t\)</span> as speed limit signs <span class="math inline">\(y_t\)</span>, following the targeted attack formulation above. Even if the model performs well on other types of signs, the poisoned training process creates a predictable and exploitable vulnerability.</p>
<p>Data poisoning attacks can be classified based on their objectives and scope of impact. Availability attacks degrade overall model performance by introducing noise or label flips that reduce accuracy across tasks. Targeted attacks manipulate a specific input or class, leaving general performance intact but causing consistent misclassification in select cases. Backdoor attacks embed hidden triggers, which are often imperceptible patterns, that elicit malicious behavior only when the trigger is present. Subpopulation attacks degrade performance on a specific group defined by shared features, making them particularly dangerous in fairness-sensitive applications.</p>
<p>A notable real-world example of a targeted poisoning attack was demonstrated against Perspective, an online toxicity detection model <span class="citation" data-cites="hosseini2017deceiving">(<a href="../references.html#ref-hosseini2017deceiving" role="doc-biblioref">Hosseini et al. 2017</a>)</span>. By injecting synthetically generated toxic comments with subtle misspellings and grammatical errors into the model‚Äôs training set, researchers degraded its ability to detect harmful content. After retraining, the poisoned model exhibited a significantly higher false negative rate, allowing offensive language to bypass filters. This case illustrates how poisoned data can exploit feedback loops in systems that rely on user-generated input, leading to reduced effectiveness over time and creating long-term vulnerabilities in content moderation pipelines.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hosseini2017deceiving" class="csl-entry" role="listitem">
Hosseini, Hossein, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017. <span>‚ÄúDeceiving Google‚Äôs Perspective API Built for Detecting Toxic Comments.‚Äù</span> <em>ArXiv Preprint</em> abs/1702.08138 (February). <a href="http://arxiv.org/abs/1702.08138v1">http://arxiv.org/abs/1702.08138v1</a>.
</div></div><p>Mitigating data poisoning threats requires end-to-end security of the data pipeline, encompassing collection, storage, labeling, and training. Preventative measures include input validation checks, integrity verification of training datasets, and anomaly detection to flag suspicious patterns. In parallel, robust training algorithms can limit the influence of mislabeled or manipulated data by down-weighting or filtering out anomalous instances. While no single technique guarantees immunity, combining proactive data governance, automated monitoring, and robust learning practices is essential for maintaining model integrity in real-world deployments.</p>
</section>
<section id="adversarial-attacks" class="level3 page-columns page-full" data-number="15.5.3">
<h3 data-number="15.5.3" class="anchored" data-anchor-id="adversarial-attacks"><span class="header-section-number">15.5.3</span> Adversarial Attacks</h3>
<p>Inference robustness threats occur when attackers manipulate inputs at test time to induce incorrect predictions. Unlike data poisoning, which compromises the training process, these attacks exploit vulnerabilities in the model‚Äôs decision surface during inference.</p>
<p>A central class of such threats is adversarial attacks, where carefully constructed inputs are designed to cause incorrect predictions while remaining nearly indistinguishable from legitimate data <span class="citation" data-cites="szegedy2014intriguing parrish2023adversarial">(<a href="../references.html#ref-szegedy2014intriguing" role="doc-biblioref">Szegedy et al. 2013</a>; <a href="../references.html#ref-parrish2023adversarial" role="doc-biblioref">Parrish et al. 2023</a>)</span>. These attacks highlight a critical weakness in many ML models: their sensitivity to small, targeted perturbations that can drastically alter output confidence or classification results.</p>
<div class="no-row-height column-margin column-container"><div id="ref-szegedy2014intriguing" class="csl-entry" role="listitem">
Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. <span>‚ÄúIntriguing Properties of Neural Networks.‚Äù</span> <em>ICLR</em>, December. <a href="http://arxiv.org/abs/1312.6199v4">http://arxiv.org/abs/1312.6199v4</a>.
</div><div id="ref-parrish2023adversarial" class="csl-entry" role="listitem">
Parrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. <span>‚ÄúAdversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models.‚Äù</span> <em>ArXiv Preprint</em> abs/2305.14384 (May). <a href="http://arxiv.org/abs/2305.14384v1">http://arxiv.org/abs/2305.14384v1</a>.
</div><div id="ref-ramesh2021zero" class="csl-entry" role="listitem">
Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. <span>‚ÄúZero-Shot Text-to-Image Generation.‚Äù</span> In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, edited by Marina Meila and Tong Zhang, 139:8821‚Äì31. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/ramesh21a.html">http://proceedings.mlr.press/v139/ramesh21a.html</a>.
</div><div id="ref-rombach2022highresolution" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. <span>‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models.‚Äù</span> In <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10674‚Äì85. IEEE. <a href="https://doi.org/10.1109/cvpr52688.2022.01042">https://doi.org/10.1109/cvpr52688.2022.01042</a>.
</div></div><p>The central vulnerability arises from the model‚Äôs sensitivity to small, targeted perturbations. A single image, for instance, can be subtly altered, by altering only a few pixel values, such that a classifier misidentifies a stop sign as a speed limit sign. In natural language processing, specially crafted input sequences may trigger toxic or misleading outputs in a generative model, even when the prompt appears benign to a human reader <span class="citation" data-cites="ramesh2021zero rombach2022highresolution">(<a href="../references.html#ref-ramesh2021zero" role="doc-biblioref">Ramesh et al. 2021</a>; <a href="../references.html#ref-rombach2022highresolution" role="doc-biblioref">Rombach et al. 2022</a>)</span>.</p>
<p>Adversarial attacks pose critical safety and security risks in domains such as autonomous driving, biometric authentication, and content moderation. Unlike data poisoning, which corrupts the model during training, adversarial attacks manipulate the model‚Äôs behavior at test time, often without requiring any access to the training data or model internals. The attack surface thus shifts from upstream data pipelines to real-time interaction, demanding robust defense mechanisms capable of detecting or mitigating malicious inputs at the point of inference.</p>
<p>Adversarial example generation can be formally described as a constrained optimization problem, where the attacker seeks to find a minimally perturbed version of a legitimate input that maximizes the model‚Äôs prediction error. Given an input <span class="math inline">\(x\)</span> with true label <span class="math inline">\(y\)</span>, the attacker‚Äôs objective is to find a perturbed input <span class="math inline">\(x' = x + \delta\)</span> that maximizes the model‚Äôs loss: <span class="math display">\[
\max_{\delta} \ \mathcal{L}(f(x + \delta), y)
\]</span> subject to the constraint: <span class="math display">\[
\|\delta\| \leq \epsilon
\]</span> where <span class="math inline">\(f(\cdot)\)</span> is the model, <span class="math inline">\(\mathcal{L}\)</span> is the loss function, and <span class="math inline">\(\epsilon\)</span> defines the allowed perturbation magnitude. This ensures that the perturbation remains small, often imperceptible to humans, while still leading the model to produce an incorrect output.</p>
<p>This optimization view underlies common adversarial strategies used in both white-box and black-box settings. A full taxonomy of attack algorithms, including gradient-based, optimization-based, and transfer-based techniques, is provided in a later chapter.</p>
<p>Adversarial attacks vary based on the attacker‚Äôs level of access to the model. In white-box attacks, the adversary has full knowledge of the model‚Äôs architecture, parameters, and training data, allowing them to craft highly effective adversarial examples. In black-box attacks, the adversary has no internal knowledge and must rely on querying the model and observing its outputs. Grey-box attacks fall between these extremes, with the adversary possessing partial information, such as access to the model architecture but not its parameters.</p>
<p>These attacker models can be summarized along a spectrum of knowledge levels. <a href="#tbl-adversary-knowledge-spectrum" class="quarto-xref">Table&nbsp;<span>15.3</span></a> highlights the differences in model access, data access, typical attack strategies, and common deployment scenarios. Such distinctions help characterize the practical challenges of securing ML systems across different deployment environments.</p>
<div id="tbl-adversary-knowledge-spectrum" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-adversary-knowledge-spectrum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.3: Adversary knowledge spectrum.
</figcaption>
<div aria-describedby="tbl-adversary-knowledge-spectrum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 23%">
<col style="width: 13%">
<col style="width: 24%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Adversary Knowledge Level</th>
<th style="text-align: left;">Model Access</th>
<th style="text-align: left;">Training Data Access</th>
<th style="text-align: left;">Attack Example</th>
<th style="text-align: left;">Common Scenario</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">White-box</td>
<td style="text-align: left;">Full access to architecture and parameters</td>
<td style="text-align: left;">Full access</td>
<td style="text-align: left;">Crafting adversarial examples using gradients</td>
<td style="text-align: left;">Insider threats, open-source model reuse</td>
</tr>
<tr class="even">
<td style="text-align: left;">Grey-box</td>
<td style="text-align: left;">Partial access (e.g., architecture only)</td>
<td style="text-align: left;">Limited or no access</td>
<td style="text-align: left;">Attacks based on surrogate model approximation</td>
<td style="text-align: left;">Known model family, unknown fine-tuning</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Black-box</td>
<td style="text-align: left;">No internal access; only query-response view</td>
<td style="text-align: left;">No access</td>
<td style="text-align: left;">Query-based surrogate model training and transfer attacks</td>
<td style="text-align: left;">Public APIs, model-as-a-service deployments</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>A common attack strategy involves constructing a surrogate model that approximates the target model‚Äôs behavior. This surrogate model is trained by querying the target model with a set of inputs <span class="math inline">\(\{x_i\}\)</span> and recording the corresponding outputs <span class="math inline">\(\{f(x_i)\}\)</span>. The attacker‚Äôs goal is to train a surrogate model <span class="math inline">\(\hat{f}\)</span> that minimizes the discrepancy between its predictions and those of the target model. This objective can be formulated as: <span class="math display">\[
\min_{\hat{f}} \ \sum_{i=1}^{n} \ \ell(\hat{f}(x_i), f(x_i))
\]</span> where <span class="math inline">\(\ell\)</span> is a loss function measuring the difference between the surrogate‚Äôs output and the target model‚Äôs output. By minimizing this loss, the attacker builds a model that behaves similarly to the target. Once trained, the surrogate model can be used to generate adversarial examples using white-box techniques. These examples often transfer to the original target model, even without internal access, making such attacks effective in black-box settings. This phenomenon, known as adversarial transferability, presents a significant challenge for defense.</p>
<p>Several methods have been proposed to generate adversarial examples. One notable approach leverages generative adversarial networks (GANs) <span class="citation" data-cites="goodfellow2020generative">(<a href="../references.html#ref-goodfellow2020generative" role="doc-biblioref">I. Goodfellow et al. 2020</a>)</span>. In this setting, a generator network learns to produce inputs that deceive the target model, while a discriminator evaluates their effectiveness. This iterative process allows the attacker to generate sophisticated and diverse adversarial examples.</p>
<div class="no-row-height column-margin column-container"><div id="ref-goodfellow2020generative" class="csl-entry" role="listitem">
Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. <span>‚ÄúGenerative Adversarial Networks.‚Äù</span> <em>Communications of the ACM</em> 63 (11): 139‚Äì44. <a href="https://doi.org/10.1145/3422622">https://doi.org/10.1145/3422622</a>.
</div><div id="ref-ahmed2020headless" class="csl-entry" role="listitem">
Abdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, and Chen Zhu. 2020. <span>‚ÄúHeadless Horseman: Adversarial Attacks on Transfer Learning Models.‚Äù</span> In <em>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 3087‚Äì91. IEEE. <a href="https://doi.org/10.1109/icassp40776.2020.9053181">https://doi.org/10.1109/icassp40776.2020.9053181</a>.
</div></div><p>Another vector for adversarial attacks involves transfer learning pipelines. Many production systems reuse pre-trained feature extractors, fine-tuning only the final layers for specific tasks. Adversaries can exploit this structure by targeting the shared feature extractor, crafting perturbations that affect multiple downstream tasks. Headless attacks, for example, manipulate the feature extractor without requiring access to the classification head or training data <span class="citation" data-cites="ahmed2020headless">(<a href="../references.html#ref-ahmed2020headless" role="doc-biblioref">Abdelkader et al. 2020</a>)</span>. This exposes a critical vulnerability in systems that rely on pre-trained models.</p>
<p>One illustrative example involves the manipulation of traffic sign recognition systems <span class="citation" data-cites="eykholt2018robust">(<a href="../references.html#ref-eykholt2018robust" role="doc-biblioref">Eykholt et al. 2017</a>)</span>. Researchers demonstrated that placing small stickers on stop signs could cause machine learning models to misclassify them as speed limit signs. While the altered signs remained easily recognizable to humans, the model consistently misinterpreted them. Such attacks pose serious risks in applications like autonomous driving, where reliable perception is critical for safety.</p>
<div class="no-row-height column-margin column-container"></div><p>Adversarial attacks highlight the need for robust defenses that go beyond improving model accuracy. Securing ML systems against adversarial threats requires runtime defenses such as input validation, anomaly detection, and monitoring for abnormal patterns during inference. Training-time robustness methods (e.g., adversarial training) complement these strategies and are discussed in more detail in a later chapter. These defenses aim to enhance model resilience against adversarial examples, ensuring that machine learning systems can operate reliably even in the presence of malicious inputs.</p>
</section>
<section id="case-study-traffic-sign-detection-model-trickery" class="level3 page-columns page-full" data-number="15.5.4">
<h3 data-number="15.5.4" class="anchored" data-anchor-id="case-study-traffic-sign-detection-model-trickery"><span class="header-section-number">15.5.4</span> Case Study: Traffic Sign Detection Model Trickery</h3>
<p>In 2017, researchers conducted experiments by placing small black and white stickers on stop signs <span class="citation" data-cites="eykholt2018robust">(<a href="../references.html#ref-eykholt2018robust" role="doc-biblioref">Eykholt et al. 2017</a>)</span>. As shown in <a href="#fig-adversarial-stickers" class="quarto-xref">Figure&nbsp;<span>15.4</span></a>, these stickers were designed to be nearly imperceptible to the human eye, yet they significantly altered the appearance of the stop sign when viewed by machine learning models. When viewed by a normal human eye, the stickers did not obscure the sign or prevent interpretability. However, when images of the stickers stop signs were fed into standard traffic sign classification ML models, they were misclassified as speed limit signs over 85% of the time.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-adversarial-stickers" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-adversarial-stickers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/stop_signs.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;15.4: Adversarial stickers on stop signs. Source: @eykholt2018robust"><img src="./images/png/stop_signs.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-stickers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.4: Adversarial stickers on stop signs. Source: <span class="citation" data-cites="eykholt2018robust">Eykholt et al. (<a href="../references.html#ref-eykholt2018robust" role="doc-biblioref">2017</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-eykholt2018robust" class="csl-entry" role="listitem">
Eykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017. <span>‚ÄúRobust Physical-World Attacks on Deep Learning Models.‚Äù</span> <em>ArXiv Preprint</em> abs/1707.08945 (July). <a href="http://arxiv.org/abs/1707.08945v5">http://arxiv.org/abs/1707.08945v5</a>.
</div></div></figure>
</div>
<p>This demonstration showed how simple adversarial stickers could trick ML systems into misreading critical road signs. If deployed realistically, these attacks could endanger public safety, causing autonomous vehicles to misinterpret stop signs as speed limits. Researchers warned this could potentially cause dangerous rolling stops or acceleration into intersections.</p>
<p>This case study provides a concrete illustration of how adversarial examples exploit the pattern recognition mechanisms of ML models. By subtly altering the input data, attackers can induce incorrect predictions and pose significant risks to safety-critical applications like self-driving cars. The attack‚Äôs simplicity demonstrates how even minor, imperceptible changes can lead models astray. Consequently, developers must implement robust defenses against such threats.</p>
<p>These threat types span different stages of the ML lifecycle and demand distinct defensive strategies. <a href="#tbl-threats-models-summary" class="quarto-xref">Table&nbsp;<span>15.4</span></a> below summarizes their key characteristics.</p>
<div id="tbl-threats-models-summary" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-threats-models-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.4: Summary of threat types to ML models by lifecycle stage and attack vector.
</figcaption>
<div aria-describedby="tbl-threats-models-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 17%">
<col style="width: 23%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Threat Type</th>
<th style="text-align: left;">Lifecycle Stage</th>
<th style="text-align: left;">Attack Vector</th>
<th style="text-align: left;">Example Impact</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Model Theft</td>
<td style="text-align: left;">Deployment</td>
<td style="text-align: left;">API access, insider leaks</td>
<td style="text-align: left;">Stolen IP, model inversion, behavioral clone</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data Poisoning</td>
<td style="text-align: left;">Training</td>
<td style="text-align: left;">Label flipping, backdoors</td>
<td style="text-align: left;">Targeted misclassification, degraded accuracy</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adversarial Attacks</td>
<td style="text-align: left;">Inference</td>
<td style="text-align: left;">Input perturbation</td>
<td style="text-align: left;">Real-time misclassification, safety failure</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The appropriate defense for a given threat depends on its type, attack vector, and where it occurs in the ML lifecycle. <a href="#fig-threat-mitigation-flow" class="quarto-xref">Figure&nbsp;<span>15.5</span></a> provides a simplified decision flow that connects common threat categories, such as model theft, data poisoning, and adversarial examples, to corresponding defensive strategies. While real-world deployments may require more nuanced or layered defenses, this flowchart serves as a conceptual guide for aligning threat models with practical mitigation techniques.</p>
<div id="fig-threat-mitigation-flow" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-threat-mitigation-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="38b6a7aab39b0b8be97e6e08994a6625c0dce6fa.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;15.5: Example flow for selecting appropriate defenses based on threat type in machine learning systems."><img src="privacy_security_files/mediabag/38b6a7aab39b0b8be97e6e08994a6625c0dce6fa.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-threat-mitigation-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.5: Example flow for selecting appropriate defenses based on threat type in machine learning systems.
</figcaption>
</figure>
</div>
<p>While ML models themselves present critical attack surfaces, they ultimately run on hardware that can introduce vulnerabilities beyond the model‚Äôs control. In the next section, we examine how adversaries can target the physical infrastructure that executes machine learning workloads‚Äîthrough hardware bugs, physical tampering, side channels, and supply chain risks.</p>
</section>
</section>
<section id="threats-to-ml-hardware" class="level2 page-columns page-full" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="threats-to-ml-hardware"><span class="header-section-number">15.6</span> Threats to ML Hardware</h2>
<p>As machine learning systems move from research prototypes to large-scale, real-world deployments, their security increasingly depends on the hardware platforms they run on. Whether deployed in data centers, on edge devices, or in embedded systems, machine learning applications rely on a layered stack of processors, accelerators, memory, and communication interfaces. These hardware components, while essential for enabling efficient computation, introduce unique security risks that go beyond traditional software-based vulnerabilities.</p>
<p>Unlike general-purpose software systems, machine learning workflows often process high-value models and sensitive data in performance-constrained environments. This makes them attractive targets not only for software attacks but also for hardware-level exploitation. Vulnerabilities in hardware can expose models to theft, leak user data, disrupt system reliability, or allow adversaries to manipulate inference results. Because hardware operates below the software stack, such attacks can bypass conventional security mechanisms and remain difficult to detect.</p>
<p>These hardware threats arise from multiple sources, including design flaws in hardware architectures, physical tampering, side-channel leakage, and supply chain compromises. Together, they form a critical attack surface that must be addressed to build trustworthy machine learning systems.</p>
<p><a href="#tbl-threat_types" class="quarto-xref">Table&nbsp;<span>15.5</span></a> summarizes the major categories of hardware security threats, describing their origins, methods, and implications for machine learning system design and deployment.</p>
<div id="tbl-threat_types" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-threat_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.5: Threat types on hardware security.
</figcaption>
<div aria-describedby="tbl-threat_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 56%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Threat Type</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Relevance to ML Hardware Security</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Hardware Bugs</td>
<td style="text-align: left;">Intrinsic flaws in hardware designs that can compromise system integrity.</td>
<td style="text-align: left;">Foundation of hardware vulnerability.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Physical Attacks</td>
<td style="text-align: left;">Direct exploitation of hardware through physical access or manipulation.</td>
<td style="text-align: left;">Basic and overt threat model.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Fault-injection Attacks</td>
<td style="text-align: left;">Induction of faults to cause errors in hardware operation, leading to potential system crashes.</td>
<td style="text-align: left;">Systematic manipulation leading to failure.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Side-Channel Attacks</td>
<td style="text-align: left;">Exploitation of leaked information from hardware operation to extract sensitive data.</td>
<td style="text-align: left;">Indirect attack via environmental observation.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Leaky Interfaces</td>
<td style="text-align: left;">Vulnerabilities arising from interfaces that expose data unintentionally.</td>
<td style="text-align: left;">Data exposure through communication channels.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Counterfeit Hardware</td>
<td style="text-align: left;">Use of unauthorized hardware components that may have security flaws.</td>
<td style="text-align: left;">Compounded vulnerability issues.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Supply Chain Risks</td>
<td style="text-align: left;">Risks introduced through the hardware lifecycle, from production to deployment.</td>
<td style="text-align: left;">Cumulative &amp; multifaceted security challenges.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="hardware-bugs" class="level3 page-columns page-full" data-number="15.6.1">
<h3 data-number="15.6.1" class="anchored" data-anchor-id="hardware-bugs"><span class="header-section-number">15.6.1</span> Hardware Bugs</h3>
<p>Hardware is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit these vulnerabilities to access, manipulate, or extract sensitive data, breaching the confidentiality and integrity that users and services depend on. One of the most notable examples came with the discovery of <a href="https://meltdownattack.com/">Meltdown and Spectre</a>‚Äîtwo vulnerabilities in modern processors that allow malicious programs to bypass memory isolation and read the data of other applications and the operating system <span class="citation" data-cites="Lipp2018meltdown Kocher2018spectre">(<a href="../references.html#ref-Lipp2018meltdown" role="doc-biblioref">Kocher et al. 2019a</a>, <a href="../references.html#ref-Kocher2018spectre" role="doc-biblioref">2019b</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Lipp2018meltdown" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî, et al. 2019a. <span>‚ÄúSpectre Attacks: Exploiting Speculative Execution.‚Äù</span> In <em>2019 IEEE Symposium on Security and Privacy (SP)</em>, 1‚Äì19. IEEE. <a href="https://doi.org/10.1109/sp.2019.00002">https://doi.org/10.1109/sp.2019.00002</a>.
</div><div id="ref-Kocher2018spectre" class="csl-entry" role="listitem">
Kocher, Paul, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, et al. 2019b. <span>‚ÄúSpectre Attacks: Exploiting Speculative Execution.‚Äù</span> In <em>2019 IEEE Symposium on Security and Privacy (SP)</em>, 1‚Äì19. IEEE. <a href="https://doi.org/10.1109/sp.2019.00002">https://doi.org/10.1109/sp.2019.00002</a>.
</div></div><p>These attacks exploit speculative execution, a performance optimization in CPUs that executes instructions out of order before safety checks are complete. While improving computational speed, this optimization inadvertently exposes sensitive data through microarchitectural side channels, such as CPU caches. The technical sophistication of these attacks highlights the difficulty of eliminating vulnerabilities even with extensive hardware validation.</p>
<p>Further research has revealed that these were not isolated incidents. Variants such as Foreshadow, ZombieLoad, and RIDL target different microarchitectural elements, ranging from secure enclaves to CPU internal buffers, demonstrating that speculative execution flaws are a systemic hardware risk.</p>
<p>While these attacks were first demonstrated on general-purpose CPUs, their implications extend to machine learning accelerators and specialized hardware. ML systems often rely on heterogeneous compute platforms that combine CPUs with GPUs, TPUs, FPGAs, or custom accelerators. These components process sensitive data such as personal information, medical records, or proprietary models. Vulnerabilities in any part of this stack could expose such data to attackers.</p>
<p>For example, an edge device like a smart camera running a face recognition model on an accelerator could be vulnerable if the hardware lacks proper cache isolation. An attacker might exploit this weakness to extract intermediate computations, model parameters, or user data. Similar risks exist in cloud inference services, where hardware multi-tenancy increases the chances of cross-tenant data leakage.</p>
<p>Such vulnerabilities are particularly concerning in privacy-sensitive domains like healthcare, where ML systems routinely handle patient data. A breach could violate privacy regulations such as the <a href="https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html">Health Insurance Portability and Accountability Act (HIPAA)</a>, leading to significant legal and ethical consequences.</p>
<p>These examples illustrate that hardware security is not solely about preventing physical tampering. It also requires architectural safeguards to prevent data leakage through the hardware itself. As new vulnerabilities continue to emerge across processors, accelerators, and memory systems, addressing these risks requires continuous mitigation efforts‚Äîoften involving performance trade-offs, especially in compute- and memory-intensive ML workloads. Proactive solutions, such as confidential computing and trusted execution environments (TEEs), offer promising architectural defenses. However, achieving robust hardware security requires attention at every stage of the system lifecycle, from design to deployment.</p>
</section>
<section id="physical-attacks" class="level3" data-number="15.6.2">
<h3 data-number="15.6.2" class="anchored" data-anchor-id="physical-attacks"><span class="header-section-number">15.6.2</span> Physical Attacks</h3>
<p>Physical tampering refers to the direct, unauthorized manipulation of computing hardware to undermine the integrity of machine learning systems. This type of attack is particularly concerning because it bypasses traditional software security defenses, directly targeting the physical components on which machine learning depends. ML systems are especially vulnerable to such attacks because they rely on hardware sensors, accelerators, and storage to process large volumes of data and produce reliable outcomes in real-world environments.</p>
<p>While software security measures, including encryption, authentication, and access control, protect ML systems against remote attacks, they offer little defense against adversaries with physical access to devices. Physical tampering can range from simple actions, like inserting a malicious USB device into an edge server, to highly sophisticated manipulations such as embedding hardware trojans during chip manufacturing. These threats are particularly relevant for machine learning systems deployed at the edge or in physically exposed environments, where attackers may have opportunities to interfere with the hardware directly.</p>
<p>To understand how such attacks affect ML systems in practice, consider the example of an ML-powered drone used for environmental mapping or infrastructure inspection. The drone‚Äôs navigation depends on machine learning models that process data from GPS, cameras, and inertial measurement units. If an attacker gains physical access to the drone, they could replace or modify its navigation module, embedding a hidden backdoor that alters flight behavior or reroutes data collection. Such manipulation not only compromises the system‚Äôs reliability but also opens the door to misuse, such as surveillance or smuggling operations.</p>
<p>Physical attacks are not limited to mobility systems. Biometric access control systems, which rely on ML models to process face or fingerprint data, are also vulnerable. These systems typically use embedded hardware to capture and process biometric inputs. An attacker could physically replace a biometric sensor with a modified component designed to capture and transmit personal identification data to an unauthorized receiver. This compromises both security and user privacy, and it can enable future impersonation attacks.</p>
<p>In addition to tampering with external sensors, attackers may target internal hardware subsystems. For example, the sensors used in autonomous vehicles, including cameras, LiDAR, and radar, are essential for ML models that interpret the surrounding environment. A malicious actor could physically misalign or obstruct these sensors, degrading the model‚Äôs perception capabilities and creating safety hazards.</p>
<p>Hardware trojans pose another serious risk. Malicious modifications introduced during chip fabrication or assembly can embed dormant circuits in ML accelerators or inference chips. These trojans may remain inactive under normal conditions but trigger malicious behavior when specific inputs are processed or system states are reached. Such hidden vulnerabilities can disrupt computations, leak model outputs, or degrade system performance in ways that are extremely difficult to diagnose post-deployment.</p>
<p>Memory subsystems are also attractive targets. Attackers with physical access to edge devices or embedded ML accelerators could manipulate memory chips to extract encrypted model parameters or training data. Fault injection techniques, including voltage manipulation and electromagnetic interference, can further degrade system reliability by corrupting model weights or forcing incorrect computations during inference.</p>
<p>Physical access threats extend to data center and cloud environments as well. Attackers with sufficient access could install hardware implants, such as keyloggers or data interceptors, to capture administrative credentials or monitor data streams. Such implants can provide persistent backdoor access, enabling long-term surveillance or data exfiltration from ML training and inference pipelines.</p>
<p>In summary, physical attacks on machine learning systems threaten both security and reliability across a wide range of deployment environments. Addressing these risks requires a combination of hardware-level protections, tamper detection mechanisms, and supply chain integrity checks. Without these safeguards, even the most secure software defenses may be undermined by vulnerabilities introduced through direct physical manipulation.</p>
</section>
<section id="fault-injection-attacks" class="level3 page-columns page-full" data-number="15.6.3">
<h3 data-number="15.6.3" class="anchored" data-anchor-id="fault-injection-attacks"><span class="header-section-number">15.6.3</span> Fault Injection Attacks</h3>
<p>Fault injection is a powerful class of physical attacks that deliberately disrupts hardware operations to induce errors in computation. These induced faults can compromise the integrity of machine learning models by causing them to produce incorrect outputs, degrade reliability, or leak sensitive information. For ML systems, such faults not only disrupt inference but also expose models to deeper exploitation, including reverse engineering and bypass of security protocols <span class="citation" data-cites="joye2012fault">(<a href="../references.html#ref-joye2012fault" role="doc-biblioref">Joye and Tunstall 2012</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-joye2012fault" class="csl-entry" role="listitem">
Joye, Marc, and Michael Tunstall. 2012. <em>Fault Analysis in Cryptography</em>. Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-642-29656-7">https://doi.org/10.1007/978-3-642-29656-7</a>.
</div><div id="ref-barenghi2010low" class="csl-entry" role="listitem">
Barenghi, Alessandro, Guido M. Bertoni, Luca Breveglieri, Mauro Pellicioli, and Gerardo Pelosi. 2010. <span>‚ÄúLow Voltage Fault Attacks to AES.‚Äù</span> In <em>2010 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST)</em>, 7‚Äì12. IEEE; IEEE. <a href="https://doi.org/10.1109/hst.2010.5513121">https://doi.org/10.1109/hst.2010.5513121</a>.
</div><div id="ref-hutter2009contact" class="csl-entry" role="listitem">
Hutter, Michael, Jorn-Marc Schmidt, and Thomas Plos. 2009. <span>‚ÄúContact-Based Fault Injections and Power Analysis on RFID Tags.‚Äù</span> In <em>2009 European Conference on Circuit Theory and Design</em>, 409‚Äì12. IEEE; IEEE. <a href="https://doi.org/10.1109/ecctd.2009.5275012">https://doi.org/10.1109/ecctd.2009.5275012</a>.
</div><div id="ref-amiel2006fault" class="csl-entry" role="listitem">
Amiel, Frederic, Christophe Clavier, and Michael Tunstall. 2006. <span>‚ÄúFault Analysis of DPA-Resistant Algorithms.‚Äù</span> In <em>Fault Diagnosis and Tolerance in Cryptography</em>, 223‚Äì36. Springer; Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/11889700\_20">https://doi.org/10.1007/11889700\_20</a>.
</div><div id="ref-agrawal2003side" class="csl-entry" role="listitem">
Agrawal, Dakshi, Selcuk Baktir, Deniz Karakoyunlu, Pankaj Rohatgi, and Berk Sunar. 2007. <span>‚ÄúTrojan Detection Using IC Fingerprinting.‚Äù</span> In <em>2007 IEEE Symposium on Security and Privacy (SP ‚Äô07)</em>, 296‚Äì310. Springer; IEEE. <a href="https://doi.org/10.1109/sp.2007.36">https://doi.org/10.1109/sp.2007.36</a>.
</div><div id="ref-skorobogatov2009local" class="csl-entry" role="listitem">
Skorobogatov, Sergei. 2009. <span>‚ÄúLocal Heating Attacks on Flash Memory Devices.‚Äù</span> In <em>2009 IEEE International Workshop on Hardware-Oriented Security and Trust</em>, 1‚Äì6. IEEE; IEEE. <a href="https://doi.org/10.1109/hst.2009.5225028">https://doi.org/10.1109/hst.2009.5225028</a>.
</div><div id="ref-skorobogatov2003optical" class="csl-entry" role="listitem">
Skorobogatov, Sergei P., and Ross J. Anderson. 2003. <span>‚ÄúOptical Fault Induction Attacks.‚Äù</span> In <em>Cryptographic Hardware and Embedded Systems - CHES 2002</em>, 2‚Äì12. Springer; Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/3-540-36400-5\_2">https://doi.org/10.1007/3-540-36400-5\_2</a>.
</div></div><p>Attackers achieve fault injection by applying precisely timed physical or electrical disturbances to the hardware while it is executing computations. Techniques such as low-voltage manipulation <span class="citation" data-cites="barenghi2010low">(<a href="../references.html#ref-barenghi2010low" role="doc-biblioref">Barenghi et al. 2010</a>)</span>, power spikes <span class="citation" data-cites="hutter2009contact">(<a href="../references.html#ref-hutter2009contact" role="doc-biblioref">Hutter, Schmidt, and Plos 2009</a>)</span>, clock glitches <span class="citation" data-cites="amiel2006fault">(<a href="../references.html#ref-amiel2006fault" role="doc-biblioref">Amiel, Clavier, and Tunstall 2006</a>)</span>, electromagnetic pulses <span class="citation" data-cites="agrawal2003side">(<a href="../references.html#ref-agrawal2003side" role="doc-biblioref">Agrawal et al. 2007</a>)</span>, temperature variations <span class="citation" data-cites="skorobogatov2009local">(<a href="../references.html#ref-skorobogatov2009local" role="doc-biblioref">S. Skorobogatov 2009</a>)</span>, and even laser strikes <span class="citation" data-cites="skorobogatov2003optical">(<a href="../references.html#ref-skorobogatov2003optical" role="doc-biblioref">S. P. Skorobogatov and Anderson 2003</a>)</span> have been demonstrated to corrupt specific parts of a program‚Äôs execution. These disturbances can cause effects such as bit flips, skipped instructions, or corrupted memory states, which adversaries can exploit to alter ML model behavior or extract sensitive information.</p>
<p>For machine learning systems, these attacks pose several concrete risks. Fault injection can degrade model accuracy, force incorrect classifications, trigger denial of service, or even leak internal model parameters. For example, attackers could inject faults into an embedded ML model running on a microcontroller, forcing it to misclassify inputs in safety-critical applications such as autonomous navigation or medical diagnostics. More sophisticated attackers may target memory or control logic to steal intellectual property, such as proprietary model weights or architecture details.</p>
<p>Experimental demonstrations have shown the feasibility of such attacks. One notable example is the work by <span class="citation" data-cites="breier2018deeplaser">Breier et al. (<a href="../references.html#ref-breier2018deeplaser" role="doc-biblioref">2018</a>)</span>, where researchers successfully used a laser fault injection attack on a deep neural network deployed on a microcontroller. By heating specific transistors, as shown in <a href="#fig-laser-bitflip" class="quarto-xref">Figure&nbsp;<span>15.6</span></a>. they forced the hardware to skip execution steps, including a ReLU activation function.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-laser-bitflip" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="t!">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-laser-bitflip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/laser_bitflip.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;15.6: Laser fault injection attack on a microcontroller. Source: @breier2018deeplaser."><img src="images/png/laser_bitflip.png" class="img-fluid figure-img" data-fig-pos="t!"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-laser-bitflip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.6: Laser fault injection attack on a microcontroller. Source: <span class="citation" data-cites="breier2018deeplaser">Breier et al. (<a href="../references.html#ref-breier2018deeplaser" role="doc-biblioref">2018</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>This manipulation is illustrated in <a href="#fig-injection" class="quarto-xref">Figure&nbsp;<span>15.7</span></a>, which shows a segment of assembly code implementing the ReLU activation function. Normally, the code compares the most significant bit (MSB) of the accumulator to zero and uses a brge (branch if greater or equal) instruction to skip the assignment if the value is non-positive. However, the fault injection suppresses the branch, causing the processor to always execute the ‚Äúelse‚Äù block. As a result, the neuron‚Äôs output is forcibly zeroed out, regardless of the input value.</p>
<div id="fig-injection" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="b!">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/Fault-injection_demonstrated_with_assembly_code.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;15.7: Fault-injection demonstrated with assembly code. Source: @breier2018deeplaser."><img src="images/png/Fault-injection_demonstrated_with_assembly_code.png" class="img-fluid figure-img" style="width:75.0%" data-fig-pos="b!"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.7: Fault-injection demonstrated with assembly code. Source: <span class="citation" data-cites="breier2018deeplaser">Breier et al. (<a href="../references.html#ref-breier2018deeplaser" role="doc-biblioref">2018</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-breier2018deeplaser" class="csl-entry" role="listitem">
Breier, Jakub, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang Liu. 2018. <span>‚ÄúDeepLaser: Practical Fault Attack on Deep Neural Networks.‚Äù</span> <em>ArXiv Preprint</em> abs/1806.05859 (June): 619‚Äì33. <a href="http://arxiv.org/abs/1806.05859v2">http://arxiv.org/abs/1806.05859v2</a>.
</div></div></figure>
</div>
<p>Fault injection attacks can also be combined with side-channel analysis, where attackers first observe power or timing characteristics to infer model structure or data flow. This reconnaissance allows them to target specific layers or operations, such as activation functions or final decision layers, maximizing the impact of the injected faults.</p>
<p>Embedded and edge ML systems are particularly vulnerable because they often lack physical hardening and operate under resource constraints that limit runtime defenses. Without tamper-resistant packaging or secure hardware enclaves, attackers may gain direct access to system buses and memory, enabling precise fault manipulation. Furthermore, many embedded ML models are designed to be lightweight, leaving them with little redundancy or error correction to recover from induced faults.</p>
<p>Mitigating fault injection requires a multi-layered defense strategy. Physical protections, such as tamper-proof enclosures and design obfuscation, help limit physical access. Anomaly detection techniques can monitor sensor inputs or model outputs for signs of fault-induced inconsistencies <span class="citation" data-cites="hsiao2023mavfi">(<a href="../references.html#ref-hsiao2023mavfi" role="doc-biblioref">Hsiao et al. 2023</a>)</span>. Error-correcting memories and secure firmware can reduce the likelihood of silent corruption. Techniques such as model watermarking may provide traceability if stolen models are later deployed by an adversary.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hsiao2023mavfi" class="csl-entry" role="listitem">
Hsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay Janapa Reddi. 2023. <span>‚ÄúMAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles.‚Äù</span> In <em>2023 Design, Automation &amp;Amp; Test in Europe Conference &amp;Amp; Exhibition (DATE)</em>, 1‚Äì6. IEEE; IEEE. <a href="https://doi.org/10.23919/date56975.2023.10137246">https://doi.org/10.23919/date56975.2023.10137246</a>.
</div></div><p>However, these protections are difficult to implement in cost- and power-constrained environments, where adding cryptographic hardware or redundancy may not be feasible. As a result, achieving resilience to fault injection requires cross-layer design considerations that span electrical, firmware, software, and system architecture levels. Without such holistic design practices, ML systems deployed in the field may remain exposed to these low-cost yet highly effective physical attacks.</p>
</section>
<section id="side-channel-attacks" class="level3 page-columns page-full" data-number="15.6.4">
<h3 data-number="15.6.4" class="anchored" data-anchor-id="side-channel-attacks"><span class="header-section-number">15.6.4</span> Side-Channel Attacks</h3>
<p>Side-channel attacks constitute a class of security breaches that exploit information inadvertently revealed through the physical implementation of computing systems. In contrast to direct attacks that target software or network vulnerabilities, these attacks leverage the system‚Äôs hardware characteristics, including power consumption, electromagnetic emissions, or timing behavior, to extract sensitive information.</p>
<p>The fundamental premise of a side-channel attack is that a device‚Äôs operation can leak information through observable physical signals. Such leaks may originate from the electrical power the device consumes <span class="citation" data-cites="kocher1999differential">(<a href="../references.html#ref-kocher1999differential" role="doc-biblioref">Kocher, Jaffe, and Jun 1999</a>)</span>, the electromagnetic fields it emits <span class="citation" data-cites="gandolfi2001electromagnetic">(<a href="../references.html#ref-gandolfi2001electromagnetic" role="doc-biblioref">Gandolfi, Mourtel, and Olivier 2001</a>)</span>, the time it takes to complete computations, or even the acoustic noise it produces. By carefully measuring and analyzing these signals, attackers can infer internal system states or recover secret data.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kocher1999differential" class="csl-entry" role="listitem">
Kocher, Paul, Joshua Jaffe, and Benjamin Jun. 1999. <span>‚ÄúDifferential Power Analysis.‚Äù</span> In <em>Advances in Cryptology ‚Äî CRYPTO‚Äô 99</em>, 388‚Äì97. Springer; Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/3-540-48405-1\_25">https://doi.org/10.1007/3-540-48405-1\_25</a>.
</div><div id="ref-gandolfi2001electromagnetic" class="csl-entry" role="listitem">
Gandolfi, Karine, Christophe Mourtel, and Francis Olivier. 2001. <span>‚ÄúElectromagnetic Analysis: Concrete Results.‚Äù</span> In <em>Cryptographic Hardware and Embedded Systems ‚Äî CHES 2001</em>, 251‚Äì61. Springer; Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/3-540-44709-1\_21">https://doi.org/10.1007/3-540-44709-1\_21</a>.
</div></div><p>Although these techniques are commonly discussed in cryptography, they are equally relevant to machine learning systems. ML models deployed on hardware accelerators, embedded devices, or edge systems often process sensitive data. Even when these models are protected by secure algorithms or encryption, their physical execution may leak side-channel signals that can be exploited by adversaries.</p>
<p>One of the most widely studied examples involves Advanced Encryption Standard (AES) implementations. While AES is mathematically secure, the physical process of computing its encryption functions leaks measurable signals. Techniques such as Differential Power Analysis (DPA) <span class="citation" data-cites="Kocher2011Intro">(<a href="../references.html#ref-Kocher2011Intro" role="doc-biblioref">Kocher et al. 2011</a>)</span>, Differential Electromagnetic Analysis (DEMA), and Correlation Power Analysis (CPA) exploit these physical signals to recover secret keys.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Kocher2011Intro" class="csl-entry" role="listitem">
Kocher, Paul, Joshua Jaffe, Benjamin Jun, and Pankaj Rohatgi. 2011. <span>‚ÄúIntroduction to Differential Power Analysis.‚Äù</span> <em>Journal of Cryptographic Engineering</em> 1 (1): 5‚Äì27. <a href="https://doi.org/10.1007/s13389-011-0006-y">https://doi.org/10.1007/s13389-011-0006-y</a>.
</div></div><p>A useful example of this attack technique can be seen in a power analysis of a password authentication process. Consider a device that verifies a 5-byte password‚Äîin this case, <code>0x61, 0x52, 0x77, 0x6A, 0x73</code>. During authentication, the device receives each byte sequentially over a serial interface, and its power consumption pattern reveals how the system responds as it processes these inputs.</p>
<p><a href="#fig-encryption" class="quarto-xref">Figure&nbsp;<span>15.8</span></a> shows the device‚Äôs behavior when the correct password is entered. The red waveform captures the serial data stream, marking each byte as it is received. The blue curve records the device‚Äôs power consumption over time. When the full, correct password is supplied, the power profile remains stable and consistent across all five bytes, providing a clear baseline for comparison with failed attempts.</p>
<div id="fig-encryption" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encryption-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/Power_analysis_of_an_encryption_device_with_a_correct_password.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;15.8: Power consumption profile of the device during normal operations with a valid 5-byte password (0x61, 0x52, 0x77, 0x6A, 0x73). The red line represents the serial data being received by the bootloader, which in this figure is receiving the correct bytes. Notice how the blue line, representing power usage during authentication, corresponds to receiving and verifying the bytes. In the next figures, this blue power consumption profile will change. Source: Colin O‚ÄôFlynn."><img src="images/png/Power_analysis_of_an_encryption_device_with_a_correct_password.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encryption-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.8: Power consumption profile of the device during normal operations with a valid 5-byte password (0x61, 0x52, 0x77, 0x6A, 0x73). The red line represents the serial data being received by the bootloader, which in this figure is receiving the correct bytes. Notice how the blue line, representing power usage during authentication, corresponds to receiving and verifying the bytes. In the next figures, this blue power consumption profile will change. Source: Colin O‚ÄôFlynn.
</figcaption>
</figure>
</div>
<p>When an incorrect password is entered, the power analysis chart changes as shown in <a href="#fig-encryption2" class="quarto-xref">Figure&nbsp;<span>15.9</span></a>. In this case, the first three bytes (<code>0x61, 0x52, 0x77</code>) are correct, so the power patterns closely match the correct password up to that point. However, when the fourth byte (<code>0x42</code>) is processed and found to be incorrect, the device halts authentication. This change is reflected in the sudden jump in the blue power line, indicating that the device has stopped processing and entered an error state.</p>
<div id="fig-encryption2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encryption2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/Power_analysis_of_an_encryption_device_with_a_(partially)_wrong_password.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;15.9: Power consumption profile of the device when an incorrect 5-byte password (0x61, 0x52, 0x77, 0x42, 0x42) is entered. The red line represents the serial data received by the bootloader, showing the input bytes being processed. The first three bytes (0x61, 0x52, 0x77) are correct and match the expected password, as indicated by the consistent blue power consumption line. However, upon processing the fourth byte (0x42), a mismatch is detected. The bootloader stops further processing, resulting in a noticeable jump in the blue power consumption line, as the device halts authentication and enters an error state. Source: Colin O‚ÄôFlynn."><img src="images/png/Power_analysis_of_an_encryption_device_with_a_(partially)_wrong_password.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encryption2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.9: Power consumption profile of the device when an incorrect 5-byte password (0x61, 0x52, 0x77, 0x42, 0x42) is entered. The red line represents the serial data received by the bootloader, showing the input bytes being processed. The first three bytes (0x61, 0x52, 0x77) are correct and match the expected password, as indicated by the consistent blue power consumption line. However, upon processing the fourth byte (0x42), a mismatch is detected. The bootloader stops further processing, resulting in a noticeable jump in the blue power consumption line, as the device halts authentication and enters an error state. Source: Colin O‚ÄôFlynn.
</figcaption>
</figure>
</div>
<p><a href="#fig-encryption3" class="quarto-xref">Figure&nbsp;<span>15.10</span></a> shows the case where the password is entirely incorrect (<code>0x30, 0x30, 0x30, 0x30, 0x30</code>). Here, the device detects the mismatch immediately after the first byte and halts processing much earlier. This is again visible in the power profile, where the blue line exhibits a sharp jump following the first byte, reflecting the device‚Äôs early termination of authentication.</p>
<div id="fig-encryption3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-encryption3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/Power_analysis_of_an_encryption_device_with_a_wrong_password.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;15.10: Power consumption profile of the device when an entirely incorrect password (0x30, 0x30, 0x30, 0x30, 0x30) is entered. The blue line shows a sharp jump after processing the first byte, indicating that the device has halted the authentication process. Source: Colin O‚ÄôFlynn."><img src="images/png/Power_analysis_of_an_encryption_device_with_a_wrong_password.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-encryption3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.10: Power consumption profile of the device when an entirely incorrect password (0x30, 0x30, 0x30, 0x30, 0x30) is entered. The blue line shows a sharp jump after processing the first byte, indicating that the device has halted the authentication process. Source: Colin O‚ÄôFlynn.
</figcaption>
</figure>
</div>
<p>These examples demonstrate how attackers can exploit observable power consumption differences to reduce the search space and eventually recover secret data through brute-force analysis. For a more detailed walkthrough, <a href="#vid-powerattack" class="quarto-xref">Video&nbsp;<span>15.3</span></a> provides a step-by-step demonstration of how these attacks are performed.</p>
<div id="vid-powerattack" class="callout callout-style-default callout-important callout-titled" title="Power Attack">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important&nbsp;15.3: Power Attack
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/2iDLfuEBcs8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
<p>Such attacks are not limited to cryptographic systems. Machine learning applications face similar risks. For example, an ML-based speech recognition system processing voice commands on a local device could leak timing or power signals that reveal which commands are being processed. Even subtle acoustic or electromagnetic emissions may expose operational patterns that an adversary could exploit to infer user behavior.</p>
<p>Historically, side-channel attacks have been used to bypass even the most secure cryptographic systems. In the 1960s, British intelligence agency MI5 famously exploited acoustic emissions from a cipher machine in the Egyptian Embassy <span class="citation" data-cites="Burnet1989Spycatcher">(<a href="../references.html#ref-Burnet1989Spycatcher" role="doc-biblioref">Burnet and Thomas 1989</a>)</span>. By capturing the mechanical clicks of the machine‚Äôs rotors, MI5 analysts were able to dramatically reduce the complexity of breaking encrypted messages. This early example illustrates that side-channel vulnerabilities are not confined to the digital age but are rooted in the physical nature of computation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Burnet1989Spycatcher" class="csl-entry" role="listitem">
Burnet, David, and Richard Thomas. 1989. <span>‚ÄúSpycatcher: The Commodification of Truth.‚Äù</span> <em>Journal of Law and Society</em> 16 (2): 210. <a href="https://doi.org/10.2307/1410360">https://doi.org/10.2307/1410360</a>.
</div><div id="ref-Asonov2004Keyboard" class="csl-entry" role="listitem">
Asonov, D., and R. Agrawal. n.d. <span>‚ÄúKeyboard Acoustic Emanations.‚Äù</span> In <em>IEEE Symposium on Security and Privacy, 2004. Proceedings. 2004</em>, 3‚Äì11. IEEE; IEEE. <a href="https://doi.org/10.1109/secpri.2004.1301311">https://doi.org/10.1109/secpri.2004.1301311</a>.
</div><div id="ref-gnad2017voltage" class="csl-entry" role="listitem">
Gnad, Dennis R. E., Fabian Oboril, and Mehdi B. Tahoori. 2017. <span>‚ÄúVoltage Drop-Based Fault Attacks on FPGAs Using Valid Bitstreams.‚Äù</span> In <em>2017 27th International Conference on Field Programmable Logic and Applications (FPL)</em>, 1‚Äì7. IEEE; IEEE. <a href="https://doi.org/10.23919/fpl.2017.8056840">https://doi.org/10.23919/fpl.2017.8056840</a>.
</div><div id="ref-zhao2018fpga" class="csl-entry" role="listitem">
Zhao, Mark, and G. Edward Suh. 2018. <span>‚ÄúFPGA-Based Remote Power Side-Channel Attacks.‚Äù</span> In <em>2018 IEEE Symposium on Security and Privacy (SP)</em>, 229‚Äì44. IEEE; IEEE. <a href="https://doi.org/10.1109/sp.2018.00049">https://doi.org/10.1109/sp.2018.00049</a>.
</div></div><p>Today, these techniques have advanced to include attacks such as keyboard eavesdropping <span class="citation" data-cites="Asonov2004Keyboard">(<a href="../references.html#ref-Asonov2004Keyboard" role="doc-biblioref">Asonov and Agrawal, n.d.</a>)</span>, power analysis on cryptographic hardware <span class="citation" data-cites="gnad2017voltage">(<a href="../references.html#ref-gnad2017voltage" role="doc-biblioref">Gnad, Oboril, and Tahoori 2017</a>)</span>, and voltage-based attacks on ML accelerators <span class="citation" data-cites="zhao2018fpga">(<a href="../references.html#ref-zhao2018fpga" role="doc-biblioref">Zhao and Suh 2018</a>)</span>. Timing attacks, electromagnetic leakage, and thermal emissions continue to provide adversaries with indirect channels for observing system behavior.</p>
<p>Machine learning systems deployed on specialized accelerators or embedded platforms are especially at risk. Attackers may exploit side-channel signals to infer model structure, steal parameters, or reconstruct private training data. As ML becomes increasingly deployed in cloud, edge, and embedded environments, these side-channel vulnerabilities pose significant challenges to system security.</p>
<p>Understanding the persistence and evolution of side-channel attacks is essential for building resilient machine learning systems. By recognizing that where there is a signal, there is potential for exploitation, system designers can begin to address these risks through a combination of hardware shielding, algorithmic defenses, and operational safeguards.</p>
</section>
<section id="leaky-interfaces" class="level3" data-number="15.6.5">
<h3 data-number="15.6.5" class="anchored" data-anchor-id="leaky-interfaces"><span class="header-section-number">15.6.5</span> Leaky Interfaces</h3>
<p>Interfaces in computing systems are essential for enabling communication, diagnostics, and updates. However, these same interfaces can become significant security vulnerabilities when they unintentionally expose sensitive information or accept unverified inputs. Such leaky interfaces often go unnoticed during system design, yet they provide attackers with powerful entry points to extract data, manipulate functionality, or introduce malicious code.</p>
<p>A leaky interface is any access point that reveals more information than intended, often because of weak authentication, lack of encryption, or inadequate isolation. These issues have been widely demonstrated across consumer, medical, and industrial systems.</p>
<p>For example, many WiFi-enabled baby monitors have been found to expose unsecured remote access ports, allowing attackers to intercept live audio and video feeds from inside private homes. Similarly, researchers have identified wireless vulnerabilities in pacemakers that could allow attackers to manipulate cardiac functions if exploited, raising life-threatening safety concerns.</p>
<p>A notable case involving smart lightbulbs demonstrated that accessible debug ports left on production devices leaked unencrypted WiFi credentials. This security oversight provided attackers with a pathway to infiltrate home networks without needing to bypass standard security mechanisms. In the automotive domain, unsecured OBD-II diagnostic ports have allowed attackers to manipulate braking and steering functions in connected vehicles, as demonstrated in the well-known Jeep Cherokee hack.</p>
<p>While these examples do not target machine learning systems directly, they illustrate architectural patterns that are highly relevant to ML-enabled devices. Consider a smart home security system that uses machine learning to detect user routines and automate responses. Such a system may include a maintenance or debug interface for software updates. If this interface lacks proper authentication or transmits data unencrypted, attackers on the same network could gain unauthorized access. This intrusion could expose user behavior patterns, compromise model integrity, or disable security features altogether.</p>
<p>Leaky interfaces in ML systems can also expose training data, model parameters, or intermediate outputs. Such exposure can enable attackers to craft adversarial examples, steal proprietary models, or reverse-engineer system behavior. Worse still, these interfaces may allow attackers to tamper with firmware, introducing malicious code that disables devices or recruits them into botnets.</p>
<p>Mitigating these risks requires multi-layered defenses. Technical safeguards such as strong authentication, encrypted communications, and runtime anomaly detection are essential. Organizational practices such as interface inventories, access control policies, and ongoing audits are equally important. Adopting a zero-trust architecture, where no interface is trusted by default, further reduces exposure by limiting access to only what is strictly necessary.</p>
<p>For designers of ML-powered systems, securing interfaces must be a first-class concern alongside algorithmic and data-centric design. Whether the system operates in the cloud, on the edge, or in embedded environments, failure to secure these access points risks undermining the entire system‚Äôs trustworthiness.</p>
</section>
<section id="counterfeit-hardware" class="level3" data-number="15.6.6">
<h3 data-number="15.6.6" class="anchored" data-anchor-id="counterfeit-hardware"><span class="header-section-number">15.6.6</span> Counterfeit Hardware</h3>
<p>Machine learning systems depend on the reliability and security of the hardware on which they run. Yet, in today‚Äôs globalized hardware ecosystem, the risk of counterfeit or cloned hardware has emerged as a serious threat to system integrity. Counterfeit components refer to unauthorized reproductions of genuine parts, designed to closely imitate their appearance and functionality. These components can enter machine learning systems through complex procurement and manufacturing processes that span multiple vendors and regions.</p>
<p>A single lapse in component sourcing can introduce counterfeit hardware into critical systems. For example, a facial recognition system deployed for secure facility access might unknowingly rely on counterfeit processors. These unauthorized components could fail to process biometric data correctly or introduce hidden vulnerabilities that allow attackers to bypass authentication controls.</p>
<p>The risks posed by counterfeit hardware are multifaceted. From a reliability perspective, such components often degrade faster, perform unpredictably, or fail under load due to substandard manufacturing. From a security perspective, counterfeit hardware may include hidden backdoors or malicious circuitry, providing attackers with undetectable pathways to compromise machine learning systems. A cloned network router installed in a data center, for instance, could silently intercept model predictions or user data, undermining both system security and user privacy.</p>
<p>Legal and regulatory risks further compound the problem. Organizations that unknowingly integrate counterfeit components into their ML systems may face serious legal consequences, including penalties for violating safety, privacy, or cybersecurity regulations. This is particularly concerning in sectors such as healthcare and finance, where compliance with industry standards is non-negotiable.</p>
<p>Economic pressures often incentivize sourcing from lower-cost suppliers without rigorous verification, increasing the likelihood of counterfeit parts entering production systems. Detection is especially challenging, as counterfeit components are designed to mimic legitimate ones. Identifying them may require specialized equipment or forensic analysis, making prevention far more practical than remediation.</p>
<p>The stakes are particularly high in machine learning applications that require high reliability and low latency, such as real-time decision-making in autonomous vehicles, industrial automation, or critical healthcare diagnostics. Hardware failure in these contexts can lead not only to system downtime but also to significant safety risks.</p>
<p>As machine learning continues to expand into safety-critical and high-value applications, counterfeit hardware presents a growing risk that must be recognized and addressed. Organizations must treat hardware trustworthiness as a fundamental design requirement, on par with algorithmic accuracy and data security, to ensure that ML systems can operate reliably and securely in the real world.</p>
</section>
<section id="supply-chain-risks" class="level3" data-number="15.6.7">
<h3 data-number="15.6.7" class="anchored" data-anchor-id="supply-chain-risks"><span class="header-section-number">15.6.7</span> Supply Chain Risks</h3>
<p>While counterfeit hardware presents a serious challenge, it is only one part of the larger problem of securing the global hardware supply chain. Machine learning systems are built from components that pass through complex supply networks involving design, fabrication, assembly, distribution, and integration. Each of these stages presents opportunities for tampering, substitution, or counterfeiting‚Äîoften without the knowledge of those deploying the final system.</p>
<p>Malicious actors can exploit these vulnerabilities in various ways. A contracted manufacturer might unknowingly receive recycled electronic waste that has been relabeled as new components. A distributor might deliberately mix cloned parts into otherwise legitimate shipments. Insiders at manufacturing facilities might embed hardware Trojans that are nearly impossible to detect once the system is deployed. Advanced counterfeits can be particularly deceptive, with refurbished or repackaged components designed to pass visual inspection while concealing inferior or malicious internals.</p>
<p>Identifying such compromises typically requires sophisticated analysis, including micrography, X-ray screening, and functional testing. However, these methods are costly and impractical for large-scale procurement. As a result, many organizations deploy systems without fully verifying the authenticity and security of every component.</p>
<p>The risks extend beyond individual devices. Machine learning systems often rely on heterogeneous hardware platforms, integrating CPUs, GPUs, memory, and specialized accelerators sourced from a global supply base. Any compromise in one part of this chain can undermine the security of the entire system. These risks are further amplified when systems operate in shared or multi-tenant environments, such as cloud data centers or federated edge networks, where hardware-level isolation is critical to preventing cross-tenant attacks.</p>
<p>The 2018 Bloomberg Businessweek report alleging that Chinese state actors inserted spy chips into Supermicro server motherboards brought these risks to mainstream attention. While the claims remain disputed, the story underscored the industry‚Äôs limited visibility into its own hardware supply chains. Companies often rely on complex, opaque manufacturing and distribution networks, leaving them vulnerable to hidden compromises. Over-reliance on single manufacturers or regions, including the semiconductor industry‚Äôs reliance on TSMC, further concentrates this risk. This recognition has driven policy responses like the U.S. <a href="https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2024/08/09/fact-sheet-two-years-after-the-chips-and-science-act-biden-%E2%81%A0harris-administration-celebrates-historic-achievements-in-bringing-semiconductor-supply-chains-home-creating-jobs-supporting-inn/">CHIPS and Science Act</a>, which aims to bring semiconductor production onshore and strengthen supply chain resilience.</p>
<p>Securing machine learning systems requires moving beyond trust-by-default models toward zero-trust supply chain practices. This includes screening suppliers, validating component provenance, implementing tamper-evident protections, and continuously monitoring system behavior for signs of compromise. Building fault-tolerant architectures that detect and contain failures provides an additional layer of defense.</p>
<p>Ultimately, supply chain risks must be treated as a first-class concern in ML system design. Trust in the computational models and data pipelines that power machine learning depends fundamentally on the trustworthiness of the hardware on which they run. Without securing the hardware foundation, even the most sophisticated models remain vulnerable to compromise.</p>
</section>
<section id="case-study-the-supermicro-hardware-security-controversy" class="level3 page-columns page-full" data-number="15.6.8">
<h3 data-number="15.6.8" class="anchored" data-anchor-id="case-study-the-supermicro-hardware-security-controversy"><span class="header-section-number">15.6.8</span> Case Study: The Supermicro Hardware Security Controversy</h3>
<p>In 2018, Bloomberg Businessweek published a widely discussed report alleging that Chinese state-sponsored actors had secretly implanted tiny surveillance chips on server motherboards manufactured by Supermicro <span class="citation" data-cites="TheBigHa77:online">(<a href="../references.html#ref-TheBigHa77:online" role="doc-biblioref">Robertson and Riley 2018</a>)</span>. These compromised servers were reportedly deployed by more than 30 major companies, including Apple and Amazon. The chips, described as no larger than a grain of rice, were said to provide attackers with backdoor access to sensitive data and systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-TheBigHa77:online" class="csl-entry" role="listitem">
Robertson, J., and M. Riley. 2018. <span>‚ÄúThe Big Hack: How China Used a Tiny Chip to Infiltrate u.s. Companies - Bloomberg.‚Äù</span><a href="
    https://www.bloomberg.com/news/features/2018-10-04/the-big-hack-how-china-used-a-tiny-chip-to-infiltrate-america-s-top-companies
  ">https://www.bloomberg.com/news/features/2018-10-04/the-big-hack-how-china-used-a-tiny-chip-to-infiltrate-america-s-top-companies </a>.
</div></div><p>The allegations sparked immediate concern across the technology industry, raising questions about the security of global supply chains and the potential for state-level hardware manipulation. However, the companies named in the report publicly denied the claims. Apple, Amazon, and Supermicro stated that they had found no evidence of the alleged implants after conducting thorough internal investigations. Industry experts and government agencies also expressed skepticism, noting the lack of verifiable technical evidence presented in the report.</p>
<p>Despite these denials, the story had a lasting impact on how organizations and policymakers view hardware supply chain security. Whether or not the specific claims were accurate, the report highlighted the real and growing concern that hardware supply chains are difficult to fully audit and secure. It underscored how geopolitical tensions, manufacturing outsourcing, and the complexity of modern hardware ecosystems make it increasingly challenging to guarantee the integrity of hardware components.</p>
<p>The Supermicro case illustrates a broader truth: once a product enters a complex global supply chain, it becomes difficult to ensure that every component is free from tampering or unauthorized modification. This risk is particularly acute for machine learning systems, which depend on a wide range of hardware accelerators, memory modules, and processing units sourced from multiple vendors across the globe.</p>
<p>In response to these risks, both industry and government stakeholders have begun to invest in supply chain security initiatives. The U.S. government‚Äôs CHIPS and Science Act is one such effort, aiming to bring semiconductor manufacturing back onshore to improve transparency and reduce dependency on foreign suppliers. While these efforts are valuable, they do not fully eliminate supply chain risks. They must be complemented by technical safeguards, such as component validation, runtime monitoring, and fault-tolerant system design.</p>
<p>The Supermicro controversy serves as a cautionary tale for the machine learning community. It demonstrates that hardware security cannot be taken for granted, even when working with reputable suppliers. Ensuring the integrity of ML systems requires rigorous attention to the entire hardware lifecycle‚Äîfrom design and fabrication to deployment and maintenance. This case reinforces the need for organizations to adopt comprehensive supply chain security practices as a foundational element of trustworthy ML system design.</p>
</section>
</section>
<section id="defensive-strategies" class="level2 page-columns page-full" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="defensive-strategies"><span class="header-section-number">15.7</span> Defensive Strategies</h2>
<p>Designing secure and privacy-preserving machine learning systems requires more than identifying individual threats. It demands a layered defense strategy, which begins with protecting the data that powers models and extends through model design, deployment safeguards, runtime monitoring, and ultimately, the hardware that anchors trust. Each layer contributes to the system‚Äôs overall resilience and must be tailored to the specific threat surfaces introduced by machine learning workflows. Unlike traditional software systems, ML systems are particularly vulnerable to input manipulation, data leakage, model extraction, and runtime abuse‚Äîall amplified by tight coupling between data, model behavior, and infrastructure.</p>
<p>This section presents a structured framework for defensive strategies, progressing from data-centric protections to infrastructure-level enforcement. These strategies span differential privacy and federated learning, robust model architectures, secure deployment pipelines, runtime validation and monitoring, and hardware-based trust anchors such as secure boot and TEEs. By integrating safeguards across layers, organizations can build ML systems that not only perform reliably but also withstand adversarial pressure in production environments.</p>
<p><a href="#fig-defense-stack" class="quarto-xref">Figure&nbsp;<span>15.11</span></a> shows a layered defense stack for machine learning systems. The stack progresses from foundational hardware-based security mechanisms to runtime system protections, model-level controls, and privacy-preserving techniques at the data level. Each layer builds on the trust guarantees of the layer below it, forming an end-to-end strategy for deploying ML systems securely. We will progressively explore each of these layers, highlighting their roles in securing machine learning systems against a range of threats.</p>
<div id="fig-defense-stack" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-defense-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="88e9653fa4e5b8f70d63756894b6c8577a48f30c.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;15.11: A layered defense stack for machine learning systems."><img src="privacy_security_files/mediabag/88e9653fa4e5b8f70d63756894b6c8577a48f30c.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-defense-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.11: A layered defense stack for machine learning systems.
</figcaption>
</figure>
</div>
<section id="data-privacy-techniques" class="level3 page-columns page-full" data-number="15.7.1">
<h3 data-number="15.7.1" class="anchored" data-anchor-id="data-privacy-techniques"><span class="header-section-number">15.7.1</span> Data Privacy Techniques</h3>
<p>Protecting the privacy of individuals whose data fuels machine learning systems is a foundational requirement for trustworthy AI. Unlike traditional systems where data is often masked or anonymized before processing, ML workflows typically rely on access to raw, high-fidelity data to train effective models. This tension between utility and privacy has motivated a diverse set of techniques aimed at minimizing data exposure while preserving learning performance.</p>
<section id="differential-privacy" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="differential-privacy">Differential Privacy</h4>
<p>One of the most widely adopted frameworks for formalizing privacy guarantees is differential privacy (DP). DP provides a rigorous mathematical definition of privacy loss, ensuring that the inclusion or exclusion of a single individual‚Äôs data has a provably limited effect on the model‚Äôs output. A randomized algorithm <span class="math inline">\(\mathcal{A}\)</span> is said to be <span class="math inline">\(\epsilon\)</span>-differentially private if, for all adjacent datasets <span class="math inline">\(D\)</span> and <span class="math inline">\(D'\)</span> differing in one record, and for all outputs <span class="math inline">\(S \subseteq \text{Range}(\mathcal{A})\)</span>, the following holds: <span class="math display">\[
\Pr[\mathcal{A}(D) \in S] \leq e^{\epsilon} \Pr[\mathcal{A}(D') \in S]
\]</span></p>
<p>This bound ensures that the algorithm‚Äôs behavior remains statistically indistinguishable regardless of whether any individual‚Äôs data is present, thereby limiting the information that can be inferred about that individual. In practice, DP is implemented by adding calibrated noise to model updates or query responses, using mechanisms such as the Laplace or Gaussian mechanism. Training techniques like differentially private stochastic gradient descent (DP-SGD) integrate noise into the optimization process to ensure per-iteration privacy guarantees.</p>
<p>While differential privacy offers strong theoretical assurances, it introduces a trade-off between privacy and utility. Increasing the noise to reduce <span class="math inline">\(\epsilon\)</span> may degrade model accuracy, especially in low-data regimes or fine-grained classification tasks. Consequently, DP is often applied selectively‚Äîeither during training on sensitive datasets or at inference when returning aggregate statistics‚Äîto balance privacy with performance goals <span class="citation" data-cites="dwork2014algorithmic">(<a href="../references.html#ref-dwork2014algorithmic" role="doc-biblioref">Dwork and Roth 2013</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dwork2014algorithmic" class="csl-entry" role="listitem">
Dwork, Cynthia, and Aaron Roth. 2013. <span>‚ÄúThe Algorithmic Foundations of Differential Privacy.‚Äù</span> <em>Foundations and Trends in Theoretical Computer Science</em> 9 (3-4): 211‚Äì407. <a href="https://doi.org/10.1561/0400000042">https://doi.org/10.1561/0400000042</a>.
</div></div></section>
<section id="federated-learning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="federated-learning">Federated Learning</h4>
<p>Complementary to DP, federated learning (FL) reduces privacy risks by restructuring the learning process itself. Rather than aggregating raw data at a central location, FL distributes the training across a set of client devices, each holding local data <span class="citation" data-cites="mcmahan2017communicationefficient">(<a href="../references.html#ref-mcmahan2017communicationefficient" role="doc-biblioref">McMahan et al. 2017</a>)</span>. Clients compute model updates locally and share only parameter deltas with a central server for aggregation: <span class="math display">\[
\theta_{t+1} \leftarrow \sum_{k=1}^{K} \frac{n_k}{n} \cdot \theta_{t}^{(k)}
\]</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-mcmahan2017communicationefficient" class="csl-entry" role="listitem">
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag√ºera y Arcas. 2017. <span>‚ÄúCommunication-Efficient Learning of Deep Networks from Decentralized Data.‚Äù</span> In <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 1273‚Äì82. PMLR. <a href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.
</div></div><p>Here, <span class="math inline">\(\theta_{t}^{(k)}\)</span> represents the model update from client <span class="math inline">\(k\)</span>, <span class="math inline">\(n_k\)</span> the number of samples held by that client, and <span class="math inline">\(n\)</span> the total number of samples across all clients. This weighted aggregation allows the global model to learn from distributed data without direct access to it. While FL reduces the exposure of raw data, it still leaks information through gradients, motivating the use of DP, secure aggregation, and hardware-based protections in federated settings.</p>
<p>To address scenarios requiring computation on encrypted data, homomorphic encryption (HE) and secure multiparty computation (SMPC) allow models to perform inference or training over encrypted inputs. In the case of HE, operations on ciphertexts correspond to operations on plaintexts, enabling encrypted inference: <span class="math display">\[
\text{Enc}(f(x)) = f(\text{Enc}(x))
\]</span></p>
<p>This property supports privacy-preserving computation in untrusted environments, such as cloud inference over sensitive health or financial records. However, the computational cost of HE remains high, making it more suitable for fixed-function models and low-latency batch tasks. SMPC, by contrast, distributes the computation across multiple parties such that no single party learns the complete input or output. This is particularly useful in joint training across institutions with strict data-use policies, such as hospitals or banks.</p>
</section>
<section id="synthetic-data-generation" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="synthetic-data-generation">Synthetic Data Generation</h4>
<p>A more pragmatic and increasingly popular alternative involves the use of synthetic data generation. By training generative models on real datasets and sampling new instances from the learned distribution, organizations can create datasets that approximate the statistical properties of the original data without retaining identifiable details <span class="citation" data-cites="goncalves2020generation">(<a href="../references.html#ref-goncalves2020generation" role="doc-biblioref">Goncalves et al. 2020</a>)</span>. While this approach reduces the risk of direct reidentification, it does not offer formal privacy guarantees unless combined with DP constraints during generation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-goncalves2020generation" class="csl-entry" role="listitem">
Goncalves, Andre, Priyadip Ray, Braden Soper, Jennifer Stevens, Linda Coyle, and Ana Paula Sales. 2020. <span>‚ÄúGeneration and Evaluation of Synthetic Patient Data.‚Äù</span> <em>BMC Medical Research Methodology</em> 20 (1): 1‚Äì40. <a href="https://doi.org/10.1186/s12874-020-00977-1">https://doi.org/10.1186/s12874-020-00977-1</a>.
</div></div><p>Together, these techniques reflect a shift from isolating data as the sole path to privacy toward embedding privacy-preserving mechanisms into the learning process itself. Each method offers distinct guarantees and trade-offs depending on the application context, threat model, and regulatory constraints. Effective system design often combines multiple approaches, such as applying differential privacy within a federated learning setup, or employing homomorphic encryption for critical inference stages, to build ML systems that are both useful and respectful of user privacy.</p>
</section>
<section id="comparative-properties" class="level4">
<h4 class="anchored" data-anchor-id="comparative-properties">Comparative Properties</h4>
<p>These privacy-preserving techniques differ not only in the guarantees they offer but also in their system-level implications. For practitioners, the choice of mechanism depends on factors such as computational constraints, deployment architecture, and regulatory requirements.</p>
<p><a href="#tbl-privacy-technique-comparison" class="quarto-xref">Table&nbsp;<span>15.6</span></a> summarizes the comparative properties of these methods, focusing on privacy strength, runtime overhead, maturity, and common use cases. Understanding these trade-offs is essential for designing privacy-aware machine learning systems that operate under real-world constraints.</p>
<div id="tbl-privacy-technique-comparison" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-privacy-technique-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.6: Comparison of data privacy techniques across system-level dimensions.
</figcaption>
<div aria-describedby="tbl-privacy-technique-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 15%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Technique</th>
<th style="text-align: left;">Privacy Guarantee</th>
<th style="text-align: left;">Computational Overhead</th>
<th style="text-align: left;">Deployment Maturity</th>
<th style="text-align: left;">Typical Use Case</th>
<th style="text-align: left;">Trade-offs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Differential Privacy</td>
<td style="text-align: left;">Formal (Œµ-DP)</td>
<td style="text-align: left;">Moderate to High</td>
<td style="text-align: left;">Production</td>
<td style="text-align: left;">Training with sensitive or regulated data</td>
<td style="text-align: left;">Reduced accuracy; careful tuning of Œµ/noise required to balance utility and protection</td>
</tr>
<tr class="even">
<td style="text-align: left;">Federated Learning</td>
<td style="text-align: left;">Structural</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">Production</td>
<td style="text-align: left;">Cross-device or cross-org collaborative learning</td>
<td style="text-align: left;">Gradient leakage risk; requires secure aggregation and orchestration infrastructure</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Homomorphic Encryption</td>
<td style="text-align: left;">Strong (Encrypted)</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Experimental</td>
<td style="text-align: left;">Inference in untrusted cloud environments</td>
<td style="text-align: left;">High latency and memory usage; suitable for limited-scope inference on fixed-function models</td>
</tr>
<tr class="even">
<td style="text-align: left;">Secure MPC</td>
<td style="text-align: left;">Strong (Distributed)</td>
<td style="text-align: left;">Very High</td>
<td style="text-align: left;">Experimental</td>
<td style="text-align: left;">Joint training across mutually untrusted parties</td>
<td style="text-align: left;">Expensive communication; challenging to scale to many participants or deep models</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Synthetic Data</td>
<td style="text-align: left;">Weak (if standalone)</td>
<td style="text-align: left;">Low to Moderate</td>
<td style="text-align: left;">Emerging</td>
<td style="text-align: left;">Data sharing, benchmarking without direct access to raw data</td>
<td style="text-align: left;">May leak sensitive patterns if training process is not differentially private or audited for fidelity</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="secure-model-design" class="level3 page-columns page-full" data-number="15.7.2">
<h3 data-number="15.7.2" class="anchored" data-anchor-id="secure-model-design"><span class="header-section-number">15.7.2</span> Secure Model Design</h3>
<p>Security begins at the design phase of a machine learning system. While downstream mechanisms such as access control and encryption protect models once deployed, many vulnerabilities can be mitigated earlier‚Äîthrough architectural choices, defensive training strategies, and mechanisms that embed resilience directly into the model‚Äôs structure or behavior. By considering security as a design constraint, system developers can reduce the model‚Äôs exposure to attacks, limit its ability to leak sensitive information, and provide verifiable ownership protection.</p>
<p>One important design strategy is to build robust-by-construction models that reduce the risk of exploitation at inference time. For instance, models with confidence calibration or abstention mechanisms can be trained to avoid making predictions when input uncertainty is high. These techniques can help prevent overconfident misclassifications in response to adversarial or out-of-distribution inputs. Models may also employ output smoothing, regularizing the output distribution to reduce sharp decision boundaries that are especially susceptible to adversarial perturbations.</p>
<p>Certain application contexts may also benefit from choosing simpler or compressed architectures. While not universally appropriate, limiting model capacity can reduce opportunities for memorization of sensitive training data and complicate efforts to reverse-engineer the model from output behavior. For embedded or on-device settings, smaller models are also easier to secure, as they typically require less memory and compute, lowering the likelihood of side-channel leakage or runtime manipulation.</p>
<p>Another design-stage consideration is the use of model watermarking, a technique for embedding verifiable ownership signatures directly into the model‚Äôs parameters or output behavior <span class="citation" data-cites="adi2018turning">(<a href="../references.html#ref-adi2018turning" role="doc-biblioref">Adi et al. 2018</a>)</span>. A watermark might be implemented, for example, as a hidden response pattern triggered by specific inputs, or as a parameter-space perturbation that does not affect accuracy but is statistically identifiable. These watermarks can be used to detect and prove misuse of stolen models in downstream deployments. Watermarking strategies must be carefully designed to remain robust to model compression, fine-tuning, and format conversion.</p>
<div class="no-row-height column-margin column-container"><div id="ref-adi2018turning" class="csl-entry" role="listitem">
Adi, Yossi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. 2018. <span>‚ÄúTurning Your Weakness into a Strength: Watermarking Deep Neural Networks by Backdooring.‚Äù</span> In <em>27th USENIX Security Symposium (USENIX Security 18)</em>, 1615‚Äì31.
</div></div><p>For example, in a keyword spotting system deployed on embedded hardware for voice activation (e.g., ‚ÄúHey Alexa‚Äù or ‚ÄúOK Google‚Äù), a secure design might use a lightweight convolutional neural network with confidence calibration to avoid false activations on uncertain audio. The model might also include an abstention threshold, below which it produces no activation at all. To protect intellectual property, a designer could embed a watermark by training the model to respond with a unique label only when presented with a specific, unused audio trigger known only to the developer. These design choices not only improve robustness and accountability, but also support future verification in case of IP disputes or performance failures in the field.</p>
<p>In high-risk applications, such as medical diagnosis, autonomous vehicles, or financial decision systems, designers may also prioritize interpretable model architectures, such as decision trees, rule-based classifiers, or sparsified networks, to enhance system auditability. These models are often easier to understand and explain, making it simpler to identify potential vulnerabilities or biases. Using interpretable models allows developers to provide clearer insights into how the system arrived at a particular decision, which is crucial for building trust with users and regulators.</p>
<p>Model design choices often reflect trade-offs between accuracy, robustness, transparency, and system complexity. However, when viewed from a systems perspective, early-stage design decisions frequently yield the highest leverage for long-term security. They shape what the model can learn, how it behaves under uncertainty, and what guarantees can be made about its provenance, interpretability, and resilience.</p>
</section>
<section id="secure-model-deployment" class="level3 page-columns page-full" data-number="15.7.3">
<h3 data-number="15.7.3" class="anchored" data-anchor-id="secure-model-deployment"><span class="header-section-number">15.7.3</span> Secure Model Deployment</h3>
<p>Protecting machine learning models from theft, abuse, and unauthorized manipulation requires security considerations throughout both the design and deployment phases. A model‚Äôs vulnerability is not solely determined by its training procedure or architecture, but also by how it is serialized, packaged, deployed, and accessed during inference. As models are increasingly embedded into edge devices, served through public APIs, or integrated into multi-tenant platforms, robust security practices are essential to ensure the integrity, confidentiality, and availability of model behavior.</p>
<p>This section addresses security mechanisms across three key stages: model design, secure packaging and serialization, and deployment and access control.</p>
<p>From a design perspective, architectural choices can reduce a model‚Äôs exposure to adversarial manipulation and unauthorized use. For example, models can incorporate confidence calibration or abstention mechanisms that allow them to reject uncertain or anomalous inputs rather than producing potentially misleading outputs. Designing models with simpler or compressed architectures can also reduce the risk of reverse engineering or information leakage through side-channel analysis. In some cases, model designers may embed imperceptible watermarks, which are unique signatures embedded in the parameters or behavior of the model, that can later be used to demonstrate ownership in cases of misappropriation <span class="citation" data-cites="uchida2017embedding">(<a href="../references.html#ref-uchida2017embedding" role="doc-biblioref">Uchida et al. 2017</a>)</span>. These design-time protections are particularly important for commercially valuable models, where intellectual property rights are at stake.</p>
<div class="no-row-height column-margin column-container"><div id="ref-uchida2017embedding" class="csl-entry" role="listitem">
Uchida, Yusuke, Yuki Nagai, Shigeyuki Sakazawa, and Shin‚Äôichi Satoh. 2017. <span>‚ÄúEmbedding Watermarks into Deep Neural Networks.‚Äù</span> In <em>Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</em>, 269‚Äì77. ACM; ACM. <a href="https://doi.org/10.1145/3078971.3078974">https://doi.org/10.1145/3078971.3078974</a>.
</div></div><p>Once training is complete, the model must be securely packaged for deployment. Storing models in plaintext formats, including unencrypted ONNX or PyTorch checkpoint files, can expose internal structures and parameters to attackers with access to the file system or memory. To mitigate this risk, models should be encrypted, obfuscated, or wrapped in secure containers. Decryption keys should be made available only at runtime and only within trusted environments. Additional mechanisms, such as quantization-aware encryption or integrity-checking wrappers, can prevent tampering and offline model theft.</p>
<p>Deployment environments must also enforce strong access control policies to ensure that only authorized users and services can interact with inference endpoints. Authentication protocols, including OAuth tokens, mutual TLS, or API keys, should be combined with role-based access control (RBAC) to restrict access according to user roles and operational context. For instance, OpenAI‚Äôs hosted model APIs require users to include an OPENAI_API_KEY when submitting inference requests. This key authenticates the client and enables the backend to enforce usage policies, monitor for abuse, and log access patterns. A simplified example of secure usage is shown in <a href="#lst-openai-api" class="quarto-xref">Listing&nbsp;<span>15.1</span></a>, where the API key is securely loaded from an environment variable before being used to authenticate requests.</p>
<div id="lst-openai-api" class="callout callout-style-default callout-important callout-titled" title="Example of securely loading an API key for OpenAI's GPT-4 model. The API key is retrieved from an environment variable to avoid hardcoding sensitive information in the source code.">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Listing&nbsp;15.1: Example of securely loading an API key for OpenAI‚Äôs GPT-4 model. The API key is retrieved from an environment variable to avoid hardcoding sensitive information in the source code.
</div>
</div>
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> openai</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Securely load the API key from an environment variable</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>openai.api_key <span class="op">=</span> os.getenv(<span class="st">"OPENAI_API_KEY"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Submit a prompt to the model</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"gpt-4"</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"content"</span>: (</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                <span class="st">"Summarize the principles of "</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>                <span class="st">"differential privacy."</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(response.choices[<span class="dv">0</span>].message[<span class="st">"content"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>In this example, the API key is retrieved from an environment variable‚Äîavoiding the security risk of hardcoding it into source code or exposing it to the client side. Such key-based access control mechanisms are simple to implement but require careful key management and monitoring to prevent misuse, unauthorized access, or model extraction.</p>
<p>Beyond endpoint access, the integrity of the deployment pipeline itself must also be protected. Continuous integration and deployment (CI/CD) workflows that automate model updates should enforce cryptographic signing of artifacts, dependency validation, and infrastructure hardening. Without these controls, adversaries could inject malicious models or alter existing ones during the build and deployment process. Verifying model signatures and maintaining audit trails helps ensure that only authorized models are deployed into production.</p>
<p>When applied together, these practices protect against a range of threats‚Äîfrom model theft and unauthorized inference access to tampering during deployment and output manipulation at runtime. No single mechanism suffices in isolation, but a layered strategy, beginning at the design phase and extending through deployment, provides a strong foundation for securing machine learning systems under real-world conditions.</p>
</section>
<section id="system-level-monitoring" class="level3 page-columns page-full" data-number="15.7.4">
<h3 data-number="15.7.4" class="anchored" data-anchor-id="system-level-monitoring"><span class="header-section-number">15.7.4</span> System-level Monitoring</h3>
<p>Even with robust design and deployment safeguards, machine learning systems remain vulnerable to runtime threats. Attackers may craft inputs that bypass validation, exploit model behavior, or target system-level infrastructure. As ML systems enter production, particularly in cloud, edge, or embedded deployments, defensive strategies must extend beyond static protection to include real-time monitoring, threat detection, and incident response. This section outlines operational defenses that maintain system trust under adversarial conditions.</p>
<p>Runtime monitoring encompasses a range of techniques for observing system behavior, detecting anomalies, and triggering mitigation. These techniques can be grouped into three categories: input validation, output monitoring, and system integrity checks.</p>
<section id="input-validation" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="input-validation">Input Validation</h4>
<p>Input validation is the first line of defense at runtime. It ensures that incoming data conforms to expected formats, statistical properties, or semantic constraints before it is passed to a machine learning model. Without these safeguards, models are vulnerable to adversarial inputs, which are crafted examples designed to trigger incorrect predictions, or to malformed inputs that cause unexpected behavior in preprocessing or inference.</p>
<p>Machine learning models, unlike traditional rule-based systems, often do not fail safely. Small, carefully chosen changes to input data can cause models to make high-confidence but incorrect predictions. Input validation helps detect and reject such inputs early in the pipeline <span class="citation" data-cites="goodfellow2015explaining">(<a href="../references.html#ref-goodfellow2015explaining" role="doc-biblioref">I. J. Goodfellow, Shlens, and Szegedy 2014</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-goodfellow2015explaining" class="csl-entry" role="listitem">
Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. 2014. <span>‚ÄúExplaining and Harnessing Adversarial Examples.‚Äù</span> <em>ICLR</em>, December. <a href="http://arxiv.org/abs/1412.6572v3">http://arxiv.org/abs/1412.6572v3</a>.
</div></div><p>Validation techniques range from low-level checks (e.g., input size, type, and value ranges) to semantic filters (e.g., verifying whether an image contains a recognizable object or whether a voice recording includes speech). For example, a facial recognition system might validate that the uploaded image is within a certain resolution range (e.g., 224√ó224 to 1024√ó1024 pixels), contains RGB channels, and passes a lightweight face detection filter. This prevents inputs like blank images, text screenshots, or synthetic adversarial patterns from reaching the model. Similarly, a voice assistant might require that incoming audio files be between 1 and 5 seconds long, have a valid sampling rate (e.g., 16kHz), and contain detectable human speech using a speech activity detector (SAD). This ensures that empty recordings, music clips, or noise bursts are filtered before model inference.</p>
<p>In generative systems such as DALL¬∑E, Stable Diffusion, or Sora, input validation often involves prompt filtering. This includes scanning the user‚Äôs text prompt for banned terms, brand names, profanity, or misleading medical claims. For example, a user prompt like ‚ÄúGenerate an image of a medication bottle labeled with Pfizer‚Äôs logo‚Äù might be rejected or rewritten due to trademark concerns. Filters may operate using keyword lists, regular expressions, or lightweight classifiers that assess prompt intent. These filters prevent the generative model from being used to produce harmful, illegal, or misleading content‚Äîeven before sampling begins.</p>
<p>In some applications, distributional checks are also used. These assess whether the incoming data statistically resembles what the model saw during training. For instance, a computer vision pipeline might compare the color histogram of the input image to a baseline distribution, flagging outliers for manual review or rejection.</p>
<p>These validations can be lightweight (heuristics or threshold rules) or learned (small models trained to detect distribution shift or adversarial artifacts). In either case, input validation serves as a critical pre-inference firewall‚Äîreducing exposure to adversarial behavior, improving system stability, and increasing trust in downstream model decisions.</p>
</section>
<section id="output-monitoring" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="output-monitoring">Output Monitoring</h4>
<p>Even when inputs pass validation, adversarial or unexpected behavior may still emerge at the model‚Äôs output. Output monitoring helps detect such anomalies by analyzing model predictions in real time. These mechanisms observe how the model behaves across inputs, by tracking its confidence, prediction entropy, class distribution, or response patterns, to flag deviations from expected behavior.</p>
<p>A key target for monitoring is prediction confidence. For example, if a classification model begins assigning high confidence to low-frequency or previously rare classes, this may indicate the presence of adversarial inputs or a shift in the underlying data distribution. Monitoring the entropy of the output distribution can similarly reveal when the model is overly certain in ambiguous contexts‚Äîan early signal of possible manipulation.</p>
<p>In content moderation systems, a model that normally outputs neutral or ‚Äúsafe‚Äù labels may suddenly begin producing high-confidence ‚Äúsafe‚Äù labels for inputs containing offensive or restricted content. Output monitoring can detect this mismatch by comparing predictions against auxiliary signals or known-safe reference sets. When deviations are detected, the system may trigger a fallback policy‚Äîsuch as escalating the content for human review or switching to a conservative baseline model.</p>
<p>Time-series models also benefit from output monitoring. For instance, an anomaly detection model used in fraud detection might track predicted fraud scores for sequences of financial transactions. A sudden drop in fraud scores, especially during periods of high transaction volume, may indicate model tampering, label leakage, or evasion attempts. Monitoring the temporal evolution of predictions provides a broader perspective than static, pointwise classification.</p>
<p>Generative models, such as text-to-image systems, introduce unique output monitoring challenges. These models can produce high-fidelity imagery that may inadvertently violate content safety policies, platform guidelines, or user expectations. To mitigate these risks, post-generation classifiers are commonly employed to assess generated content for objectionable characteristics such as violence, nudity, or brand misuse. These classifiers operate downstream of the generative model and can suppress, blur, or reject outputs based on predefined thresholds. Some systems also inspect internal representations (e.g., attention maps or latent embeddings) to anticipate potential misuse before content is rendered.</p>
<p>However, prompt filtering alone is insufficient for safety. Research has shown that text-to-image systems can be manipulated through implicitly adversarial prompts, which are queries that appear benign but lead to policy-violating outputs. The Adversarial Nibbler project introduces an open red teaming methodology that identifies such prompts and demonstrates how models like Stable Diffusion can produce unintended content despite the absence of explicit trigger phrases <span class="citation" data-cites="quaye2024adversarial">(<a href="../references.html#ref-quaye2024adversarial" role="doc-biblioref">Quaye et al. 2024</a>)</span>. These failure cases often bypass prompt filters because their risk arises from model behavior during generation, not from syntactic or lexical cues.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-adversarial-nibbler" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-adversarial-nibbler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/adversarial_nibbler_example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;15.12: Example of an implicitly adversarial prompt (‚Äúsplatter of red paint‚Äù) generating unintended content in a text-to-image system. These types of failures bypass prompt filters and highlight the need for post-generation safety monitoring. Source: Adapted from @quaye2024adversarial."><img src="images/png/adversarial_nibbler_example.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-nibbler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.12: Example of an implicitly adversarial prompt (‚Äúsplatter of red paint‚Äù) generating unintended content in a text-to-image system. These types of failures bypass prompt filters and highlight the need for post-generation safety monitoring. Source: Adapted from <span class="citation" data-cites="quaye2024adversarial">Quaye et al. (<a href="../references.html#ref-quaye2024adversarial" role="doc-biblioref">2024</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-quaye2024adversarial" class="csl-entry" role="listitem">
Quaye, Jessica, Alicia Parrish, Oana Inel, Charvi Rastogi, Hannah Rose Kirk, Minsuk Kahng, Erin Van Liemt, et al. 2024. <span>‚ÄúAdversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation.‚Äù</span> In <em>The 2024 ACM Conference on Fairness, Accountability, and Transparency</em>, 388‚Äì406. ACM. <a href="https://doi.org/10.1145/3630106.3658913">https://doi.org/10.1145/3630106.3658913</a>.
</div></div></figure>
</div>
<p>As shown in <a href="#fig-adversarial-nibbler" class="quarto-xref">Figure&nbsp;<span>15.12</span></a>, even prompts that appear innocuous can trigger unsafe generations. Such examples highlight the limitations of pre-generation safety checks and reinforce the necessity of output-based monitoring as a second line of defense. This two-stage pipeline‚Äîconsisting of prompt filtering followed by post-hoc content analysisis essential for ensuring the safe deployment of generative models in open-ended or user-facing environments.</p>
<p>In the domain of language generation, output monitoring plays a different but equally important role. Here, the goal is often to detect toxicity, hallucinated claims, or off-distribution responses. For example, a customer support chatbot may be monitored for keyword presence, tonal alignment, or semantic coherence. If a response contains profanity, unsupported assertions, or syntactically malformed text, the system may trigger a rephrasing, initiate a fallback to scripted templates, or halt the response altogether.</p>
<p>Effective output monitoring combines rule-based heuristics with learned detectors trained on historical outputs. These detectors are deployed to flag deviations in real time and feed alerts into incident response pipelines. In contrast to model-centric defenses like adversarial training, which aim to improve model robustness, output monitoring emphasizes containment and remediation. Its role is not to prevent exploitation but to detect its symptoms and initiate appropriate countermeasures <span class="citation" data-cites="savas2022ml">(<a href="../references.html#ref-savas2022ml" role="doc-biblioref">Savas et al. 2022</a>)</span>. In safety-critical or policy-sensitive applications, such mechanisms form a critical layer of operational resilience.</p>
<div class="no-row-height column-margin column-container"><div id="ref-savas2022ml" class="csl-entry" role="listitem">
Savas, Esra, Reza Shokri, Lalith Singaravelu, Nithya Swamy, and Mitali Bafna. 2022. <span>‚ÄúML-ExRay: Visibility and Explainability for Monitoring ML Model Behavior.‚Äù</span> In <em>Proceedings of the 2022 IEEE Symposium on Security and Privacy (SP)</em>, 1352‚Äì69. IEEE.
</div><div id="ref-lee2023llmguard" class="csl-entry" role="listitem">
Inan, Hakan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, et al. 2023. <span>‚ÄúLlama Guard: LLM-Based Input-Output Safeguard for Human-AI Conversations,‚Äù</span> December. <a href="http://arxiv.org/abs/2312.06674v1">http://arxiv.org/abs/2312.06674v1</a>.
</div></div><p>These principles have been implemented in recent output filtering frameworks. For example, LLM Guard combines transformer-based classifiers with safety dimensions such as toxicity, misinformation, and illegal content to assess and reject prompts or completions in instruction-tuned LLMs <span class="citation" data-cites="lee2023llmguard">(<a href="../references.html#ref-lee2023llmguard" role="doc-biblioref">Inan et al. 2023</a>)</span>. Similarly, <a href="https://ai.google.dev/gemma/docs/shieldgemma">ShieldGemma</a>, developed as part of Google‚Äôs open Gemma model release, applies configurable scoring functions to detect and filter undesired outputs during inference. Both systems exemplify how safety classifiers and output monitors are being integrated into the runtime stack to support scalable, policy-aligned deployment of generative language models.</p>
</section>
<section id="integrity-checks" class="level4">
<h4 class="anchored" data-anchor-id="integrity-checks">Integrity Checks</h4>
<p>While input and output monitoring focus on model behavior, system integrity checks ensure that the underlying model files, execution environment, and serving infrastructure remain untampered throughout deployment. These checks detect unauthorized modifications, verify that the model running in production is authentic, and alert operators to suspicious system-level activity.</p>
<p>One of the most common integrity mechanisms is cryptographic model verification. Before a model is loaded into memory, the system can compute a cryptographic hash (e.g., SHA-256) of the model file and compare it against a known-good signature. This process ensures that the model has not been altered during transit or storage. For example, a PyTorch .pt or TensorFlow .pb model artifact stored in object storage (e.g., S3) might be verified using a signed hash from a deployment registry before loading into a production container. If the verification fails, the system can block inference, alert an operator, or revert to a trusted model version.</p>
<p>Access control and audit logging complement cryptographic checks. ML systems should restrict access to model files using role-based permissions and monitor file access patterns. For instance, repeated attempts to read model checkpoints from a non-standard path, or inference requests from unauthorized IP ranges, may indicate tampering, privilege escalation, or insider threats.</p>
<p>In cloud environments, container- or VM-based isolation helps enforce process and memory boundaries, but these protections can erode over time due to misconfiguration or supply chain vulnerabilities. To reinforce runtime assurance, systems may deploy periodic attestation checks‚Äîverifying not just the model artifact, but also the software environment, installed dependencies, and hardware identity. These techniques are often backed by hardware trust anchors (e.g., TPMs or TEEs) discussed later on in this chapter.</p>
<p>For example, in a regulated healthcare ML deployment, integrity checks might include: verifying the model hash against a signed manifest, validating that the runtime environment uses only approved Python packages, and checking that inference occurs inside a signed and attested virtual machine. These checks ensure compliance, limit the risk of silent failures, and create a forensic trail in case of audit or breach.</p>
<p>Some systems also implement runtime memory verification, such as scanning for unexpected model parameter changes or checking that memory-mapped model weights remain unaltered during execution. While more common in high-assurance systems, such checks are becoming more feasible with the adoption of secure enclaves and trusted runtimes.</p>
<p>Taken together, system integrity checks play a critical role in protecting machine learning systems from low-level attacks that bypass the model interface. When coupled with input/output monitoring, they provide layered assurance that both the model and its execution environment remain trustworthy under adversarial conditions.</p>
</section>
<section id="response-and-rollback" class="level4">
<h4 class="anchored" data-anchor-id="response-and-rollback">Response and Rollback</h4>
<p>When a security breach, anomaly, or performance degradation is detected in a deployed machine learning system, rapid and structured incident response is critical to minimizing impact. The goal is not only to contain the issue but to restore system integrity and ensure that future deployments benefit from the insights gained. Unlike traditional software systems, ML responses may require handling model state, data drift, or inference behavior, making recovery more complex.</p>
<p>The first step is to define incident detection thresholds that trigger escalation. These thresholds may come from input validation (e.g., invalid input rates), output monitoring (e.g., drop in prediction confidence), or system integrity checks (e.g., failed model signature verification). When a threshold is crossed, the system should initiate an automated or semi-automated response protocol.</p>
<p>One common strategy is model rollback, where the system reverts to a previously verified version of the model. For instance, if a newly deployed fraud detection model begins misclassifying transactions, the system may fall back to the last known-good checkpoint, restoring service while the affected version is quarantined. Rollback mechanisms require version-controlled model storage, typically supported by MLOps platforms such as MLflow, TFX, or SageMaker.</p>
<p>In high-availability environments, model isolation may be used to contain failures. The affected model instance can be removed from load balancers or shadowed in a canary deployment setup. This allows continued service with unaffected replicas while maintaining forensic access to the compromised model for analysis.</p>
<p>Traffic throttling is another immediate response tool. If an adversarial actor is probing a public inference API at high volume, the system can rate-limit or temporarily block offending IP ranges while continuing to serve trusted clients. This containment technique helps prevent abuse without requiring full system shutdown.</p>
<p>Once immediate containment is in place, investigation and recovery can begin. This may include forensic analysis of input logs, parameter deltas between model versions, or memory snapshots from inference containers. In regulated environments, organizations may also need to notify users or auditors, particularly if personal or safety-critical data was affected.</p>
<p>Recovery typically involves retraining or patching the model. This must occur through a secure update process, using signed artifacts, trusted build pipelines, and validated data. To prevent recurrence, the incident should feed back into model evaluation pipelines‚Äîupdating tests, refining monitoring thresholds, or hardening input defenses. For example, if a prompt injection attack bypassed a content filter in a generative model, retraining might include adversarially crafted prompts, and the prompt validation logic would be updated to reflect newly discovered patterns.</p>
<p>Finally, organizations should establish post-incident review practices. This includes documenting root causes, identifying gaps in detection or response, and updating policies and playbooks. Incident reviews help translate operational failures into actionable improvements across the design-deploy-monitor lifecycle.</p>
</section>
</section>
<section id="hardware-based-security" class="level3 page-columns page-full" data-number="15.7.5">
<h3 data-number="15.7.5" class="anchored" data-anchor-id="hardware-based-security"><span class="header-section-number">15.7.5</span> Hardware-based Security</h3>
<p>Machine learning systems are increasingly deployed in environments where hardware-based security features can provide additional layers of protection. These features can help ensure the integrity of model execution, protect sensitive data, and prevent unauthorized access to system resources. This section discusses several key hardware-based security mechanisms that can enhance the security posture of machine learning systems.</p>
<section id="trusted-execution-environments" class="level4">
<h4 class="anchored" data-anchor-id="trusted-execution-environments">Trusted Execution Environments</h4>
<p>A Trusted Execution Environment (TEE) is a hardware-isolated region within a processor designed to protect sensitive computations and data from potentially compromised software. TEEs enforce confidentiality, integrity, and runtime isolation, ensuring that even if the host operating system or application layer is attacked, sensitive operations within the TEE remain secure.</p>
<p>In the context of machine learning, TEEs are increasingly important for preserving the confidentiality of models, securing sensitive user data during inference, and ensuring that model outputs remain trustworthy. For example, a TEE can protect model parameters from being extracted by malicious software running on the same device, or ensure that computations involving biometric inputs, including facial data or fingerprint data, are performed securely. This capability is particularly critical in applications where model integrity, user privacy, or regulatory compliance are non-negotiable.</p>
<p>One widely deployed example is <a href="https://support.apple.com/guide/security/secure-enclave-sec59b0b31ff/web">Apple‚Äôs Secure Enclave</a>, which provides isolated execution and secure key storage for iOS devices. By separating cryptographic operations and biometric data from the main processor, the Secure Enclave ensures that user credentials and Face ID features remain protected, even in the event of a broader system compromise.</p>
<p>Trusted Execution Environments are essential across a range of industries with high security requirements. In telecommunications, TEEs are used to safeguard encryption keys and secure critical 5G control-plane operations. In finance, they enable secure mobile payments and protect PIN-based authentication workflows. In healthcare, TEEs help enforce patient data confidentiality during edge-based ML inference on wearable or diagnostic devices. In the automotive industry, they are deployed in advanced driver-assistance systems (ADAS) to ensure that safety-critical perception and decision-making modules operate on verified software.</p>
<p>In machine learning systems, TEEs can provide several important protections. They secure the execution of model inference or training, shielding intermediate computations and final predictions from system-level observation. They protect the confidentiality of sensitive inputs, including biometric or clinical signals, used in personal identification or risk scoring tasks. TEEs also serve to prevent reverse engineering of deployed models by restricting access to weights and architecture internals. When models are updated, TEEs ensure the authenticity of new parameters and block unauthorized tampering. Furthermore, in distributed ML settings, TEEs can protect data exchanged between components by enabling encrypted and attested communication channels.</p>
<p>The core security properties of a TEE are achieved through four mechanisms: isolated execution, secure storage, integrity protection, and in-TEE data encryption. Code that runs inside the TEE is executed in a separate processor mode, inaccessible to the normal-world operating system. Sensitive assets such as cryptographic keys or authentication tokens are stored in memory that only the TEE can access. Code and data can be verified for integrity before execution using hardware-anchored hashes or signatures. Finally, data processed inside the TEE can be encrypted, ensuring that even intermediate results are inaccessible without appropriate keys, which are also managed internally by the TEE.</p>
<p>Several commercial platforms provide TEE functionality tailored for different deployment contexts. <a href="https://www.arm.com/technologies/trustzone-for-cortex-m">ARM TrustZone</a> offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT applications. <a href="https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html">Intel SGX</a> implements enclave-based security for cloud and desktop systems, enabling secure computation even on untrusted infrastructure. <a href="https://www.qualcomm.com/products/features/mobile-security-solutions">Qualcomm‚Äôs Secure Execution Environment</a> supports secure mobile transactions and user authentication. Apple‚Äôs Secure Enclave remains a canonical example of a hardware-isolated security coprocessor for consumer devices.</p>
<p><a href="#fig-enclave" class="quarto-xref">Figure&nbsp;<span>15.13</span></a> illustrates a secure enclave integrated into a system-on-chip (SoC) architecture. The enclave includes a dedicated processor, an AES engine, a true random number generator (TRNG), a public key accelerator (PKA), and a secure I¬≤C interface to nonvolatile storage. These components operate in isolation from the main application processor and memory subsystem. A memory protection engine enforces access control, while cryptographic operations such as NAND flash encryption are handled internally using enclave-managed keys. By physically separating secure execution and key management from the main system, this architecture limits the impact of system-level compromises and forms the foundation of hardware-enforced trust.</p>
<div id="fig-enclave" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-enclave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="780b5b818607d05236bd1662895ed48e7423453a.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;15.13: System-on-chip secure enclave. Source: Apple."><img src="privacy_security_files/mediabag/780b5b818607d05236bd1662895ed48e7423453a.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-enclave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.13: System-on-chip secure enclave. Source: Apple.
</figcaption>
</figure>
</div>
<p>This architecture underpins the secure deployment of machine learning applications on consumer devices. For example, Apple‚Äôs Face ID system uses a secure enclave to perform facial recognition entirely within a hardware-isolated environment. The face embedding model is executed inside the enclave, and biometric templates are stored in secure nonvolatile memory accessible only via the enclave‚Äôs I¬≤C interface. During authentication, input data from the infrared camera is processed locally, and no facial features or predictions ever leave the secure region. Even if the application processor or operating system is compromised, the enclave prevents access to sensitive model inputs, parameters, and outputs‚Äîensuring that biometric identity remains protected end to end.</p>
<p>Despite their strengths, Trusted Execution Environments come with notable trade-offs. Implementing a TEE increases both direct hardware costs and indirect costs associated with developing and maintaining secure software. Integrating TEEs into existing systems may require architectural redesigns, especially for legacy infrastructure. Developers must adhere to strict protocols for isolation, attestation, and secure update management, which can extend development cycles and complicate testing workflows. TEEs can also introduce performance overhead, particularly when cryptographic operations are involved, or when context switching between trusted and untrusted modes is frequent.</p>
<p>Energy efficiency is another consideration, particularly in battery-constrained devices. TEEs typically consume additional power due to secure memory accesses, cryptographic computation, and hardware protection logic. In resource-limited embedded systems, these costs may limit their use. In terms of scalability and flexibility, the secure boundaries enforced by TEEs may complicate distributed training or federated inference workloads, where secure coordination between enclaves is required.</p>
<p>Market demand also varies. In some consumer applications, perceived threat levels may be too low to justify the integration of TEEs. Moreover, systems with TEEs may be subject to formal security certifications, such as <a href="https://www.commoncriteriaportal.org/ccra/index.cfm">Common Criteria</a> or evaluation under <a href="https://www.enisa.europa.eu/">ENISA</a>, which can introduce additional time and expense. For this reason, TEEs are typically adopted only when the expected threat model, including adversarial users, cloud tenants, and malicious insiders, justifies the investment.</p>
<p>Nonetheless, TEEs remain a powerful hardware primitive in the machine learning security landscape. When paired with software- and system-level defenses, they provide a trusted foundation for executing ML models securely, privately, and verifiably, especially in scenarios where adversarial compromise of the host environment is a serious concern.</p>
<p>Here is the revised 7.5.2 Secure Boot section, rewritten in formal textbook tone with all original technical content, hyperlinks, and figures preserved. The structure emphasizes narrative clarity, avoids bullet lists, and integrates the Apple Face ID case study naturally.</p>
</section>
<section id="secure-boot" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="secure-boot">Secure Boot</h4>
<p>Secure Boot is a mechanism that ensures a device only boots software components that are cryptographically verified and explicitly authorized by the manufacturer. At startup, each stage of the boot process, comprising the bootloader, kernel, and base operating system, is checked against a known-good digital signature. If any signature fails verification, the boot sequence is halted, preventing unauthorized or malicious code from executing. This chain-of-trust model establishes system integrity from the very first instruction executed.</p>
<p>In ML systems, especially those deployed on embedded or edge hardware, Secure Boot plays an important role. A compromised boot process may result in malicious software loading before the ML runtime begins, enabling attackers to intercept model weights, tamper with training data, or reroute inference results. Such breaches can lead to incorrect or manipulated predictions, unauthorized data access, or device repurposing for botnets or crypto-mining.</p>
<p>For machine learning systems, Secure Boot offers several guarantees. First, it protects model-related data, such as training data, inference inputs, and outputs, during the boot sequence, preventing pre-runtime tampering. Second, it ensures that only authenticated model binaries and supporting software are loaded, which helps guard against deployment-time model substitution. Third, Secure Boot enables secure model updates by verifying that firmware or model changes are signed and have not been altered in transit.</p>
<p>Secure Boot frequently works in tandem with hardware-based Trusted Execution Environments (TEEs) to create a fully trusted execution stack. As shown in <a href="#fig-secure-boot" class="quarto-xref">Figure&nbsp;<span>15.14</span></a>, this layered boot process verifies firmware, operating system components, and TEE integrity before permitting execution of cryptographic operations or ML workloads. In embedded systems, this architecture provides resilience even under severe adversarial conditions or physical device compromise.</p>
<div id="fig-secure-boot" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-secure-boot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="88d90ad9c333e0d339dc3a3412204fc54bdf87ee.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;15.14: Secure Boot flow. Source: @Rashmi2018Secure."><img src="privacy_security_files/mediabag/88d90ad9c333e0d339dc3a3412204fc54bdf87ee.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-secure-boot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.14: Secure Boot flow. Source: <span class="citation" data-cites="Rashmi2018Secure">R. V. and A. (<a href="../references.html#ref-Rashmi2018Secure" role="doc-biblioref">2018</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-Rashmi2018Secure" class="csl-entry" role="listitem">
R. V., Rashmi, and Karthikeyan A. 2018. <span>‚ÄúSecure Boot of Embedded Applications - a Review.‚Äù</span> In <em>2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)</em>, 291‚Äì98. IEEE. <a href="https://doi.org/10.1109/iceca.2018.8474730">https://doi.org/10.1109/iceca.2018.8474730</a>.
</div></div></figure>
</div>
<p>A well-known real-world implementation of Secure Boot appears in Apple‚Äôs Face ID system, which leverages advanced machine learning for facial recognition. For Face ID to operate securely, the entire device stack, from the initial power-on to the execution of the model, must be verifiably trusted.</p>
<p>Upon device startup, Secure Boot initiates within Apple‚Äôs <a href="https://support.apple.com/en-us/102381">Secure Enclave</a>, a dedicated security coprocessor that handles biometric data. The firmware loaded onto the Secure Enclave is digitally signed by Apple, and any unauthorized modification causes the boot process to fail. Once verified, the Secure Enclave performs continuous checks in coordination with the central processor to maintain a trusted boot chain. Each system component, ranging from the iOS kernel to the application-level code, is verified using cryptographic signatures.</p>
<p>After completing the secure boot sequence, the Secure Enclave activates the ML-based Face ID system. The facial recognition model projects over 30,000 infrared points to map a user‚Äôs face, generating a depth image and computing a mathematical representation that is compared against a securely stored profile. These facial data artifacts are never written to disk, transmitted off-device, or shared externally. All processing occurs within the enclave to protect against eavesdropping or exfiltration, even in the presence of a compromised kernel.</p>
<p>To support continued integrity, Secure Boot also governs software updates. Only firmware or model updates signed by Apple are accepted, ensuring that even over-the-air patches do not introduce risk. This process maintains a robust chain of trust over time, enabling the secure evolution of the ML system while preserving user privacy and device security.</p>
<p>While Secure Boot provides strong protection, its adoption presents technical and operational challenges. Managing the cryptographic keys used to sign and verify system components is complex, especially at scale. Enterprises must securely provision, rotate, and revoke keys, ensuring that no trusted root is compromised. Any such breach would undermine the entire security chain.</p>
<p>Performance is also a consideration. Verifying signatures during the boot process introduces latency, typically on the order of tens to hundreds of milliseconds per component. Although acceptable in many applications, these delays may be problematic for real-time or power-constrained systems. Developers must also ensure that all components, including bootloaders, firmware, kernels, drivers, and even ML models, are correctly signed. Integrating third-party software into a Secure Boot pipeline introduces additional complexity.</p>
<p>Some systems limit user control in favor of vendor-locked security models, restricting upgradability or customization. In response, open-source bootloaders like <a href="https://source.denx.de/u-boot/u-boot">u-boot</a> and <a href="https://www.coreboot.org/">coreboot</a> have emerged, offering Secure Boot features while supporting extensibility and transparency. To further scale trusted device deployments, emerging industry standards such as the <a href="https://www.microsoft.com/en-us/research/project/dice-device-identifier-composition-engine/">Device Identifier Composition Engine (DICE)</a> and <a href="https://1.ieee802.org/security/802-1ar/">IEEE 802.1AR IDevID</a> provide mechanisms for secure device identity, key provisioning, and cross-vendor trust assurance.</p>
<p>Secure Boot, when implemented carefully and complemented by trusted hardware and secure software update processes, forms the backbone of system integrity for embedded and distributed ML. It provides the assurance that the machine learning model running in production is not only the correct version, but is also executing in a known-good environment, anchored to hardware-level trust.</p>
</section>
<section id="hardware-security-modules" class="level4">
<h4 class="anchored" data-anchor-id="hardware-security-modules">Hardware Security Modules</h4>
<p>A Hardware Security Module (HSM) is a tamper-resistant physical device designed to perform cryptographic operations and securely manage digital keys. HSMs are widely used across security-critical industries such as finance, defense, and cloud infrastructure, and they are increasingly relevant for securing the machine learning pipeline‚Äîparticularly in deployments where key confidentiality, model integrity, and regulatory compliance are essential.</p>
<p>HSMs provide an isolated, hardened environment for performing sensitive operations such as key generation, digital signing, encryption, and decryption. Unlike general-purpose processors, they are engineered to withstand physical tampering and side-channel attacks, and they typically include protected storage, cryptographic accelerators, and internal audit logging. HSMs may be implemented as standalone appliances, plug-in modules, or integrated chips embedded within broader systems.</p>
<p>In machine learning systems, HSMs enhance security across several dimensions. They are commonly used to protect encryption keys associated with sensitive data that may be processed during training or inference. These keys might encrypt data at rest in model checkpoints or enable secure transmission of inference requests across networked environments. By ensuring that the keys are generated, stored, and used exclusively within the HSM, the system minimizes the risk of key leakage, unauthorized reuse, or tampering.</p>
<p>HSMs also play a role in maintaining the integrity of machine learning models. In many production pipelines, models must be signed before deployment to ensure that only verified versions are accepted into runtime environments. The signing keys used to authenticate models can be stored and managed within the HSM, providing cryptographic assurance that the deployed artifact is authentic and untampered. Similarly, secure firmware updates and configuration changes, regardless of whether they pertain to models, hyperparameters, or supporting infrastructure, can be validated using signatures produced by the HSM.</p>
<p>In addition to protecting inference workloads, HSMs can be used to secure model training. During training, data may originate from distributed and potentially untrusted sources. HSM-backed protocols can help ensure that training pipelines perform encryption, integrity checks, and access control enforcement securely and in compliance with organizational or legal requirements. In regulated industries such as healthcare and finance, such protections are often mandatory.</p>
<p>Despite these benefits, incorporating HSMs into embedded or resource-constrained ML systems introduces several trade-offs. First, HSMs are specialized hardware components and often come at a premium. Their cost may be justified in data center settings or safety-critical applications but can be prohibitive for low-margin embedded products or wearables. Physical space is also a concern. Embedded systems often operate under strict size, weight, and form factor constraints, and integrating an HSM may require redesigning circuit layouts or sacrificing other functionality.</p>
<p>From a performance standpoint, HSMs introduce latency, particularly for operations like key exchange, signature verification, or on-the-fly decryption. In real-time inference systems, including autonomous vehicles, industrial robotics, and live translation devices, these delays can affect responsiveness. While HSMs are typically optimized for cryptographic throughput, they are not general-purpose processors, and offloading secure operations must be carefully coordinated.</p>
<p>Power consumption is another concern. The continuous secure handling of keys, signing of transactions, and cryptographic validations can consume more power than basic embedded components, impacting battery life in mobile or remote deployments.</p>
<p>Integration complexity also grows when HSMs are introduced into existing ML pipelines. Interfacing between the HSM and the host processor requires dedicated APIs and often specialized software development. Firmware and model updates must be routed through secure, signed channels, and update orchestration must account for device-specific key provisioning. These requirements increase the operational burden, especially in large deployments.</p>
<p>Scalability presents its own set of challenges. Managing a distributed fleet of HSM-equipped devices requires secure provisioning of individual keys, secure identity binding, and coordinated trust management. In large ML deployments, including fleets of smart sensors or edge inference nodes, ensuring uniform security posture across all devices is nontrivial.</p>
<p>Finally, the use of HSMs often requires organizations to engage in certification and compliance processes, particularly when handling regulated data. Meeting standards such as FIPS 140-2 or Common Criteria adds time and cost to development. Access to the HSM is typically restricted to a small set of authorized personnel, which can complicate development workflows and slow iteration cycles.</p>
<p>Despite these operational complexities, HSMs remain a valuable option for machine learning systems that require high assurance of cryptographic integrity and access control. When paired with TEEs, secure boot, and software-based defenses, HSMs contribute to a multilayered security model that spans hardware, system software, and ML runtime.</p>
</section>
<section id="physical-unclonable-functions" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="physical-unclonable-functions">Physical Unclonable Functions</h4>
<p>Physical Unclonable Functions (PUFs) provide a hardware-intrinsic mechanism for cryptographic key generation and device authentication by leveraging physical randomness in semiconductor fabrication <span class="citation" data-cites="gassend2002silicon">(<a href="../references.html#ref-gassend2002silicon" role="doc-biblioref">Gassend et al. 2002</a>)</span>. Unlike traditional keys stored in memory, a PUF generates secret values based on microscopic variations in a chip‚Äôs physical properties‚Äîvariations that are inherent to manufacturing processes and difficult to clone or predict, even by the manufacturer.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gassend2002silicon" class="csl-entry" role="listitem">
Gassend, Blaise, Dwaine Clarke, Marten van Dijk, and Srinivas Devadas. 2002. <span>‚ÄúSilicon Physical Random Functions.‚Äù</span> In <em>Proceedings of the 9th ACM Conference on Computer and Communications Security - CCS ‚Äô02</em>, 148‚Äì60. ACM; ACM Press. <a href="https://doi.org/10.1145/586131.586132">https://doi.org/10.1145/586131.586132</a>.
</div></div><p>These variations arise from uncontrollable physical factors such as doping concentration, line edge roughness, and dielectric thickness. As a result, even chips fabricated with the same design masks exhibit small but measurable differences in timing, power consumption, or voltage behavior. PUF circuits amplify these variations to produce a device-unique digital output. When a specific input challenge is applied to a PUF, it generates a corresponding response based on the chip‚Äôs physical fingerprint. Because these characteristics are effectively impossible to replicate, the same challenge will yield different responses across devices.</p>
<p>This challenge-response mechanism allows PUFs to serve several cryptographic purposes. They can be used to derive device-specific keys that never need to be stored externally, reducing the attack surface for key exfiltration. The same mechanism also supports secure authentication and attestation, where devices must prove their identity to trusted servers or hardware gateways. These properties make PUFs a natural fit for machine learning systems deployed in embedded and distributed environments.</p>
<p>In ML applications, PUFs offer unique advantages for securing resource-constrained systems. For example, consider a smart camera drone that uses onboard computer vision to track objects. A PUF embedded in the drone‚Äôs processor can generate a private key to encrypt the model during boot. Even if the model were extracted, it would be unusable on another device lacking the same PUF response. That same PUF-derived key could also be used to watermark the model parameters, creating a cryptographically verifiable link between a deployed model and its origin hardware. If the model were leaked or pirated, the embedded watermark could help prove the source of the compromise.</p>
<p>PUFs also support authentication in distributed ML pipelines. If the drone offloads computation to a cloud server, the PUF can help verify that the drone has not been cloned or tampered with. The cloud backend can issue a challenge, verify the correct response from the device, and permit access only if the PUF proves device authenticity. These protections enhance trust not only in the model and data, but in the execution environment itself.</p>
<p>The internal operation of a PUF is illustrated in <a href="#fig-pfu" class="quarto-xref">Figure&nbsp;<span>15.15</span></a>. At a high level, a PUF accepts a challenge input and produces a unique response determined by the physical microstructure of the chip <span class="citation" data-cites="Gao2020Physical">(<a href="../references.html#ref-Gao2020Physical" role="doc-biblioref">Gao, Al-Sarawi, and Abbott 2020</a>)</span>. Variants include optical PUFs, in which the challenge consists of a light pattern and the response is a speckle image, and electronic PUFs such as Arbiter PUFs (APUFs), where timing differences between circuit paths produce a binary output. Another common implementation is the SRAM PUF, which exploits the power-up state of uninitialized SRAM cells: due to threshold voltage mismatch, each cell tends to settle into a preferred value when power is first applied. These response patterns form a stable, reproducible hardware fingerprint.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-pfu" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-pfu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/PUF_basics.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;15.15: PUF basics. Source: @Gao2020Physical."><img src="images/png/PUF_basics.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pfu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.15: PUF basics. Source: <span class="citation" data-cites="Gao2020Physical">Gao, Al-Sarawi, and Abbott (<a href="../references.html#ref-Gao2020Physical" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-Gao2020Physical" class="csl-entry" role="listitem">
Gao, Yansong, Said F. Al-Sarawi, and Derek Abbott. 2020. <span>‚ÄúPhysical Unclonable Functions.‚Äù</span> <em>Nature Electronics</em> 3 (2): 81‚Äì91. <a href="https://doi.org/10.1038/s41928-020-0372-5">https://doi.org/10.1038/s41928-020-0372-5</a>.
</div></div></figure>
</div>
<p>Despite their promise, PUFs present several challenges in system design. Their outputs can be sensitive to environmental variation, such as changes in temperature or voltage, which can introduce instability or bit errors in the response. To ensure reliability, PUF systems must often incorporate error correction codes or helper data schemes. Managing large sets of challenge-response pairs also raises questions about storage, consistency, and revocation. Additionally, the unique statistical structure of PUF outputs may make them vulnerable to machine learning-based modeling attacks if not carefully shielded from external observation.</p>
<p>From a manufacturing perspective, incorporating PUF technology can increase device cost or require additional layout complexity. While PUFs eliminate the need for external key storage, thereby reducing long-term security risk and provisioning cost, they may require calibration and testing during fabrication to ensure consistent performance across environmental conditions and device aging.</p>
<p>Nevertheless, Physical Unclonable Functions remain a compelling building block for securing embedded machine learning systems. By embedding hardware identity directly into the chip, PUFs support lightweight cryptographic operations, reduce key management burden, and help establish root-of-trust anchors in distributed or resource-constrained environments. When integrated thoughtfully, they complement other hardware-assisted security mechanisms such as Secure Boot, TEEs, and HSMs to provide defense-in-depth across the ML system lifecycle.</p>
</section>
<section id="mechanisms-comparison" class="level4">
<h4 class="anchored" data-anchor-id="mechanisms-comparison">Mechanisms Comparison</h4>
<p>Hardware-assisted security mechanisms play a foundational role in establishing trust within modern machine learning systems. While software-based defenses offer flexibility, they ultimately rely on the security of the hardware platform. As machine learning workloads increasingly operate on edge devices, embedded platforms, and untrusted infrastructure, hardware-backed protections become essential for maintaining system integrity, confidentiality, and trust.</p>
<p>Trusted Execution Environments (TEEs) provide runtime isolation for model inference and sensitive data handling. Secure Boot enforces integrity from power-on, ensuring that only verified software is executed. Hardware Security Modules (HSMs) offer tamper-resistant storage and cryptographic processing for secure key management, model signing, and firmware validation. Physical Unclonable Functions (PUFs) bind secrets and authentication to the physical characteristics of a specific device, enabling lightweight and unclonable identities.</p>
<p>These mechanisms address different layers of the system stack, ranging from initialization and attestation to runtime protection and identity binding, and complement one another when deployed together. <a href="#tbl-hw-security-comparison" class="quarto-xref">Table&nbsp;<span>15.7</span></a> below compares their roles, use cases, and trade-offs in machine learning system design.</p>
<div id="tbl-hw-security-comparison" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-hw-security-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.7: Hardware security mechanisms comparison.
</figcaption>
<div aria-describedby="tbl-hw-security-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 20%">
<col style="width: 28%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Mechanism</th>
<th style="text-align: left;">Primary Function</th>
<th style="text-align: left;">Common Use in ML</th>
<th style="text-align: left;">Trade-offs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Trusted Execution Environment (TEE)</td>
<td style="text-align: left;">Isolated runtime environment for secure computation</td>
<td style="text-align: left;">Secure inference and on-device privacy for sensitive inputs and outputs</td>
<td style="text-align: left;">Added complexity, memory limits, perf. cost Requires trusted code development</td>
</tr>
<tr class="even">
<td style="text-align: left;">Secure Boot</td>
<td style="text-align: left;">Verified boot sequence and firmware validation</td>
<td style="text-align: left;">Ensures only signed ML models and firmware execute on embedded devices</td>
<td style="text-align: left;">Key management complexity, vendor lock-in Performance impact during startup</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Hardware Security Module (HSM)</td>
<td style="text-align: left;">Secure key generation and storage, crypto-processing</td>
<td style="text-align: left;">Signing ML models, securing training pipelines, verifying firmware</td>
<td style="text-align: left;">High cost, integration overhead, limited I/O Requires infrastructure-level provisioning</td>
</tr>
<tr class="even">
<td style="text-align: left;">Physical Unclonable Function (PUF)</td>
<td style="text-align: left;">Hardware-bound identity and key derivation</td>
<td style="text-align: left;">Model binding, device authentication, protecting IP in embedded deployments</td>
<td style="text-align: left;">Environmental sensitivity, modeling attacks Needs error correction and calibration</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Together, these hardware primitives form the foundation of a defense-in-depth strategy for securing ML systems in adversarial environments. Their integration is especially important in domains that demand provable trust, such as autonomous vehicles, healthcare devices, federated learning systems, and critical infrastructure.</p>
</section>
</section>
<section id="toward-trustworthy-systems" class="level3" data-number="15.7.6">
<h3 data-number="15.7.6" class="anchored" data-anchor-id="toward-trustworthy-systems"><span class="header-section-number">15.7.6</span> Toward Trustworthy Systems</h3>
<p>Defending machine learning systems against adversarial threats, misuse, and system compromise requires more than isolated countermeasures. As this section has shown, effective defense emerges from the careful integration of mechanisms at multiple layers of the ML stack‚Äîfrom privacy-preserving data handling and robust model design to runtime monitoring and hardware-enforced isolation. No single component can provide complete protection; instead, a trustworthy system is the result of coordinated design decisions that address risk across the data, model, system, and infrastructure layers.</p>
<p>Defensive strategies must align with the deployment context and threat model. What is appropriate for a public cloud API may differ from the requirements of an embedded medical device or a fleet of edge-deployed sensors. Design choices must balance security, performance, and usability, recognizing that protections often introduce operational trade-offs. Monitoring and incident response mechanisms ensure resilience during live operation, while hardware-based roots of trust ensure system integrity even when higher layers are compromised.</p>
<p>As machine learning continues to expand into safety-critical, privacy-sensitive, and decentralized environments, the need for robust, end-to-end defense becomes increasingly urgent. Building ML systems that are not only accurate, but secure, private, and auditable, is fundamental to long-term deployment success and public trust. The principles introduced in this section lay the groundwork for such systems‚Äîwhile connecting forward to broader concerns explored in subsequent chapters, including robustness, responsible AI, and MLOps operations.</p>
<p>The process of engineering trustworthy ML systems requires a structured approach that connects threat modeling to layered defenses and runtime resilience. <a href="#fig-trustworthy-ml-recipe" class="quarto-xref">Figure&nbsp;<span>15.16</span></a> provides a conceptual framework to guide this process across technical and deployment dimensions. The design flow begins with a thorough assessment of the threat model and deployment context, which informs the selection of appropriate defenses across the system stack. This includes data-layer protections such as differential privacy (DP), federated learning (FL), and encryption; model-layer defenses like robustness techniques, watermarking, and secure deployment practices; runtime-layer measures such as input validation and output monitoring; and hardware-layer solutions including TEEs, secure boot, and PUFs.</p>
<div id="fig-trustworthy-ml-recipe" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trustworthy-ml-recipe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="bf7ca2a0807141f719526a774986a1880851e63d.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;15.16: A design flow for building secure and trustworthy ML systems."><img src="privacy_security_files/mediabag/bf7ca2a0807141f719526a774986a1880851e63d.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trustworthy-ml-recipe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.16: A design flow for building secure and trustworthy ML systems.
</figcaption>
</figure>
</div>
<p>This design flow emphasizes the importance of a comprehensive approach to security, where each layer of the system is fortified against potential threats while remaining adaptable to evolving risks. By integrating these principles into the design and deployment of machine learning systems, organizations can build solutions that are not only effective but also resilient, trustworthy, and aligned with ethical standards.</p>
</section>
</section>
<section id="offensive-capabilities" class="level2 page-columns page-full" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="offensive-capabilities"><span class="header-section-number">15.8</span> Offensive Capabilities</h2>
<p>While machine learning systems are often treated as assets to protect, they may also serve as tools for launching attacks. In adversarial settings, the same models used to enhance productivity, automate perception, or assist decision-making can be repurposed to execute or amplify offensive operations. This dual-use characteristic of machine learning, its capacity to secure systems as well as to subvert them, marks a fundamental shift in how ML must be considered within system-level threat models.</p>
<p>An offensive use of machine learning refers to any scenario in which a machine learning model is employed to facilitate the compromise of another system. In such cases, the model itself is not the object under attack, but the mechanism through which an adversary advances their objectives. These applications may involve reconnaissance, inference, subversion, impersonation, or the automation of exploit strategies that would otherwise require manual execution.</p>
<p>Importantly, such offensive applications are not speculative. Attackers are already integrating machine learning into their toolchains across a wide range of activities, from spam filtering evasion to model, driven malware generation. What distinguishes these scenarios is the deliberate use of learning-based systems to extract, manipulate, or generate information in ways that undermine the confidentiality, integrity, or availability of targeted components.</p>
<p>To clarify the diversity and structure of these applications, <a href="#tbl-offensive-ml-use-cases" class="quarto-xref">Table&nbsp;<span>15.8</span></a> summarizes several representative use cases. For each, the table identifies the type of machine learning model typically employed, the underlying system vulnerability it exploits, and the primary advantage conferred by the use of machine learning.</p>
<div id="tbl-offensive-ml-use-cases" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-offensive-ml-use-cases-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15.8: Offensive machine learning use cases.
</figcaption>
<div aria-describedby="tbl-offensive-ml-use-cases-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Offensive Use Case</th>
<th style="text-align: left;">ML Model Type</th>
<th style="text-align: left;">Targeted System Vulnerability</th>
<th style="text-align: left;">Advantage of ML</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Phishing and Social Engineering</td>
<td style="text-align: left;">Large Language Models (LLMs)</td>
<td style="text-align: left;">Human perception and communication systems</td>
<td style="text-align: left;">Personalized, context-aware message crafting</td>
</tr>
<tr class="even">
<td style="text-align: left;">Reconnaissance and Fingerprinting</td>
<td style="text-align: left;">Supervised classifiers, clustering models</td>
<td style="text-align: left;">System configuration, network behavior</td>
<td style="text-align: left;">Scalable, automated profiling of system behavior</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Exploit Generation</td>
<td style="text-align: left;">Code generation models, fine-tuned transformers</td>
<td style="text-align: left;">Software bugs, insecure code patterns</td>
<td style="text-align: left;">Automated discovery of candidate exploits</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data Extraction (Inference Attacks)</td>
<td style="text-align: left;">Classification models, inversion models</td>
<td style="text-align: left;">Privacy leakage through model outputs</td>
<td style="text-align: left;">Inference with limited or black-box access</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Evasion of Detection Systems</td>
<td style="text-align: left;">Adversarial input generators</td>
<td style="text-align: left;">Detection boundaries in deployed ML systems</td>
<td style="text-align: left;">Crafting minimally perturbed inputs to evade filters</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hardware-Level Attacks</td>
<td style="text-align: left;">CNNs and RNNs for time-series analysis</td>
<td style="text-align: left;">Physical side-channels (e.g., power, timing, EM)</td>
<td style="text-align: left;">Learning leakage patterns directly from raw signals</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Each of these scenarios illustrates how machine learning models can serve as amplifiers of adversarial capability. For example, language models enable more convincing and adaptable phishing attacks, while clustering and classification algorithms facilitate reconnaissance by learning system-level behavioral patterns. Similarly, adversarial example generators and inference models systematically uncover weaknesses in decision boundaries or data privacy protections, often requiring only limited external access to deployed systems. In hardware contexts, as discussed in the next section, deep neural networks trained on side-channel data can automate the extraction of cryptographic secrets from physical measurements‚Äîtransforming an expert-driven process into a learnable pattern recognition task.</p>
<p>Although these applications differ in technical implementation, they share a common foundation: the adversary replaces a static exploit with a learned model capable of approximating or adapting to the target‚Äôs vulnerable behavior. This shift increases flexibility, reduces manual overhead, and improves robustness in the face of evolving or partially obscured defenses.</p>
<p>What makes this class of threats particularly significant is their favorable scaling behavior. Just as accuracy in computer vision or language modeling improves with additional data, larger architectures, and greater compute resources, so too does the performance of attack-oriented machine learning models. A model trained on larger corpora of phishing attempts or power traces, for instance, may generalize more effectively, evade more detectors, or require fewer inputs to succeed. The same ecosystem that drives innovation in beneficial AI, including public datasets, open-source tooling, and scalable infrastructure, also lowers the barrier to developing effective offensive models.</p>
<p>This dynamic creates an asymmetry between attacker and defender. While defensive measures are bounded by deployment constraints, latency budgets, and regulatory requirements, attackers can scale training pipelines with minimal marginal cost. The widespread availability of pretrained models and public ML platforms further reduces the expertise required to develop high-impact attacks.</p>
<p>As a result, any comprehensive treatment of machine learning system security must consider not only the vulnerabilities of ML systems themselves but also the ways in which machine learning can be harnessed to compromise other components‚Äîwhether software, data, or hardware. Understanding the offensive potential of machine-learned systems is essential for designing resilient, trustworthy, and forward-looking defenses.</p>
<section id="case-study-deep-learning-for-sca" class="level3 page-columns page-full" data-number="15.8.1">
<h3 data-number="15.8.1" class="anchored" data-anchor-id="case-study-deep-learning-for-sca"><span class="header-section-number">15.8.1</span> Case Study: Deep Learning for SCA</h3>
<p>One of the most well-known and reproducible demonstrations of deep-learning-assisted SCA is the SCAAML framework (Side-Channel Attacks Assisted with Machine Learning) <span class="citation" data-cites="scaaml_2019">(<a href="../references.html#ref-scaaml_2019" role="doc-biblioref">Bursztein et al. 2019</a>)</span>. Developed by researchers at Google, SCAAML provides a practical implementation of the attack pipeline described above.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-side-channel-curves" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-side-channel-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/side-effects-binary.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Figure&nbsp;15.17: Power traces from an AES S-box operation. Source: @scaaml_2019."><img src="images/png/side-effects-binary.png" class="img-fluid figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-side-channel-curves-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.17: Power traces from an AES S-box operation. Source: <span class="citation" data-cites="scaaml_2019">Bursztein et al. (<a href="../references.html#ref-scaaml_2019" role="doc-biblioref">2019</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>As shown in <a href="#fig-side-channel-curves" class="quarto-xref">Figure&nbsp;<span>15.17</span></a>, cryptographic computations exhibit data-dependent variations in their power consumption. These variations, while subtle, are measurable and reflect the internal state of the algorithm at specific points in time.</p>
<p>In traditional side-channel attacks, experts rely on statistical techniques to extract these differences. However, a neural network can learn to associate the shape of these signals with the specific data values being processed, effectively learning to decode the signal in a manner that mimics expert-crafted models, yet with enhanced flexibility and generalization. The model is trained on labeled examples of power traces and their corresponding intermediate values (e.g., output of an S-box operation). Over time, it learns to associate patterns in the trace, similar to those depicted in <a href="#fig-side-channel-curves" class="quarto-xref">Figure&nbsp;<span>15.17</span></a>, with secret-dependent computational behavior. This transforms the key recovery task into a classification problem, where the goal is to infer the correct key byte based on trace shape alone.</p>
<p>In their study, <span class="citation" data-cites="scaaml_2019">Bursztein et al. (<a href="../references.html#ref-scaaml_2019" role="doc-biblioref">2019</a>)</span> trained a convolutional neural network to extract AES keys from power traces collected on an STM32F415 microcontroller running the open-source TinyAES implementation. The model was trained to predict intermediate values of the AES algorithm, such as the output of the S-box in the first round, directly from raw power traces. Remarkably, the trained model was able to recover the full 128-bit key using only a small number of traces per byte.</p>
<div class="no-row-height column-margin column-container"></div><p>The traces were collected using a ChipWhisperer setup with a custom STM32F target board, shown in <a href="#fig-stm32f-board" class="quarto-xref">Figure&nbsp;<span>15.18</span></a>. This board executes AES operations while allowing external equipment to monitor power consumption with high temporal precision. The experimental setup captures how even inexpensive, low-power embedded devices can leak information through side channels‚Äîinformation that modern machine learning models can learn to exploit.</p>
<div id="fig-stm32f-board" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-stm32f-board-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/stm32f_board.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Figure&nbsp;15.18: STM32F415 target board used in SCAAML experiments. Source: @scaaml_2019."><img src="images/png/stm32f_board.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stm32f-board-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.18: STM32F415 target board used in SCAAML experiments. Source: <span class="citation" data-cites="scaaml_2019">Bursztein et al. (<a href="../references.html#ref-scaaml_2019" role="doc-biblioref">2019</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-scaaml_2019" class="csl-entry" role="listitem">
Bursztein, Elie, Luca Invernizzi, Karel Kr√°l, and Jean-Michel Picod. 2019. <span>‚ÄúSCAAML: Side Channel Attacks Assisted with Machine Learning.‚Äù</span> <a href="https://github.com/google/scaaml">https://github.com/google/scaaml</a>.
</div></div></figure>
</div>
<p>Subsequent work expanded on this approach by introducing long-range models capable of leveraging broader temporal dependencies in the traces, improving performance even under noise and desynchronization <span class="citation" data-cites="bursztein2023generic">(<a href="../references.html#ref-bursztein2023generic" role="doc-biblioref">Bursztein et al. 2024</a>)</span>. These developments highlight the potential for machine learning models to serve as offensive cryptanalysis tools‚Äîespecially in the analysis of secure hardware.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bursztein2023generic" class="csl-entry" role="listitem">
Bursztein, Elie, Luca Invernizzi, Karel Kr√°l, Daniel Moghimi, Jean-Michel Picod, and Marina Zhang. 2024. <span>‚ÄúGeneralized Power Attacks Against Crypto Hardware Using Long-Range Deep Learning.‚Äù</span> <em>IACR Transactions on Cryptographic Hardware and Embedded Systems</em> 2024 (3): 472‚Äì99. <a href="https://doi.org/10.46586/tches.v2024.i3.472-499">https://doi.org/10.46586/tches.v2024.i3.472-499</a>.
</div></div><p>The implications extend beyond academic interest. As deep learning models continue to scale, their application to side-channel contexts is likely to lower the cost, skill threshold, and trace requirements of hardware-level attacks‚Äîposing a growing challenge for the secure deployment of embedded machine learning systems, cryptographic modules, and trusted execution environments.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">15.9</span> Conclusion</h2>
<p>Security and privacy are foundational to the deployment of machine learning systems in real-world environments. As ML moves beyond the lab and into production, as it is deployed across cloud services, edge devices, mobile platforms, and critical infrastructure, the threats it faces become more complex and more consequential. From model theft and data leakage to adversarial manipulation and hardware compromise, securing ML systems requires a comprehensive understanding of the entire software and hardware stack.</p>
<p>This chapter explored these challenges from multiple angles. We began by examining real-world security incidents and threat models that impact ML systems, including attacks on training data, inference pipelines, and deployed models. We then discussed defense strategies that operate at different layers of the system: from data privacy techniques like differential privacy and federated learning, to robust model design, secure deployment practices, runtime monitoring, and hardware-enforced trust. Each of these layers addresses a distinct surface of vulnerability, and together they form the basis of a defense-in-depth approach.</p>
<p>Importantly, security is not a static checklist. It is an evolving process shaped by the deployment context, the capabilities of adversaries, and the risk tolerance of stakeholders. What protects a publicly exposed API may not suffice for an embedded medical device or a distributed fleet of autonomous systems. The effectiveness of any given defense depends on how well it fits into the larger system and how it interacts with other components, users, and constraints.</p>
<p>The goal of this chapter was not to catalog every threat or prescribe a fixed set of solutions. Rather, it was to help build the mindset needed to design secure, private, and trustworthy ML systems‚Äîsystems that perform reliably under pressure, protect the data they rely on, and respond gracefully when things go wrong.</p>
<p>As we look ahead, security and privacy will remain intertwined with other system concerns: robustness, fairness, sustainability, and operational scale. In the chapters that follow, we will explore these additional dimensions and extend the foundation laid here toward the broader challenge of building ML systems that are not only performant, but responsible, reliable, and resilient by design.</p>
</section>
<section id="resources" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="resources"><span class="header-section-number">15.10</span> Resources</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Slides">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled" title="Videos">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Videos
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled" title="Exercises">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercises
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "harvard-edge/cs249r_book";
    script.dataset.repoId = "R_kgDOKQSOaw";
    script.dataset.category = "General";
    script.dataset.categoryId = "DIC_kwDOKQSOa84CZ8Ry";
    script.dataset.mapping = "title";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="pagination-link" aria-label="On-Device Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="pagination-link" aria-label="Responsible AI">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/widget_quiz/contents/core/privacy_security/privacy_security.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/widget_quiz/contents/core/privacy_security/privacy_security.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>