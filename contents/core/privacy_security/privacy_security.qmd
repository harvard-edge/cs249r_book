---
bibliography: privacy_security.bib
---

# Security & Privacy

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-security-and-privacy-resource),  [Videos](#sec-security-and-privacy-resource), [Exercises](#sec-security-and-privacy-resource)
:::

![_DALL·E 3 Prompt: An illustration on privacy and security in machine learning systems. The image shows a digital landscape with a network of interconnected nodes and data streams, symbolizing machine learning algorithms. In the foreground, there's a large lock superimposed over the network, representing privacy and security. The lock is semi-transparent, allowing the underlying network to be partially visible. The background features binary code and digital encryption symbols, emphasizing the theme of cybersecurity. The color scheme is a mix of blues, greens, and grays, suggesting a high-tech, digital environment._](images/png/cover_security_privacy.png)

## Purpose {.unnumbered}

_What principles guide the protection of machine learning systems, and how do security and privacy requirements shape system architecture?_

Protection mechanisms are a fundamental dimension of modern AI system design. Security considerations expose critical patterns for safeguarding data, models, and infrastructure while sustaining operational effectiveness. Implementing defensive strategies reveals inherent trade-offs between protection, performance, and usability—trade-offs that influence architectural decisions throughout the AI lifecycle. Understanding these dynamics is essential for creating trustworthy systems, grounding the principles needed to preserve privacy and defend against adversarial threats while maintaining functionality in production environments.

::: {.callout-tip title="Learning Objectives"}

* Understand key ML privacy and security risks, such as data leaks, model theft, adversarial attacks, bias, and unintended data access.

* Learn from historical hardware and embedded systems security incidents.

* Identify threats to ML models like data poisoning, model extraction, membership inference, and adversarial examples.

* Recognize hardware security threats to embedded ML spanning hardware bugs, physical attacks, side channels, counterfeit components, etc.

* Explore embedded ML defenses, such as trusted execution environments, secure boot, physical unclonable functions, and hardware security modules.

* Discuss privacy issues handling sensitive user data with embedded ML, including regulations.

* Learn privacy-preserving ML techniques like differential privacy, federated learning, homomorphic encryption, and synthetic data generation.

* Understand trade-offs between privacy, accuracy, efficiency, threat models, and trust assumptions.

* Recognize the need for a cross-layer perspective spanning electrical, firmware, software, and physical design when securing embedded ML devices.

:::

## Overview

Machine learning systems, like all computational systems, must be designed not only for performance and accuracy but also for security and privacy. These concerns shape the architecture and operation of ML systems across their lifecycle—from data collection and model training to deployment and user interaction. While traditional system security focuses on software vulnerabilities, network protocols, and hardware defenses, machine learning systems introduce unique attack surfaces. These include threats to the data that fuels learning, the models that encode behavior, and the infrastructure that serves predictions.

Security and privacy mechanisms in ML systems perform roles comparable to trust and access control layers in classical computing systems. Just as operating systems enforce user permissions and secure resource boundaries, ML systems must implement controls that safeguard sensitive data, protect proprietary models, and defend against adversarial manipulation. These mechanisms span software, hardware, and organizational processes, forming a critical layer of system reliability.

Although closely related, security and privacy address different aspects of protecting machine learning systems. Security focuses on safeguarding system integrity and availability, while privacy focuses on controlling the exposure and use of sensitive information. The following definitions clarify these distinctions.

:::{.callout-note title="Security Definition"}
**Security** in machine learning systems is the *protection of data, models, and infrastructure* from unauthorized access, manipulation, or disruption. It spans the *design and implementation* of defensive mechanisms that protect against data poisoning, model theft, adversarial manipulation, and system-level vulnerabilities. Security mechanisms ensure the *integrity*, *confidentiality*, and *availability* of machine learning services across development, deployment, and operational environments.
:::

:::{.callout-note title="Privacy Definition"}
**Privacy** in machine learning systems is the *protection of sensitive information* from unauthorized disclosure, inference, or misuse. It spans the *design and implementation* of methods that reduce the risk of exposing personal, proprietary, or regulated data while enabling machine learning systems to operate effectively. Privacy mechanisms help preserve *confidentiality* and *control* over data usage across development, deployment, and operational environments.
:::

Security and privacy are not mutually exclusive; they are interdependent. A secure system is essential for maintaining privacy, as breaches in security can lead to unauthorized access to sensitive data. Conversely, privacy measures can enhance security by limiting the exposure of sensitive information and reducing the attack surface for potential threats.

Security and privacy are inherently multi-faceted and trade-off-laden. Effective defenses often introduce overheads in computation, memory, and usability. Achieving the right balance requires understanding the threat landscape, selecting appropriate defense mechanisms, and making informed design trade-offs that align with system goals.

The landscape of security and privacy challenges in ML systems continues to evolve. High-profile incidents such as model extraction attacks, data leakage from generative AI systems, and hardware-level vulnerabilities have highlighted the need for comprehensive and adaptive defenses. Solutions must address not only technical threats but also regulatory, ethical, and operational requirements across cloud, edge, and embedded deployments.

As we advance through this chapter, we will explore the threats facing ML systems, the defensive mechanisms available, and the trade-offs involved in securing real-world deployments. Understanding these concepts is critical for building trustworthy machine learning systems that perform reliably under adversarial and privacy-sensitive conditions.

## Historical Incidents

While the security of machine learning systems introduces new technical challenges, valuable lessons can be drawn from well-known security breaches across a range of computing systems. These incidents demonstrate how weaknesses in system design—whether in industrial control systems, connected vehicles, or consumer devices—can lead to widespread, and sometimes physical, consequences. Although the examples discussed in this section do not all involve machine learning directly, they provide critical insights into the importance of designing secure systems. These lessons apply broadly to machine learning applications deployed across cloud, edge, and embedded environments.

### Stuxnet

In 2010, security researchers discovered a highly sophisticated computer worm later named [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf), which targeted industrial control systems used in Iran’s Natanz nuclear facility [@farwell2011stuxnet]. Stuxnet exploited four previously unknown "[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)" vulnerabilities in Microsoft Windows, allowing it to spread undetected through both networked and isolated systems.

Unlike typical malware designed to steal information or perform espionage, Stuxnet was engineered to cause physical damage. Its objective was to disrupt uranium enrichment by sabotaging the centrifuges used in the process. Despite the facility being air-gapped from external networks, the malware is believed to have entered the system via an infected USB device, demonstrating how physical access can compromise even isolated environments.

Stuxnet represents a landmark in cybersecurity, revealing how malicious software can bridge the digital and physical worlds to manipulate industrial infrastructure. It specifically targeted programmable logic controllers (PLCs) responsible for automating electromechanical processes, such as controlling the speed of centrifuges. By exploiting vulnerabilities in the Windows operating system and the Siemens Step7 software used to program the PLCs, Stuxnet achieved highly targeted, real-world disruption.

While Stuxnet did not target machine learning systems directly, its relevance extends to any system where software interacts with physical processes. Machine learning is increasingly integrated into industrial control, robotics, and cyber-physical systems, making these lessons applicable to the security of modern ML deployments. Figure @fig-stuxnet illustrates the operation of Stuxnet in greater detail.

![Stuxnet explained. Source: IEEE Spectrum](images/png/stuxnet.png){#fig-stuxnet}

### Jeep Cherokee Hack

In 2015, security researchers publicly demonstrated a remote cyberattack on a Jeep Cherokee that exposed critical vulnerabilities in automotive system design \[@miller2019lessons]. Conducted as a controlled experiment, the researchers exploited a vulnerability in the vehicle’s Uconnect entertainment system, which was connected to the internet via a cellular network. By gaining remote access to this system, they were able to send commands that affected the vehicle’s engine, transmission, and braking systems—without physical access to the car.

This demonstration served as a wake-up call for the automotive industry. It highlighted the risks posed by the growing connectivity of modern vehicles. Traditionally isolated automotive control systems, such as those managing steering and braking, were shown to be vulnerable when exposed through externally accessible software interfaces. The ability to remotely manipulate safety-critical functions raised serious concerns about passenger safety, regulatory oversight, and industry best practices.

:::{#vid-jeephack .callout-important title="Jeep Cherokee Hack"}

{{< video [https://www.youtube.com/watch?v=MK0SrxBC1xs\&ab\_channel=WIRED](https://www.youtube.com/watch?v=MK0SrxBC1xs&ab_channel=WIRED) >}}

:::

The incident also led to a recall of over 1.4 million vehicles to patch the vulnerability, highlighting the need for manufacturers to prioritize cybersecurity in their designs. The National Highway Traffic Safety Administration (NHTSA) issued guidelines for automakers to improve vehicle cybersecurity, including recommendations for secure software development practices and incident response protocols.

The automotive industry has since made significant strides in addressing these vulnerabilities, but the incident serves as a cautionary tale for all sectors that rely on connected systems. As machine learning becomes more prevalent in safety-critical applications, the lessons learned from the Jeep Cherokee hack will be essential for ensuring the security and reliability of future ML deployments.

Although this incident did not involve machine learning, the architectural patterns it exposed are highly relevant to ML system security. Modern vehicles increasingly rely on machine learning for driver-assistance, navigation, and in-cabin intelligence—features that operate alongside connected software services. This integration expands the potential attack surface if systems are not properly isolated or secured. The Jeep Cherokee hack highlights the need for defense-in-depth strategies, secure software updates, authenticated communications, and rigorous security testing—principles that apply broadly to machine learning systems deployed across automotive, industrial, and consumer environments.

As machine learning continues to be integrated into connected and safety-critical applications, the lessons from the Jeep Cherokee hack remain highly relevant. They emphasize that securing externally connected software is not just a best practice but a necessity for protecting the integrity and safety of machine learning-enabled systems.

### Mirai Botnet

In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/) emerged as one of the most disruptive distributed denial-of-service (DDoS) attacks in internet history [@antonakakis2017understanding]. The botnet infected thousands of networked devices, including digital cameras, DVRs, and other consumer electronics. These devices, often deployed with factory-default usernames and passwords, were easily compromised by the Mirai malware and enlisted into a large-scale attack network.

The Mirai botnet was used to overwhelm major internet infrastructure providers, disrupting access to popular online services across the United States and beyond. The scale of the attack demonstrated how vulnerable consumer and industrial devices can become a platform for widespread disruption when security is not prioritized in their design and deployment.

:::{#vid-mirai .callout-important title="Mirai Botnet"}

{{< video [https://www.youtube.com/watch?v=1pywzRTJDaY](https://www.youtube.com/watch?v=1pywzRTJDaY) >}}

:::

While the devices exploited by Mirai did not include machine learning components, the architectural patterns exposed by this incident are increasingly relevant as machine learning expands into edge computing and Internet of Things (IoT) devices. Many ML-enabled products, such as smart cameras, voice assistants, and edge analytics platforms, share similar deployment characteristics—operating on networked devices with limited hardware resources, often managed at scale.

The Mirai botnet highlights the critical importance of basic security hygiene, including secure credential management, authenticated software updates, and network access control. Without these protections, even powerful machine learning models can become part of larger attack infrastructures if deployed on insecure hardware.

As machine learning continues to move beyond centralized data centers into distributed and networked environments, the lessons from the Mirai botnet remain highly relevant. They emphasize the need for secure device provisioning, ongoing vulnerability management, and industry-wide coordination to prevent large-scale exploitation of ML-enabled systems.

## Secure Design Priorities

The historical breaches described earlier reveal how weaknesses in system design—whether in hardware, software, or network infrastructure—can lead to widespread and often physical consequences. While these incidents did not directly target machine learning systems, they offer valuable insights into architectural and operational patterns that increasingly characterize modern ML deployments. These lessons point to three overarching areas of concern: device-level security, system-level isolation and control, and protection against large-scale network exploitation.

### Device-Level Security

The Mirai botnet exemplifies how large-scale exploitation of poorly secured devices can lead to significant disruption. This attack succeeded by exploiting common weaknesses such as default usernames and passwords, unsecured firmware update mechanisms, and unencrypted communications. While often associated with consumer-grade IoT products, these vulnerabilities are increasingly relevant to machine learning systems, particularly those deployed at the edge.

Edge ML devices—such as smart cameras, industrial controllers, and wearable health monitors—typically rely on lightweight embedded hardware like ARM-based processors running minimal operating systems. These systems are designed for low-power, distributed operation but often lack the comprehensive security features found in larger computing platforms. As these devices take on more responsibility for local data processing and real-time decision-making, they become attractive targets for remote compromise.

A compromised population of such devices can be aggregated into a botnet, similar to Mirai, and leveraged for large-scale attacks. Beyond denial-of-service threats, attackers could use these ML-enabled devices to exfiltrate sensitive data, interfere with model execution, or manipulate system outputs. Without strong device-level protections—including secure boot processes, authenticated firmware updates, and encrypted communications—edge ML deployments remain vulnerable to being turned into platforms for broader system disruption.

### System-Level Isolation

The Jeep Cherokee hack highlighted the risks that arise when externally connected software services are insufficiently isolated from safety-critical system functions. By exploiting a vulnerability in the vehicle’s Uconnect entertainment system, researchers were able to remotely manipulate core control functions such as steering and braking. This incident demonstrated that network connectivity, if not carefully managed, can expose critical system pathways to external threats.

Machine learning systems increasingly operate in similar contexts, particularly in domains such as automotive safety, healthcare, and industrial automation. Modern vehicles, for example, integrate machine learning models for driver-assistance, autonomous navigation, and sensor fusion. These models run alongside connected software services that provide infotainment, navigation updates, and remote diagnostics. Without strong system-level isolation, attackers can exploit these externally facing services to gain access to safety-critical ML components, expanding the overall attack surface.

The automotive industry’s response to the Jeep Cherokee incident—including large-scale recalls, over-the-air software patches, and the development of industry-wide cybersecurity standards through organizations such as Auto-ISAC and the National Highway Traffic Safety Administration (NHTSA)—provides a valuable example of how industries can address emerging ML security risks.

Similar isolation principles apply to other machine learning deployments, including medical devices that analyze patient data in real time, industrial controllers that optimize manufacturing processes, and infrastructure systems that manage power grids or water supplies. Securing these systems requires architectural compartmentalization of subsystems, authenticated communication channels, and validated update mechanisms. These measures help prevent external actors from escalating access or manipulating ML-driven decision-making in safety-critical environments.

### Large-Scale Network Exploitation

The Stuxnet attack demonstrated the ability of targeted cyber operations to cross from digital systems into the physical world, resulting in real-world disruption and damage. By exploiting software vulnerabilities in industrial control systems, the attack caused mechanical failures in uranium enrichment equipment. While Stuxnet did not target machine learning systems directly, it revealed critical risks that apply broadly to cyber-physical systems—particularly those involving supply chain vulnerabilities, undisclosed (zero-day) exploits, and techniques for bypassing network isolation, such as air gaps.

As machine learning increasingly powers decision-making in manufacturing, energy management, robotics, and other operational technologies, similar risks emerge. ML-based controllers that influence physical processes—such as adjusting production lines, managing industrial robots, or optimizing power distribution—represent new attack surfaces. Compromising these models or the systems that deploy them can result in physical harm, operational disruption, or strategic manipulation of critical infrastructure.

Stuxnet’s sophistication highlights the potential for state-sponsored or well-resourced adversaries to target ML-driven systems as part of larger geopolitical or economic campaigns. As machine learning takes on more influential roles in controlling real-world systems, securing these deployments against both cyber and physical threats becomes essential for ensuring operational resilience and public safety.

### Toward Secure Design

Collectively, these incidents illustrate that security must be designed into machine learning systems from the outset. Protecting such systems requires attention to multiple layers of the stack, including model-level protections to defend against attacks such as model theft, adversarial manipulation, and data leakage; data pipeline security to ensure the confidentiality, integrity, and governance of training and inference data across cloud, edge, and embedded environments; system-level isolation and access control to prevent external interfaces from compromising model execution or manipulating safety-critical outputs; secure deployment and update mechanisms to safeguard runtime environments from tampering or exploitation; and continuous monitoring and incident response capabilities to detect and recover from breaches in dynamic, distributed deployments.

These priorities reflect the lessons drawn from past incidents—emphasizing the need to protect device-level resources, isolate critical system functions, and defend against large-scale exploitation. The remainder of this chapter builds on these principles, beginning with a closer examination of threats specific to machine learning models and data. It then expands the discussion to hardware-level vulnerabilities and the unique considerations of embedded ML systems. Finally, it explores defensive strategies, including privacy-preserving techniques, secure hardware mechanisms, and system-level design practices, forming a foundation for building trustworthy machine learning systems capable of withstanding both known and emerging threats.

## Threats to ML Models

Building on the lessons from historical security incidents, we now turn to threats that are specific to machine learning models. These threats span the entire ML lifecycle—from training-time manipulations to inference-time evasion—and fall into three broad categories: threats to model confidentiality (e.g., model theft), threats to training integrity (e.g., data poisoning), and threats to inference robustness (e.g., adversarial examples). Each category targets different vulnerabilities and requires distinct defensive strategies.

Three primary threats stand out in this context: model theft, where adversaries steal proprietary models and the sensitive knowledge they encode; data poisoning, where attackers manipulate training data to corrupt model behavior; and adversarial attacks, where carefully crafted inputs deceive models into making incorrect predictions. Each of these threats exploits different stages of the machine learning lifecycle—from data ingestion and model training to deployment and inference.

We begin with model theft, examining how attackers extract or replicate models to undermine economic value and privacy.

### Model Theft

Threats to model confidentiality arise when adversaries gain access to a trained model’s parameters, architecture, or output behavior. These attacks can undermine the economic value of machine learning systems, enable competitors to replicate proprietary functionality, or expose private information encoded in model weights.

Such threats arise across a range of deployment settings, including public APIs, cloud-hosted services, on-device inference engines, and shared model repositories. Machine learning models may be vulnerable due to exposed interfaces, insecure serialization formats, or insufficient access controls—factors that create opportunities for unauthorized extraction or replication [@ateniese2015hacking].

High-profile legal cases have highlighted the strategic and economic value of machine learning models. For example, former Google engineer Anthony Levandowski was accused of [stealing proprietary designs from Waymo](https://www.nytimes.com/2017/02/23/technology/google-self-driving-waymo-uber-otto-lawsuit.html), including critical components of its autonomous vehicle technology, before founding a competing startup. Such cases illustrate the potential for insider threats to bypass technical protections and gain access to sensitive intellectual property.

The consequences of model theft extend beyond economic loss. Stolen models can be used to extract sensitive information, replicate proprietary algorithms, or enable further attacks. For instance, a competitor who obtains a stolen recommendation model from an e-commerce platform might gain insights into customer behavior, business analytics, and embedded trade secrets. This knowledge can also be used to conduct model inversion attacks, where an attacker attempts to infer private details about the model’s training data.

In a model inversion attack, the adversary queries the model through a legitimate interface, such as a public API, and observes its outputs. By analyzing confidence scores or output probabilities, the attacker can optimize inputs to reconstruct data resembling the model’s training set. For example, a facial recognition model used for secure access could be manipulated to reveal statistical properties of the employee photos on which it was trained. Similar vulnerabilities have been demonstrated in studies on the Netflix Prize dataset, where researchers were able to infer individual movie preferences from anonymized data [@narayanan2006break].

Model theft can target two distinct objectives: extracting exact model properties, such as architecture and parameters, or replicating approximate model behavior to produce similar outputs without direct access to internal representations. Both forms of theft undermine the security and value of machine learning systems, as explored in the following subsections.

#### Exact Model Theft

Exact model property theft refers to attacks aimed at extracting the internal structure and learned parameters of a machine learning model. These attacks often target deployed models that are exposed through APIs, embedded in on-device inference engines, or shared as downloadable model files on collaboration platforms. Exploiting weak access control, insecure model packaging, or unprotected deployment interfaces, attackers can recover proprietary model assets without requiring full control of the underlying infrastructure.

These attacks typically seek three types of information. The first is the model’s learned parameters, such as weights and biases. By extracting these parameters, attackers can replicate the model’s functionality without incurring the cost of training. This replication allows them to benefit from the model’s performance while bypassing the original development effort.

The second target is the model’s fine-tuned hyperparameters, including training configurations such as learning rate, batch size, and regularization settings. These hyperparameters significantly influence model performance, and stealing them enables attackers to reproduce high-quality results with minimal additional experimentation.

Finally, attackers may seek to reconstruct the model’s architecture. This includes the sequence and types of layers, activation functions, and connectivity patterns that define the model’s behavior. Architecture theft may be accomplished through side-channel attacks, reverse engineering, or analysis of observable model behavior. Revealing the architecture not only compromises intellectual property but also gives competitors strategic insights into the design choices that provide competitive advantage.

System designers must account for these risks by securing model serialization formats, restricting access to runtime APIs, and hardening deployment pipelines. Protecting models requires a combination of software engineering practices, including access control, encryption, and obfuscation techniques, to reduce the risk of unauthorized extraction.

#### Approximate Model Theft

While some attackers seek to extract a model’s exact internal properties, others focus on replicating its external behavior. Approximate model behavior theft refers to attacks that attempt to recreate a model’s decision-making capabilities without directly accessing its parameters or architecture. Instead, attackers observe the model’s inputs and outputs to build a substitute model that performs similarly on the same tasks.

This type of theft often targets models deployed as services, where the model is exposed through an API or embedded in a user-facing application. By repeatedly querying the model and recording its responses, an attacker can train their own model to mimic the behavior of the original. This process, often called model distillation or knockoff modeling, enables attackers to achieve comparable functionality without access to the original model’s proprietary internals.

Attackers may evaluate the success of behavior replication in two ways. The first is by measuring the level of effectiveness of the substitute model. This involves assessing whether the cloned model achieves similar accuracy, precision, recall, or other performance metrics on benchmark tasks. By aligning the substitute’s performance with that of the original, attackers can build a model that is practically indistinguishable in effectiveness, even if its internal structure differs.

The second is by testing prediction consistency. This involves checking whether the substitute model produces the same outputs as the original model when presented with the same inputs. Matching not only correct predictions but also the original model’s mistakes can provide attackers with a high-fidelity reproduction of the target model’s behavior. This is particularly concerning in applications such as natural language processing, where attackers might replicate sentiment analysis models to gain competitive insights or bypass proprietary systems.

Approximate behavior theft is particularly challenging to defend against in open-access deployment settings, such as public APIs or consumer-facing applications. Limiting the rate of queries, detecting automated extraction patterns, and watermarking model outputs are among the techniques that can help mitigate this risk. However, these defenses must be balanced with usability and performance considerations, especially in production environments.

#### Case Study: Tesla IP Theft

In 2018, Tesla filed a [lawsuit](https://storage.courtlistener.com/recap/gov.uscourts.nvd.131251/gov.uscourts.nvd.131251.1.0_1.pdf) against the self-driving car startup [Zoox](https://zoox.com/), alleging that former Tesla employees had stolen proprietary data and trade secrets related to Tesla’s autonomous driving technology. According to the lawsuit, several employees transferred over 10 gigabytes of confidential files, including machine learning models and source code, before leaving Tesla to join Zoox.

Among the stolen materials was a key image recognition model used for object detection in Tesla’s self-driving system. By obtaining this model, Zoox could have bypassed years of research and development, giving the company a competitive advantage. Beyond the economic implications, there were concerns that the stolen model could expose Tesla to further security risks, such as model inversion attacks aimed at extracting sensitive data from the model’s training set.

The Zoox employees denied any wrongdoing, and the case was ultimately settled out of court. Nevertheless, the incident highlights the real-world risks of model theft, particularly in industries where machine learning models represent significant intellectual property. The theft of models not only undermines competitive advantage but also raises broader concerns about privacy, safety, and the potential for downstream exploitation.

This case demonstrates that model theft is not limited to theoretical attacks conducted over APIs or public interfaces. Insider threats, supply chain vulnerabilities, and unauthorized access to development infrastructure pose equally serious risks to machine learning systems deployed in commercial environments. 

### Data Poisoning

Training integrity threats stem from the manipulation of data used to train machine learning models. These attacks aim to corrupt the learning process by introducing examples that appear benign but induce harmful or biased behavior in the final model.

Data poisoning attacks are a prominent example, in which adversaries inject carefully crafted data points into the training set to influence model behavior in targeted or systemic ways [@biggio2012poisoning]. Poisoned data may cause a model to make incorrect predictions, degrade its generalization ability, or embed failure modes that remain dormant until triggered post-deployment.

Data poisoning is a security threat because it involves intentional manipulation of the training data by an adversary, with the goal of embedding vulnerabilities or subverting model behavior. These attacks are especially concerning in applications where models retrain on data collected from external sources—such as user interactions, crowdsourced annotations, or online scraping—since attackers can inject poisoned data without direct access to the training pipeline. Even in more controlled settings, poisoning may occur through compromised data storage, insider manipulation, or insecure data transfer processes.

From a security perspective, poisoning attacks vary depending on the attacker’s level of access and knowledge. In white-box scenarios, the adversary may have detailed insight into the model architecture or training process, enabling more precise manipulation. In contrast, black-box or limited-access attacks exploit open data submission channels or indirect injection vectors. Poisoning can target different stages of the ML pipeline—from data collection and preprocessing to labeling and storage—making the attack surface both broad and system-dependent.

Poisoning attacks typically follow a three-stage process. First, the attacker injects malicious data into the training set. These examples are often designed to appear legitimate but introduce subtle distortions that alter the model’s learning process. Second, the model trains on this compromised data, embedding the attacker’s intended behavior. Finally, once the model is deployed, the attacker may exploit the altered behavior to cause mispredictions, bypass safety checks, or degrade overall reliability.

Formally, data poisoning can be viewed as a bilevel optimization problem, where the attacker seeks to select poisoning data $D_p$ that maximizes the model’s loss on a validation or target dataset $D_{\text{test}}$. Let $D$ represent the original training data. The attacker’s objective can be written as:

$$
\max_{D_p} \ \mathcal{L}(f_{D \cup D_p}, D_{\text{test}})
$$

where $f_{D \cup D_p}$ is the model trained on the combined dataset. For targeted attacks, this objective may focus on specific inputs $x_t$ and target labels $y_t$:

$$
\max_{D_p} \ \mathcal{L}(f_{D \cup D_p}, x_t, y_t)
$$

This formulation captures the adversary’s goal of introducing carefully crafted data points to manipulate the model’s decision boundaries.

For example, consider a traffic sign classification model trained to distinguish between stop signs and speed limit signs. An attacker might inject a small number of stop sign images labeled as speed limit signs into the training data. The attacker’s goal is to subtly shift the model’s decision boundary so that future stop signs are misclassified as speed limit signs. In this case, the poisoning data $D_p$ consists of mislabeled stop sign images, and the attacker’s objective is to maximize the misclassification of legitimate stop signs $x_t$ as speed limit signs $y_t$, following the targeted attack formulation above. Even if the model performs well on other types of signs, the poisoned training process creates a predictable and exploitable vulnerability.

Data poisoning attacks can be classified based on their objectives and scope of impact. Availability attacks degrade overall model performance by introducing noise or label flips that reduce accuracy across tasks. Targeted attacks manipulate a specific input or class, leaving general performance intact but causing consistent misclassification in select cases. Backdoor attacks embed hidden triggers—often imperceptible patterns—that elicit malicious behavior only when the trigger is present. Subpopulation attacks degrade performance on a specific group defined by shared features, making them particularly dangerous in fairness-sensitive applications.

A notable real-world example of a targeted poisoning attack was demonstrated against Perspective, an online toxicity detection model [@hosseini2017deceiving]. By injecting synthetically generated toxic comments with subtle misspellings and grammatical errors into the model’s training set, researchers degraded its ability to detect harmful content. After retraining, the poisoned model exhibited a significantly higher false negative rate, allowing offensive language to bypass filters. This case illustrates how poisoned data can exploit feedback loops in systems that rely on user-generated input, leading to reduced effectiveness over time and creating long-term vulnerabilities in content moderation pipelines.

Mitigating data poisoning threats requires end-to-end security of the data pipeline, encompassing collection, storage, labeling, and training. Preventative measures include input validation checks, integrity verification of training datasets, and anomaly detection to flag suspicious patterns. In parallel, robust training algorithms can limit the influence of mislabeled or manipulated data by down-weighting or filtering out anomalous instances. While no single technique guarantees immunity, combining proactive data governance, automated monitoring, and robust learning practices is essential for maintaining model integrity in real-world deployments.

### Adversarial Attacks

Inference robustness threats occur when attackers manipulate inputs at test time to induce incorrect predictions. Unlike data poisoning, which compromises the training process, these attacks exploit vulnerabilities in the model’s decision surface during inference.

A central class of such threats is adversarial attacks, where carefully constructed inputs are designed to cause incorrect predictions while remaining nearly indistinguishable from legitimate data [@parrish2023adversarial]. These attacks highlight a critical weakness in many ML models: their sensitivity to small, targeted perturbations that can drastically alter output confidence or classification results.

The central vulnerability arises from the model's sensitivity to small, targeted perturbations. A single image, for instance, can be subtly altered—changing just a few pixel values—such that a classifier misidentifies a stop sign as a speed limit sign. In natural language processing, specially crafted input sequences may trigger toxic or misleading outputs in a generative model, even when the prompt appears benign to a human reader [@ramesh2021zero; @rombach2022highresolution].

Adversarial attacks pose critical safety and security risks in domains such as autonomous driving, biometric authentication, and content moderation. Unlike data poisoning, which corrupts the model during training, adversarial attacks manipulate the model's behavior at test time, often without requiring any access to the training data or model internals. The attack surface thus shifts from upstream data pipelines to real-time interaction, demanding robust defense mechanisms capable of detecting or mitigating malicious inputs at the point of inference.

Adversarial example generation can be formally described as a constrained optimization problem, where the attacker seeks to find a minimally perturbed version of a legitimate input that maximizes the model’s prediction error. Given an input $x$ with true label $y$, the attacker’s objective is to find a perturbed input $x' = x + \delta$ that maximizes the model’s loss:

$$
\max_{\delta} \ \mathcal{L}(f(x + \delta), y)
$$

subject to the constraint:

$$
\|\delta\| \leq \epsilon
$$

where $f(\cdot)$ is the model, $\mathcal{L}$ is the loss function, and $\epsilon$ defines the allowed perturbation magnitude. This ensures that the perturbation remains small, often imperceptible to humans, while still leading the model to produce an incorrect output.

This optimization view underlies common adversarial strategies used in both white-box and black-box settings. A full taxonomy of attack algorithms—such as gradient-based, optimization-based, and transfer-based techniques—is provided in a later chapter.

Adversarial attacks vary based on the attacker’s level of access to the model. In white-box attacks, the adversary has full knowledge of the model’s architecture, parameters, and training data, allowing them to craft highly effective adversarial examples. In black-box attacks, the adversary has no internal knowledge and must rely on querying the model and observing its outputs. Grey-box attacks fall between these extremes, with the adversary possessing partial information, such as access to the model architecture but not its parameters.

+------------------------------+----------------------------------------------+--------------------------+-------------------------------------------------+---------------------------------------------+
| Adversary Knowledge Level    | Model Access                                 | Training Data Access     | Attack Example                                  | Common Scenario                             |
+:=============================+:=============================================+:=========================+:================================================+:============================================+
| White-box                    | Full access to architecture and parameters   | Full access              | Crafting adversarial examples using gradients   | Insider threats, open-source model reuse    |
+------------------------------+----------------------------------------------+--------------------------+-------------------------------------------------+---------------------------------------------+
| Grey-box                     | Partial access (e.g., architecture only)     | Limited or no access     | Attacks based on surrogate model approximation  | Known model family, unknown fine-tuning     |
+------------------------------+----------------------------------------------+--------------------------+-------------------------------------------------+---------------------------------------------+
| Black-box                    | No internal access; only query-response view | No access                | Query-based surrogate model training and        | Public APIs, model-as-a-service deployments |
|                              |                                              |                          | transfer attacks                                |                                             |
+------------------------------+----------------------------------------------+--------------------------+-------------------------------------------------+---------------------------------------------+

: Adversary knowledge spectrum. {#tbl-adversary-knowledge-spectrum .striped .hover}

These attacker models can be summarized along a spectrum of knowledge levels. This comparison highlights the differences in model access, data access, typical attack strategies, and common deployment scenarios. Such distinctions help characterize the practical challenges of securing ML systems across different deployment environments.

A common attack strategy involves constructing a surrogate model that approximates the target model’s behavior. This surrogate model is trained by querying the target model with a set of inputs $\{x_i\}$ and recording the corresponding outputs $\{f(x_i)\}$. The attacker’s goal is to train a surrogate model $\hat{f}$ that minimizes the discrepancy between its predictions and those of the target model. This objective can be formulated as:

$$
\min_{\hat{f}} \ \sum_{i=1}^{n} \ \ell(\hat{f}(x_i), f(x_i))
$$

where $\ell$ is a loss function measuring the difference between the surrogate’s output and the target model’s output. By minimizing this loss, the attacker builds a model that behaves similarly to the target. Once trained, the surrogate model can be used to generate adversarial examples using white-box techniques. These examples often transfer to the original target model, even without internal access, making such attacks effective in black-box settings. This phenomenon, known as adversarial transferability, presents a significant challenge for defense.

Several methods have been proposed to generate adversarial examples. One notable approach leverages generative adversarial networks (GANs) [@goodfellow2020generative]. In this setting, a generator network learns to produce inputs that deceive the target model, while a discriminator evaluates their effectiveness. This iterative process allows the attacker to generate sophisticated and diverse adversarial examples.

Another vector for adversarial attacks involves transfer learning pipelines. Many production systems reuse pre-trained feature extractors, fine-tuning only the final layers for specific tasks. Adversaries can exploit this structure by targeting the shared feature extractor, crafting perturbations that affect multiple downstream tasks. Headless attacks, for example, manipulate the feature extractor without requiring access to the classification head or training data [@ahmed2020headless]. This exposes a critical vulnerability in systems that rely on pre-trained models.

One illustrative example involves the manipulation of traffic sign recognition systems [@eykholt2018robust]. Researchers demonstrated that placing small stickers on stop signs could cause machine learning models to misclassify them as speed limit signs. While the altered signs remained easily recognizable to humans, the model consistently misinterpreted them. Such attacks pose serious risks in applications like autonomous driving, where reliable perception is critical for safety.

Adversarial attacks highlight the need for robust defenses that go beyond improving model accuracy. Securing ML systems against adversarial threats requires runtime defenses such as input validation, anomaly detection, and monitoring for abnormal patterns during inference. Training-time robustness methods (e.g., adversarial training) complement these strategies and are discussed in more detail in a later chapter. These defenses aim to enhance model resilience against adversarial examples, ensuring that machine learning systems can operate reliably even in the presence of malicious inputs.

### Case Study: Traffic Sign Detection Model Trickery

In 2017, researchers conducted experiments by placing small black and white stickers on stop signs [@eykholt2018robust]. When viewed by a normal human eye, the stickers did not obscure the sign or prevent interpretability. However, when images of the stickers stop signs were fed into standard traffic sign classification ML models, they were misclassified as speed limit signs over 85% of the time.

This demonstration showed how simple adversarial stickers could trick ML systems into misreading critical road signs. If deployed realistically, these attacks could endanger public safety, causing autonomous vehicles to misinterpret stop signs as speed limits. Researchers warned this could potentially cause dangerous rolling stops or acceleration into intersections.

This case study provides a concrete illustration of how adversarial examples exploit the pattern recognition mechanisms of ML models. By subtly altering the input data, attackers can induce incorrect predictions and pose significant risks to safety-critical applications like self-driving cars. The attack's simplicity demonstrates how even minor, imperceptible changes can lead models astray. Consequently, developers must implement robust defenses against such threats.

These threat types span different stages of the ML lifecycle and demand distinct defensive strategies. The table below summarizes their key characteristics.

+------------------------+---------------------+----------------------------+----------------------------------------------+
| Threat Type            | Lifecycle Stage     | Attack Vector              | Example Impact                               |
+:=======================+:====================+:===========================+:=============================================+
| Model Theft            | Deployment          | API access, insider leaks  | Stolen IP, model inversion, behavioral clone |
+------------------------+---------------------+----------------------------+----------------------------------------------+
| Data Poisoning         | Training            | Label flipping, backdoors  | Targeted misclassification, degraded accuracy|
+------------------------+---------------------+----------------------------+----------------------------------------------+
| Adversarial Attacks    | Inference           | Input perturbation         | Real-time misclassification, safety failure  |
+------------------------+---------------------+----------------------------+----------------------------------------------+

: Summary of threat types to ML models by lifecycle stage and attack vector. {#tbl-threats-models-summary .striped .hover}

While ML models themselves present critical attack surfaces, they ultimately run on hardware that can introduce vulnerabilities beyond the model’s control. In the next section, we examine how adversaries can target the physical infrastructure that executes machine learning workloads—through hardware bugs, physical tampering, side channels, and supply chain risks.

## Threats to ML Hardware

As machine learning systems move from research prototypes to large-scale, real-world deployments, their security increasingly depends on the **hardware platforms** they run on. Whether deployed in data centers, on edge devices, or in embedded systems, machine learning applications rely on a layered stack of processors, accelerators, memory, and communication interfaces. These hardware components, while essential for enabling efficient computation, introduce **unique security risks** that go beyond traditional software-based vulnerabilities.

Unlike general-purpose software systems, machine learning workflows often process **high-value models** and **sensitive data** in **performance-constrained environments**. This makes them attractive targets not only for software attacks but also for **hardware-level exploitation**. Vulnerabilities in hardware can expose models to theft, leak user data, disrupt system reliability, or allow adversaries to manipulate inference results. Because hardware operates below the software stack, such attacks can bypass conventional security mechanisms and remain difficult to detect.

These hardware threats arise from multiple sources, including **design flaws in hardware architectures**, **physical tampering**, **side-channel leakage**, and **supply chain compromises**. Together, they form a **critical attack surface** that must be addressed to build trustworthy machine learning systems.

Table @tbl-threat_types summarizes the major categories of hardware security threats, describing their origins, methods, and implications for machine learning system design and deployment.

+-------------------------+--------------------------------------------------------------------------------------------------+------------------------------------------------+
| Threat Type             | Description                                                                                      | Relevance to ML Hardware Security              |
+:========================+:=================================================================================================+:===============================================+
| Hardware Bugs           | Intrinsic flaws in hardware designs that can compromise system integrity.                        | Foundation of hardware vulnerability.          |
+-------------------------+--------------------------------------------------------------------------------------------------+------------------------------------------------+
| Physical Attacks        | Direct exploitation of hardware through physical access or manipulation.                         | Basic and overt threat model.                  |
+-------------------------+--------------------------------------------------------------------------------------------------+------------------------------------------------+
| Fault-injection Attacks | Induction of faults to cause errors in hardware operation, leading to potential system crashes.  | Systematic manipulation leading to failure.    |
+-------------------------+--------------------------------------------------------------------------------------------------+------------------------------------------------+
| Side-Channel Attacks    | Exploitation of leaked information from hardware operation to extract sensitive data.            | Indirect attack via environmental observation. |
+-------------------------+--------------------------------------------------------------------------------------------------+------------------------------------------------+
| Leaky Interfaces        | Vulnerabilities arising from interfaces that expose data unintentionally.                        | Data exposure through communication channels.  |
+-------------------------+--------------------------------------------------------------------------------------------------+------------------------------------------------+
| Counterfeit Hardware    | Use of unauthorized hardware components that may have security flaws.                            | Compounded vulnerability issues.               |
+-------------------------+--------------------------------------------------------------------------------------------------+------------------------------------------------+
| Supply Chain Risks      | Risks introduced through the hardware lifecycle, from production to deployment.                  | Cumulative & multifaceted security challenges. |
+-------------------------+--------------------------------------------------------------------------------------------------+------------------------------------------------+

: Threat types on hardware security. {#tbl-threat_types .striped .hover}

### Hardware Bugs

Hardware is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit these vulnerabilities to access, manipulate, or extract sensitive data, breaching the confidentiality and integrity that users and services depend on. One of the most notable examples came with the discovery of [Meltdown and Spectre](https://meltdownattack.com/)—two vulnerabilities in modern processors that allow malicious programs to bypass memory isolation and read the data of other applications and the operating system [@Lipp2018meltdown; @Kocher2018spectre].

These attacks exploit **speculative execution**, a performance optimization in CPUs that executes instructions out of order before safety checks are complete. While improving computational speed, this optimization inadvertently exposes sensitive data through microarchitectural side channels, such as CPU caches. The technical sophistication of these attacks highlights the difficulty of eliminating vulnerabilities even with extensive hardware validation.

Further research has revealed that these were not isolated incidents. Variants such as Foreshadow[^fn-foreshadow], ZombieLoad[^fn-zombieload], and RIDL[^fn-ridl] target different microarchitectural elements—from secure enclaves to CPU internal buffers—demonstrating that speculative execution flaws are a **systemic hardware risk**.

[^fn-foreshadow]: Foreshadow: A speculative execution vulnerability that targets Intel's SGX enclaves, allowing data leaks from supposedly secure memory regions.

[^fn-zombieload]: ZombieLoad: A side-channel attack that exploits Intel's CPU microarchitectural buffers to leak sensitive data processed by other applications.

[^fn-ridl]: RIDL (Rogue In-Flight Data Load): A speculative execution attack that leaks in-flight data from CPU internal buffers, bypassing memory isolation boundaries.

While these attacks were first demonstrated on general-purpose CPUs, their implications extend to **machine learning accelerators and specialized hardware**. ML systems often rely on heterogeneous compute platforms that combine CPUs with GPUs, TPUs, FPGAs, or custom accelerators. These components process sensitive data such as personal information, medical records, or proprietary models. Vulnerabilities in any part of this stack could expose such data to attackers.

For example, an edge device like a smart camera running a face recognition model on an accelerator could be vulnerable if the hardware lacks proper cache isolation. An attacker might exploit this weakness to extract intermediate computations, model parameters, or user data. Similar risks exist in **cloud inference services**, where hardware multi-tenancy increases the chances of cross-tenant data leakage.

Such vulnerabilities are particularly concerning in privacy-sensitive domains like healthcare, where ML systems routinely handle patient data. A breach could violate privacy regulations such as the [Health Insurance Portability and Accountability Act (HIPAA)](https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html), leading to significant legal and ethical consequences.

These examples illustrate that hardware security is not solely about preventing physical tampering. It also requires **architectural safeguards** to prevent data leakage through the hardware itself. As new vulnerabilities continue to emerge across processors, accelerators, and memory systems, addressing these risks requires continuous mitigation efforts—often involving **performance trade-offs**, especially in compute- and memory-intensive ML workloads. Proactive solutions, such as **confidential computing** and **trusted execution environments (TEEs)**, offer promising architectural defenses. However, achieving robust hardware security requires attention at **every stage of the system lifecycle**, from design to deployment.

### Physical Attacks

Physical tampering refers to the direct, unauthorized manipulation of computing hardware to undermine the integrity of machine learning systems. This type of attack is particularly concerning because it bypasses traditional software security defenses, directly targeting the physical components on which machine learning depends. ML systems are especially vulnerable to such attacks because they rely on hardware sensors, accelerators, and storage to process large volumes of data and produce reliable outcomes in real-world environments.

While software security measures—such as encryption, authentication, and access control—protect ML systems against remote attacks, they offer little defense against adversaries with **physical access** to devices. Physical tampering can range from simple actions, like inserting a malicious USB device into an edge server, to highly sophisticated manipulations such as embedding **hardware trojans** during chip manufacturing. These threats are particularly relevant for machine learning systems deployed at the edge or in physically exposed environments, where attackers may have opportunities to interfere with the hardware directly.

To understand how such attacks affect ML systems in practice, consider the example of an ML-powered drone used for environmental mapping or infrastructure inspection. The drone’s navigation depends on machine learning models that process data from GPS, cameras, and inertial measurement units. If an attacker gains physical access to the drone, they could replace or modify its navigation module, embedding a hidden backdoor that alters flight behavior or reroutes data collection. Such manipulation not only compromises the system’s reliability but also opens the door to misuse, such as surveillance or smuggling operations.

Physical attacks are not limited to mobility systems. Biometric access control systems, which rely on ML models to process face or fingerprint data, are also vulnerable. These systems typically use embedded hardware to capture and process biometric inputs. An attacker could physically replace a biometric sensor with a modified component designed to capture and transmit personal identification data to an unauthorized receiver. This compromises both security and user privacy, and it can enable future impersonation attacks.

In addition to tampering with external sensors, attackers may target **internal hardware subsystems**. For example, the sensors used in autonomous vehicles—such as cameras, LiDAR, and radar—are essential for ML models that interpret the surrounding environment. A malicious actor could physically misalign or obstruct these sensors, degrading the model’s perception capabilities and creating safety hazards.

Hardware trojans pose another serious risk. Malicious modifications introduced during chip fabrication or assembly can embed dormant circuits in ML accelerators or inference chips. These trojans may remain inactive under normal conditions but trigger malicious behavior when specific inputs are processed or system states are reached. Such hidden vulnerabilities can disrupt computations, leak model outputs, or degrade system performance in ways that are extremely difficult to diagnose post-deployment.

Memory subsystems are also attractive targets. Attackers with physical access to edge devices or embedded ML accelerators could manipulate memory chips to extract encrypted model parameters or training data. Fault injection techniques—such as voltage manipulation or electromagnetic interference—can further degrade system reliability by corrupting model weights or forcing incorrect computations during inference.

Physical access threats extend to data center and cloud environments as well. Attackers with sufficient access could install hardware implants, such as keyloggers or data interceptors, to capture administrative credentials or monitor data streams. Such implants can provide persistent backdoor access, enabling long-term surveillance or data exfiltration from ML training and inference pipelines.

In summary, physical attacks on machine learning systems threaten both **security** and **reliability** across a wide range of deployment environments. Addressing these risks requires a combination of **hardware-level protections**, **tamper detection mechanisms**, and **supply chain integrity checks**. Without these safeguards, even the most secure software defenses may be undermined by vulnerabilities introduced through direct physical manipulation.

### Fault Injection Attacks

Fault injection is a powerful class of physical attacks that **deliberately disrupts hardware operations** to induce errors in computation. These induced faults can compromise the **integrity** of machine learning models by causing them to produce incorrect outputs, degrade reliability, or leak sensitive information. For ML systems, such faults not only disrupt inference but also expose models to deeper exploitation, including **reverse engineering** and **bypass of security protocols** \[@joye2012fault].

Attackers achieve fault injection by applying **precisely timed physical or electrical disturbances** to the hardware while it is executing computations. Techniques such as low-voltage manipulation \[@barenghi2010low], power spikes \[@hutter2009contact], clock glitches \[@amiel2006fault], electromagnetic pulses \[@agrawal2003side], temperature variations \[@skorobogatov2009local], and even laser strikes \[@skorobogatov2003optical] have been demonstrated to corrupt specific parts of a program's execution. These disturbances can cause effects such as **bit flips**, **skipped instructions**, or **corrupted memory states**, which adversaries can exploit to alter ML model behavior or extract sensitive information.

For machine learning systems, these attacks pose several concrete risks. Fault injection can degrade **model accuracy**, force **incorrect classifications**, trigger **denial of service**, or even **leak internal model parameters**. For example, attackers could inject faults into an embedded ML model running on a microcontroller, forcing it to misclassify inputs in safety-critical applications such as **autonomous navigation** or **medical diagnostics**. More sophisticated attackers may target memory or control logic to steal **intellectual property**, such as proprietary model weights or architecture details.

Experimental demonstrations have shown the feasibility of such attacks. One notable example is the work by @breier2018deeplaser, where researchers successfully used a **laser fault injection** attack on a deep neural network deployed on a microcontroller. By heating specific transistors, they forced the hardware to skip execution steps, including a ReLU activation function. This manipulation caused the neurons in the affected layer to produce a constant output of zero, effectively disabling part of the network and leading to misclassifications. The assembly code in @fig-injection illustrates how the injected fault caused the program to skip a conditional jump, forcing incorrect behavior and demonstrating the fragility of ML execution on compromised hardware.

![Fault-injection demonstrated with assembly code. Source: @breier2018deeplaser.](images/png/Fault-injection_demonstrated_with_assembly_code.png){#fig-injection}

Fault injection attacks can also be combined with **side-channel analysis**, where attackers first observe power or timing characteristics to infer model structure or data flow. This reconnaissance allows them to target **specific layers or operations**, such as activation functions or final decision layers, maximizing the impact of the injected faults.

Embedded and edge ML systems are particularly vulnerable because they often lack **physical hardening** and operate under **resource constraints** that limit runtime defenses. Without tamper-resistant packaging or secure hardware enclaves, attackers may gain **direct access to system buses and memory**, enabling precise fault manipulation. Furthermore, many embedded ML models are designed to be lightweight, leaving them with little **redundancy or error correction** to recover from induced faults.

Mitigating fault injection requires a **multi-layered defense strategy**. Physical protections, such as **tamper-proof enclosures** and **design obfuscation**, help limit physical access. **Anomaly detection techniques** can monitor sensor inputs or model outputs for signs of fault-induced inconsistencies \[@hsiao2023mavfi]. **Error-correcting memories** and **secure firmware** can reduce the likelihood of silent corruption. Techniques such as **model watermarking** may provide traceability if stolen models are later deployed by an adversary.

However, these protections are difficult to implement in **cost- and power-constrained environments**, where adding cryptographic hardware or redundancy may not be feasible. As a result, achieving resilience to fault injection requires **cross-layer design considerations** that span electrical, firmware, software, and system architecture levels. Without such holistic design practices, ML systems deployed in the field may remain exposed to these low-cost yet highly effective physical attacks.

### Side-Channel Attacks

Side-channel attacks constitute a class of security breaches that exploit information inadvertently revealed through the physical implementation of computing systems. In contrast to direct attacks that target software or network vulnerabilities, these attacks leverage the system's hardware characteristics—such as power consumption, electromagnetic emissions, or timing behavior—to extract sensitive information.

The fundamental premise of a side-channel attack is that a device’s operation can leak information through observable physical signals. Such leaks may originate from the electrical power the device consumes \[@kocher1999differential], the electromagnetic fields it emits \[@gandolfi2001electromagnetic], the time it takes to complete computations, or even the acoustic noise it produces. By carefully measuring and analyzing these signals, attackers can infer internal system states or recover secret data.

Although these techniques are commonly discussed in cryptography, they are equally relevant to machine learning systems. ML models deployed on hardware accelerators, embedded devices, or edge systems often process sensitive data. Even when these models are protected by secure algorithms or encryption, their physical execution may leak side-channel signals that can be exploited by adversaries.

One of the most widely studied examples involves **Advanced Encryption Standard (AES)** implementations. While AES is mathematically secure, the physical process of computing its encryption functions leaks measurable signals. Techniques such as **Differential Power Analysis (DPA)** \[@Kocher2011Intro], **Differential Electromagnetic Analysis (DEMA)**, and **Correlation Power Analysis (CPA)** exploit these physical signals to recover secret keys.

A useful example of this attack technique can be seen in a power analysis of a password authentication process. Consider a device that verifies a **5-byte password**—in this case, `0x61, 0x52, 0x77, 0x6A, 0x73`. During authentication, the device receives each byte sequentially over a serial interface, and its **power consumption pattern** reveals how the system responds as it processes these inputs.

@fig-encryption shows the device’s behavior when the correct password is entered. The **red waveform** captures the serial data stream, marking each byte as it is received. The **blue curve** records the device’s power consumption over time. When the full, correct password is supplied, the power profile remains stable and consistent across all five bytes, providing a clear baseline for comparison with failed attempts.

![Power consumption profile of the device during normal operations with a valid 5-byte password (0x61, 0x52, 0x77, 0x6A, 0x73). The red line represents the serial data being received by the bootloader, which in this figure is receiving the correct bytes. Notice how the blue line, representing power usage during authentication, corresponds to receiving and verifying the bytes. In the next figures, this blue power consumption profile will change. Source: Colin O'Flynn.](images/png/Power_analysis_of_an_encryption_device_with_a_correct_password.png){#fig-encryption}

When an **incorrect password** is entered, the power analysis chart changes as shown in @fig-encryption2. In this case, the first three bytes (`0x61, 0x52, 0x77`) are correct, so the power patterns closely match the correct password up to that point. However, when the fourth byte (`0x42`) is processed and found to be incorrect, the device halts authentication. This change is reflected in the **sudden jump** in the blue power line, indicating that the device has stopped processing and entered an error state.

![Power consumption profile of the device when an incorrect 5-byte password (0x61, 0x52, 0x77, 0x42, 0x42) is entered. The red line represents the serial data received by the bootloader, showing the input bytes being processed. The first three bytes (0x61, 0x52, 0x77) are correct and match the expected password, as indicated by the consistent blue power consumption line. However, upon processing the fourth byte (0x42), a mismatch is detected. The bootloader stops further processing, resulting in a noticeable jump in the blue power consumption line, as the device halts authentication and enters an error state. Source: Colin O'Flynn.](images/png/Power_analysis_of_an_encryption_device_with_a_\(partially\)_wrong_password.png){#fig-encryption2}

@fig-encryption3 shows the case where the **password is entirely incorrect** (`0x30, 0x30, 0x30, 0x30, 0x30`). Here, the device detects the mismatch immediately after the first byte and halts processing much earlier. This is again visible in the power profile, where the blue line exhibits a **sharp jump** following the first byte, reflecting the device’s early termination of authentication.

![Power consumption profile of the device when an entirely incorrect password (0x30, 0x30, 0x30, 0x30, 0x30) is entered. The blue line shows a sharp jump after processing the first byte, indicating that the device has halted the authentication process. Source: Colin O'Flynn.](images/png/Power_analysis_of_an_encryption_device_with_a_wrong_password.png){#fig-encryption3}

These examples demonstrate how attackers can exploit **observable power consumption differences** to reduce the search space and eventually recover secret data through brute-force analysis. For a more detailed walkthrough, @vid-powerattack provides a step-by-step demonstration of how these attacks are performed.

:::{#vid-powerattack .callout-important title="Power Attack"}

{{< video [https://www.youtube.com/watch?v=2iDLfuEBcs8](https://www.youtube.com/watch?v=2iDLfuEBcs8) >}}

:::

Such attacks are not limited to cryptographic systems. Machine learning applications face similar risks. For example, an ML-based **speech recognition system** processing voice commands on a local device could leak **timing or power signals** that reveal which commands are being processed. Even subtle acoustic or electromagnetic emissions may expose operational patterns that an adversary could exploit to infer user behavior.

Historically, side-channel attacks have been used to bypass even the most secure cryptographic systems. In the 1960s, British intelligence agency MI5 famously exploited **acoustic emissions** from a cipher machine in the Egyptian Embassy \[@Burnet1989Spycatcher]. By capturing the mechanical clicks of the machine’s rotors, MI5 analysts were able to dramatically reduce the complexity of breaking encrypted messages. This early example illustrates that **side-channel vulnerabilities** are not confined to the digital age but are rooted in the **physical nature of computation**.

Today, these techniques have advanced to include attacks such as **keyboard eavesdropping** \[@Asonov2004Keyboard], **power analysis on cryptographic hardware** \[@gnad2017voltage], and **voltage-based attacks on ML accelerators** \[@zhao2018fpga]. Timing attacks, electromagnetic leakage, and thermal emissions continue to provide adversaries with indirect channels for observing system behavior.

Machine learning systems deployed on specialized accelerators or embedded platforms are especially at risk. Attackers may exploit side-channel signals to infer **model structure**, **steal parameters**, or **reconstruct private training data**. As ML becomes increasingly deployed in **cloud, edge, and embedded environments**, these side-channel vulnerabilities pose significant challenges to system security.

Understanding the persistence and evolution of side-channel attacks is essential for building **resilient machine learning systems**. By recognizing that **where there is a signal, there is potential for exploitation**, system designers can begin to address these risks through a combination of **hardware shielding**, **algorithmic defenses**, and **operational safeguards**.

### Leaky Interfaces

Interfaces in computing systems are essential for enabling communication, diagnostics, and updates. However, these same interfaces can become significant security vulnerabilities when they unintentionally expose sensitive information or accept unverified inputs. Such **leaky interfaces** often go unnoticed during system design, yet they provide attackers with powerful entry points to extract data, manipulate functionality, or introduce malicious code.

A leaky interface is any access point that reveals more information than intended, often because of weak authentication, lack of encryption, or inadequate isolation. These issues have been widely demonstrated across consumer, medical, and industrial systems.

For example, many WiFi-enabled baby monitors have been found to expose unsecured remote access ports, allowing attackers to intercept live audio and video feeds from inside private homes[^fn-baby-monitor]. Similarly, researchers have identified wireless vulnerabilities in pacemakers that could allow attackers to manipulate cardiac functions if exploited, raising life-threatening safety concerns[^fn-pacemaker].

[^fn-baby-monitor]: See [this report](https://www.fox19.com/story/25310628/hacked-baby-monitor/) on baby monitor vulnerabilities that allowed remote attackers to eavesdrop on live feeds in private homes.

[^fn-pacemaker]: Vulnerabilities in connected pacemakers raised concerns about remote manipulation of cardiac functions, as described in [this medical advisory](https://www.moph.gov.lb/userfiles/files/Medical%20Devices/Medical%20Devices%20Recalls%202021/17-05-2021/AssurityandEndurity.pdf).

A notable case involving smart lightbulbs demonstrated that accessible debug ports left on production devices leaked unencrypted WiFi credentials. This security oversight provided attackers with a pathway to infiltrate home networks without needing to bypass standard security mechanisms.[^fn-lightbulb] In the automotive domain, unsecured OBD-II diagnostic ports have allowed attackers to manipulate braking and steering functions in connected vehicles, as demonstrated in the well-known Jeep Cherokee hack.[^fn-jeep]

[^fn-lightbulb]: Debug ports on consumer smart lightbulbs leaked unencrypted network credentials, as documented by [Greengard (2021)](https://doi.org/10.1145/3451349).

[^fn-jeep]: The Jeep Cherokee hack demonstrated how attackers could control vehicle functions through the OBD-II port. See [Miller and Valasek (2015)](https://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/).

While these examples do not target machine learning systems directly, they illustrate architectural patterns that are highly relevant to ML-enabled devices. Consider a smart home security system that uses machine learning to detect user routines and automate responses. Such a system may include a maintenance or debug interface for software updates. If this interface lacks proper authentication or transmits data unencrypted, attackers on the same network could gain unauthorized access. This intrusion could expose user behavior patterns, compromise model integrity, or disable security features altogether.

Leaky interfaces in ML systems can also expose training data, model parameters, or intermediate outputs. Such exposure can enable attackers to craft adversarial examples, steal proprietary models, or reverse-engineer system behavior. Worse still, these interfaces may allow attackers to tamper with firmware, introducing malicious code that disables devices or recruits them into botnets.

Mitigating these risks requires **multi-layered defenses**. Technical safeguards such as strong authentication, encrypted communications, and runtime anomaly detection are essential. Organizational practices such as interface inventories, access control policies, and ongoing audits are equally important. Adopting a **zero-trust architecture**, where no interface is trusted by default, further reduces exposure by limiting access to only what is strictly necessary.

For designers of ML-powered systems, securing interfaces must be a first-class concern alongside algorithmic and data-centric design. Whether the system operates in the cloud, on the edge, or in embedded environments, failure to secure these access points risks undermining the entire system's trustworthiness.

### Counterfeit Hardware

Machine learning systems depend on the reliability and security of the hardware on which they run. Yet, in today’s globalized hardware ecosystem, the risk of counterfeit or cloned hardware has emerged as a serious threat to system integrity. Counterfeit components refer to unauthorized reproductions of genuine parts, designed to closely imitate their appearance and functionality. These components can enter machine learning systems through complex procurement and manufacturing processes that span multiple vendors and regions.

A single lapse in component sourcing can introduce counterfeit hardware into critical systems. For example, a facial recognition system deployed for secure facility access might unknowingly rely on counterfeit processors. These unauthorized components could fail to process biometric data correctly or introduce hidden vulnerabilities that allow attackers to bypass authentication controls.

The risks posed by counterfeit hardware are multifaceted. From a reliability perspective, such components often degrade faster, perform unpredictably, or fail under load due to substandard manufacturing. From a security perspective, counterfeit hardware may include hidden backdoors or malicious circuitry, providing attackers with undetectable pathways to compromise machine learning systems. A cloned network router installed in a data center, for instance, could silently intercept model predictions or user data, undermining both system security and user privacy.

Legal and regulatory risks further compound the problem. Organizations that unknowingly integrate counterfeit components into their ML systems may face serious legal consequences, including penalties for violating safety, privacy, or cybersecurity regulations. This is particularly concerning in sectors such as healthcare and finance, where compliance with industry standards is non-negotiable.

Economic pressures often incentivize sourcing from lower-cost suppliers without rigorous verification, increasing the likelihood of counterfeit parts entering production systems. Detection is especially challenging, as counterfeit components are designed to mimic legitimate ones. Identifying them may require specialized equipment or forensic analysis, making prevention far more practical than remediation.

The stakes are particularly high in machine learning applications that require high reliability and low latency, such as real-time decision-making in autonomous vehicles, industrial automation, or critical healthcare diagnostics. Hardware failure in these contexts can lead not only to system downtime but also to significant safety risks.

As machine learning continues to expand into safety-critical and high-value applications, counterfeit hardware presents a growing risk that must be recognized and addressed. Organizations must treat hardware trustworthiness as a fundamental design requirement, on par with algorithmic accuracy and data security, to ensure that ML systems can operate reliably and securely in the real world.

### Supply Chain Risks

While counterfeit hardware presents a serious challenge, it is only one part of the larger problem of securing the **global hardware supply chain**. Machine learning systems are built from components that pass through complex supply networks involving design, fabrication, assembly, distribution, and integration. Each of these stages presents opportunities for tampering, substitution, or counterfeiting—often without the knowledge of those deploying the final system.

Malicious actors can exploit these vulnerabilities in various ways. A contracted manufacturer might unknowingly receive recycled electronic waste that has been relabeled as new components. A distributor might deliberately mix cloned parts into otherwise legitimate shipments. Insiders at manufacturing facilities might embed hardware Trojans that are nearly impossible to detect once the system is deployed. Advanced counterfeits can be particularly deceptive, with refurbished or repackaged components designed to pass visual inspection while concealing inferior or malicious internals.

Identifying such compromises typically requires sophisticated analysis, including micrography, X-ray screening, and functional testing. However, these methods are costly and impractical for large-scale procurement. As a result, many organizations deploy systems without fully verifying the authenticity and security of every component.

The risks extend beyond individual devices. Machine learning systems often rely on **heterogeneous hardware platforms**, integrating CPUs, GPUs, memory, and specialized accelerators sourced from a global supply base. Any compromise in one part of this chain can undermine the security of the entire system. These risks are further amplified when systems operate in **shared or multi-tenant environments**, such as cloud data centers or federated edge networks, where hardware-level isolation is critical to preventing cross-tenant attacks.

The 2018 Bloomberg Businessweek report alleging that Chinese state actors inserted spy chips into Supermicro server motherboards brought these risks to mainstream attention. While the claims remain disputed, the story underscored the industry’s limited visibility into its own hardware supply chains. Companies often rely on complex, opaque manufacturing and distribution networks, leaving them vulnerable to hidden compromises. Over-reliance on single manufacturers or regions—such as the semiconductor industry’s dependence on TSMC—further concentrates this risk. This recognition has driven policy responses like the U.S. [CHIPS and Science Act](https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2024/08/09/fact-sheet-two-years-after-the-chips-and-science-act-biden-%E2%81%A0harris-administration-celebrates-historic-achievements-in-bringing-semiconductor-supply-chains-home-creating-jobs-supporting-inn/), which aims to bring semiconductor production onshore and strengthen supply chain resilience.

Securing machine learning systems requires moving beyond **trust-by-default** models toward **zero-trust supply chain practices**. This includes screening suppliers, validating component provenance, implementing tamper-evident protections, and continuously monitoring system behavior for signs of compromise. Building fault-tolerant architectures that detect and contain failures provides an additional layer of defense.

Ultimately, supply chain risks must be treated as a **first-class concern in ML system design**. Trust in the computational models and data pipelines that power machine learning depends fundamentally on the trustworthiness of the hardware on which they run. Without securing the hardware foundation, even the most sophisticated models remain vulnerable to compromise.

### Case Study: The Supermicro Hardware Security Controversy

In 2018, Bloomberg Businessweek published a widely discussed report alleging that Chinese state-sponsored actors had secretly implanted tiny surveillance chips on server motherboards manufactured by Supermicro. These compromised servers were reportedly deployed by more than 30 major companies, including Apple and Amazon. The chips, described as no larger than a grain of rice, were said to provide attackers with backdoor access to sensitive data and systems.

The allegations sparked immediate concern across the technology industry, raising questions about the security of global supply chains and the potential for state-level hardware manipulation. However, the companies named in the report publicly denied the claims. Apple, Amazon, and Supermicro stated that they had found no evidence of the alleged implants after conducting thorough internal investigations. Industry experts and government agencies also expressed skepticism, noting the lack of verifiable technical evidence presented in the report.

Despite these denials, the story had a lasting impact on how organizations and policymakers view hardware supply chain security. Whether or not the specific claims were accurate, the report highlighted the **real and growing concern** that hardware supply chains are difficult to fully audit and secure. It underscored how geopolitical tensions, manufacturing outsourcing, and the complexity of modern hardware ecosystems make it increasingly challenging to guarantee the integrity of hardware components.

The Supermicro case illustrates a broader truth: once a product enters a complex global supply chain, it becomes difficult to ensure that every component is free from tampering or unauthorized modification. This risk is particularly acute for machine learning systems, which depend on a wide range of hardware accelerators, memory modules, and processing units sourced from multiple vendors across the globe.

In response to these risks, both industry and government stakeholders have begun to invest in supply chain security initiatives. The U.S. government’s CHIPS and Science Act is one such effort, aiming to bring semiconductor manufacturing back onshore to improve transparency and reduce dependency on foreign suppliers. While these efforts are valuable, they do not fully eliminate supply chain risks. They must be complemented by **technical safeguards**, such as component validation, runtime monitoring, and fault-tolerant system design.

The Supermicro controversy serves as a cautionary tale for the machine learning community. It demonstrates that **hardware security cannot be taken for granted**, even when working with reputable suppliers. Ensuring the integrity of ML systems requires rigorous attention to the entire hardware lifecycle—from design and fabrication to deployment and maintenance. This case reinforces the need for organizations to adopt **comprehensive supply chain security practices** as a foundational element of trustworthy ML system design.

## Defensive Strategies

Designing secure and privacy-preserving machine learning systems requires more than identifying threats—it demands proactive, layered defenses tailored to the system's architecture, threat surface, and deployment context. Defensive strategies must account for the distinct challenges posed by model behavior, data pipelines, hardware vulnerabilities, and system integration. In contrast to traditional software systems, ML systems introduce unique risks, such as susceptibility to adversarial examples, sensitivity to data corruption, and increased complexity due to model-data coupling.

This section outlines a range of defensive strategies that can be employed to enhance the security and privacy of machine learning systems. These strategies span multiple layers, from hardware and software to data management and operational practices. By adopting a holistic approach, organizations can build ML systems capable of withstanding a variety of threats while maintaining user trust and compliance with privacy regulations.

