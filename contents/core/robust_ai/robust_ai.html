<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/privacy_security/privacy_security.html" rel="next">
<link href="../../../contents/core/ondevice_learning/ondevice_learning.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-cbc843cc95873402613d6df7a37f2654.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
</style>
<style>
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/ops/ops.html">Robust Deployment</a></li><li class="breadcrumb-item"><a href="../../../contents/core/robust_ai/robust_ai.html">Robust AI</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#robust-ai" id="toc-robust-ai" class="nav-link active" data-scroll-target="#robust-ai">Robust AI</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-robust-ai-overview-6451" id="toc-sec-robust-ai-overview-6451" class="nav-link" data-scroll-target="#sec-robust-ai-overview-6451">Overview</a></li>
  <li><a href="#sec-robust-ai-realworld-applications-d887" id="toc-sec-robust-ai-realworld-applications-d887" class="nav-link" data-scroll-target="#sec-robust-ai-realworld-applications-d887">Real-World Applications</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-cloud-9eaf" id="toc-sec-robust-ai-cloud-9eaf" class="nav-link" data-scroll-target="#sec-robust-ai-cloud-9eaf">Cloud</a></li>
  <li><a href="#sec-robust-ai-edge-8dde" id="toc-sec-robust-ai-edge-8dde" class="nav-link" data-scroll-target="#sec-robust-ai-edge-8dde">Edge</a></li>
  <li><a href="#sec-robust-ai-embedded-91cc" id="toc-sec-robust-ai-embedded-91cc" class="nav-link" data-scroll-target="#sec-robust-ai-embedded-91cc">Embedded</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-hardware-faults-81ee" id="toc-sec-robust-ai-hardware-faults-81ee" class="nav-link" data-scroll-target="#sec-robust-ai-hardware-faults-81ee">Hardware Faults</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-transient-faults-2227" id="toc-sec-robust-ai-transient-faults-2227" class="nav-link" data-scroll-target="#sec-robust-ai-transient-faults-2227">Transient Faults</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-characteristics-5133" id="toc-sec-robust-ai-characteristics-5133" class="nav-link" data-scroll-target="#sec-robust-ai-characteristics-5133">Characteristics</a></li>
  <li><a href="#sec-robust-ai-causes-b285" id="toc-sec-robust-ai-causes-b285" class="nav-link" data-scroll-target="#sec-robust-ai-causes-b285">Causes</a></li>
  <li><a href="#sec-robust-ai-mechanisms-3fd9" id="toc-sec-robust-ai-mechanisms-3fd9" class="nav-link" data-scroll-target="#sec-robust-ai-mechanisms-3fd9">Mechanisms</a></li>
  <li><a href="#sec-robust-ai-impact-ml-a44d" id="toc-sec-robust-ai-impact-ml-a44d" class="nav-link" data-scroll-target="#sec-robust-ai-impact-ml-a44d">Impact on ML</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-permanent-faults-f290" id="toc-sec-robust-ai-permanent-faults-f290" class="nav-link" data-scroll-target="#sec-robust-ai-permanent-faults-f290">Permanent Faults</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-characteristics-6f8d" id="toc-sec-robust-ai-characteristics-6f8d" class="nav-link" data-scroll-target="#sec-robust-ai-characteristics-6f8d">Characteristics</a></li>
  <li><a href="#sec-robust-ai-causes-6e76" id="toc-sec-robust-ai-causes-6e76" class="nav-link" data-scroll-target="#sec-robust-ai-causes-6e76">Causes</a></li>
  <li><a href="#sec-robust-ai-mechanisms-6e17" id="toc-sec-robust-ai-mechanisms-6e17" class="nav-link" data-scroll-target="#sec-robust-ai-mechanisms-6e17">Mechanisms</a></li>
  <li><a href="#sec-robust-ai-impact-ml-7efd" id="toc-sec-robust-ai-impact-ml-7efd" class="nav-link" data-scroll-target="#sec-robust-ai-impact-ml-7efd">Impact on ML</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-intermittent-faults-7778" id="toc-sec-robust-ai-intermittent-faults-7778" class="nav-link" data-scroll-target="#sec-robust-ai-intermittent-faults-7778">Intermittent Faults</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-characteristics-6317" id="toc-sec-robust-ai-characteristics-6317" class="nav-link" data-scroll-target="#sec-robust-ai-characteristics-6317">Characteristics</a></li>
  <li><a href="#sec-robust-ai-causes-49f1" id="toc-sec-robust-ai-causes-49f1" class="nav-link" data-scroll-target="#sec-robust-ai-causes-49f1">Causes</a></li>
  <li><a href="#sec-robust-ai-mechanisms-0ca2" id="toc-sec-robust-ai-mechanisms-0ca2" class="nav-link" data-scroll-target="#sec-robust-ai-mechanisms-0ca2">Mechanisms</a></li>
  <li><a href="#sec-robust-ai-impact-ml-db72" id="toc-sec-robust-ai-impact-ml-db72" class="nav-link" data-scroll-target="#sec-robust-ai-impact-ml-db72">Impact on ML</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-detection-mitigation-10f7" id="toc-sec-robust-ai-detection-mitigation-10f7" class="nav-link" data-scroll-target="#sec-robust-ai-detection-mitigation-10f7">Detection and Mitigation</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-detection-techniques-870b" id="toc-sec-robust-ai-detection-techniques-870b" class="nav-link" data-scroll-target="#sec-robust-ai-detection-techniques-870b">Detection Techniques</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-summary-18f0" id="toc-sec-robust-ai-summary-18f0" class="nav-link" data-scroll-target="#sec-robust-ai-summary-18f0">Summary</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-model-robustness-f537" id="toc-sec-robust-ai-model-robustness-f537" class="nav-link" data-scroll-target="#sec-robust-ai-model-robustness-f537">Model Robustness</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-adversarial-attacks-f700" id="toc-sec-robust-ai-adversarial-attacks-f700" class="nav-link" data-scroll-target="#sec-robust-ai-adversarial-attacks-f700">Adversarial Attacks</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-mechanisms-77b4" id="toc-sec-robust-ai-mechanisms-77b4" class="nav-link" data-scroll-target="#sec-robust-ai-mechanisms-77b4">Mechanisms</a></li>
  <li><a href="#sec-robust-ai-impact-ml-d199" id="toc-sec-robust-ai-impact-ml-d199" class="nav-link" data-scroll-target="#sec-robust-ai-impact-ml-d199">Impact on ML</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-data-poisoning-2769" id="toc-sec-robust-ai-data-poisoning-2769" class="nav-link" data-scroll-target="#sec-robust-ai-data-poisoning-2769">Data Poisoning</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-characteristics-3c33" id="toc-sec-robust-ai-characteristics-3c33" class="nav-link" data-scroll-target="#sec-robust-ai-characteristics-3c33">Characteristics</a></li>
  <li><a href="#sec-robust-ai-mechanisms-043d" id="toc-sec-robust-ai-mechanisms-043d" class="nav-link" data-scroll-target="#sec-robust-ai-mechanisms-043d">Mechanisms</a></li>
  <li><a href="#sec-robust-ai-impact-ml-1992" id="toc-sec-robust-ai-impact-ml-1992" class="nav-link" data-scroll-target="#sec-robust-ai-impact-ml-1992">Impact on ML</a></li>
  <li><a href="#sec-robust-ai-case-study-art-protection-via-poisoning-ab11" id="toc-sec-robust-ai-case-study-art-protection-via-poisoning-ab11" class="nav-link" data-scroll-target="#sec-robust-ai-case-study-art-protection-via-poisoning-ab11">Case Study: Art Protection via Poisoning</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-distribution-shifts-cffa" id="toc-sec-robust-ai-distribution-shifts-cffa" class="nav-link" data-scroll-target="#sec-robust-ai-distribution-shifts-cffa">Distribution Shifts</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-characteristics-9dc4" id="toc-sec-robust-ai-characteristics-9dc4" class="nav-link" data-scroll-target="#sec-robust-ai-characteristics-9dc4">Characteristics</a></li>
  <li><a href="#sec-robust-ai-mechanisms-ae94" id="toc-sec-robust-ai-mechanisms-ae94" class="nav-link" data-scroll-target="#sec-robust-ai-mechanisms-ae94">Mechanisms</a></li>
  <li><a href="#sec-robust-ai-impact-ml-de22" id="toc-sec-robust-ai-impact-ml-de22" class="nav-link" data-scroll-target="#sec-robust-ai-impact-ml-de22">Impact on ML</a></li>
  <li><a href="#sec-robust-ai-summary-distribution-shifts-system-implications-01fa" id="toc-sec-robust-ai-summary-distribution-shifts-system-implications-01fa" class="nav-link" data-scroll-target="#sec-robust-ai-summary-distribution-shifts-system-implications-01fa">Summary of Distribution Shifts and System Implications</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-detection-mitigation-3a67" id="toc-sec-robust-ai-detection-mitigation-3a67" class="nav-link" data-scroll-target="#sec-robust-ai-detection-mitigation-3a67">Detection and Mitigation</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-adversarial-attacks-e168" id="toc-sec-robust-ai-adversarial-attacks-e168" class="nav-link" data-scroll-target="#sec-robust-ai-adversarial-attacks-e168">Adversarial Attacks</a></li>
  <li><a href="#sec-robust-ai-data-poisoning-fbae" id="toc-sec-robust-ai-data-poisoning-fbae" class="nav-link" data-scroll-target="#sec-robust-ai-data-poisoning-fbae">Data Poisoning</a></li>
  <li><a href="#sec-robust-ai-distribution-shifts-dd9d" id="toc-sec-robust-ai-distribution-shifts-dd9d" class="nav-link" data-scroll-target="#sec-robust-ai-distribution-shifts-dd9d">Distribution Shifts</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-robust-ai-software-faults-7c4a" id="toc-sec-robust-ai-software-faults-7c4a" class="nav-link" data-scroll-target="#sec-robust-ai-software-faults-7c4a">Software Faults</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-characteristics-a367" id="toc-sec-robust-ai-characteristics-a367" class="nav-link" data-scroll-target="#sec-robust-ai-characteristics-a367">Characteristics</a></li>
  <li><a href="#sec-robust-ai-mechanisms-7544" id="toc-sec-robust-ai-mechanisms-7544" class="nav-link" data-scroll-target="#sec-robust-ai-mechanisms-7544">Mechanisms</a></li>
  <li><a href="#sec-robust-ai-impact-ml-90e2" id="toc-sec-robust-ai-impact-ml-90e2" class="nav-link" data-scroll-target="#sec-robust-ai-impact-ml-90e2">Impact on ML</a></li>
  <li><a href="#sec-robust-ai-detection-mitigation-710f" id="toc-sec-robust-ai-detection-mitigation-710f" class="nav-link" data-scroll-target="#sec-robust-ai-detection-mitigation-710f">Detection and Mitigation</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-tools-frameworks-c8a4" id="toc-sec-robust-ai-tools-frameworks-c8a4" class="nav-link" data-scroll-target="#sec-robust-ai-tools-frameworks-c8a4">Tools and Frameworks</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-fault-error-models-15bc" id="toc-sec-robust-ai-fault-error-models-15bc" class="nav-link" data-scroll-target="#sec-robust-ai-fault-error-models-15bc">Fault and Error Models</a></li>
  <li><a href="#sec-robust-ai-hardwarebased-fault-injection-909a" id="toc-sec-robust-ai-hardwarebased-fault-injection-909a" class="nav-link" data-scroll-target="#sec-robust-ai-hardwarebased-fault-injection-909a">Hardware-Based Fault Injection</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-methods-0afc" id="toc-sec-robust-ai-methods-0afc" class="nav-link" data-scroll-target="#sec-robust-ai-methods-0afc">Methods</a></li>
  <li><a href="#sec-robust-ai-limitations-853f" id="toc-sec-robust-ai-limitations-853f" class="nav-link" data-scroll-target="#sec-robust-ai-limitations-853f">Limitations</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-softwarebased-fault-injection-5e51" id="toc-sec-robust-ai-softwarebased-fault-injection-5e51" class="nav-link" data-scroll-target="#sec-robust-ai-softwarebased-fault-injection-5e51">Software-Based Fault Injection</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-advantages-tradeoffs-2761" id="toc-sec-robust-ai-advantages-tradeoffs-2761" class="nav-link" data-scroll-target="#sec-robust-ai-advantages-tradeoffs-2761">Advantages and Trade-offs</a></li>
  <li><a href="#sec-robust-ai-limitations-b71a" id="toc-sec-robust-ai-limitations-b71a" class="nav-link" data-scroll-target="#sec-robust-ai-limitations-b71a">Limitations</a></li>
  <li><a href="#sec-robust-ai-tool-types-f097" id="toc-sec-robust-ai-tool-types-f097" class="nav-link" data-scroll-target="#sec-robust-ai-tool-types-f097">Tool Types</a></li>
  <li><a href="#sec-robust-ai-domainspecific-examples-d10d" id="toc-sec-robust-ai-domainspecific-examples-d10d" class="nav-link" data-scroll-target="#sec-robust-ai-domainspecific-examples-d10d">Domain-Specific Examples</a></li>
  </ul></li>
  <li><a href="#sec-robust-ai-bridging-hardwaresoftware-gap-291b" id="toc-sec-robust-ai-bridging-hardwaresoftware-gap-291b" class="nav-link" data-scroll-target="#sec-robust-ai-bridging-hardwaresoftware-gap-291b">Bridging Hardware-Software Gap</a>
  <ul class="collapse">
  <li><a href="#sec-robust-ai-fidelity-b076" id="toc-sec-robust-ai-fidelity-b076" class="nav-link" data-scroll-target="#sec-robust-ai-fidelity-b076">Fidelity</a></li>
  <li><a href="#sec-robust-ai-capturing-hardware-behavior-a43e" id="toc-sec-robust-ai-capturing-hardware-behavior-a43e" class="nav-link" data-scroll-target="#sec-robust-ai-capturing-hardware-behavior-a43e">Capturing Hardware Behavior</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-robust-ai-summary-cb3f" id="toc-sec-robust-ai-summary-cb3f" class="nav-link" data-scroll-target="#sec-robust-ai-summary-cb3f">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/ops/ops.html">Robust Deployment</a></li><li class="breadcrumb-item"><a href="../../../contents/core/robust_ai/robust_ai.html">Robust AI</a></li></ol></nav></header>




<section id="robust-ai" class="level1 page-columns page-full">
<h1>Robust AI</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: Create an image featuring an advanced AI system symbolized by an intricate, glowing neural network, deeply nested within a series of progressively larger and more fortified shields. Each shield layer represents a layer of defense, showcasing the system’s robustness against external threats and internal errors. The neural network, at the heart of this fortress of shields, radiates with connections that signify the AI’s capacity for learning and adaptation. This visual metaphor emphasizes not only the technological sophistication of the AI but also its resilience and security, set against the backdrop of a state-of-the-art, secure server room filled with the latest in technological advancements. The image aims to convey the concept of ultimate protection and resilience in the field of artificial intelligence.</em></p>
</div></div><p> <img src="./images/png/cover_robust_ai.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>How do we develop fault-tolerant and resilient machine learning systems for real-world deployment?</em></p>
<p>The integration of machine learning systems into real-world applications demands fault-tolerant execution. However, these systems are inherently vulnerable to a spectrum of challenges that can degrade their capabilities. From subtle hardware anomalies to sophisticated adversarial attacks and the unpredictable nature of real-world data, the potential for failure is ever-present. This reality underscores the need to fundamentally rethink how AI systems are designed and deployed, placing robustness and trustworthiness at the forefront. Building resilient machine learning systems is not merely a technical objective; it is a foundational requirement for ensuring their safe and effective operation in dynamic and uncertain environments.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Identify common hardware faults impacting AI performance.</li>
<li>Explain how hardware faults (transient and permanent) affect AI systems.</li>
<li>Define adversarial attacks and their impact on ML models.</li>
<li>Recognize the vulnerabilities of ML models to data poisoning.</li>
<li>Explain the challenges posed by distribution shifts in ML models.</li>
<li>Describe the role of software fault detection and mitigation in AI systems.</li>
<li>Understand the importance of a holistic software development approach for robust AI.</li>
</ul>
</div>
</div>
</section>
<section id="sec-robust-ai-overview-6451" class="level2">
<h2 class="anchored" data-anchor-id="sec-robust-ai-overview-6451">Overview</h2>
<p>As ML systems become increasingly integrated into various domains, ranging from cloud-based services to edge devices and embedded systems, the impact of hardware and software faults on their performance and reliability grows more pronounced. Looking ahead, as these systems become more complex and are deployed in safety-critical applications, the need for robust and fault-tolerant designs becomes paramount.</p>
<p>ML systems are expected to play critical roles in autonomous vehicles, smart cities, healthcare, and industrial automation. In these domains, the consequences of systemic failures, including hardware and software faults, and malicious inputs such as adversarial attacks and data poisoning, and environmental shifts, can be severe, potentially resulting in loss of life, economic disruption, or environmental harm.</p>
<p>To address these risks, researchers and engineers must develop advanced techniques for fault detection, isolation, and recovery, ensuring the reliable operation of future ML systems.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition of Robust AI">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of Robust AI
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Robust Artificial Intelligence (Robust AI)</strong> refers to the ability of AI systems to maintain <em>performance and reliability</em> in the presence of <em>internal and external system errors, and malicious inputs and changes to the data or environment</em>. Robust AI systems are designed to be <em>fault-tolerant</em> and <em>error-resilient</em>, capable of functioning effectively despite <em>variations and errors within the operational environment</em>. Achieving Robust AI involves strategies for <em>fault detection, mitigation, and recovery</em>, as well as prioritizing <em>resilience throughout the AI development lifecycle</em>.</p>
</div>
</div>
<p>We focus specifically on categories of faults and errors that can impact the robustness of ML systems: errors arising from the underlying system, malicious manipulation, and environmental changes.</p>
<p>Systemic hardware failures present significant challenges across computing systems. Whether transient, permanent, or intermittent, these faults can corrupt computations and degrade system performance. The impact ranges from temporary glitches to complete component failures, requiring robust detection and mitigation strategies to maintain reliable operation.</p>
<p>Malicious manipulation of ML models remains a critical concern as ML systems face various threats to their integrity. Adversarial attacks, data poisoning attempts, and distribution shifts can cause models to misclassify inputs, exhibit distorted behavior patterns, or produce unreliable outputs. These vulnerabilities underscore the importance of developing resilient architectures and defensive mechanisms to protect model performance.</p>
<p>Environmental changes introduce another dimension of potential faults that must be carefully managed. Bugs, design flaws, and implementation errors within algorithms, libraries, and frameworks can propagate through the system, creating systemic vulnerabilities. Rigorous testing, monitoring, and quality control processes help identify and address these software-related issues before they impact production systems.</p>
<p>The specific approaches to achieving robustness vary significantly based on deployment context and system constraints. Large-scale cloud computing environments and data centers typically emphasize fault tolerance through redundancy, distributed processing architectures, and sophisticated error detection mechanisms. In contrast, edge devices and embedded systems must address robustness challenges within strict computational, memory, and energy limitations. This necessitates careful optimization and targeted hardening strategies appropriate for resource-constrained environments.</p>
<p>Regardless of deployment context, the essential characteristics of a robust ML system include fault tolerance, error resilience, and sustained performance. By understanding and addressing these multifaceted challenges, it is possible to develop reliable ML systems capable of operating effectively in real-world environments.</p>
<p>This chapter not only explores the tools, frameworks, and techniques used to detect and mitigate faults, attacks, and distribution shifts, but also emphasizes the importance of prioritizing resilience throughout the AI development lifecycle—from data collection and model training to deployment and monitoring. Proactively addressing robustness challenges is key to unlocking the full potential of ML technologies while ensuring their safe, dependable, and responsible deployment.</p>
</section>
<section id="sec-robust-ai-realworld-applications-d887" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-robust-ai-realworld-applications-d887">Real-World Applications</h2>
<p>Understanding the importance of robustness in machine learning systems requires examining how faults manifest in practice. Real-world case studies illustrate the consequences of hardware and software faults across cloud, edge, and embedded environments. These examples highlight the critical need for fault-tolerant design, rigorous testing, and robust system architectures to ensure reliable operation in diverse deployment scenarios.</p>
<section id="sec-robust-ai-cloud-9eaf" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-cloud-9eaf">Cloud</h3>
<p>In February 2017, Amazon Web Services (AWS) experienced <a href="https://aws.amazon.com/message/41926/">a significant outage</a> due to human error during routine maintenance. An engineer inadvertently entered an incorrect command, resulting in the shutdown of multiple servers. This outage disrupted many AWS services, including Amazon’s AI-powered assistant, Alexa. As a consequence, Alexa-enabled devices, including Amazon Echo and third-party products that utilize Alexa Voice Service, were unresponsive for several hours. This incident underscores the impact of human error on cloud-based ML systems and the importance of robust maintenance protocols and failsafe mechanisms.</p>
<p>In another case <span class="citation" data-cites="dixit2021silent">(<a href="#ref-dixit2021silent" role="doc-biblioref">Vangal et al. 2021</a>)</span>, Facebook encountered a silent data corruption (SDC) issue in its distributed querying infrastructure, illustrated in <a href="#fig-sdc-example" class="quarto-xref">Figure&nbsp;1</a>. SDC refers to undetected errors during computation or data transfer that propagate silently through system layers. Facebook’s system processed SQL-like queries across datasets and supported a compression application designed to reduce data storage footprints. Files were compressed when not in use and decompressed upon read requests. A size check was performed before decompression to ensure the file was valid. However, an unexpected fault occasionally returned a file size of zero for valid files, leading to decompression failures and missing entries in the output database. The issue appeared sporadically, with some computations returning correct file sizes, making it particularly difficult to diagnose.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dixit2021silent" class="csl-entry" role="listitem">
Vangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar, Ram Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, and Chris H. Kim. 2021. <span>“Wide-Range Many-Core SoC Design in Scaled CMOS: Challenges and Opportunities.”</span> <em>IEEE Transactions on Very Large Scale Integration (VLSI) Systems</em> 29 (5): 843–56. <a href="https://doi.org/10.1109/tvlsi.2021.3061649">https://doi.org/10.1109/tvlsi.2021.3061649</a>.
</div></div><div id="fig-sdc-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="5ba9156e068c7703100f85255aef5a828bd8dc51.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Silent Data Corruption: Unexpected Faults Can Return Incorrect File Sizes, Leading to Data Loss During Decompression and Propagating Errors Through Distributed Querying Systems Despite Apparent Operational Success. This Example From Facebook Emphasizes the Challenge of Undetected Errors—silent Data Corruption—and the Importance of Robust Error Detection Mechanisms in Large-Scale Data Processing Pipelines. Source: Facebook."><img src="robust_ai_files/mediabag/5ba9156e068c7703100f85255aef5a828bd8dc51.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Silent Data Corruption</strong>: Unexpected Faults Can Return Incorrect File Sizes, Leading to Data Loss During Decompression and Propagating Errors Through Distributed Querying Systems Despite Apparent Operational Success. This Example From Facebook Emphasizes the Challenge of Undetected Errors—silent Data Corruption—and the Importance of Robust Error Detection Mechanisms in Large-Scale Data Processing Pipelines. Source: <a href="HTTPS://arxiv.org/PDF/2102.11245">Facebook</a>.
</figcaption>
</figure>
</div>
<p>This case illustrates how silent data corruption can propagate across multiple layers of the application stack, resulting in data loss and application failures in large-scale distributed systems. Left unaddressed, such errors can degrade ML system performance. For example, corrupted training data or inconsistencies in data pipelines due to SDC may compromise model accuracy and reliability. Similar challenges have been reported by other major companies. As shown in <a href="#fig-sdc-jeffdean" class="quarto-xref">Figure&nbsp;2</a>, <a href="https://en.wikipedia.org/wiki/Jeff_Dean">Jeff Dean</a>, Chief Scientist at Google DeepMind and Google Research, highlighted these issues in AI hypercomputers during a keynote at <a href="https://mlsys.org/">MLSys 2024</a>.</p>
<div id="fig-sdc-jeffdean" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-jeffdean-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/jpg/sdc-google-jeff-dean.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Silent Data Corruption: Modern AI Systems, Particularly Those Employing Large-Scale Data Processing Like Spark, Are Vulnerable to Silent Data Corruption (SDC) – Subtle Errors Accumulating During Data Transfer and Storage. SDC Manifests in a Shuffle and Merge Database, Highlighting Corrupted Data Blocks (Red) Amidst Healthy Data (Blue/Gray) and Emphasizing the Challenge of Detecting These Errors in Distributed Systems Using the Figure. Source: Jeff Dean at MLSys 2024, Keynote (Google)."><img src="./images/jpg/sdc-google-jeff-dean.jpeg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-jeffdean-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Silent Data Corruption</strong>: Modern AI Systems, Particularly Those Employing Large-Scale Data Processing Like Spark, Are Vulnerable to Silent Data Corruption (SDC) – Subtle Errors Accumulating During Data Transfer and Storage. SDC Manifests in a Shuffle and Merge Database, Highlighting Corrupted Data Blocks (Red) Amidst Healthy Data (Blue/Gray) and Emphasizing the Challenge of Detecting These Errors in Distributed Systems Using the Figure. Source: Jeff Dean at MLSys 2024, Keynote (Google).
</figcaption>
</figure>
</div>
</section>
<section id="sec-robust-ai-edge-8dde" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-edge-8dde">Edge</h3>
<p>In the edge computing domain, self-driving vehicles provide prominent examples of how faults can critically affect ML systems. These vehicles depend on machine learning for perception, decision-making, and control, making them particularly vulnerable to both hardware and software faults.</p>
<div id="fig-tesla-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tesla-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/jpg/tesla_example.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Autopilot Perception Failure: This Crash Provides the Critical Safety Risks of Relying on Machine Learning for Perception in Autonomous Systems, Where Failures to Correctly Classify Objects Can Lead to Catastrophic Outcomes. The Incident Underscores the Need for Robust Validation, Redundancy, and Failsafe Mechanisms in Self-Driving Vehicle Designs to Mitigate the Impact of Imperfect AI Models. Source: BBC News."><img src="./images/jpg/tesla_example.jpg" class="img-fluid figure-img" data-fig-pos="htb"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tesla-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Autopilot Perception Failure</strong>: This Crash Provides the Critical Safety Risks of Relying on Machine Learning for Perception in Autonomous Systems, Where Failures to Correctly Classify Objects Can Lead to Catastrophic Outcomes. The Incident Underscores the Need for Robust Validation, Redundancy, and Failsafe Mechanisms in Self-Driving Vehicle Designs to Mitigate the Impact of Imperfect AI Models. Source: BBC News.
</figcaption>
</figure>
</div>
<p>In May 2016, a fatal crash occurred when a Tesla Model S operating in Autopilot mode<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> collided with a white semi-trailer truck. The system, relying on computer vision and ML algorithms, failed to distinguish the trailer against a bright sky, leading to a high-speed impact. The driver, reportedly distracted at the time, did not intervene, as shown in <a href="#fig-tesla-example" class="quarto-xref">Figure&nbsp;3</a>. This incident raised serious concerns about the reliability of AI-based perception systems and emphasized the need for robust failsafe mechanisms in autonomous vehicles. A similar case occurred in March 2018, when an Uber self-driving test vehicle <a href="https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html?iid=EL">struck</a> and killed a pedestrian in Tempe, Arizona. The accident was attributed to a flaw in the vehicle’s object recognition software, which failed to classify the pedestrian as an obstacle requiring avoidance.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Autopilot</strong>: Tesla’s driver assistance system that provides semi-autonomous capabilities like steering, braking, and acceleration while requiring active driver supervision.</p></div></div></section>
<section id="sec-robust-ai-embedded-91cc" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-embedded-91cc">Embedded</h3>
<p>Embedded systems operate in resource-constrained and often safety-critical environments. As AI capabilities are increasingly integrated into these systems, the complexity and consequences of faults grow significantly.</p>
<p>One example comes from space exploration. In 1999, NASA’s Mars Polar Lander mission experienced <a href="https://spaceref.com/uncategorized/nasa-reveals-probable-cause-of-mars-polar-lander-and-deep-space-2-mission-failures/">a catastrophic failure</a> due to a software error in its touchdown detection system (<a href="#fig-nasa-example" class="quarto-xref">Figure&nbsp;4</a>). The lander’s software misinterpreted the vibrations from the deployment of its landing legs as a successful touchdown, prematurely shutting off its engines and causing a crash. This incident underscores the importance of rigorous software validation and robust system design, particularly for remote missions where recovery is impossible. As AI becomes more integral to space systems, ensuring robustness and reliability will be essential to mission success.</p>
<div id="fig-nasa-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nasa-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/nasa_example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Touchdown Detection Failure: Erroneous Sensor Readings During the Mars Polar Lander Mission Triggered a Premature Engine Shutdown, Demonstrating the Critical Need for Robust Failure Modes and Rigorous Validation of Embedded Systems—particularly Those Operating in Inaccessible Environments. This Incident Underscores How Software Errors Can Lead to Catastrophic Consequences in Safety-Critical Applications and Emphasizes the Growing Importance of Reliable AI Integration in Complex Systems. Source: Slashgear."><img src="./images/png/nasa_example.png" class="img-fluid figure-img" data-fig-pos="htb"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nasa-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Touchdown Detection Failure</strong>: Erroneous Sensor Readings During the Mars Polar Lander Mission Triggered a Premature Engine Shutdown, Demonstrating the Critical Need for Robust Failure Modes and Rigorous Validation of Embedded Systems—particularly Those Operating in Inaccessible Environments. This Incident Underscores How Software Errors Can Lead to Catastrophic Consequences in Safety-Critical Applications and Emphasizes the Growing Importance of Reliable AI Integration in Complex Systems. Source: Slashgear.
</figcaption>
</figure>
</div>
<p>Another example occurred in 2015, when a Boeing 787 Dreamliner experienced a complete electrical shutdown mid-flight due to a software bug in its generator control units. The failure stemmed from a scenario in which powering up all four generator control units simultaneously, following 248 days of uninterrupted operation, caused them to enter failsafe mode, disabling all AC electrical power.</p>
<blockquote class="blockquote">
<p><em>“If the four main generator control units (associated with the engine-mounted generators) were powered up at the same time, after 248 days of continuous power, all four GCUs will go into failsafe mode at the same time, resulting in a loss of all AC electrical power regardless of flight phase.” — <a href="https://s3.amazonaws.com/public-inspection.federalregister.gov/2015-10066.pdf">Federal Aviation Administration directive</a> (2015)</em></p>
</blockquote>
<p>As AI is increasingly applied in aviation, including tasks such as autonomous flight control and predictive maintenance, the robustness of embedded systems becomes critical for passenger safety.</p>
<p>Finally, consider the case of implantable medical devices. For instance, a smart <a href="https://www.bbc.com/future/article/20221011-how-space-weather-causes-computer-errors">pacemaker</a> that experiences a fault or unexpected behavior due to software or hardware failure could place a patient’s life at risk. As AI systems take on perception, decision-making, and control roles in such applications, new sources of vulnerability emerge, including data-related errors, model uncertainty<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, and unpredictable behaviors in rare edge cases. Moreover, the opaque nature of some AI models complicates fault diagnosis and recovery.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<strong>Model Uncertainty</strong>: The inadequacy of a machine learning model to capture the full complexity of the underlying data-generating process.</p></div></div><div id="quiz-question-sec-robust-ai-realworld-applications-d887" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Which of the following incidents highlights the impact of human error on cloud-based ML systems?</p>
<ol type="a">
<li>Tesla Model S crash due to Autopilot failure</li>
<li>AWS outage due to incorrect command entry</li>
<li>Facebook’s silent data corruption issue</li>
<li>NASA Mars Polar Lander software error</li>
</ol></li>
<li><p>Explain how silent data corruption (SDC) can affect machine learning system performance in large-scale distributed systems.</p></li>
<li><p>True or False: The Tesla Model S crash in 2016 was primarily due to a software bug in its object recognition system.</p></li>
</ol>
<p><a href="#quiz-answer-sec-robust-ai-realworld-applications-d887" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-robust-ai-hardware-faults-81ee" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-robust-ai-hardware-faults-81ee">Hardware Faults</h2>
<p>Hardware faults are a significant challenge in computing systems, including both traditional and ML systems. These faults occur when physical components, including processors, memory modules, storage devices, and interconnects, malfunction or behave abnormally. Hardware faults can cause incorrect computations, data corruption, system crashes, or complete system failure, compromising the integrity and trustworthiness of the computations performed by the system <span class="citation" data-cites="jha2019ml">(<a href="#ref-jha2019ml" role="doc-biblioref">Jha et al. 2019</a>)</span>. A complete system failure refers to a situation where the entire computing system becomes unresponsive or inoperable due to a critical hardware malfunction. This type of failure is the most severe, as it renders the system unusable and may lead to data loss or corruption, requiring manual intervention to repair or replace the faulty components.</p>
<div class="no-row-height column-margin column-container"></div><p>ML systems depend on complex hardware architectures and large-scale computations to train and deploy models that learn from data and make intelligent predictions. As a result, hardware faults can disrupt the <a href="../../../contents/core/ops/ops.html">MLOps pipeline</a>, introducing errors that compromise model accuracy, robustness, and reliability <span class="citation" data-cites="li2017understanding">(<a href="#ref-li2017understanding" role="doc-biblioref">G. Li et al. 2017</a>)</span>. Understanding the types of hardware faults, their mechanisms, and their impact on system behavior is essential for developing strategies to detect, mitigate, and recover from these issues.</p>
<p>The following sections will explore the three main categories of hardware faults: transient, permanent, and intermittent. We will discuss their definitions, characteristics, causes, mechanisms, and examples of how they manifest in computing systems. Detection and mitigation techniques specific to each fault type will also be covered.</p>
<ul>
<li><p><strong>Transient Faults</strong>: Transient faults are temporary and non-recurring. They are often caused by external factors such as cosmic rays, electromagnetic interference, or power fluctuations. A common example of a transient fault is a bit flip, where a single bit in a memory location or register changes its value unexpectedly. Transient faults can lead to incorrect computations or data corruption, but they do not cause permanent damage to the hardware.</p></li>
<li><p><strong>Permanent Faults</strong>: Permanent faults, also called hard errors, are irreversible and persist over time. They are typically caused by physical defects or wear-out of hardware components. Examples of permanent faults include stuck-at faults, where a bit or signal is permanently set to a specific value (e.g., always 0 or always 1), and device failures, such as a malfunctioning processor or a damaged memory module. Permanent faults can result in complete system failure or significant performance degradation.</p></li>
<li><p><strong>Intermittent Faults</strong>: Intermittent faults are recurring faults that appear and disappear intermittently. Unstable hardware conditions, such as loose connections, aging components, or manufacturing defects, often cause them. Intermittent faults can be challenging to diagnose and reproduce because they may occur sporadically and under specific conditions. Examples include intermittent short circuits or contact resistance issues. These faults can lead to unpredictable system behavior and sporadic errors.</p></li>
</ul>
<p>Understanding this fault taxonomy and its relevance to both traditional computing and ML systems provides a foundation for making informed decisions when designing, implementing, and deploying fault-tolerant solutions. This knowledge is crucial for improving the reliability and trustworthiness of computing systems and ML applications.</p>
<section id="sec-robust-ai-transient-faults-2227" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-transient-faults-2227">Transient Faults</h3>
<p>Transient faults in hardware can manifest in various forms, each with its own unique characteristics and causes. These faults are temporary in nature and do not result in permanent damage to the hardware components.</p>
<section id="sec-robust-ai-characteristics-5133" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-characteristics-5133">Characteristics</h4>
<p>All transient faults are characterized by their short duration and non-permanent nature. They do not persist or leave any lasting impact on the hardware. However, they can still lead to incorrect computations, data corruption, or system misbehavior if not properly handled. A classic example is shown in <a href="#fig-bit-flip" class="quarto-xref">Figure&nbsp;5</a>, where a single bit in memory unexpectedly changes state, potentially altering critical data or computations.</p>
<p>Some of the common types of transient faults include Single Event Upsets (SEUs) caused by ionizing radiation, voltage fluctuations <span class="citation" data-cites="reddi2013resilient">(<a href="#ref-reddi2013resilient" role="doc-biblioref">Reddi and Gupta 2013</a>)</span> due to power supply noise or electromagnetic interference, Electromagnetic Interference (EMI) induced by external electromagnetic fields, Electrostatic Discharge (ESD) resulting from sudden static electricity flow, crosstalk caused by unintended signal coupling, ground bounce triggered by simultaneous switching of multiple outputs, timing violations due to signal timing constraint breaches, and soft errors in combinational logic affecting the output of logic circuits <span class="citation" data-cites="mukherjee2005soft">(<a href="#ref-mukherjee2005soft" role="doc-biblioref">Mukherjee, Emer, and Reinhardt, n.d.</a>)</span>. Understanding these different types of transient faults is crucial for designing robust and resilient hardware systems that can mitigate their impact and ensure reliable operation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reddi2013resilient" class="csl-entry" role="listitem">
Reddi, Vijay Janapa, and Meeta Sharma Gupta. 2013. <em>Resilient Architecture Design for Voltage Variation</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01739-1">https://doi.org/10.1007/978-3-031-01739-1</a>.
</div><div id="ref-mukherjee2005soft" class="csl-entry" role="listitem">
Mukherjee, S. S., J. Emer, and S. K. Reinhardt. n.d. <span>“The Soft Error Problem: An Architectural Perspective.”</span> In <em>11th International Symposium on High-Performance Computer Architecture</em>, 243–47. IEEE; IEEE. <a href="https://doi.org/10.1109/hpca.2005.37">https://doi.org/10.1109/hpca.2005.37</a>.
</div></div><div id="fig-bit-flip" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bit-flip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="f6af1dacc879c79406592827af495633f2db137d.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Bit-Flip Error: Transient faults can alter individual bits in memory, corrupting data or program instructions and potentially causing system malfunctions. These single-bit errors exemplify the vulnerability of hardware to transient faults like those induced by radiation or electromagnetic interference."><img src="robust_ai_files/mediabag/f6af1dacc879c79406592827af495633f2db137d.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bit-flip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Bit-Flip Error</strong>: Transient faults can alter individual bits in memory, corrupting data or program instructions and potentially causing system malfunctions. These single-bit errors exemplify the vulnerability of hardware to transient faults like those induced by radiation or electromagnetic interference.
</figcaption>
</figure>
</div>
</section>
<section id="sec-robust-ai-causes-b285" class="level4">
<h4 class="anchored" data-anchor-id="sec-robust-ai-causes-b285">Causes</h4>
<p>Transient faults can be attributed to various external factors. One common cause is cosmic rays—high-energy particles originating from outer space. When these particles strike sensitive areas of the hardware, such as memory cells or transistors, they can induce charge disturbances that alter the stored or transmitted data. This is illustrated in <a href="#fig-transient-fault" class="quarto-xref">Figure&nbsp;6</a>. Another cause of transient faults is <a href="https://www.trentonsystems.com/en-us/resource-hub/blog/what-is-electromagnetic-interference">electromagnetic interference (EMI)</a> from nearby devices or power fluctuations. EMI can couple with the circuits and cause voltage spikes or glitches that temporarily disrupt the normal operation of the hardware.</p>
<div id="fig-transient-fault" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transient-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/transient_fault.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Transient Fault Mechanism: Cosmic rays and electromagnetic interference induce bit flips within hardware by altering electrical charges in memory cells and transistors, potentially corrupting data and causing system errors. Understanding these fault sources is critical for building robust ai systems that can tolerate unpredictable hardware behavior. Source: NTT."><img src="./images/png/transient_fault.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transient-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Transient Fault Mechanism</strong>: Cosmic rays and electromagnetic interference induce bit flips within hardware by altering electrical charges in memory cells and transistors, potentially corrupting data and causing system errors. Understanding these fault sources is critical for building robust ai systems that can tolerate unpredictable hardware behavior. Source: <a href="HTTPS://group.ntt/en/newsrelease/2018/11/22/181122a.HTML">NTT</a>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-robust-ai-mechanisms-3fd9" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-mechanisms-3fd9">Mechanisms</h4>
<p>Transient faults can manifest through different mechanisms depending on the affected hardware component. In memory devices like DRAM or SRAM, transient faults often lead to bit flips, where a single bit changes its value from 0 to 1 or vice versa. This can corrupt the stored data or instructions. In logic circuits, transient faults can cause glitches<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> or voltage spikes propagating through the combinational logic<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, resulting in incorrect outputs or control signals. Transient faults can also affect communication channels, causing bit errors or packet losses during data transmission.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Glitches</strong>: Momentary deviation in voltage, current, or signal, often causing incorrect operation.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Combinational logic</strong>: Digital logic, wherein the output depends only on the current input states, not any past states.</p></div></div></section>
<section id="sec-robust-ai-impact-ml-a44d" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-impact-ml-a44d">Impact on ML</h4>
<p>A common example of a transient fault is a bit flip in the main memory. If an important data structure or critical instruction is stored in the affected memory location, it can lead to incorrect computations or program misbehavior. For instance, a bit flip in the memory storing a loop counter can cause the loop to execute indefinitely or terminate prematurely. Transient faults in control registers or flag bits can alter the flow of program execution, leading to unexpected jumps or incorrect branch decisions. In communication systems, transient faults can corrupt transmitted data packets, resulting in retransmissions or data loss.</p>
<p>In ML systems, transient faults can have significant implications during the training phase <span class="citation" data-cites="he2023understanding">(<a href="#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>. ML training involves iterative computations and updates to model parameters based on large datasets. If a transient fault occurs in the memory storing the model weights or gradients, it can lead to incorrect updates and compromise the convergence and accuracy of the training process. For example, a bit flip in the weight matrix of a neural network can cause the model to learn incorrect patterns or associations, leading to degraded performance <span class="citation" data-cites="wan2021analyzing">(<a href="#ref-wan2021analyzing" role="doc-biblioref">Wan et al. 2021</a>)</span>. Transient faults in the data pipeline, such as corruption of training samples or labels, can also introduce noise and affect the quality of the learned model.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wan2021analyzing" class="csl-entry" role="listitem">
Wan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi, and Arijit Raychowdhury. 2021. <span>“Analyzing and Improving Fault Tolerance of Learning-Based Navigation Systems.”</span> In <em>2021 58th ACM/IEEE Design Automation Conference (DAC)</em>, 841–46. IEEE; IEEE. <a href="https://doi.org/10.1109/dac18074.2021.9586116">https://doi.org/10.1109/dac18074.2021.9586116</a>.
</div></div><p>As shown in <a href="#fig-sdc-training-fault" class="quarto-xref">Figure&nbsp;7</a>, a real-world example from Google’s production fleet highlights how an SDC anomaly caused a significant deviation in the gradient norm—a measure of the magnitude of updates to the model parameters. Such deviations can disrupt the optimization process, leading to slower convergence or failure to reach an optimal solution.</p>
<div id="fig-sdc-training-fault" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-training-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/jpg/google_sdc_jeff_dean_anomaly.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Gradient Norm Deviation: Transient hardware faults, such as single data corruption (SDC), disrupt optimization by causing abrupt changes in gradient norms during model training, potentially leading to convergence issues or inaccurate models. Real-world data from Google’s production fleet confirms that SDC anomalies manifest as visible spikes in gradient norm over time, indicating a disruption to the expected parameter update process. Source: jeff dean, mlsys 2024 keynote (Google)."><img src="./images/jpg/google_sdc_jeff_dean_anomaly.jpg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-training-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Gradient Norm Deviation</strong>: Transient hardware faults, such as single data corruption (SDC), disrupt optimization by causing abrupt changes in gradient norms during model training, potentially leading to convergence issues or inaccurate models. Real-world data from Google’s production fleet confirms that SDC anomalies manifest as visible spikes in gradient norm over time, indicating a disruption to the expected parameter update process. Source: jeff dean, mlsys 2024 keynote (Google).
</figcaption>
</figure>
</div>
<p>During the inference phase, transient faults can impact the reliability and trustworthiness of ML predictions. If a transient fault occurs in the memory storing the trained model parameters or during the computation of inference results, it can lead to incorrect or inconsistent predictions. For instance, a bit flip in the activation values of a neural network can alter the final classification or regression output <span class="citation" data-cites="mahmoud2020pytorchfi">(<a href="#ref-mahmoud2020pytorchfi" role="doc-biblioref">Mahmoud et al. 2020</a>)</span>. In safety-critical applications, such as autonomous vehicles or medical diagnosis, these faults can have severe consequences, resulting in incorrect decisions or actions that may compromise safety or lead to system failures <span class="citation" data-cites="li2017understanding jha2019ml">(<a href="#ref-li2017understanding" role="doc-biblioref">G. Li et al. 2017</a>; <a href="#ref-jha2019ml" role="doc-biblioref">Jha et al. 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017understanding" class="csl-entry" role="listitem">
Li, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, and Stephen W. Keckler. 2017. <span>“Understanding Error Propagation in Deep Learning Neural Network (DNN) Accelerators and Applications.”</span> In <em>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>, 1–12. ACM. <a href="https://doi.org/10.1145/3126908.3126964">https://doi.org/10.1145/3126908.3126964</a>.
</div><div id="ref-courbariaux2016binarized" class="csl-entry" role="listitem">
Courbariaux, Matthieu, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. <span>“Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1.”</span> <em>arXiv Preprint arXiv:1602.02830</em>, February. <a href="http://arxiv.org/abs/1602.02830v3">http://arxiv.org/abs/1602.02830v3</a>.
</div><div id="ref-Aygun2021BSBNN" class="csl-entry" role="listitem">
Aygun, Sercan, Ece Olcay Gunes, and Christophe De Vleeschouwer. 2021. <span>“Efficient and Robust Bitstream Processing in Binarised Neural Networks.”</span> <em>Electronics Letters</em> 57 (5): 219–22. <a href="https://doi.org/10.1049/ell2.12045">https://doi.org/10.1049/ell2.12045</a>.
</div><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Stochastic Computing</strong>: A collection of techniques using random bits and logic operations to perform arithmetic and data processing, promising better fault tolerance.</p></div></div><p>Transient faults can be amplified in resource-constrained environments like TinyML, where limited computational and memory resources exacerbate their impact. One prominent example is Binarized Neural Networks (BNNs) <span class="citation" data-cites="courbariaux2016binarized">(<a href="#ref-courbariaux2016binarized" role="doc-biblioref">Courbariaux et al. 2016</a>)</span>, which represent network weights in single-bit precision to achieve computational efficiency and faster inference times. While this binary representation is advantageous for resource-constrained systems, it also makes BNNs particularly fragile to bit-flip errors. For instance, prior work <span class="citation" data-cites="Aygun2021BSBNN">(<a href="#ref-Aygun2021BSBNN" role="doc-biblioref">Aygun, Gunes, and De Vleeschouwer 2021</a>)</span> has shown that a two-hidden-layer BNN architecture for a simple task such as MNIST classification suffers performance degradation from 98% test accuracy to 70% when random bit-flipping soft errors are inserted through model weights with a 10% probability. To address these vulnerabilities, techniques like flip-aware training and emerging approaches such as <a href="https://en.wikipedia.org/wiki/Stochastic_computing">stochastic computing</a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> are being explored to enhance fault tolerance.</p>
</section>
</section>
<section id="sec-robust-ai-permanent-faults-f290" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-permanent-faults-f290">Permanent Faults</h3>
<p>Permanent faults are hardware defects that persist and cause irreversible damage to the affected components. These faults are characterized by their persistent nature and require repair or replacement of the faulty hardware to restore normal system functionality.</p>
<section id="sec-robust-ai-characteristics-6f8d" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-characteristics-6f8d">Characteristics</h4>
<p>Permanent faults cause persistent and irreversible malfunctions in hardware components. The faulty component remains non-operational until it is repaired or replaced. These faults are consistent and reproducible, meaning the faulty behavior is observed every time the affected component is used. They can impact processors, memory modules, storage devices, or interconnects—potentially leading to system crashes, data corruption, or complete system failure.</p>
<p>One notable example of a permanent fault is the <a href="https://en.wikipedia.org/wiki/Pentium_FDIV_bug">Intel FDIV bug</a>, discovered in 1994. This flaw affected the floating-point division (FDIV) units of certain Intel Pentium processors, causing incorrect results for specific division operations and leading to inaccurate calculations.</p>
<p>The FDIV bug occurred due to an error in the lookup table<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> used by the division unit. In rare cases, the processor would fetch an incorrect value, resulting in a slightly less precise result than expected. For instance, <a href="#fig-permanent-fault" class="quarto-xref">Figure&nbsp;8</a> shows a fraction 4195835/3145727 plotted on a Pentium processor with the FDIV fault. The triangular regions highlight where erroneous calculations occurred. Ideally, all correct values would round to 1.3338, but the faulty results showed 1.3337, indicating a mistake in the 5th digit.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Lookup Table</strong>: A data structure used to replace a runtime computation with a simpler array indexing operation.</p></div></div><p>Although the error was small, it could compound across many operations, significantly affecting results in precision-critical applications such as scientific simulations, financial calculations, and computer-aided design. The bug ultimately led to incorrect outcomes in these domains and underscored the severe consequences permanent faults can have.</p>
<div id="fig-permanent-fault" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-permanent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/permanent_fault.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: FDIV Error Regions: The triangular areas indicate where the pentium processor’s faulty division unit produced incorrect results when calculating 4195835/3145727; ideally, all values should round to 1.3338, but the bug caused a slight inaccuracy in the fifth digit. Source: byte magazine."><img src="./images/png/permanent_fault.png" class="img-fluid figure-img" style="width:70.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-permanent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>FDIV Error Regions</strong>: The triangular areas indicate where the pentium processor’s faulty division unit produced incorrect results when calculating 4195835/3145727; ideally, all values should round to 1.3338, but the bug caused a slight inaccuracy in the fifth digit. Source: byte magazine.
</figcaption>
</figure>
</div>
<p>The FDIV bug serves as a cautionary tale for ML systems. In such systems, permanent faults in hardware components can result in incorrect computations, impacting model accuracy and reliability. For example, if an ML system relies on a processor with a faulty floating-point unit, similar to the FDIV bug, it could introduce persistent errors during training or inference. These errors may propagate through the model, leading to inaccurate predictions or skewed learning outcomes.</p>
<p>This is especially critical in safety-sensitive applications like autonomous driving, medical diagnosis, or financial forecasting, where the consequences of incorrect computations can be severe. ML practitioners must be aware of these risks and incorporate fault-tolerant techniques, including hardware redundancy, error detection and correction, and robust algorithm design, to mitigate them. Additionally, thorough hardware validation and testing can help identify and resolve permanent faults before they affect system performance and reliability.</p>
</section>
<section id="sec-robust-ai-causes-6e76" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-causes-6e76">Causes</h4>
<p>Permanent faults can arise from two primary sources: manufacturing defects and wear-out mechanisms. <a href="https://www.sciencedirect.com/science/article/pii/B9780128181058000206">Manufacturing defects</a> are flaws introduced during the fabrication process, including improper etching, incorrect doping, or contamination. These defects may result in non-functional or partially functional components. In contrast, <a href="https://semiengineering.com/what-causes-semiconductor-aging/">wear-out mechanisms</a> occur over time due to prolonged use and operational stress. Phenomena like electromigration<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>, oxide breakdown<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, and thermal stress<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> degrade component integrity, eventually leading to permanent failure.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;The movement of metal atoms in a conductor under the influence of an electric field.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;The failure of an oxide layer in a transistor due to excessive electric field stress.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;Degradation caused by repeated cycling through high and low temperatures.</p></div></div></section>
<section id="sec-robust-ai-mechanisms-6e17" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-mechanisms-6e17">Mechanisms</h4>
<p>Permanent faults manifest through several mechanisms, depending on their nature and location. A common example is the stuck-at fault <span class="citation" data-cites="seong2010safer">(<a href="#ref-seong2010safer" role="doc-biblioref">Seong et al. 2010</a>)</span>, where a signal or memory cell becomes permanently fixed at either 0 or 1, regardless of the intended input, as shown in <a href="#fig-stuck-fault" class="quarto-xref">Figure&nbsp;9</a>. This type of fault can occur in logic gates, memory cells, or interconnects and typically results in incorrect computations or persistent data corruption.</p>
<div class="no-row-height column-margin column-container"><div id="ref-seong2010safer" class="csl-entry" role="listitem">
Seong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers, and Hsien-Hsin S. Lee. 2010. <span>“SAFER: Stuck-at-Fault Error Recovery for Memories.”</span> In <em>2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture</em>, 115–24. IEEE; IEEE. <a href="https://doi.org/10.1109/micro.2010.46">https://doi.org/10.1109/micro.2010.46</a>.
</div></div><div id="fig-stuck-fault" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stuck-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="fc3b2f80ed1caf4ce70e86feaa49dd694c37e759.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Stuck-at Fault Model: Digital circuits can experience permanent faults where a signal line becomes fixed at a logical 0 or 1, regardless of input; this figure represents a simplified depiction of a stuck-at-0 fault, where a signal is persistently low, potentially leading to incorrect computations or system failures. Source: accendo reliability"><img src="robust_ai_files/mediabag/fc3b2f80ed1caf4ce70e86feaa49dd694c37e759.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stuck-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Stuck-at Fault Model</strong>: Digital circuits can experience permanent faults where a signal line becomes fixed at a logical 0 or 1, regardless of input; this figure represents a simplified depiction of a stuck-at-0 fault, where a signal is persistently low, potentially leading to incorrect computations or system failures. <em>Source: <a href="HTTPS://accendoreliability.com/digital-circuits-stuck-fault-model/">accendo reliability</a></em>
</figcaption>
</figure>
</div>
<p>Other mechanisms include device failures, in which hardware components such as transistors or memory cells cease functioning entirely due to manufacturing defects or degradation over time. Bridging faults, which occur when two or more signal lines are unintentionally connected, can introduce short circuits or incorrect logic behaviors that are difficult to isolate.</p>
<p>In more subtle cases, delay faults can arise when the propagation time of a signal exceeds the allowed timing constraints. Although the logical values may be correct, the violation of timing expectations can still result in erroneous behavior. Similarly, interconnect faults, including open circuits caused by broken connections, high-resistance paths that impede current flow, and increased capacitance that distorts signal transitions, can significantly degrade circuit performance and reliability.</p>
<p>Memory subsystems are particularly vulnerable to permanent faults. Transition faults can prevent a memory cell from successfully changing its state, while coupling faults result from unwanted interference between adjacent cells, leading to unintentional state changes. Additionally, neighborhood pattern sensitive faults occur when the state of a memory cell is incorrectly influenced by the data stored in nearby cells, reflecting a more complex interaction between circuit layout and logic behavior.</p>
<p>Finally, permanent faults can also occur in critical infrastructure components such as the power supply network or clock distribution system. Failures in these subsystems can affect circuit-wide functionality, introduce timing errors, or cause widespread operational instability.</p>
<p>Taken together, these mechanisms illustrate the varied and often complex ways in which permanent faults can undermine the behavior of computing systems. For ML applications in particular, where correctness and consistency are vital, understanding these fault modes is essential for developing resilient hardware and software solutions.</p>
</section>
<section id="sec-robust-ai-impact-ml-7efd" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-impact-ml-7efd">Impact on ML</h4>
<p>Permanent faults can severely disrupt the behavior and reliability of computing systems. For example, a stuck-at fault in a processor’s arithmetic logic unit (ALU) can produce persistent computational errors, leading to incorrect program behavior or crashes. In memory modules, such faults may corrupt stored data, while in storage devices, they can result in bad sectors or total data loss. Interconnect faults may interfere with data transmission, leading to system hangs or corruption.</p>
<p>For ML systems, these faults pose significant risks in both the training and inference phases. During training, permanent faults in processors or memory can lead to incorrect gradient calculations, corrupt model parameters, or prematurely halted training processes <span class="citation" data-cites="he2023understanding">(<a href="#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>. Similarly, faults in storage can compromise training datasets or saved models, affecting consistency and reliability.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2018analyzing" class="csl-entry" role="listitem">
Zhang, Jeff Jun, Tianyu Gu, Kanad Basu, and Siddharth Garg. 2018. <span>“Analyzing and Mitigating the Impact of Permanent Faults on a Systolic Array Based Neural Network Accelerator.”</span> In <em>2018 IEEE 36th VLSI Test Symposium (VTS)</em>, 1–6. IEEE; IEEE. <a href="https://doi.org/10.1109/vts.2018.8368656">https://doi.org/10.1109/vts.2018.8368656</a>.
</div></div><p>In the inference phase, faults can distort prediction results or lead to runtime failures. For instance, errors in the hardware storing model weights might lead to outdated or corrupted models being used, while processor faults could yield incorrect outputs <span class="citation" data-cites="zhang2018analyzing">(<a href="#ref-zhang2018analyzing" role="doc-biblioref">J. J. Zhang et al. 2018</a>)</span>.</p>
<p>To mitigate these impacts, ML systems must incorporate both hardware and software fault-tolerant techniques. Hardware-level methods include component redundancy and error-correcting codes <span class="citation" data-cites="kim2015bamboo">(<a href="#ref-kim2015bamboo" role="doc-biblioref">Kim, Sullivan, and Erez 2015</a>)</span>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> Software approaches, like checkpoint and restart mechanisms<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> <span class="citation" data-cites="egwutuoha2013survey">(<a href="#ref-egwutuoha2013survey" role="doc-biblioref">Egwutuoha et al. 2013</a>)</span>, allow systems to recover to a known-good state after a failure. Regular monitoring, testing, and maintenance can also help detect and replace failing components before critical errors occur.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kim2015bamboo" class="csl-entry" role="listitem">
Kim, Jungrae, Michael Sullivan, and Mattan Erez. 2015. <span>“Bamboo ECC: Strong, Safe, and Flexible Codes for Reliable Computer Memory.”</span> In <em>2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</em>, 101–12. IEEE; IEEE. <a href="https://doi.org/10.1109/hpca.2015.7056025">https://doi.org/10.1109/hpca.2015.7056025</a>.
</div><div id="fn10"><p><sup>10</sup>&nbsp;<strong>Error-Correcting Codes</strong>: Methods used in data storage and transmission to detect and correct errors.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Checkpoint and Restart Mechanisms</strong>: Techniques that periodically save a program’s state so it can resume from the last saved state after a failure.</p></div><div id="ref-egwutuoha2013survey" class="csl-entry" role="listitem">
Egwutuoha, Ifeanyi P., David Levy, Bran Selic, and Shiping Chen. 2013. <span>“A Survey of Fault Tolerance Mechanisms and Checkpoint/Restart Implementations for High Performance Computing Systems.”</span> <em>The Journal of Supercomputing</em> 65 (3): 1302–26. <a href="https://doi.org/10.1007/s11227-013-0884-0">https://doi.org/10.1007/s11227-013-0884-0</a>.
</div></div><p>Ultimately, designing ML systems with built-in fault tolerance is essential to ensure resilience. Incorporating redundancy, error-checking, and fail-safe mechanisms helps preserve model integrity, accuracy, and trustworthiness—even in the face of permanent hardware faults.</p>
</section>
</section>
<section id="sec-robust-ai-intermittent-faults-7778" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-intermittent-faults-7778">Intermittent Faults</h3>
<p>Intermittent faults are hardware faults that occur sporadically and unpredictably in a system. An example is illustrated in <a href="#fig-intermittent-fault" class="quarto-xref">Figure&nbsp;10</a>, where cracks in the material can introduce increased resistance in circuitry. These faults are particularly challenging to detect and diagnose because they appear and disappear intermittently, making it difficult to reproduce and isolate the root cause. Depending on their frequency and location, intermittent faults can lead to system instability, data corruption, and performance degradation.</p>
<div id="fig-intermittent-fault" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intermittent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/intermittent_fault.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Intermittent Fault Mechanism: Increased resistance from cracks between copper bumps and package solder represents a common source of intermittent faults, disrupting signal transmission and potentially causing unpredictable system behavior. Microscopic material defects like these highlight the vulnerability of hardware to latent failures that are difficult to detect during testing but can manifest during operation. Source: constantinescu."><img src="./images/png/intermittent_fault.png" class="img-fluid figure-img" style="width:75.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intermittent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Intermittent Fault Mechanism</strong>: Increased resistance from cracks between copper bumps and package solder represents a common source of intermittent faults, disrupting signal transmission and potentially causing unpredictable system behavior. Microscopic material defects like these highlight the vulnerability of hardware to latent failures that are difficult to detect during testing but can manifest during operation. Source: <a href="HTTPS://ieeexplore.ieee.org/document/4925824">constantinescu</a>.
</figcaption>
</figure>
</div>
<section id="sec-robust-ai-characteristics-6317" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-characteristics-6317">Characteristics</h4>
<p>Intermittent faults are defined by their sporadic and non-deterministic behavior. They occur irregularly and may manifest for short durations, disappearing without a consistent pattern. Unlike permanent faults, they do not appear every time the affected component is used, which makes them particularly difficult to detect and reproduce. These faults can affect a variety of hardware components, including processors, memory modules, storage devices, and interconnects. As a result, they may lead to transient errors, unpredictable system behavior, or data corruption.</p>
<p>Their impact on system reliability can be significant. For instance, an intermittent fault in a processor’s control logic may disrupt the normal execution path, causing irregular program flow or unexpected system hangs. In memory modules, such faults can alter stored values inconsistently, leading to errors that are difficult to trace. Storage devices affected by intermittent faults may suffer from sporadic read/write errors or data loss, while intermittent faults in communication channels can cause data corruption, packet loss, or unstable connectivity. Over time, these failures can accumulate, degrading system performance and reliability <span class="citation" data-cites="rashid2014characterizing">(<a href="#ref-rashid2014characterizing" role="doc-biblioref">Rashid, Pattabiraman, and Gopalakrishnan 2015</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rashid2014characterizing" class="csl-entry" role="listitem">
———. 2015. <span>“Characterizing the Impact of Intermittent Hardware Faults on Programs.”</span> <em>IEEE Transactions on Reliability</em> 64 (1): 297–310. <a href="https://doi.org/10.1109/tr.2014.2363152">https://doi.org/10.1109/tr.2014.2363152</a>.
</div></div></section>
<section id="sec-robust-ai-causes-49f1" class="level4">
<h4 class="anchored" data-anchor-id="sec-robust-ai-causes-49f1">Causes</h4>
<p>The causes of intermittent faults are diverse, ranging from physical degradation to environmental influences. One common cause is the aging and wear-out of electronic components. As hardware endures prolonged operation, thermal cycling, and mechanical stress, it may develop cracks, fractures, or fatigue that introduce intermittent faults. For instance, solder joints in ball grid arrays (BGAs) or flip-chip packages can degrade over time, leading to intermittent open circuits or short circuits.</p>
<p>Manufacturing defects and process variations can also introduce marginal components that behave reliably under most circumstances but fail intermittently under stress or extreme conditions. For example, <a href="#fig-intermittent-fault-dram" class="quarto-xref">Figure&nbsp;11</a> shows a residue-induced intermittent fault in a DRAM chip that leads to sporadic failures.</p>
<div id="fig-intermittent-fault-dram" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intermittent-fault-dram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/intermittent_fault_dram.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: DRAM Residue Fault: Intermittent failures in DRAM chips commonly arise from microscopic residue accumulation, creating unreliable electrical connections. Physical defects can induce sporadic errors, highlighting the need for fault-tolerant system design and hardware testing via this figure. Source: hynix semiconductor"><img src="./images/png/intermittent_fault_dram.png" class="img-fluid figure-img" style="width:70.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intermittent-fault-dram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>DRAM Residue Fault</strong>: Intermittent failures in DRAM chips commonly arise from microscopic residue accumulation, creating unreliable electrical connections. Physical defects can induce sporadic errors, highlighting the need for fault-tolerant system design and hardware testing via this figure. <em>Source: <a href="HTTPS://ieeexplore.ieee.org/document/4925824">hynix semiconductor</a></em>
</figcaption>
</figure>
</div>
<p>Environmental factors such as thermal cycling, humidity, mechanical vibrations, or electrostatic discharge can exacerbate these weaknesses and trigger faults that would not otherwise appear. Loose or degrading physical connections, including those found in connectors or printed circuit boards, are also common sources of intermittent failures, particularly in systems exposed to movement or temperature variation.</p>
</section>
<section id="sec-robust-ai-mechanisms-0ca2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-mechanisms-0ca2">Mechanisms</h4>
<p>Intermittent faults can manifest through various physical and logical mechanisms depending on their root causes. One such mechanism is the intermittent open or short circuit, where physical discontinuities or partial connections cause signal paths to behave unpredictably. These faults may momentarily disrupt signal integrity, leading to glitches or unexpected logic transitions.</p>
<p>Another common mechanism is the intermittent delay fault <span class="citation" data-cites="zhang2018thundervolt">(<a href="#ref-zhang2018thundervolt" role="doc-biblioref">J. Zhang et al. 2018</a>)</span>, where signal propagation times fluctuate due to marginal timing conditions, resulting in synchronization issues and incorrect computations. In memory cells or registers, intermittent faults can appear as transient bit flips or soft errors, corrupting data in ways that are difficult to detect or reproduce. Because these faults are often condition-dependent, they may only emerge under specific thermal, voltage, or workload conditions, adding further complexity to their diagnosis.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2018thundervolt" class="csl-entry" role="listitem">
Zhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, and Siddharth Garg. 2018. <span>“ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Learning Accelerators.”</span> In <em>2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)</em>, 1–6. IEEE. <a href="https://doi.org/10.1109/dac.2018.8465918">https://doi.org/10.1109/dac.2018.8465918</a>.
</div></div></section>
<section id="sec-robust-ai-impact-ml-db72" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-impact-ml-db72">Impact on ML</h4>
<p>Intermittent faults pose significant challenges for ML systems by undermining computational consistency and model reliability. During the training phase, such faults in processing units or memory can cause sporadic errors in the computation of gradients, weight updates, or loss values. These errors may not be persistent but can accumulate across iterations, degrading convergence and leading to unstable or suboptimal models. Intermittent faults in storage may corrupt input data or saved model checkpoints, further affecting the training pipeline <span class="citation" data-cites="he2023understanding">(<a href="#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2023understanding" class="csl-entry" role="listitem">
He, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju, Nishant Patil, and Yanjing Li. 2023. <span>“Understanding and Mitigating Hardware Failures in Deep Learning Training Systems.”</span> In <em>Proceedings of the 50th Annual International Symposium on Computer Architecture</em>, 1–16. IEEE; ACM. <a href="https://doi.org/10.1145/3579371.3589105">https://doi.org/10.1145/3579371.3589105</a>.
</div></div><p>In the inference phase, intermittent faults may result in inconsistent or erroneous predictions. Processing errors or memory corruption can distort activations, outputs, or intermediate representations of the model, particularly when faults affect model parameters or input data. Intermittent faults in data pipelines, such as unreliable sensors or storage systems, can introduce subtle input errors that degrade model robustness and output accuracy. In high-stakes applications like autonomous driving or medical diagnosis, these inconsistencies can result in dangerous decisions or failed operations.</p>
<p>Mitigating the effects of intermittent faults in ML systems requires a multi-layered approach <span class="citation" data-cites="rashid2012intermittent">(<a href="#ref-rashid2012intermittent" role="doc-biblioref">Rashid, Pattabiraman, and Gopalakrishnan 2012</a>)</span>. At the hardware level, robust design practices, environmental controls, and the use of higher-quality or more reliable components can reduce susceptibility to fault conditions. Redundancy and error detection mechanisms can help identify and recover from transient manifestations of intermittent faults.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rashid2012intermittent" class="csl-entry" role="listitem">
Rashid, Layali, Karthik Pattabiraman, and Sathish Gopalakrishnan. 2012. <span>“Intermittent Hardware Errors Recovery: Modeling and Evaluation.”</span> In <em>2012 Ninth International Conference on Quantitative Evaluation of Systems</em>, 220–29. IEEE; IEEE. <a href="https://doi.org/10.1109/qest.2012.37">https://doi.org/10.1109/qest.2012.37</a>.
</div></div><p>At the software level, techniques such as runtime monitoring, anomaly detection, and adaptive control strategies can provide resilience. Data validation checks, outlier detection, model ensembling, and runtime model adaptation are examples of fault-tolerant methods that can be integrated into ML pipelines to improve reliability in the presence of sporadic errors.</p>
<p>Ultimately, designing ML systems that can gracefully handle intermittent faults is essential to maintaining their accuracy, consistency, and dependability. This involves proactive fault detection, regular system monitoring, and ongoing maintenance to ensure early identification and remediation of issues. By embedding resilience into both the architecture and operational workflow, ML systems can remain robust even in environments prone to sporadic hardware failures.</p>
</section>
</section>
<section id="sec-robust-ai-detection-mitigation-10f7" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-detection-mitigation-10f7">Detection and Mitigation</h3>
<p>Various fault detection techniques, including hardware-level and software-level approaches, and effective mitigation strategies can enhance the resilience of ML systems. Additionally, resilient ML system design considerations, case studies and examples, and future research directions in fault-tolerant ML systems provide insights into building robust systems.</p>
<section id="sec-robust-ai-detection-techniques-870b" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-detection-techniques-870b">Detection Techniques</h4>
<p>Fault detection techniques are important for identifying and localizing hardware faults in ML systems. These techniques can be broadly categorized into hardware-level and software-level approaches, each offering unique capabilities and advantages.</p>
<section id="sec-robust-ai-hardwarelevel-detection-aec8" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-robust-ai-hardwarelevel-detection-aec8">Hardware-Level Detection</h5>
<p>Hardware-level fault detection techniques are implemented at the physical level of the system and aim to identify faults in the underlying hardware components. There are several hardware techniques, but broadly, we can bucket these different mechanisms into the following categories.</p>
<section id="sec-robust-ai-builtin-selftest-bist-mechanisms-93ae" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="sec-robust-ai-builtin-selftest-bist-mechanisms-93ae">Built-in self-test (BIST) Mechanisms</h6>
<p>BIST is a powerful technique for detecting faults in hardware components <span class="citation" data-cites="bushnell2002built">(<a href="#ref-bushnell2002built" role="doc-biblioref">Bushnell and Agrawal 2002</a>)</span>. It involves incorporating additional hardware circuitry into the system for self-testing and fault detection. BIST can be applied to various components, such as processors, memory modules, or application-specific integrated circuits (ASICs). For example, BIST can be implemented in a processor using scan chains<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>, which are dedicated paths that allow access to internal registers and logic for testing purposes.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bushnell2002built" class="csl-entry" role="listitem">
Bushnell, Michael L, and Vishwani D Agrawal. 2002. <span>“Built-in Self-Test.”</span> <em>Essentials of Electronic Testing for Digital, Memory and Mixed-Signal VLSI Circuits</em>, 489–548.
</div><div id="fn12"><p><sup>12</sup>&nbsp;<strong>Scan Chains</strong>: Dedicated paths incorporated within a processor that grant access to internal registers and logic for testing.</p></div></div><p>During the BIST process, predefined test patterns are applied to the processor’s internal circuitry, and the responses are compared against expected values. Any discrepancies indicate the presence of faults. Intel’s Xeon processors, for instance, include BIST mechanisms to test the CPU cores, cache memory, and other critical components during system startup.</p>
<div id="fig-parity" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="1540a9fc8a8176bedfc1f1421304ffcfb3408374.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Parity Bit Error Detection: This figure provides a simple error detection scheme where an extra bit (the parity bit) ensures the total number of 1s in a data sequence is either even or odd. The second sequence includes a flipped bit, triggering the parity check and indicating a data corruption event during transmission or storage. Source: computer hope."><img src="robust_ai_files/mediabag/1540a9fc8a8176bedfc1f1421304ffcfb3408374.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>Parity Bit Error Detection</strong>: This figure provides a simple error detection scheme where an extra bit (the parity bit) ensures the total number of 1s in a data sequence is either even or odd. The second sequence includes a flipped bit, triggering the parity check and indicating a data corruption event during transmission or storage. Source: computer hope.
</figcaption>
</figure>
</div>
</section>
<section id="sec-robust-ai-error-detection-codes-6273" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="sec-robust-ai-error-detection-codes-6273">Error Detection Codes</h6>
<p>Error detection codes are widely used to detect data storage and transmission errors <span class="citation" data-cites="hamming1950error">(<a href="#ref-hamming1950error" role="doc-biblioref">Hamming 1950</a>)</span><a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>. These codes add redundant bits to the original data, allowing the detection of bit errors. Example: Parity checks are a simple form of error detection code shown in <a href="#fig-parity" class="quarto-xref">Figure&nbsp;12</a><a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. In a single-bit parity scheme, an extra bit is appended to each data word, making the number of 1s in the word even (even parity) or odd (odd parity).</p>
<div class="no-row-height column-margin column-container"><div id="ref-hamming1950error" class="csl-entry" role="listitem">
Hamming, R. W. 1950. <span>“Error Detecting and Error Correcting Codes.”</span> <em>Bell System Technical Journal</em> 29 (2): 147–60. <a href="https://doi.org/10.1002/j.1538-7305.1950.tb00463.x">https://doi.org/10.1002/j.1538-7305.1950.tb00463.x</a>.
</div><div id="fn13"><p><sup>13</sup>&nbsp;R. W. Hamming’s seminal paper introduced error detection and correction codes, significantly advancing digital communication reliability.</p></div><div id="fn14"><p><sup>14</sup>&nbsp;In parity checks, an extra bit accounts for the total number of 1s in a data word, enabling fundamental error detection.</p></div></div><p>When reading the data, the parity is checked, and if it doesn’t match the expected value, an error is detected. More advanced error detection codes, such as cyclic redundancy checks (CRC), calculate a checksum based on the data and append it to the message. The checksum is recalculated at the receiving end and compared with the transmitted checksum to detect errors. Error-correcting code (ECC) memory modules, commonly used in servers and critical systems, employ advanced error detection and correction codes to detect and correct single-bit or multi-bit errors in memory.</p>
</section>
<section id="sec-robust-ai-hardware-redundancy-voting-mechanisms-1247" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="sec-robust-ai-hardware-redundancy-voting-mechanisms-1247">Hardware redundancy and voting mechanisms</h6>
<p>Hardware redundancy involves duplicating critical components and comparing their outputs to detect and mask faults <span class="citation" data-cites="sheaffer2007hardware">(<a href="#ref-sheaffer2007hardware" role="doc-biblioref">Sheaffer, Luebke, and Skadron 2007</a>)</span>. Voting mechanisms, such as double modular redundancy (DMR)<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> or triple modular redundancy (TMR)<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, employ multiple instances of a component and compare their outputs to identify and mask faulty behavior <span class="citation" data-cites="arifeen2020approximate">(<a href="#ref-arifeen2020approximate" role="doc-biblioref">Arifeen, Hassan, and Lee 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sheaffer2007hardware" class="csl-entry" role="listitem">
Sheaffer, Jeremy W., David P. Luebke, and Kevin Skadron. 2007. <span>“A Hardware Redundancy and Recovery Mechanism for Reliable Scientific Computation on Graphics Processors.”</span> In <em>Graphics Hardware</em>, 2007:55–64. Citeseer. <a href="https://doi.org/10.2312/EGGH/EGGH07/055-064">https://doi.org/10.2312/EGGH/EGGH07/055-064</a>.
</div><div id="fn15"><p><sup>15</sup>&nbsp;<strong>Double Modular Redundancy (DMR)</strong>: A fault-tolerance process in which computations are duplicated to identify and correct errors.</p></div><div id="fn16"><p><sup>16</sup>&nbsp;<strong>Triple Modular Redundancy (TMR)</strong>: A fault-tolerance process where three instances of a computation are performed to identify and correct errors.</p></div><div id="ref-arifeen2020approximate" class="csl-entry" role="listitem">
Arifeen, Tooba, Abdus Sami Hassan, and Jeong-A Lee. 2020. <span>“Approximate Triple Modular Redundancy: A Survey.”</span> <em>IEEE Access</em> 8: 139851–67. <a href="https://doi.org/10.1109/access.2020.3012673">https://doi.org/10.1109/access.2020.3012673</a>.
</div><div id="ref-yeh1996triple" class="csl-entry" role="listitem">
Yeh, Y. C. n.d. <span>“Triple-Triple Redundant 777 Primary Flight Computer.”</span> In <em>1996 IEEE Aerospace Applications Conference. Proceedings</em>, 1:293–307. IEEE; IEEE. <a href="https://doi.org/10.1109/aero.1996.495891">https://doi.org/10.1109/aero.1996.495891</a>.
</div></div><p>In a DMR or TMR system, two or three identical instances of a hardware component, such as a processor or a sensor, perform the same computation in parallel. The outputs of these instances are fed into a voting circuit, which compares the results and selects the majority value as the final output. If one of the instances produces an incorrect result due to a fault, the voting mechanism masks the error and maintains the correct output. TMR is commonly used in aerospace and aviation systems, where high reliability is critical. For instance, the Boeing 777 aircraft employs TMR in its primary flight computer system to ensure the availability and correctness of flight control functions <span class="citation" data-cites="yeh1996triple">(<a href="#ref-yeh1996triple" role="doc-biblioref">Yeh, n.d.</a>)</span>.</p>
<p>Tesla’s self-driving computers, on the other hand, employ a DMR architecture to ensure the safety and reliability of critical functions such as perception, decision-making, and vehicle control, as shown in <a href="#fig-tesla-dmr" class="quarto-xref">Figure&nbsp;13</a>. In Tesla’s implementation, two identical hardware units, often called “redundant computers” or “redundant control units,” perform the same computations in parallel. Each unit independently processes sensor data, executes algorithms, and generates control commands for the vehicle’s actuators, such as steering, acceleration, and braking <span class="citation" data-cites="bannon2019computer">(<a href="#ref-bannon2019computer" role="doc-biblioref">Bannon et al. 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bannon2019computer" class="csl-entry" role="listitem">
Bannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, and Emil Talpes. 2019. <span>“Computer and Redundancy Solution for the Full Self-Driving Computer.”</span> In <em>2019 IEEE Hot Chips 31 Symposium (HCS)</em>, 1–22. IEEE Computer Society; IEEE. <a href="https://doi.org/10.1109/hotchips.2019.8875645">https://doi.org/10.1109/hotchips.2019.8875645</a>.
</div></div><div id="fig-tesla-dmr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tesla-dmr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/tesla_dmr.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: Dual Modular Redundancy: Tesla’s full self-driving computer employs a DMR architecture, replicating critical computations across two independent system-on-chips (socs) to mitigate hardware faults and ensure continuous operation. This redundancy enables the system to mask errors—if one soc fails, the other continues functioning, maintaining safety-critical functions like perception and control. Source: Tesla"><img src="./images/png/tesla_dmr.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tesla-dmr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: <strong>Dual Modular Redundancy</strong>: Tesla’s full self-driving computer employs a DMR architecture, replicating critical computations across two independent system-on-chips (socs) to mitigate hardware faults and ensure continuous operation. This redundancy enables the system to mask errors—if one soc fails, the other continues functioning, maintaining safety-critical functions like perception and control. <em>Source: <a href="HTTPS://old.hotchips.org/hc31/HC31_2.3_tesla_hotchips_ppt_final_0817.PDF">Tesla</a></em>
</figcaption>
</figure>
</div>
<p>The outputs of these two redundant units are continuously compared to detect any discrepancies or faults. If the outputs match, the system assumes that both units function correctly, and the control commands are sent to the vehicle’s actuators. However, if there is a mismatch between the outputs, the system identifies a potential fault in one of the units and takes appropriate action to ensure safe operation.</p>
<p>DMR in Tesla’s self-driving computer provides an extra safety and fault tolerance layer. By having two independent units performing the same computations, the system can detect and mitigate faults that may occur in one of the units. This redundancy helps prevent single points of failure and ensures that critical functions remain operational despite hardware faults.</p>
<p>The system may employ additional mechanisms to determine which unit is faulty in a mismatch. This can involve using diagnostic algorithms, comparing the outputs with data from other sensors or subsystems, or analyzing the consistency of the outputs over time. Once the faulty unit is identified, the system can isolate it and continue operating using the output from the non-faulty unit.</p>
<p>Tesla also incorporates redundancy mechanisms beyond DMR. For example, they use redundant power supplies, steering and braking systems, and diverse sensor suites (e.g., cameras, radar, and ultrasonic sensors) to provide multiple layers of fault tolerance. These redundancies collectively contribute to the overall safety and reliability of the self-driving system.</p>
<p>It’s important to note that while DMR provides fault detection and some level of fault tolerance, TMR may provide a different level of fault masking. In DMR, if both units experience simultaneous faults or the fault affects the comparison mechanism, the system may be unable to identify the fault. Therefore, Tesla’s SDCs rely on a combination of DMR and other redundancy mechanisms to achieve a high level of fault tolerance.</p>
<p>The use of DMR in Tesla’s self-driving computer highlights the importance of hardware redundancy in safety-critical applications. By employing redundant computing units and comparing their outputs, the system can detect and mitigate faults, enhancing the overall safety and reliability of the self-driving functionality.</p>
<p>Another approach to hardware redundancy is the use of hot spares<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>, as employed by Google in its data centers to address SDC during ML training. Unlike DMR and TMR, which rely on parallel processing and voting mechanisms to detect and mask faults, hot spares provide fault tolerance by maintaining backup hardware units that can seamlessly take over computations when a fault is detected. As illustrated in <a href="#fig-sdc-controller" class="quarto-xref">Figure&nbsp;14</a>, during normal ML training, multiple synchronous training workers process data in parallel. However, if a worker becomes defective and causes SDC, an SDC checker automatically identifies the issues. Upon detecting the SDC, the SDC checker moves the training to a hot spare and sends the defective machine for repair. This redundancy safeguards the continuity and reliability of ML training, effectively minimizing downtime and preserving data integrity.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;<strong>Hot Spares</strong>: In a system redundancy design, these are the backup components kept ready to instantaneously replace failing components without disrupting the operation.</p></div></div><div id="fig-sdc-controller" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-controller-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="da436042c94abf8e5710acf600473dab8d009ae2.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;14: Hot Spare Redundancy: Google’s data centers utilize hot spare cores to maintain uninterrupted ML training despite hardware failures, seamlessly transitioning workloads from defective machines to backup resources. This approach contrasts with parallel redundancy techniques like DMR/TMR by providing a reactive fault tolerance mechanism that minimizes downtime and preserves data integrity during ML training. Source: jeff dean, mlsys 2024 keynote (Google)."><img src="robust_ai_files/mediabag/da436042c94abf8e5710acf600473dab8d009ae2.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-controller-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: <strong>Hot Spare Redundancy</strong>: Google’s data centers utilize hot spare cores to maintain uninterrupted ML training despite hardware failures, seamlessly transitioning workloads from defective machines to backup resources. This approach contrasts with parallel redundancy techniques like DMR/TMR by providing a reactive fault tolerance mechanism that minimizes downtime and preserves data integrity during ML training. Source: jeff dean, mlsys 2024 keynote (Google).
</figcaption>
</figure>
</div>
</section>
<section id="sec-robust-ai-watchdog-timers-4d52" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="sec-robust-ai-watchdog-timers-4d52">Watchdog timers</h6>
<p>Watchdog timers are hardware components that monitor the execution of critical tasks or processes <span class="citation" data-cites="pont2002using">(<a href="#ref-pont2002using" role="doc-biblioref">Pont and Ong 2002</a>)</span>. They are commonly used to detect and recover from software or hardware faults that cause a system to become unresponsive or stuck in an infinite loop. In an embedded system, a watchdog timer can be configured to monitor the execution of the main control loop, as illustrated in <a href="#fig-watchdog" class="quarto-xref">Figure&nbsp;15</a>. The software periodically resets the watchdog timer to indicate that it functions correctly. Suppose the software fails to reset the timer within a specified time limit (timeout period). In that case, the watchdog timer assumes that the system has encountered a fault and triggers a predefined recovery action, such as resetting the system or switching to a backup component. Watchdog timers are widely used in automotive electronics, industrial control systems, and other safety-critical applications to ensure the timely detection and recovery from faults.</p>
<div class="no-row-height column-margin column-container"><div id="ref-pont2002using" class="csl-entry" role="listitem">
Pont, Michael J, and Royan HL Ong. 2002. <span>“Using Watchdog Timers to Improve the Reliability of Single-Processor Embedded Systems: Seven New Patterns and a Case Study.”</span> In <em>Proceedings of the First Nordic Conference on Pattern Languages of Programs</em>, 159–200. Citeseer.
</div></div><div id="fig-watchdog" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-watchdog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/watchdog.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;15: Watchdog Timer Operation: Embedded systems utilize watchdog timers to detect and recover from software or hardware faults by periodically resetting a timeout counter; failure to reset within the allotted time triggers a system reset or recovery action, ensuring continued operation. Source: ablic"><img src="./images/png/watchdog.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-watchdog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: <strong>Watchdog Timer Operation</strong>: Embedded systems utilize watchdog timers to detect and recover from software or hardware faults by periodically resetting a timeout counter; failure to reset within the allotted time triggers a system reset or recovery action, ensuring continued operation. Source: <a href="https://www.ablic.com/en/semicon/products/automotive/automotive-watchdog-timer/intro/">ablic</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-robust-ai-softwarelevel-detection-4052" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-robust-ai-softwarelevel-detection-4052">Software-Level Detection</h5>
<p>Software-level fault detection techniques rely on software algorithms and monitoring mechanisms to identify system faults. These techniques can be implemented at various levels of the software stack, including the operating system, middleware, or application level.</p>
<section id="sec-robust-ai-runtime-monitoring-anomaly-detection-2235" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="sec-robust-ai-runtime-monitoring-anomaly-detection-2235">Runtime monitoring and anomaly detection</h6>
<p>Runtime monitoring involves continuously observing the behavior of the system and its components during execution <span class="citation" data-cites="francalanza2017foundation">(<a href="#ref-francalanza2017foundation" role="doc-biblioref">Francalanza et al. 2017</a>)</span>. It helps detect anomalies, errors, or unexpected behavior that may indicate the presence of faults. For example, consider an ML-based image classification system deployed in a self-driving car. Runtime monitoring can be implemented to track the classification model’s performance and behavior <span class="citation" data-cites="mahmoud2021issre">(<a href="#ref-mahmoud2021issre" role="doc-biblioref">Mahmoud et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-francalanza2017foundation" class="csl-entry" role="listitem">
Francalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard, Ian Cassar, Dario Della Monica, and Anna Ingólfsdóttir. 2017. <span>“A Foundation for Runtime Monitoring.”</span> In <em>Runtime Verification</em>, 8–29. Springer; Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-67531-2\_2">https://doi.org/10.1007/978-3-319-67531-2\_2</a>.
</div><div id="ref-mahmoud2021issre" class="csl-entry" role="listitem">
Mahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler. 2021. <span>“Optimizing Selective Protection for CNN Resilience.”</span> In <em>2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE)</em>, 127–38. IEEE. <a href="https://doi.org/10.1109/issre52982.2021.00025">https://doi.org/10.1109/issre52982.2021.00025</a>.
</div><div id="ref-chandola2009anomaly" class="csl-entry" role="listitem">
Chandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. <span>“Anomaly Detection: A Survey.”</span> <em>ACM Computing Surveys</em> 41 (3): 1–58. <a href="https://doi.org/10.1145/1541880.1541882">https://doi.org/10.1145/1541880.1541882</a>.
</div></div><p>Anomaly detection algorithms can be applied to the model’s predictions or intermediate layer activations, such as statistical outlier detection or machine learning-based approaches (e.g., One-Class SVM or Autoencoders) <span class="citation" data-cites="chandola2009anomaly">(<a href="#ref-chandola2009anomaly" role="doc-biblioref">Chandola, Banerjee, and Kumar 2009</a>)</span>. <a href="#fig-ad" class="quarto-xref">Figure&nbsp;16</a> shows example of anomaly detection. Suppose the monitoring system detects a significant deviation from the expected patterns, such as a sudden drop in classification accuracy or out-of-distribution samples. In that case, it can raise an alert indicating a potential fault in the model or the input data pipeline. This early detection allows for timely intervention and fault mitigation strategies to be applied.</p>
<div id="fig-ad" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="63347cecdd15c94b4cf2e0b28455a7b6b3d9c217.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;16: Anomaly Detection With SVM: Support vector machines identify deviations from normal system behavior by mapping log data into a high-dimensional space and defining boundaries around expected values, enabling the detection of potential faults. Unsupervised anomaly detection techniques, like the one shown, are particularly valuable when labeled fault data is scarce, allowing systems to learn patterns from unlabeled operational data. Source: [Google](HTTPS://www.Google.com/url?sa=i&amp;url=HTTP%3A%2F%2fresearch.Google%2fblog%2funsupervised-and-semi-supervised-"><img src="robust_ai_files/mediabag/63347cecdd15c94b4cf2e0b28455a7b6b3d9c217.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: <strong>Anomaly Detection With SVM</strong>: Support vector machines identify deviations from normal system behavior by mapping log data into a high-dimensional space and defining boundaries around expected values, enabling the detection of potential faults. Unsupervised anomaly detection techniques, like the one shown, are particularly valuable when labeled fault data is scarce, allowing systems to learn patterns from unlabeled operational data. Source: [Google](HTTPS://www.Google.com/url?sa=i&amp;url=HTTP%3A%2F%2fresearch.Google%2fblog%2funsupervised-and-semi-supervised-
</figcaption>
</figure>
</div>
</section>
<section id="sec-robust-ai-consistency-checks-data-validation-63f2" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="sec-robust-ai-consistency-checks-data-validation-63f2">Consistency checks and data validation</h6>
<p>Consistency checks and data validation techniques ensure data integrity and correctness at different processing stages in an ML system <span class="citation" data-cites="lindholm2019data">(<a href="#ref-lindholm2019data" role="doc-biblioref">Lindholm et al. 2019</a>)</span>. These checks help detect data corruption, inconsistencies, or errors that may propagate and affect the system’s behavior. Example: In a distributed ML system where multiple nodes collaborate to train a model, consistency checks can be implemented to validate the integrity of the shared model parameters. Each node can compute a checksum or hash of the model parameters before and after the training iteration, as shown in <a href="#fig-ad" class="quarto-xref">Figure&nbsp;16</a>. Any inconsistencies or data corruption can be detected by comparing the checksums across nodes. Additionally, range checks can be applied to the input data and model outputs to ensure they fall within expected bounds. For instance, if an autonomous vehicle’s perception system detects an object with unrealistic dimensions or velocities, it can indicate a fault in the sensor data or the perception algorithms <span class="citation" data-cites="wan2023vpp">(<a href="#ref-wan2023vpp" role="doc-biblioref">Wan et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lindholm2019data" class="csl-entry" role="listitem">
Lindholm, Andreas, Dave Zachariah, Petre Stoica, and Thomas B. Schon. 2019. <span>“Data Consistency Approach to Model Validation.”</span> <em>IEEE Access</em> 7: 59788–96. <a href="https://doi.org/10.1109/access.2019.2915109">https://doi.org/10.1109/access.2019.2915109</a>.
</div><div id="ref-wan2023vpp" class="csl-entry" role="listitem">
Wan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, and Y Zhu. 2023. <span>“Vpp: The Vulnerability-Proportional Protection Paradigm Towards Reliable Autonomous Machines.”</span> In <em>Proceedings of the 5th International Workshop on Domain Specific System Architecture (DOSSA)</em>, 1–6.
</div></div></section>
<section id="sec-robust-ai-heartbeat-timeout-mechanisms-c3a4" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="sec-robust-ai-heartbeat-timeout-mechanisms-c3a4">Heartbeat and timeout mechanisms</h6>
<p>Heartbeat mechanisms and timeouts are commonly used to detect faults in distributed systems and ensure the liveness and responsiveness of components <span class="citation" data-cites="kawazoe1997heartbeat">(<a href="#ref-kawazoe1997heartbeat" role="doc-biblioref">Kawazoe Aguilera, Chen, and Toueg 1997</a>)</span>. These are quite similar to the watchdog timers found in hardware. For example, in a distributed ML system, where multiple nodes collaborate to perform tasks such as data preprocessing, model training, or inference, heartbeat mechanisms can be implemented to monitor the health and availability of each node. Each node periodically sends a heartbeat message to a central coordinator or its peer nodes, indicating its status and availability. Suppose a node fails to send a heartbeat within a specified timeout period, as shown in <a href="#fig-heartbeat" class="quarto-xref">Figure&nbsp;17</a>. In that case, it is considered faulty, and appropriate actions can be taken, such as redistributing the workload or initiating a failover mechanism. Timeouts can also be used to detect and handle hanging or unresponsive components. For example, if a data loading process exceeds a predefined timeout threshold, it may indicate a fault in the data pipeline, and the system can take corrective measures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kawazoe1997heartbeat" class="csl-entry" role="listitem">
Kawazoe Aguilera, Marcos, Wei Chen, and Sam Toueg. 1997. <span>“Heartbeat: A Timeout-Free Failure Detector for Quiescent Reliable Communication.”</span> In <em>Distributed Algorithms</em>, 126–40. Springer; Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/bfb0030680">https://doi.org/10.1007/bfb0030680</a>.
</div></div><div id="fig-heartbeat" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-heartbeat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="52375154b012fc1e9074094af1734087facd410e.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;17: Heartbeat and Timeout: Distributed Systems Employ Periodic Heartbeat Messages to Detect Node Failures; A Lack of Response Within a Defined Timeout Indicates a Fault, Triggering Corrective Actions Like Workload Redistribution or Failover. This Mechanism, Analogous to Watchdog Timers, Ensures System Robustness and Continuous Operation Despite Component Failures. Source: geeksforgeeks."><img src="robust_ai_files/mediabag/52375154b012fc1e9074094af1734087facd410e.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-heartbeat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: <strong>Heartbeat and Timeout</strong>: Distributed Systems Employ Periodic Heartbeat Messages to Detect Node Failures; A Lack of Response Within a Defined Timeout Indicates a Fault, Triggering Corrective Actions Like Workload Redistribution or Failover. This Mechanism, Analogous to Watchdog Timers, Ensures System Robustness and Continuous Operation Despite Component Failures. Source: <a href="HTTPS://www.geeksforgeeks.org/what-are-heartbeat-messages/">geeksforgeeks</a>.
</figcaption>
</figure>
</div>
<!-- @fig-Reed-Solomon Heartbeat messages in distributed systems. Source: [GeeksforGeeks]%(https://www.geeksforgeeks.org/what-are-heartbeat-messages/) -->
</section>
<section id="sec-robust-ai-softwareimplemented-fault-tolerance-sift-techniques-0aa3" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="sec-robust-ai-softwareimplemented-fault-tolerance-sift-techniques-0aa3">Software-implemented fault tolerance (SIFT) techniques</h6>
<p>SIFT techniques introduce redundancy and fault detection mechanisms at the software level to improve the reliability and fault tolerance of the system <span class="citation" data-cites="reis2005swift">(<a href="#ref-reis2005swift" role="doc-biblioref">Reis et al., n.d.</a>)</span>. Example: N-version programming is a SIFT technique where multiple functionally equivalent software component versions are developed independently by different teams. This can be applied to critical components such as the model inference engine in an ML system. Multiple versions of the inference engine can be executed in parallel, and their outputs can be compared for consistency. It is considered the correct result if most versions produce the same output. If there is a discrepancy, it indicates a potential fault in one or more versions, and appropriate error-handling mechanisms can be triggered. Another example is using software-based error correction codes, such as Reed-Solomon codes <span class="citation" data-cites="plank1997tutorial">(<a href="#ref-plank1997tutorial" role="doc-biblioref">Plank 1997</a>)</span>, to detect and correct errors in data storage or transmission, as shown in <a href="#fig-Reed-Solomon" class="quarto-xref">Figure&nbsp;18</a>. These codes add redundancy to the data, enabling detecting and correcting certain errors and enhancing the system’s fault tolerance.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reis2005swift" class="csl-entry" role="listitem">
Reis, G. A., J. Chang, N. Vachharajani, R. Rangan, and D. I. August. n.d. <span>“SWIFT: Software Implemented Fault Tolerance.”</span> In <em>International Symposium on Code Generation and Optimization</em>, 243–54. IEEE; IEEE. <a href="https://doi.org/10.1109/cgo.2005.34">https://doi.org/10.1109/cgo.2005.34</a>.
</div><div id="ref-plank1997tutorial" class="csl-entry" role="listitem">
Plank, James S. 1997. <span>“A Tutorial on Reed-Solomon Coding for Fault-Tolerance in RAID-Like Systems.”</span> <em>Software: Practice and Experience</em> 27 (9): 995–1012. <a href="https://doi.org/10.1002/(sici)1097-024x(199709)27:9<995::aid-spe111>3.0.co;2-6">https://doi.org/10.1002/(sici)1097-024x(199709)27:9&lt;995::aid-spe111&gt;3.0.co;2-6</a>.
</div></div><div id="fig-Reed-Solomon" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-Reed-Solomon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4e97daa824145b42ad25899d35585fbe79f1d9af.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Figure&nbsp;18: Heartbeat Monitoring: Redundant Node Connections and Periodic Heartbeat Messages Detect and Isolate Failing Components in Distributed Systems, Ensuring Continued Operation Despite Hardware Faults. These Mechanisms Enable Fault Tolerance by Allowing Nodes to Identify Unresponsive Peers and Reroute Communication Accordingly. Source: geeksforgeeks."><img src="robust_ai_files/mediabag/4e97daa824145b42ad25899d35585fbe79f1d9af.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-Reed-Solomon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: <strong>Heartbeat Monitoring</strong>: Redundant Node Connections and Periodic Heartbeat Messages Detect and Isolate Failing Components in Distributed Systems, Ensuring Continued Operation Despite Hardware Faults. These Mechanisms Enable Fault Tolerance by Allowing Nodes to Identify Unresponsive Peers and Reroute Communication Accordingly. Source: <a href="HTTPS://www.geeksforgeeks.org/what-is-reed-solomon-code/">geeksforgeeks</a>.
</figcaption>
</figure>
</div>
</section>
</section>
</section>
</section>
<section id="sec-robust-ai-summary-18f0" class="level3">
<h3 class="anchored" data-anchor-id="sec-robust-ai-summary-18f0">Summary</h3>
<p><a href="#tbl-fault_types" class="quarto-xref">Table&nbsp;1</a> provides a comparative analysis of transient, permanent, and intermittent faults. It outlines the primary characteristics or dimensions that distinguish these fault types. Here, we summarize the relevant dimensions we examined and explore the nuances that differentiate transient, permanent, and intermittent faults in greater detail.</p>
<div id="tbl-fault_types" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-fault_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Fault Characteristics</strong>: Transient, permanent, and intermittent faults differ by duration, persistence, and recurrence, impacting system reliability and requiring distinct mitigation strategies for robust AI deployments. Understanding these distinctions guides the design of fault-tolerant systems capable of handling diverse hardware failures during operation.
</figcaption>
<div aria-describedby="tbl-fault_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Dimension</th>
<th style="text-align: left;">Transient Faults</th>
<th style="text-align: left;">Permanent Faults</th>
<th style="text-align: left;">Intermittent Faults</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Duration</td>
<td style="text-align: left;">Short-lived, temporary</td>
<td style="text-align: left;">Persistent, remains until repair or replacement</td>
<td style="text-align: left;">Sporadic, appears and disappears intermittently</td>
</tr>
<tr class="even">
<td style="text-align: left;">Persistence</td>
<td style="text-align: left;">Disappears after the fault condition passes</td>
<td style="text-align: left;">Consistently present until addressed</td>
<td style="text-align: left;">Recurs irregularly, not always present</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Causes</td>
<td style="text-align: left;">External factors (e.g., electromagnetic interference cosmic rays)</td>
<td style="text-align: left;">Hardware defects, physical damage, wear-out</td>
<td style="text-align: left;">Unstable hardware conditions, loose connections, aging components</td>
</tr>
<tr class="even">
<td style="text-align: left;">Manifestation</td>
<td style="text-align: left;">Bit flips, glitches, temporary data corruption</td>
<td style="text-align: left;">Stuck-at faults, broken components, complete device failures</td>
<td style="text-align: left;">Occasional bit flips, intermittent signal issues, sporadic malfunctions</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Impact on ML Systems</td>
<td style="text-align: left;">Introduces temporary errors or noise in computations</td>
<td style="text-align: left;">Causes consistent errors or failures, affecting reliability</td>
<td style="text-align: left;">Leads to sporadic and unpredictable errors, challenging to diagnose and mitigate</td>
</tr>
<tr class="even">
<td style="text-align: left;">Detection</td>
<td style="text-align: left;">Error detection codes, comparison with expected values</td>
<td style="text-align: left;">Built-in self-tests, error detection codes, consistency checks</td>
<td style="text-align: left;">Monitoring for anomalies, analyzing error patterns and correlations</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mitigation</td>
<td style="text-align: left;">Error correction codes, redundancy, checkpoint and restart</td>
<td style="text-align: left;">Hardware repair or replacement, component redundancy, failover mechanisms</td>
<td style="text-align: left;">Robust design, environmental control, runtime monitoring, fault-tolerant techniques</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="quiz-question-sec-robust-ai-hardware-faults-81ee" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which type of hardware fault is characterized by its sporadic and unpredictable nature, making it difficult to diagnose?</p>
<ol type="a">
<li>Transient Fault</li>
<li>Permanent Fault</li>
<li>Intermittent Fault</li>
<li>Logical Fault</li>
</ol></li>
<li><p>Explain how transient faults can affect the training phase of a machine learning model.</p></li>
<li><p>In ML systems, a common detection technique for hardware faults is the use of ____ codes, which add redundant bits to detect errors.</p></li>
<li><p>True or False: Permanent faults in ML systems can be mitigated by using error correction codes alone.</p></li>
<li><p>Order the following steps in handling a detected hardware fault in an ML system: [Implement redundancy, Monitor system, Detect fault, Mitigate fault].</p></li>
</ol>
<p><a href="#quiz-answer-sec-robust-ai-hardware-faults-81ee" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-robust-ai-model-robustness-f537" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-robust-ai-model-robustness-f537">Model Robustness</h2>
<section id="sec-robust-ai-adversarial-attacks-f700" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-adversarial-attacks-f700">Adversarial Attacks</h3>
<p>We first introduced adversarial attacks when discussing how slight changes to input data can trick a model into making incorrect predictions. These attacks often involve adding small, carefully designed perturbations to input data, which can cause the model to misclassify it, as shown in <a href="#fig-adversarial-attack-noise-example" class="quarto-xref">Figure&nbsp;19</a>. In this section, we will look at the different types of adversarial attacks and their impact on machine learning models. Understanding these attacks highlights why it is important to build models that are robust and able to handle these kinds of challenges.</p>
<div id="fig-adversarial-attack-noise-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-attack-noise-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/adversarial_attack_detection.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Figure&nbsp;19: Adversarial Perturbation: Subtle, Intentionally Crafted Noise Can Cause Neural Networks to Misclassify Images With High Confidence, Exposing a Vulnerability in Model Robustness. These Perturbations, Imperceptible to Humans, Alter the Input in a Way That Maximizes Prediction Error, Highlighting the Need for Defenses Against Adversarial Attacks. Source: Sutanto (2019)."><img src="./images/png/adversarial_attack_detection.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-attack-noise-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: <strong>Adversarial Perturbation</strong>: Subtle, Intentionally Crafted Noise Can Cause Neural Networks to Misclassify Images With High Confidence, Exposing a Vulnerability in Model Robustness. These Perturbations, Imperceptible to Humans, Alter the Input in a Way That Maximizes Prediction Error, Highlighting the Need for Defenses Against Adversarial Attacks. Source: Sutanto (2019).
</figcaption>
</figure>
</div>
<section id="sec-robust-ai-mechanisms-77b4" class="level4">
<h4 class="anchored" data-anchor-id="sec-robust-ai-mechanisms-77b4">Mechanisms</h4>
<section id="sec-robust-ai-gradientbased-attacks-4c84" class="level5">
<h5 class="anchored" data-anchor-id="sec-robust-ai-gradientbased-attacks-4c84">Gradient-based Attacks</h5>
<p>One prominent category of adversarial attacks is gradient-based attacks. These attacks leverage the gradients of the ML model’s loss function to craft adversarial examples. The <a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">Fast Gradient Sign Method</a> (FGSM) is a well-known technique in this category. FGSM perturbs the input data by adding small noise in the direction of the gradient of the loss with respect to the input. The goal is to maximize the model’s prediction error with minimal distortion to the original input.</p>
<p>The adversarial example is generated using the following formula: <span class="math display">\[
x_{\text{adv}} = x + \epsilon \cdot \text{sign}\big(\nabla_x J(\theta, x, y)\big)
\]</span> Where:</p>
<ul>
<li><span class="math inline">\(x\)</span> is the original input,</li>
<li><span class="math inline">\(y\)</span> is the true label,</li>
<li><span class="math inline">\(\theta\)</span> represents the model parameters,</li>
<li><span class="math inline">\(J(\theta, x, y)\)</span> is the loss function,</li>
<li><span class="math inline">\(\epsilon\)</span> is a small scalar that controls the magnitude of the perturbation.</li>
</ul>
<p>This method allows for fast and efficient generation of adversarial examples by taking a single step in the direction that increases the loss most rapidly, as shown in <a href="#fig-gradient-attack" class="quarto-xref">Figure&nbsp;20</a>.</p>
<div id="fig-gradient-attack" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-attack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/gradient_attack.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="Figure&nbsp;20: Adversarial Perturbations: Gradient-based attacks generate subtle, intentionally crafted input noise – with magnitude controlled by \epsilon – that maximizes the loss function j(\theta, x, y) and causes misclassification by the model. These perturbations, imperceptible to humans, exploit model vulnerabilities by moving the input x across the decision boundary. Source: ivezic"><img src="./images/png/gradient_attack.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-attack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: <strong>Adversarial Perturbations</strong>: Gradient-based attacks generate subtle, intentionally crafted input noise – with magnitude controlled by <span class="math inline">\(\epsilon\)</span> – that maximizes the loss function <span class="math inline">\(j(\theta, x, y)\)</span> and causes misclassification by the model. These perturbations, imperceptible to humans, exploit model vulnerabilities by moving the input <span class="math inline">\(x\)</span> across the decision boundary. Source: <a href="HTTPS://defence.AI/AI-security/gradient-based-attacks/">ivezic</a>
</figcaption>
</figure>
</div>
<p>Another variant, the Projected Gradient Descent (PGD) attack, extends FGSM by iteratively applying the gradient update step, allowing for more refined and powerful adversarial examples. PGD projects each perturbation step back into a constrained norm ball around the original input, ensuring that the adversarial example remains within a specified distortion limit. This makes PGD a stronger white-box attack and a benchmark for evaluating model robustness.</p>
<p>The Jacobian-based Saliency Map Attack (JSMA) is another gradient-based approach that identifies the most influential input features and perturbs them to create adversarial examples. By constructing a saliency map based on the Jacobian of the model’s outputs with respect to inputs, JSMA selectively alters a small number of input dimensions that are most likely to influence the target class. This makes JSMA more precise and targeted than FGSM or PGD, often requiring fewer perturbations to fool the model.</p>
<p>Gradient-based attacks are particularly effective in white-box settings, where the attacker has access to the model’s architecture and gradients. Their efficiency and relative simplicity have made them popular tools for both attacking and evaluating model robustness in research.</p>
</section>
<section id="sec-robust-ai-optimizationbased-attacks-a1e1" class="level5">
<h5 class="anchored" data-anchor-id="sec-robust-ai-optimizationbased-attacks-a1e1">Optimization-based Attacks</h5>
<p>These attacks formulate the generation of adversarial examples as an optimization problem. The Carlini and Wagner (C&amp;W) attack is a prominent example in this category. It finds the smallest perturbation that can cause misclassification while maintaining the perceptual similarity to the original input. The C&amp;W attack employs an iterative optimization process to minimize the perturbation while maximizing the model’s prediction error. It uses a customized loss function with a confidence term to generate more confident misclassifications.</p>
<p>C&amp;W attacks are especially difficult to detect because the perturbations are typically imperceptible to humans, and they often bypass many existing defenses. The attack can be formulated under various norm constraints (e.g., L2, L∞) depending on the desired properties of the adversarial perturbation.</p>
<p>Another optimization-based approach is the Elastic Net Attack to DNNs (EAD), which incorporates elastic net regularization (a combination of L1 and L2 penalties) to generate adversarial examples with sparse perturbations. This can lead to minimal and localized changes in the input, which are harder to identify and filter. EAD is particularly useful in settings where perturbations need to be constrained in both magnitude and spatial extent.</p>
<p>These attacks are more computationally intensive than gradient-based methods but offer finer control over the adversarial example’s properties. They are often used in high-stakes domains where stealth and precision are critical.</p>
</section>
<section id="sec-robust-ai-transferbased-attacks-a420" class="level5">
<h5 class="anchored" data-anchor-id="sec-robust-ai-transferbased-attacks-a420">Transfer-based Attacks</h5>
<p>Transfer-based attacks exploit the transferability property of adversarial examples. Transferability refers to the phenomenon where adversarial examples crafted for one ML model can often fool other models, even if they have different architectures or were trained on different datasets. This enables attackers to generate adversarial examples using a surrogate model and then transfer them to the target model without requiring direct access to its parameters or gradients.</p>
<p>This property underlies the feasibility of black-box attacks, where the adversary cannot query gradients but can still fool a model by crafting attacks on a publicly available or similar substitute model. Transfer-based attacks are particularly relevant in practical threat scenarios, such as attacking commercial ML APIs, where the attacker can observe inputs and outputs but not internal computations.</p>
<p>Attack success often depends on factors like similarity between models, alignment in training data, and the regularization techniques used. Techniques like input diversity (random resizing, cropping) and momentum during optimization can be used to increase transferability.</p>
</section>
<section id="sec-robust-ai-physicalworld-attacks-768a" class="level5">
<h5 class="anchored" data-anchor-id="sec-robust-ai-physicalworld-attacks-768a">Physical-world Attacks</h5>
<p>Physical-world attacks bring adversarial examples into the realm of real-world scenarios. These attacks involve creating physical objects or manipulations that can deceive ML models when captured by sensors or cameras. Adversarial patches, for example, are small, carefully designed patterns that can be placed on objects to fool object detection or classification models. These patches are designed to work under varying lighting conditions, viewing angles, and distances, making them robust in real-world environments.</p>
<p>When attached to real-world objects, such as a stop sign or a piece of clothing, these patches can cause models to misclassify or fail to detect the objects accurately. Notably, the effectiveness of these attacks persists even after being printed out and viewed through a camera lens, bridging the digital and physical divide in adversarial ML.</p>
<p>Adversarial objects, such as 3D-printed sculptures or modified road signs, can also be crafted to deceive ML systems in physical environments. For example, a 3D turtle object was shown to be consistently classified as a rifle by an image classifier, even when viewed from different angles. These attacks underscore the risks facing AI systems deployed in physical spaces, such as autonomous vehicles, drones, and surveillance systems.</p>
<p>Research into physical-world attacks also includes efforts to develop universal adversarial perturbations—perturbations that can fool a wide range of inputs and models. These threats raise serious questions about safety, robustness, and generalization in AI systems.</p>
</section>
<section id="sec-robust-ai-summary-75fc" class="level5">
<h5 class="anchored" data-anchor-id="sec-robust-ai-summary-75fc">Summary</h5>
<p><a href="#tbl-attack_types" class="quarto-xref">Table&nbsp;2</a> provides a concise overview of the different categories of adversarial attacks, including gradient-based attacks (FGSM, PGD, JSMA), optimization-based attacks (C&amp;W, EAD), transfer-based attacks, and physical-world attacks (adversarial patches and objects). Each attack is briefly described, highlighting its key characteristics and mechanisms.</p>
<div id="tbl-attack_types" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-attack_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Adversarial Attack Categories</strong>: Machine learning model robustness relies on defending against attacks that intentionally perturb input data to cause misclassification; this table categorizes these attacks by their underlying mechanism, including gradient-based, optimization-based, transfer-based, and physical-world approaches, each exploiting different model vulnerabilities. Understanding these categories is crucial for developing effective defense strategies and evaluating model security.
</figcaption>
<div aria-describedby="tbl-attack_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 25%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Attack Category</th>
<th style="text-align: left;">Attack Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Gradient-based</td>
<td style="text-align: left;">Fast Gradient Sign Method (FGSM) Projected Gradient Descent (PGD) Jacobian-based Saliency Map Attack (JSMA)</td>
<td style="text-align: left;">Perturbs input data by adding small noise in the gradient direction to maximize prediction error. Extends FGSM by iteratively applying the gradient update step for more refined adversarial examples. Identifies influential input features and perturbs them to create adversarial examples.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Optimization-based</td>
<td style="text-align: left;">Carlini and Wagner (C&amp;W) Attack Elastic Net Attack to DNNs (EAD)</td>
<td style="text-align: left;">Finds the smallest perturbation that causes misclassification while maintaining perceptual similarity. Incorporates elastic net regularization to generate adversarial examples with sparse perturbations.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Transfer-based</td>
<td style="text-align: left;">Transferability-based Attacks</td>
<td style="text-align: left;">Exploits the transferability of adversarial examples across different models, enabling black-box attacks.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Physical-world</td>
<td style="text-align: left;">Adversarial Patches Adversarial Objects</td>
<td style="text-align: left;">Small, carefully designed patches placed on objects to fool object detection or classification models. Physical objects (e.g., 3D-printed sculptures, modified road signs) crafted to deceive ML systems in real-world scenarios.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The mechanisms of adversarial attacks reveal the intricate interplay between the ML model’s decision boundaries, the input data, and the attacker’s objectives. By carefully manipulating the input data, attackers can exploit the model’s sensitivities and blind spots, leading to incorrect predictions. The success of adversarial attacks highlights the need for a deeper understanding of ML models’ robustness and generalization properties.</p>
<p>Defending against adversarial attacks requires a multifaceted approach. Adversarial training is one common defense strategy in which models are trained on adversarial examples to improve robustness. Exposing the model to adversarial examples during training teaches it to classify them correctly and become more resilient to attacks. Defensive distillation, input preprocessing, and ensemble methods are other techniques that can help mitigate the impact of adversarial attacks.</p>
<p>As adversarial machine learning evolves, researchers explore new attack mechanisms and develop more sophisticated defenses. The arms race between attackers and defenders drives the need for constant innovation and vigilance in securing ML systems against adversarial threats. Understanding the mechanisms of adversarial attacks is crucial for developing robust and reliable ML models that can withstand the ever-evolving landscape of adversarial examples.</p>
</section>
</section>
<section id="sec-robust-ai-impact-ml-d199" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-impact-ml-d199">Impact on ML</h4>
<p>Adversarial attacks on machine learning systems have emerged as a significant concern in recent years, highlighting the potential vulnerabilities and risks associated with the widespread adoption of ML technologies. These attacks involve carefully crafted perturbations to input data that can deceive or mislead ML models, leading to incorrect predictions or misclassifications, as shown in <a href="#fig-adversarial-googlenet" class="quarto-xref">Figure&nbsp;21</a>. The impact of adversarial attacks on ML systems is far-reaching and can have serious consequences in various domains.</p>
<div id="fig-adversarial-googlenet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-googlenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/adversarial_googlenet.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="Figure&nbsp;21: Adversarial Perturbations: Subtle, intentionally crafted noise added to an image can cause a trained deep neural network (googlenet) to misclassify it, even though the perturbed image remains visually indistinguishable to humans. This vulnerability underscores the lack of robustness in many machine learning models and motivates research into adversarial training and defense mechanisms. Source: goodfellow et al., 2014."><img src="./images/png/adversarial_googlenet.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-googlenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: <strong>Adversarial Perturbations</strong>: Subtle, intentionally crafted noise added to an image can cause a trained deep neural network (googlenet) to misclassify it, even though the perturbed image remains visually indistinguishable to humans. This vulnerability underscores the lack of robustness in many machine learning models and motivates research into adversarial training and defense mechanisms. Source: goodfellow et al., 2014.
</figcaption>
</figure>
</div>
<p>One striking example of the impact of adversarial attacks was demonstrated by researchers in 2017. They experimented with small black and white stickers on stop signs <span class="citation" data-cites="eykholt2018robust">(<a href="#ref-eykholt2018robust" role="doc-biblioref">Eykholt et al. 2017</a>)</span>. To the human eye, these stickers did not obscure the sign or prevent its interpretability. However, when images of the sticker-modified stop signs were fed into standard traffic sign classification ML models, a shocking result emerged. The models misclassified the stop signs as speed limit signs over 85% of the time.</p>
<div class="no-row-height column-margin column-container"><div id="ref-eykholt2018robust" class="csl-entry" role="listitem">
Eykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017. <span>“Robust Physical-World Attacks on Deep Learning Models.”</span> <em>ArXiv Preprint</em> abs/1707.08945 (July). <a href="http://arxiv.org/abs/1707.08945v5">http://arxiv.org/abs/1707.08945v5</a>.
</div></div><p>This demonstration shed light on the alarming potential of simple adversarial stickers to trick ML systems into misreading critical road signs. The implications of such attacks in the real world are significant, particularly in the context of autonomous vehicles. If deployed on actual roads, these adversarial stickers could cause self-driving cars to misinterpret stop signs as speed limits, leading to dangerous situations, as shown in <a href="#fig-graffiti" class="quarto-xref">Figure&nbsp;22</a>. Researchers warned that this could result in rolling stops or unintended acceleration into intersections, endangering public safety.</p>
<div id="fig-graffiti" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graffiti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/graffiti.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="Figure&nbsp;22: Adversarial Perturbation: Subtle, physically realizable modifications to input data can cause machine learning models to make incorrect predictions, even when imperceptible to humans. This example shows how small stickers on a stop sign caused a traffic sign classifier to misidentify it as a 45 mph speed limit sign with over 85% accuracy, highlighting the vulnerability of ML systems to adversarial attacks. Source: eykholt"><img src="./images/png/graffiti.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graffiti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: <strong>Adversarial Perturbation</strong>: Subtle, physically realizable modifications to input data can cause machine learning models to make incorrect predictions, even when imperceptible to humans. This example shows how small stickers on a stop sign caused a traffic sign classifier to misidentify it as a 45 mph speed limit sign with over 85% accuracy, highlighting the vulnerability of ML systems to adversarial attacks. Source: <a href="https://arxiv.org/abs/1707.08945">eykholt</a>
</figcaption>
</figure>
</div>
<p>The case study of the adversarial stickers on stop signs provides a concrete illustration of how adversarial examples exploit how ML models recognize patterns. By subtly manipulating the input data in ways that are invisible to humans, attackers can induce incorrect predictions and create serious risks, especially in safety-critical applications like autonomous vehicles. The attack’s simplicity highlights the vulnerability of ML models to even minor changes in the input, emphasizing the need for robust defenses against such threats.</p>
<p>The impact of adversarial attacks extends beyond the degradation of model performance. These attacks raise significant security and safety concerns, particularly in domains where ML models are relied upon for critical decision-making. In healthcare applications, adversarial attacks on medical imaging models could lead to misdiagnosis or incorrect treatment recommendations, jeopardizing patient well-being <span class="citation" data-cites="tsai2023adversarial">(<a href="#ref-tsai2023adversarial" role="doc-biblioref">M.-J. Tsai, Lin, and Lee 2023</a>)</span>. In financial systems, adversarial attacks could enable fraud or manipulation of trading algorithms, resulting in substantial economic losses.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tsai2023adversarial" class="csl-entry" role="listitem">
Tsai, Min-Jen, Ping-Yi Lin, and Ming-En Lee. 2023. <span>“Adversarial Attacks on Medical Image Classification.”</span> <em>Cancers</em> 15 (17): 4228. <a href="https://doi.org/10.3390/cancers15174228">https://doi.org/10.3390/cancers15174228</a>.
</div><div id="ref-fursov2021adversarial" class="csl-entry" role="listitem">
Fursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, and Evgeny Burnaev. 2021. <span>“Adversarial Attacks on Deep Models for Financial Transaction Records.”</span> In <em>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp;Amp; Data Mining</em>, 2868–78. ACM. <a href="https://doi.org/10.1145/3447548.3467145">https://doi.org/10.1145/3447548.3467145</a>.
</div></div><p>Moreover, adversarial vulnerabilities undermine the trustworthiness and interpretability of ML models. If carefully crafted perturbations can easily fool models, confidence in their predictions and decisions erodes. Adversarial examples expose the models’ reliance on superficial patterns and the inability to capture the true underlying concepts, challenging the reliability of ML systems <span class="citation" data-cites="fursov2021adversarial">(<a href="#ref-fursov2021adversarial" role="doc-biblioref">Fursov et al. 2021</a>)</span>.</p>
<p>Defending against adversarial attacks often requires additional computational resources and can impact the overall system performance. Techniques like adversarial training, where models are trained on adversarial examples to improve robustness, can significantly increase training time and computational requirements <span class="citation" data-cites="bai2021recent">(<a href="#ref-bai2021recent" role="doc-biblioref">Bai et al. 2021</a>)</span>. Runtime detection and mitigation mechanisms, such as input preprocessing <span class="citation" data-cites="addepalli2020towards">(<a href="#ref-addepalli2020towards" role="doc-biblioref">Addepalli et al. 2020</a>)</span> or prediction consistency checks, introduce latency and affect the real-time performance of ML systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bai2021recent" class="csl-entry" role="listitem">
Bai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021. <span>“Recent Advances in Adversarial Training for Adversarial Robustness.”</span> <em>arXiv Preprint arXiv:2102.01356</em>, February. <a href="http://arxiv.org/abs/2102.01356v5">http://arxiv.org/abs/2102.01356v5</a>.
</div><div id="ref-addepalli2020towards" class="csl-entry" role="listitem">
Addepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, and R. Venkatesh Babu. 2020. <span>“Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes.”</span> In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 1020–29. IEEE. <a href="https://doi.org/10.1109/cvpr42600.2020.00110">https://doi.org/10.1109/cvpr42600.2020.00110</a>.
</div></div><p>The presence of adversarial vulnerabilities also complicates the deployment and maintenance of ML systems. System designers and operators must consider the potential for adversarial attacks and incorporate appropriate defenses and monitoring mechanisms. Regular updates and retraining of models become necessary to adapt to new adversarial techniques and maintain system security and performance over time.</p>
<p>The impact of adversarial attacks on ML systems is significant and multifaceted. These attacks expose ML models’ vulnerabilities, from degrading model performance and raising security and safety concerns to challenging model trustworthiness and interpretability. Developers and researchers must prioritize the development of robust defenses and countermeasures to mitigate the risks posed by adversarial attacks. By addressing these challenges, we can build more secure, reliable, and trustworthy ML systems that can withstand the ever-evolving landscape of adversarial threats.</p>
</section>
</section>
<section id="sec-robust-ai-data-poisoning-2769" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-data-poisoning-2769">Data Poisoning</h3>
<p>Data poisoning presents a critical challenge to the integrity and reliability of machine learning systems. By introducing carefully crafted malicious data into the training pipeline, adversaries can subtly manipulate model behavior in ways that are difficult to detect through standard validation procedures. Unlike adversarial examples, which target models at inference time, poisoning attacks exploit upstream components of the system—such as data collection, labeling, or ingestion. As ML systems are increasingly deployed in automated and high-stakes environments, understanding how poisoning occurs and how it propagates through the system is essential for developing effective defenses.</p>
<section id="sec-robust-ai-characteristics-3c33" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-characteristics-3c33">Characteristics</h4>
<p>Data poisoning is an attack in which the training data is deliberately manipulated to compromise the performance or behavior of a machine learning model, as described in <span class="citation" data-cites="biggio2012poisoning">(<a href="#ref-biggio2012poisoning" role="doc-biblioref">Biggio, Nelson, and Laskov 2012</a>)</span> and illustrated in <a href="#fig-dirty-label-example" class="quarto-xref">Figure&nbsp;23</a>. Attackers may alter existing training samples, introduce malicious examples, or interfere with the data collection pipeline. The result is a model that learns biased, inaccurate, or exploitable patterns.</p>
<div class="no-row-height column-margin column-container"><div id="ref-biggio2012poisoning" class="csl-entry" role="listitem">
Biggio, Battista, Blaine Nelson, and Pavel Laskov. 2012. <span>“Poisoning Attacks Against Support Vector Machines.”</span> In <em>Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012</em>. icml.cc / Omnipress. <a href="http://icml.cc/2012/papers/880.pdf">http://icml.cc/2012/papers/880.pdf</a>.
</div></div><div id="fig-dirty-label-example" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-dirty-label-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/dirty_label_example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23" title="Figure&nbsp;23: Data Poisoning Examples: Mismatched image-text pairs represent a common data poisoning attack, where manipulated training data causes models to misclassify inputs. These adversarial examples can compromise model integrity and introduce vulnerabilities in real-world applications. Source: [@shan2023prompt]."><img src="./images/png/dirty_label_example.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dirty-label-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: <strong>Data Poisoning Examples</strong>: Mismatched image-text pairs represent a common data poisoning attack, where manipulated training data causes models to misclassify inputs. These adversarial examples can compromise model integrity and introduce vulnerabilities in real-world applications. Source: <span class="citation" data-cites="shan2023prompt">(<a href="#ref-shan2023prompt" role="doc-biblioref">Shan et al. 2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>In most cases, data poisoning unfolds in three stages.</p>
<p>In the injection stage, the attacker introduces poisoned samples into the training dataset. These samples may be altered versions of existing data or entirely new instances designed to blend in with clean examples. While they appear benign on the surface, these inputs are engineered to influence model behavior in subtle but deliberate ways. The attacker may target specific classes, insert malicious triggers, or craft outliers intended to distort the decision boundary.</p>
<p>During the training phase, the machine learning model incorporates the poisoned data and learns spurious or misleading patterns. These learned associations may bias the model toward incorrect classifications, introduce vulnerabilities, or embed backdoors. Because the poisoned data is often statistically similar to clean data, the corruption process typically goes unnoticed during standard model training and evaluation.</p>
<p>Finally, in the deployment stage, the attacker leverages the compromised model for malicious purposes. This could involve triggering specific behaviors, including the misclassification of an input that contains a hidden pattern, or simply exploiting the model’s degraded accuracy in production. In real-world systems, such attacks can be difficult to trace back to training data, especially if the system’s behavior appears erratic only in edge cases or under adversarial conditions.</p>
<p>The consequences of such manipulation are especially severe in high-stakes domains like healthcare, where even small disruptions to training data can lead to dangerous misdiagnoses or loss of trust in AI-based systems <span class="citation" data-cites="marulli2022sensitivity">(<a href="#ref-marulli2022sensitivity" role="doc-biblioref">Marulli, Marrone, and Verde 2022</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-marulli2022sensitivity" class="csl-entry" role="listitem">
Marulli, Fiammetta, Stefano Marrone, and Laura Verde. 2022. <span>“Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain.”</span> <em>Journal of Sensor and Actuator Networks</em> 11 (2): 21. <a href="https://doi.org/10.3390/jsan11020021">https://doi.org/10.3390/jsan11020021</a>.
</div><div id="ref-oprea2022poisoning" class="csl-entry" role="listitem">
Oprea, Alina, Anoop Singhal, and Apostol Vassilev. 2022. <span>“Poisoning Attacks Against Machine Learning: Can Machine Learning Be Trustworthy?”</span> <em>Computer</em> 55 (11): 94–99. <a href="https://doi.org/10.1109/mc.2022.3190787">https://doi.org/10.1109/mc.2022.3190787</a>.
</div></div><p>Four main categories of poisoning attacks have been identified in the literature <span class="citation" data-cites="oprea2022poisoning">(<a href="#ref-oprea2022poisoning" role="doc-biblioref">Oprea, Singhal, and Vassilev 2022</a>)</span>. In availability attacks, a substantial portion of the training data is poisoned with the aim of degrading overall model performance. A classic example involves flipping labels—for instance, systematically changing instances with true label <span class="math inline">\(y = 1\)</span> to <span class="math inline">\(y = 0\)</span> in a binary classification task. These attacks render the model unreliable across a wide range of inputs, effectively making it unusable.</p>
<p>In contrast, targeted poisoning attacks aim to compromise only specific classes or instances. Here, the attacker modifies just enough data to cause a small set of inputs to be misclassified, while overall accuracy remains relatively stable. This subtlety makes targeted attacks especially hard to detect.</p>
<p>Backdoor poisoning introduces hidden triggers into training data—subtle patterns or features that the model learns to associate with a particular output. When the trigger appears at inference time, the model is manipulated into producing a predetermined response. These attacks are often effective even if the trigger pattern is imperceptible to human observers.</p>
<p>Subpopulation poisoning focuses on compromising a specific subset of the data population. While similar in intent to targeted attacks, subpopulation poisoning applies availability-style degradation to a localized group, for example, a particular demographic or feature cluster, while leaving the rest of the model’s performance intact. This distinction makes such attacks both highly effective and especially dangerous in fairness-sensitive applications.</p>
<p>A common thread across these poisoning strategies is their subtlety. Manipulated samples are typically indistinguishable from clean data, making them difficult to identify through casual inspection or standard data validation. These manipulations might involve small changes to numeric values, slight label inconsistencies, or embedded visual patterns—each designed to blend into the data distribution while still affecting model behavior.</p>
<p>Such attacks may be carried out by internal actors, like data engineers or annotators with privileged access, or by external adversaries who exploit weak points in the data collection pipeline. In crowdsourced environments or open data collection scenarios, poisoning can be as simple as injecting malicious samples into a shared dataset or influencing user-generated content.</p>
<p>Crucially, poisoning attacks often target the early stages of the ML pipeline, such as collection and preprocessing, where there may be limited oversight. If data is pulled from unverified sources or lacks strong validation protocols, attackers can slip in poisoned data that appears statistically normal. The absence of integrity checks, robust outlier detection, or lineage tracking only heightens the risk.</p>
<p>Ultimately, the goal of these attacks is to corrupt the learning process itself. A model trained on poisoned data may learn spurious correlations, overfit to false signals, or become vulnerable to highly specific exploit conditions. Whether the result is a degraded model or one with a hidden exploit path, the trustworthiness and safety of the system are fundamentally compromised.</p>
</section>
<section id="sec-robust-ai-mechanisms-043d" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-mechanisms-043d">Mechanisms</h4>
<p>Data poisoning can be implemented through a variety of mechanisms, depending on the attacker’s access to the system and understanding of the data pipeline. These mechanisms reflect different strategies for how the training data can be corrupted to achieve malicious outcomes.</p>
<p>One of the most direct approaches involves modifying the labels of training data. In this method, an attacker selects a subset of training samples and alters their labels—flipping <span class="math inline">\(y = 1\)</span> to <span class="math inline">\(y = 0\)</span>, or reassigning categories in multi-class settings. As shown in <a href="#fig-distribution-shift-example" class="quarto-xref">Figure&nbsp;24</a>, even small-scale label inconsistencies can lead to significant distributional shifts and learning disruptions.</p>
<div id="fig-distribution-shift-example" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-distribution-shift-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/distribution_shift_example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24" title="Figure&nbsp;24: Data Poisoning Impact: Subtle perturbations to training data labels can induce significant distributional shifts, leading to model inaccuracies and compromised performance in machine learning systems. These shifts exemplify how even limited adversarial control over training data can disrupt model learning and highlight the vulnerability of data-driven approaches to malicious manipulation. Source: [@shan2023prompt]."><img src="./images/png/distribution_shift_example.png" class="img-fluid figure-img" style="width:65.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distribution-shift-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: <strong>Data Poisoning Impact</strong>: Subtle perturbations to training data labels can induce significant distributional shifts, leading to model inaccuracies and compromised performance in machine learning systems. These shifts exemplify how even limited adversarial control over training data can disrupt model learning and highlight the vulnerability of data-driven approaches to malicious manipulation. Source: <span class="citation" data-cites="shan2023prompt">(<a href="#ref-shan2023prompt" role="doc-biblioref">Shan et al. 2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>Another mechanism involves modifying the input features of training examples without changing the labels. This might include imperceptible pixel-level changes in images, subtle perturbations in structured data, or embedding fixed patterns that act as triggers for backdoor attacks. These alterations are often designed using optimization techniques that maximize their influence on the model while minimizing detectability.</p>
<p>More sophisticated attacks generate entirely new, malicious training examples. These synthetic samples may be created using adversarial methods, generative models, or even data synthesis tools. The aim is to carefully craft inputs that will distort the decision boundary of the model when incorporated into the training set. Such inputs may appear natural and legitimate but are engineered to introduce vulnerabilities.</p>
<p>Other attackers focus on weaknesses in data collection and preprocessing. If the training data is sourced from web scraping, social media, or untrusted user submissions, poisoned samples can be introduced upstream. These samples may pass through insufficient cleaning or validation checks, reaching the model in a “trusted” form. This is particularly dangerous in automated pipelines where human review is limited or absent.</p>
<p>In physically deployed systems, attackers may manipulate data at the source—for example, altering the environment captured by a sensor. A self-driving car might encounter poisoned data if visual markers on a road sign are subtly altered, causing the model to misclassify it during training. This kind of environmental poisoning blurs the line between adversarial attacks and data poisoning, but the mechanism, which involves compromising the training data, is the same.</p>
<p>Online learning systems represent another unique attack surface. These systems continuously adapt to new data streams, making them particularly susceptible to gradual poisoning. An attacker may introduce malicious samples incrementally, causing slow but steady shifts in model behavior. This form of attack is illustrated in <a href="#fig-poisoning-attack-example" class="quarto-xref">Figure&nbsp;25</a>.</p>
<div id="fig-poisoning-attack-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poisoning-attack-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="3c1fabb4a70840b2cea6ec04d02676619bdf4f4a.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-25" title="Figure&nbsp;25: Data Poisoning Attack: Adversarial manipulation of training data introduces subtle perturbations that compromise model integrity; incremental poisoning gradually shifts model behavior over time, making detection challenging in online learning systems. This attack surface differs from adversarial examples because it targets the model during training rather than at inference."><img src="robust_ai_files/mediabag/3c1fabb4a70840b2cea6ec04d02676619bdf4f4a.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisoning-attack-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: <strong>Data Poisoning Attack</strong>: Adversarial manipulation of training data introduces subtle perturbations that compromise model integrity; incremental poisoning gradually shifts model behavior over time, making detection challenging in online learning systems. This attack surface differs from adversarial examples because it targets the model <em>during</em> training rather than at inference.
</figcaption>
</figure>
</div>
<p>Insider collaboration adds a final layer of complexity. Malicious actors with legitimate access to training data, including annotators, researchers, or data vendors, can craft poisoning strategies that are more targeted and subtle than external attacks. These insiders may have knowledge of the model architecture or training procedures, giving them an advantage in designing effective poisoning schemes.</p>
<p>Defending against these diverse mechanisms requires a multi-pronged approach: secure data collection protocols, anomaly detection, robust preprocessing pipelines, and strong access control. Validation mechanisms must be sophisticated enough to detect not only outliers but also cleverly disguised poisoned samples that sit within the statistical norm.</p>
</section>
<section id="sec-robust-ai-impact-ml-1992" class="level4">
<h4 class="anchored" data-anchor-id="sec-robust-ai-impact-ml-1992">Impact on ML</h4>
<p>The effects of data poisoning extend far beyond simple accuracy degradation. In the most general sense, a poisoned dataset leads to a corrupted model. But the specific consequences depend on the attack vector and the adversary’s objective.</p>
<p>One common outcome is the degradation of overall model performance. When large portions of the training set are poisoned, often through label flipping or the introduction of noisy features, the model struggles to identify valid patterns, leading to lower accuracy, recall, or precision. In mission-critical applications like medical diagnosis or fraud detection, even small performance losses can result in significant real-world harm.</p>
<p>Targeted poisoning presents a different kind of danger. Rather than undermining the model’s general performance, these attacks cause specific misclassifications. A malware detector, for instance, may be engineered to ignore one particular signature, allowing a single attack to bypass security. Similarly, a facial recognition model might be manipulated to misidentify a specific individual, while functioning normally for others.</p>
<p>Some poisoning attacks introduce hidden vulnerabilities in the form of backdoors or trojans. These poisoned models behave as expected during evaluation but respond in a malicious way when presented with specific triggers. In such cases, attackers can “activate” the exploit on demand, bypassing system protections without triggering alerts.</p>
<p>Bias is another insidious impact of data poisoning. If an attacker poisons samples tied to a specific demographic or feature group, they can skew the model’s outputs in biased or discriminatory ways. Such attacks threaten fairness, amplify existing societal inequities, and are difficult to diagnose if the overall model metrics remain high.</p>
<p>Ultimately, data poisoning undermines the trustworthiness of the system itself. A model trained on poisoned data cannot be considered reliable, even if it performs well in benchmark evaluations. This erosion of trust has profound implications, particularly in fields like autonomous systems, financial modeling, and public policy.</p>
</section>
<section id="sec-robust-ai-case-study-art-protection-via-poisoning-ab11" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-case-study-art-protection-via-poisoning-ab11">Case Study: Art Protection via Poisoning</h4>
<p>Interestingly, not all data poisoning is malicious. Researchers have begun to explore its use as a defensive tool, particularly in the context of protecting creative work from unauthorized use by generative AI models.</p>
<p>A compelling example is Nightshade, developed by researchers at the University of Chicago to help artists prevent their work from being scraped and used to train image generation models without consent <span class="citation" data-cites="shan2023prompt">(<a href="#ref-shan2023prompt" role="doc-biblioref">Shan et al. 2023</a>)</span>. Nightshade allows artists to apply subtle perturbations to their images before publishing them online. These changes are invisible to human viewers but cause serious degradation in generative models that incorporate them into training.</p>
<div class="no-row-height column-margin column-container"></div><p>When Stable Diffusion was trained on just 300 poisoned images, the model began producing bizarre outputs—such as cows when prompted with “car,” or cat-like creatures in response to “dog.” These results, visualized in <a href="#fig-poisoning" class="quarto-xref">Figure&nbsp;26</a>, show how effectively poisoned samples can distort a model’s conceptual associations.</p>
<div id="fig-poisoning" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-poisoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/poisoning_example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-26" title="Figure&nbsp;26: Poisoning Attack: An incremental process where malicious samples are introduced to gradually shift model behavior during online learning. Continuous data streams can be manipulated without immediate detection through this. Source: [@shan2023prompt]."><img src="./images/png/poisoning_example.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: <strong>Poisoning Attack</strong>: An incremental process where malicious samples are introduced to gradually shift model behavior during online learning. Continuous data streams can be manipulated without immediate detection through this. Source: <span class="citation" data-cites="shan2023prompt">(<a href="#ref-shan2023prompt" role="doc-biblioref">Shan et al. 2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>What makes Nightshade especially potent is the cascading effect of poisoned concepts. Because generative models rely on semantic relationships between categories, a poisoned “car” can bleed into related concepts like “truck,” “bus,” or “train,” leading to widespread hallucinations.</p>
<p>However, like any powerful tool, Nightshade also introduces risks. The same technique used to protect artistic content could be repurposed to sabotage legitimate training pipelines, highlighting the dual-use dilemma<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> at the heart of modern machine learning security.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Dual-use Dilemma</strong>: In AI, the challenge of mitigating misuse of technology that has both positive and negative potential uses.</p></div></div></section>
</section>
<section id="sec-robust-ai-distribution-shifts-cffa" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-distribution-shifts-cffa">Distribution Shifts</h3>
<section id="sec-robust-ai-characteristics-9dc4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-characteristics-9dc4">Characteristics</h4>
<p>Distribution shift refers to the phenomenon where the data distribution encountered by a machine learning model during deployment differs from the distribution it was trained on, as shown in <a href="#fig-distribution-shift" class="quarto-xref">Figure&nbsp;27</a>. This change in distribution is not necessarily the result of a malicious attack. Rather, it often reflects the natural evolution of real-world environments over time. In essence, the statistical properties, patterns, or assumptions in the data may change between training and inference phases, which can lead to unexpected or degraded model performance.</p>
<div id="fig-distribution-shift" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-distribution-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4db25859f1a7d1c03ecd9edc6625969d8581f1ab.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-27" title="Figure&nbsp;27: Distribution Shift: Small inconsistencies between training and deployment data (represented by differing distributions of spurious feature z) can significantly disrupt model performance, even without altering the true label y. This figure emphasizes how data poisoning attacks exploit distributional differences to induce model errors and emphasizes the vulnerability of machine learning systems to subtle data manipulations. Source: [@shan2023prompt]."><img src="robust_ai_files/mediabag/4db25859f1a7d1c03ecd9edc6625969d8581f1ab.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distribution-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: <strong>Distribution Shift</strong>: Small inconsistencies between training and deployment data (represented by differing distributions of spurious feature <em>z</em>) can significantly disrupt model performance, even without altering the true label <em>y</em>. This figure emphasizes how data poisoning attacks exploit distributional differences to induce model errors and emphasizes the vulnerability of machine learning systems to subtle data manipulations. Source: <span class="citation" data-cites="shan2023prompt">(<a href="#ref-shan2023prompt" role="doc-biblioref">Shan et al. 2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-shan2023prompt" class="csl-entry" role="listitem">
Shan, Shawn, Wenxin Ding, Josephine Passananti, Stanley Wu, Haitao Zheng, and Ben Y. Zhao. 2023. <span>“Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models.”</span> <em>ArXiv Preprint</em> abs/2310.13828 (October). <a href="http://arxiv.org/abs/2310.13828v3">http://arxiv.org/abs/2310.13828v3</a>.
</div></div></figure>
</div>
<p>A distribution shift typically takes one of several forms:</p>
<ul>
<li><strong>Covariate shift</strong>, where the input distribution <span class="math inline">\(P(x)\)</span> changes while the conditional label distribution <span class="math inline">\(P(y \mid x)\)</span> remains stable.</li>
<li><strong>Label shift</strong>, where the label distribution <span class="math inline">\(P(y)\)</span> changes while <span class="math inline">\(P(x \mid y)\)</span> stays the same.</li>
<li><strong>Concept drift</strong>, where the relationship between inputs and outputs—<span class="math inline">\(P(y \mid x)\)</span>—evolves over time.</li>
<li><strong>Concept drift</strong>, where the relationship between inputs and outputs, <span class="math inline">\(P(y \mid x)\)</span>, evolves over time.</li>
</ul>
<p>These formal definitions help frame more intuitive examples of shift that are commonly encountered in practice.</p>
<p>One of the most common causes is domain mismatch, where the model is deployed on data from a different domain than it was trained on. For example, a sentiment analysis model trained on movie reviews may perform poorly when applied to tweets, due to differences in language, tone, and structure. In this case, the model has learned domain-specific features that do not generalize well to new contexts.</p>
<p>Another major source is temporal drift, where the input distribution evolves gradually or suddenly over time. In production settings, data changes due to new trends, seasonal effects, or shifts in user behavior. For instance, in a fraud detection system, fraud patterns may evolve as adversaries adapt. Without ongoing monitoring or retraining, models become stale and ineffective. This form of shift is visualized in <a href="#fig-drift-over-time" class="quarto-xref">Figure&nbsp;28</a>.</p>
<p>Contextual changes arise when deployment environments differ from training conditions due to external factors such as lighting, sensor variation, or user behavior. For example, a vision model trained in a lab under controlled lighting may underperform when deployed in outdoor or dynamic environments.</p>
<p>Another subtle but critical factor is unrepresentative training data. If the training dataset fails to capture the full variability of the production environment, the model may generalize poorly. For example, a facial recognition model trained predominantly on one demographic group may produce biased or inaccurate predictions when deployed more broadly. In this case, the shift reflects missing diversity or structure in the training data.</p>
<div id="fig-drift-over-time" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-drift-over-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c0570a22ea86debc32273c2ec1df3d42b1b4c0aa.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-28" title="Figure&nbsp;28: Temporal Drift: Shifting data distributions over time degrade model performance unless systems adapt through continuous monitoring and retraining. Concept drift manifests as changes in input patterns—such as evolving fraud schemes or seasonal trends—that require models to learn new relationships and maintain accuracy in dynamic environments."><img src="robust_ai_files/mediabag/c0570a22ea86debc32273c2ec1df3d42b1b4c0aa.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-drift-over-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: <strong>Temporal Drift</strong>: Shifting data distributions over time degrade model performance unless systems adapt through continuous monitoring and retraining. Concept drift manifests as changes in input patterns—such as evolving fraud schemes or seasonal trends—that require models to learn new relationships and maintain accuracy in dynamic environments.
</figcaption>
</figure>
</div>
<p>Distribution shifts like these can dramatically reduce the performance and reliability of ML models in production. Building robust systems requires not only understanding these shifts, but actively detecting and responding to them as they emerge.</p>
</section>
<section id="sec-robust-ai-mechanisms-ae94" class="level4">
<h4 class="anchored" data-anchor-id="sec-robust-ai-mechanisms-ae94">Mechanisms</h4>
<p>Distribution shifts arise from a variety of underlying mechanisms—both natural and system-driven. Understanding these mechanisms helps practitioners detect, diagnose, and design mitigation strategies.</p>
<p>One common mechanism is a change in data sources. When data collected at inference time comes from different sensors, APIs, platforms, or hardware than the training data, even subtle differences in resolution, formatting, or noise can introduce significant shifts. For example, a speech recognition model trained on audio from one microphone type may struggle with data from a different device.</p>
<p>Temporal evolution refers to changes in the underlying data over time. In recommendation systems, user preferences shift. In finance, market conditions change. These shifts may be slow and continuous or abrupt and disruptive. Without temporal awareness or continuous evaluation, models can become obsolete, frequently without prior indication. To illustrate this, <a href="#fig-temporal-evolution" class="quarto-xref">Figure&nbsp;29</a> shows how selective breeding over generations has significantly changed the physical characteristics of a dog breed. The earlier version of the breed exhibits a lean, athletic build, while the modern version is stockier, with a distinctively different head shape and musculature. This transformation is analogous to how data distributions can shift in real-world systems—initial data used to train a model may differ substantially from the data encountered over time. Just as evolutionary pressures shape biological traits, dynamic user behavior, market forces, or changing environments can shift the distribution of data in machine learning applications. Without periodic retraining or adaptation, models exposed to these evolving distributions may underperform or become unreliable.</p>
<div id="fig-temporal-evolution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-temporal-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/temporal_evoltion.png" class="lightbox" data-gallery="quarto-lightbox-gallery-29" title="Figure&nbsp;29: Breed Evolution: Selective breeding over generations produces substantial shifts in phenotypic characteristics, mirroring how data distributions change in machine learning systems over time. These temporal shifts necessitate model retraining or adaptation to maintain performance, as initial training data may no longer accurately represent current input distributions."><img src="./images/png/temporal_evoltion.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-temporal-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29: <strong>Breed Evolution</strong>: Selective breeding over generations produces substantial shifts in phenotypic characteristics, mirroring how data distributions change in machine learning systems over time. These temporal shifts necessitate model retraining or adaptation to maintain performance, as initial training data may no longer accurately represent current input distributions.
</figcaption>
</figure>
</div>
<p>Domain-specific variation arises when a model trained on one setting is applied to another. A medical diagnosis model trained on data from one hospital may underperform in another due to differences in equipment, demographics, or clinical workflows. These variations often require explicit adaptation strategies, such as domain generalization or fine-tuning.</p>
<p>Selection bias occurs when the training data does not accurately reflect the target population. This may result from sampling strategies, data access constraints, or labeling choices. The result is a model that overfits to specific segments and fails to generalize. Addressing this requires thoughtful data collection and continuous validation.</p>
<p>Feedback loops are a particularly subtle mechanism. In some systems, model predictions influence user behavior, which in turn affects future inputs. For instance, a dynamic pricing model might set prices that change buying patterns, which then distort the distribution of future training data. These loops can reinforce narrow patterns and make model behavior difficult to predict.</p>
<p>Lastly, adversarial manipulation can induce distribution shifts deliberately. Attackers may introduce out-of-distribution samples or craft inputs that exploit weak spots in the model’s decision boundary. These inputs may lie far from the training distribution and can cause unexpected or unsafe predictions.</p>
<p>These mechanisms often interact, making real-world distribution shift detection and mitigation complex. From a systems perspective, this complexity necessitates ongoing monitoring, logging, and feedback pipelines—features often absent in early-stage or static ML deployments.</p>
</section>
<section id="sec-robust-ai-impact-ml-de22" class="level4">
<h4 class="anchored" data-anchor-id="sec-robust-ai-impact-ml-de22">Impact on ML</h4>
<p>Distribution shift can affect nearly every dimension of ML system performance, from prediction accuracy and latency to user trust and system maintainability.</p>
<p>A common and immediate consequence is degraded predictive performance. When the data at inference time differs from training data, the model may produce systematically inaccurate or inconsistent predictions. This erosion of accuracy is particularly dangerous in high-stakes applications like fraud detection, autonomous vehicles, or clinical decision support.</p>
<p>Another serious effect is loss of reliability and trustworthiness. As distribution shifts, users may notice inconsistent or erratic behavior. For example, a recommendation system might begin suggesting irrelevant or offensive content. Even if overall accuracy metrics remain acceptable, loss of user trust can undermine the system’s value.</p>
<p>Distribution shift also amplifies model bias. If certain groups or data segments are underrepresented in the training data, the model may fail more frequently on those groups. Under shifting conditions, these failures can become more pronounced, resulting in discriminatory outcomes or fairness violations.</p>
<p>There is also a rise in uncertainty and operational risk. In many production settings, model decisions feed directly into business operations or automated actions. Under shift, these decisions become less predictable and harder to validate, increasing the risk of cascading failures or poor decisions downstream.</p>
<p>From a system maintenance perspective, distribution shifts complicate retraining and deployment workflows. Without robust mechanisms for drift detection and performance monitoring, shifts may go unnoticed until performance degrades significantly. Once detected, retraining may be required—raising challenges related to data collection, labeling, model rollback, and validation. This creates friction in continuous integration and deployment (CI/CD) workflows and can significantly slow down iteration cycles.</p>
<p>Moreover, distribution shift increases vulnerability to adversarial attacks. Attackers can exploit the model’s poor calibration on unfamiliar data, using slight perturbations to push inputs outside the training distribution and cause failures. This is especially concerning when system feedback loops or automated decisioning pipelines are in place.</p>
<p>From a systems perspective, distribution shift is not just a modeling concern—it is a core operational challenge. It requires end-to-end system support: mechanisms for data logging, drift detection, automated alerts, model versioning, and scheduled retraining. ML systems must be designed to detect when performance degrades in production, diagnose whether a distribution shift is the cause, and trigger appropriate mitigation actions. This might include human-in-the-loop review, fallback strategies, model retraining pipelines, or staged deployment rollouts.</p>
<p>In mature ML systems, handling distribution shift becomes a matter of infrastructure, observability, and automation, not just modeling technique. Failing to account for it risks silent model failure in dynamic, real-world environments—precisely where ML systems are expected to deliver the most value.</p>
<p>A summary of common types of distribution shifts, their effects on model performance, and potential system-level responses is shown in <a href="#tbl-distribution-shift-summary" class="quarto-xref">Table&nbsp;3</a>.</p>
</section>
<section id="sec-robust-ai-summary-distribution-shifts-system-implications-01fa" class="level4">
<h4 class="anchored" data-anchor-id="sec-robust-ai-summary-distribution-shifts-system-implications-01fa">Summary of Distribution Shifts and System Implications</h4>
<div id="tbl-distribution-shift-summary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-distribution-shift-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: <strong>Distribution Shift Types</strong>: Real-world ML systems encounter various forms of distribution shift—including covariate, concept, and prior shift—that degrade performance by altering the relationship between inputs and outputs, or the prevalence of different outcomes. Understanding these shifts and implementing system-level mitigations—such as monitoring, adaptive learning, and robust training—is crucial for maintaining reliable performance in dynamic environments.
</figcaption>
<div aria-describedby="tbl-distribution-shift-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 32%">
<col style="width: 27%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Type of Shift</th>
<th>Cause or Example</th>
<th>Consequence for Model</th>
<th>System-Level Response</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Covariate Shift</td>
<td>Change in input features (e.g., sensor calibration drift)</td>
<td>Model misclassifies new inputs despite consistent labels</td>
<td>Monitor input distributions; retrain with updated features</td>
</tr>
<tr class="even">
<td>Label Shift</td>
<td>Change in label distribution (e.g., new class frequencies in usage)</td>
<td>Prediction probabilities become skewed</td>
<td>Track label priors; reweight or adapt output calibration</td>
</tr>
<tr class="odd">
<td>Concept Drift</td>
<td>Evolving relationship between inputs and outputs (e.g.&nbsp;fraud tactics)</td>
<td>Model performance degrades over time</td>
<td>Retrain frequently; use continual or online learning</td>
</tr>
<tr class="even">
<td>Domain Mismatch</td>
<td>Train on reviews, deploy on tweets</td>
<td>Poor generalization due to different vocabularies or styles</td>
<td>Use domain adaptation or fine-tuning</td>
</tr>
<tr class="odd">
<td>Contextual Change</td>
<td>New deployment environment (e.g., lighting, user behavior)</td>
<td>Performance varies by context</td>
<td>Collect contextual data; monitor conditional accuracy</td>
</tr>
<tr class="even">
<td>Selection Bias</td>
<td>Underrepresentation during training</td>
<td>Biased predictions for unseen groups</td>
<td>Validate dataset balance; augment training data</td>
</tr>
<tr class="odd">
<td>Feedback Loops</td>
<td>Model outputs affect future inputs (e.g., recommender systems)</td>
<td>Reinforced drift, unpredictable patterns</td>
<td>Monitor feedback effects; consider counterfactual logging</td>
</tr>
<tr class="even">
<td>Adversarial Shift</td>
<td>Attackers introduce OOD inputs or perturbations</td>
<td>Model becomes vulnerable to targeted failures</td>
<td>Use robust training; detect out-of-distribution inputs</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="sec-robust-ai-detection-mitigation-3a67" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-detection-mitigation-3a67">Detection and Mitigation</h3>
<p>The detection and mitigation of threats to ML systems requires combining defensive strategies across multiple layers. These include techniques to identify and counter adversarial attacks, data poisoning attempts, and distribution shifts that can degrade model performance and reliability. Through systematic application of these protections, ML systems can maintain robustness when deployed in dynamic real-world environments.</p>
<section id="sec-robust-ai-adversarial-attacks-e168" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-adversarial-attacks-e168">Adversarial Attacks</h4>
<p>As discussed earlier, adversarial attacks pose a significant threat to the robustness and reliability of ML systems. These attacks involve crafting carefully designed inputs, known as adversarial examples, to deceive ML models and cause them to make incorrect predictions. To safeguard ML systems against such attacks, it is crucial to develop effective techniques for detecting and mitigating these threats.</p>
<section id="sec-robust-ai-detection-techniques-e220" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-robust-ai-detection-techniques-e220">Detection Techniques</h5>
<p>Detecting adversarial examples is the first line of defense against adversarial attacks. Several techniques have been proposed to identify and flag suspicious inputs that may be adversarial.</p>
<p>Statistical methods represent one approach to detecting adversarial examples by analyzing the distributional properties of input data. These methods compare the input data distribution to a reference distribution, such as the training data distribution or a known benign distribution. Techniques like the <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm">Kolmogorov-Smirnov</a> <span class="citation" data-cites="berger2014kolmogorov">(<a href="#ref-berger2014kolmogorov" role="doc-biblioref">Berger and Zhou 2014</a>)</span> test or the <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35e.htm">Anderson-Darling</a> test can measure the discrepancy between distributions and flag inputs that deviate significantly from the expected distribution. A related approach, <a href="https://mathisonian.github.io/kde/">kernel density estimation (KDE)</a>, estimates the probability density function of benign examples in the input space. Since adversarial examples often lie in low-density regions, inputs with estimated density below a threshold can be flagged as potentially adversarial.</p>
<div class="no-row-height column-margin column-container"><div id="ref-berger2014kolmogorov" class="csl-entry" role="listitem">
Berger, Vance W., and YanYan Zhou. 2014. <span>“Wiley StatsRef: Statistics Reference Online.”</span> <em>Wiley Statsref: Statistics Reference Online</em>. Wiley. <a href="https://doi.org/10.1002/9781118445112.stat06558">https://doi.org/10.1002/9781118445112.stat06558</a>.
</div><div id="ref-panda2019discretization" class="csl-entry" role="listitem">
Panda, Priyadarshini, Indranil Chakraborty, and Kaushik Roy. 2019. <span>“Discretization Based Solutions for Secure Machine Learning Against Adversarial Attacks.”</span> <em>IEEE Access</em> 7: 70157–68. <a href="https://doi.org/10.1109/access.2019.2919463">https://doi.org/10.1109/access.2019.2919463</a>.
</div></div><p>Beyond distributional analysis, input transformation methods offer an alternative detection strategy. Feature squeezing <span class="citation" data-cites="panda2019discretization">(<a href="#ref-panda2019discretization" role="doc-biblioref">Panda, Chakraborty, and Roy 2019</a>)</span> reduces input space complexity through dimensionality reduction or discretization, eliminating the small, imperceptible perturbations that adversarial examples typically rely on. Inconsistencies between model predictions on original and squeezed inputs indicate potential adversarial manipulation.</p>
<p>Model uncertainty estimation provides yet another detection paradigm by quantifying the confidence associated with predictions. Since adversarial examples often exploit regions of high uncertainty in the model’s decision boundary, inputs with elevated uncertainty can be flagged as suspicious. Several approaches exist for uncertainty estimation, each with distinct trade-offs between accuracy and computational cost.</p>
<p>Bayesian neural networks provide the most principled uncertainty estimates by treating model weights as probability distributions, capturing both aleatoric (data inherent) and epistemic (model) uncertainty through approximate inference methods. Ensemble methods achieve uncertainty estimation by combining predictions from multiple independently trained models, using prediction variance as an uncertainty measure. While both approaches offer robust uncertainty quantification, they incur significant computational overhead.</p>
<p>Dropout, originally designed as a regularization technique to prevent overfitting during training <span class="citation" data-cites="hinton2012improvingneuralnetworkspreventing">(<a href="#ref-hinton2012improvingneuralnetworkspreventing" role="doc-biblioref">Hinton et al. 2012</a>)</span>, works by randomly deactivating a fraction of neurons during each training iteration, forcing the network to avoid over-reliance on specific neurons and improving generalization. This mechanism can be repurposed for uncertainty estimation through Monte Carlo dropout at inference time, where multiple forward passes with different dropout masks approximate the uncertainty distribution. However, this approach provides less precise uncertainty estimates since dropout was not specifically designed for uncertainty quantification but rather for preventing overfitting through enforced redundancy. Hybrid approaches that combine dropout with lightweight ensemble methods or Bayesian approximations can balance computational efficiency with estimation quality, making uncertainty-based detection more practical for real-world deployment.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hinton2012improvingneuralnetworkspreventing" class="csl-entry" role="listitem">
Hinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. <span>“Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.”</span> <a href="https://arxiv.org/abs/1207.0580">https://arxiv.org/abs/1207.0580</a>.
</div></div></section>
<section id="sec-robust-ai-defense-strategies-b574" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-robust-ai-defense-strategies-b574">Defense Strategies</h5>
<p>Once adversarial examples are detected, various defense strategies can be employed to mitigate their impact and improve the robustness of ML models.</p>
<p>Adversarial training is a technique that involves augmenting the training data with adversarial examples and retraining the model on this augmented dataset. Exposing the model to adversarial examples during training teaches it to classify them correctly and becomes more robust to adversarial attacks. Adversarial training can be performed using various attack methods, such as the <a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">Fast Gradient Sign Method</a> or Projected Gradient Descent <span class="citation" data-cites="madry2017towards">(<a href="#ref-madry2017towards" role="doc-biblioref">Madry et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-madry2017towards" class="csl-entry" role="listitem">
Madry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. <span>“Towards Deep Learning Models Resistant to Adversarial Attacks.”</span> <em>arXiv Preprint arXiv:1706.06083</em>, June. <a href="http://arxiv.org/abs/1706.06083v4">http://arxiv.org/abs/1706.06083v4</a>.
</div><div id="ref-papernot2016distillation" class="csl-entry" role="listitem">
Papernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. <span>“Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks.”</span> In <em>2016 IEEE Symposium on Security and Privacy (SP)</em>, 582–97. IEEE; IEEE. <a href="https://doi.org/10.1109/sp.2016.41">https://doi.org/10.1109/sp.2016.41</a>.
</div></div><p>Defensive distillation <span class="citation" data-cites="papernot2016distillation">(<a href="#ref-papernot2016distillation" role="doc-biblioref">Papernot et al. 2016</a>)</span> is a technique that trains a second model (the student model) to mimic the behavior of the original model (the teacher model). The student model is trained on the soft labels produced by the teacher model, which are less sensitive to small perturbations. Using the student model for inference can reduce the impact of adversarial perturbations, as the student model learns to generalize better and is less sensitive to adversarial noise.</p>
<p>Input preprocessing and transformation techniques try to remove or mitigate the effect of adversarial perturbations before feeding the input to the ML model. These techniques include image denoising, JPEG compression, random resizing, padding, or applying random transformations to the input data. By reducing the impact of adversarial perturbations, these preprocessing steps can help improve the model’s robustness to adversarial attacks.</p>
<p>Ensemble methods combine multiple models to make more robust predictions. The ensemble can reduce the impact of adversarial attacks by using a diverse set of models with different architectures, training data, or hyperparameters. Adversarial examples that fool one model may not fool others in the ensemble, leading to more reliable and robust predictions. Model diversification techniques, such as using different preprocessing techniques or feature representations for each model in the ensemble, can further enhance the robustness.</p>
</section>
<section id="sec-robust-ai-evaluation-testing-6884" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-robust-ai-evaluation-testing-6884">Evaluation and Testing</h5>
<p>Conduct thorough evaluation and testing to assess the effectiveness of adversarial defense techniques and measure the robustness of ML models.</p>
<p>Adversarial robustness metrics quantify the model’s resilience to adversarial attacks. These metrics can include the model’s accuracy on adversarial examples, the average distortion required to fool the model, or the model’s performance under different attack strengths. By comparing these metrics across different models or defense techniques, practitioners can assess and compare their robustness levels.</p>
<p>Standardized adversarial attack benchmarks and datasets provide a common ground for evaluating and comparing the robustness of ML models. These benchmarks include datasets with pre-generated adversarial examples and tools and frameworks for generating adversarial attacks. Examples of popular adversarial attack benchmarks include the <a href="https://github.com/google-research/mnist-c">MNIST-C</a>, <a href="https://paperswithcode.com/dataset/cifar-10c">CIFAR-10-C</a>, and ImageNet-C <span class="citation" data-cites="hendrycks2019benchmarking">(<a href="#ref-hendrycks2019benchmarking" role="doc-biblioref">Hendrycks and Dietterich 2019</a>)</span> datasets, which contain corrupted or perturbed versions of the original datasets.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hendrycks2019benchmarking" class="csl-entry" role="listitem">
Hendrycks, Dan, and Thomas Dietterich. 2019. <span>“Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.”</span> <em>arXiv Preprint arXiv:1903.12261</em>, March. <a href="http://arxiv.org/abs/1903.12261v1">http://arxiv.org/abs/1903.12261v1</a>.
</div></div><p>Practitioners can develop more robust and resilient ML systems by leveraging these adversarial example detection techniques, defense strategies, and robustness evaluation methods. However, it is important to note that adversarial robustness is an ongoing research area, and no single technique provides complete protection against all types of adversarial attacks. A comprehensive approach that combines multiple defense mechanisms and regular testing is essential to maintain the security and reliability of ML systems in the face of evolving adversarial threats.</p>
</section>
</section>
<section id="sec-robust-ai-data-poisoning-fbae" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-data-poisoning-fbae">Data Poisoning</h4>
<p>Data poisoning attacks aim to corrupt training data used to build ML models, undermining their integrity. As illustrated in <a href="#fig-adversarial-attack-injection" class="quarto-xref">Figure&nbsp;30</a>, these attacks can manipulate or pollute the training data in ways that cause models to learn incorrect patterns, leading to erroneous predictions or undesirable behaviors when deployed. Given the foundational role of training data in ML system performance, detecting and mitigating data poisoning is critical for maintaining model trustworthiness and reliability.</p>
<div id="fig-adversarial-attack-injection" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-attack-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c479b4ef8046423367b9faa7e68dcf1254db944a.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-30" title="Figure&nbsp;30: Data Poisoning Attack: Adversaries inject malicious data into the training set to manipulate model behavior, potentially causing misclassification or performance degradation during deployment. This attack emphasizes the vulnerability of machine learning systems to compromised data integrity and the need for robust data validation techniques. Source: li"><img src="robust_ai_files/mediabag/c479b4ef8046423367b9faa7e68dcf1254db944a.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-attack-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30: <strong>Data Poisoning Attack</strong>: Adversaries inject malicious data into the training set to manipulate model behavior, potentially causing misclassification or performance degradation during deployment. This attack emphasizes the vulnerability of machine learning systems to compromised data integrity and the need for robust data validation techniques. <em>Source: <a href="HTTPS://www.mdpi.com/2227-7390/12/2/247">li</a></em>
</figcaption>
</figure>
</div>
<section id="sec-robust-ai-anomaly-detection-techniques-3336" class="level5">
<h5 class="anchored" data-anchor-id="sec-robust-ai-anomaly-detection-techniques-3336">Anomaly Detection Techniques</h5>
<p>Statistical outlier detection methods identify data points that deviate significantly from most data. These methods assume that poisoned data instances are likely to be statistical outliers. Techniques such as the <a href="https://ubalt.pressbooks.pub/mathstatsguides/chapter/z-score-basics/">Z-score method</a>, <a href="https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm">Tukey’s method</a>, or the <a href="https://www.statisticshowto.com/mahalanobis-distance/">Mahalanobis distance</a> can be used to measure the deviation of each data point from the central tendency of the dataset. Data points that exceed a predefined threshold are flagged as potential outliers and considered suspicious for data poisoning.</p>
<p>Clustering-based methods group similar data points together based on their features or attributes. The assumption is that poisoned data instances may form distinct clusters or lie far away from the normal data clusters. By applying clustering algorithms like <a href="https://www.oreilly.com/library/view/data-algorithms/9781491906170/ch12.html">K-means</a>, <a href="https://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/50efb27d-abbe-4855-ad81-a5357050161f.xhtml">DBSCAN</a>, or <a href="https://www.oreilly.com/library/view/cluster-analysis-5th/9780470978443/chapter04.html">hierarchical clustering</a>, anomalous clusters or data points that do not belong to any cluster can be identified. These anomalous instances are then treated as potentially poisoned data.</p>
<p>Autoencoders are neural networks trained to reconstruct the input data from a compressed representation, as shown in <a href="#fig-autoencoder" class="quarto-xref">Figure&nbsp;31</a>. They can be used for anomaly detection by learning the normal patterns in the data and identifying instances that deviate from them. During training, the autoencoder is trained on clean, unpoisoned data. At inference time, the reconstruction error for each data point is computed. Data points with high reconstruction errors are considered abnormal and potentially poisoned, as they do not conform to the learned normal patterns.</p>
<div id="fig-autoencoder" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-autoencoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="2106df9e0de5e907ea8ab741d33097ba20258cdf.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-31" title="Figure&nbsp;31: Autoencoder Architecture: Autoencoders learn compressed data representations by minimizing reconstruction error, enabling anomaly detection by identifying inputs with high reconstruction loss. During training on normal data, the network learns efficient encoding and decoding, making it sensitive to deviations indicative of potential poisoning attacks. Source: dertat"><img src="robust_ai_files/mediabag/2106df9e0de5e907ea8ab741d33097ba20258cdf.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-autoencoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31: <strong>Autoencoder Architecture</strong>: Autoencoders learn compressed data representations by minimizing reconstruction error, enabling anomaly detection by identifying inputs with high reconstruction loss. During training on normal data, the network learns efficient encoding and decoding, making it sensitive to deviations indicative of potential poisoning attacks. <em>Source: <a href="HTTPS://medium.com/towards-data-science/applied-deep-learning-part-3-autoencoders-1c083af4d798">dertat</a></em>
</figcaption>
</figure>
</div>
</section>
<section id="sec-robust-ai-sanitization-preprocessing-8712" class="level5">
<h5 class="anchored" data-anchor-id="sec-robust-ai-sanitization-preprocessing-8712">Sanitization and Preprocessing</h5>
<p>Data poisoning can be avoided by cleaning data, which involves identifying and removing or correcting noisy, incomplete, or inconsistent data points. Techniques such as data deduplication, missing value imputation, and outlier removal can be applied to improve the quality of the training data. By eliminating or filtering out suspicious or anomalous data points, the impact of poisoned instances can be reduced.</p>
<p>Data validation involves verifying the integrity and consistency of the training data. This can include checking for data type consistency, range validation, and cross-field dependencies. By defining and enforcing data validation rules, anomalous or inconsistent data points indicative of data poisoning can be identified and flagged for further investigation.</p>
<p>Data provenance and lineage tracking involve maintaining a record of data’s origin, transformations, and movements throughout the ML pipeline. By documenting the data sources, preprocessing steps, and any modifications made to the data, practitioners can trace anomalies or suspicious patterns back to their origin. This helps identify potential points of data poisoning and facilitates the investigation and mitigation process.</p>
</section>
<section id="sec-robust-ai-robust-training-7184" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-robust-ai-robust-training-7184">Robust Training</h5>
<p>Robust optimization techniques can be used to modify the training objective to minimize the impact of outliers or poisoned instances. This can be achieved by using robust loss functions less sensitive to extreme values, such as the Huber loss or the modified Huber loss<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>. Regularization techniques<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>, such as <a href="https://medium.com/towards-data-science/l1-and-l2-regularization-methods-ce25e7fc831c">L1 or L2 regularization</a>, can also help in reducing the model’s sensitivity to poisoned data by constraining the model’s complexity and preventing overfitting.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<strong>Huber Loss</strong>: A loss function used in robust regression that is less sensitive to outliers in data than squared error loss.</p></div><div id="fn20"><p><sup>20</sup>&nbsp;<strong>Regularization</strong>: A method used in neural networks to prevent overfitting in models by adding a cost term to the loss function.</p></div><div id="ref-beaton1974fitting" class="csl-entry" role="listitem">
Beaton, Albert E., and John W. Tukey. 1974. <span>“The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data.”</span> <em>Technometrics</em> 16 (2): 147. <a href="https://doi.org/10.2307/1267936">https://doi.org/10.2307/1267936</a>.
</div><div id="fn21"><p><sup>21</sup>&nbsp;<strong>Minimax</strong>: A decision-making strategy, used in game theory and decision theory, which tries to minimize the maximum possible loss.</p></div></div><p>Robust loss functions are designed to be less sensitive to outliers or noisy data points. Examples include the modified <a href="https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html">Huber loss</a>, the Tukey loss <span class="citation" data-cites="beaton1974fitting">(<a href="#ref-beaton1974fitting" role="doc-biblioref">Beaton and Tukey 1974</a>)</span>, and the trimmed mean loss. These loss functions down-weight or ignore the contribution of abnormal instances during training, reducing their impact on the model’s learning process. Robust objective functions, such as the minimax<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> or distributionally robust objective, aim to optimize the model’s performance under worst-case scenarios or in the presence of adversarial perturbations.</p>
<p>Data augmentation techniques involve generating additional training examples by applying random transformations or perturbations to the existing data <a href="#fig-data-augmentation" class="quarto-xref">Figure&nbsp;32</a>. This helps in increasing the diversity and robustness of the training dataset. By introducing controlled variations in the data, the model becomes less sensitive to specific patterns or artifacts that may be present in poisoned instances. Randomization techniques, such as random subsampling or bootstrap aggregating, can also help reduce the impact of poisoned data by training multiple models on different subsets of the data and combining their predictions.</p>
<div id="fig-data-augmentation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-augmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/data_augmentation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-32" title="Figure&nbsp;32: Data Augmentation Techniques: Applying transformations like horizontal flips, rotations, and cropping expands training datasets, improving model robustness to variations in input data and reducing overfitting. These techniques generate new training examples without requiring additional labeled data, effectively increasing dataset diversity and enhancing generalization performance."><img src="./images/png/data_augmentation.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-augmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32: <strong>Data Augmentation Techniques</strong>: Applying transformations like horizontal flips, rotations, and cropping expands training datasets, improving model robustness to variations in input data and reducing overfitting. These techniques generate new training examples without requiring additional labeled data, effectively increasing dataset diversity and enhancing generalization performance.
</figcaption>
</figure>
</div>
</section>
<section id="sec-robust-ai-secure-data-sourcing-1f73" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-robust-ai-secure-data-sourcing-1f73">Secure Data Sourcing</h5>
<p>Implementing the best data collection and curation practices can help mitigate the risk of data poisoning. This includes establishing clear data collection protocols, verifying the authenticity and reliability of data sources, and conducting regular data quality assessments. Sourcing data from trusted and reputable providers and following secure data handling practices can reduce the likelihood of introducing poisoned data into the training pipeline.</p>
<p>Strong data governance and access control mechanisms are essential to prevent unauthorized modifications or tampering with the training data. This involves defining clear roles and responsibilities for data access, implementing access control policies based on the principle of least privilege,<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> and monitoring and logging data access activities. By restricting access to the training data and maintaining an audit trail, potential data poisoning attempts can be detected and investigated.</p>
<div class="no-row-height column-margin column-container"><div id="fn22"><p><sup>22</sup>&nbsp;<strong>Principle of Least Privilege</strong>: A security concept in which a user is given the minimum levels of access necessary to complete his/her job functions.</p></div><div id="fn23"><p><sup>23</sup>&nbsp;<strong>Data Sanitization</strong>: The process of deliberately, permanently, and irreversibly removing or destroying the data stored on a memory device to make it unrecoverable.</p></div></div><p>Detecting and mitigating data poisoning attacks requires a multifaceted approach that combines anomaly detection, data sanitization,<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> robust training techniques, and secure data sourcing practices. By implementing these measures, ML practitioners can improve the resilience of their models against data poisoning and ensure the integrity and trustworthiness of the training data. However, it is important to note that data poisoning is an active area of research, and new attack vectors and defense mechanisms continue to emerge. Staying informed about the latest developments and adopting a proactive and adaptive approach to data security is crucial for maintaining the robustness of ML systems.</p>
</section>
</section>
<section id="sec-robust-ai-distribution-shifts-dd9d" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-distribution-shifts-dd9d">Distribution Shifts</h4>
<section id="sec-robust-ai-detection-mitigation-5b56" class="level5">
<h5 class="anchored" data-anchor-id="sec-robust-ai-detection-mitigation-5b56">Detection and Mitigation</h5>
<p>Recall that distribution shifts occur when the data distribution encountered by a machine learning (ML) model during deployment differs from the distribution it was trained on. These shifts can significantly impact the model’s performance and generalization ability, leading to suboptimal or incorrect predictions. Detecting and mitigating distribution shifts is crucial to ensure the robustness and reliability of ML systems in real-world scenarios.</p>
</section>
<section id="sec-robust-ai-detection-techniques-f976" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-robust-ai-detection-techniques-f976">Detection Techniques</h5>
<p>Statistical tests can be used to compare the distributions of the training and test data to identify significant differences. Techniques such as the Kolmogorov-Smirnov test or the Anderson-Darling test measure the discrepancy between two distributions and provide a quantitative assessment of the presence of distribution shift. By applying these tests to the input features or the model’s predictions, practitioners can detect if there is a statistically significant difference between the training and test distributions.</p>
<p>Divergence metrics quantify the dissimilarity between two probability distributions. Commonly used divergence metrics include the <a href="https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254">Kullback-Leibler (KL) divergence</a> and the <a href="https://medium.com/towards-data-science/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6">Jensen-Shannon (JS) divergence</a>. By calculating the divergence between the training and test data distributions, practitioners can assess the extent of the distribution shift. High divergence values indicate a significant difference between the distributions, suggesting the presence of a distribution shift.</p>
<p>Uncertainty quantification techniques, such as Bayesian neural networks<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> or ensemble methods<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>, can estimate the uncertainty associated with the model’s predictions. When a model is applied to data from a different distribution, its predictions may have higher uncertainty. By monitoring the uncertainty levels, practitioners can detect distribution shifts. If the uncertainty consistently exceeds a predetermined threshold for test samples, it suggests that the model is operating outside its trained distribution.</p>
<div class="no-row-height column-margin column-container"><div id="fn24"><p><sup>24</sup>&nbsp;<strong>Bayesian Neural Networks</strong>: Neural networks that incorporate probability distributions over their weights, enabling uncertainty quantification in predictions and more robust decision making.</p></div><div id="fn25"><p><sup>25</sup>&nbsp;<strong>Ensemble Methods</strong>: An ML approach that combines several models to improve prediction accuracy.</p></div></div><p>In addition, domain classifiers are trained to distinguish between different domains or distributions. Practitioners can detect distribution shifts by training a classifier to differentiate between the training and test domains. If the domain classifier achieves high accuracy in distinguishing between the two domains, it indicates a significant difference in the underlying distributions. The performance of the domain classifier serves as a measure of the distribution shift.</p>
</section>
<section id="sec-robust-ai-mitigation-techniques-c88d" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="sec-robust-ai-mitigation-techniques-c88d">Mitigation Techniques</h5>
<p>Transfer learning leverages knowledge gained from one domain to improve performance in another, as shown in <a href="#fig-transfer-learning" class="quarto-xref">Figure&nbsp;33</a>. By using pre-trained models or transferring learned features from a source domain to a target domain, transfer learning can help mitigate the impact of distribution shifts. The pre-trained model can be fine-tuned on a small amount of labeled data from the target domain, allowing it to adapt to the new distribution. Transfer learning is particularly effective when the source and target domains share similar characteristics or when labeled data in the target domain is scarce.</p>
<div id="fig-transfer-learning" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="7e6fd2c06da17b781e4dafc40220dd5866d68a56.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-33" title="Figure&nbsp;33: Knowledge Transfer: Pre-training on large datasets enables models to learn generalizable features, which can then be fine-tuned for specific target tasks with limited labeled data. This approach mitigates data scarcity and accelerates learning in new domains by leveraging previously acquired knowledge. Source: bhavsar"><img src="robust_ai_files/mediabag/7e6fd2c06da17b781e4dafc40220dd5866d68a56.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;33: <strong>Knowledge Transfer</strong>: Pre-training on large datasets enables models to learn generalizable features, which can then be fine-tuned for specific target tasks with limited labeled data. This approach mitigates data scarcity and accelerates learning in new domains by leveraging previously acquired knowledge. <em>Source: <a href="HTTPS://medium.com/modern-nlp/transfer-learning-in-nlp-f5035cc3f62f">bhavsar</a></em>
</figcaption>
</figure>
</div>
<p>Continual learning, also known as lifelong learning, enables ML models to learn continuously from new data distributions while retaining knowledge from previous distributions. Techniques such as elastic weight consolidation (EWC) <span class="citation" data-cites="kirkpatrick2017overcoming">(<a href="#ref-kirkpatrick2017overcoming" role="doc-biblioref">Kirkpatrick et al. 2017</a>)</span> or gradient episodic memory (GEM) <span class="citation" data-cites="lopez2017gradient">(<a href="#ref-lopez2017gradient" role="doc-biblioref">Lopez-Paz and Ranzato 2017</a>)</span> allow models to adapt to evolving data distributions over time. These techniques aim to balance the plasticity of the model (ability to learn from new data) with the stability of the model (retaining previously learned knowledge). By incrementally updating the model with new data and mitigating catastrophic forgetting, continual learning helps models stay robust to distribution shifts.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kirkpatrick2017overcoming" class="csl-entry" role="listitem">
Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017. <span>“Overcoming Catastrophic Forgetting in Neural Networks.”</span> <em>Proceedings of the National Academy of Sciences</em> 114 (13): 3521–26. <a href="https://doi.org/10.1073/pnas.1611835114">https://doi.org/10.1073/pnas.1611835114</a>.
</div><div id="ref-lopez2017gradient" class="csl-entry" role="listitem">
Lopez-Paz, David, and Marc’Aurelio Ranzato. 2017. <span>“Gradient Episodic Memory for Continual Learning.”</span> In <em>NIPS</em>, 30:6467–76. <a href="https://proceedings.neurips.cc/paper/2017/hash/f87522788a2be2d171666752f97ddebb-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/f87522788a2be2d171666752f97ddebb-Abstract.html</a>.
</div></div><p>Data augmentation techniques, such as those we have seen previously, involve applying transformations or perturbations to the existing training data to increase its diversity and improve the model’s robustness to distribution shifts. By introducing variations in the data, such as rotations, translations, scaling, or adding noise, data augmentation helps the model learn invariant features and generalize better to unseen distributions. Data augmentation can be performed during training and inference to improve the model’s ability to handle distribution shifts.</p>
<p>Ensemble methods combine multiple models to make predictions more robust to distribution shifts. By training models on different subsets of the data, using different algorithms, or with different hyperparameters, ensemble methods can capture diverse aspects of the data distribution. When presented with a shifted distribution, the ensemble can leverage the strengths of individual models to make more accurate and stable predictions. Techniques like bagging, boosting, or stacking can create effective ensembles.</p>
<p>Regularly updating models with new data from the target distribution is crucial to mitigate the impact of distribution shifts. As the data distribution evolves, models should be retrained or fine-tuned on the latest available data to adapt to the changing patterns. Monitoring model performance and data characteristics can help detect when an update is necessary. By keeping the models up to date, practitioners can ensure they remain relevant and accurate in the face of distribution shifts.</p>
<p>Evaluating models using robust metrics less sensitive to distribution shifts can provide a more reliable assessment of model performance. Metrics such as the area under the precision-recall curve (AUPRC) or the F1 score<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> are more robust to class imbalance and can better capture the model’s performance across different distributions. Additionally, using domain-specific evaluation metrics that align with the desired outcomes in the target domain can provide a more meaningful measure of the model’s effectiveness.</p>
<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;<strong>F1 Score</strong>: A measure of a model’s accuracy that combines precision (correct positive predictions) and recall (proportion of actual positives identified) into a single metric. Calculated as the harmonic mean of precision and recall.</p></div></div><p>Detecting and mitigating distribution shifts is an ongoing process that requires continuous monitoring, adaptation, and improvement. By employing a combination of detection techniques and mitigation strategies, ML practitioners can proactively identify and address distribution shifts, ensuring the robustness and reliability of their models in real-world deployments. It is important to note that distribution shifts can take various forms and may require domain-specific approaches depending on the nature of the data and the application. Staying informed about the latest research and best practices in handling distribution shifts is essential for building resilient ML systems.</p>
<div id="quiz-question-sec-robust-ai-model-robustness-f537" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the Fast Gradient Sign Method (FGSM)?</p>
<ol type="a">
<li>A method that perturbs input data by adding noise in random directions.</li>
<li>A technique that uses elastic net regularization to create sparse perturbations.</li>
<li>A gradient-based attack that adds noise in the direction of the gradient to maximize prediction error.</li>
<li>A method that exploits the transferability of adversarial examples across different models.</li>
</ol></li>
<li><p>Explain how transfer-based attacks can be used in black-box scenarios to fool machine learning models.</p></li>
<li><p>True or False: Optimization-based attacks like the Carlini and Wagner (C&amp;W) attack are less computationally intensive than gradient-based attacks.</p></li>
<li><p>Adversarial patches are designed to work under varying conditions, such as lighting and viewing angles, making them effective in _______ attacks.</p></li>
<li><p>Discuss the potential real-world impact of adversarial attacks on autonomous vehicles.</p></li>
</ol>
<p><a href="#quiz-answer-sec-robust-ai-model-robustness-f537" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
</section>
<section id="sec-robust-ai-software-faults-7c4a" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-robust-ai-software-faults-7c4a">Software Faults</h2>
<p>Machine learning systems rely on complex software infrastructures that extend far beyond the models themselves. These systems are built on top of frameworks, libraries, and runtime environments that facilitate model training, evaluation, and deployment. As with any large-scale software system, the components that support ML workflows are susceptible to faults—unintended behaviors resulting from defects, bugs, or design oversights in the software. These faults can manifest across all stages of an ML pipeline and, if not identified and addressed, may impair performance, compromise security, or even invalidate results. This section examines the nature, causes, and consequences of software faults in ML systems, as well as strategies for their detection and mitigation.</p>
<section id="sec-robust-ai-characteristics-a367" class="level3">
<h3 class="anchored" data-anchor-id="sec-robust-ai-characteristics-a367">Characteristics</h3>
<p>Software faults in ML frameworks originate from various sources, including programming errors, architectural misalignments, and version incompatibilities. These faults exhibit several important characteristics that influence how they arise and propagate in practice.</p>
<p>One defining feature of software faults is their diversity. Faults can range from syntactic and logical errors to more complex manifestations such as memory leaks, concurrency bugs, or failures in integration logic. The broad variety of potential fault types complicates both their identification and resolution, as they often surface in non-obvious ways.</p>
<p>A second key characteristic is their tendency to propagate across system boundaries. An error introduced in a low-level module, such as a tensor allocation routine or a preprocessing function, can produce cascading effects that disrupt model training, inference, or evaluation. Because ML frameworks are often composed of interconnected components, a fault in one part of the pipeline can introduce failures in seemingly unrelated modules.</p>
<p>Some faults are intermittent, manifesting only under specific conditions such as high system load, particular hardware configurations, or rare data inputs. These transient faults are notoriously difficult to reproduce and diagnose, as they may not consistently appear during standard testing procedures.</p>
<p>Furthermore, software faults may subtly interact with ML models themselves. For example, a bug in a data transformation script might introduce systematic noise or shift the distribution of inputs, leading to biased or inaccurate predictions. Similarly, faults in the serving infrastructure may result in discrepancies between training-time and inference-time behaviors, undermining deployment consistency.</p>
<p>The consequences of software faults extend to a range of system properties. Faults may impair performance by introducing latency or inefficient memory usage; they may reduce scalability by limiting parallelism; or they may compromise reliability and security by exposing the system to unexpected behaviors or malicious exploitation.</p>
<p>Finally, the manifestation of software faults is often shaped by external dependencies, such as hardware platforms, operating systems, or third-party libraries. Incompatibilities arising from version mismatches or hardware-specific behavior may result in subtle, hard-to-trace bugs that only appear under certain runtime conditions.</p>
<p>A thorough understanding of these characteristics is essential for developing robust software engineering practices in ML. It also provides the foundation for the detection and mitigation strategies described later in this section.</p>
</section>
<section id="sec-robust-ai-mechanisms-7544" class="level3">
<h3 class="anchored" data-anchor-id="sec-robust-ai-mechanisms-7544">Mechanisms</h3>
<p>Software faults in ML frameworks arise through a variety of mechanisms, reflecting the complexity of modern ML pipelines and the layered architecture of supporting tools. These mechanisms correspond to specific classes of software failures that commonly occur in practice.</p>
<p>One prominent class involves resource mismanagement, particularly with respect to memory. Improper memory allocation, including the failure to release buffers or file handles, can lead to memory leaks and, eventually, to resource exhaustion. This is especially detrimental in deep learning applications, where large tensors and GPU memory allocations are common. As shown in <a href="#fig-gpu-out-of-memory" class="quarto-xref">Figure&nbsp;34</a>, inefficient memory usage or the failure to release GPU resources can cause training procedures to halt or significantly degrade runtime performance.</p>
<div id="fig-gpu-out-of-memory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpu-out-of-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/gpu_out_of_memory.png" class="lightbox" data-gallery="quarto-lightbox-gallery-34" title="Figure&nbsp;34: GPU Resource Management: Inefficient memory usage or failure to release GPU resources can lead to out-of-memory errors and suboptimal performance during training."><img src="./images/png/gpu_out_of_memory.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gpu-out-of-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34: <strong>GPU Resource Management</strong>: Inefficient memory usage or failure to release GPU resources can lead to out-of-memory errors and suboptimal performance during training.
</figcaption>
</figure>
</div>
<p>Another recurring fault mechanism stems from concurrency and synchronization errors. In distributed or multi-threaded environments, incorrect coordination among parallel processes can lead to race conditions, deadlocks, or inconsistent states. These issues are often tied to the improper use of <a href="https://odsc.medium.com/optimizing-ml-serving-with-asynchronous-architectures-1071fc1be8e2">asynchronous operations</a>, such as non-blocking I/O or parallel data ingestion. Synchronization bugs can corrupt the consistency of training states or produce unreliable model checkpoints.</p>
<p>Compatibility problems frequently arise from changes to the software environment. For example, upgrading a third-party library without validating downstream effects may introduce subtle behavioral changes or break existing functionality. These issues are exacerbated when the training and inference environments differ in hardware, operating system, or dependency versions. Reproducibility in ML experiments often hinges on managing these environmental inconsistencies.</p>
<p>Faults related to numerical instability are also common in ML systems, particularly in optimization routines. Improper handling of floating-point precision, division by zero, or underflow/overflow conditions can introduce instability into gradient computations and convergence procedures. As described in <a href="https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter22.04-Numerical-Error-and-Instability.html">this resource</a>, the accumulation of rounding errors across many layers of computation can distort learned parameters or delay convergence.</p>
<p>Exception handling, though often overlooked, plays a crucial role in the stability of ML pipelines. Inadequate or overly generic exception management can cause systems to fail silently or crash under non-critical errors. Moreover, ambiguous error messages and poor logging practices impede diagnosis and prolong resolution times.</p>
<p>These fault mechanisms, while diverse in origin, share the potential to significantly impair ML systems. Understanding how they arise provides the basis for effective system-level safeguards.</p>
</section>
<section id="sec-robust-ai-impact-ml-90e2" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-impact-ml-90e2">Impact on ML</h3>
<p>The consequences of software faults can be profound, affecting not only the correctness of model outputs but also the broader usability and reliability of an ML system in production.</p>
<p>Performance degradation is a common symptom, often resulting from memory leaks, inefficient resource scheduling, or contention between concurrent threads. These issues tend to accumulate over time, leading to increased latency, reduced throughput, or even system crashes. As noted by <span class="citation" data-cites="maas2008combining">(<a href="#ref-maas2008combining" role="doc-biblioref">Maas et al. 2024</a>)</span>, the accumulation of performance regressions across components can severely restrict the operational capacity of ML systems deployed at scale.</p>
<div class="no-row-height column-margin column-container"><div id="ref-maas2008combining" class="csl-entry" role="listitem">
Maas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi Javanmard, Kathryn S. McKinley, and Colin Raffel. 2024. <span>“Combining Machine Learning and Lifetime-Based Resource Management for Memory Allocation and Beyond.”</span> <em>Communications of the ACM</em> 67 (4): 87–96. <a href="https://doi.org/10.1145/3611018">https://doi.org/10.1145/3611018</a>.
</div></div><p>In addition to slowing system performance, faults can lead to inaccurate predictions. For example, preprocessing errors or inconsistencies in feature encoding can subtly alter the input distribution seen by the model, producing biased or unreliable outputs. These kinds of faults are particularly insidious, as they may not trigger any obvious failure but still compromise downstream decisions. Over time, rounding errors and precision loss can amplify inaccuracies, particularly in deep architectures with many layers or long training durations.</p>
<p>Reliability is also undermined by software faults. Systems may crash unexpectedly, fail to recover from errors, or behave inconsistently across repeated executions. Intermittent faults are especially problematic in this context, as they erode user trust while eluding conventional debugging efforts. In distributed settings, faults in checkpointing or model serialization can cause training interruptions or data loss, reducing the resilience of long-running training pipelines.</p>
<p>Security vulnerabilities frequently arise from overlooked software faults. Buffer overflows, improper validation, or unguarded inputs can open the system to manipulation or unauthorized access. Attackers may exploit these weaknesses to alter the behavior of models, extract private data, or induce denial-of-service conditions. As described by <span class="citation" data-cites="li2021survey">(<a href="#ref-li2021survey" role="doc-biblioref">Q. Li et al. 2023</a>)</span>, such vulnerabilities pose serious risks, particularly when ML systems are integrated into critical infrastructure or handle sensitive user data.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2021survey" class="csl-entry" role="listitem">
Li, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. 2023. <span>“A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection.”</span> <em>IEEE Transactions on Knowledge and Data Engineering</em> 35 (4): 3347–66. <a href="https://doi.org/10.1109/tkde.2021.3124599">https://doi.org/10.1109/tkde.2021.3124599</a>.
</div></div><p>Moreover, the presence of faults complicates development and maintenance. Debugging becomes more time-consuming, especially when fault behavior is non-deterministic or dependent on external configurations. Frequent software updates or library patches may introduce regressions that require repeated testing. This increased engineering overhead can slow iteration, inhibit experimentation, and divert resources from model development.</p>
<p>Taken together, these impacts underscore the importance of systematic software engineering practices in ML—practices that anticipate, detect, and mitigate the diverse failure modes introduced by software faults.</p>
</section>
<section id="sec-robust-ai-detection-mitigation-710f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-detection-mitigation-710f">Detection and Mitigation</h3>
<p>Addressing software faults in ML systems requires an integrated strategy that spans development, testing, deployment, and monitoring. A comprehensive mitigation framework should combine proactive detection methods with robust design patterns and operational safeguards.</p>
<p>To help summarize these techniques and clarify where each strategy fits in the ML lifecycle, <a href="#tbl-software-faults-summary" class="quarto-xref">Table&nbsp;4</a> below categorizes detection and mitigation approaches by phase and objective. This table provides a high-level overview that complements the detailed explanations that follow.</p>
<div id="tbl-software-faults-summary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-software-faults-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: <strong>Fault Mitigation Strategies</strong>: Software faults in ML systems require layered detection and mitigation techniques applied throughout the development lifecycle—from initial testing to ongoing monitoring—to ensure reliability and robustness. This table categorizes these strategies by phase and objective, providing a framework for building comprehensive fault tolerance into machine learning deployments.
</figcaption>
<div aria-describedby="tbl-software-faults-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 32%">
<col style="width: 32%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Technique</th>
<th style="text-align: left;">Purpose</th>
<th style="text-align: left;">When to Apply</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Testing and Validation</td>
<td style="text-align: left;">Unit testing, integration testing, regression testing</td>
<td style="text-align: left;">Verify correctness and identify regressions</td>
<td style="text-align: left;">During development</td>
</tr>
<tr class="even">
<td style="text-align: left;">Static Analysis and Linting</td>
<td style="text-align: left;">Static analyzers, linters, code reviews</td>
<td style="text-align: left;">Detect syntax errors, unsafe operations, enforce best practices</td>
<td style="text-align: left;">Before integration</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Runtime Monitoring &amp; Logging</td>
<td colspan="3" style="text-align: left;">Metric collection, error logging, profiling | Observe system behavior, detect anomalies | During training and deployment |</td>
</tr>
<tr class="even">
<td style="text-align: left;">Fault-Tolerant Design</td>
<td style="text-align: left;">Exception handling, modular architecture, checkpointing</td>
<td style="text-align: left;">Minimize impact of failures, support recovery</td>
<td style="text-align: left;">Design and implementation phase</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Update Management</td>
<td style="text-align: left;">Dependency auditing, test staging, version tracking</td>
<td style="text-align: left;">Prevent regressions and compatibility issues</td>
<td style="text-align: left;">Before system upgrades or deployment</td>
</tr>
<tr class="even">
<td style="text-align: left;">Environment Isolation</td>
<td style="text-align: left;">Containerization (e.g., Docker, Kubernetes), virtual environments</td>
<td style="text-align: left;">Ensure reproducibility, avoid environment-specific bugs</td>
<td style="text-align: left;">Development, testing, deployment</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CI/CD and Automation</td>
<td style="text-align: left;">Automated test pipelines, monitoring hooks, deployment gates</td>
<td style="text-align: left;">Enforce quality assurance and catch faults early</td>
<td style="text-align: left;">Continuously throughout development</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The first line of defense involves systematic testing. Unit testing verifies that individual components behave as expected under normal and edge-case conditions. Integration testing ensures that modules interact correctly across boundaries, while regression testing detects errors introduced by code changes. Continuous testing is essential in fast-moving ML environments, where pipelines evolve rapidly and small modifications may have system-wide consequences. As shown in <a href="#fig-regression-testing" class="quarto-xref">Figure&nbsp;35</a>, automated regression tests help preserve functional correctness over time.</p>
<div id="fig-regression-testing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-regression-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/regression_testing.png" class="lightbox" data-gallery="quarto-lightbox-gallery-35" title="Figure&nbsp;35: Regression Test Automation: Automated regression tests verify that new code changes do not introduce unintended errors into existing functionality, preserving system reliability throughout the development lifecycle. Continuous execution of these tests is crucial in rapidly evolving machine learning systems where even small modifications can have widespread consequences. Source: UTOR"><img src="./images/png/regression_testing.png" class="img-fluid figure-img" style="width:75.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-regression-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35: <strong>Regression Test Automation</strong>: Automated regression tests verify that new code changes do not introduce unintended errors into existing functionality, preserving system reliability throughout the development lifecycle. Continuous execution of these tests is crucial in rapidly evolving machine learning systems where even small modifications can have widespread consequences. <em>Source: <a href="HTTPS://u-tor.com/topic/regression-vs-integration">UTOR</a></em>
</figcaption>
</figure>
</div>
<p>Static code analysis tools complement dynamic tests by identifying potential issues at compile time. These tools catch common errors such as variable misuse, unsafe operations, or violation of language-specific best practices. Combined with code reviews and consistent style enforcement, static analysis reduces the incidence of avoidable programming faults.</p>
<p>Runtime monitoring is critical for observing system behavior under real-world conditions. Logging frameworks should capture key signals such as memory usage, input/output traces, and exception events. Monitoring tools can track model throughput, latency, and failure rates, providing early warnings of software faults. Profiling, as illustrated in this <a href="https://microsoft.github.io/code-with-engineering-playbook/machine-learning/profiling-ml-and-mlops-code/">Microsoft resource</a>, helps identify performance bottlenecks and inefficiencies indicative of deeper architectural issues.</p>
<p>Robust system design further improves fault tolerance. Structured exception handling and assertion checks prevent small errors from cascading into system-wide failures. Redundant computations, fallback models, and failover mechanisms improve availability in the presence of component failures. Modular architectures that encapsulate state and isolate side effects make it easier to diagnose and contain faults. Checkpointing techniques, such as those discussed in <span class="citation" data-cites="eisenman2022check">(<a href="#ref-eisenman2022check" role="doc-biblioref">Eisenman et al. 2022</a>)</span>, enable recovery from mid-training interruptions without data loss.</p>
<div class="no-row-height column-margin column-container"><div id="ref-eisenman2022check" class="csl-entry" role="listitem">
Eisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and Murali Annavaram. 2022. <span>“Check-n-Run: A Checkpointing System for Training Deep Learning Recommendation Models.”</span> In <em>19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)</em>, 929–43. <a href="https://www.usenix.org/conference/nsdi22/presentation/eisenman">https://www.usenix.org/conference/nsdi22/presentation/eisenman</a>.
</div></div><p>Keeping ML software up to date is another key strategy. Applying regular updates and security patches helps address known bugs and vulnerabilities. However, updates must be validated through test staging environments to avoid regressions. Reviewing <a href="https://github.com/pytorch/pytorch/releases">release notes</a> and change logs ensures teams are aware of any behavioral changes introduced in new versions.</p>
<p>Containerization technologies like <a href="https://www.docker.com">Docker</a> and <a href="https://kubernetes.io">Kubernetes</a> allow teams to define reproducible runtime environments that mitigate compatibility issues. By isolating system dependencies, containers prevent faults introduced by system-level discrepancies across development, testing, and production.</p>
<p>Finally, automated pipelines built around continuous integration and continuous deployment (CI/CD) provide an infrastructure for enforcing fault-aware development. Testing, validation, and monitoring can be embedded directly into the CI/CD flow. As shown in <a href="#fig-CI-CD-procedure" class="quarto-xref">Figure&nbsp;36</a>, such pipelines reduce the risk of unnoticed regressions and ensure only tested code reaches deployment environments.</p>
<div id="fig-CI-CD-procedure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-CI-CD-procedure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/ci_cd_procedure.png" class="lightbox" data-gallery="quarto-lightbox-gallery-36" title="Figure&nbsp;36: CI/CD Pipeline: Automated CI/CD pipelines enforce fault-aware development by integrating testing and validation directly into the software delivery process, reducing the risk of regressions and ensuring only tested code reaches production. Containerization technologies, such as Docker and Kubernetes, further enhance reliability by providing reproducible runtime environments across these pipeline stages. Source: geeksforgeeks"><img src="./images/png/ci_cd_procedure.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-CI-CD-procedure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;36: <strong>CI/CD Pipeline</strong>: Automated CI/CD pipelines enforce fault-aware development by integrating testing and validation directly into the software delivery process, reducing the risk of regressions and ensuring only tested code reaches production. Containerization technologies, such as Docker and Kubernetes, further enhance reliability by providing reproducible runtime environments across these pipeline stages. <em>Source: <a href="HTTPS://www.geeksforgeeks.org/ci-cd-continuous-integration-and-continuous-delivery/">geeksforgeeks</a></em>
</figcaption>
</figure>
</div>
<p>Together, these practices form a holistic approach to software fault management in ML systems. When adopted comprehensively, they reduce the likelihood of system failures, improve long-term maintainability, and foster trust in model performance and reproducibility.</p>
<div id="quiz-question-sec-robust-ai-software-faults-7c4a" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a common consequence of software faults in ML systems?</p>
<ol type="a">
<li>Improved model accuracy</li>
<li>Increased system reliability</li>
<li>Performance degradation</li>
<li>Enhanced security</li>
</ol></li>
<li><p>In ML systems, _______ errors can lead to numerical instability, affecting gradient computations and convergence.</p></li>
<li><p>True or False: Static code analysis tools can help detect runtime errors in ML systems.</p></li>
</ol>
<p><a href="#quiz-answer-sec-robust-ai-software-faults-7c4a" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-robust-ai-tools-frameworks-c8a4" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-robust-ai-tools-frameworks-c8a4">Tools and Frameworks</h2>
<p>Given the importance of developing robust AI systems, in recent years, researchers and practitioners have developed a wide range of tools and frameworks to understand how hardware faults manifest and propagate to impact ML systems. These tools and frameworks play a crucial role in evaluating the resilience of ML systems to hardware faults by simulating various fault scenarios and analyzing their impact on the system’s performance. This enables designers to identify potential vulnerabilities and develop effective mitigation strategies, ultimately creating more robust and reliable ML systems that can operate safely despite hardware faults. This section provides an overview of widely used fault models in the literature and the tools and frameworks developed to evaluate the impact of such faults on ML systems.</p>
<section id="sec-robust-ai-fault-error-models-15bc" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-fault-error-models-15bc">Fault and Error Models</h3>
<p>As discussed previously, hardware faults can manifest in various ways, including transient, permanent, and intermittent faults. In addition to the type of fault under study, how the fault manifests is also important. For example, does the fault happen in a memory cell or during the computation of a functional unit? Is the impact on a single bit, or does it impact multiple bits? Does the fault propagate all the way and impact the application (causing an error), or does it get masked quickly and is considered benign? All these details impact what is known as the fault model, which plays a major role in simulating and measuring what happens to a system when a fault occurs.</p>
<p>To effectively study and understand the impact of hardware faults on ML systems, it is essential to understand the concepts of fault models and error models. A fault model describes how a hardware fault manifests itself in the system, while an error model represents how the fault propagates and affects the system’s behavior.</p>
<p>Fault models are often classified by several key properties. First, they can be defined by their duration: transient faults are temporary and vanish quickly; permanent faults persist indefinitely; and intermittent faults occur sporadically, making them particularly difficult to identify or predict. Another dimension is fault location, with faults arising in hardware components such as memory cells, functional units, or interconnects. Faults can also be characterized by their granularity—some faults affect only a single bit (e.g., a bitflip), while others impact multiple bits simultaneously, as in burst errors.</p>
<p>Error models, in contrast, describe the behavioral effects of faults as they propagate through the system. These models help researchers understand how initial hardware-level disturbances might manifest in the system’s behavior, such as through corrupted weights or miscomputed activations in an ML model. These models may operate at various abstraction levels, from low-level hardware errors to higher-level logical errors in ML frameworks.</p>
<p>The choice of fault or error model is central to robustness evaluation. For example, a system built to study single-bit transient faults <span class="citation" data-cites="sangchoolie2017one">(<a href="#ref-sangchoolie2017one" role="doc-biblioref">Sangchoolie, Pattabiraman, and Karlsson 2017</a>)</span> will not offer meaningful insight into the effects of permanent multi-bit faults <span class="citation" data-cites="wilkening2014calculating">(<a href="#ref-wilkening2014calculating" role="doc-biblioref">Wilkening et al. 2014</a>)</span>, since its design and assumptions are grounded in a different fault model entirely.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wilkening2014calculating" class="csl-entry" role="listitem">
Wilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva Gurumurthi, and David R. Kaeli. 2014. <span>“Calculating Architectural Vulnerability Factors for Spatial Multi-Bit Transient Faults.”</span> In <em>2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</em>, 293–305. IEEE; IEEE. <a href="https://doi.org/10.1109/micro.2014.15">https://doi.org/10.1109/micro.2014.15</a>.
</div><div id="ref-binkert2011gem5" class="csl-entry" role="listitem">
Binkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. <span>“The Gem5 Simulator.”</span> <em>ACM SIGARCH Computer Architecture News</em> 39 (2): 1–7. <a href="https://doi.org/10.1145/2024716.2024718">https://doi.org/10.1145/2024716.2024718</a>.
</div></div><p>It’s also important to consider how and where an error model is implemented. A single-bit flip at the architectural register level, modeled using simulators like gem5 <span class="citation" data-cites="binkert2011gem5">(<a href="#ref-binkert2011gem5" role="doc-biblioref">Binkert et al. 2011</a>)</span>, differs meaningfully from a similar bit flip in a PyTorch model’s weight tensor. While both simulate value-level perturbations, the lower-level model captures microarchitectural effects that are often abstracted away in software frameworks.</p>
<p>Interestingly, certain fault behavior patterns remain consistent regardless of abstraction level. For example, research has consistently demonstrated that single-bit faults cause more disruption than multi-bit faults, whether examining hardware-level effects or software-visible impacts <span class="citation" data-cites="sangchoolie2017one papadimitriou2021demystifying">(<a href="#ref-sangchoolie2017one" role="doc-biblioref">Sangchoolie, Pattabiraman, and Karlsson 2017</a>; <a href="#ref-papadimitriou2021demystifying" role="doc-biblioref">Papadimitriou and Gizopoulos 2021</a>)</span>. However, other important behaviors like error masking <span class="citation" data-cites="mohanram2003partial">(<a href="#ref-mohanram2003partial" role="doc-biblioref">Mohanram and Touba, n.d.</a>)</span> may only be observable at lower abstraction levels. As illustrated in <a href="#fig-error-masking" class="quarto-xref">Figure&nbsp;37</a>, this masking phenomenon can cause faults to be filtered out before they propagate to higher levels, meaning software-based tools may miss these effects entirely.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sangchoolie2017one" class="csl-entry" role="listitem">
Sangchoolie, Behrooz, Karthik Pattabiraman, and Johan Karlsson. 2017. <span>“One Bit Is (Not) Enough: An Empirical Study of the Impact of Single and Multiple Bit-Flip Errors.”</span> In <em>2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 97–108. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn.2017.30">https://doi.org/10.1109/dsn.2017.30</a>.
</div><div id="ref-papadimitriou2021demystifying" class="csl-entry" role="listitem">
Papadimitriou, George, and Dimitris Gizopoulos. 2021. <span>“Demystifying the System Vulnerability Stack: Transient Fault Effects Across the Layers.”</span> In <em>2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</em>, 902–15. IEEE; IEEE. <a href="https://doi.org/10.1109/isca52012.2021.00075">https://doi.org/10.1109/isca52012.2021.00075</a>.
</div><div id="ref-mohanram2003partial" class="csl-entry" role="listitem">
Mohanram, K., and N. A. Touba. n.d. <span>“Partial Error Masking to Reduce Soft Error Failure Rate in Logic Circuits.”</span> In <em>Proceedings. 16th IEEE Symposium on Computer Arithmetic</em>, 433–40. IEEE; IEEE Comput. Soc. <a href="https://doi.org/10.1109/dftvs.2003.1250141">https://doi.org/10.1109/dftvs.2003.1250141</a>.
</div></div><div id="fig-error-masking" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-error-masking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="f0383a469c73201b142a43954810f09c3f4f294c.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-37" title="Figure&nbsp;37: Error Masking: Microarchitectural redundancy can absorb single-bit faults before they propagate to observable system errors, highlighting a discrepancy between hardware-level and software-level fault models. This figure details how fault masking occurs within microarchitectural components, demonstrating that software-based error detection tools may underestimate the true resilience of a system to transient errors. [@ko2021characterizing]"><img src="robust_ai_files/mediabag/f0383a469c73201b142a43954810f09c3f4f294c.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-error-masking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37: <strong>Error Masking</strong>: Microarchitectural redundancy can absorb single-bit faults before they propagate to observable system errors, highlighting a discrepancy between hardware-level and software-level fault models. This figure details how fault masking occurs within microarchitectural components, demonstrating that software-based error detection tools may underestimate the true resilience of a system to transient errors. <em><span class="citation" data-cites="ko2021characterizing">(<a href="#ref-ko2021characterizing" role="doc-biblioref">Ko 2021</a>)</span></em>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-ko2021characterizing" class="csl-entry" role="listitem">
Ko, Yohan. 2021. <span>“Characterizing System-Level Masking Effects Against Soft Errors.”</span> <em>Electronics</em> 10 (18): 2286. <a href="https://doi.org/10.3390/electronics10182286">https://doi.org/10.3390/electronics10182286</a>.
</div></div></figure>
</div>
<p>To address these discrepancies, tools like Fidelity <span class="citation" data-cites="he2020fidelity">(<a href="#ref-he2020fidelity" role="doc-biblioref">He, Balaprakash, and Li 2020</a>)</span> have been developed to align fault models across abstraction layers. By mapping software-observed fault behaviors to corresponding hardware-level patterns <span class="citation" data-cites="cheng2016clear">(<a href="#ref-cheng2016clear" role="doc-biblioref">Cheng et al. 2016</a>)</span>, Fidelity offers a more accurate means of simulating hardware faults at the software level. While lower-level tools capture the true propagation of errors through a hardware system, they are generally slower and more complex. Software-level tools, such as those implemented in PyTorch or TensorFlow, are faster and easier to use for large-scale robustness testing, albeit with less precision.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cheng2016clear" class="csl-entry" role="listitem">
Cheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher, Hyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. <span>“CLEAR: &lt;U&gt;c&lt;/u&gt; Ross &lt;u&gt;-l&lt;/u&gt; Ayer &lt;u&gt;e&lt;/u&gt; Xploration for &lt;u&gt;a&lt;/u&gt; Rchitecting &lt;u&gt;r&lt;/u&gt; Esilience - Combining Hardware and Software Techniques to Tolerate Soft Errors in Processor Cores.”</span> In <em>Proceedings of the 53rd Annual Design Automation Conference</em>, 1–6. ACM. <a href="https://doi.org/10.1145/2897937.2897996">https://doi.org/10.1145/2897937.2897996</a>.
</div></div></section>
<section id="sec-robust-ai-hardwarebased-fault-injection-909a" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-hardwarebased-fault-injection-909a">Hardware-Based Fault Injection</h3>
<p>Hardware-based fault injection methods allow researchers to directly introduce faults into physical systems and observe their effects on machine learning (ML) models. These approaches are essential for validating assumptions made in software-level fault injection tools and for studying how real-world hardware faults influence system behavior. While most error injection tools used in ML robustness research are software-based, because of their speed and scalability, hardware-based approaches remain critical for grounding higher-level error models. They are considered the most accurate means of studying the impact of faults on ML systems by manipulating the hardware directly to introduce errors.</p>
<p>As illustrated in <a href="#fig-hardware-errors" class="quarto-xref">Figure&nbsp;38</a>, hardware faults can arise at various points within a deep neural network (DNN) processing pipeline. These faults may affect the control unit, on-chip memory (SRAM), off-chip memory (DRAM), processing elements, and accumulators, leading to erroneous results. In the depicted example, a DNN tasked with recognizing traffic signals correctly identifies a red light under normal conditions. However, hardware-induced faults, caused by phenomena such as aging, electromigration, soft errors, process variations, and manufacturing defects, can introduce errors that cause the DNN to misclassify the signal as a green light, potentially leading to catastrophic consequences in real-world applications.</p>
<div id="fig-hardware-errors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hardware-errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/hardware_errors.png" class="lightbox" data-gallery="quarto-lightbox-gallery-38" title="Figure&nbsp;38: Hardware Faults: This figure enables where hardware-induced errors can occur within a deep neural network (DNN) processing pipeline, highlighting potential points of failure such as control units and memory modules that can lead to misclassifications in real-world applications."><img src="./images/png/hardware_errors.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38: <strong>Hardware Faults</strong>: This figure enables where hardware-induced errors can occur within a deep neural network (DNN) processing pipeline, highlighting potential points of failure such as control units and memory modules that can lead to misclassifications in real-world applications.
</figcaption>
</figure>
</div>
<p>These methods enable researchers to observe the system’s behavior under real-world fault conditions. Both software-based and hardware-based error injection tools are described in this section in more detail.</p>
<section id="sec-robust-ai-methods-0afc" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-methods-0afc">Methods</h4>
<p>Two of the most common hardware-based fault injection methods are FPGA-based fault injection and radiation or beam testing.</p>
<p><strong>FPGA-based Fault Injection.</strong> Field-Programmable Gate Arrays (FPGAs) are reconfigurable integrated circuits that can be programmed to implement various hardware designs. In the context of fault injection, FPGAs offer high precision and accuracy, as researchers can target specific bits or sets of bits within the hardware. By modifying the FPGA configuration, faults can be introduced at specific locations and times during the execution of an ML model. FPGA-based fault injection allows for fine-grained control over the fault model, enabling researchers to study the impact of different types of faults, such as single-bit flips or multi-bit errors. This level of control makes FPGA-based fault injection a valuable tool for understanding the resilience of ML systems to hardware faults.</p>
<p>While FPGA-based methods allow precise, controlled fault injection, other approaches aim to replicate fault conditions found in natural environments.</p>
<p><strong>Radiation or Beam Testing.</strong> Radiation or beam testing <span class="citation" data-cites="velazco2010combining">(<a href="#ref-velazco2010combining" role="doc-biblioref">Velazco, Foucard, and Peronnard 2010</a>)</span> exposes hardware running ML models to high-energy particles like protons or neutrons. As shown in <a href="#fig-beam-testing" class="quarto-xref">Figure&nbsp;39</a>, specialized test facilities enable controlled radiation exposure to induce bitflips and other hardware-level faults. This approach is widely regarded as one of the most accurate methods for measuring error rates from particle strikes during application execution. Beam testing provides highly realistic fault scenarios that mirror conditions in radiation-rich environments, making it particularly valuable for validating systems destined for space missions or particle physics experiments. However, while beam testing offers exceptional realism, it lacks the precise targeting capabilities of FPGA-based injection - particle beams cannot be aimed at specific hardware bits or components with high precision. Despite this limitation and its significant operational complexity and cost, beam testing remains a trusted industry practice for rigorously evaluating hardware reliability under real-world radiation effects.</p>
<div class="no-row-height column-margin column-container"><div id="ref-velazco2010combining" class="csl-entry" role="listitem">
Velazco, Raoul, Gilles Foucard, and Paul Peronnard. 2010. <span>“Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in SRAM-Based FPGAs.”</span> <em>IEEE Transactions on Nuclear Science</em> 57 (6): 3500–3505. <a href="https://doi.org/10.1109/tns.2010.2087355">https://doi.org/10.1109/tns.2010.2087355</a>.
</div></div><div id="fig-beam-testing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-beam-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/image14.png" class="lightbox" data-gallery="quarto-lightbox-gallery-39" title="Figure&nbsp;39: Radiation Testing Setup: Beam testing facilities induce hardware faults by exposing semiconductor components to high-energy particles, simulating realistic radiation environments encountered in space or particle physics experiments. This controlled fault injection method provides valuable data for assessing hardware reliability and error rates under extreme conditions, though it lacks the precise targeting capabilities of FPGA-based fault injection. Source: JD instruments [HTTPS://jdinstruments.net/tester-capabilities-radiation-test/]"><img src="./images/png/image14.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-beam-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;39: <strong>Radiation Testing Setup</strong>: Beam testing facilities induce hardware faults by exposing semiconductor components to high-energy particles, simulating realistic radiation environments encountered in space or particle physics experiments. This controlled fault injection method provides valuable data for assessing hardware reliability and error rates under extreme conditions, though it lacks the precise targeting capabilities of FPGA-based fault injection. <em>Source: JD instruments [HTTPS://jdinstruments.net/tester-capabilities-radiation-test/]</em>
</figcaption>
</figure>
</div>
</section>
<section id="sec-robust-ai-limitations-853f" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-limitations-853f">Limitations</h4>
<p>Despite their high accuracy, hardware-based fault injection methods have several limitations that can hinder their widespread adoption.</p>
<p>First, cost is a major barrier. Both FPGA-based and beam testing<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> approaches require specialized hardware and facilities, which can be expensive to set up and maintain. This makes them less accessible to research groups with limited funding or infrastructure.</p>
<div class="no-row-height column-margin column-container"><div id="fn27"><p><sup>27</sup>&nbsp;<strong>Beam Testing</strong>: A testing method that exposes hardware to controlled particle radiation to evaluate its resilience to soft errors. Common in aerospace, medical devices, and high-reliability computing.</p></div></div><p>Second, these methods face challenges in scalability. Injecting faults and collecting data directly on hardware is time-consuming, which limits the number of experiments that can be run in a reasonable timeframe. This is especially restrictive when analyzing large ML systems or performing statistical evaluations across many fault scenarios.</p>
<p>Third, there are flexibility limitations. Hardware-based methods may not be as adaptable as software-based alternatives when modeling a wide variety of fault and error types. Changing the experimental setup to accommodate a new fault model often requires time-intensive hardware reconfiguration.</p>
<p>Despite these limitations, hardware-based fault injection remains essential for validating the accuracy of software-based tools and for studying system behavior under real-world fault conditions. By combining the high fidelity of hardware-based methods with the scalability and flexibility of software-based tools, researchers can develop a more complete understanding of ML systems’ resilience to hardware faults and craft effective mitigation strategies.</p>
</section>
</section>
<section id="sec-robust-ai-softwarebased-fault-injection-5e51" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-softwarebased-fault-injection-5e51">Software-Based Fault Injection</h3>
<p>As machine learning frameworks like TensorFlow, PyTorch, and Keras have become the dominant platforms for developing and deploying ML models, software-based fault injection tools have emerged as a flexible and scalable way to evaluate the robustness of these systems to hardware faults. Unlike hardware-based approaches, which operate directly on physical systems, software-based methods simulate the effects of hardware faults by modifying a model’s underlying computational graph, tensor values, or intermediate computations.</p>
<p>These tools have become increasingly popular in recent years because they integrate directly with ML development pipelines, require no specialized hardware, and allow researchers to conduct large-scale fault injection experiments quickly and cost-effectively. By simulating hardware-level faults, including bit flips in weights, activations, or gradients, at the software level, these tools enable efficient testing of fault tolerance mechanisms and provide valuable insight into model vulnerabilities.</p>
<p>In the remainder of this section, we will examine the advantages and limitations of software-based fault injection methods, introduce major classes of tools (both general-purpose and domain-specific), and discuss how they contribute to building resilient ML systems.</p>
<section id="sec-robust-ai-advantages-tradeoffs-2761" class="level4">
<h4 class="anchored" data-anchor-id="sec-robust-ai-advantages-tradeoffs-2761">Advantages and Trade-offs</h4>
<p>Software-based fault injection tools offer several advantages that make them attractive for studying the resilience of ML systems.</p>
<p>One of the primary benefits is speed. Since these tools operate entirely within the software stack, they avoid the overhead associated with modifying physical hardware or configuring specialized test environments. This efficiency enables researchers to perform a large number of fault injection experiments in significantly less time. The ability to simulate a wide range of faults quickly makes these tools particularly useful for stress-testing large-scale ML models or conducting statistical analyses that require thousands of injections.</p>
<p>Another major advantage is flexibility. Software-based fault injectors can be easily adapted to model various types of faults. Researchers can simulate single-bit flips, multi-bit corruptions, or even more complex behaviors such as burst errors or partial tensor corruption. Additionally, software tools allow faults to be injected at different stages of the ML pipeline, at the stages of training, inference, or gradient computation, enabling precise targeting of different system components or layers.</p>
<p>These tools are also highly accessible, as they require only standard ML development environments. Unlike hardware-based methods, there is no need for costly experimental setups, custom circuitry, or radiation testing facilities. This accessibility opens up fault injection research to a broader range of institutions and developers, including those working in academia, startups, or resource-constrained environments.</p>
<p>However, these advantages come with certain trade-offs. Chief among them is accuracy. Because software-based tools model faults at a higher level of abstraction, they may not fully capture the low-level hardware interactions that influence how faults actually propagate. For example, a simulated bit flip in an ML framework may not account for how data is buffered, cached, or manipulated at the hardware level, potentially leading to oversimplified conclusions.</p>
<p>Closely related is the issue of fidelity. While it is possible to approximate real-world fault behaviors, software-based tools may diverge from true hardware behavior, particularly when it comes to subtle interactions like masking, timing, or data movement. The results of such simulations depend heavily on the underlying assumptions of the error model and may require validation against real hardware measurements to be reliable.</p>
<p>Despite these limitations, software-based fault injection tools play an indispensable role in the study of ML robustness. Their speed, flexibility, and accessibility allow researchers to perform wide-ranging evaluations and inform the development of fault-tolerant ML architectures. In subsequent sections, we explore the major tools in this space, highlighting their capabilities and use cases.</p>
</section>
<section id="sec-robust-ai-limitations-b71a" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-limitations-b71a">Limitations</h4>
<p>While software-based fault injection tools offer significant advantages in terms of speed, flexibility, and accessibility, they are not without limitations. These constraints can impact the accuracy and realism of fault injection experiments, particularly when assessing the robustness of ML systems to real-world hardware faults.</p>
<p>One major concern is accuracy. Because software-based tools operate at higher levels of abstraction, they may not always capture the full spectrum of effects that hardware faults can produce. Low-level hardware interactions, including subtle timing errors, voltage fluctuations, and architectural side effects, can be missed entirely in high-level simulations. As a result, fault injection studies that rely solely on software models may under- or overestimate a system’s true vulnerability to certain classes of faults.</p>
<p>Closely related is the issue of fidelity. While software-based methods are often designed to emulate specific fault behaviors, the extent to which they reflect real-world hardware conditions can vary. For example, simulating a single-bit flip in the value of a neural network weight may not fully replicate how that same bit error would propagate through memory hierarchies or affect computation units on an actual chip. The more abstract the tool, the greater the risk that the simulated behavior will diverge from physical behavior under fault conditions.</p>
<p>Moreover, because software-based tools are easier to modify, there is a risk of unintentionally deviating from realistic fault assumptions. This can occur if the chosen fault model is overly simplified or not grounded in empirical data from actual hardware behavior. As discussed later in the section on bridging the hardware-software gap, tools like Fidelity <span class="citation" data-cites="he2020fidelity">(<a href="#ref-he2020fidelity" role="doc-biblioref">He, Balaprakash, and Li 2020</a>)</span> attempt to address these concerns by aligning software-level models with known hardware-level fault characteristics.</p>
<div class="no-row-height column-margin column-container"></div><p>Despite these limitations, software-based fault injection remains a critical part of the ML robustness research toolkit. When used appropriately, particularly when used in conjunction with hardware-based validation, these tools provide a scalable and efficient way to explore large design spaces, identify vulnerable components, and develop mitigation strategies. As fault modeling techniques continue to evolve, the integration of hardware-aware insights into software-based tools will be key to improving their realism and impact.</p>
</section>
<section id="sec-robust-ai-tool-types-f097" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-tool-types-f097">Tool Types</h4>
<p>Over the past several years, software-based fault injection tools have been developed for a wide range of ML frameworks and use cases. These tools vary in their level of abstraction, target platforms, and the types of faults they can simulate. Many are built to integrate with popular machine learning libraries such as PyTorch and TensorFlow, making them accessible to researchers and practitioners already working within those ecosystems.</p>
<p>One of the earliest and most influential tools is Ares <span class="citation" data-cites="reagen2018ares">(<a href="#ref-reagen2018ares" role="doc-biblioref">Reagen et al. 2018</a>)</span>, initially designed for the Keras framework. Developed at a time when deep neural networks (DNNs) were growing in popularity, Ares was one of the first tools to systematically explore the effects of hardware faults on DNNs. It provided support for injecting single-bit flips and evaluating bit-error rates (BER) across weights and activation values. Importantly, Ares was validated against a physical DNN accelerator implemented in silicon, demonstrating its relevance for hardware-level fault modeling. As the field matured, Ares was extended to support PyTorch, allowing researchers to analyze fault behavior in more modern ML settings.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reagen2018ares" class="csl-entry" role="listitem">
Reagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. 2018. <span>“Ares: A Framework for Quantifying the Resilience of Deep Neural Networks.”</span> In <em>2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)</em>, 1–6. IEEE. <a href="https://doi.org/10.1109/dac.2018.8465834">https://doi.org/10.1109/dac.2018.8465834</a>.
</div><div id="ref-mahmoud2020pytorchfi" class="csl-entry" role="listitem">
Mahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez Vicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, and Siva Kumar Sastry Hari. 2020. <span>“PyTorchFI: A Runtime Perturbation Tool for DNNs.”</span> In <em>2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-w)</em>, 25–31. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn-w50199.2020.00014">https://doi.org/10.1109/dsn-w50199.2020.00014</a>.
</div></div><p>Building on this foundation, PyTorchFI <span class="citation" data-cites="mahmoud2020pytorchfi">(<a href="#ref-mahmoud2020pytorchfi" role="doc-biblioref">Mahmoud et al. 2020</a>)</span> was introduced as a dedicated fault injection library for PyTorch. Developed in collaboration with Nvidia Research, PyTorchFI allows fault injection into key components of ML models, including weights, activations, and gradients. Its native support for GPU acceleration makes it especially well-suited for evaluating large models efficiently. As shown in <a href="#fig-phantom-objects" class="quarto-xref">Figure&nbsp;40</a>, even simple bit-level faults can cause severe visual and classification errors, including the appearance of ‘phantom’ objects in images, which could have downstream safety implications in domains like autonomous driving.</p>
<div id="fig-phantom-objects" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-phantom-objects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./images/png/phantom_objects.png" class="lightbox" data-gallery="quarto-lightbox-gallery-40" title="Figure&nbsp;40: Fault Injection Effects: Bit-level hardware faults can induce phantom objects and misclassifications in machine learning models, potentially leading to safety-critical errors in applications like autonomous driving; the left image represents correct classification, while the right image presents a false positive detection resulting from a single bit flip injected using pytorchfi."><img src="./images/png/phantom_objects.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-phantom-objects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;40: <strong>Fault Injection Effects</strong>: Bit-level hardware faults can induce phantom objects and misclassifications in machine learning models, potentially leading to safety-critical errors in applications like autonomous driving; the left image represents correct classification, while the right image presents a false positive detection resulting from a single bit flip injected using pytorchfi.
</figcaption>
</figure>
</div>
<p>The modular and accessible design of PyTorchFI has led to its adoption in several follow-on projects. For example, PyTorchALFI (developed by Intel xColabs) extends PyTorchFI’s capabilities to evaluate system-level safety in automotive applications. Similarly, Dr.&nbsp;DNA <span class="citation" data-cites="ma2024dr">(<a href="#ref-ma2024dr" role="doc-biblioref">Ma et al. 2024</a>)</span> from Meta introduces a more streamlined, Pythonic API to simplify fault injection workflows. Another notable extension is GoldenEye <span class="citation" data-cites="mahmoud2022dsn">(<a href="#ref-mahmoud2022dsn" role="doc-biblioref">Mahmoud et al. 2022</a>)</span>, which incorporates alternative numeric datatypes, including AdaptivFloat <span class="citation" data-cites="tambe2020algorithm">(<a href="#ref-tambe2020algorithm" role="doc-biblioref">Tambe et al. 2020</a>)</span> and BlockFloat, with bfloat16 as a specific example, to study the fault tolerance of non-traditional number formats under hardware-induced bit errors.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ma2024dr" class="csl-entry" role="listitem">
Ma, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore, Sriram Sankar, and Xun Jiao. 2024. <span>“Dr. DNA: Combating Silent Data Corruptions in Deep Learning Using Distribution of Neuron Activations.”</span> In <em>Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</em>, 239–52. ACM. <a href="https://doi.org/10.1145/3620666.3651349">https://doi.org/10.1145/3620666.3651349</a>.
</div><div id="ref-mahmoud2022dsn" class="csl-entry" role="listitem">
Mahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, and Gu-Yeon Wei. 2022. <span>“GoldenEye: A Platform for Evaluating Emerging Numerical Data Formats in DNN Accelerators.”</span> In <em>2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 206–14. IEEE. <a href="https://doi.org/10.1109/dsn53405.2022.00031">https://doi.org/10.1109/dsn53405.2022.00031</a>.
</div><div id="ref-tambe2020algorithm" class="csl-entry" role="listitem">
Tambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks, and Gu-Yeon Wei. 2020. <span>“Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference.”</span> In <em>2020 57th ACM/IEEE Design Automation Conference (DAC)</em>, 1–6. IEEE; IEEE. <a href="https://doi.org/10.1109/dac18072.2020.9218516">https://doi.org/10.1109/dac18072.2020.9218516</a>.
</div><div id="ref-chen2020tensorfi" class="csl-entry" role="listitem">
Chen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben. 2020. <span>“TensorFI: A Flexible Fault Injection Framework for TensorFlow Applications.”</span> In <em>2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE)</em>, 426–35. IEEE; IEEE. <a href="https://doi.org/10.1109/issre5003.2020.00047">https://doi.org/10.1109/issre5003.2020.00047</a>.
</div><div id="ref-chen2019sc" class="csl-entry" role="listitem">
Chen, Zitao, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben. 2019. <span>“&lt;I&gt;BinFI&lt;/i&gt;: An Efficient Fault Injector for Safety-Critical Machine Learning Systems.”</span> In <em>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>, 1–23. SC ’19. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3295500.3356177">https://doi.org/10.1145/3295500.3356177</a>.
</div></div><p>For researchers working within the TensorFlow ecosystem, TensorFI <span class="citation" data-cites="chen2020tensorfi">(<a href="#ref-chen2020tensorfi" role="doc-biblioref">Chen et al. 2020</a>)</span> provides a parallel solution. Like PyTorchFI, TensorFI enables fault injection into the TensorFlow computational graph and supports a variety of fault models. One of TensorFI’s strengths is its broad applicability—it can be used to evaluate many types of ML models beyond DNNs. Additional extensions such as BinFi <span class="citation" data-cites="chen2019sc">(<a href="#ref-chen2019sc" role="doc-biblioref">Chen et al. 2019</a>)</span> aim to accelerate the fault injection process by focusing on the most critical bits in a model. This prioritization can help reduce simulation time while still capturing the most meaningful error patterns.</p>
<p>At a lower level of the software stack, NVBitFI <span class="citation" data-cites="tsai2021nvbitfi">(<a href="#ref-tsai2021nvbitfi" role="doc-biblioref">T. Tsai et al. 2021</a>)</span> offers a platform-independent tool for injecting faults directly into GPU assembly code. Developed by Nvidia, NVBitFI is capable of performing fault injection on any GPU-accelerated application, not just ML workloads. This makes it an especially powerful tool for studying resilience at the instruction level, where errors can propagate in subtle and complex ways. NVBitFI represents an important complement to higher-level tools like PyTorchFI and TensorFI, offering fine-grained control over GPU-level behavior and supporting a broader class of applications beyond machine learning.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tsai2021nvbitfi" class="csl-entry" role="listitem">
Tsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa, and Stephen W. Keckler. 2021. <span>“NVBitFI: Dynamic Fault Injection for GPUs.”</span> In <em>2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 284–91. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn48987.2021.00041">https://doi.org/10.1109/dsn48987.2021.00041</a>.
</div></div><p>Together, these tools offer a wide spectrum of fault injection capabilities. While some are tightly integrated with high-level ML frameworks for ease of use, others enable lower-level fault modeling with higher fidelity. By choosing the appropriate tool based on the level of abstraction, performance needs, and target application, researchers can tailor their studies to gain more actionable insights into the robustness of ML systems. The next section focuses on how these tools are being applied in domain-specific contexts, particularly in safety-critical systems such as autonomous vehicles and robotics.</p>
</section>
<section id="sec-robust-ai-domainspecific-examples-d10d" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-domainspecific-examples-d10d">Domain-Specific Examples</h4>
<p>To address the unique challenges posed by specific application domains, researchers have developed specialized fault injection tools tailored to different machine learning (ML) systems. In high-stakes environments such as autonomous vehicles and robotics, domain-specific tools play a crucial role in evaluating system safety and reliability under hardware fault conditions. This section highlights three such tools: DriveFI and PyTorchALFI, which focus on autonomous vehicles, and MAVFI, which targets uncrewed aerial vehicles (UAVs). Each tool enables the injection of faults into mission-critical components, including perception, control, and sensor systems, providing researchers with insights into how hardware errors may propagate through real-world ML pipelines.</p>
<p>DriveFI <span class="citation" data-cites="jha2019ml">(<a href="#ref-jha2019ml" role="doc-biblioref">Jha et al. 2019</a>)</span> is a fault injection tool developed for autonomous vehicle systems. It facilitates the injection of hardware faults into the perception and control pipelines, enabling researchers to study how such faults affect system behavior and safety. Notably, DriveFI integrates with industry-standard platforms like Nvidia DriveAV and Baidu Apollo, offering a realistic environment for testing. Through this integration, DriveFI enables practitioners to evaluate the end-to-end resilience of autonomous vehicle architectures in the presence of fault conditions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jha2019ml" class="csl-entry" role="listitem">
Jha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B. Sullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, and Ravishankar K. Iyer. 2019. <span>“ML-Based Fault Injection for Autonomous Vehicles: A Case for Bayesian Fault Injection.”</span> In <em>2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 112–24. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn.2019.00025">https://doi.org/10.1109/dsn.2019.00025</a>.
</div><div id="ref-grafe2023large" class="csl-entry" role="listitem">
Gräfe, Ralf, Qutub Syed Sha, Florian Geissler, and Michael Paulitsch. 2023. <span>“Large-Scale Application of Fault Injection into PyTorch Models -an Extension to PyTorchFI for Validation Efficiency.”</span> In <em>2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-s)</em>, 56–62. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn-s58398.2023.00025">https://doi.org/10.1109/dsn-s58398.2023.00025</a>.
</div><div id="fn28"><p><sup>28</sup>&nbsp;<strong>Multimodal Sensor Data</strong>: Information collected simultaneously from multiple types of sensors (e.g., cameras, LiDAR, radar) to provide complementary perspectives of the environment. Critical for robust perception in autonomous systems.</p></div></div><p>PyTorchALFI <span class="citation" data-cites="grafe2023large">(<a href="#ref-grafe2023large" role="doc-biblioref">Gräfe et al. 2023</a>)</span> extends the capabilities of PyTorchFI for use in the autonomous vehicle domain. Developed by Intel xColabs, PyTorchALFI enhances the underlying fault injection framework with domain-specific features. These include the ability to inject faults into multimodal sensor data<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>, such as inputs from cameras and LiDAR systems. This allows for a deeper examination of how perception systems in autonomous vehicles respond to underlying hardware faults, further refining our understanding of system vulnerabilities and potential failure modes.</p>
<p>MAVFI <span class="citation" data-cites="hsiao2023mavfi">(<a href="#ref-hsiao2023mavfi" role="doc-biblioref">Hsiao et al. 2023</a>)</span> is a domain-specific fault injection framework tailored for robotics applications, particularly uncrewed aerial vehicles. Built atop the Robot Operating System (ROS), MAVFI provides a modular and extensible platform for injecting faults into various UAV subsystems, including sensors, actuators, and flight control algorithms. By assessing how injected faults impact flight stability and mission success, MAVFI offers a practical means for developing and validating fault-tolerant UAV architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hsiao2023mavfi" class="csl-entry" role="listitem">
Hsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay Janapa Reddi. 2023. <span>“MAVFI: An End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles.”</span> In <em>2023 Design, Automation &amp;Amp; Test in Europe Conference &amp;Amp; Exhibition (DATE)</em>, 1–6. IEEE; IEEE. <a href="https://doi.org/10.23919/date56975.2023.10137246">https://doi.org/10.23919/date56975.2023.10137246</a>.
</div></div><p>Together, these tools demonstrate the growing sophistication of fault injection research across application domains. By enabling fine-grained control over where and how faults are introduced, domain-specific tools provide actionable insights that general-purpose frameworks may overlook. Their development has greatly expanded the ML community’s capacity to design and evaluate resilient systems—particularly in contexts where reliability, safety, and real-time performance are critical.</p>
</section>
</section>
<section id="sec-robust-ai-bridging-hardwaresoftware-gap-291b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-robust-ai-bridging-hardwaresoftware-gap-291b">Bridging Hardware-Software Gap</h3>
<p>While software-based fault injection tools offer many advantages in speed, flexibility, and accessibility, they do not always capture the full range of effects that hardware faults can impose on a system. This is largely due to the abstraction gap: software-based tools operate at a higher level and may overlook low-level hardware interactions or nuanced error propagation mechanisms that influence the behavior of ML systems in critical ways.</p>
<p>As discussed in the work by <span class="citation" data-cites="bolchini2022fast">(<a href="#ref-bolchini2022fast" role="doc-biblioref">Bolchini et al. 2023</a>)</span>, hardware faults can exhibit complex spatial distribution patterns that are difficult to replicate using purely software-based fault models. They identify four characteristic fault propagation patterns: single point, where the fault corrupts a single value in a feature map; same row, where a partial or entire row in a feature map is corrupted; bullet wake, where the same location across multiple feature maps is affected; and shatter glass, a more complex combination of both same row and bullet wake behaviors. These diverse patterns, visualized in <a href="#fig-hardware-errors-bolchini" class="quarto-xref">Figure&nbsp;41</a>, highlight the limits of simplistic injection strategies and emphasize the need for hardware-aware modeling when evaluating ML system robustness.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-hardware-errors-bolchini" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hardware-errors-bolchini-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c9fd239520d4539840250eb1fe4e6291e704be55.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-41" title="Figure&nbsp;41: Hardware Fault Patterns: Dnns exhibit distinct error manifestations from hardware faults, categorized by their spatial distribution across feature maps and layers. These patterns—single point, same row, bullet wake, and shatter glass—represent localized versus widespread corruption, impacting model predictions and highlighting the need for fault-tolerant system design. Source: [@bolchini2022fast]."><img src="robust_ai_files/mediabag/c9fd239520d4539840250eb1fe4e6291e704be55.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-errors-bolchini-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;41: <strong>Hardware Fault Patterns</strong>: Dnns exhibit distinct error manifestations from hardware faults, categorized by their spatial distribution across feature maps and layers. These patterns—single point, same row, bullet wake, and shatter glass—represent localized versus widespread corruption, impacting model predictions and highlighting the need for fault-tolerant system design. Source: <span class="citation" data-cites="bolchini2022fast">(<a href="#ref-bolchini2022fast" role="doc-biblioref">Bolchini et al. 2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-bolchini2022fast" class="csl-entry" role="listitem">
Bolchini, Cristiana, Luca Cassano, Antonio Miele, and Alessandro Toschi. 2023. <span>“Fast and Accurate Error Simulation for CNNs Against Soft Errors.”</span> <em>IEEE Transactions on Computers</em> 72 (4): 984–97. <a href="https://doi.org/10.1109/tc.2022.3184274">https://doi.org/10.1109/tc.2022.3184274</a>.
</div></div></figure>
</div>
<p>To address this abstraction gap, researchers have developed tools that explicitly aim to map low-level hardware error behavior to software-visible effects. One such tool is Fidelity, which bridges this gap by studying how hardware-level faults propagate and become observable at higher software layers. The next section discusses Fidelity in more detail.</p>
<section id="sec-robust-ai-fidelity-b076" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-robust-ai-fidelity-b076">Fidelity</h4>
<p>Fidelity <span class="citation" data-cites="he2020fidelity">(<a href="#ref-he2020fidelity" role="doc-biblioref">He, Balaprakash, and Li 2020</a>)</span> is a tool designed to model hardware faults more accurately within software-based fault injection experiments. Its core goal is to bridge the gap between low-level hardware fault behavior and the higher-level effects observed in machine learning systems by simulating how faults propagate through the compute stack.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2020fidelity" class="csl-entry" role="listitem">
He, Yi, Prasanna Balaprakash, and Yanjing Li. 2020. <span>“FIdelity: Efficient Resilience Analysis Framework for Deep Learning Accelerators.”</span> In <em>2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</em>, 270–81. IEEE; IEEE. <a href="https://doi.org/10.1109/micro50266.2020.00033">https://doi.org/10.1109/micro50266.2020.00033</a>.
</div></div><p>The central insight behind Fidelity is that not all faults need to be modeled individually at the hardware level to yield meaningful results. Instead, Fidelity focuses on how faults manifest at the software-visible state and identifies equivalence relationships that allow representative modeling of entire fault classes. To accomplish this, it relies on several key principles:</p>
<p>First, fault propagation is studied to understand how a fault originating in hardware can move through various layers, including architectural registers, memory hierarchies, and numerical operations, eventually altering values in software. Fidelity captures these pathways to ensure that injected faults in software reflect the way faults would actually manifest in a real system.</p>
<p>Second, the tool identifies fault equivalence, which refers to grouping hardware faults that lead to similar observable outcomes in software. By focusing on representative examples rather than modeling every possible hardware bit flip individually, Fidelity allows more efficient simulations without sacrificing accuracy.</p>
<p>Finally, Fidelity uses a layered modeling approach, capturing the system’s behavior at various abstraction levels—from hardware fault origin to its effect in the ML model’s weights, activations, or predictions. This layering ensures that the impact of hardware faults is realistically simulated in the context of the ML system.</p>
<p>By combining these techniques, Fidelity allows researchers to run fault injection experiments that closely mirror the behavior of real hardware systems, but with the efficiency and flexibility of software-based tools. This makes Fidelity especially valuable in safety-critical settings, where the cost of failure is high and an accurate understanding of hardware-induced faults is essential.</p>
</section>
<section id="sec-robust-ai-capturing-hardware-behavior-a43e" class="level4">
<h4 class="anchored" data-anchor-id="sec-robust-ai-capturing-hardware-behavior-a43e">Capturing Hardware Behavior</h4>
<p>Capturing the true behavior of hardware faults in software-based fault injection tools is critical for advancing the reliability and robustness of ML systems. This fidelity becomes especially important when hardware faults have subtle but significant effects that may not be evident when modeled at a high level of abstraction.</p>
<p>There are several reasons why accurately reflecting hardware behavior is essential. First, accuracy is paramount. Software-based tools that mirror the actual propagation and manifestation of hardware faults provide more dependable insights into how faults influence model behavior. These insights are crucial for designing and validating fault-tolerant architectures and ensuring that mitigation strategies are grounded in realistic system behavior.</p>
<p>Second, reproducibility is improved when hardware effects are faithfully captured. This allows fault injection results to be reliably reproduced across different systems and environments, which is a cornerstone of rigorous scientific research. Researchers can better compare results, validate findings, and ensure consistency across studies.</p>
<p>Third, efficiency is enhanced when fault models focus on the most representative and impactful fault scenarios. Rather than exhaustively simulating every possible bit flip, tools can target a subset of faults that are known, by means of accurate modeling, to affect the system in meaningful ways. This selective approach saves computational resources while still providing comprehensive insights.</p>
<p>Finally, understanding how hardware faults appear at the software level is essential for designing effective mitigation strategies. When researchers know how specific hardware-level issues affect different components of an ML system, they can develop more targeted hardening techniques—such as retraining specific layers, applying redundancy selectively, or improving architectural resilience in bottleneck components.</p>
<p>Tools like Fidelity are central to this effort. By establishing mappings between low-level hardware behavior and higher-level software effects, Fidelity and similar tools empower researchers to conduct fault injection experiments that are not only faster and more scalable, but also grounded in real-world system behavior.</p>
<p>As ML systems continue to increase in scale and are deployed in increasingly safety-critical environments, this kind of hardware-aware modeling will become even more important. Ongoing research in this space aims to further refine the translation between hardware and software fault models and to develop tools that offer both efficiency and realism in evaluating ML system resilience. These advances will provide the community with more powerful, reliable methods for understanding and defending against the effects of hardware faults.</p>
<div id="quiz-question-sec-robust-ai-tools-frameworks-c8a4" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the role of a fault model in evaluating ML system robustness?</p>
<ol type="a">
<li>It predicts the exact time and location of hardware faults.</li>
<li>It describes how hardware faults manifest and affect system behavior.</li>
<li>It provides a complete list of all possible hardware faults.</li>
<li>It eliminates the need for error models in fault injection studies.</li>
</ol></li>
<li><p>Explain the trade-offs between hardware-based and software-based fault injection methods in terms of accuracy and scalability.</p></li>
<li><p>True or False: Software-based fault injection tools can fully replicate the low-level hardware interactions that influence fault propagation in ML systems.</p></li>
<li><p>Discuss how domain-specific fault injection tools, like DriveFI, contribute to evaluating system safety in autonomous vehicles.</p></li>
</ol>
<p><a href="#quiz-answer-sec-robust-ai-tools-frameworks-c8a4" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-robust-ai-summary-cb3f" class="level2">
<h2 class="anchored" data-anchor-id="sec-robust-ai-summary-cb3f">Summary</h2>
<p>The pursuit of robust AI is a multifaceted endeavor that is critical for the reliable deployment of machine learning systems in real-world environments. As ML move from controlled research settings to practical applications, robustness becomes not just a desirable feature but a foundational requirement. Deploying AI in practice means engaging directly with the challenges that can compromise system performance, safety, and reliability.</p>
<p>We examined the broad spectrum of issues that threaten AI robustness, beginning with hardware-level faults. Transient faults may introduce temporary computational errors, while permanent faults, including the well-known Intel FDIV bug, can lead to persistent inaccuracies that affect system behavior over time.</p>
<p>Beyond hardware, machine learning models themselves are susceptible to a variety of threats. Adversarial examples, such as the misclassification of modified stop signs, reveal how subtle input manipulations can cause erroneous outputs. Likewise, data poisoning techniques, exemplified by the Nightshade project, illustrate how malicious training data can degrade model performance or implant hidden backdoors, posing serious security risks in practical deployments.</p>
<p>The chapter also addressed the impact of distribution shifts, which often result from temporal evolution or domain mismatches between training and deployment environments. Such shifts challenge a model’s ability to generalize and perform reliably under changing conditions. Compounding these issues are faults in the software infrastructure, including frameworks, libraries, and runtime components, which can propagate unpredictably and undermine system integrity.</p>
<p>To navigate these risks, the use of robust tools and evaluation frameworks is essential. Tools such as PyTorchFI and Fidelity enable researchers and practitioners to simulate fault scenarios, assess vulnerabilities, and systematically improve system resilience. These resources are critical for translating theoretical robustness principles into operational safeguards.</p>
<p>Ultimately, building robust AI requires a comprehensive and proactive approach. Fault tolerance, security mechanisms, and continuous monitoring must be embedded throughout the AI development lifecycle—from data collection and model training to deployment and maintenance. As this chapter has demonstrated, applying AI in real-world contexts means addressing these robustness challenges head-on to ensure that systems operate safely, reliably, and effectively in complex and evolving environments.</p>


<div id="quiz-question-sec-robust-ai-summary-cb3f" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>Which of the following scenarios best illustrates the challenge of distribution shifts in machine learning systems?</p>
<ol type="a">
<li>A model trained on daytime traffic data fails to perform well at night.</li>
<li>A model experiences a hardware fault due to a transient error.</li>
<li>A model is attacked using adversarial examples to misclassify inputs.</li>
<li>A model’s training data is poisoned with malicious samples.</li>
</ol></li>
<li><p>Explain why fault tolerance and continuous monitoring are essential components of deploying robust AI systems.</p></li>
<li><p>The use of tools like PyTorchFI and Fidelity is crucial for simulating fault scenarios and assessing system _______.</p></li>
</ol>
<p><a href="#quiz-answer-sec-robust-ai-summary-cb3f" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-robust-ai-realworld-applications-d887" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following incidents highlights the impact of human error on cloud-based ML systems?</strong></p>
<ol type="a">
<li>Tesla Model S crash due to Autopilot failure</li>
<li>AWS outage due to incorrect command entry</li>
<li>Facebook’s silent data corruption issue</li>
<li>NASA Mars Polar Lander software error</li>
</ol>
<p><em>Answer</em>: The correct answer is B. The AWS outage exemplifies how human error, specifically an incorrect command during maintenance, can disrupt cloud-based ML systems, emphasizing the need for robust maintenance protocols.</p>
<p><em>Learning Objective</em>: Understand the impact of human error on cloud-based ML systems and the importance of robust maintenance protocols.</p></li>
<li><p><strong>Explain how silent data corruption (SDC) can affect machine learning system performance in large-scale distributed systems.</strong></p>
<p><em>Answer</em>: SDC can lead to undetected errors that propagate through system layers, resulting in data loss and application failures. This can degrade ML system performance by corrupting training data or causing inconsistencies in data pipelines, ultimately compromising model accuracy and reliability.</p>
<p><em>Learning Objective</em>: Analyze the effects of silent data corruption on ML system performance and reliability.</p></li>
<li><p><strong>True or False: The Tesla Model S crash in 2016 was primarily due to a software bug in its object recognition system.</strong></p>
<p><em>Answer</em>: False. The crash was due to the system’s inability to distinguish the trailer against a bright sky, highlighting limitations in AI-based perception systems rather than a specific software bug.</p>
<p><em>Learning Objective</em>: Evaluate the limitations of AI-based perception systems in autonomous vehicles.</p></li>
</ol>
<p><a href="#quiz-question-sec-robust-ai-realworld-applications-d887" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-robust-ai-hardware-faults-81ee" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which type of hardware fault is characterized by its sporadic and unpredictable nature, making it difficult to diagnose?</strong></p>
<ol type="a">
<li>Transient Fault</li>
<li>Permanent Fault</li>
<li>Intermittent Fault</li>
<li>Logical Fault</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Intermittent Fault. Intermittent faults occur sporadically and unpredictably, making them challenging to diagnose and reproduce, unlike transient or permanent faults.</p>
<p><em>Learning Objective</em>: Understand the characteristics of different hardware fault types.</p></li>
<li><p><strong>Explain how transient faults can affect the training phase of a machine learning model.</strong></p>
<p><em>Answer</em>: Transient faults can introduce errors in the memory storing model weights or gradients, leading to incorrect updates and compromising the convergence and accuracy of the training process. This can result in the model learning incorrect patterns or associations.</p>
<p><em>Learning Objective</em>: Analyze the impact of transient faults on ML training processes.</p></li>
<li><p><strong>In ML systems, a common detection technique for hardware faults is the use of ____ codes, which add redundant bits to detect errors.</strong></p>
<p><em>Answer</em>: error detection. Error detection codes, such as parity checks and cyclic redundancy checks, are used to identify errors in data storage and transmission by adding redundant bits.</p>
<p><em>Learning Objective</em>: Identify techniques used for detecting hardware faults in ML systems.</p></li>
<li><p><strong>True or False: Permanent faults in ML systems can be mitigated by using error correction codes alone.</strong></p>
<p><em>Answer</em>: False. Permanent faults require hardware repair or replacement, as error correction codes alone cannot address the persistent nature of these faults.</p>
<p><em>Learning Objective</em>: Evaluate the limitations of error correction codes in addressing permanent faults.</p></li>
<li><p><strong>Order the following steps in handling a detected hardware fault in an ML system: [Implement redundancy, Monitor system, Detect fault, Mitigate fault].</strong></p>
<p><em>Answer</em>: 1. Monitor system: Continuously observe system behavior to identify anomalies. 2. Detect fault: Use detection techniques to identify specific hardware faults. 3. Mitigate fault: Apply appropriate strategies to address the fault. 4. Implement redundancy: Ensure future fault tolerance by incorporating redundancy.</p>
<p><em>Learning Objective</em>: Understand the sequence of actions in handling hardware faults in ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-robust-ai-hardware-faults-81ee" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-robust-ai-model-robustness-f537" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the Fast Gradient Sign Method (FGSM)?</strong></p>
<ol type="a">
<li>A method that perturbs input data by adding noise in random directions.</li>
<li>A technique that uses elastic net regularization to create sparse perturbations.</li>
<li>A gradient-based attack that adds noise in the direction of the gradient to maximize prediction error.</li>
<li>A method that exploits the transferability of adversarial examples across different models.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. FGSM is a gradient-based attack that perturbs input data by adding noise in the direction of the gradient of the loss function, aiming to maximize the model’s prediction error with minimal distortion.</p>
<p><em>Learning Objective</em>: Understand the mechanism of gradient-based adversarial attacks, specifically FGSM.</p></li>
<li><p><strong>Explain how transfer-based attacks can be used in black-box scenarios to fool machine learning models.</strong></p>
<p><em>Answer</em>: Transfer-based attacks exploit the transferability property of adversarial examples, where examples crafted for one model can fool others. In black-box scenarios, attackers use a surrogate model to generate adversarial examples and transfer them to the target model, which they cannot directly access. This approach is effective because adversarial examples often generalize across different models and architectures.</p>
<p><em>Learning Objective</em>: Analyze the application of transfer-based attacks in practical black-box scenarios.</p></li>
<li><p><strong>True or False: Optimization-based attacks like the Carlini and Wagner (C&amp;W) attack are less computationally intensive than gradient-based attacks.</strong></p>
<p><em>Answer</em>: False. Optimization-based attacks like the C&amp;W attack are more computationally intensive than gradient-based attacks because they involve iterative optimization processes to find the smallest perturbation that causes misclassification while maintaining perceptual similarity.</p>
<p><em>Learning Objective</em>: Differentiate between the computational requirements of optimization-based and gradient-based adversarial attacks.</p></li>
<li><p><strong>Adversarial patches are designed to work under varying conditions, such as lighting and viewing angles, making them effective in _______ attacks.</strong></p>
<p><em>Answer</em>: physical-world. Adversarial patches are crafted to deceive ML models in real-world environments, maintaining their effectiveness across different physical conditions.</p>
<p><em>Learning Objective</em>: Identify the characteristics and applications of physical-world adversarial attacks.</p></li>
<li><p><strong>Discuss the potential real-world impact of adversarial attacks on autonomous vehicles.</strong></p>
<p><em>Answer</em>: Adversarial attacks on autonomous vehicles can lead to critical misclassifications, such as interpreting stop signs as speed limit signs. This can result in dangerous driving behaviors, like rolling stops or unintended acceleration, endangering public safety. The attacks exploit model vulnerabilities, highlighting the need for robust defenses in safety-critical applications.</p>
<p><em>Learning Objective</em>: Evaluate the real-world implications of adversarial attacks on safety-critical ML systems like autonomous vehicles.</p></li>
</ol>
<p><a href="#quiz-question-sec-robust-ai-model-robustness-f537" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-robust-ai-software-faults-7c4a" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a common consequence of software faults in ML systems?</strong></p>
<ol type="a">
<li>Improved model accuracy</li>
<li>Increased system reliability</li>
<li>Performance degradation</li>
<li>Enhanced security</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Performance degradation is a common consequence of software faults, often resulting from memory leaks, inefficient resource scheduling, or contention between concurrent threads, leading to increased latency and reduced throughput.</p>
<p><em>Learning Objective</em>: Understand the impact of software faults on the performance of ML systems.</p></li>
<li><p><strong>In ML systems, _______ errors can lead to numerical instability, affecting gradient computations and convergence.</strong></p>
<p><em>Answer</em>: floating-point. Floating-point errors can lead to numerical instability, affecting gradient computations and convergence procedures in ML systems, particularly in optimization routines.</p>
<p><em>Learning Objective</em>: Identify the types of errors that can cause numerical instability in ML systems.</p></li>
<li><p><strong>True or False: Static code analysis tools can help detect runtime errors in ML systems.</strong></p>
<p><em>Answer</em>: False. Static code analysis tools detect potential issues at compile time, such as syntax errors and unsafe operations, but they do not detect runtime errors, which require dynamic testing and monitoring.</p>
<p><em>Learning Objective</em>: Differentiate between static code analysis and runtime monitoring in detecting software faults.</p></li>
</ol>
<p><a href="#quiz-question-sec-robust-ai-software-faults-7c4a" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-robust-ai-tools-frameworks-c8a4" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the role of a fault model in evaluating ML system robustness?</strong></p>
<ol type="a">
<li>It predicts the exact time and location of hardware faults.</li>
<li>It describes how hardware faults manifest and affect system behavior.</li>
<li>It provides a complete list of all possible hardware faults.</li>
<li>It eliminates the need for error models in fault injection studies.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. A fault model describes how hardware faults manifest and affect system behavior, which is crucial for simulating and measuring the impact of faults on ML systems.</p>
<p><em>Learning Objective</em>: Understand the purpose and importance of fault models in evaluating ML system robustness.</p></li>
<li><p><strong>Explain the trade-offs between hardware-based and software-based fault injection methods in terms of accuracy and scalability.</strong></p>
<p><em>Answer</em>: Hardware-based methods offer high accuracy by directly manipulating physical systems, but they are costly and less scalable. Software-based methods are more scalable and flexible, allowing for rapid testing, but they may lack the low-level accuracy of hardware-based approaches.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs between different fault injection methods in terms of accuracy and scalability.</p></li>
<li><p><strong>True or False: Software-based fault injection tools can fully replicate the low-level hardware interactions that influence fault propagation in ML systems.</strong></p>
<p><em>Answer</em>: False. Software-based tools operate at a higher level of abstraction and may not capture all low-level hardware interactions, potentially leading to oversimplified conclusions about fault propagation.</p>
<p><em>Learning Objective</em>: Recognize the limitations of software-based fault injection tools in capturing low-level hardware interactions.</p></li>
<li><p><strong>Discuss how domain-specific fault injection tools, like DriveFI, contribute to evaluating system safety in autonomous vehicles.</strong></p>
<p><em>Answer</em>: Domain-specific tools like DriveFI allow for targeted fault injection into critical components of autonomous vehicles, such as perception and control systems. This helps identify vulnerabilities and assess the system’s resilience under fault conditions, contributing to safer and more reliable vehicle operations.</p>
<p><em>Learning Objective</em>: Apply the concept of domain-specific fault injection tools to evaluate system safety in real-world applications.</p></li>
</ol>
<p><a href="#quiz-question-sec-robust-ai-tools-frameworks-c8a4" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-robust-ai-summary-cb3f" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following scenarios best illustrates the challenge of distribution shifts in machine learning systems?</strong></p>
<ol type="a">
<li>A model trained on daytime traffic data fails to perform well at night.</li>
<li>A model experiences a hardware fault due to a transient error.</li>
<li>A model is attacked using adversarial examples to misclassify inputs.</li>
<li>A model’s training data is poisoned with malicious samples.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. A model trained on daytime traffic data failing at night exemplifies distribution shifts, where the training and deployment environments differ, challenging the model’s ability to generalize.</p>
<p><em>Learning Objective</em>: Understand the impact of distribution shifts on model performance and generalization.</p></li>
<li><p><strong>Explain why fault tolerance and continuous monitoring are essential components of deploying robust AI systems.</strong></p>
<p><em>Answer</em>: Fault tolerance and continuous monitoring are essential because they help detect and mitigate errors and anomalies in real-time, ensuring that AI systems remain reliable and effective despite hardware faults, software bugs, or distribution shifts. These components enable proactive management of risks and maintain system integrity and performance in dynamic environments.</p>
<p><em>Learning Objective</em>: Analyze the role of fault tolerance and monitoring in maintaining AI system robustness.</p></li>
<li><p><strong>The use of tools like PyTorchFI and Fidelity is crucial for simulating fault scenarios and assessing system _______.</strong></p>
<p><em>Answer</em>: vulnerabilities. These tools help practitioners identify weaknesses in AI systems, allowing for targeted improvements in robustness and resilience against potential faults.</p>
<p><em>Learning Objective</em>: Recognize the importance of simulation tools in evaluating and enhancing AI system robustness.</p></li>
</ol>
<p><a href="#quiz-question-sec-robust-ai-summary-cb3f" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="pagination-link" aria-label="On-Device Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">On-Device Learning</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/privacy_security/privacy_security.html" class="pagination-link" aria-label="Security &amp; Privacy">
        <span class="nav-page-text">Security &amp; Privacy</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>