---
bibliography: hw_acceleration.bib
---

# AI Acceleration {#sec-ai_acceleration}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-ai-acceleration-resource), [Videos](#sec-ai-acceleration-resource), [Exercises](#sec-ai-acceleration-resource)
:::

![_DALL·E 3 Prompt: Create an intricate and colorful representation of a System on Chip (SoC) design in a rectangular format. Showcase a variety of specialized machine learning accelerators and chiplets, all integrated into the processor. Provide a detailed view inside the chip, highlighting the rapid movement of electrons. Each accelerator and chiplet should be designed to interact with neural network neurons, layers, and activations, emphasizing their processing speed. Depict the neural networks as a network of interconnected nodes, with vibrant data streams flowing between the accelerator pieces, showcasing the enhanced computation speed._](images/png/cover_ai_hardware.png)

## Purpose {.unnumbered}

_How do hardware acceleration strategies impact machine learning system performance, and what principles should ML engineers understand to effectively design and deploy their systems?_

Machine learning systems has driven a fundamental shift in computer architecture. Traditional processors, designed for general-purpose computing, prove inefficient for the repeated mathematical operations and data movement patterns in neural networks. Modern accelerators address this challenge by matching hardware structures to ML computation patterns. These accelerators introduce fundamental trade-offs in performance, power consumption, and flexibility. Effective utilization of hardware acceleration requires an understanding of these trade-offs, as well as the architectural principles that govern accelerator design. Machine learning engineers must consider how model structure, precision requirements, and computational constraints influence accelerator performance. By optimizing models for specific hardware platforms, engineers can balance computational efficiency with deployment constraints, whether training in large-scale data centers or performing inference on resource-constrained edge devices.

::: {.callout-tip title="Learning Objectives"}

* Understand the historical context of hardware acceleration.

* Identify key AI compute primitives and their role in model execution.

* Explain the memory hierarchy and its impact on AI accelerator performance.

* Describe strategies for mapping neural networks to hardware.

* Analyze the role of compilers and runtimes in optimizing AI workloads.

* Compare single-chip and multi-chip AI architectures.
:::

## Overview

The increasing demand for machine learning workloads imposes significant computational challenges that exceed the capabilities of traditional general-purpose architectures. These workloads require highly parallel computation, efficient memory access, and real-time processing capabilities, making traditional CPUs inefficient due to their sequential execution model and limited parallelism. This fundamental mismatch has driven the development of Machine Learning Accelerators (ML Accelerators)—specialized hardware designed to optimize ML workloads for throughput, latency, and energy efficiency.

::: {.callout-note title="Definition of ML Accelerator"}  

Machine Learning Accelerator (ML Accelerator) refers to a *specialized computing hardware* designed to *efficiently execute machine learning workloads*. These accelerators optimize *matrix multiplications, tensor operations, and data movement*, enabling *high-throughput and energy-efficient* computation. ML accelerators operate at various *power and performance scales*, ranging from *edge devices with milliwatt-level consumption* to *data center-scale accelerators requiring kilowatts of power*. They are specifically designed to address *the computational and memory demands* of machine learning models, often incorporating *optimized memory hierarchies, parallel processing units, and custom instruction sets* to maximize performance. ML accelerators are widely used in *training, inference, and real-time AI applications* across cloud, edge, and embedded systems.  

:::  

Hardware acceleration for AI involves multiple architectural approaches, each balancing trade-offs in performance, power efficiency, and programmability. Graphics Processing Units (GPUs) use thousands of parallel compute cores to achieve significant speedup for data-parallel operations compared to CPUs. Tensor Processing Units (TPUs) implement specialized dataflow architectures optimized for matrix multiplication. Field Programmable Gate Arrays (FPGAs) provide reconfigurable logic that allows workload-specific optimization. Understanding these core design principles and their trade-offs is essential for system designers aiming to develop efficient AI acceleration strategies.

The evolution of AI accelerators reflects broader shifts in computer architecture. Traditional von Neumann designs separate processing and memory, creating data movement bottlenecks for AI computation. Moving data between DRAM and processing elements often consumes far more energy than the computation itself. Modern accelerators seek to overcome these limitations through processing-in-memory, near-memory computing, and custom dataflow architectures that minimize data movement costs while maximizing computational throughput. These innovations work in conjunction with advances in software, including optimized compilers, runtime environments, and framework-specific acceleration techniques.

This chapter explores AI acceleration from a systems perspective, focusing on how hardware acceleration integrates with the broader machine learning stack. While architectural principles remain a foundational element, the emphasis is on the interplay between algorithmic requirements, software optimization, and hardware capabilities that determine real-world acceleration effectiveness.

## Hardware Evolution

The progression of computing architectures follows a recurring pattern: as computational workloads grow in complexity, general-purpose processors become increasingly inefficient, prompting the development of specialized hardware accelerators. This transition is driven by the need for higher computational efficiency, reduced energy consumption, and optimized execution of domain-specific workloads. Machine learning acceleration is the latest stage in this ongoing evolution, following a well-established trajectory observed in prior domains such as floating-point arithmetic, graphics processing, and digital signal processing.

Hardware specialization involves the targeted optimization of frequently executed computational patterns through dedicated circuit implementations. This approach enhances performance and efficiency but introduces trade-offs in flexibility, silicon area utilization, and programming complexity. The development of specialized accelerators typically progresses through distinct phases: identifying computational bottlenecks, designing dedicated processing elements, refining hardware-software interfaces, and ultimately integrating specialized capabilities into general-purpose architectures.

The historical trajectory of hardware specialization provides a foundational perspective for understanding modern machine learning accelerators. Many of the principles that shaped the development of early floating-point and graphics accelerators now inform the design of AI-specific hardware. Examining these past trends offers a systematic framework for analyzing contemporary approaches to AI acceleration and anticipating future developments in specialized computing.

### Specialized Computing

The transition toward specialized computing architectures arises from the fundamental limitations of general-purpose processors. Early computing systems relied on central processing units (CPUs) to execute all computational tasks sequentially, following a one-size-fits-all approach. However, as computing workloads became more diverse, certain operations—particularly floating-point arithmetic—emerged as performance bottlenecks that could not be efficiently handled by CPUs alone. These inefficiencies prompted the development of specialized hardware designed to accelerate specific computational patterns.

One of the earliest examples of hardware specialization was the Intel 8087 mathematics coprocessor, introduced in 1980. This floating-point unit (FPU) was designed to offload arithmetic-intensive computations from the main CPU, significantly improving performance for scientific and engineering applications. The 8087 achieved performance gains of up to 100× for floating-point operations compared to software-based implementations running on general-purpose processors. This milestone established a key principle: hardware specialization provides substantial benefits for well-defined, computationally intensive tasks by executing them more efficiently than general-purpose hardware.

The success of floating-point coprocessors led to their eventual integration into mainstream processors. For example, the Intel 486DX, released in 1989, incorporated an on-chip floating-point unit, eliminating the need for an external coprocessor. This integration not only improved processing efficiency but also marked a recurring pattern in computer architecture: successful specialized functions tend to become standard features in future generations of general-purpose processors.

The principles established through early floating-point acceleration continue to influence modern hardware specialization. These include:

1. Identification of computational bottlenecks through workload analysis
2. Development of specialized circuits for frequent operations
3. Creation of efficient hardware-software interfaces
4. Progressive integration of proven specialized functions

This progression from domain-specific specialization to general-purpose integration has played a central role in shaping modern computing. As computational workloads expanded beyond arithmetic operations, these same principles were applied to new domains, such as graphics processing, digital signal processing, and ultimately, machine learning acceleration. Each of these domains introduced specialized architectures tailored to their unique computational requirements, reinforcing the role of hardware specialization as a key strategy for improving performance and efficiency in evolving workloads.

### Expanding Specialized Computing

The principles established through floating-point acceleration provided a blueprint for addressing emerging computational challenges. As computing applications diversified, new computational patterns emerged that exceeded the capabilities of general-purpose processors. This expansion of specialized computing manifested across multiple domains, each contributing unique insights to hardware acceleration strategies.

Graphics processing emerged as a significant driver of hardware specialization in the 1990s. Early graphics accelerators focused on specific operations like bitmap transfers and polygon filling. The introduction of programmable graphics pipelines with NVIDIA's GeForce 256 in 1999 represented a crucial advancement in specialized computing. Graphics Processing Units (GPUs) demonstrated how parallel processing architectures could efficiently handle data-parallel workloads, achieving orders of magnitude improvement over CPU implementations for graphics operations.

Digital Signal Processing (DSP) represents another fundamental domain of hardware specialization. DSP processors introduced architectural innovations specifically designed for efficient signal processing operations. These included specialized multiply-accumulate units, circular buffers, and parallel data paths optimized for filtering and transform operations. Texas Instruments' TMS32010, introduced in 1983, established how domain-specific instruction sets and memory architectures could dramatically improve performance for signal processing applications.

Network processing introduced additional patterns of specialization. Network processors developed unique architectures to handle packet processing at line rate, incorporating multiple processing cores, specialized packet manipulation units, and sophisticated memory management systems. Intel's IXP2800 network processor demonstrated how multiple levels of hardware specialization could be combined to address complex processing requirements.

These diverse domains of specialization shared several common themes:

1. Identification of domain-specific computational patterns
2. Development of specialized processing elements and memory hierarchies
3. Creation of domain-specific programming models
4. Progressive evolution toward more flexible architectures

This period of expanding specialization demonstrated that hardware acceleration strategies could successfully address diverse computational requirements. The lessons learned from these domains would prove crucial for the development of modern accelerators, particularly in the emerging field of machine learning computation.

### Domain-Specific Architectures 

The emergence of domain-specific architectures (DSA) marks a fundamental shift in computer system design, driven by two key factors: the breakdown of traditional scaling laws and the increasing computational demands of specialized workloads. The slowdown of Moore's Law, which previously ensured regular improvements in transistor density, and the end of Dennard scaling, which limited further frequency scaling due to power constraints, created a performance and efficiency bottleneck in general-purpose computing. As John Hennessy and David Patterson noted in their 2017 Turing Lecture, these limitations signaled the onset of a new era in computer architecture—one centered on domain-specific solutions that optimize hardware for specialized workloads.

Historically, improvements in processor performance relied on semiconductor process scaling and increasing clock speeds. However, as power density limitations restricted further frequency scaling, and as transistor miniaturization faced increasing physical and economic constraints, architects were forced to explore alternative approaches to sustain computational growth. The result was a shift toward domain-specific architectures, which dedicate silicon resources to optimize computation for specific application domains, trading flexibility for efficiency.

Domain-specific architectures achieve superior performance and energy efficiency through several key principles:

1. Customized datapaths that match the computational patterns of target applications
2. Specialized memory hierarchies optimized for domain-specific data access patterns
3. Reduced instruction overhead through domain-specific instruction sets
4. Hardware structures that directly implement frequently used operations

One of the most successful applications of domain-specific architecture can be seen in multimedia processing. Modern smartphones, introduced in the late 2000s, can decode high-definition video for extended durations—often exceeding 20 hours—while consuming minimal battery power, despite the high computational demands of video playback. This efficiency is achieved through dedicated hardware video codecs that implement industry standards such as H.264/AVC (introduced in 2003) and H.265/HEVC (finalized in 2013). These specialized circuits provide orders-of-magnitude improvements in performance and power efficiency compared to software-based decoding on general-purpose processors. A similar trend can be observed in audio processing, where dedicated decoders efficiently handle compressed formats like AAC (standardized in 1997) and MP3 (introduced in the early 1990s), enabling high-quality playback with minimal power consumption.

The success of multimedia accelerators set a precedent for broader adoption of domain-specific architectures, paving the way for specialized computing in emerging domains. A prominent example is Google's Tensor Processing Unit (TPU), introduced in 2016, which demonstrated the significant benefits of domain-specific architectures for machine learning workloads. The TPU's systolic array architecture, optimized specifically for matrix multiplication, exemplifies how identifying and targeting computational bottlenecks can yield substantial gains in both performance and energy efficiency. Unlike general-purpose processors, which must execute a wide variety of operations, the TPU is designed to maximize throughput for tensor operations, making it highly effective for machine learning inference and training.

The trend toward specialization continues to accelerate, with new architectures emerging for an expanding range of domains. Genomics processing, for example, benefits from custom accelerators that optimize sequence alignment and variant calling, reducing the time required for DNA analysis. Similarly, blockchain computation has given rise to application-specific integrated circuits (ASICs) optimized for cryptographic hashing, dramatically increasing the efficiency of mining operations. These examples illustrate that domain-specific architecture is not merely a transient trend but a fundamental transformation in computing systems, offering tailored solutions that address the growing complexity and diversity of modern computational workloads.

### ML as a Computational Domain  

The increasing complexity of modern workloads has led to the widespread adoption of domain-specific architectures. Among the many fields benefiting from specialized computing, machine learning has emerged as one of the most computationally demanding domains, making it a natural candidate for dedicated hardware acceleration.  

Machine learning workloads exhibit distinct computational characteristics that align well with domain-specific hardware optimizations. The core operations in deep neural networks rely on structured parallelism and high numerical intensity, particularly in the form of dense matrix multiplications and tensor operations. These computational patterns share similarities with those found in graphics processing and digital signal processing, both of which previously led to the development of GPUs and DSPs as dedicated accelerators. Similarly, the increasing efficiency demands of machine learning have led to the design of specialized ML accelerators, tailored to address the unique challenges of training and inference.  

Machine learning workloads impose different computational demands depending on whether they are in the training or inference phase.  

- Training requires both forward and backward propagation, necessitating extensive memory resources to store intermediate activations, gradients, and weight updates. This phase also relies on higher numerical precision (e.g., FP32 or FP16) to ensure stable gradient updates and proper model convergence.  
- Inference, in contrast, involves only forward propagation, which allows for reduced precision arithmetic (e.g., INT8 or lower) while still maintaining acceptable accuracy. This enables optimizations that significantly reduce computational costs and improve energy efficiency.  

Despite these differences, both training and inference share a common computational foundation, as their workloads are dominated by dense matrix multiplications and constrained by memory bandwidth limitations. The efficiency of machine learning accelerators is therefore determined not only by their raw computational power but also by their ability to efficiently manage data movement between processing units and memory hierarchies.  

A defining characteristic of machine learning acceleration is the flexibility in numerical precision. While training benefits from higher precision arithmetic to maintain gradient stability, inference can be executed at lower precision, often without significant accuracy loss. This observation has driven the development of mixed-precision arithmetic hardware, enabling accelerators to dynamically adjust numerical precision based on computational needs. These optimizations improve both performance efficiency and power consumption, making them particularly valuable for large-scale training and real-time inference applications.  

These computational patterns have guided the development of various ML acceleration approaches. Graphics Processing Units (GPUs), originally designed for parallel graphics rendering, extended their capabilities to machine learning by incorporating tensor cores, specialized memory hierarchies, and high-throughput execution engines. The same parallelism that made GPUs well-suited for image processing proved equally effective for deep neural network computations. Google's Tensor Processing Unit (TPU) represents a more specialized approach to machine learning acceleration. The first-generation TPU, introduced in 2016, was optimized for inference, emphasizing low-latency execution and energy efficiency. Later generations expanded to support training workloads, integrating higher precision compute units and optimized memory architectures. The TPU's systolic array architecture, specifically designed for matrix multiplication, illustrates how domain-specific design can maximize computational efficiency by aligning hardware capabilities with the mathematical structure of neural networks.  

The success of machine learning accelerators reinforces the broader shift toward domain-specific computing. Just as dedicated video codecs enabled efficient multimedia playback on mobile devices, machine learning accelerators are now enabling low-power neural network inference on edge devices, while high-performance training accelerators facilitate rapid model development in large-scale data centers. The continued evolution of ML hardware reflects a broader trend in computing: as computational domains grow in complexity, hardware specialization follows, ensuring that systems remain efficient, scalable, and adaptable to future advancements.  

### Application-specific Specialization

The evolution of specialized hardware architectures illustrates a fundamental principle in computing systems: as computational patterns emerge and mature, hardware specialization follows to achieve optimal performance and energy efficiency. This progression is particularly evident in machine learning acceleration, where domain-specific architectures have evolved to meet the increasing computational demands of machine learning models. Unlike general-purpose processors, which prioritize flexibility, specialized accelerators optimize execution for well-defined workloads, balancing performance, energy efficiency, and integration with software frameworks.

@tbl-hw-evolution outlines key milestones in hardware specialization, highlighting how each computing era has produced accelerators tailored to dominant workloads. This historical trajectory provides context for the rise of AI accelerators, which follow similar design principles but must also integrate seamlessly with machine learning frameworks, compilers, and deployment environments to maximize efficiency.

+-------+------------------------------------+---------------------------------------------+------------------------------------+
| Era   | Computational Pattern              | Architecture Examples                       | Key Characteristics                |
+:======+:===================================+:============================================+:===================================+
| 1980s | Floating-Point & Signal Processing | FPU, DSP                                    | • Single-purpose engines<br>       |
|       |                                    |                                             | • Focused instruction sets<br>     |
|       |                                    |                                             | • Coprocessor interfaces           |
+-------+------------------------------------+---------------------------------------------+------------------------------------+
| 1990s | 3D Graphics & Multimedia           | GPU, SIMD Units                             | • Many identical compute units<br> |
|       |                                    |                                             | • Regular data patterns<br>        |
|       |                                    |                                             | • Wide memory interfaces           |
+-------+------------------------------------+---------------------------------------------+------------------------------------+
| 2000s | Real-time Media Coding             | Media Codecs, Network Processors            | • Fixed-function pipelines<br>     |
|       |                                    |                                             | • High throughput processing<br>   |
|       |                                    |                                             | • Power-performance optimization   |
+-------+------------------------------------+---------------------------------------------+------------------------------------+
| 2010s | Deep Learning Tensor Operations    | TPU, GPU Tensor Cores                       | • Matrix multiplication units<br>  |
|       |                                    |                                             | • Massive parallelism<br>          |
|       |                                    |                                             | • Memory bandwidth optimization    |
+-------+------------------------------------+---------------------------------------------+------------------------------------+
| 2020s | Application-Specific Acceleration  | ML Engines, Smart NICs, Domain Accelerators | • Workload-specific datapaths<br>  |
|       |                                    |                                             | • Customized memory hierarchies<br>|
|       |                                    |                                             | • Application-optimized designs    |
+-------+------------------------------------+---------------------------------------------+------------------------------------+

: Evolution of hardware specialization across computing eras. {#tbl-hw-evolution .striped .hover}

This historical progression reveals that hardware specialization is not a recent phenomenon but rather a consistent approach to improving computational efficiency. As new workloads become dominant, specialized architectures emerge to optimize their execution, balancing raw performance with power efficiency and software compatibility.

In the case of AI acceleration, this transition has introduced new challenges that extend beyond hardware design. Machine learning accelerators must integrate effectively into ML workflows, requiring optimizations at multiple levels of the computing stack, including:

* **Framework Compatibility:** ML accelerators must be accessible through widely used frameworks such as TensorFlow, PyTorch, and JAX, ensuring seamless deployment across different hardware platforms.
* **Compiler and Runtime Support:** Optimization techniques such as graph-level transformations, kernel fusion, and memory scheduling play a critical role in achieving high performance on specialized accelerators.
* **Scalability Across Environments:** AI accelerators are deployed across data centers, edge devices, and mobile platforms, requiring different levels of performance tuning and energy efficiency optimizations.
* **Interoperability with Existing Systems:** Many ML workloads rely on heterogeneous computing, where specialized accelerators must work alongside CPUs and GPUs in distributed environments.

The emergence of AI accelerators is therefore not simply a matter of hardware optimization but also a system-level transformation, where improvements in computation must be tightly coupled with advances in compilers, software frameworks, and distributed computing strategies. Understanding these principles is essential for designing and deploying efficient machine learning systems. The following sections explore how modern ML accelerators address these challenges, focusing on their architectural approaches, system-level optimizations, and integration into the broader machine learning ecosystem.

## AI Compute Primitives

Neural network computations exhibit distinctive computational patterns that emerge from their mathematical foundations. These patterns fundamentally differ from traditional computing workloads in several key aspects. While general-purpose computing often involves complex control flow and scalar operations[^fn-scalar], machine learning workloads are characterized by highly structured, repetitive operations that process large arrays of data in parallel. Understanding these patterns reveals why specific processor primitives become essential for efficient execution.

[^fn-scalar]: **Scalar Operations** Computations that involve single values rather than vectors or matrices.  

When implementing neural networks in hardware, four fundamental requirements emerge:

1. Efficient parallel processing of independent data elements
2. Structured coordination of computation across multiple dimensions
3. Systematic movement of data through memory hierarchies
4. Hardware acceleration of non-linear mathematical functions

These requirements drive the development of specialized processor primitives that form the foundation of modern AI accelerators. The sections that follow examine four critical categories of architectural primitives:

```python
# High-level framework code
dense = Dense(512)(input_tensor)
```

This framework-level abstraction decomposes into mathematical operations:

```python
# Mathematical operations
output = matmul(input_weights) + bias
output = activation(output)
```

The mathematical representation further decomposes into processor-level computation:

```python
# Computational implementation
for n in range(batch_size):
    for m in range(output_size):
        sum = bias[m]
        for k in range(input_size):
            sum += input[n,k] * weights[k,m]
        output[n,m] = activation(sum)
```

Analysis of this computational decomposition reveals four fundamental characteristics:

1. Data parallelism across independent elements, enabling vector processing
2. Structured matrix operations that dominate computational complexity
3. Systematic data movement patterns that influence memory system design
4. Recurring non-linear transformations requiring specialized hardware support

The implementation of hardware primitives to accelerate these patterns depends on three primary criteria:

1. Utilization frequency sufficient to justify dedicated silicon area
2. Performance or efficiency advantages over general-purpose implementation
3. Architectural stability across multiple generations of neural network models

The following sections examine four categories of processor primitives that have emerged as essential building blocks for neural network acceleration: vector operations, matrix operations, and special function units. Each category addresses specific computational requirements while complementing the capabilities of the others. Together, these primitives form the foundation of modern machine learning accelerators.

### Vector Operations

Neural network computations contain inherent parallelism at multiple scales, from individual neurons to entire layers. Vector operations accelerate these computations by processing multiple data elements simultaneously, forming the foundation of efficient neural network execution. By examining how framework-level code translates to hardware instructions, we can understand the critical role of vector processing in neural accelerators.

#### Framework to Hardware Execution

Machine learning frameworks hide hardware complexity through high-level abstractions. These abstractions decompose into progressively lower-level operations, revealing opportunities for hardware acceleration. Consider the execution flow of a linear layer:

```python
# Framework Level: What ML developers write
layer = nn.Linear(256, 512)  # Layer transforms 256 inputs to 512 outputs
output = layer(input_tensor) # Process a batch of inputs
```

This abstraction represents a fully connected layer that transforms input features through learned weights. The framework translates this high-level expression into mathematical operations:

```python
# Framework Internal: Mathematical operations
Z = matmul(weights, input) + bias    # Each output needs all inputs
output = activation(Z)               # Transform each result
```

These mathematical operations decompose into explicit computational steps during processor execution. Each output value requires a sequence of multiply-accumulate operations:

```python
# Computational Level: Implementation
for batch in range(32):              # Process 32 samples at once
    for out_neuron in range(512):    # Compute each output neuron
        sum = 0.0
        for in_feature in range(256): # Each output needs all inputs
            sum += input[batch, in_feature] * weights[out_neuron, in_feature]
        output[batch, out_neuron] = activation(sum + bias[out_neuron])
```

#### Sequential Execution on Scalar Processors

Traditional scalar processors execute these operations sequentially, processing individual values one at a time. For a batch of 32 samples, computing the layer outputs requires over 4 million multiply-accumulate operations. Each operation involves loading an input value and a weight value, multiplying them, and accumulating the result. This sequential approach becomes highly inefficient when processing the massive number of identical operations required by neural networks.

Vector processing units transform this execution pattern by operating on multiple data elements simultaneously. The following RISC-V assembly code demonstrates modern vector processing:

```assembly
# Vector hardware execution (RISC-V Vector Extension)
vsetvli t0, a0, e32   # Process 8 elements at once
loop_batch:
    loop_neuron:
        vxor.vv v0, v0, v0    # Clear 8 accumulators
        loop_feature:
            vle32.v v1, (in_ptr)   # Load 8 inputs together
            vle32.v v2, (wt_ptr)   # Load 8 weights together
            vfmacc.vv v0, v1, v2   # 8 multiply-adds at once
            add in_ptr, in_ptr, 32  # Move to next 8 inputs
            add wt_ptr, wt_ptr, 32  # Move to next 8 weights
            bnez feature_cnt, loop_feature
```

#### Parallel Execution with Vector Processing

This vector implementation processes eight data elements in parallel, reducing both computation time and energy consumption. Vector load instructions transfer eight values simultaneously, maximizing memory bandwidth utilization. The vector multiply-accumulate instruction processes eight pairs of values in parallel, dramatically reducing the total instruction count from over 4 million to approximately 500,000.

Modern vector processors support additional specialized operations that accelerate common neural network patterns. @tbl-vector summarizes key vector operations and their applications in neural network computation:

+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Vector Operation        | Description                                         | Neural Network Application                  |
+:========================+:====================================================+:============================================+
| Reduction               | Combines elements across a vector (e.g., sum, max)  | Pooling layers, attention score computation |
+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Gather                  | Loads multiple non-consecutive memory elements      | Embedding lookups, sparse operations        |
+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Scatter                 | Writes to multiple non-consecutive memory locations | Gradient updates for embeddings             |
+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Masked operations       | Selectively operates on vector elements             | Attention masks, padding handling           |
+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Vector-scalar broadcast | Applies scalar to all vector elements               | Bias addition, scaling operations           |
+-------------------------+-----------------------------------------------------+---------------------------------------------+

: Vector operations and their neural network applications. {#tbl-vector .striped .hover}

The efficiency gains from vector processing extend beyond instruction count reduction. Memory bandwidth utilization improves as vector loads transfer multiple values per operation. Energy efficiency increases because control logic is shared across multiple operations. These improvements compound across the deep layers of modern neural networks, where billions of operations execute for each forward pass.

Vector operations establish the foundation for neural network acceleration by enabling efficient parallel processing of independent data elements. However, the core transformations in neural networks require coordinating computation across multiple dimensions simultaneously. This need for structured parallel computation leads to the next architectural primitive: matrix operations.

#### Historical Foundations of Vector Processing

The principles underlying vector operations have long played a central role in high-performance computing. In the 1970s and 1980s, vector processors emerged as a critical architectural solution for scientific computing, weather modeling, and physics simulations, where large arrays of data required efficient parallel processing. Early systems such as the Cray-1, one of the first commercially successful supercomputers, introduced dedicated vector units to perform arithmetic operations on entire data vectors in a single instruction. This approach dramatically improved computational throughput compared to traditional scalar execution.

These foundational concepts have reemerged in the context of machine learning, where neural networks exhibit an inherent structure well suited to vectorized execution. The same fundamental operations—vector addition, multiplication, and reduction—that once accelerated numerical simulations now drive the execution of machine learning workloads. While the scale and specialization of modern AI accelerators differ from their historical predecessors, the underlying architectural principles remain the same. The resurgence of vector processing in neural network acceleration highlights its enduring utility as a mechanism for achieving high computational efficiency.

Vector operations establish the foundation for neural network acceleration by enabling efficient parallel processing of independent data elements. However, the core transformations in neural networks require coordinating computation across multiple dimensions simultaneously. This need for structured parallel computation leads to the next architectural primitive: matrix operations.

### Matrix Operations

Matrix operations are the computational workhorse of neural networks, transforming high-dimensional data through structured patterns of weights, activations, and gradients. While vector operations process elements independently, matrix operations orchestrate computations across multiple dimensions simultaneously. Understanding these operations reveals fundamental patterns that drive hardware acceleration strategies.

#### Matrix Operations in Neural Networks

Neural network computations decompose into hierarchical matrix operations. A linear layer illustrates this hierarchy:

```python
# Framework Level: What ML developers write
layer = nn.Linear(256, 512)  # Layer transforms 256 inputs to 512 outputs
output = layer(input_batch)  # Process a batch of 32 samples

# Framework Internal: Core operations
Z = matmul(weights, input)   # Matrix: transforms [256 x 32] input to [512 x 32] output
Z = Z + bias                 # Vector: adds bias to each output independently
output = relu(Z)             # Vector: applies activation to each element independently
```

This computation illustrates the scale of matrix operations in neural networks. Each output neuron (512 total) must process all input features (256 total) for every sample in the batch (32 samples). The weight matrix alone contains 256 × 512 = 131,072 parameters that define these connections, demonstrating why efficient matrix multiplication becomes crucial for performance.

#### Types of Matrix Computations in Neural Networks

Matrix operations appear consistently across modern neural architectures. Consider these fundamental patterns:

```python
# Linear Layers - Direct matrix multiply
hidden = matmul(weights, inputs)    # weights: [out_dim x in_dim], inputs: [in_dim x batch]
                                   # Result combines all inputs for each output

# Attention Mechanisms - Multiple matrix operations
Q = matmul(Wq, inputs)       # Project inputs to query space [query_dim x batch]
K = matmul(Wk, inputs)       # Project inputs to key space [key_dim x batch]
attention = matmul(Q, K.T)   # Compare all queries with all keys [query_dim x key_dim]

# Convolutions - Matrix multiply after reshaping
patches = im2col(input)           # Convert [H x W x C] image to matrix of patches
output = matmul(kernel, patches)  # Apply kernels to all patches simultaneously
```

This pervasive pattern of matrix multiplication has direct implications for hardware design. Modern processors implement dedicated matrix units that extend beyond vector processing capabilities:

#### Hardware Acceleration of Matrix Operations

The computational demands of matrix operations have driven specialized hardware optimizations. Modern processors implement dedicated matrix units that extend beyond vector processing capabilities. Consider the following example of matrix acceleration in hardware:

```assembly
# Matrix processing unit operation for a block of the computation
mload mr1, (weight_ptr)     # Load e.g., 16x16 block of weight matrix 
mload mr2, (input_ptr)      # Load corresponding input block
matmul.mm mr3, mr1, mr2     # Multiply and accumulate entire blocks at once
mstore (output_ptr), mr3    # Store computed output block
```

These matrix operations complement vectorized computation by enabling structured many-to-many transformations.

#### Comparison of Matrix and Vector Operations

The interplay between matrix and vector operations shapes the efficiency of neural network execution. Table @tbl-matrix summarizes the distinct characteristics of these operations.

+-------------------+-------------------------+--------------------------+-------------------------------------------------+
| Operation Type    | Best For                | Examples                 | Key Characteristic                              |
+:==================+:========================+:=========================+:================================================+
|                   |                         | • Layer transformations  |                                                 |
| Matrix Operations | Many-to-many transforms | • Attention computation  | Each output depends on multiple inputs          |
|                   |                         | • Convolutions           |                                                 |
+-------------------+-------------------------+--------------------------+-------------------------------------------------+
|                   |                         | • Activation functions   |                                                 |
| Vector Operations | One-to-one transforms   | • Layer normalization    | Each output depends only on corresponding input |
|                   |                         | • Element-wise gradients |                                                 |
+-------------------+-------------------------+--------------------------+-------------------------------------------------+

: Comparison of matrix and vector operation characteristics. {#tbl-matrix .striped .hover}

This complementary relationship guides hardware design decisions. Accelerators provide dedicated matrix units for intensive many-to-many computations while maintaining efficient vector units for the numerous element-wise operations that connect them. However, achieving peak performance requires careful orchestration of data movement between these units, leading to our next consideration: dataflow patterns.

#### Historical Foundations of Matrix Compuation

Matrix operations have long served as a cornerstone of computational mathematics, with applications extending from numerical simulations to graphics processing. The structured nature of matrix multiplications and transformations made them a natural target for acceleration in early computing architectures. In the 1980s and 1990s, specialized digital signal processors (DSPs) and graphics processing units (GPUs) optimized for matrix computations played a critical role in accelerating workloads such as image processing, scientific computing, and 3D rendering.

The widespread adoption of machine learning has reinforced the importance of efficient matrix computation. Neural networks, fundamentally built on matrix multiplications and tensor operations, have driven the development of dedicated hardware architectures that extend beyond traditional vector processing. Modern tensor processing units (TPUs) and AI accelerators implement matrix multiplication at scale, reflecting the same architectural principles that once underpinned early scientific computing and graphics workloads. The resurgence of matrix-centric architectures highlights the deep connection between classical numerical computing and contemporary AI acceleration.

While vector and matrix operations form the computational backbone of neural networks, machine learning workloads also require efficient implementations of non-linear functions such as activation functions, normalization layers, and exponential transformations. These functions introduce computational challenges that demand dedicated hardware support. The next section examines special function units (SFUs) and their role in accelerating non-linear operations.

### Special Function Units

Neural networks combine linear transformations with non-linear functions to model complex patterns. While vector and matrix units efficiently handle linear operations, non-linear functions present unique computational challenges. This section examines how Special Function Units (SFUs) accelerate these essential non-linear computations through dedicated hardware implementations.

#### Non-Linear Functions

Non-linear functions play a fundamental role in machine learning by enabling neural networks to model complex relationships. Consider a typical neural network layer sequence:Edit

```python
# Framework Level Operation
layer = nn.Sequential(
    nn.Linear(256, 512),
    nn.ReLU(),
    nn.BatchNorm1d(512)
)
output = layer(input_tensor)
```

This sequence introduces multiple non-linear transformations. The framework decomposes it into mathematical operations:

```python
# Mathematical Operations
Z = matmul(weights, input) + bias    # Linear transformation
H = max(0, Z)                        # ReLU activation
mean = reduce_mean(H, axis=0)        # BatchNorm statistics
var = reduce_mean((H - mean)**2)     # Variance computation
output = gamma * (H - mean)/sqrt(var + eps) + beta  # Normalization
```

#### Implementing the Non-Linear Functions

On traditional processors, these seemingly simple mathematical operations translate into complex sequences of instructions. Consider the computation of batch normalization: calculating the square root requires multiple iterations of numerical approximation, while exponential functions in operations like softmax need series expansion or lookup tables. Even a simple ReLU activation requires conditional branching, which can disrupt instruction pipelining:

```python
# Traditional Implementation Overhead
for batch in range(32):
    for feature in range(512):
        # ReLU: Requires branch prediction and potential pipeline stalls
        z = matmul_output[batch, feature]
        h = max(0.0, z)    # Conditional operation
        
        # BatchNorm: Multiple passes over data
        mean_sum[feature] += h        # First pass for mean
        var_sum[feature] += h * h     # Additional pass for variance
        
        temp[batch, feature] = h      # Extra memory storage needed

# Normalization requires complex arithmetic
for feature in range(512):
    mean = mean_sum[feature] / batch_size
    var = (var_sum[feature] / batch_size) - mean * mean
    
    # Square root computation: Multiple iterations
    scale = gamma[feature] / sqrt(var + eps)  # Iterative approximation
    shift = beta[feature] - mean * scale
    
    # Additional pass over data for final computation
    for batch in range(32):
        output[batch, feature] = temp[batch, feature] * scale + shift
```

These operations introduce several key inefficiencies:

1. Multiple passes over data, increasing memory bandwidth requirements
2. Complex arithmetic requiring many instruction cycles
3. Conditional operations that can cause pipeline stalls
4. Additional memory storage for intermediate results
5. Poor utilization of vector processing units

More specfiically, the code must scan through data multiple times. For example, batch normalization requires one pass to compute the mean (`mean_sum[feature] += h`), another for variance computation (`var_sum[feature] += h * h`), and a final pass for the output transformation. Each pass requires loading and storing data through the memory hierarchy. Operations that appear simple in mathematical notation often expand into many instructions. The square root in `scale = gamma[feature] / sqrt(var + eps)` typically requires 10-20 iterations of numerical methods like Newton-Raphson approximation to achieve suitable precision. Conditional operations like ReLU's `h = max(0.0, z)` require branch instructions. Modern processors use pipelining to process multiple instructions simultaneously, but branches can force the pipeline to stall while waiting for the condition to be evaluated. The code requires temporary storage (like `temp[batch, feature] = h`) to hold intermediate values between passes. This not only increases memory usage but also adds additional load/store operations that consume bandwidth and energy. While vector units excel at regular, streaming computations, functions like exponentials and square roots often require scalar operations that cannot fully utilize vector processing capabilities. This leads to idle computational resources and reduced efficiency. 

#### Hardware Acceleration

Special Function Units (SFUs) address these inefficiencies through dedicated hardware implementation. Modern ML accelerators include specialized circuits that transform these complex operations into single-cycle or fixed-latency computations. The accelerator can load a vector of values and apply non-linear functions directly, eliminating the need for multiple passes and complex instruction sequences:

```assembly
# Example hardware execution with Special Function Units
vld.v v1, (input_ptr)      # Load vector of values
vrelu.v v2, v1             # Single-cycle ReLU on entire vector
vsigm.v v3, v1             # Fixed-latency sigmoid computation
vtanh.v v4, v1             # Direct hardware tanh implementation
vrsqrt.v v5, v1            # Fast reciprocal square root
```

Each SFU implements a specific function through specialized circuitry. For instance, a ReLU unit performs the comparison and selection in dedicated logic, eliminating branching overhead. Square root operations use hardware implementations of algorithms like Newton-Raphson with fixed iteration counts, providing guaranteed latency. Exponential and logarithmic functions often combine small lookup tables with hardware interpolation circuits. Using these custom instructions, the SFU implementation eliminates multiple passes over data, removes complex arithmetic sequences, and maintains high computational efficiency. @tbl-sfu shows the various hardware implementations and their typical latencies. 

+------------------+---------------------+---------------------------------------+-----------------+
| Function Unit    | Operation           | Implementation Strategy               | Typical Latency |
+:=================+:====================+:======================================+:================+
| Activation Unit  | ReLU, sigmoid, tanh | Piece-wise approximation circuits     | 1-2 cycles      |
+------------------+---------------------+---------------------------------------+-----------------+
| Statistics Unit  | Mean, variance      | Parallel reduction trees              | log(N) cycles   |
+------------------+---------------------+---------------------------------------+-----------------+
| Exponential Unit | exp, log            | Table lookup + hardware interpolation | 2-4 cycles      |
+------------------+---------------------+---------------------------------------+-----------------+
| Root/Power Unit  | sqrt, rsqrt         | Fixed-iteration Newton-Raphson        | 4-8 cycles      |
+------------------+---------------------+---------------------------------------+-----------------+

: Special function unit implementation. {#tbl-sfu .striped .hover}

#### Historical Foundations of SFUs

The need for efficient non-linear function evaluation has shaped computer architecture for decades. Early processors incorporated hardware support for complex mathematical functions, such as logarithms and trigonometric operations, to accelerate workloads in scientific computing and signal processing.

In the 1970s and 1980s, floating-point co-processors were introduced to handle complex mathematical operations separately from the main CPU. In the 1990s, instruction set extensions such as Intel's SSE and ARM's NEON provided dedicated hardware for vectorized mathematical transformations, improving efficiency for multimedia and signal processing applications.

Machine learning workloads have reintroduced a strong demand for specialized functional units, as activation functions, normalization layers, and exponential transformations are fundamental to neural network computations. Rather than relying on iterative software approximations, modern AI accelerators implement fast, fixed-latency SFUs for these operations, mirroring historical trends in scientific computing.

The reemergence of dedicated special function units underscores the ongoing cycle in hardware evolution, where domain-specific requirements drive the reinvention of classical architectural concepts in new computational paradigms.

While vector, matrix, and special function units provide the computational backbone of modern AI accelerators, their effectiveness depends on how efficiently data is moved and accessed. The next section explores dataflow architectures, memory hierarchies, and data reuse strategies that are critical for sustaining high performance in neural network execution.

### Compute Units and Execution Models

The vector operations, matrix operations, and special function units examined in previous sections represent the fundamental computational primitives in AI accelerators. However, these primitives do not exist in isolation. Modern AI processors package these capabilities into execution units that define how computations are structured in hardware. Understanding this organization of primitives into execution units reveals both the theoretical capabilities and practical performance characteristics of contemporary AI accelerators.

#### From SIMD to SIMT

Single Instruction Multiple Data (SIMD) execution implements vector primitives by applying identical operations to multiple data elements in parallel. This execution model minimizes instruction overhead while maximizing data throughput. The implementation of SIMD operations in modern processors reveals the architectural mechanisms that enable efficient vector processing. The ARM Scalable Vector Extension (SVE) provides a representative example:

```assembly
# Vector operation implementation using ARM SVE
ptrue p0.s               # Create predicate for vector length
ld1w z0.s, p0/z, [x0]   # Load vector of inputs
fmul z1.s, z0.s, z0.s   # Multiply elements
fadd z2.s, z1.s, z0.s   # Add elements
st1w z2.s, p0, [x1]     # Store results
```

Contemporary processor architectures implement SIMD units of increasing width to accommodate the vector operations prevalent in neural network computations. Intel's Advanced Matrix Extensions (AMX) provide 1024-bit vectors, enabling simultaneous processing of 64 INT8 or 32 FP16 values. ARM's SVE2 architecture extends this capability with scalable vectors up to 2048 bits, allowing software adaptation across different hardware implementations.

The parallel processing capabilities of SIMD, while significant, prove insufficient for the computational demands of modern AI workloads. Single Instruction Multiple Thread (SIMT) architecture extends SIMD principles by enabling parallel execution across multiple independent threads, each maintaining its own program counter and architectural state. In NVIDIA's GPU architectures, each Streaming Multiprocessor (SM) coordinates thousands of threads executing identical instructions while following potentially divergent control paths:

```c
// CUDA kernel demonstrating SIMT execution
__global__ void matrix_multiply(float* C, float* A, float* B, int N) {
    // Each thread processes one output element
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    float sum = 0.0f;
    for (int k = 0; k < N; k++) {
        // Threads in a warp execute in parallel
        sum += A[row * N + k] * B[k * N + col];
    }
    C[row * N + col] = sum;
}
```

SIMT execution enables efficient scaling of neural network computations across thousands of parallel threads while maintaining the benefits of shared instruction processing. Similar SIMT architectures appear in AMD's RDNA and Intel's Xe designs, establishing SIMT as a fundamental execution model for AI acceleration.

#### Tensor Processing Units

The prevalence of matrix operations in neural networks necessitates execution units optimized for structured multi-dimensional computation. Tensor processing units extend the principles of SIMD and SIMT to operate on entire matrix blocks simultaneously. The NVIDIA A100 GPU's tensor core instruction exemplifies this architectural approach:

```text
Tensor Core Operation (NVIDIA A100):
mma.sync.aligned.m16n16k16.f16.f16 
  {d0,d1,d2,d3},     // Destination registers
  {a0,a1,a2,a3},     // Source matrix A  
  {b0,b1,b2,b3},     // Source matrix B
  {c0,c1,c2,c3}      // Accumulator
```

A single tensor core instruction processes an entire matrix block while maintaining intermediate results in local registers, substantially reducing the instruction overhead compared to implementations based on scalar or vector operations. The dimensionality and precision of tensor processing units vary across architectures, reflecting different optimization priorities. NVIDIA's Ampere architecture implements 4x4x4 FP16 tensor cores optimized for training flexibility. Google's TPUv4 employs 128x128 bfloat16 matrix units designed for sustained training throughput. Apple's M1 neural engine utilizes 16x16 matrix processors balanced for mobile inference workloads, while Intel's Sapphire Rapids introduces 32x32 AMX tiles targeting datacenter applications.

#### Processing Elements

The highest level of execution unit organization integrates multiple tensor cores with local memory into processing elements (PEs). Each PE encompasses vector units for element-wise operations, tensor cores for matrix computation, special function units for non-linear operations, and dedicated memory resources. This integration enables processing elements to execute complete neural network operations while minimizing data movement overhead.

Processing element implementations vary significantly across AI architectures, reflecting different approaches to computational scaling. Graphcore's Intelligence Processing Unit (IPU) distributes computation across 1,472 tiles, each containing independent processing elements optimized for fine-grained parallelism. Cerebras extends this approach in their CS-2 system, implementing 850,000 processing elements optimized for sparse computation across a wafer-scale device. Tesla's D1 processor arranges processing elements with substantial local memory to support the combined throughput and latency requirements of autonomous vehicle workloads.

#### Architectural Integration

The organization of primitives into execution units fundamentally determines the efficiency of neural network computation. Table @tbl-execution-units summarizes the execution unit configurations across contemporary AI processors:

+---------------------+-----------------+-------------------+----------------------+--------------------+
| Processor           | SIMD Width      | Tensor Core Size  | Processing Elements  | Primary Workloads  |
+:====================+:================+:==================+:=====================+:===================+
| NVIDIA A100         | 1024-bit        | 4x4x4 FP16        | 108 SMs              | Training, HPC      |
+---------------------+-----------------+-------------------+----------------------+--------------------+
| Google TPUv4        | 128-wide        | 128x128 BF16      | 2 cores/chip         | Training           |
+---------------------+-----------------+-------------------+----------------------+--------------------+
| Intel Sapphire      | 512-bit AVX     | 32x32 INT8/BF16   | 56 cores             | Inference          |
+---------------------+-----------------+-------------------+----------------------+--------------------+
| Apple M1            | 128-bit NEON    | 16x16 FP16        | 8 NPU cores          | Mobile inference   |
+---------------------+-----------------+-------------------+----------------------+--------------------+

: Execution unit configurations across modern AI processors {#tbl-execution-units .striped .hover}

The progression from vector primitives to integrated processing elements reflects the increasing sophistication of AI accelerator architectures. While the fundamental operations remain grounded in vector and matrix computation, their organization into execution units enables efficient implementation of neural network operations in hardware. The effectiveness of these execution units, however, depends critically on data movement patterns and memory hierarchy design, which form the focus of the next chapter.

## Memory Systems

Machine learning accelerators are designed to maximize computational throughput, leveraging specialized primitives such as vector units, matrix engines, and systolic arrays. However, the efficiency of these compute units is fundamentally constrained by the availability of data. Unlike conventional workloads, ML models require frequent access to large volumes of parameters, activations, and intermediate results, leading to substantial memory bandwidth demands. If data cannot be delivered to the processing elements at the required rate, memory bottlenecks can significantly limit performance, regardless of the accelerator's raw computational capability.

To address this challenge, modern AI hardware integrates sophisticated memory hierarchies, data movement strategies, and compression techniques to optimize performance. This section explores the interaction between ML workloads and memory systems, memory bandwidth constraints, and architectural innovations that enable efficient execution. Understanding these aspects will help us understand how to optimize AI acceleration and mitigate memory-related bottlenecks.

### AI Memory Wall

#### ML Workloads Are Memory-Intensive  

Machine learning workloads place substantial demands on memory systems due to the large volume of data involved in computation. Unlike traditional compute-bound applications, where performance is often dictated by the speed of arithmetic operations, ML workloads are characterized by high data movement requirements. The efficiency of an accelerator is not solely determined by its computational throughput but also by its ability to continuously supply data to processing units without introducing stalls or delays.

A neural network processes multiple types of data throughout its execution, each with distinct memory access patterns:

- **Model parameters (weights and biases):** machine learning models, particularly those used in large-scale applications such as natural language processing and computer vision, often contain millions to billions of parameters. Storing and accessing these weights efficiently is essential for maintaining throughput.
- **Intermediate activations:** During both training and inference, each layer produces intermediate results that must be temporarily stored and retrieved for subsequent operations. These activations can contribute significantly to memory overhead, particularly in deep architectures.
- **Gradients (during training):** Backpropagation requires storing and accessing gradients for every parameter, further increasing the volume of data movement between compute units and memory.

As models grow in size and complexity, the reliance on memory bandwidth increases proportionally. While specialized compute units accelerate operations such as matrix multiplications, their effectiveness is limited by how quickly data can be delivered. If the memory system cannot keep up with the demands of the compute hardware, performance bottlenecks emerge, leading to inefficient execution and increased energy consumption. Addressing these challenges requires an understanding of how memory systems are structured and how they interact with different ML workloads.

$$
T_{\text{mem}} = \frac{M_{\text{total}}}{B_{\text{mem}}}, \quad T_{\text{compute}} = \frac{\text{FLOPs}}{P_{\text{peak}}}
$$

where $T_{\text{mem}}$ represents data transfer time, determined by the total data volume $M_{\text{total}}$ and available memory bandwidth $B_{\text{mem}}$. The compute time $T_{\text{compute}}$ depends on the number of floating-point operations and peak hardware throughput. When $T_{\text{mem}} > T_{\text{compute}}$, the system is memory-bound, meaning the accelerator spends more time waiting for data than performing useful computation. This imbalance highlights the need for memory-optimized architectures and efficient data movement strategies to sustain high performance.

#### The Compute-Memory Imbalance  

Neural networks rely on specialized computational primitives such as vector operations, matrix multiplications, and domain-specific functional units that accelerate key aspects of machine learning workloads. These operations are designed for highly parallel execution, enabling accelerators to perform vast amounts of computation in each cycle. Given this level of specialization, one might expect neural networks to execute efficiently without significant bottlenecks. However, the primary constraint is not the raw compute power but rather the ability to continuously supply data to these processing units.  

While these compute units can execute millions of operations per second, they remain heavily dependent on memory bandwidth to sustain peak performance. Each matrix multiplication or vector operation requires a steady flow of weights, activations, and intermediate results, all of which must be fetched from memory. If data cannot be delivered at the required rate, memory stalls occur, leaving many compute units idle. This imbalance between computational capability and data availability is often referred to as the memory wall—a fundamental challenge in AI acceleration.  

Over time, the gap between computation and memory performance has widened. While specialized accelerators continue to increase their ability to perform highly parallelized operations, memory bandwidth has not scaled at the same rate, leading to frequent stalls and reduced utilization of available compute resources. This inefficiency is particularly evident in machine learning models, where large parameter sizes, frequent memory accesses, and non-uniform data movement patterns exacerbate memory bottlenecks.  

<!-- insert the memory and compute delta graph here -->  

Beyond performance limitations, memory access also imposes a significant energy cost. Fetching data from off-chip DRAM, in particular, consumes far more energy than performing arithmetic operations. As a result, optimizing memory usage is critical not only for improving performance but also for enhancing the efficiency and sustainability of AI accelerators.  

<!-- compute vs. data movement cost -->

#### Irregular Memory Access Patterns

Unlike traditional computing workloads, where memory access follows well-structured and predictable patterns, machine learning models often exhibit irregular memory access behaviors that make efficient data retrieval a challenge. These irregularities arise due to the nature of ML computations, where memory access patterns are influenced by factors such as batch size, layer type, and sparsity. As a result, standard caching mechanisms and memory hierarchies often struggle to optimize performance, leading to increased memory latency and inefficient bandwidth utilization.  

To better understand how ML workloads differ from traditional computing workloads, it is useful to compare their respective memory access patterns (@tbl-traditional-vs-ml-mem). Traditional workloads, such as scientific computing, general-purpose CPU applications, and database processing, typically exhibit well-defined memory access characteristics that benefit from standard caching and prefetching techniques. ML workloads, on the other hand, introduce highly dynamic access patterns that challenge conventional memory optimization strategies.

+------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------+
| Feature                      | Traditional Computing Workloads                                         | Machine Learning Workloads                                   |
+:=============================+:========================================================================+:=============================================================+
| Memory Access Pattern        | Regular and predictable (e.g., sequential reads, structured patterns)   | Irregular and dynamic (e.g., sparsity, attention mechanisms) |
+------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------+
| Cache Locality               | High temporal and spatial locality                                      | Often low locality, especially in large models               |
+------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------+
| Data Reuse                   | Structured loops with frequent data reuse                               | Sparse and dynamic reuse depending on layer type             |
+------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------+
| Data Dependencies            | Well-defined dependencies allow efficient prefetching                   | Variable dependencies based on network structure             |
+------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------+
| Workload Example             | Scientific computing (e.g., matrix factorizations, physics simulations) | Neural networks (e.g., CNNs, Transformers, sparse models)    |
+------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------+
| Memory Bottleneck            | DRAM latency, cache misses                                              | Off-chip bandwidth constraints, memory fragmentation         |
+------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------+
| Impact on Energy Consumption | Moderate, driven by FLOP-heavy execution                                | High, dominated by data movement costs                       |
+------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------+

: Comparison of memory access patterns in traditional vs. ML workloads. {#tbl-traditional-vs-ml-mem .striped .hover}

One key source of irregularity in ML workloads stems from batch size and execution order. The way input data is processed in batches directly affects memory reuse. When batch sizes are small, the likelihood of reusing cached activations and weights decreases, resulting in frequent memory fetches from slower, off-chip memory. Conversely, while larger batch sizes can improve reuse and amortize memory access costs, they also place higher demands on available memory bandwidth, potentially creating congestion at different levels of the memory hierarchy. The optimal batch size varies depending on the architecture of the model and the available hardware resources, making it a non-trivial parameter to tune for memory efficiency.  

In addition to batch size, different neural network layers interact with memory in distinct ways. Convolutional layers, for example, benefit from spatial locality, as neighboring pixels in an image are processed together, allowing for efficient caching of small weight kernels. Fully connected layers, on the other hand, require frequent access to large weight matrices, often leading to more randomized memory access patterns that do not align well with standard caching policies. Transformers introduce a different set of challenges, as attention mechanisms require accessing large key-value pairs stored across different locations in memory. Since the sequence length and attention span vary dynamically during execution, prefetching strategies that work well for structured workloads often fail in these cases, leading to unpredictable memory latencies.  

Another significant factor contributing to irregular memory access is sparsity in neural networks. Many modern ML models employ techniques such as weight pruning, activation sparsity, and structured sparsity to reduce computational overhead. However, these optimizations come at the cost of non-uniform memory access, as sparse representations often require fetching scattered elements from memory rather than sequential blocks of data. This makes it difficult to leverage hardware caching effectively. Similarly, models that incorporate dynamic computation paths, such as Mixture of Experts or Adaptive Computation Time, introduce highly non-deterministic memory access patterns, where the set of active neurons or model components varies from one inference step to the next. Since different computations are activated on demand, memory access patterns become irregular, creating additional challenges for efficient prefetching and caching.  

The consequences of these irregularities are significant. ML workloads often experience reduced cache efficiency, as activations and weights may not be accessed in predictable sequences. This leads to increased reliance on off-chip memory traffic, which not only slows down execution but also consumes more energy. Additionally, irregular access patterns contribute to memory fragmentation, where the way data is allocated and retrieved results in inefficient utilization of available memory resources. The combined effect of these factors is that ML accelerators frequently encounter memory bottlenecks that limit their ability to fully utilize available compute power.  

### Memory Hierarchy

To address the memory challenges in ML acceleration, hardware designers implement sophisticated memory hierarchies that balance speed, capacity, and energy efficiency. Understanding this hierarchy is essential before examining how different ML architectures utilize memory resources. Unlike general-purpose computing, where memory access patterns are often unpredictable, ML workloads exhibit structured reuse patterns that can be optimized through careful organization of data across multiple memory levels.

Unlike general-purpose computing, where memory access patterns are often unpredictable, machine learning workloads exhibit structured reuse patterns that can be optimized by carefully organizing data across multiple levels of memory. At the highest level, large-capacity but slow storage devices provide long-term model storage. At the lowest level, high-speed registers and caches ensure that compute units can access operands with minimal latency. Between these extremes, intermediate memory levels—including scratchpad memory, high-bandwidth memory (HBM), and off-chip DRAM—offer trade-offs between performance and capacity.

@tbl-memory-heirarchy summarizes the key characteristics of different memory levels in modern AI accelerators. Each level in the hierarchy has distinct latency, bandwidth, and capacity properties, which directly influence how neural network data, such as weights, activations, and intermediate results, should be allocated.

+----------------------------------+-----------------+-----------+------------+----------------------------------------------------------------------+
| Memory Level                     | Approx. Latency | Bandwidth | Capacity   | Example Use in Deep Learning                                         |
+:=================================+:================+:==========+:===========+:=====================================================================+
| Registers                        | ~1 cycle        | Highest   | Few values | Storing operands for immediate computation                           |
+----------------------------------+-----------------+-----------+------------+----------------------------------------------------------------------+
| L1/L2 Cache (SRAM)               | ~1-10 ns        | High      | KBs-MBs    | Caching frequently accessed activations and small weight blocks      |
+----------------------------------+-----------------+-----------+------------+----------------------------------------------------------------------+
| Scratchpad Memory                | ~5-20 ns        | High      | MBs        | Software-managed storage for intermediate computations               |
+----------------------------------+-----------------+-----------+------------+----------------------------------------------------------------------+
| High-Bandwidth Memory (HBM)      | ~100 ns         | Very High | GBs        | Storing large model parameters and activations for high-speed access |
+----------------------------------+-----------------+-----------+------------+----------------------------------------------------------------------+
| Off-Chip DRAM (DDR, GDDR, LPDDR) | ~50-150 ns      | Moderate  | GBs-TBs    | Storing entire model weights that do not fit on-chip                 |
+----------------------------------+-----------------+-----------+------------+----------------------------------------------------------------------+
| Flash Storage (SSD/NVMe)         | ~100 µs - 1 ms  | Low       | TBs        | Storing pre-trained models and checkpoints for later loading         |
+----------------------------------+-----------------+-----------+------------+----------------------------------------------------------------------+

: Memory hierarchy characteristics and their impact on machine learning. {#tbl-memory-heirarchy .striped .hover}

#### On-chip Memory

Each level of the memory hierarchy serves a distinct role in AI acceleration, with different trade-offs in speed, capacity, and accessibility. Registers, located within compute cores, provide the fastest access but can only store a few operands at a time. These are best utilized for immediate computations, where the operands needed for an operation can be loaded and consumed within a few cycles. However, because register storage is so limited, frequent memory accesses are required to fetch new operands and store intermediate results.

To reduce the need for constant data movement between registers and external memory, small but fast caches serve as an intermediary buffer. These caches store recently accessed activations, weights, and intermediate values, ensuring that frequently used data remains available with minimal delay. However, the size of caches is limited, making them insufficient for storing full feature maps or large weight tensors in machine learning models. As a result, only the most frequently used portions of a model's parameters or activations can reside here at any given time.

For larger working datasets, many AI accelerators include scratchpad memory, which offers more storage than caches but with a crucial difference: it allows explicit software control over what data is stored and when it is evicted. Unlike caches, which rely on hardware-based eviction policies, scratchpad memory enables machine learning workloads to retain key values such as activations and filter weights for multiple layers of computation. This capability is particularly useful in models like convolutional neural networks, where the same input feature maps and filter weights are reused across multiple operations. By keeping this data in scratchpad memory rather than reloading it from external memory, accelerators can significantly reduce unnecessary memory transfers and improve overall efficiency.

#### Off-Chip Memory

Beyond on-chip memory, high-bandwidth memory (HBM) provides rapid access to larger model parameters and activations that do not fit within caches or scratchpad buffers. HBM achieves its high performance by stacking multiple memory dies and using wide memory interfaces, allowing it to transfer large amounts of data with minimal latency compared to traditional DRAM. Because of its high bandwidth and lower latency, HBM is often used to store entire layers of machine learning models that must be accessed quickly during execution. However, its cost and power consumption limit its use primarily to high-performance AI accelerators, making it less common in power-constrained environments such as edge devices.

When a machine learning model exceeds the capacity of on-chip memory and HBM, it must rely on off-chip DRAM, such as DDR, GDDR, or LPDDR. While DRAM offers significantly greater storage capacity, its access latency is higher, meaning that frequent retrievals from DRAM can introduce execution bottlenecks. To make effective use of DRAM, models must be structured so that only the necessary portions of weights and activations are retrieved at any given time, minimizing the impact of long memory fetch times.

At the highest level of the hierarchy, flash storage and solid-state drives (SSDs) store large pre-trained models, datasets, and checkpointed weights. These storage devices offer large capacities but are too slow for real-time execution, requiring models to be loaded into faster memory tiers before computation begins. For instance, in training scenarios, checkpointed models stored in SSDs must be loaded into DRAM or HBM before resuming computation, as direct execution from SSDs would be too slow to maintain efficient accelerator utilization.

#### Memory Bottlenecks

The memory hierarchy balances competing objectives of speed, capacity, and energy efficiency. However, moving data through multiple memory levels introduces bottlenecks that limit accelerator performance. Data transfers between memory levels incur latency costs, particularly for off-chip accesses. Limited bandwidth restricts data flow between memory tiers. Memory capacity constraints force constant data movement as models exceed local storage.

These bottlenecks affect ML architectures differently. The next section examines how specific model types---MLPs, CNNs, and Transformers---interact with memory hierarchies. Their distinct memory access patterns and resource requirements explain why accelerators achieve varying performance across different model architectures.

### Model Memory Pressure

With an understanding of memory challenges and hierarchies, we can examine how different ML architectures interact with memory systems. While all machine learning models require large parameter sets and high data movement, their memory access patterns and pressure points vary significantly. These differences directly influence how accelerators must manage their memory hierarchies to maintain efficiency.

#### Multilayer Perceptrons (MLPs)

Multilayer perceptrons (MLPs), also referred to as fully connected networks, are among the simplest neural network architectures. Each layer in an MLP consists of a dense matrix multiplication, where every neuron is connected to all neurons in the previous layer. This results in high memory bandwidth demands for weights, since every input activation interacts with all neurons in the subsequent layer.  

From a memory perspective, MLPs are characterized by:  

- Large, dense weight matrices, requiring frequent access to off-chip memory when model sizes exceed on-chip capacity.  
- Low activation reuse, as each input typically contributes to a small set of computations before being discarded.  
- Sequential memory access patterns, making them more cache-friendly compared to sparse workloads.  

Although MLPs are relatively bandwidth-heavy, their regular and predictable memory access patterns make them easier to optimize. Accelerators handling MLP workloads often prefetch weights efficiently, leveraging streaming memory accesses and fast SRAM caches to sustain high throughput.  

#### Convolutional Neural Networks (CNNs)

Convolutional neural networks (CNNs) are widely used in image processing and computer vision tasks. Unlike MLPs, which require dense matrix multiplications, CNNs process input feature maps using small filter kernels that slide across the image. This localized computation structure results in high spatial data reuse, where the same input pixels contribute to multiple convolutions.  

From a memory standpoint, CNNs exhibit:  

- Small weight footprints, as convolution kernels are reused extensively across an entire input image.  
- High activation reuse, allowing for efficient caching of input feature maps in on-chip memory.  
- Spatial locality, making CNNs highly compatible with tiling strategies that optimize memory movement.  

Since CNN weights are typically much smaller than those of MLPs, accelerators can store filter kernels entirely in fast local memory, reducing off-chip memory accesses. However, the challenge lies in efficiently handling large activation maps, which must be stored, retrieved, and reused multiple times during execution. CNN accelerators often employ tiling techniques, where feature maps are split into smaller regions that fit within on-chip buffers, reducing the need to frequently access slower external memory.  

#### Transformer Networks

Transformers have become the dominant architecture for natural language processing and are increasingly used in other domains such as vision and speech recognition. Unlike CNNs, which rely on local computations, transformers perform global attention mechanisms, where each token in an input sequence can interact with all other tokens. This leads to irregular and bandwidth-intensive memory access patterns, as large key-value matrices must be fetched and updated frequently.  

Memory challenges in transformers include:  

- Large parameter sizes, often reaching hundreds of billions of parameters, far exceeding on-chip memory capacity.  
- High memory bandwidth consumption, as attention layers require frequent access to large key-value matrices.  
- Low data reuse, since attention operations dynamically determine which inputs interact, making caching strategies less effective.  

Due to these demands, transformer models are often memory-bound rather than compute-bound. Accelerators optimized for transformers must rely on high-bandwidth memory (HBM), quantization techniques, and efficient memory partitioning to sustain high throughput. Additionally, attention caching and specialized tensor layouts help reduce redundant memory fetches.  

### Implications for ML Accelerators  

The diverse memory requirements of MLPs, CNNs, and Transformers highlight the importance of tailoring memory architectures to specific workloads. @tbl-model-mem-compare compares the memory access patterns across these different models.

+-------------+-----------------+------------------+---------------------------+----------------------+
| Model Type  | Weight Size     | Activation Reuse | Memory Access Pattern     | Primary Bottleneck   |
+:============+:================+:=================+:==========================+:=====================+
| MLP (Dense) | Large, dense    | Low              | Regular, sequential       | Bandwidth            |
+-------------+-----------------+------------------+---------------------------+----------------------+
| CNN         | Small, reused   | High             | Spatial locality          | Feature map movement |
+-------------+-----------------+------------------+---------------------------+----------------------+
| Transformer | Massive, sparse | Low              | Irregular, high bandwidth | Memory capacity      |
+-------------+-----------------+------------------+---------------------------+----------------------+

: Comparison of memory access characteristics across different ML models. {#tbl-model-mem-compare .striped .hover}

Each model type presents unique challenges that directly impact accelerator design. MLPs benefit from fast streaming access to dense weight matrices, making bandwidth a key factor in performance. CNNs, with their high activation reuse and structured memory access patterns, can take advantage of memory hierarchies that prioritize efficient data locality and caching strategies. Transformers, on the other hand, impose significant demands on both bandwidth and capacity due to their large key-value matrices and irregular memory access patterns.

To address these challenges, modern AI accelerators incorporate memory hierarchies that balance speed, capacity, and energy efficiency. On-chip memory structures are used to store frequently accessed data, while external memory subsystems provide scalability for larger models. These architectures leverage a mix of cache-based designs, software-managed scratchpad memories, and high-bandwidth memory solutions to optimize data movement.

As ML workloads continue to grow in complexity, memory efficiency is becoming just as critical as raw compute power. Efficient data movement strategies, workload-specific optimizations, and advanced memory architectures play a fundamental role in sustaining high performance. The following section explores the design of memory hierarchies in AI accelerators, detailing how different levels of memory interact to maximize efficiency.

## Mapping Neural Networks

Efficient execution of machine learning models on specialized hardware requires a structured approach to computation, ensuring that available resources are fully utilized while minimizing performance bottlenecks. Unlike general-purpose processors, which rely on dynamic task scheduling, AI accelerators operate under a structured execution model that maximizes throughput by carefully assigning computations to processing elements. This process, known as mapping, dictates how computations are distributed across hardware resources, influencing execution speed, memory access patterns, and overall efficiency.

::: {.callout-note title="Definition of Mapping in AI Acceleration"}  

Mapping refers to the assignment of machine learning computations to hardware processing units in a way that optimizes execution efficiency. This involves spatial allocation, which distributes computations across processing elements, temporal scheduling, which sequences operations to maintain balanced workloads, and memory-aware execution, which places data strategically to reduce access latency. Effective mapping ensures high resource utilization, low memory stalls, and energy-efficient execution, making it a critical factor in AI acceleration.  

:::  

Mapping machine learning models onto AI accelerators presents several challenges due to hardware constraints and the diversity of model architectures. Given the hierarchical memory system of modern accelerators, mapping strategies must carefully manage when and where data is accessed to minimize latency and power overhead while ensuring that compute units remain actively engaged. Poor mapping decisions can lead to underutilized compute resources, excessive data movement, and increased execution time, ultimately reducing overall efficiency.

Mapping involves three interrelated aspects:  

- **Computation Placement:** Assigning operations, such as matrix multiplications and convolutions, to processing elements to maximize parallelism and minimize idle time.  
- **Memory Allocation:** Determining where model parameters, activations, and intermediate results reside within the memory hierarchy to optimize access efficiency.  
- **Dataflow and Execution Scheduling:** Structuring the movement of data between compute units to reduce bandwidth bottlenecks and ensure smooth execution.  

Effective mapping strategies minimize off-chip memory accesses, maximize compute utilization, and efficiently manage data movement across different levels of the memory hierarchy. The following sections explore the key mapping choices that influence execution efficiency and lay the groundwork for optimization strategies that refine these decisions.  

### Computation Placement

Modern AI accelerators are designed to execute machine learning models with massive parallelism, leveraging thousands to millions of processing elements (PEs) to perform computations simultaneously. However, simply having a large number of compute units is not enough—how computations are assigned to these units determines overall efficiency. 

Without careful placement, some processing elements may sit idle while others are overloaded, leading to wasted resources, increased memory traffic, and reduced performance. Computation placement is the process of strategically mapping operations onto available hardware resources to sustain high throughput, minimize stalls, and optimize execution efficiency.

#### Defining Computation Placement  

AI accelerators contain thousands to millions of processing elements, making computation placement a large-scale problem. Modern GPUs, such as the NVIDIA H100, feature over 16,000 CUDA cores and more than 500 specialized tensor cores, each designed to accelerate matrix operations. TPUs utilize systolic arrays composed of thousands of interconnected multiply-accumulate (MAC) units, while wafer-scale processors like Cerebras’ CS-2 push parallelism even further, integrating over 850,000 cores on a single chip. In these architectures, even minor inefficiencies in computation placement can lead to significant performance losses, as idle cores or excessive memory movement compound across the system.

At its core, computation placement ensures that all processing elements contribute effectively to execution. This means that workloads must be distributed in a way that avoids imbalanced execution, where some processing elements sit idle while others remain overloaded. Similarly, placement must minimize unnecessary data movement, as excessive memory transfers introduce latency and power overheads that degrade system performance.

Neural network computations vary significantly based on the model architecture, influencing how placement strategies are applied. For example, in a convolutional neural network (CNN), placement focuses on dividing image regions across processing elements to maximize parallelism. A 256×256 image processed through thousands of GPU cores might be broken into small tiles, each mapped to a different processing unit to execute convolutional operations simultaneously. In contrast, a transformer-based model requires placement strategies that accommodate self-attention mechanisms, where each token in a sequence interacts with all others, leading to irregular and memory-intensive computation patterns. Meanwhile, graph neural networks (GNNs) introduce additional complexity, as computations depend on sparse and dynamic graph structures that require adaptive workload distribution.

Because computation placement directly impacts resource utilization, execution speed, and power efficiency, it is one of the most critical factors in AI acceleration. A well-placed computation can reduce latency by orders of magnitude, while a poorly placed one can render thousands of processing units underutilized. The next section explores why efficient computation placement is essential and the consequences of suboptimal mapping strategies.

#### Why Computation Placement Matters  

While computation placement is a hardware-driven process, its importance is fundamentally shaped by the structure of neural network workloads. Different types of machine learning models exhibit distinct computation patterns, which directly influence how efficiently they can be mapped onto accelerators. Without careful placement, workloads can become unbalanced, memory access patterns can become inefficient, and the overall performance of the system can degrade significantly.  

For models with structured computation patterns, such as convolutional neural networks (CNNs), computation placement is relatively straightforward. CNNs process images using filters that are applied to small, localized regions, meaning their computations can be evenly distributed across processing elements. Because these operations are highly parallelizable, CNNs benefit from spatial partitioning, where the input is divided into tiles that are processed independently. This structured execution makes CNNs well-suited for accelerators that favor regular dataflows, minimizing the complexity of placement decisions.  

However, for models with irregular computation patterns, such as transformers and graph neural networks (GNNs), computation placement becomes significantly more challenging. Transformers, which rely on self-attention mechanisms, require each token in a sequence to interact with all others, resulting in non-uniform computation demands. Unlike CNNs, where each processing element performs a similar amount of work, transformers introduce workload imbalance, where certain operations—such as computing attention scores—require far more computation than others. Without careful placement, this imbalance can lead to stalls, where some processing elements remain idle while others struggle to keep up.  

The challenge is even greater in graph neural networks (GNNs), where computation depends on sparse and dynamically changing graph structures. Unlike CNNs, which operate on dense and regularly structured data, GNNs must process nodes and edges with highly variable degrees of connectivity. Some regions of a graph may require significantly more computation than others, making workload balancing across processing elements difficult. If computations are not placed strategically, some compute units will sit idle while others remain overloaded, leading to underutilization and inefficiencies in execution.  

Poor computation placement has three major consequences for AI execution:  

1. **Workload Imbalance:** Uneven distribution of computations leads to idle processing elements, preventing full hardware utilization and reducing throughput.  
2. **Excessive Data Movement:** Inefficient placement increases memory traffic, forcing frequent data transfers between memory hierarchies, which introduces latency and higher power consumption.  
3. **Execution Stalls and Bottlenecks:** When operations are poorly placed, some computations wait on data dependencies, leading to pipeline inefficiencies and lower overall system performance.  

Ultimately, computation placement is not just about assigning operations to processing elements—it is about ensuring that models execute efficiently given their unique computational structure. A well-placed workload reduces execution time, memory overhead, and power consumption, while a poorly placed one can lead to stalled execution pipelines and inefficient resource utilization. The next section explores the key considerations that must be addressed to ensure that computation placement is both efficient and adaptable to different model architectures.  

#### Key Considerations for Effective Computation Placement  

Computation placement is a balancing act between hardware constraints and workload characteristics. To achieve high efficiency, placement strategies must account for parallelism, memory access, and workload variability while ensuring that processing elements (PEs) remain fully utilized. Poor placement leads to imbalanced execution, increased data movement, and performance degradation, making it essential to consider key factors when designing placement strategies.  

As summarized in @tbl-placement-challenges, computation placement faces several critical challenges that impact execution efficiency. Effective mapping strategies must address these challenges by balancing workload distribution, minimizing data movement, and optimizing communication across processing elements.

+--------------------------------+--------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+
| Challenge                      | Impact on Execution                                                                                                | Key Considerations for Placement                                                            |
+:===============================+:===================================================================================================================+:============================================================================================+
| Workload Imbalance             | Some processing elements finish early while others remain overloaded, leading to idle compute resources.           | Distribute operations evenly to prevent stalls and ensure full utilization of PEs.          |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+
| Irregular Computation Patterns | Models like transformers and GNNs introduce non-uniform computation demands, making static placement difficult.    | Use adaptive placement strategies that adjust execution based on workload characteristics.  |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+
| Excessive Data Movement        | Frequent memory transfers introduce latency and increase power consumption.                                        | Keep frequently used data close to the compute units and minimize off-chip memory accesses. |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+
| Limited Interconnect Bandwidth | Poorly placed operations can create congestion, slowing data movement between PEs.                                 | Optimize spatial and temporal placement to reduce communication overhead.                   |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+
| Model-Specific Execution Needs | CNNs, transformers, and GNNs require different execution patterns, making a single placement strategy ineffective. | Tailor placement strategies to match the computational structure of each model type.        |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+

: Primary challenges in computation placement and key considerations for effective mapping strategies. {#tbl-placement-challenges .striped .hover}

Each of these challenges highlights a core trade-off in computation placement: maximizing parallelism while minimizing memory overhead. For CNNs, placement strategies prioritize structured tiling to maintain efficient data reuse. For transformers, placement must ensure balanced execution across attention layers. For GNNs, placement must dynamically adjust to sparse computation patterns.  

Beyond model-specific needs, effective computation placement must also be scalable. As models grow in size and complexity, placement strategies must adapt dynamically rather than relying on static execution patterns. Future AI accelerators increasingly integrate runtime-aware scheduling mechanisms, where placement is optimized based on real-time workload behavior rather than predetermined execution plans.  

Ultimately, effective computation placement requires a holistic approach that balances hardware capabilities with model characteristics. The next section explores how computation placement interacts with memory allocation and data movement, ensuring that AI accelerators operate at peak efficiency.  

### Memory Allocation

#### Defining Memory Allocation

Efficient memory allocation is essential for sustaining high performance in AI accelerators. While computation placement determines where operations are executed, memory allocation defines where data is stored and how it is accessed throughout execution. AI models require vast amounts of data movement, from loading model parameters to storing intermediate activations and gradients. How this data is allocated across the hierarchical memory system—ranging from on-chip caches and scratchpads to high-bandwidth memory (HBM) and DRAM—directly affects execution efficiency.

The primary goal of memory allocation is to minimize latency and reduce power consumption by keeping frequently accessed data as close as possible to the processing elements (PEs). Poor memory allocation can lead to excessive off-chip memory accesses, increasing bandwidth contention and slowing down execution. Since AI accelerators operate at teraflop and petaflop scales, inefficient memory access patterns can result in substantial performance bottlenecks.

Different hardware architectures implement memory hierarchies tailored for AI workloads. GPUs rely on a mix of global memory, shared memory, and registers, requiring careful tiling strategies to optimize locality. TPUs use on-chip SRAM scratchpads, where activations and weights must be efficiently preloaded to sustain systolic array execution. Wafer-scale processors, with their hundreds of thousands of cores, demand sophisticated memory partitioning strategies to avoid excessive interconnect traffic. In all cases, the effectiveness of memory allocation determines the overall throughput, power efficiency, and scalability of AI execution.

#### Why Memory Allocation Matters  

Memory allocation plays a central role in AI acceleration because how and where data is stored directly impacts execution efficiency. Unlike general-purpose computing, where memory management is abstracted by caches and dynamic allocation, AI accelerators require explicit data placement strategies to sustain high throughput and avoid unnecessary stalls. When memory is not allocated efficiently, AI workloads suffer from latency overhead, excessive power consumption, and bottlenecks that limit computational performance.  

Neural network architectures have varying memory demands, which influence the importance of proper allocation. Convolutional Neural Networks (CNNs) rely on structured and localized data access patterns, meaning that inefficient memory allocation can lead to redundant data loads and cache inefficiencies. In contrast, transformer models require frequent access to large model parameters and intermediate activations, making them highly sensitive to memory bandwidth constraints. Graph Neural Networks (GNNs) introduce even greater challenges, as their irregular and sparse data structures result in unpredictable memory access patterns that can lead to inefficient use of memory resources.  

Poor memory allocation has three major consequences for AI execution:  

1. **Increased Memory Latency:** When frequently accessed data is not stored in the right location, accelerators must retrieve it from higher-latency memory, slowing down execution.  
2. **Higher Power Consumption:** Off-chip memory accesses consume significantly more energy than on-chip storage, leading to inefficiencies at scale.  
3. **Reduced Computational Throughput:** If data is not available when needed, processing elements remain idle, reducing the overall performance of the system.  

As AI models continue to grow in size and complexity, the importance of scalable and efficient memory allocation increases. Memory limitations can dictate how large of a model can be deployed on a given accelerator, affecting feasibility and performance. The next section explores the key considerations that impact memory allocation strategies and the constraints that must be addressed to optimize execution efficiency.  

#### Key Considerations for Effective Memory Allocation  

Memory allocation plays a crucial role in AI acceleration by determining how and where data is stored throughout execution. Since accelerators operate under strict memory constraints, allocation strategies must carefully balance storage limitations, bandwidth availability, and workload demands. Inefficient allocation leads to frequent stalls, excessive memory traffic, and power inefficiencies, all of which degrade overall performance.  

As summarized in @tbl-memory-allocation, memory allocation in AI accelerators must address several key challenges that influence execution efficiency. Effective allocation strategies mitigate high latency, bandwidth limitations, and irregular access patterns by carefully managing data placement and movement. Ensuring that frequently accessed data is stored in faster memory locations while minimizing unnecessary transfers is essential for maintaining performance and energy efficiency.

+----------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Challenge                        | Impact on Execution                                                                    | Key Considerations for Allocation                                                                 |
+:=================================+:=======================================================================================+:==================================================================================================+
| High Memory Latency              | Slow data access delays execution and reduces throughput.                              | Prioritize placing frequently accessed data in faster memory locations.                           |
+----------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Limited On-Chip Storage          | Small local memory constrains the amount of data available near compute units.         | Allocate storage efficiently to maximize data availability without exceeding hardware limits.     |
+----------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+
| High Off-Chip Bandwidth Demand   | Frequent access to external memory increases delays and power consumption.             | Reduce unnecessary memory transfers by carefully managing when and how data is moved.             |
+----------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Irregular Memory Access Patterns | Some models require accessing data unpredictably, leading to inefficient memory usage. | Organize memory layout to align with access patterns and minimize unnecessary data movement.      |
+----------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+
| Model-Specific Memory Needs      | Different models require different allocation strategies to optimize performance.      | Tailor allocation decisions based on the structure and execution characteristics of the workload. |
+----------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+

: Key challenges in memory allocation and considerations for efficient execution. {#tbl-memory-allocation .striped .hover}

Each of these challenges requires careful memory management to balance execution efficiency with hardware constraints. While structured models may benefit from well-defined memory layouts that facilitate predictable access, others, like transformer-based and graph-based models, require more adaptive allocation strategies to handle variable and complex memory demands.  

Beyond workload-specific considerations, memory allocation must also be scalable. As model sizes continue to grow, accelerators must dynamically manage memory resources rather than relying on static allocation schemes. Ensuring that frequently used data is accessible when needed without overwhelming memory capacity is essential for maintaining high efficiency.  

In summary, mapping neural network computations to specialized hardware is a foundational step in AI acceleration, directly influencing performance, memory efficiency, and energy consumption. However, selecting an effective mapping strategy is not a trivial task—hardware constraints, workload variability, and execution dependencies create a vast and complex design space. While the principles of computation placement, memory allocation, and data movement provide a structured foundation, optimizing these decisions requires advanced techniques to navigate the trade-offs involved. The next section explores optimization strategies that refine mapping decisions, focusing on techniques that efficiently search the design space to maximize execution efficiency while balancing hardware constraints.

### Combinatorial Complexity

The efficient execution of machine learning models on AI accelerators requires careful consideration of placement—the spatial assignment of computations and data—and allocation—the temporal distribution of resources. These decisions are interdependent, and each introduces trade-offs that impact performance, energy efficiency, and scalability. @tbl-combinatorial-complexity outlines the fundamental trade-offs between computation placement and resource allocation in AI accelerators. Placement decisions influence parallelism, memory access patterns, and communication overhead, while allocation strategies determine how resources are distributed over time to balance execution efficiency. The interplay between these factors shapes overall performance, requiring a careful balance to avoid bottlenecks such as excessive synchronization, memory congestion, or underutilized compute resources. Optimizing these trade-offs is essential for ensuring that AI accelerators operate at peak efficiency.

+-----------------------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+
| Dimension                         | Placement Considerations                                                                                 | Allocation Considerations                                                                            |
+===================================+==========================================================================================================+======================================================================================================+
| Computational Granularity         | Fine-grained placement enables greater parallelism but increases synchronization overhead.               | Coarse-grained allocation reduces synchronization overhead but may limit flexibility.                |
+-----------------------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+
| Spatial vs. Temporal Mapping      | Spatial placement enhances parallel execution but can lead to resource contention and memory congestion. | Temporal allocation balances resource sharing but may reduce overall throughput.                     |
+-----------------------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+
| Memory and Data Locality          | Placing data closer to compute units minimizes latency but may reduce overall memory availability.       | Allocating data across multiple memory levels increases capacity but introduces higher access costs. |
+-----------------------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+
| Communication and Synchronization | Co-locating compute units reduces communication latency but may introduce contention.                    | Allocating synchronization mechanisms mitigates stalls but can introduce additional overhead.        |
+-----------------------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+
| Dataflow and Execution Ordering   | Static placement simplifies execution but limits adaptability to workload variations.                    | Dynamic allocation improves adaptability but adds scheduling complexity.                             |
+-----------------------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+

: Trade-offs between computation placement and resource allocation in AI accelerators. {#tbl-combinatorial-complexity .striped .hover}

Each of these dimensions requires balancing trade-offs between placement and allocation. For instance, spatially distributing computations across multiple processing elements can increase throughput; however, if data allocation is not optimized, memory bandwidth limitations may introduce bottlenecks. Likewise, allocating resources for fine-grained computations may enhance flexibility but, without appropriate placement strategies, may lead to excessive synchronization overhead.  

Because AI accelerator architectures impose constraints on both where computations execute and how resources are assigned over time, selecting an effective mapping strategy necessitates a coordinated approach to placement and allocation. Understanding how these trade-offs influence execution efficiency is essential for optimizing performance on AI accelerators.  

#### Mapping Configuration Space

The efficiency of AI accelerators is determined not only by their computational capabilities but also by how neural network computations are mapped to hardware resources. Mapping defines how computations are assigned to processing elements (PEs), how data is placed and moved through the memory hierarchy, and how execution is scheduled. The choices made in this process significantly impact performance, influencing compute utilization, memory bandwidth efficiency, and energy consumption.  

Mapping machine learning models to hardware presents a large and complex design space. Unlike traditional computational workloads, model execution involves multiple interacting factors—computation, data movement, parallelism, and scheduling—each introducing constraints and trade-offs. The hierarchical memory structure of accelerators, as discussed in the Memory Systems section, further complicates this process by imposing limits on bandwidth, latency, and data reuse. As a result, effective mapping strategies must carefully balance competing objectives to maximize efficiency.  

At the heart of this design space are three fundamental aspects:  

- **Data placement:** The location of data across different memory levels—on-chip buffers, caches, or off-chip DRAM—determines both latency and energy consumption. Poor placement can lead to frequent, costly memory accesses, while efficient placement ensures that frequently used data remains in fast-access storage.  
- **Computation scheduling:** The order in which operations are executed affects both compute efficiency and memory access patterns. Some schedules may maximize parallelism but introduce synchronization overheads, while others may improve data locality but limit throughput.  
- **Data movement timing:** Transferring data across memory hierarchies incurs significant overhead in both latency and energy consumption. Efficient mapping strategies seek to minimize unnecessary transfers by reusing data whenever possible and overlapping communication with computation.  

These factors define a vast combinatorial design space, where small variations in mapping decisions can lead to large differences in performance and energy efficiency. A poor mapping strategy can result in underutilized compute resources, excessive data movement, or imbalanced workloads, creating bottlenecks that degrade overall efficiency. Conversely, a well-designed mapping maximizes both throughput and resource utilization, making efficient use of available hardware.  

Because of the interconnected nature of mapping decisions, there is no single optimal solution—different workloads and hardware architectures demand different approaches. The next sections examine the structure of this design space and how different mapping choices shape the execution of machine learning workloads.  


Mapping machine learning computations onto specialized hardware requires balancing multiple constraints, including compute efficiency, memory bandwidth, and execution scheduling. The challenge arises from the vast number of possible ways to assign computations to processing elements, order execution, and manage data movement. Each decision contributes to a high-dimensional search space, where even minor variations in mapping choices can significantly impact performance.  

Unlike traditional workloads with predictable execution patterns, machine learning models introduce diverse computational structures that require flexible mappings adapted to data reuse, parallelization opportunities, and memory constraints. The search space grows combinatorially, making exhaustive search infeasible. To understand this complexity, we analyze three key sources of variation:

#### Ordering of Computation and Execution Dependencies

Machine learning workloads are often structured as nested loops, iterating over various dimensions of computation. For instance, a matrix multiplication kernel may loop over batch size ($N$), input features ($C$), and output features ($K$). The order in which these loops execute has a profound effect on data locality, reuse patterns, and computational efficiency.

The number of ways to arrange $d$ loops follows a factorial growth pattern:

$$
\mathcal{O} = d!
$$

which scales rapidly. A typical convolutional layer may involve up to seven loop dimensions, leading to:

$$
7! = 5,040 \text{ possible execution orders.}
$$

Furthermore, when considering multiple memory levels, the search space expands as:

$$
(d!)^l
$$

where $l$ is the number of memory hierarchy levels. This rapid expansion highlights why execution order optimization is crucial—poor loop ordering can lead to excessive memory traffic, while an optimized order improves cache utilization.

#### Parallelization Across Processing Elements (PEs)

Modern AI accelerators leverage thousands of processing elements (PEs) to maximize parallelism, but determining which computations should be parallelized is non-trivial. Excessive parallelization can introduce synchronization overheads and increased bandwidth demands, while insufficient parallelization leads to underutilized hardware.

The number of ways to distribute computations among parallel units follows the binomial coefficient:

$$
\mathcal{P} = \frac{d!}{(d-k)!}
$$

where $d$ is the number of loops, and $k$ is the number selected for parallel execution. For a six-loop computation where three loops are chosen for parallel execution, the number of valid configurations is:

$$
\frac{6!}{(6-3)!} = 120.
$$

Even for a single layer, there can be hundreds of valid parallelization strategies, each affecting data synchronization, memory contention, and overall compute efficiency. Expanding this across multiple layers and model architectures further magnifies the complexity.

#### Memory Placement and Data Movement

The hierarchical memory structure of AI accelerators introduces additional constraints, as data must be efficiently placed across registers, caches, shared memory, and off-chip DRAM. Data placement impacts latency, bandwidth consumption, and energy efficiency—frequent access to slow memory creates bottlenecks, while optimized placement reduces costly memory transfers.

The number of ways to allocate data across memory levels follows an exponential growth function:

$$
\mathcal{M} = n^{d \times l}
$$

where:

- $n$ = number of placement choices per level,
- $d$ = number of computational dimensions,
- $l$ = number of memory hierarchy levels.

For a model with:

- $d = 5$ computational dimensions,  
- $l = 3$ memory levels,  
- $n = 4$ possible placement choices per level,  

the number of possible memory allocations is:

$$
4^{5 \times 3} = 4^{15} = 1,073,741,824.
$$

This highlights how even a single layer may have over a billion possible memory configurations, making manual optimization impractical.

#### Total Mapping Search Space

By combining the complexity from computation ordering, parallelization, and memory placement, the total mapping search space can be approximated as:

$$
\mathcal{S} = \left( n^d \times d! \times \frac{d!}{(d-k)!} \right)^l
$$

where:  
 
- $n^d$ represents memory placement choices,  
- $d!$ accounts for computation ordering choices,  
- $\frac{d!}{(d-k)!}$ captures parallelization possibilities,  
- $l$ is the number of memory hierarchy levels.  

This equation illustrates the exponential growth of the search space, making brute-force search infeasible for all but the simplest cases.

## Mapping Strategies  

Efficiently mapping machine learning computations onto hardware is a complex challenge due to the vast number of possible configurations. As models grow in complexity, the number of potential mappings increases exponentially. Even for a single layer, there are thousands of ways to order computation loops, hundreds of parallelization strategies, and an exponentially growing number of memory placement choices. This combinatorial explosion makes exhaustive search impractical.

To overcome this challenge, AI accelerators rely on structured mapping strategies that systematically balance computational efficiency, data locality, and parallel execution. Rather than evaluating every possible configuration, these approaches use a combination of heuristic, analytical, and machine learning-based techniques to find high-performance mappings efficiently.

The key to effective mapping lies in understanding and applying a set of core techniques that optimize data movement, memory access, and computation. These building blocks of mapping strategies provide a structured foundation for efficient execution, which we explore in the next section.

### Building Blocks of Mapping Strategies

To navigate the complexity of mapping decisions, we leverage a set of foundational techniques that optimize execution across **data movement, memory access, and computation efficiency**. These techniques provide the necessary structure for mapping strategies that maximize hardware performance while minimizing bottlenecks.  

Key techniques include:  

- **Data Movement Strategies:** Determine where data is staged during computation to reduce redundant transfers (e.g., Weight Stationary, Output Stationary, Input Stationary).  
- **Memory-Aware Tensor Layouts**, which influence memory access patterns and cache efficiency (e.g., row-major vs. channel-major storage formats).  
- **Kernel Fusion**, which minimizes redundant memory writes by combining multiple operations into a single computational step.  
- **Tiling**, which partitions large computations into smaller, memory-friendly blocks to improve cache efficiency and reduce memory bandwidth requirements.  
- **Balancing Computation and Communication**, which involves managing trade-offs between parallel execution and memory access to achieve high throughput.  

Each of these building blocks plays a crucial role in structuring high-performance execution, forming the basis for both heuristic and model-driven optimization techniques. In the next section, we explore how these strategies are adapted to different types of AI models.  

#### Data Movement Patterns  

While computational mapping determines where and when operations occur, its success depends heavily on how efficiently data is accessed and transferred across the memory hierarchy. Unlike traditional computing workloads, which often exhibit structured and predictable memory access patterns, machine learning workloads present irregular access behaviors due to frequent retrieval of weights, activations, and intermediate values.

Even when computational units are mapped efficiently, poor data movement strategies can severely degrade performance, leading to frequent memory stalls and underutilized hardware resources. If data cannot be supplied to processing elements at the required rate, computational units remain idle, increasing latency, memory traffic, and energy consumption.

To illustrate the impact of data movement inefficiencies, consider a typical matrix multiplication operation, which forms the backbone of many machine learning models:

```python
## Matrix multiplication where:
## weights: [512 x 256] - model parameters
## input:   [256 x 32]  - batch of activations
## Z:       [512 x 32]  - output activations

## Computing each output element Z[i,j]:
for i in range(512):
    for j in range(32):
        for k in range(256):
            Z[i,j] += weights[i,k] * input[k,j]
```

This computation reveals several critical dataflow challenges.  

The first challenge is the number of memory accesses required. For each output $Z[i, j]$, the computation must fetch an entire row of weights from the weight matrix and a full column of activations from the input matrix. Since the weight matrix contains 512 rows and the input matrix contains 32 columns, this results in repeated memory accesses that place a significant burden on memory bandwidth.  

The second challenge comes from weight reuse. The same weights are applied to multiple inputs, meaning that an ideal mapping strategy should maximize weight locality to avoid redundant memory fetches. Without proper reuse, the accelerator would waste bandwidth loading the same weights multiple times.  

The third challenge involves the accumulation of intermediate results. Since each element in $Z[i,j]$ requires contributions from 256 different weight-input pairs, partial sums must be stored and retrieved before the final value is computed. If these intermediate values are stored inefficiently, the system will require frequent memory accesses, further increasing bandwidth demands.  

A natural way to mitigate these challenges is to leverage SIMD and SIMT execution models, which allow multiple values to be fetched in parallel. However, even with these optimizations, data movement remains a bottleneck. The issue is not just how quickly data is retrieved but how often it must be moved and where it is placed within the memory hierarchy.  

To address these constraints, accelerators implement dataflow strategies that determine which data remains fixed in memory and which data is streamed dynamically. These strategies aim to maximize reuse of frequently accessed data, thereby reducing the need for redundant memory fetches. The effectiveness of a given dataflow strategy depends on the specific workload—for example, deep convolutional networks benefit from keeping weights stationary, while fully connected layers may require a different approach.

##### Weight Stationary   

The Weight Stationary strategy keeps weights fixed in local memory, while input activations and partial sums are streamed through the system. This approach is particularly beneficial in convolutional neural networks (CNNs) and matrix multiplications, where the same set of weights is applied across multiple inputs. By ensuring weights remain stationary, this method reduces redundant memory fetches, which helps alleviate bandwidth bottlenecks and improves energy efficiency.

A key advantage of the weight stationary approach is that it maximizes weight reuse, reducing the frequency of memory accesses to external storage. Since weight parameters are often shared across multiple computations, keeping them in local memory eliminates unnecessary data movement, lowering the overall energy cost of computation. This makes it particularly effective for architectures where weights represent the dominant memory overhead, such as systolic arrays and custom accelerators designed for machine learning.

A simplified Weight Stationary implementation for matrix multiplication is illustrated below:

```python
## Weight Stationary Matrix Multiplication
## - Weights remain fixed in local memory
## - Input activations stream through
## - Partial sums accumulate for final output

for weight_block in weights:   # Load and keep weights stationary
    load_to_local(weight_block)  # Fixed in local storage
    for input_block in inputs:   # Stream inputs dynamically
        for output_block in outputs:  # Compute results
            output_block += compute(weight_block, input_block)  # Reuse weights across inputs
```

This implementation follows the core principles of weight stationary execution:

- **Weights are loaded once into local memory** and remain fixed.
- **Inputs are streamed dynamically**, ensuring minimal redundant memory accesses.
- **Partial sums accumulate efficiently**, avoiding unnecessary data movement.

By keeping weights fixed in local storage, memory bandwidth requirements are significantly reduced, as weights do not need to be reloaded for each new computation. Instead, the system efficiently reuses the stored weights across multiple input activations, allowing for high throughput execution. This makes weight stationary dataflow highly effective for workloads with heavy weight reuse patterns, such as CNNs and matrix multiplications.

However, while this strategy reduces weight-related memory traffic, it introduces trade-offs in input and output movement. Since inputs must be streamed dynamically while weights remain fixed, the efficiency of this approach depends on how well input activations can be delivered to the computational units without causing stalls. Additionally, partial sums—representing intermediate results—must be carefully accumulated to avoid excessive memory traffic. The total performance gain depends on the size of available on-chip memory, as storing larger weight matrices locally can become a constraint in models with millions or billions of parameters.

The weight stationary strategy is well-suited for workloads where weights exhibit high reuse and memory bandwidth is a limiting factor. It is commonly employed in CNNs, systolic arrays, and matrix multiplication kernels, where structured weight reuse leads to significant performance improvements. However, for models where input or output reuse is more critical, alternative dataflow strategies, such as output stationary or input stationary, may provide better trade-offs.

##### Output Stationary   

The Output Stationary strategy keeps partial sums fixed in local memory, while weights and input activations stream through the system. This approach is particularly effective for fully connected layers, systolic arrays, and other operations where an output element accumulates contributions from multiple weight-input pairs. By keeping partial sums stationary, this method reduces redundant memory writes, minimizing bandwidth consumption and improving energy efficiency.

A key advantage of the output stationary approach is that it optimizes accumulation efficiency, ensuring that each output element is computed as efficiently as possible before being written to memory. Unlike Weight Stationary, which prioritizes weight reuse, Output Stationary execution is designed to minimize memory bandwidth overhead caused by frequent writes of intermediate results. This makes it well-suited for workloads where accumulation dominates the computational pattern, such as fully connected layers and matrix multiplications in transformer-based models.

A simplified Output Stationary implementation for matrix multiplication is illustrated below:

```python
## Output Stationary Matrix Multiplication
## - Partial sums remain in local memory
## - Weights and input activations stream through dynamically
## - Final outputs are written only once

for output_block in outputs:   # Keep partial sums stationary
    accumulator = 0             # Initialize accumulation buffer
    for weight_block, input_block in zip(weights, inputs):  
        accumulator += compute(weight_block, input_block)  # Accumulate partial sums
    store_output(accumulator)  # Single write to memory
```

This implementation follows the core principles of output stationary execution:
- Partial sums are kept in local memory throughout the computation.
- Weights and inputs are streamed dynamically, ensuring that intermediate results remain locally accessible.
- Final outputs are written back to memory only once, reducing unnecessary memory traffic.

By accumulating partial sums locally, this approach eliminates excessive memory writes, improving overall system efficiency. In architectures such as systolic arrays, where computation progresses through a grid of processing elements, keeping partial sums stationary aligns naturally with structured accumulation workflows, reducing synchronization overhead.

However, while Output Stationary reduces memory write traffic, it introduces trade-offs in weight and input movement. Since weights and activations must be streamed dynamically, the efficiency of this approach depends on how well data can be fed into the system without causing stalls. Additionally, parallel implementations must carefully synchronize updates to partial sums, especially in architectures where multiple processing elements contribute to the same output.

The Output Stationary strategy is most effective for workloads where accumulation is the dominant operation and minimizing intermediate memory writes is critical. It is commonly employed in fully connected layers, attention mechanisms, and systolic arrays, where structured accumulation leads to significant performance improvements. However, for models where input reuse is more critical, alternative dataflow strategies, such as Input Stationary, may provide better trade-offs.

##### Input Stationary   

The Input Stationary strategy keeps input activations fixed in local memory, while weights and partial sums stream through the system. This approach is particularly effective for batch processing, transformer models, and sequence-based architectures, where input activations are reused across multiple computations. By ensuring that activations remain in local memory, this method reduces redundant input fetches, improving data locality and minimizing memory traffic.

A key advantage of the Input Stationary approach is that it maximizes input reuse, reducing the frequency of memory accesses for activations. Since many models, especially those in natural language processing (NLP) and recommendation systems, process the same input data across multiple computations, keeping inputs stationary eliminates unnecessary memory transfers, thereby lowering energy consumption. This strategy is particularly useful when dealing with large batch sizes, where a single batch of input activations contributes to multiple weight transformations.

A simplified Input Stationary implementation for matrix multiplication is illustrated below:

```python
## Input Stationary Matrix Multiplication
## - Input activations remain in local memory
## - Weights stream through dynamically
## - Partial sums accumulate and are written out

for input_block in inputs:   # Keep input activations stationary
    load_to_local(input_block)  # Fixed in local storage
    for weight_block in weights:   # Stream weights dynamically
        for output_block in outputs:  # Compute results
            output_block += compute(weight_block, input_block)  # Reuse inputs across weights
```

This implementation follows the core principles of input stationary execution:
- **Input activations are loaded into local memory** and remain fixed during computation.
- **Weights are streamed dynamically**, ensuring efficient application across multiple inputs.
- **Partial sums are accumulated and written out**, optimizing memory bandwidth usage.

By keeping input activations stationary, this strategy minimizes redundant memory accesses to input data, significantly reducing external memory bandwidth requirements. This is particularly beneficial in transformer architectures, where each token in an input sequence is used across multiple attention heads and layers. Additionally, in batch processing scenarios, keeping input activations in local memory improves data locality, making it well-suited for fully connected layers and matrix multiplications.

However, while Input Stationary reduces memory traffic for activations, it introduces trade-offs in weight and output movement. Since weights must be streamed dynamically while inputs remain fixed, the efficiency of this approach depends on how well weights can be delivered to the computational units without causing stalls. Additionally, partial sums must be accumulated efficiently before being written back to memory, which may require additional buffering mechanisms.

The Input Stationary strategy is most effective for workloads where input activations exhibit high reuse, and memory bandwidth for inputs is a critical constraint. It is commonly employed in transformers, recurrent networks, and batch processing workloads, where structured input reuse leads to significant performance improvements. However, for models where output accumulation is more critical, alternative dataflow strategies, such as Output Stationary, may provide better trade-offs.

#### Memory-Aware Tensor Layouts

Efficient execution of machine learning workloads depends not only on how data moves (dataflow strategies) but also on how data is stored and accessed in memory. Tensor layouts—the way multidimensional data is arranged in memory—can significantly impact memory access efficiency, cache performance, and computational throughput. Poorly chosen layouts can lead to excessive memory stalls, inefficient cache usage, and increased data movement costs.

In AI accelerators, tensor layout optimization is particularly important because data is frequently accessed in patterns dictated by the underlying hardware architecture. Choosing the right layout ensures that memory accesses align with hardware-friendly access patterns, minimizing overhead from costly memory transactions.

While developers can sometimes manually specify tensor layouts, the choice is often determined automatically by machine learning frameworks (e.g., TensorFlow, PyTorch, JAX), compilers, or AI accelerator runtimes. Low-level optimization tools such as cuDNN (for NVIDIA GPUs), XLA (for TPUs), and MLIR (for custom accelerators) may rearrange tensor layouts dynamically to optimize performance. In high-level frameworks, layout transformations are typically applied transparently, but developers working with custom kernels or low-level libraries (e.g., CUDA, Metal, or OpenCL) may have direct control over tensor format selection.

For example, in PyTorch, users can manually modify layouts using `tensor.permute()` or `tensor.contiguous()` to ensure efficient memory access. In TensorFlow, layout optimizations are often applied internally by the XLA compiler, choosing between NHWC (row-major) and NCHW (channel-major) based on the target hardware. Hardware-aware machine learning libraries, such as cuDNN for GPUs or OneDNN for CPUs, enforce specific memory layouts to maximize cache locality and SIMD efficiency. Ultimately, while developers may have some control over tensor layout selection, most layout decisions are driven by the compiler and runtime system, ensuring that tensors are stored in memory in a way that best suits the underlying hardware.

##### Row-Major Layout (NHWC - Batch, Height, Width, Channels)  

Row-major layout refers to the way multi-dimensional tensors are stored in memory, where elements are arranged row by row, ensuring that all values in a given row are placed contiguously before moving to the next row. This storage format is widely used in general-purpose CPUs and some machine learning frameworks because it aligns naturally with sequential memory access patterns, making it more cache-efficient for certain types of operations.

To understand how row-major layout works, consider a single RGB image represented as a tensor of shape (Height, Width, Channels). If the image has a size of 3 × 3 pixels with 3 channels (RGB), the corresponding tensor is structured as (3, 3, 3). The values are stored in memory as follows:

$$
I(0,0,0), I(0,0,1), I(0,0,2), I(0,1,0), I(0,1,1), I(0,1,2), I(0,2,0), I(0,2,1), I(0,2,2), \dots
$$

Each row is stored contiguously, meaning all pixel values in the first row are placed sequentially in memory before moving on to the second row. This ordering is advantageous because CPUs and cache hierarchies are optimized for sequential memory access. When data is accessed in a row-wise fashion, such as when applying element-wise operations like activation functions or basic arithmetic transformations, memory fetches are efficient, and cache utilization is maximized.

The efficiency of row-major storage becomes particularly evident in CPU-based machine learning workloads, where operations such as batch normalization, matrix multiplications, and element-wise arithmetic frequently process rows of data sequentially. Since modern CPUs employ cache prefetching mechanisms, a row-major layout allows the next required data values to be preloaded into cache ahead of execution, reducing memory latency and improving overall computational throughput.

However, row-major layout can introduce inefficiencies when performing operations that require accessing data across channels rather than across rows. Consider a convolutional layer that applies a filter across multiple channels of an input image. Since channel values are interleaved in row-major storage, the convolution operation must jump across memory locations to fetch all the necessary channel values for a given pixel. These strided memory accesses can be costly on hardware architectures that rely on vectorized execution and coalesced memory access, such as GPUs and TPUs.

Despite these limitations, row-major layout remains a dominant storage format in CPU-based machine learning frameworks. TensorFlow, for instance, defaults to the NHWC (row-major) format on CPUs, ensuring that cache locality is optimized for sequential processing. However, when targeting GPUs, frameworks often rearrange data dynamically to take advantage of more efficient memory layouts, such as channel-major storage, which aligns better with parallelized computation.

##### Channel-Major Layout (NCHW - Batch, Channels, Height, Width)  

In contrast to row-major layout, channel-major layout arranges data in memory such that all values for a given channel are stored together before moving to the next channel. This format is particularly beneficial for GPUs, TPUs, and other AI accelerators, where vectorized operations and memory coalescing significantly impact computational efficiency.  

To understand how channel-major layout works, consider the same RGB image tensor of size (Height, Width, Channels) = (3, 3, 3). Instead of storing pixel values row by row, the data is structured channel-first in memory as follows:

$$
I(0,0,0), I(1,0,0), I(2,0,0), I(0,1,0), I(1,1,0), I(2,1,0), \dots, I(0,0,1), I(1,0,1), I(2,0,1), \dots, I(0,0,2), I(1,0,2), I(2,0,2), \dots
$$

In this format, all red channel values for the entire image are stored first, followed by all green values, and then all blue values. This ordering allows hardware accelerators to efficiently load and process data across channels in parallel, which is crucial for convolution operations and SIMD (Single Instruction, Multiple Data) execution models.

The advantage of channel-major layout becomes clear when performing convolutions in machine learning models. Convolutional layers process images by applying a shared set of filters across all channels. When the data is stored in a channel-major format, a convolution kernel can load an entire channel efficiently, reducing the number of scattered memory fetches. This reduces memory latency, improves throughput, and enhances data locality for matrix multiplications, which are fundamental to machine learning workloads.

Because GPUs and TPUs rely on memory coalescing—a technique where consecutive threads fetch contiguous memory addresses—channel-major layout aligns naturally with the way these processors execute parallel computations. For example, in NVIDIA GPUs, each thread in a warp (a group of threads executed simultaneously) processes different elements of the same channel, ensuring that memory accesses are efficient and reducing the likelihood of strided memory accesses, which can degrade performance.

Despite its advantages in machine learning accelerators, channel-major layout can introduce inefficiencies when running on general-purpose CPUs. Since CPUs optimize for sequential memory access, storing all values for a single channel before moving to the next disrupts cache locality for row-wise operations. This is why many machine learning frameworks (e.g., TensorFlow, PyTorch) default to row-major (NHWC) on CPUs and channel-major (NCHW) on GPUs—optimizing for the strengths of each hardware type.

Modern AI frameworks and compilers often transform tensor layouts dynamically depending on the execution environment. For instance, TensorFlow and PyTorch automatically switch between NHWC and NCHW based on whether a model is running on a CPU, GPU, or TPU, ensuring that the memory layout aligns with the most efficient execution path.

##### Comparing Row-Major and Channel-Major Layouts  

Both row-major (NHWC) and channel-major (NCHW) layouts serve distinct purposes in machine learning workloads, with their efficiency largely determined by the hardware architecture, memory access patterns, and computational requirements. The choice of layout directly influences cache utilization, memory bandwidth efficiency, and processing throughput. @tbl-major summarizes the differences between row-major (NHWC) and channel-major (NCHW) layouts in terms of performance trade-offs and hardware compatibility.

+-------------------------+--------------------------------------------------------+----------------------------------------------------------+
| Feature                 | Row-Major (NHWC)                                       | Channel-Major (NCHW)                                     |
+:========================+:=======================================================+:=========================================================+
| Memory Storage Order    | Pixels are stored row-by-row, channel interleaved      | All values for a given channel are stored together first |
+-------------------------+--------------------------------------------------------+----------------------------------------------------------+
| Best for                | CPUs, element-wise operations                          | GPUs, TPUs, convolution operations                       |
+-------------------------+--------------------------------------------------------+----------------------------------------------------------+
| Cache Efficiency        | High cache locality for sequential row access          | Optimized for memory coalescing across channels          |
+-------------------------+--------------------------------------------------------+----------------------------------------------------------+
| Convolution Performance | Requires strided memory accesses (inefficient on GPUs) | Efficient for GPU convolution kernels                    |
+-------------------------+--------------------------------------------------------+----------------------------------------------------------+
| Memory Fetching         | Good for operations that process rows sequentially     | Optimized for SIMD execution across channels             |
+-------------------------+--------------------------------------------------------+----------------------------------------------------------+
| Default in Frameworks   | Default on CPUs (e.g., TensorFlow NHWC)                | Default on GPUs (e.g., cuDNN prefers NCHW)               |
+-------------------------+--------------------------------------------------------+----------------------------------------------------------+

: Comparison of row-major (NHWC) vs. channel-major (NCHW) layouts. {#tbl-major .striped .hover}

The decision to use row-major (NHWC) or channel-major (NCHW) layouts is not always made manually by developers. Instead, machine learning frameworks and AI compilers often determine the optimal layout dynamically based on the target hardware and operation type. CPUs tend to favor NHWC due to cache-friendly sequential memory access, while GPUs perform better with NCHW, which reduces memory fetch overhead for machine learning computations.

In practice, modern AI compilers such as TensorFlow’s XLA and PyTorch’s TorchScript perform automatic layout transformations, converting tensors between NHWC and NCHW as needed to optimize performance across different processing units. This ensures that machine learning models achieve the highest possible throughput without requiring developers to manually specify tensor layouts.

#### Kernel Fusion

##### Intermediate Memory Write

Optimizing memory access is a fundamental challenge in AI acceleration. While AI models rely on high-throughput computation, their performance is often constrained by memory bandwidth and intermediate memory writes rather than pure arithmetic operations. Every time an operation produces an intermediate result that must be written to memory and later read back, execution stalls occur due to data movement overhead.

To better understand why kernel fusion is necessary, consider a simple sequence of operations in a machine learning model. Many AI workloads, particularly those involving element-wise transformations, introduce unnecessary intermediate memory writes, leading to increased memory bandwidth consumption and reduced execution efficiency.

In a naïve execution model, each operation is treated as a separate kernel, meaning that each intermediate result is written to memory, only to be read back for the next operation. The execution flow looks like this:

```python
import torch

## Input tensor
X = torch.randn(1024, 1024).cuda()

## Step-by-step execution (naïve approach)
X1 = torch.relu(X)            # Intermediate tensor stored in memory
X2 = torch.batch_norm(X1)     # Another intermediate tensor stored
Y  = 2.0 * X2 + 1.0           # Final result
```

Each operation produces an intermediate tensor that must be written to memory and retrieved for the next operation. On large tensors, this overhead of moving data can outweigh the computational cost of the operations. @tbl-memory-footprint illustrates the memory overhead in a naïve execution model. While only the final result $Y$ is needed, storing multiple intermediate tensors creates unnecessary memory traffic and inefficient memory usage. This data movement bottleneck significantly impacts performance, making memory optimization crucial for AI accelerators.

+--------------+----------------------------------+
| Tensor       | Size (MB) for 1024 × 1024 Tensor |
+:=============+:=================================+
| X            | 4 MB                             |
+--------------+----------------------------------+
| X'           | 4 MB                             |
+--------------+----------------------------------+
| X''          | 4 MB                             |
+--------------+----------------------------------+
| Y            | 4 MB                             |
+--------------+----------------------------------+
| Total Memory | 16 MB                            |
+--------------+----------------------------------+

: Memory footprint of a naïve execution model with intermediate tensor storage. {#tbl-memory-footprint .striped .hover}

Even though only the final result $Y$ is needed, three additional intermediate tensors consume extra memory without contributing to final output storage. This excessive memory usage limits scalability and wastes memory bandwidth, particularly in AI accelerators where minimizing data movement is critical.

##### Fusing Kernels for Efficient Memory Reuse

Kernel fusion is a key optimization technique that aims to minimize intermediate memory writes, reducing the memory footprint and bandwidth consumption of machine learning workloads.

Kernel fusion involves merging multiple computation steps into a single, optimized operation, eliminating the need for storing and reloading intermediate tensors. Instead of executing each layer or element-wise operation separately—where each step writes its output to memory before the next step begins—fusion enables direct data propagation between operations, keeping computations within high-speed registers or local memory.

A common machine learning sequence might involve applying a nonlinear activation function (e.g., ReLU), followed by batch normalization, and then scaling the values for input to the next layer. In a naïve implementation, each of these steps generates an intermediate tensor, which is written to memory, read back, and then modified again:

$$
X' = \text{ReLU}(X)
X'' = \text{BatchNorm}(X')
Y = \alpha \cdot X'' + \beta
$$

With kernel fusion, these operations are combined into a single computation step, allowing the entire transformation to occur without generating unnecessary intermediate tensors:

$$
Y = \alpha \cdot \text{BatchNorm}(\text{ReLU}(X)) + \beta
$$

@tbl-fusion-benefits highlights the impact of operation fusion on memory efficiency. By keeping intermediate results in registers or local memory rather than writing them to main memory, fusion significantly reduces memory traffic. This optimization is especially beneficial on highly parallel architectures like GPUs and TPUs, where minimizing memory accesses translates directly into improved execution throughput. Compared to the naïve execution model, fused execution eliminates the need for storing intermediate tensors, dramatically lowering the total memory footprint and improving overall efficiency.

+-----------------+-----------------------------+-------------------------+
| Execution Model | Intermediate Tensors Stored | Total Memory Usage (MB) |
+:================+:============================+:========================+
| Naïve Execution | X', X''                     | 16 MB                   |
+-----------------+-----------------------------+-------------------------+
| Fused Execution | None                        | 4 MB                    |
+-----------------+-----------------------------+-------------------------+

: Reduction in memory usage through operation fusion. {#tbl-fusion-benefits .striped .hover}

Kernel fusion reduces total memory consumption from 16 MB to 4 MB, eliminating redundant memory writes while improving execution efficiency.

##### Performance Benefits and Hardware Constraints

Kernel fusion brings several key advantages that enhance memory efficiency and computation throughput. By reducing memory accesses, fused kernels ensure that intermediate values stay within registers instead of being repeatedly written to and read from memory. This significantly lowers memory traffic, which is one of the primary bottlenecks in machine learning workloads. GPUs and TPUs, in particular, benefit from kernel fusion because high-bandwidth memory is a scarce resource, and reducing memory transactions leads to better utilization of compute units.

However, not all operations can be fused. Element-wise operations, such as ReLU, batch normalization, and simple arithmetic transformations, are ideal candidates for fusion since their computations depend only on single elements from the input tensor. In contrast, operations with complex data dependencies, such as matrix multiplications and convolutions, involve global data movement, making direct fusion impractical. These operations require values from multiple input elements to compute a single output, which prevents them from being executed as a single fused kernel.

Another major consideration is register pressure. Fusing multiple operations means all temporary values must be kept in registers rather than memory. While this eliminates redundant memory writes, it also increases register demand. If a fused kernel exceeds the available registers per thread, the system must spill excess values into shared memory, introducing additional latency and potentially negating the benefits of fusion. On GPUs, where thread occupancy (the number of threads that can run in parallel) is limited by available registers, excessive fusion can reduce parallelism, leading to diminishing returns.

Different AI accelerators and compilers handle fusion in distinct ways. NVIDIA GPUs, for example, favor warp-level parallelism, where element-wise fusion is straightforward. TPUs, on the other hand, prioritize systolic array execution, which is optimized for matrix-matrix operations rather than element-wise fusion. AI compilers such as XLA (TensorFlow), TorchScript (PyTorch), TensorRT (NVIDIA), and MLIR automatically detect fusion opportunities and apply heuristics to balance memory savings and execution efficiency.

Despite its advantages, fusion is not always beneficial. Some AI frameworks allow developers to disable fusion selectively, especially when debugging performance issues or making frequent model modifications. The decision to fuse operations must consider trade-offs between memory efficiency, register usage, and hardware execution constraints to ensure that fusion leads to tangible performance improvements.

#### Tiling for Memory Efficiency

One of the fundamental challenges in AI acceleration is efficiently managing memory access. While modern AI accelerators offer high computational throughput, their performance is often limited by memory bandwidth rather than raw processing power. If data cannot be supplied to processing units fast enough, execution stalls occur, leading to wasted cycles and inefficient hardware utilization.  

Tiling is a technique used to mitigate this issue by restructuring computations into smaller, memory-friendly subproblems. Instead of processing entire matrices or tensors at once—leading to excessive memory traffic—tiling partitions computations into smaller blocks (tiles) that fit within fast local memory (e.g., caches, shared memory, or registers). By doing so, tiling increases data reuse, minimizes memory fetches, and improves overall computational efficiency.  

A classic example of inefficient memory access is matrix multiplication, which is widely used in AI models. Without tiling, the naïve approach results in repeated memory accesses for the same data, leading to unnecessary bandwidth consumption:

```python
## Naïve Matrix Multiplication (No Tiling)
for i in range(N):
    for j in range(N):
        for k in range(N):
            C[i, j] += A[i, k] * B[k, j]  # Repeatedly fetching A[i, k] and B[k, j]
```

Each iteration requires loading elements from matrices $A$ and $B$ multiple times from memory, causing excessive data movement. As the size of the matrices increases, the memory bottleneck worsens, limiting performance.  

Tiling addresses this problem by ensuring that smaller portions of matrices are loaded into fast memory, reused efficiently, and only written back to main memory when necessary. This technique is especially crucial in AI accelerators, where memory accesses dominate execution time.  

In the following sections, we will explore the fundamental principles of tiling, its different strategies, and the key trade-offs involved in selecting an effective tiling approach.

##### Fundamentals of Tiling

Tiling is based on a simple but powerful principle: instead of operating on an entire data structure at once, computations are divided into smaller tiles that fit within the available fast memory. By structuring execution around these tiles, data reuse is maximized, reducing redundant memory accesses and improving overall efficiency.  

Consider matrix multiplication, a key operation in machine learning workloads. The operation computes the output matrix $C$ from two input matrices $A$ and $B$:  

$$
C = A \times B
$$

where each element $C[i,j]$ is computed as:

$$
C[i,j] = \sum_{k} A[i,k] \times B[k,j]
$$

A naïve implementation follows this formula directly:

```python
## Naïve Matrix Multiplication (No Tiling)
for i in range(N):
    for j in range(N):
        for k in range(N):
            C[i, j] += A[i, k] * B[k, j]  # Repeatedly fetching A[i, k] and B[k, j]
```

At first glance, this approach seems correct—it computes the desired result and follows the mathematical definition. However, the issue lies in how memory is accessed. Every time the innermost loop runs, it fetches an element from matrix $A$ and matrix $B$ from memory, performs a multiplication, and updates an element in matrix $C$. Because matrices are large, the processor frequently reloads the same values from memory, even though they were just used in previous computations.  

This unnecessary data movement is expensive. Fetching values from main memory (DRAM) is hundreds of times slower than accessing values stored in on-chip cache or registers. If the same values must be reloaded multiple times instead of being stored in fast memory, execution slows down significantly.  

##### How Tiling Improves Performance

Instead of computing one element at a time and constantly moving data in and out of slow memory, tiling processes submatrices (tiles) at a time, keeping frequently used values in fast memory. The idea is to divide the matrices into smaller blocks that fit within the processor’s cache or shared memory, ensuring that once a block is loaded, it is reused multiple times before moving to the next one.  

A tiled implementation of matrix multiplication looks like this:  

```python
## Tiled Matrix Multiplication
TILE_SIZE = 32  # Choose a tile size based on hardware constraints

for i in range(0, N, TILE_SIZE):
    for j in range(0, N, TILE_SIZE):
        for k in range(0, N, TILE_SIZE):
            # Compute the submatrix C[i:i+TILE_SIZE, j:j+TILE_SIZE]
            for ii in range(i, i + TILE_SIZE):
                for jj in range(j, j + TILE_SIZE):
                    for kk in range(k, k + TILE_SIZE):
                        C[ii, jj] += A[ii, kk] * B[kk, jj]
```

This restructuring significantly improves performance for three main reasons:  

1. **Better Memory Reuse:** Instead of fetching elements from $A$ and $B$ repeatedly from slow memory, this approach loads a small tile of data into fast memory, performs multiple computations using it, and only then moves on to the next tile. This minimizes redundant memory accesses.  

2. **Reduced Memory Bandwidth Usage:** Since each tile is used multiple times before being evicted, memory traffic is reduced. Instead of repeatedly accessing DRAM, most required data is available in L1/L2 cache or shared memory, leading to faster execution.  

3. **Increased Compute Efficiency:** Processors spend less time waiting for data and more time performing useful computations. In architectures like GPUs and TPUs, where thousands of parallel processing units operate simultaneously, tiling ensures that data is read and processed in a structured manner, avoiding unnecessary stalls.  

This technique is particularly effective in AI accelerators, where machine learning workloads consist of large matrix multiplications and tensor transformations. Without tiling, these workloads quickly become memory-bound, meaning performance is constrained by how fast data can be retrieved rather than by the raw computational power of the processor.  

##### Tiling Methods

While the general principle of tiling remains the same—partitioning large computations into smaller subproblems to improve memory reuse—there are different ways to apply tiling based on the structure of the computation and hardware constraints. The two primary tiling strategies are spatial tiling and temporal tiling. These strategies optimize different aspects of computation and memory access, and in practice, they are often combined to achieve the best performance.

###### Spatial Tiling

Spatial tiling focuses on partitioning data structures into smaller blocks that fit within the fast memory of the processor. This approach ensures that each tile is fully processed before moving to the next, reducing redundant memory accesses. Spatial tiling is widely used in operations such as matrix multiplication, convolutions, and attention mechanisms in transformer models.  

Returning to our tiled matrix multiplication example, we can see spatial tiling in action:

```python
## Tiled Matrix Multiplication (Spatial Tiling)
TILE_SIZE = 32  # Tile size chosen based on available fast memory

for i in range(0, N, TILE_SIZE):
    for j in range(0, N, TILE_SIZE):
        for k in range(0, N, TILE_SIZE):
            # Process a submatrix (tile) at a time
            for ii in range(i, i + TILE_SIZE):
                for jj in range(j, j + TILE_SIZE):
                    for kk in range(k, k + TILE_SIZE):
                        C[ii, jj] += A[ii, kk] * B[kk, jj]
```

In this implementation, each tile of $A$ and $B$ is loaded into cache or shared memory before processing, ensuring that the same data does not need to be fetched repeatedly from slower memory. The tile is fully used before moving to the next block, minimizing redundant memory accesses. Since data is accessed in a structured, localized way, cache efficiency improves significantly.  

Spatial tiling is particularly beneficial when dealing with large tensors that do not fit entirely in fast memory. By breaking them into smaller tiles, computations remain localized, avoiding excessive data movement between memory levels. This technique is widely used in AI accelerators where machine learning workloads involve large-scale tensor operations that require careful memory management to achieve high performance.  

###### Temporal Tiling

While spatial tiling optimizes how data is partitioned, temporal tiling focuses on reorganizing the computation itself to improve data reuse over time. Many machine learning workloads involve operations where the same data is accessed repeatedly across multiple iterations. Without temporal tiling, this often results in redundant memory fetches, leading to inefficiencies. Temporal tiling, also known as loop blocking, restructures the computation to ensure that frequently used data stays in fast memory for as long as possible before moving on to the next computation.  

A classic example where temporal tiling is beneficial is convolutional operations, where the same set of weights is applied to multiple input regions. Without loop blocking, these weights might be loaded from memory multiple times for each computation. With temporal tiling, the computation is reordered so that the weights remain in fast memory across multiple inputs, reducing unnecessary memory fetches and improving overall efficiency.  

A simplified example of loop blocking in matrix multiplication is shown below:

```python
## Matrix Multiplication with Temporal Tiling (Loop Blocking)
for i in range(0, N, TILE_SIZE):
    for j in range(0, N, TILE_SIZE):
        for k in range(0, N, TILE_SIZE):
            # Load tile into fast memory before computation
            A_tile = A[i:i+TILE_SIZE, k:k+TILE_SIZE]
            B_tile = B[k:k+TILE_SIZE, j:j+TILE_SIZE]

            for ii in range(TILE_SIZE):
                for jj in range(TILE_SIZE):
                    for kk in range(TILE_SIZE):
                        C[i+ii, j+jj] += A_tile[ii, kk] * B_tile[kk, jj]
```

Temporal tiling improves performance by ensuring that the data loaded into fast memory is used multiple times before being evicted. In this implementation, small tiles of matrices $A$ and $B$ are explicitly loaded into temporary storage before performing computations, reducing memory fetch overhead. This restructuring allows the computation to process an entire tile before moving to the next, thereby reducing the number of times data must be loaded from slower memory.  

This technique is particularly useful in workloads where certain values are used repeatedly, such as convolutions, recurrent neural networks (RNNs), and self-attention mechanisms in transformers. By applying loop blocking, AI accelerators can significantly reduce memory stalls and improve execution throughput.

##### Challenges and Trade-offs in Tiling  

While tiling significantly improves performance by optimizing memory reuse and reducing redundant memory accesses, it introduces several challenges and trade-offs. Selecting the right tile size is a critical decision, as it directly affects computational efficiency and memory bandwidth usage. If the tile size is too small, the benefits of tiling diminish, as memory fetches still dominate execution time. On the other hand, if the tile size is too large, it may exceed the available fast memory, causing cache thrashing and performance degradation.

Load balancing is another key concern. In architectures such as GPUs and TPUs, computations are executed in parallel across thousands of processing units. If tiles are not evenly distributed, some units may remain idle while others are overloaded, leading to suboptimal utilization of computational resources. Effective tile scheduling ensures that parallel execution remains balanced and efficient.

Data movement overhead is also an important consideration. Although tiling reduces the number of slow memory accesses, transferring tiles between different levels of memory still incurs a cost. This is especially relevant in hierarchical memory systems, where accessing data from cache is much faster than accessing it from DRAM. Efficient memory prefetching and scheduling strategies are required to minimize latency and ensure that data is available when needed.

Beyond spatial and temporal tiling, hybrid approaches combine elements of both strategies to achieve optimal performance. Hybrid tiling adapts to workload-specific constraints by dynamically adjusting tile sizes or reordering computations based on real-time execution conditions. For example, some AI accelerators use spatial tiling for matrix multiplications while employing temporal tiling for weight reuse in convolutional layers. 

In addition to tiling, there are other methods for optimizing memory usage and computational efficiency. Techniques such as register blocking, double buffering, and hierarchical tiling extend the basic tiling principles to further optimize execution. AI compilers and runtime systems, such as TensorFlow XLA, TVM, and MLIR, automatically select tiling strategies based on hardware constraints, allowing for fine-tuned performance optimization without manual intervention.

@tbl-tiling-strategies provides a comparative overview of spatial, temporal, and hybrid tiling approaches, highlighting their respective benefits and trade-offs. 

+--------------------+--------------------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------+
| Aspect             | Spatial Tiling (Data Tiling)                                                               | Temporal Tiling (Loop Blocking)                                        | Hybrid Tiling                                                  |
+:===================+:===========================================================================================+:=======================================================================+:===============================================================+
| Primary Goal       | Reduce memory accesses by keeping data in fast memory longer                               | Increase data reuse across loop iterations                             | Adapt dynamically to workload constraints                      |
+--------------------+--------------------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------+
| Optimization Focus | Partitioning data structures into smaller, memory-friendly blocks                          | Reordering computations to maximize reuse before eviction              | Balancing spatial and temporal reuse strategies                |
+--------------------+--------------------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------+
| Memory Usage       | Improves cache locality and reduces DRAM access                                            | Keeps frequently used data in fast memory for multiple iterations      | Minimizes data movement while ensuring high reuse              |
+--------------------+--------------------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------+
| Common Use Cases   | Matrix multiplications, CNNs, self-attention in transformers                               | Convolutions, recurrent neural networks (RNNs), iterative computations | AI accelerators with hierarchical memory, mixed workloads      |
+--------------------+--------------------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------+
| Performance Gains  | Reduced memory bandwidth requirements, better cache utilization                            | Lower memory fetch latency, improved data locality                     | Maximized efficiency across multiple hardware types            |
+--------------------+--------------------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------+
| Challenges         | Requires careful tile size selection, inefficient for workloads with minimal spatial reuse | Can increase register pressure, requires loop restructuring            | Complexity in tuning tile size and execution order dynamically |
+--------------------+--------------------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------+
| Best When          | Data is large and needs to be partitioned for efficient processing                         | The same data is accessed multiple times across iterations             | Both data partitioning and iteration-based reuse are important |
+--------------------+--------------------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------+

: Comparative analysis of spatial, temporal, and hybrid tiling strategies. {#tbl-tiling-strategies .striped .hover}

As machine learning models continue to grow in size and complexity, tiling remains a critical tool for improving hardware efficiency, ensuring that AI accelerators operate at their full potential. While manual tiling strategies can provide substantial benefits, modern compilers and hardware-aware optimization techniques further enhance performance by automatically selecting the most effective tiling strategies for a given workload.

### Applying Mapping Strategies  

While foundational mapping techniques apply broadly, their effectiveness varies based on the computational structure, data access patterns, and parallelization opportunities of different neural network architectures. Each architecture imposes distinct constraints on data movement, memory hierarchy, and computation scheduling, requiring tailored mapping strategies to optimize performance.

A structured approach to mapping is essential to address the combinatorial explosion of choices that arise when assigning computations to AI accelerators. Rather than treating each model as a separate optimization problem, we recognize that the same fundamental principles apply across different architectures—only their priority shifts based on workload characteristics. The goal is to systematically select and apply mapping strategies that maximize efficiency for different types of machine learning models.

To demonstrate this, we examine three representative AI workloads, each with unique computational demands:  

- **Convolutional Neural Networks (CNNs):** These models feature spatial data reuse, making weight-stationary execution and tiling particularly effective.  
- **Transformers:** These are memory-bound workloads, requiring optimizations such as KV-cache management, fused attention mechanisms, and parallel execution strategiesto minimize memory traffic.  
- **Multi-Layer Perceptrons (MLPs):** MLPs involve large matrix multiplications, where structured tiling, efficient weight layouts, and memory-aware executionare critical for performance.  

Despite their differences, each of these models follows a common set of mapping principles, with variations in how optimizations are prioritized. The following table provides a structured mapping between different optimization strategies and their suitability for Convolutional Neural Networks (CNNs), Transformers, and Multi-Layer Perceptrons (MLPs). This table serves as a roadmap for selecting appropriate mapping strategies for different machine learning workloads.

+------------------------------+--------------------------+-----------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Optimization Technique       | CNNs                     | Transformers          | MLPs              | Rationale                                                                                                                                                           |
+:=============================+:=========================+:======================+:==================+:====================================================================================================================================================================+
| Dataflow Strategy            | Weight Stationary        | Activation Stationary | Weight Stationary | CNNs reuse filters across spatial locations; Transformers reuse activations (KV-cache); MLPs reuse weights across batches.                                          |
+------------------------------+--------------------------+-----------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Memory-Aware Tensor Layouts  | NCHW (Channel-Major)     | NHWC (Row-Major)      | NHWC              | CNNs favor channel-major for convolution efficiency; Transformers and MLPs prioritize row-major for fast memory access.                                             |
+------------------------------+--------------------------+-----------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Kernel Fusion                | Convolution + Activation | Fused Attention       | GEMM Fusion       | CNNs optimize convolution+activation fusion; Transformers fuse attention mechanisms; MLPs benefit from fused matrix multiplications.                                |
+------------------------------+--------------------------+-----------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Tiling for Memory Efficiency | Spatial Tiling           | Temporal Tiling       | Blocked Tiling    | CNNs tile along spatial dimensions; Transformers use loop blocking to improve sequence memory efficiency; MLPs use blocked tiling for large matrix multiplications. |
+------------------------------+--------------------------+-----------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+

This table highlights that each machine learning model benefits from a different combination of optimization techniques, reinforcing the importance of tailoring execution strategies to the computational and memory characteristics of the workload.

In the following sections, we explore how these optimizations apply to each network type, explaining how CNNs, Transformers, and MLPs leverage specific mapping strategies to improve execution efficiency and hardware utilization.

#### Convolutional Neural Networks (CNNs)  

CNNs are characterized by their structured spatial computations, where small filters (or kernels) are repeatedly applied across an input feature map. This structured weight reuse makes weight stationary execution the most effective strategy for CNNs. Keeping filter weights in fast memory while streaming activations ensures that weights do not need to be repeatedly fetched from slower external memory, significantly reducing memory bandwidth demands. Since each weight is applied to multiple spatial locations, weight stationary execution maximizes arithmetic intensity and minimizes redundant memory transfers. 

Memory-aware tensor layouts also play a critical role in CNN execution. Convolution operations benefit from a channel-major memory format, often represented as NCHW (batch, channels, height, width). This layout aligns with the access patterns of convolutions, enabling efficient memory coalescing on accelerators such as GPUs and TPUs. By storing data in a format that optimizes cache locality, accelerators can fetch contiguous memory blocks efficiently, reducing latency and improving throughput. 

Kernel fusion is another important optimization for CNNs. In a typical machine learning pipeline, convolution operations are often followed by activation functions such as ReLU and batch normalization. Instead of treating these operations as separate computational steps, fusing them into a single kernel reduces intermediate memory writes and improves execution efficiency. This optimization minimizes memory bandwidth pressure by keeping intermediate values in registers rather than writing them to memory and fetching them back in subsequent steps.

Given the size of input images and feature maps, tiling is necessary to ensure that computations fit within fast memory hierarchies. Spatial tiling, where input feature maps are processed in smaller subregions, allows for efficient utilization of on-chip memory while avoiding excessive off-chip memory transfers. This technique ensures that input activations, weights, and intermediate outputs remain within high-speed caches or shared memory as long as possible, reducing memory stalls and improving overall performance.

Together, these optimizations ensure that CNNs make efficient use of available compute resources by maximizing weight reuse, optimizing memory access patterns, reducing redundant memory writes, and structuring computation to fit within fast memory constraints.

#### Transformers  

Unlike CNNs, which rely on structured spatial computations, Transformers process variable-length sequences and rely heavily on attention mechanisms. The primary computational bottleneck in Transformers is memory bandwidth, as attention mechanisms require frequent access to stored key-value pairs across multiple query vectors. Given this access pattern, activation stationary execution is the most effective strategy. By keeping key-value activations in fast memory and streaming query vectors dynamically, activation reuse is maximized while minimizing redundant memory fetches. This approach is critical in reducing bandwidth overhead, especially in long-sequence tasks such as natural language processing.

Memory layout optimization is equally important for Transformers. Unlike CNNs, which benefit from channel-major layouts, Transformers require efficient access to sequences of activations, making a row-major format (NHWC) the preferred choice. This layout ensures that activations are accessed contiguously in memory, reducing cache misses and improving memory coalescing for matrix multiplications. Since attention mechanisms involve multiple batched matrix multiplications, using an optimized memory format ensures that operations are executed with minimal memory overhead.

Kernel fusion plays a key role in optimizing Transformer execution. In self-attention, multiple computational steps—including query-key dot products, softmax normalization, and weighted summation—can be fused into a single operation. Fused attention kernels eliminate intermediate memory writes by computing attention scores and performing weighted summations within a single execution step. This optimization significantly reduces memory traffic, particularly for large batch sizes and long sequences.

Due to the nature of sequence processing, tiling must be adapted to improve memory efficiency. Instead of spatial tiling, which is effective for CNNs, Transformers benefit from temporal tiling, where computations are structured to process sequence blocks efficiently. This method ensures that activations are loaded into fast memory in manageable chunks, reducing excessive memory transfers. Temporal tiling is particularly beneficial for long-sequence models, where the memory footprint of key-value activations grows significantly. By tiling sequences into smaller segments, memory locality is improved, enabling efficient cache utilization and reducing bandwidth pressure.

These optimizations collectively address the primary bottlenecks in Transformer models by prioritizing activation reuse, structuring memory layouts for efficient batched computations, fusing attention operations to reduce intermediate memory writes, and employing tiling techniques suited to sequence-based processing.

#### Multi-Layer Perceptrons (MLPs)  

MLPs primarily consist of fully connected layers, where large matrices of weights and activations are multiplied to produce output representations. Given this structure, weight stationary execution is the most effective strategy for MLPs. Similar to CNNs, MLPs benefit from keeping weights in local memory while streaming activations dynamically, as this ensures that weight matrices, which are typically reused across multiple activations in a batch, do not need to be frequently reloaded.

The preferred memory layout for MLPs aligns with that of Transformers, as matrix multiplications are more efficient when using a row-major (NHWC) format. Since activation matrices are processed in batches, this layout ensures that input activations are accessed efficiently without introducing memory fragmentation. By aligning tensor storage with compute-friendly memory access patterns, cache utilization is improved, reducing memory stalls.

Kernel fusion in MLPs is primarily applied to General Matrix Multiplication (GEMM) operations. Since dense layers are often followed by activation functions and bias additions, fusing these operations into a single computation step reduces memory traffic. GEMM fusion ensures that activations, weights, and biases are processed within a single optimized kernel, avoiding unnecessary memory writes and reloads.

To further improve memory efficiency, MLPs rely on blocked tiling strategies, where large matrix multiplications are divided into smaller sub-blocks that fit within the accelerator’s shared memory. This method ensures that frequently accessed portions of matrices remain in fast memory throughout computation, reducing external memory accesses. By structuring computations in a way that balances memory utilization with efficient parallel execution, blocked tiling minimizes bandwidth limitations and maximizes throughput.

These optimizations ensure that MLPs achieve high computational efficiency by structuring execution around weight reuse, optimizing memory layouts for dense matrix operations, reducing redundant memory writes through kernel fusion, and employing blocked tiling strategies to maximize on-chip memory utilization.

### Hybrid Mapping Strategies  

While general mapping strategies provide a structured framework for optimizing machine learning models, real-world architectures often involve diverse computational requirements that cannot be effectively addressed with a single, fixed approach. Hybrid mapping strategies allow AI accelerators to dynamically apply different optimizations to specific layers or components within a model, ensuring that each computation is executed with maximum efficiency.  

Machine learning models typically consist of multiple layer types, each exhibiting distinct memory access patterns, data reuse characteristics, and parallelization opportunities. By tailoring mapping strategies to these specific properties, hybrid approaches achieve higher computational efficiency, improved memory bandwidth utilization, and reduced data movement overhead compared to a uniform mapping approach.  

#### Layer-Specific Mapping in Hybrid Strategies  

Hybrid mapping strategies are particularly beneficial in models that combine spatially localized computations, such as convolutions, with fully connected operations, such as dense layers or attention mechanisms. These operations possess distinct characteristics that require different mapping strategies for optimal performance.  

For example, in convolutional neural networks, hybrid strategies typically involve:  

- Weight stationary execution for convolutional layers, keeping filters in local memory while activations stream dynamically.  
- Output stationary execution for fully connected layers, reducing redundant memory writes during matrix multiplications.  
- Kernel fusion, integrating activation functions, batch normalization, and element-wise operations to reduce intermediate memory traffic.  

Similarly, transformers employ:  

- Activation stationary mapping for self-attention layers, optimizing the reuse of stored key-value pairs to minimize memory fetches.  
- Weight stationary mapping for feedforward layers, ensuring efficient reuse of large weight matrices.  
- Fused attention kernels, integrating softmax and weighted summation into a single computation step to enhance execution speed.  

For multilayer perceptrons, hybrid mapping strategies optimize:  

- Weight stationary execution to maximize weight reuse across activations.  
- Blocked tiling strategies for large matrix multiplications, improving cache locality.  
- General matrix multiplication fusion, reducing memory stalls by merging matrix multiplications with subsequent operations.  

By applying the most appropriate mapping strategy to each layer type, hybrid execution models enhance both memory efficiency and computational throughput.  

#### Example: Hybrid Mapping in Vision Transformers  

Hybrid mapping strategies are widely used in vision transformers, which integrate both convolutional and self-attention operations. In these models:  

- The patch embedding layer performs a convolution-like operation, benefiting from weight stationary mapping.  
- The self-attention layers require activation stationary execution, as the key-value cache must be reused across multiple queries.  
- The multilayer perceptron layers leverage general matrix multiplication fusion and blocked tiling, ensuring efficient execution of dense matrix multiplications.  

This layer-specific optimization allows vision transformers to balance memory locality and computational efficiency, making them well-suited for AI accelerators.  

### Hardware Implementations of Hybrid Strategies  

Several modern AI accelerators incorporate hybrid mapping strategies to optimize execution:  

- Google TPUs use weight stationary mapping for convolutions and activation stationary mapping for attention layers in transformers.  
- NVIDIA GPUs implement fused kernels and hybrid memory layouts, allowing different mapping strategies within the same model.  
- Graphcore IPUs dynamically select layer-specific execution strategies, optimizing memory access for different operations.  

These real-world implementations illustrate how hybrid mapping strategies bridge the gap between different types of machine learning computations, ensuring that each layer executes with maximum efficiency. However, hardware support is essential for these techniques to be practical. Accelerators must provide architectural features such as programmable memory hierarchies, efficient interconnects, and specialized execution pipelines to fully exploit hybrid mapping.

Hybrid mapping provides a flexible and efficient approach to deep learning execution, enabling AI accelerators to adapt to the diverse computational requirements of modern architectures. By selecting the optimal mapping technique for each layer, hybrid strategies help reduce memory bandwidth constraints, improve data locality, and maximize parallelism.

While hybrid mapping strategies offer an effective way to optimize computations at a layer-specific level, they remain static design-time optimizations. In real-world AI workloads, execution conditions can change dynamically due to varying input sizes, memory contention, or hardware resource availability. Machine learning compilers and runtime systems extend these mapping techniques by introducing dynamic scheduling, memory optimizations, and automatic tuning mechanisms. These systems ensure that hybrid strategies are not just predefined execution choices, but rather adaptive mechanisms that allow deep learning workloads to operate efficiently across different accelerators and deployment environments. In the next section, we explore how machine learning compilers and runtime stacks enable these adaptive optimizations through just-in-time scheduling, memory-aware execution, and workload balancing strategies.

## Compiler Support

The performance of machine learning acceleration depends not only on hardware capabilities but also on how efficiently models are translated into executable operations. The optimizations discussed earlier in this chapter—kernel fusion, tiling, memory scheduling, and data movement strategies—are essential for maximizing efficiency. However, these optimizations must be systematically applied before execution to ensure they align with hardware constraints and computational requirements.

This process is handled by machine learning compilers, which form the software stack responsible for bridging high-level model representations with low-level hardware execution. The compiler optimizes the model by restructuring computations, selecting efficient execution kernels, and placing operations in a way that maximizes hardware utilization.

While traditional compilers are designed for general-purpose computing, machine learning workloads require specialized approaches due to their reliance on tensor computations, parallel execution, and memory-intensive operations. To understand how these systems differ, we first compare machine learning compilers to their traditional counterparts.

### ML vs. Traditional Compilers

Machine learning workloads introduce unique challenges that traditional compilers were not designed to handle. Unlike conventional software execution, which primarily involves sequential or multi-threaded program flow, machine learning models are expressed as computation graphs that describe large-scale tensor operations. These graphs require specialized optimizations that traditional compilers cannot efficiently apply.  

@tbl-ml-vs-traditional-compilers outlines the fundamental differences between traditional compilers and those designed for machine learning workloads. While traditional compilers optimize linear program execution through techniques like instruction scheduling and register allocation, ML compilers focus on optimizing computation graphs for efficient tensor operations. This distinction is critical, as ML compilers must incorporate domain-specific transformations such as kernel fusion, memory-aware scheduling, and hardware-accelerated execution plans to achieve high performance on specialized accelerators like GPUs and TPUs.


+-------------------------+-------------------------------------------------------------+----------------------------------------------------------------+
| Aspect                  | Traditional Compiler                                        | Machine Learning Compiler                                      |
+:========================+:============================================================+:===============================================================+
| Input Representation    | Linear program code (C, Python)                             | Computational graph (ML models)                                |
+-------------------------+-------------------------------------------------------------+----------------------------------------------------------------+
| Execution Model         | Sequential or multi-threaded execution                      | Massively parallel tensor-based execution                      |
+-------------------------+-------------------------------------------------------------+----------------------------------------------------------------+
| Optimization Priorities | Instruction scheduling, loop unrolling, register allocation | Graph transformations, kernel fusion, memory-aware execution   |
+-------------------------+-------------------------------------------------------------+----------------------------------------------------------------+
| Memory Management       | Stack and heap memory allocation                            | Tensor layout transformations, tiling, memory-aware scheduling |
+-------------------------+-------------------------------------------------------------+----------------------------------------------------------------+
| Target Hardware         | CPUs (general-purpose execution)                            | GPUs, TPUs, and custom accelerators                            |
+-------------------------+-------------------------------------------------------------+----------------------------------------------------------------+
| Compilation Output      | CPU-specific machine code                                   | Hardware-specific execution plan (kernels, memory scheduling)  |
+-------------------------+-------------------------------------------------------------+----------------------------------------------------------------+

: Traditional vs. machine learning compilers and their optimization priorities. {#tbl-ml-vs-traditional-compilers .striped .hover}

This comparison highlights why machine learning models require a different compilation approach. Instead of optimizing instruction-level execution, machine learning compilers must transform entire computation graphs, apply tensor-aware memory optimizations, and schedule operations across thousands of parallel processing elements. These requirements make traditional compiler techniques insufficient for modern deep learning workloads.

### The ML Compilation Pipeline  

Machine learning models, as defined in frameworks such as TensorFlow and PyTorch, are initially represented in a high-level computation graph that describes operations on tensors. However, these representations are not directly executable on hardware accelerators such as GPUs, TPUs, and custom AI chips. To achieve efficient execution, models must go through a compilation process that transforms them into optimized execution plans suited for the target hardware.  

The machine learning compilation workflow consists of several key stages, each responsible for applying specific optimizations that ensure minimal memory overhead, maximum parallel execution, and optimal compute utilization. These stages include:  

1. **Graph Optimization:** The computation graph is restructured to eliminate inefficiencies.  
2. **Kernel Selection:** Each operation is mapped to an optimized hardware-specific implementation.  
3. **Memory Planning:** Tensor layouts and memory access patterns are optimized to reduce bandwidth consumption.  
4. **Computation Scheduling:** Workloads are distributed across parallel processing elements to maximize hardware utilization.  
5. **Code Generation:** The optimized execution plan is translated into machine-specific instructions for execution.  

At each stage, the compiler applies theoretical optimizations discussed earlier—such as kernel fusion, tiling, data movement strategies, and computation placement—ensuring that these optimizations are systematically incorporated into the final execution plan.  

By understanding this workflow, we can see how machine learning acceleration is realized not just through hardware improvements but also through compiler-driven software optimizations.  

### Graph Optimization

AI accelerators provide specialized hardware to speed up computation, but raw model representations are not inherently optimized for execution on these accelerators. Machine learning frameworks define models using high-level computation graphs, where nodes represent operations (such as convolutions, matrix multiplications, and activations), and edges define data dependencies. However, if executed as defined, these graphs often contain redundant operations, inefficient memory access patterns, and suboptimal execution sequences that can prevent the hardware from operating at peak efficiency.  

For example, in a Transformer model, the self-attention mechanism involves repeated accesses to the same key-value pairs across multiple attention heads. If compiled naïvely, the model may reload the same data multiple times, leading to excessive memory traffic. Similarly, in a Convolutional Neural Network (CNN), applying batch normalization and activation functions as separate operations after each convolution leads to unnecessary intermediate memory writes, increasing memory bandwidth usage. These inefficiencies are addressed during graph optimization, where the compiler restructures the computation graph to eliminate unnecessary operations and improve memory locality.  

The graph optimization phase of compilation is responsible for transforming this high-level computation graph into an optimized execution plan before it is mapped to hardware. Rather than requiring manual optimization, the compiler systematically applies transformations that improve data movement, reduce redundant computations, and restructure operations for efficient parallel execution.  

At this stage, the compiler is still working at a hardware-agnostic level, focusing on high-level restructuring that improves efficiency before more hardware-specific optimizations are applied later.  

#### How the Compiler Optimizes the Computation Graph  

Graph optimization consists of a series of structured transformations that improve execution efficiency. While the specific techniques vary depending on the compiler and hardware target, they generally include:  

- **Kernel Fusion:** Merging consecutive operations to eliminate unnecessary memory writes. This is especially important in CNNs, where fusing convolution, batch normalization, and activation functions reduces the number of kernel launches and improves execution speed.  
- **Computation Reordering:** Adjusting the execution order of operations to improve data locality and maximize parallel execution. In Transformer models, this can ensure that attention computations reuse cached key-value pairs rather than repeatedly loading them from memory.  
- **Redundant Computation Elimination:** Identifying duplicate or unnecessary operations and removing them. This is useful in models with residual connections, where common subexpressions might be computed multiple times unnecessarily.  
- **Memory-Aware Dataflow Adjustments:** Ensuring that tensor layouts and memory movement patterns align with efficient execution. In hardware like TPUs, this means ensuring that matrix multiplications are tiled appropriately to fit within the systolic array structure.  

These transformations prepare the model for acceleration by ensuring that computations are structured in a way that minimizes overhead and maximizes hardware utilization.  

#### Practical Implementation in AI Compilers  

Modern AI compilers implement graph optimization using automated pattern recognition and structured rewrite rules. These transformations are applied systematically, ensuring that models execute efficiently without requiring manual intervention.  

- XLA (Accelerated Linear Algebra), used in TensorFlow, applies graph-level transformations such as fusion and layout optimizations to streamline execution on TPUs and GPUs. For instance, in an LSTM-based model, XLA can merge multiple element-wise operations into a single kernel, reducing memory access overhead.  
- TVM (Tensor Virtual Machine) optimizes tensor layouts and computational structure while tuning execution strategies for diverse hardware backends. This is particularly useful for deploying models on embedded devices, where memory constraints require more aggressive graph transformations.  
- TensorRT, NVIDIA’s deep learning compiler, specializes in reducing kernel launch overhead, fusing operations, and optimizing execution scheduling for GPUs. In large-scale CNN inference, TensorRT applies layer fusion to improve GPU utilization, reducing execution latency.  
- MLIR (Multi-Level Intermediate Representation) enables flexible graph optimization across different AI accelerators by supporting multi-stage transformations that improve execution order and memory access patterns. This framework is particularly useful for adapting models across different hardware architectures, such as switching from a CPU implementation to an accelerator-optimized one.  

These compilers do not change the mathematical correctness of the model but rewrite the computation graph to ensure that execution is structured for efficiency before proceeding to hardware-specific optimization phases.  

#### Why Graph Optimization is Critical for AI Acceleration  

Graph optimization plays a crucial role in ensuring that AI accelerators operate at peak efficiency. Without this phase, even the most optimized hardware would be underutilized, as models would be executed in a way that introduces unnecessary memory stalls, redundant computations, and inefficient data movement.  

By systematically restructuring computation graphs, the compiler ensures that:  

- Operations are arranged for efficient execution, eliminating bottlenecks before hardware mapping.  
- Memory movement is minimized, ensuring that tensors remain in high-speed memory whenever possible.  
- Parallel execution is optimized, reducing unnecessary serialization and increasing hardware utilization.  

For instance, without graph optimization, a large Transformer model running on an edge device might suffer from excessive memory stalls due to suboptimal data access patterns. With proper graph restructuring, the same model can execute with significantly lower memory bandwidth consumption and reduced latency, making real-time inference feasible on resource-constrained hardware.  

With the computation graph now fully optimized, the next step in compilation is kernel selection, where the compiler determines which hardware-specific implementation should be used for each operation. This ensures that the structured execution plan is translated into optimized low-level instructions for the target accelerator.

### Kernel Selection

Once the computation graph has been optimized for efficiency, the next phase of compilation is kernel selection, where the compiler assigns each operation to a hardware-specific implementation. At this stage, the compiler translates the abstract operations in the computation graph into optimized low-level functions, ensuring that execution is performed as efficiently as possible given the constraints of the target accelerator.  

A kernel is a specialized implementation of a computational operation designed to run efficiently on a particular hardware architecture. Most accelerators, including GPUs, TPUs, and custom AI chips, provide multiple kernel implementations for the same operation, each optimized for different execution scenarios. Choosing the right kernel for each operation is essential for maximizing computational throughput, minimizing memory stalls, and ensuring that the accelerator’s specialized processing elements are fully utilized.  

Kernel selection builds upon the graph optimization phase, ensuring that the structured execution plan is mapped to the most efficient implementation available. While graph optimization eliminates inefficiencies at the model level, kernel selection ensures that each individual operation is executed using the most efficient hardware-specific routine. The effectiveness of this process directly impacts the model’s overall performance, as poor kernel choices can nullify the benefits of prior optimizations by introducing unnecessary computation overhead or memory bottlenecks.  

For example, in a Transformer model, the matrix multiplications that dominate self-attention computations can be executed in different ways depending on the hardware:  

- On a CPU, a general-purpose matrix multiplication routine may be used, leveraging vectorized execution for efficiency.  
- On a GPU, the compiler may instead select an implementation that exploits tensor cores, which can accelerate matrix multiplications using mixed-precision arithmetic.  
- If the model is running on a TPU, the compiler may map the same operation onto a systolic array, ensuring that data flows through the accelerator in a way that maximizes reuse and minimizes off-chip memory accesses.  
- For inference workloads, an integer arithmetic kernel may be preferable, allowing computations to be performed in INT8 instead of floating-point precision, reducing power consumption without significantly sacrificing accuracy.  

In many cases, compilers do not generate custom kernels from scratch but instead select from vendor-optimized kernel libraries that provide highly tuned implementations for different architectures. For instance, cuDNN and cuBLAS offer optimized kernels for deep learning on NVIDIA GPUs, while oneDNN provides optimized execution for Intel architectures. Similarly, ACL (Arm Compute Library) is optimized for Arm-based devices, and Eigen and BLIS provide efficient CPU-based implementations of deep learning operations. These libraries allow the compiler to choose pre-optimized, high-performance kernels rather than having to reinvent execution strategies for each hardware platform.  

#### How AI Compilers Perform Kernel Selection  

AI compilers use heuristics, profiling, and cost models to determine the best kernel for each operation. These strategies ensure that each computation is executed in a way that maximizes throughput and minimizes memory bottlenecks.  

In rule-based selection, the compiler applies predefined heuristics based on the known capabilities of the hardware. For instance, XLA, the compiler used in TensorFlow, automatically selects tensor core-optimized kernels for NVIDIA GPUs when mixed-precision execution is enabled. These predefined rules allow the compiler to make fast, reliable decisions about which kernel to use without requiring extensive analysis.  

Profile-guided selection takes a more dynamic approach, benchmarking different kernel options and choosing the one that performs best for a given workload. TVM, an open-source AI compiler, uses AutoTVM to empirically evaluate kernel performance, tuning execution strategies based on real-world execution times. By testing different kernels before deployment, profile-guided selection helps ensure that operations are assigned to the most efficient implementation under actual execution conditions.  

Another approach, cost model-based selection, relies on performance predictions to estimate execution time and memory consumption for various kernels before choosing the most efficient one. MLIR, a compiler infrastructure designed for machine learning workloads, applies this technique to determine the most effective tiling and memory access strategies. By modeling how different kernels interact with the accelerator’s compute units and memory hierarchy, the compiler can select the kernel that minimizes execution cost while maximizing performance.  

Many AI compilers also incorporate precision-aware kernel selection, where the selected kernel is optimized for specific numerical formats such as FP32, FP16, BF16, or INT8. Training workloads often prioritize higher precision (FP32, BF16) to maintain model accuracy, whereas inference workloads favor lower precision (FP16, INT8) to increase speed and reduce power consumption. For example, an NVIDIA GPU running inference with TensorRT can dynamically select FP16 or INT8 kernels based on a model’s accuracy constraints. This trade-off between precision and performance is a key aspect of kernel selection, especially when deploying models in resource-constrained environments.  

Some compilers go beyond static kernel selection and implement adaptive kernel tuning, where execution strategies are adjusted at runtime based on the system’s workload and available resources. AutoTVM in TVM measures kernel performance across different workloads and dynamically refines execution strategies. TensorRT applies real-time optimizations based on batch size, memory constraints, and GPU load, adjusting kernel selection dynamically. Google’s TPU compiler takes a similar approach, optimizing kernel selection based on cloud resource availability and execution environment constraints.  

#### Why Kernel Selection is Essential

The efficiency of AI acceleration depends not only on how computations are structured but also on how they are executed. Even the best-designed computation graph will fail to achieve peak performance if the selected kernels do not fully utilize the hardware’s capabilities.  

Proper kernel selection allows models to execute using the most efficient algorithms available for the given hardware, ensuring that memory is accessed in a way that avoids unnecessary stalls and that specialized acceleration features, such as tensor cores or systolic arrays, are leveraged wherever possible. Selecting an inappropriate kernel can lead to underutilized compute resources, excessive memory transfers, and increased power consumption, all of which limit the performance of AI accelerators.  

For instance, if a Transformer model running on a GPU is assigned a non-tensor-core kernel for its matrix multiplications, it may execute at only a fraction of the possible performance. Conversely, if a model designed for FP32 execution is forced to run on an INT8-optimized kernel, it may experience significant numerical instability, degrading accuracy. These choices illustrate why kernel selection is as much about maintaining numerical correctness as it is about optimizing performance.  

With kernel selection complete, the next stage in compilation involves execution scheduling and memory management, where the compiler determines how kernels are launched and how data is transferred between different levels of the memory hierarchy. These final steps in the compilation pipeline ensure that computations run with maximum parallelism while minimizing the overhead of data movement. As kernel selection determines what to execute, execution scheduling and memory management dictate when and how those kernels are executed, ensuring that AI accelerators operate at peak efficiency.  

### Memory Planning

Now that graph optimization has structured computations and kernel selection has assigned efficient hardware implementations, the next step in the compilation pipeline is memory planning. This phase ensures that data is allocated and accessed in a way that minimizes memory bandwidth consumption, reduces latency, and maximizes cache efficiency. Even with the most optimized execution plan, a model can still suffer from severe performance degradation if memory is not managed efficiently.  

Machine learning workloads are often memory-intensive, requiring frequent movement of large tensors between different levels of the memory hierarchy. The compiler must determine how tensors are stored, how they are accessed, and how intermediate results are handled to ensure that memory does not become a bottleneck.  

#### What Happens in Memory Planning?  

The memory planning phase is responsible for optimizing tensor layouts, memory access patterns, and buffer reuse to ensure that machine learning workloads can execute without unnecessary stalls or memory contention. The key objectives of this phase include:  

- **Tensor Layout Optimization:** Ensuring that tensors are stored in a memory-efficient format that aligns with hardware access patterns, minimizing unnecessary format conversions.  
- **Memory Access Optimization:** Structuring memory reads and writes to minimize cache misses and memory stalls, reducing overall bandwidth consumption.  
- **Memory Reuse and Buffering:** Reducing redundant memory allocations by reusing intermediate results and dynamically managing memory buffers.  

By optimizing how data is placed and accessed, memory planning directly impacts both computational efficiency and energy consumption, making it a crucial part of AI acceleration.  

#### How AI Compilers Perform Memory Planning  

Memory planning is a complex problem because AI models must balance memory availability, reuse, and access efficiency while operating across multiple levels of the memory hierarchy. AI compilers use several key strategies to manage memory effectively and prevent unnecessary data movement.  

The first step in memory planning is tensor layout optimization, where the compiler determines how tensors should be arranged in memory to maximize locality and prevent unnecessary data format conversions. Different hardware accelerators have different preferred storage layouts—for instance, NVIDIA GPUs often use row-major storage (NHWC format), while TPUs favor channel-major layouts (NCHW format) to optimize memory coalescing. The compiler automatically transforms tensor layouts based on the expected access patterns of the target hardware, ensuring that memory accesses are aligned for maximum efficiency.  

Beyond layout optimization, memory planning also includes buffer allocation and reuse, where the compiler minimizes memory footprint by reusing intermediate storage whenever possible. Deep learning workloads generate many temporary tensors, such as activations and gradients, which can quickly overwhelm on-chip memory if not carefully managed. Instead of allocating new memory for each tensor, the compiler analyzes the computation graph to identify opportunities for buffer reuse, ensuring that intermediate values are stored and overwritten efficiently.  

Another critical aspect of memory planning is minimizing data movement between different levels of the memory hierarchy. AI accelerators typically have a mix of high-speed on-chip memory (such as caches or shared SRAM) and larger, but slower, external DRAM. If tensor data is repeatedly moved between these memory levels, the model may become memory-bound, reducing computational efficiency. To prevent this, compilers use tiling strategies that break large computations into smaller, memory-friendly chunks, allowing execution to fit within fast, local memory and reducing the need for costly off-chip memory accesses.  

#### Why Memory Planning is Essential for AI Acceleration  

Without proper memory planning, even the most optimized computation graph and kernel selection will fail to deliver high performance. Excessive memory transfers, inefficient memory layouts, and redundant memory allocations can all lead to bottlenecks that prevent AI accelerators from reaching their peak throughput.  

For instance, a CNN running on a GPU may achieve high computational efficiency in theory, but if its convolutional feature maps are stored in an incompatible format, constant tensor format conversions may introduce significant overhead. Similarly, a Transformer model deployed on an edge device may struggle to meet real-time inference requirements if memory is not carefully planned, leading to frequent off-chip memory accesses that increase latency and power consumption.  

By carefully managing tensor placement, optimizing memory access patterns, and reducing unnecessary data movement, memory planning ensures that AI accelerators operate efficiently and deliver real-world performance improvements.  

With memory planning complete, the next phase in compilation is computation scheduling, where the compiler determines how parallel workloads are assigned across processing elements to maximize hardware utilization. Scheduling ensures that the selected kernels and optimized memory layout are executed in a way that avoids idle compute resources and maximizes throughput.

### Computation Scheduling

With graph optimization completed, kernels selected, and memory planning finalized, the next step in the compilation pipeline is computation scheduling. This phase determines when and where each computation should be executed, ensuring that workloads are efficiently distributed across available processing elements while avoiding unnecessary stalls and resource contention.  

AI accelerators achieve high performance through massive parallelism, but without an effective scheduling strategy, computational units may sit idle, memory bandwidth may be underutilized, and execution efficiency may degrade. Computation scheduling is responsible for ensuring that all processing elements remain active, execution dependencies are managed correctly, and workloads are distributed optimally.  

#### What Happens in Computation Scheduling?  

The scheduling phase plays a crucial role in managing parallel execution, synchronization, and resource allocation across the accelerator. It involves three key challenges:  

- **Task Partitioning:** Breaking down large computations into smaller tasks that can be distributed across multiple compute cores.  
- **Execution Order Optimization:** Determining the best sequence in which operations should be launched to maximize hardware efficiency and minimize stalls.  
- **Resource Allocation and Synchronization:** Ensuring that compute cores, memory bandwidth, and shared caches are allocated efficiently without causing contention.  

By carefully managing these aspects, computation scheduling ensures that hardware resources are fully utilized, memory access is optimized, and execution proceeds without unnecessary delays.  

#### How AI Compilers Perform Computation Scheduling  

Computation scheduling is highly dependent on the underlying hardware architecture, as different AI accelerators have unique execution models that must be considered when determining how workloads are scheduled. AI compilers implement several key strategies to optimize scheduling for efficient execution.  

One of the most fundamental aspects of scheduling is task partitioning, where the compiler divides large computational graphs into smaller, manageable units that can be executed in parallel. On GPUs, this typically means mapping matrix multiplications and convolutions to thousands of CUDA cores, while on TPUs, tasks are partitioned to fit within systolic arrays that operate on structured data flows. In CPUs, partitioning is often focused on breaking computations into vectorized chunks that align with SIMD execution. The goal is to map workloads to available processing units efficiently, ensuring that each core remains active throughout execution.  

In addition to task partitioning, scheduling also involves optimizing execution order to minimize dependencies and maximize throughput. Many AI models include operations that can be computed independently (e.g., different batches in a batch processing pipeline) alongside operations that have strict dependencies (e.g., recurrent layers in an RNN). AI compilers analyze these dependencies and attempt to rearrange execution where possible, reducing idle time and improving parallel efficiency. For example, in Transformer models, scheduling may prioritize preloading attention matrices into memory while earlier layers are still executing, ensuring that data is ready when needed.  

Another crucial aspect of computation scheduling is resource allocation and synchronization, where the compiler determines how compute cores share memory and coordinate execution. Modern AI accelerators often support overlapping computation and data transfers, meaning that while one task executes, the next task can begin fetching its required data. Compilers take advantage of this by scheduling tasks in a way that hides memory latency, ensuring that execution remains compute-bound rather than memory-bound. TensorRT and XLA, for example, employ streaming execution strategies where multiple kernels are launched in parallel, and synchronization is carefully managed to prevent execution stalls.  

#### Why Computation Scheduling is Essential for AI Acceleration  

Without effective scheduling, even the most optimized model can suffer from underutilized compute resources, memory bottlenecks, and execution inefficiencies. Poor scheduling decisions can lead to idle processing elements, forcing expensive compute cores to wait for data or synchronization events before continuing execution.  

For instance, a CNN running on a GPU may have highly optimized kernels and efficient memory layouts, but if its execution is not scheduled correctly, compute units may remain idle between kernel launches, reducing throughput. Similarly, a Transformer model deployed on a TPU may perform matrix multiplications efficiently but could experience performance degradation if attention layers are not scheduled to overlap efficiently with memory transfers.  

By managing parallel workloads effectively, computation scheduling ensures that:  
- Processing elements remain fully utilized, avoiding idle cores and maximizing throughput.  
- Memory latency is hidden where possible, ensuring that computations are not stalled waiting for data.  
- Execution dependencies are resolved in a way that minimizes waiting time and maximizes overlap between compute and data movement.  

With computation scheduling complete, the final stage of compilation is code generation, where the optimized execution plan is translated into machine-specific instructions that can be executed by the hardware. This final step ensures that all scheduling, memory, and compute optimizations are effectively realized in the final executable.

#### Code Generation

Once computation scheduling is finalized, the compiler proceeds to the final step: code generation. At this stage, the optimized execution plan is converted into low-level machine instructions that can be directly executed by the hardware.

Unlike the previous phases, which required AI-specific optimizations, code generation follows many of the same principles as traditional compilers. This process includes instruction selection, register allocation, and final optimization passes, ensuring that execution makes full use of hardware-specific features such as vectorized execution, memory prefetching, and instruction reordering.

For CPUs and GPUs, AI compilers typically generate machine code or optimized assembly instructions, while for TPUs, FPGAs, and other accelerators, the output may be optimized bytecode or execution graphs that are interpreted by the hardware’s runtime system.

At this point, the compilation pipeline is complete: the original high-level model representation has been transformed into an optimized, executable format tailored for efficient execution on the target hardware. The combination of graph transformations, kernel selection, memory-aware execution, and parallel scheduling ensures that AI accelerators run workloads with maximum efficiency, minimal memory overhead, and optimal computational throughput.

### Compilation to Runtime Support

The compiler plays a fundamental role in AI acceleration, transforming high-level machine learning models into optimized execution plans tailored to the constraints of specialized hardware. Throughout this section, we have seen how graph optimization restructures computation, kernel selection maps operations to hardware-efficient implementations, memory planning optimizes data placement, and computation scheduling ensures efficient parallel execution. Each of these phases is crucial in enabling AI models to fully leverage modern accelerators, ensuring high throughput, minimal memory overhead, and efficient execution pipelines.

However, compilation alone is not enough to guarantee efficient execution in real-world AI workloads. While compilers statically optimize computation based on known model structures and hardware capabilities, AI execution environments are often dynamic and unpredictable. Batch sizes fluctuate, hardware resources may be shared across multiple workloads, and accelerators must adapt to real-time performance constraints. In these cases, a static execution plan is insufficient, and runtime management becomes critical in ensuring that models execute optimally under real-world conditions.

This transition from static compilation to adaptive execution is where AI runtimes come into play. Runtimes provide dynamic memory allocation, real-time kernel selection, workload scheduling, and multi-chip coordination, allowing AI models to adapt to varying execution conditions while maintaining efficiency. In the next section, we explore how AI runtimes extend the capabilities of compilers, enabling models to run effectively in diverse and scalable deployment scenarios.

## Runtime Support

While compilers optimize AI models before execution, real-world deployment introduces dynamic and unpredictable conditions that static compilation alone cannot fully address. AI workloads operate in varied execution environments, where factors such as fluctuating batch sizes, shared hardware resources, memory contention, and latency constraints necessitate real-time adaptation. Precompiled execution plans, optimized for a fixed set of assumptions, may become suboptimal when actual runtime conditions change.

To bridge this gap, AI runtimes provide a dynamic layer of execution management, extending the optimizations performed at compile time with real-time decision-making. Unlike traditional compiled programs that execute a fixed sequence of instructions, AI workloads require adaptive control over memory allocation, kernel execution, and resource scheduling. AI runtimes continuously monitor execution conditions and make on-the-fly adjustments to ensure that machine learning models fully utilize available hardware while maintaining efficiency and performance guarantees.

At a high level, AI runtimes manage three critical aspects of execution:  

1. **Kernel Execution Management:** AI runtimes dynamically select and dispatch computation kernels based on the current system state, ensuring that workloads are executed with minimal latency.  
2. **Memory Adaptation and Allocation:** Since AI workloads frequently process large tensors with varying memory footprints, runtimes adjust memory allocation dynamically to prevent bottlenecks and excessive data movement.  
3. **Execution Scaling:** AI runtimes handle workload distribution across multiple accelerators, supporting large-scale execution in multi-chip, multi-node, or cloud environments**.  

By dynamically handling these execution aspects, AI runtimes complement compiler-based optimizations, ensuring that models continue to perform efficiently under varying runtime conditions. The next section explores how AI runtimes differ from traditional software runtimes, highlighting why machine learning workloads require fundamentally different execution strategies compared to conventional CPU-based programs.

###  ML vs. Traditional Runtimes   

Traditional software runtimes are designed for managing general-purpose program execution, primarily handling sequential and multi-threaded workloads on CPUs. These runtimes allocate memory, schedule tasks, and optimize execution at the level of individual function calls and instructions. In contrast, AI runtimes are specialized for machine learning workloads, which require massively parallel computation, large-scale tensor operations, and dynamic memory management.

@tbl-runtime-comparison highlights the fundamental differences between traditional and AI runtimes. One of the key distinctions lies in execution flow. Traditional software runtimes operate on a predictable, structured execution model where function calls and CPU threads follow a predefined control path. AI runtimes, however, execute computational graphs, requiring complex scheduling decisions that account for dependencies between tensor operations, parallel kernel execution, and efficient memory access.

+-------------------------+----------------------------------------+---------------------------------------------------------+
| Aspect                  | Traditional Runtime                    | AI Runtime                                              |
+:========================+:=======================================+:========================================================+
| Execution Model         | Sequential or multi-threaded execution | Massively parallel tensor execution                     |
+-------------------------+----------------------------------------+---------------------------------------------------------+
| Task Scheduling         | CPU thread management                  | Kernel dispatch across accelerators                     |
+-------------------------+----------------------------------------+---------------------------------------------------------+
| Memory Management       | Static allocation (stack/heap)         | Dynamic tensor allocation, buffer reuse                 |
+-------------------------+----------------------------------------+---------------------------------------------------------+
| Optimization Priorities | Low-latency instruction execution      | Minimizing memory stalls, maximizing parallel execution |
+-------------------------+----------------------------------------+---------------------------------------------------------+
| Adaptability            | Mostly static execution plan           | Adapts to batch size and hardware availability          |
+-------------------------+----------------------------------------+---------------------------------------------------------+
| Target Hardware         | CPUs (general-purpose execution)       | GPUs, TPUs, and custom accelerators                     |
+-------------------------+----------------------------------------+---------------------------------------------------------+

: Key differences between traditional and AI runtimes. {#tbl-runtime-comparison .striped .hover}

Memory management is another major differentiator. Traditional software runtimes handle small, frequent memory allocations, optimizing for cache efficiency and low-latency access. AI runtimes, in contrast, must dynamically allocate, reuse, and optimize large tensors, ensuring that memory access patterns align with accelerator-friendly execution. Poor memory management in AI workloads can lead to performance bottlenecks, particularly due to excessive off-chip memory transfers and inefficient cache usage.

Moreover, AI runtimes are inherently designed for adaptability. While traditional runtimes often follow a mostly static execution plan, AI workloads typically operate in highly variable execution environments, such as cloud-based accelerators or multi-tenant hardware. As a result, AI runtimes must continuously adjust batch sizes, reallocate compute resources, and manage real-time scheduling decisions to maintain high throughput and minimize execution delays.

These distinctions demonstrate why  AI runtimes require fundamentally different execution strategies  compared to traditional software runtimes. Rather than simply managing CPU processes, AI runtimes must oversee  large-scale tensor execution, multi-device coordination, and real-time workload adaptation  to ensure that machine learning models can run efficiently under diverse and ever-changing deployment conditions.  

###  Dynamic Kernel Execution   

Dynamic kernel execution represents a critical mechanism in the process of mapping machine learning models to hardware and optimizing runtime execution. While static compilation provides a solid foundation, efficient execution of machine learning workloads requires  real-time adaptation  to fluctuating conditions such as available memory, data sizes, and computational loads. The runtime functions as an intermediary that continuously adjusts execution strategies to match both the  constraints of the underlying hardware  and the  characteristics of the workload .

When mapping a machine learning model to hardware, individual  computational operations —such as matrix multiplications, convolutions, and activation functions—must be assigned to the most appropriate processing units. This mapping is not fixed; it must be modified during runtime in response to changes in input data, memory availability, and overall system load. Dynamic kernel execution allows the runtime to make real-time decisions regarding  kernel selection ,  execution order , and  memory management , ensuring that workloads remain efficient despite these changing conditions.  

For example, in an accelerator with a hierarchical memory system, a static execution plan may lead to memory stalls if a kernel requires more data than can fit within the high-speed cache. The runtime can resolve this by using  dynamic tiling  techniques that adjust the computation, ensuring that the kernel makes optimal use of available cache and avoids excessive off-chip memory transfers. Similarly, by scheduling kernels dynamically, AI runtimes can ensure that hardware is continuously utilized, preventing idle compute units and optimizing throughput.  

Moreover,  overlapping computation with memory movement  is a vital strategy to mitigate performance bottlenecks. AI workloads often encounter delays due to memory-bound issues, where data movement between memory hierarchies limits computation speed. To combat this, AI runtimes implement techniques like  asynchronous execution  and  double buffering , ensuring that computations proceed without waiting for memory transfers to complete. In a large-scale model, for instance,  image data  can be prefetched while computations are performed on the previous batch, thus maintaining a steady flow of data and avoiding pipeline stalls.  

Dynamic kernel execution plays an essential role in ensuring that machine learning models are executed efficiently. By dynamically adjusting execution strategies in response to real-time system conditions, AI runtimes optimize both  training  and  inference  performance across various hardware platforms.  

###  Kernel Selection at Runtime   

While compilers may perform an initial selection of kernels based on static analysis of the machine learning model and hardware target, AI runtimes often need to  override these decisions  during execution. Real-time factors, such as  available memory ,  hardware utilization , and  workload priorities , may differ significantly from the assumptions made during compilation. By dynamically selecting and switching kernels at runtime, AI runtimes can adapt to these changing conditions, ensuring that models continue to perform efficiently.

For instance, consider  transformer-based language models , where a significant portion of execution time is spent on matrix multiplications. The AI runtime must determine the most efficient way to execute these operations based on the current system state. If the model is running on a GPU with specialized  Tensor Cores , the runtime may switch from a standard FP32 kernel to an FP16 kernel to take advantage of hardware acceleration. Conversely, if the lower precision of FP16 causes unacceptable numerical instability, the runtime can opt for mixed-precision execution, selectively using FP32 where higher precision is necessary.  

Memory constraints also influence kernel selection. When  memory bandwidth is limited , the runtime may adjust its execution strategy, reordering operations or changing the tiling strategy to fit computations into the available cache rather than relying on slower main memory. For example, a large matrix multiplication may be broken into smaller chunks, ensuring that the computation fits into the  on-chip memory  of the GPU, reducing overall latency.  

Additionally,  batch size  can influence kernel selection. For workloads that handle a mix of small and large batches, the AI runtime may choose a  latency-optimized kernel  for small batches and a  throughput-optimized kernel  for large-scale batch processing. This adjustment ensures that the model continues to operate efficiently across different execution scenarios, without the need for manual tuning.  

###  Kernel Scheduling and Resource Utilization   

Once the AI runtime selects an appropriate kernel, the next step is  scheduling  it in a way that maximizes  parallelism  and  resource utilization . Unlike traditional task schedulers, which are designed to manage CPU threads, AI runtimes must coordinate a much larger number of tasks across  parallel execution units  such as  GPU cores ,  tensor processing units (TPUs) , or custom AI accelerators. Effective scheduling ensures that these computational resources are kept fully engaged, preventing bottlenecks and maximizing throughput.  

For example, in  image recognition models  that use convolutional layers, operations can be distributed across multiple processing units, enabling different filters to run concurrently. This parallelization ensures that the available hardware is fully utilized, speeding up execution. Similarly,  batch normalization  and  activation functions  must be scheduled efficiently to avoid unnecessary delays. If these operations are not interleaved with other computations, they may block the pipeline and reduce overall throughput.  

Efficient kernel scheduling can also be influenced by  real-time memory management . AI runtimes ensure that intermediate data, such as feature maps in  deep neural networks , are  preloaded into cache  before they are needed. This proactive management helps prevent delays caused by waiting for data to be loaded from slower memory tiers, ensuring continuous execution.

These techniques enable AI runtimes to ensure optimal  resource utilization  and efficient  parallel computation , which are essential for the high-performance execution of machine learning models, particularly in environments that require scaling across multiple hardware accelerators.

## Multi-chip AI

Much of the discussion so far has focused on AI acceleration within a single processor or a tightly coupled system. We have examined the fundamental computational primitives—vector, matrix, and special function units—and explored how these operations are mapped to modern AI accelerators. Additionally, we have discussed memory hierarchy, computation placement, mapping strategies, and runtime optimizations that enable efficient execution within a given hardware system. However, real-world AI workloads often exceed the computational and memory limits of a single accelerator. To address these challenges, AI systems are increasingly adopting multi-chip architectures, where multiple accelerators are interconnected to scale performance and handle larger models.

This section explores how AI systems transition from single-chip execution to multi-chip architectures. We begin by examining real-world examples, including multi-GPU systems, TPU pods, and wafer-scale AI chips, to understand the motivation behind scaling. We then analyze how multi-chip architectures introduce new challenges in computation mapping, memory consistency, interconnect bandwidth, and runtime coordination. While the foundational concepts of computation placement and memory management still apply, scaling AI systems requires additional considerations, such as cross-chip communication, workload partitioning, and efficient scheduling across heterogeneous accelerators. 

By the end of this section, we will have established a clear progression from single-chip to multi-chip AI acceleration, highlighting how each layer of the AI hardware stack—compute units, memory systems, mapping strategies, and runtimes—adapts to the challenges of large-scale AI execution.

### Scaling AI Systems

AI hardware scales begin with chiplet-based architectures, moving to multi-GPU systems, expanding further into distributed TPU Pods, and culminating in wafer-scale AI. Each of these approaches introduces new challenges—communication overhead, memory access patterns, workload distribution, and hardware integration. Understanding these scalability strategies provides the foundation for later discussions on mapping, compilation, and runtime execution in large-scale AI systems.  

#### Chiplet-Based Architectures: Scaling Within a Single Package  

The first step in scaling AI accelerators is to move beyond a single monolithic chip while still maintaining a compact, tightly integrated design. Chiplet architectures achieve this by partitioning large designs into smaller, modular dies that are interconnected within a single package.  

Modern AI accelerators, such as AMD’s Instinct MI300, take this approach by integrating multiple compute chiplets alongside memory chiplets, linked by high-speed die-to-die interconnects. This modular design allows manufacturers to bypass the manufacturing limits of monolithic chips while still achieving high-density compute.  

However, even within a single package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence, and thermal management become critical factors as more chiplets are integrated. Unlike traditional multi-chip systems, chiplet-based designs must carefully balance latency-sensitive workloads across multiple dies without introducing excessive bottlenecks.

#### Multi-GPU Systems: Scaling Beyond a Single Accelerator  

Beyond chiplet-based designs, AI workloads often require multiple discrete GPUs working together. In multi-GPU systems, each accelerator has its own dedicated memory and compute resources, but they must efficiently share data and synchronize execution.  

A common example is NVIDIA DGX systems, which integrate multiple GPUs connected via NVLink or PCIe. This architecture enables workloads to be split across GPUs, typically using data parallelism (where each GPU processes a different batch of data) or model parallelism (where different GPUs handle different parts of a neural network).  

However, increasing the number of GPUs in a system introduces new challenges. Cross-GPU communication bandwidth, memory consistency, and workload scheduling become bottlenecks, particularly when large-scale models require frequent data exchanges. Unlike chiplets, which have high-speed die-to-die interconnects, discrete GPUs rely on external interconnects that introduce higher latency and synchronization overhead.

#### TPU Pods: Scaling Across Distributed Systems  

As models and datasets continue to expand, AI training and inference must extend beyond a single server, requiring distributed systems where multiple accelerators communicate across a network. Google’s TPU Pods exemplify this large-scale distributed approach, where hundreds of TPUs are interconnected to act as a unified system.  

Unlike multi-GPU systems, which rely on NVLink or PCIe within a single machine, TPU Pods use high-bandwidth optical links to interconnect accelerators at a data center scale. The 2D torus interconnect topology allows accelerators to efficiently exchange data, minimizing bottlenecks as workloads scale across many nodes.  

However, distributing AI workloads across an entire data center introduces new scaling challenges. Interconnect congestion, synchronization delays, and efficient workload partitioning become fundamental problems. Unlike multi-GPU setups, where accelerators share memory hierarchies, TPU Pods operate in a fully distributed memory system, requiring explicit communication strategies to manage data movement.

#### Wafer-Scale AI: Scaling to a Single Massive Processor  

At the extreme end of AI scaling is wafer-scale integration, which bypasses multi-chip communication entirely by designing an entire wafer as a single AI processor. This approach eliminates the need for discrete chips, instead treating the entire silicon wafer as a unified compute fabric.  

Cerebras’ Wafer-Scale Engine (WSE-2) is the most prominent example of this approach, featuring 850,000 AI cores integrated onto a single wafer. Unlike chiplets, GPUs, or TPU Pods, where data must travel across discrete devices, wafer-scale AI achieves near-instantaneous communication across cores, drastically reducing latency for large-scale neural networks.  

However, this level of integration comes with significant technical hurdles. Thermal dissipation, fault tolerance, and manufacturing yield become major concerns when designing a processor of this scale. Unlike distributed TPU systems that handle failure by re-routing tasks to different nodes, wafer-scale AI requires built-in redundancy mechanisms to tolerate localized defects in the silicon.

#### The Scaling Trajectory of AI Systems  

@tbl-scaling-trajectory illustrates the progressive scaling of AI acceleration, from single-chip processors to increasingly complex architectures such as chiplet-based designs, multi-GPU systems, TPU Pods, and wafer-scale AI. Each step in this evolution introduces new challenges related to data movement, memory access, interconnect efficiency, and workload distribution. While chiplets enable modular scaling within a package, they introduce latency and memory coherence issues. Multi-GPU systems rely on high-speed interconnects like NVLink but face synchronization and communication bottlenecks. TPU Pods push scalability further by distributing workloads across clusters, yet they must contend with interconnect congestion and workload partitioning. At the extreme end, wafer-scale AI integrates an entire wafer into a single computational unit, presenting unique challenges in thermal management and fault tolerance.

+------------------+-------------------------------------+-----------------------------------------------------+
| Scaling Approach | Key Feature                         | Challenges                                          |
+:=================+:====================================+:====================================================+
| Chiplets         | Modular scaling within a package    | Inter-chiplet latency, memory coherence             |
+------------------+-------------------------------------+-----------------------------------------------------+
| Multi-GPU        | External GPU interconnects (NVLink) | Synchronization overhead, communication bottlenecks |
+------------------+-------------------------------------+-----------------------------------------------------+
| TPU Pods         | Distributed accelerator clusters    | Interconnect congestion, workload partitioning      |
+------------------+-------------------------------------+-----------------------------------------------------+
| Wafer-Scale AI   | Entire wafer as a single processor  | Thermal dissipation, fault tolerance                |
+------------------+-------------------------------------+-----------------------------------------------------+

: Scaling trajectory of AI systems and associated challenges. {#tbl-scaling-trajectory .striped .hover}

### Scaling Changes Computation and Memory

As AI systems scale from single-chip accelerators to multi-chip architectures, the fundamental challenges in computation and memory evolve. In a single accelerator, execution is primarily optimized for locality—ensuring that computations are mapped efficiently to available processing elements while minimizing memory access latency. However, as AI systems extend beyond a single chip, the scope of these optimizations expands significantly. Computation must now be distributed across multiple accelerators, and memory access patterns become constrained by interconnect bandwidth and communication overhead.  

This section examines how computation placement, memory hierarchy, and data movement shift as AI acceleration scales beyond a single processor.  

#### Computation Placement Becomes a Multi-Chip Problem  

In single-chip AI accelerators, computation placement is primarily concerned with assigning workloads to processing elements (PEs), ensuring efficient parallel execution across cores and vector or tensor units. Placement strategies focus on minimizing data movement within the chip, optimizing cache reuse, and leveraging parallelism across execution units.  

As AI workloads scale to multi-chip architectures, computation placement must now consider the entire system topology. Instead of optimizing placement for local PEs, workloads must be partitioned across multiple accelerators, GPUs, TPUs, or custom AI processors. The primary challenges introduced by multi-chip scaling include:  

- **Cross-accelerator workload distribution**: Ensuring balanced computation across multiple chips to avoid load imbalance.  
- **Interconnect-aware placement**: Assigning computations in a way that minimizes high-latency off-chip communication.  
- **Synchronization overhead**: Managing dependencies between computations that span different accelerators.  

For example, in multi-GPU systems, computation placement must account for NVLink or PCIe bandwidth constraints, ensuring that operations requiring frequent communication are co-located on GPUs with high-bandwidth links. In TPU Pods, placement is influenced by the 2D torus interconnect topology, requiring structured data exchanges to optimize performance.  

Thus, while single-chip computation placement is primarily a local optimization problem, multi-chip computation placement introduces a global optimization challenge where interconnect topology and data transfer costs must be considered.  

#### Memory Hierarchy Shifts from On-Chip to Distributed Memory  

Memory organization in single-chip AI accelerators is designed to minimize latency and maximize data locality. Hierarchical memory structures, such as L1 and L2 caches, on-chip SRAM, and high-bandwidth memory (HBM), are carefully optimized to reduce reliance on slow off-chip DRAM accesses.  

However, as AI systems scale beyond a single chip, the memory hierarchy extends beyond a single accelerator, introducing new constraints:  

- **Inter-chip memory access latency**: Unlike on-chip caches, memory access between chips incurs significantly higher latency.  
- **Limited interconnect bandwidth**: Moving data between chips is orders of magnitude slower than on-chip data movement.  
- **Distributed memory management**: Memory is no longer shared across all compute units; instead, it is **partitioned** across accelerators, requiring explicit data transfer mechanisms.  

In chiplet-based architectures, for example, accelerators rely on high-speed die-to-die interconnects to exchange data between chiplets. While this enables modular scaling, it also introduces latency penalties compared to monolithic chips. In multi-GPU systems, each GPU has its own local memory (HBM or GDDR), requiring explicit communication via NVLink, PCIe, or RDMA to access data stored on another GPU.  

As a result, memory optimization at scale requires new strategies beyond those used in single-chip accelerators. Data locality, prefetching, and caching policies must now be designed to minimize inter-chip transfers, as off-chip memory accesses become the dominant bottleneck in performance.  

#### Data Movement Is No Longer Just a Local Concern  

In single-chip architectures, data movement optimizations primarily focus on minimizing unnecessary memory accesses, maximizing on-chip reuse, and leveraging efficient data layouts (e.g., tiling, weight stationarity, or kernel fusion). While these techniques remain relevant, their impact diminishes as AI systems scale to multi-chip execution.  

At scale, inter-chip data movement becomes the dominant constraint, introducing several new challenges:  

- **Cross-chip bandwidth limitations**: Data must now traverse interconnects such as **PCIe, NVLink, or proprietary links**, which are significantly slower than intra-chip data transfers.  
- **Synchronization penalties**: When data dependencies span multiple chips, execution may be stalled until required data is transferred.  
- **Explicit memory management**: Unlike a single-chip system where caches and shared memory handle data reuse automatically, distributed systems require **explicit data communication and partitioning**.  

For example, in TPU Pods, data movement is carefully structured using the systolic execution model, where each TPU unit passes data to its neighbor in a predictable manner. This minimizes redundant memory fetches and ensures that interconnect bandwidth is used efficiently. Similarly, in multi-GPU training, techniques such as all-reduce communication are used to synchronize weights across GPUs with minimal overhead.  

Thus, while traditional AI acceleration techniques focus on local memory optimization, large-scale AI systems must now prioritize minimizing inter-chip data movement to maintain efficiency.  

#### Summary: How Compilers and Runtimes Adapt to Scaling  

@tbl-scaling-adaptations highlights how compilers and runtimes adapt to the challenges introduced by scaling AI execution beyond a single-chip accelerator. In a single-chip environment, computation placement focuses on optimizing workload distribution among processing elements (PEs), tensor cores, and vector units. However, in a multi-chip system, compilers must implement interconnect-aware scheduling to minimize costly inter-chip communication while ensuring balanced execution across accelerators.

+-----------------------+---------------------------------------+-----------------------------------------------------+
| Aspect                | Single-Chip AI Accelerator            | Multi-Chip AI System & How Compilers/Runtimes Adapt |
+:======================+:======================================+:====================================================+
| Computation Placement | Local PEs, tensor cores, vector units | Hierarchical mapping, interconnect-aware scheduling |
+-----------------------+---------------------------------------+-----------------------------------------------------+
| Memory Management     | Caching, HBM reuse, local tiling      | Distributed allocation, prefetching, caching        |
+-----------------------+---------------------------------------+-----------------------------------------------------+
| Data Movement         | On-chip reuse, minimal DRAM access    | Communication-aware execution, overlap transfers    |
+-----------------------+---------------------------------------+-----------------------------------------------------+
| Execution Scheduling  | Parallelism, compute occupancy        | Global scheduling, interconnect-aware balancing     |
+-----------------------+---------------------------------------+-----------------------------------------------------+

: Adaptations in computation placement, memory management, and scheduling for multi-chip AI execution. {#tbl-scaling-adaptations .striped .hover}

Memory management also evolves significantly with scaling. While a single-chip accelerator benefits from caching, HBM reuse, and efficient tiling, multi-chip systems require explicit memory partitioning and coordination. Compilers optimize memory layouts for distributed execution, and runtimes introduce prefetching and caching mechanisms to reduce inter-chip memory access overhead.

Data movement becomes increasingly critical at scale. Single-chip accelerators emphasize on-chip data reuse and minimal DRAM accesses, but multi-chip systems must implement communication-aware execution strategies to overlap computation with data transfers. Runtimes handle inter-chip synchronization to prevent execution stalls due to data dependencies.

Finally, execution scheduling extends from local parallelism and compute occupancy optimization to global coordination across accelerators. Multi-chip systems require dynamic scheduling strategies that balance workload distribution while accounting for interconnect bandwidth and synchronization latency. By adapting to these scaling challenges, compilers and runtimes ensure that AI systems can efficiently leverage distributed architectures for maximum performance.

### Mapping Complexity Increases at Scale

As AI systems scale from single-chip accelerators to multi-chip architectures, the fundamental challenges in computation and memory evolve. In a single accelerator, execution is primarily optimized for locality—ensuring that computations are mapped efficiently to available processing elements while minimizing memory access latency. However, as AI systems extend beyond a single chip, the scope of these optimizations expands significantly. Computation must now be distributed across multiple accelerators, and memory access patterns become constrained by interconnect bandwidth and communication overhead.  

This section examines how computation placement, memory hierarchy, and data movement shift as AI acceleration scales beyond a single processor.  

#### Mapping Complexity Increases at Scale  

As AI accelerators scale beyond a single chip, the challenge of mapping computations to hardware becomes significantly more complex. In single-chip architectures, mapping strategies focus on placing computations efficiently within a fixed set of processing elements (PEs), while memory allocation ensures efficient reuse of on-chip storage to minimize latency and energy consumption.  

However, in multi-chip architectures, mapping strategies must now consider a broader set of constraints. Computation, memory, and data movement must be coordinated across multiple accelerators, each with independent execution units and local memory hierarchies. This shift introduces new challenges in hierarchical computation mapping, distributed memory allocation, and inter-chip data transfer minimization.  

This section explores how mapping strategies evolve as AI systems scale, highlighting key considerations for efficient execution in multi-chip architectures.  

#### Mapping From Local to Distributed Execution  

In single-chip AI accelerators, computation placement is concerned with mapping workloads to PEs, vector units, and tensor cores. Mapping strategies aim to maximize data locality, ensuring that computations access nearby memory to reduce costly data movement.  

As AI systems scale to multi-chip execution, computation placement must now account for:  

- **Inter-chip execution partitioning**: Workloads must be split across accelerators, requiring explicit coordination of execution order and dependencies.  
- **Interconnect-aware computation scheduling**: Unlike single-chip execution, where computations rely on shared on-chip memory, **cross-chip communication incurs significant latency**.  
- **Load balancing across accelerators**: Uneven distribution of workloads can lead to **idle hardware** on some accelerators while others remain heavily utilized.  

For example, in multi-GPU training, computation mapping must ensure that each GPU has a balanced portion of the workload while minimizing expensive cross-GPU communication. Similarly, in TPU Pods, mapping strategies must align with the torus interconnect topology, ensuring that computation is placed to minimize long-distance data transfers.  

Thus, while computation placement in single-chip systems is a local optimization problem, in multi-chip architectures, it becomes a global optimization challenge where execution efficiency depends on minimizing inter-chip communication and balancing workload distribution.  

#### Memory Allocation for Distributed Access  

Memory allocation strategies in single-chip AI accelerators are designed to minimize off-chip memory accesses by leveraging on-chip caches, SRAM, and high-bandwidth memory (HBM). Techniques such as tiling, data reuse, and kernel fusion ensure that computations make efficient use of fast local memory.  

In multi-chip architectures, memory allocation must now address:  

- **Partitioning memory across accelerators**: Each chip has its own local memory, requiring explicit **allocation of model parameters, activations, and intermediate data** across multiple accelerators.  
- **Minimizing redundant data transfers**: Unlike single-chip execution, where data can be **fetched once and reused**, multi-chip execution requires **explicit data communication between accelerators**.  
- **Synchronization of shared data**: When multiple accelerators process overlapping data, **synchronization overhead** can become a significant bottleneck.  

For instance, in multi-GPU deep learning, gradient synchronization across GPUs is a memory-intensive operation that must be optimized to avoid network congestion. In wafer-scale AI, memory allocation must account for fault tolerance and redundancy mechanisms, ensuring that defective regions of the wafer do not disrupt execution.  

Thus, while memory allocation in single-chip accelerators focuses on local cache efficiency, in multi-chip architectures, it must be explicitly coordinated across accelerators to balance memory bandwidth, minimize redundant transfers, and reduce synchronization overhead.  

#### Data Movement Becomes the Dominant Constraint  

In single-chip AI accelerators, data movement optimization is largely focused on minimizing on-chip memory access latency. Techniques such as weight stationarity, input stationarity, and tiling ensure that frequently used data remains close to the execution units, reducing off-chip memory traffic.  

However, in multi-chip architectures, data movement is no longer just an intra-chip concern—it becomes a system-wide bottleneck. The primary challenges introduced by scaling include:  

- **Inter-chip bandwidth constraints**: Communication links between accelerators (e.g., **PCIe, NVLink, TPU interconnects**) are significantly slower than on-chip memory accesses.  
- **Data synchronization overhead**: When accelerators share model parameters or intermediate computations, **latency and contention** can slow execution.  
- **Optimizing collective communication**: Workloads requiring frequent data exchange (e.g., **gradient updates in deep learning training**) must minimize **synchronization penalties**.  

For example, in TPU Pods, systolic execution models ensure that data moves in structured patterns, reducing unnecessary off-chip transfers. In multi-GPU inference, techniques like asynchronous data fetching and overlapping computation with communication help mitigate inter-chip latency.  

Thus, while data movement optimization in single-chip systems focuses on cache locality and tiling, in multi-chip architectures, the primary challenge is reducing inter-chip communication overhead to maximize efficiency.  

#### Summary: How Compilers and Runtimes Adapt to Scaling  

As AI acceleration extends beyond a single chip, compilers and runtimes must adapt to manage computation placement, memory organization, and execution scheduling across multiple accelerators. The fundamental principles of locality, parallelism, and efficient scheduling remain essential, but their implementation requires new strategies for distributed execution.

One of the primary challenges in scaling AI execution is computation placement. In a single-chip accelerator, workloads are mapped to processing elements (PEs), vector units, and tensor cores with an emphasis on minimizing on-chip data movement and maximizing parallel execution. However, in a multi-chip system, computation must be partitioned hierarchically, where workloads are distributed not just across cores within a chip, but also across multiple accelerators. Compilers handle this by implementing interconnect-aware scheduling, optimizing workload placement to minimize costly inter-chip communication.  

Similarly, memory management evolves as scaling extends beyond a single accelerator. In a single-chip system, local caching, HBM reuse, and efficient tiling strategies ensure that frequently accessed data remains close to computation units. However, in a multi-chip system, each accelerator has its own independent memory, requiring explicit memory partitioning and coordination. Compilers optimize memory layouts for distributed execution, while runtimes introduce data prefetching and caching mechanisms to reduce inter-chip memory access overhead.  

Beyond computation and memory, data movement becomes a major bottleneck at scale. In a single-chip accelerator, efficient on-chip caching and minimized DRAM accesses ensure that data is reused efficiently. However, in a multi-chip system, communication-aware execution becomes critical, requiring compilers to generate execution plans that overlap computation with data transfers. Runtimes handle inter-chip synchronization, ensuring that workloads are not stalled by waiting for data to arrive from remote accelerators.  

Finally, execution scheduling must be extended for global coordination. In single-chip AI execution, scheduling is primarily concerned with parallelism and maximizing compute occupancy within the accelerator. However, in a multi-chip system, scheduling must balance workload distribution across accelerators while taking interconnect bandwidth and synchronization latency into account. Runtimes manage this complexity by implementing adaptive scheduling strategies that dynamically adjust execution plans based on system state and network congestion.  

@tbl-scaling-adaptations summarizes these key adaptations, highlighting how compilers and runtimes extend their capabilities to efficiently support multi-chip AI execution.

+-----------------------+---------------------------------------+-----------------------------------------------------+
| Aspect                | Single-Chip AI Accelerator            | Multi-Chip AI System & How Compilers/Runtimes Adapt |
+:======================+:======================================+:====================================================+
| Computation Placement | Local PEs, tensor cores, vector units | Hierarchical mapping, interconnect-aware scheduling |
+-----------------------+---------------------------------------+-----------------------------------------------------+
| Memory Management     | Caching, HBM reuse, local tiling      | Distributed allocation, prefetching, caching        |
+-----------------------+---------------------------------------+-----------------------------------------------------+
| Data Movement         | On-chip reuse, minimal DRAM access    | Communication-aware execution, overlap transfers    |
+-----------------------+---------------------------------------+-----------------------------------------------------+
| Execution Scheduling  | Parallelism, compute occupancy        | Global scheduling, interconnect-aware balancing     |
+-----------------------+---------------------------------------+-----------------------------------------------------+

: Adaptations in computation placement, memory management, and scheduling for multi-chip AI execution. {#tbl-scaling-adaptations .striped .hover}

Thus, while the fundamentals of AI acceleration remain intact, compilers and runtimes must extend their functionality to operate efficiently across distributed systems. The next section will explore how mapping strategies evolve to further optimize multi-chip AI execution.

### Execution Models Must Adapt

As AI accelerators scale beyond a single chip, execution models must evolve to account for the complexities introduced by distributed computation, memory partitioning, and inter-chip communication. In single-chip accelerators, execution is optimized for local processing elements (PEs), with scheduling strategies that balance parallelism, locality, and data reuse. However, in multi-chip AI systems, execution must now be coordinated across multiple accelerators, introducing new challenges in workload scheduling, memory coherence, and interconnect-aware execution.  

This section explores how execution models change as AI acceleration scales, focusing on scheduling, memory coordination, and runtime management in multi-chip systems.  

#### Local Scheduling to Cross-Accelerator Scheduling  

In single-chip AI accelerators, execution scheduling focuses on optimizing parallelism within the processor, ensuring that workloads are efficiently mapped to tensor cores, vector units, and special function units (SFUs). Common techniques include:  

- **Static scheduling**: Pre-determined execution order, optimized for locality and reuse.  
- **Dynamic scheduling**: Adaptive scheduling that adjusts to workload variations.  
- **Pipeline execution**: Breaking computations into stages to maximize hardware utilization.  

However, in multi-chip architectures, scheduling must now account for inter-chip dependencies, requiring:  

- Workload partitioning across accelerators: Ensuring that each accelerator receives an optimal workload share without excessive communication.  
- Interconnect-aware scheduling: Synchronizing execution with inter-chip bandwidth constraints to prevent stalls.  
- Latency hiding techniques: Overlapping computation with communication to reduce waiting times.  

For example, in multi-GPU inference, execution scheduling must ensure that data is prefetched while computations are performed, reducing memory stalls. In TPU Pods, execution scheduling is tightly coupled with the systolic array model, ensuring that data arrives at each TPU core just in time for execution.  

Thus, while single-chip execution scheduling focuses on maximizing parallelism within a processor, in multi-chip systems, scheduling must explicitly manage communication overhead and workload synchronization.  

#### Memory and Computation Coordination Across Accelerators  

In single-chip AI accelerators, memory coordination is handled through local caching strategies, ensuring that frequently used data remains close to the execution units. Techniques such as tiling, kernel fusion, and data reuse are used to reduce reliance on slower memory hierarchies.  

In multi-chip architectures, however, memory coordination becomes a distributed challenge, requiring:  

- **Explicit memory partitioning**: Since each accelerator has independent memory, data placement must be carefully managed to minimize cross-chip accesses.  
- **Consistency and synchronization**: Shared data must be synchronized across accelerators to ensure correctness.  
- **Efficient communication mechanisms**: Data transfers must be scheduled efficiently to avoid excessive synchronization overhead.  

For example, in distributed deep learning training, model parameters must be synchronized across GPUs using techniques such as all-reduce, where gradients are aggregated across accelerators while minimizing communication latency. In wafer-scale AI, memory coordination must handle fault-tolerant execution, ensuring that defective regions of the wafer do not disrupt overall system performance.  

Thus, while memory coordination in single-chip systems focuses on cache optimization, in multi-chip architectures, it must explicitly manage distributed memory access, synchronization, and communication overhead.  

#### Runtimes Must Manage Cross-Accelerator Execution  

Execution in single-chip AI accelerators is managed by AI runtimes that handle workload scheduling, memory allocation, and hardware execution. These runtimes optimize execution at the kernel level, ensuring that computations are executed efficiently within the available resources.  

In multi-chip AI systems, runtimes must now manage:  

- **Distributed execution orchestration**: Ensuring that computation and memory access are coordinated across accelerators.  
- **Cross-chip workload synchronization**: Preventing execution stalls due to inter-chip communication delays.  
- **Adaptive execution models**: Dynamically adjusting execution plans based on hardware availability and communication constraints.  

For example, in Google’s TPU Pods, the TPU runtime is responsible for scheduling computations across multiple TPU cores, ensuring that workloads are executed in a way that minimizes communication bottlenecks. In multi-GPU frameworks like PyTorch and TensorFlow, runtime execution must synchronize operations across GPUs, ensuring that data is transferred efficiently while maintaining execution order.  

Thus, while single-chip runtimes focus on optimizing execution within a single processor, multi-chip runtimes must handle system-wide execution, balancing computation, memory, and interconnect performance.  

#### Summary: How Compilers and Runtimes Adapt Computation Placement  

As AI systems expand beyond single-chip execution, computation placement must adapt to account for inter-chip workload distribution and interconnect efficiency. In single-chip accelerators, compilers optimize placement by mapping workloads to tensor cores, vector units, and PEs, ensuring maximum parallelism while minimizing on-chip data movement. However, in multi-chip systems, placement strategies must address interconnect bandwidth constraints, synchronization latency, and hierarchical workload partitioning across multiple accelerators.

@tbl-computation-placement highlights these adaptations. To reduce expensive cross-chip communication, compilers now implement interconnect-aware workload partitioning, strategically assigning computations to accelerators based on communication cost. For instance, in multi-GPU training, compilers optimize placement to minimize NVLink or PCIe traffic, whereas TPU Pods leverage the torus interconnect topology to enhance data exchanges.

+-----------------------+---------------------------------------+-----------------------------------------------------------------------+
| Aspect                | Single-Chip AI Accelerator            | Multi-Chip AI System & How Compilers/Runtimes Adapt                   |
+:======================+:======================================+:======================================================================+
| Computation Placement | Local PEs, tensor cores, vector units | Hierarchical mapping, interconnect-aware scheduling                   |
+-----------------------+---------------------------------------+-----------------------------------------------------------------------+
| Workload Distribution | Optimized within a single chip        | Partitioning across accelerators, minimizing inter-chip communication |
+-----------------------+---------------------------------------+-----------------------------------------------------------------------+
| Synchronization       | Managed within local execution units  | Runtimes dynamically balance workloads, adjust execution plans        |
+-----------------------+---------------------------------------+-----------------------------------------------------------------------+

: Adaptations in computation placement strategies for multi-chip AI execution. {#tbl-computation-placement .striped .hover}

Runtimes complement this by dynamically managing execution workloads, adjusting placement in real-time to balance loads across accelerators. Unlike static compilation, which assumes a fixed hardware topology, AI runtimes continuously monitor system conditions and migrate tasks as needed to prevent bottlenecks. This ensures efficient execution even in environments with fluctuating workload demands or varying hardware availability.

By extending local execution strategies to multi-chip environments, computation placement now requires a careful balance between parallel execution, memory locality, and interconnect-aware scheduling. The next section explores how memory hierarchy must evolve to support efficient execution across distributed AI architectures.

Thus, computation placement at scale builds upon local execution optimizations while introducing new challenges in inter-chip coordination, communication-aware execution, and dynamic load balancing. In the next section, we explore how memory hierarchy must adapt to support efficient execution across multi-chip architectures.

### Navigating the Complexities of Multi-Chip AI

The evolution of AI hardware, from single-chip accelerators to multi-chip systems and wafer-scale integration, highlights the increasing complexity of efficiently executing large-scale machine learning workloads. As we've explored in this chapter, scaling AI systems introduces new challenges in computation placement, memory management, and data movement. While the fundamental principles of AI acceleration remain consistent, their implementation must adapt to the constraints of distributed execution, interconnect bandwidth limitations, and synchronization overhead.

Multi-chip AI architectures represent a significant step forward in addressing the computational demands of modern machine learning models. By distributing workloads across multiple accelerators, these systems offer increased performance, memory capacity, and scalability. However, realizing these benefits requires careful consideration of how computations are mapped to hardware, how memory is partitioned and accessed, and how execution is scheduled across a distributed system.

While we an overview of the key concepts and challenges in multi-chip AI acceleration as they extend beyond a single system, there is still much more to explore. As AI models continue to grow in size and complexity, new architectural innovations, mapping strategies, and runtime optimizations will be needed to sustain efficient execution. The ongoing development of AI hardware and software reflects a broader trend in computing, where specialization and domain-specific architectures are becoming increasingly important for addressing the unique demands of emerging workloads.

By understanding the principles and trade-offs involved in multi-chip AI acceleration, machine learning engineers and system designers can make informed decisions about how to best deploy and optimize their models. Whether training large language models on TPU pods or deploying computer vision applications on multi-GPU systems, the ability to efficiently map computations to hardware will continue to be a critical factor in realizing the full potential of AI.

## Conclusion

The rapid advancement of machine learning has fundamentally reshaped computer architecture and system design, driving the need for specialized hardware and optimized software to support the increasing computational demands of AI workloads. This chapter has explored the foundational principles of AI acceleration, analyzing how domain-specific architectures, memory hierarchies, and data movement strategies work in concert to maximize performance and mitigate bottlenecks.

We began by examining the historical progression of AI hardware, tracing the shift from general-purpose processors to specialized accelerators tailored for machine learning workloads. This evolution has been driven by the computational intensity of AI models, necessitating vectorized execution, matrix processing, and specialized function units to accelerate key operations.

Memory systems play a pivotal role in AI acceleration, as modern workloads require efficient management of large-scale tensor data across hierarchical memory structures. This chapter detailed the challenges posed by memory bandwidth limitations, irregular access patterns, and off-chip communication, highlighting techniques such as tiling, kernel fusion, and memory-aware data placement that optimize data movement and reuse.

Mapping neural networks to hardware requires balancing computation placement, memory allocation, and execution scheduling. We analyzed key mapping strategies, including weight-stationary, output-stationary, and hybrid approaches, and explored how compilers and runtimes transform high-level models into optimized execution plans that maximize hardware utilization.

As AI workloads scale beyond single-chip accelerators, new challenges emerge in distributed execution, memory coherence, and inter-chip communication. This chapter examined how multi-GPU architectures, TPU pods, and wafer-scale AI systems address these challenges by leveraging hierarchical workload partitioning, distributed memory management, and interconnect-aware scheduling. We also explored how compilers and runtimes must adapt to orchestrate execution across multiple accelerators, ensuring efficient workload distribution and minimizing communication overhead.

The increasing complexity of AI models and the growing scale of machine learning workloads underscore a broader shift in computing—one where specialization and hardware-software co-design are essential for achieving efficiency and scalability. Understanding the fundamental trade-offs in AI acceleration enables system designers, researchers, and engineers to make informed decisions about deploying and optimizing AI models across diverse hardware platforms.

This chapter has provided a comprehensive foundation in AI acceleration, equipping readers with the knowledge to navigate the evolving intersection of machine learning systems, hardware design, and system optimization. As AI continues to advance, the ability to efficiently map computations to hardware will remain a key determinant of performance, scalability, and future innovation in artificial intelligence.
