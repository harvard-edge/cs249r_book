---
bibliography: hw_acceleration.bib
---

# AI Acceleration {#sec-ai_acceleration}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-ai-acceleration-resource), [Videos](#sec-ai-acceleration-resource), [Exercises](#sec-ai-acceleration-resource)
:::

![_DALL·E 3 Prompt: Create an intricate and colorful representation of a System on Chip (SoC) design in a rectangular format. Showcase a variety of specialized machine learning accelerators and chiplets, all integrated into the processor. Provide a detailed view inside the chip, highlighting the rapid movement of electrons. Each accelerator and chiplet should be designed to interact with neural network neurons, layers, and activations, emphasizing their processing speed. Depict the neural networks as a network of interconnected nodes, with vibrant data streams flowing between the accelerator pieces, showcasing the enhanced computation speed._](images/png/cover_ai_hardware.png)

## Purpose {.unnumbered}

_How do hardware acceleration strategies impact machine learning system performance, and what principles should ML engineers understand to effectively design and deploy their systems?_

Machine learning systems has driven a fundamental shift in computer architecture. Traditional processors, designed for general-purpose computing, prove inefficient for the repeated mathematical operations and data movement patterns in neural networks. Modern accelerators address this challenge by matching hardware structures to ML computation patterns, demonstrating essential trade-offs between performance, power efficiency, and design complexity. Understanding these trade-offs helps ML engineers optimize their models for specific accelerators and design systems that effectively balance computational capabilities against deployment constraints, whether training in data centers or running inference on edge devices.

::: {.callout-tip title="Learning Objectives"}

* Coming soon.

:::

## Overview  

The increasing demand for machine learning workloads imposes intensive computational challenges that exceed the capabilities of traditional computer architectures. These workloads are not only computationally demanding but also require careful optimization for both performance efficiency and energy efficiency. General-purpose processors, designed for sequential computation, cannot efficiently handle the parallel matrix operations and data movement patterns of modern AI models. This fundamental mismatch has led to the development of Machine Learning Accelerators (ML Accelerators)—specialized computing hardware designed to efficiently execute machine learning workloads.  

::: {.callout-note title="Definition of ML Accelerator"}  

Machine Learning Accelerator (ML Accelerator) refers to a *specialized computing hardware* designed to *efficiently execute machine learning workloads*. These accelerators optimize *matrix multiplications, tensor operations, and data movement*, enabling *high-throughput and energy-efficient* computation. ML accelerators operate at various *power and performance scales*, ranging from *edge devices with milliwatt-level consumption* to *data center-scale accelerators requiring kilowatts of power*. They are specifically designed to address *the computational and memory demands* of deep learning models, often incorporating *optimized memory hierarchies, parallel processing units, and custom instruction sets* to maximize performance. ML accelerators are widely used in *training, inference, and real-time AI applications* across cloud, edge, and embedded systems.  

:::  

Hardware acceleration for AI encompasses multiple architectural approaches, each founded on a core set of design principles. Graphics Processing Units (GPUs) employ thousands of parallel compute cores that achieve up to 100× speedup for data-parallel operations compared to CPUs. Tensor Processing Units (TPUs) implement systolic arrays that are specifically designed for efficient matrix multiplication. Field Programmable Gate Arrays (FPGAs) provide reconfigurable logic circuits for workload-specific optimizations. Understanding these fundamental design principles, along with their inherent trade-offs in performance, power efficiency, and programmability, enables system designers to develop effective acceleration strategies for AI workloads.  

The evolution of AI accelerators reflects fundamental shifts in computer architecture. Traditional von Neumann designs separate processing and memory, creating data movement bottlenecks for AI computation. For example, moving data between DRAM and processing elements can consume 100–1000× more energy than the actual computation. Modern accelerators reshape this paradigm through processing-in-memory, near-memory computing, and novel dataflow architectures. These approaches minimize data movement costs while maximizing computational throughput for machine learning operations. The co-design of hardware and software enables further optimization through specialized instruction sets and compiler techniques.  

This chapter examines hardware acceleration strategies for AI workloads, focusing on architectural approaches that enhance performance across training and inference. While the primary emphasis is on hardware design principles, we also explore the critical interplay between algorithmic requirements, compiler optimization, and hardware capabilities that determines real-world acceleration effectiveness.  

## Hardware Evolution

The advancement of computing systems reveals a fundamental pattern in computer architecture: the progression from general-purpose processing to specialized hardware acceleration. This evolutionary path demonstrates how computational requirements drive architectural innovation through cycles of specialization and integration. When general-purpose processors encounter performance limitations for specific computational patterns, hardware specialization emerges as a systematic solution.

Specialized hardware acceleration represents the targeted optimization of frequently executed computational patterns through dedicated circuit implementations. This architectural approach introduces fundamental trade-offs between computational efficiency, silicon area utilization, and programming complexity. The development cycle typically progresses through distinct phases: identification of computational bottlenecks, implementation of specialized circuits, optimization of hardware-software interfaces, and eventual integration of proven specialized functions into mainstream processors.

The historical progression of hardware specialization provides essential insights into the underlying principles that guide modern accelerator architectures. These principles establish a framework for analyzing current trends in domain-specific computing and evaluating emerging approaches to hardware acceleration across diverse application domains.

### Specialized Computing

The trend toward specialized computing architectures emerges from fundamental limitations in general-purpose processing. Early computer systems relied on central processing units (CPUs) to handle all computational tasks through sequential instruction execution. However, certain computational patterns, particularly floating-point arithmetic, emerged as significant performance bottlenecks that demanded architectural innovation.

The introduction of the Intel 8087 mathematics coprocessor in 1980 marked a pivotal moment in hardware specialization. This dedicated floating-point unit (FPU) demonstrated how specialized circuits could dramatically accelerate specific computational patterns. The 8087 achieved performance improvements of up to 100x for floating-point operations compared to software implementations on the main processor. This performance gain established a critical principle: hardware specialization provides substantial benefits for well-defined, computationally intensive operations.

The success of FPU coprocessors led to their eventual integration into mainstream processors, establishing a pattern that would repeat throughout computer architecture evolution. The Intel 486DX in 1989 integrated floating-point capabilities directly into the CPU, improving system efficiency while maintaining specialized circuitry for floating-point computation. This integration pattern demonstrates how successful specialized functions eventually become standard features of general-purpose processors.

The principles established through early floating-point acceleration continue to influence modern hardware specialization. These include:

1. Identification of computational bottlenecks through workload analysis
2. Development of specialized circuits for frequent operations
3. Creation of efficient hardware-software interfaces
4. Progressive integration of proven specialized functions

This period in hardware specialization set the stage for increasingly sophisticated forms of acceleration. As computational demands grew more diverse and intensive, these basic principles extended to new domains including graphics processing, digital signal processing, and eventually, machine learning acceleration.

### Expanding Specialized Computing

The principles established through floating-point acceleration provided a blueprint for addressing emerging computational challenges. As computing applications diversified, new computational patterns emerged that exceeded the capabilities of general-purpose processors. This expansion of specialized computing manifested across multiple domains, each contributing unique insights to hardware acceleration strategies.

Graphics processing emerged as a significant driver of hardware specialization in the 1990s. Early graphics accelerators focused on specific operations like bitmap transfers and polygon filling. The introduction of programmable graphics pipelines with NVIDIA's GeForce 256 in 1999 represented a crucial advancement in specialized computing. Graphics Processing Units (GPUs) demonstrated how parallel processing architectures could efficiently handle data-parallel workloads, achieving orders of magnitude improvement over CPU implementations for graphics operations.

Digital Signal Processing (DSP) represents another fundamental domain of hardware specialization. DSP processors introduced architectural innovations specifically designed for efficient signal processing operations. These included specialized multiply-accumulate units, circular buffers, and parallel data paths optimized for filtering and transform operations. Texas Instruments' TMS32010, introduced in 1983, established how domain-specific instruction sets and memory architectures could dramatically improve performance for signal processing applications.

Network processing introduced additional patterns of specialization. Network processors developed unique architectures to handle packet processing at line rate, incorporating multiple processing cores, specialized packet manipulation units, and sophisticated memory management systems. Intel's IXP2800 network processor demonstrated how multiple levels of hardware specialization could be combined to address complex processing requirements.

These diverse domains of specialization shared several common themes:

1. Identification of domain-specific computational patterns
2. Development of specialized processing elements and memory hierarchies
3. Creation of domain-specific programming models
4. Progressive evolution toward more flexible architectures

This period of expanding specialization demonstrated that hardware acceleration strategies could successfully address diverse computational requirements. The lessons learned from these domains would prove crucial for the development of modern accelerators, particularly in the emerging field of machine learning computation.

### Domain-Specific Architectures 

The emergence of domain-specific architectures (DSA) represents a fundamental shift in computer system design, driven by two critical factors: the ending of traditional scaling laws and the increasing computational demands of specialized workloads. The slowdown of Moore's Law and the end of Dennard scaling created a performance and efficiency crisis in general-purpose computing. As John Hennessy and David Patterson noted in their 2017 Turing Lecture, these limitations herald a new golden age of computer architecture focused on domain-specific solutions.

Traditional processor performance scaling relied on semiconductor process improvements and increased clock frequencies. However, power density limitations prevented continued frequency scaling, while manufacturing complexity slowed the pace of transistor density improvements. These constraints forced architects to reconsider fundamental design approaches. The solution emerged through specialized architectures that optimize silicon resources for specific computational domains, trading generality for efficiency.

Domain-specific architectures achieve superior performance and energy efficiency through several key principles:

1. Customized datapaths that match the computational patterns of target applications
2. Specialized memory hierarchies optimized for domain-specific data access patterns
3. Reduced instruction overhead through domain-specific instruction sets
4. Hardware structures that directly implement frequently used operations

One of the most successful deployments of domain-specific architectures can be found in multimedia processing. Modern smartphones, introduced in the late 2000s, can play high-definition video for extended durations—potentially over 20 hours—while consuming minimal battery power, despite the computational complexity of video decoding. This remarkable efficiency stems from dedicated hardware video codecs implementing standards like H.264/AVC (introduced in 2003) and H.265/HEVC (finalized in 2013). These specialized circuits achieve orders of magnitude better performance and energy efficiency compared to software implementations on general-purpose processors. Similar specialization exists for audio processing, where dedicated hardware decoders handle formats like AAC (standardized in 1997) and MP3 (introduced in the early 1990s) with exceptional efficiency.

The success of these early domain-specific accelerators encouraged broader adoption of specialized computing approaches. Google's Tensor Processing Unit (TPU), introduced in 2016, demonstrated how domain-specific architectures could achieve dramatic improvements in performance and energy efficiency for machine learning workloads. The TPU's systolic array architecture, optimized for matrix multiplication operations, exemplifies how understanding domain-specific computational patterns enables targeted architectural optimization.

This trend toward specialization continues to accelerate, with new architectures emerging for domains ranging from genomics processing to blockchain computation. The diversity of these solutions demonstrates that domain-specific architecture design has become a primary approach for addressing modern computational challenges. This shift represents not just a temporary trend but a fundamental change in how computing systems will be designed and optimized in the future.

### ML as a Computational Domain

Machine learning workloads present distinctive computational patterns that align naturally with domain-specific acceleration strategies. The core operations in machine learning, particularly deep neural networks, exhibit high computational intensity with structured parallelism. These characteristics make machine learning an ideal candidate for specialized hardware implementations that can exploit these patterns for improved performance and energy efficiency.

The computational requirements of machine learning workloads vary significantly between training and inference phases. During training, the hardware must handle both forward and backward propagation, requiring additional memory for storing intermediate activations and gradients. This process demands higher precision arithmetic to ensure stable gradient updates and convergence. In contrast, inference focuses solely on forward propagation and can often operate with reduced precision, enabling different optimization strategies.

The foundational operations remain similar across both phases - dominated by dense matrix multiplication operations that form the backbone of neural network computation. This computational intensity couples with substantial memory bandwidth demands, as large volumes of weight parameters and activation data must be efficiently moved between storage and processing elements. Training particularly stresses these requirements, needing to maintain multiple batches of data and their corresponding gradients in memory.

A distinctive aspect of machine learning computation is the varying precision requirements between training and inference. While training typically demands higher precision (often FP32 or FP16) to maintain gradient fidelity, inference can often operate with dramatically reduced precision (INT8 or even lower) while maintaining acceptable accuracy. This precision flexibility has led to specialized hardware designs that can dynamically adapt their arithmetic units between training and inference modes.

These patterns have driven the development of diverse acceleration approaches. Graphics Processing Units (GPUs) extended their parallel processing capabilities to machine learning computation through specialized tensor cores and enhanced memory hierarchies. Modern GPUs often include dedicated hardware for both training and inference, with features like mixed-precision arithmetic and specialized memory management for gradient accumulation. The transition was natural - the same architectural features that made GPUs efficient for graphics rendering proved valuable for neural network computation.

Google's Tensor Processing Unit demonstrates a more targeted approach to machine learning acceleration. While early TPU versions focused primarily on inference workloads, later generations expanded to support training through architectural adaptations like higher precision units and more sophisticated memory hierarchies. By focusing specifically on neural network computation, the TPU achieves higher efficiency than more general-purpose accelerators. Its systolic array architecture directly implements the matrix multiplication patterns central to neural network processing, demonstrating how understanding domain-specific requirements enables focused optimization.

The success of machine learning accelerators validates the broader principles of domain-specific architecture design. Just as video codecs made multimedia processing practical on mobile devices, specialized ML accelerators are making neural network inference feasible on edge devices, while training accelerators enable faster model development in data centers. This parallel highlights how domain-specific architectures can transform the practical applications of computationally intensive algorithms across different deployment scenarios.

### Application-specific Specialization

The evolution of specialized hardware architectures demonstrates a fundamental principle in computer systems: as computational patterns emerge and mature, hardware specialization follows to achieve optimal performance and energy efficiency. @tbl-hw-evolution illustrates this progression across different computing eras, showing how specific computational needs consistently drive architectural innovation. While not comprehensive, the table highlights a clear pattern---specialized hardware solutions emerge to address the dominant computational requirements of each period, optimizing for both performance and energy efficiency.

+-------+------------------------------------+---------------------------------------------+------------------------------------+
| Era   | Computational Pattern              | Architecture Examples                       | Key Characteristics                |
+:======+:===================================+:============================================+:===================================+
| 1980s | Floating-Point & Signal Processing | FPU, DSP                                    | • Single-purpose engines<br>       |
|       |                                    |                                             | • Focused instruction sets<br>     |
|       |                                    |                                             | • Coprocessor interfaces           |
+-------+------------------------------------+---------------------------------------------+------------------------------------+
| 1990s | 3D Graphics & Multimedia           | GPU, SIMD Units                             | • Many identical compute units<br> |
|       |                                    |                                             | • Regular data patterns<br>        |
|       |                                    |                                             | • Wide memory interfaces           |
+-------+------------------------------------+---------------------------------------------+------------------------------------+
| 2000s | Real-time Media Coding             | Media Codecs, Network Processors            | • Fixed-function pipelines<br>     |
|       |                                    |                                             | • High throughput processing<br>   |
|       |                                    |                                             | • Power-performance optimization   |
+-------+------------------------------------+---------------------------------------------+------------------------------------+
| 2010s | Deep Learning Tensor Operations    | TPU, GPU Tensor Cores                       | • Matrix multiplication units<br>  |
|       |                                    |                                             | • Massive parallelism<br>          |
|       |                                    |                                             | • Memory bandwidth optimization    |
+-------+------------------------------------+---------------------------------------------+------------------------------------+
| 2020s | Application-Specific Acceleration  | ML Engines, Smart NICs, Domain Accelerators | • Workload-specific datapaths<br>  |
|       |                                    |                                             | • Customized memory hierarchies<br>|
|       |                                    |                                             | • Application-optimized designs    |
+-------+------------------------------------+---------------------------------------------+------------------------------------+

: Evolution of hardware specialization across computing eras. {#tbl-hw-evolution .striped .hover}

This historical progression reveals that hardware specialization is not a recent phenomenon but rather a consistent approach to achieving computational efficiency. As new computational patterns become important enough to justify dedicated hardware, specialized architectures emerge to optimize their execution. The success of these specialized solutions depends on fundamental principles of computer architecture: matching hardware structures to computational patterns, optimizing memory hierarchies for specific access patterns, and balancing specialization with programmability. Understanding these principles of hardware specialization is needed for developing efficient machine learning systems. The next sections explore how these principles manifest in modern AI accelerators, examining the architectural approaches that enable efficient execution of machine learning workloads.

## ML Processor Architecture Primitives

Neural network computations exhibit distinctive computational patterns that emerge from their mathematical foundations. These patterns fundamentally differ from traditional computing workloads in several key aspects. While general-purpose computing often involves complex control flow and scalar operations, machine learning workloads are characterized by highly structured, repetitive operations that process large arrays of data in parallel. Understanding these patterns reveals why specific processor primitives become essential for efficient execution.

When implementing neural networks in hardware, four fundamental requirements emerge:

1. Efficient parallel processing of independent data elements
2. Structured coordination of computation across multiple dimensions
3. Systematic movement of data through memory hierarchies
4. Hardware acceleration of non-linear mathematical functions

These requirements drive the development of specialized processor primitives that form the foundation of modern AI accelerators. The sections that follow examine four critical categories of architectural primitives:

```python
# High-level framework code
dense = Dense(512)(input_tensor)
```

This framework-level abstraction decomposes into mathematical operations:

```python
# Mathematical operations
output = matmul(input_weights) + bias
output = activation(output)
```

The mathematical representation further decomposes into processor-level computation:

```python
# Computational implementation
for n in range(batch_size):
    for m in range(output_size):
        sum = bias[m]
        for k in range(input_size):
            sum += input[n,k] * weights[k,m]
        output[n,m] = activation(sum)
```

Analysis of this computational decomposition reveals four fundamental characteristics:

1. Data parallelism across independent elements, enabling vector processing
2. Structured matrix operations that dominate computational complexity
3. Systematic data movement patterns that influence memory system design
4. Recurring non-linear transformations requiring specialized hardware support

The implementation of hardware primitives to accelerate these patterns depends on three primary criteria:

1. Utilization frequency sufficient to justify dedicated silicon area
2. Performance or efficiency advantages over general-purpose implementation
3. Architectural stability across multiple generations of neural network models

The following sections examine four categories of processor primitives that have emerged as essential building blocks for neural network acceleration: vector operations, matrix operations, and special function units. Each category addresses specific computational requirements while complementing the capabilities of the others. Together, these primitives form the foundation of modern machine learning accelerators.

### Vector Operations

Neural network computations contain inherent parallelism at multiple scales, from individual neurons to entire layers. Vector operations accelerate these computations by processing multiple data elements simultaneously, forming the foundation of efficient neural network execution. By examining how framework-level code translates to hardware instructions, we can understand the critical role of vector processing in neural accelerators.

#### Framework to Hardware Execution

Machine learning frameworks hide hardware complexity through high-level abstractions. These abstractions decompose into progressively lower-level operations, revealing opportunities for hardware acceleration. Consider the execution flow of a linear layer:

```python
# Framework Level: What ML developers write
layer = nn.Linear(256, 512)  # Layer transforms 256 inputs to 512 outputs
output = layer(input_tensor) # Process a batch of inputs
```

This abstraction represents a fully connected layer that transforms input features through learned weights. The framework translates this high-level expression into mathematical operations:

```python
# Framework Internal: Mathematical operations
Z = matmul(weights, input) + bias    # Each output needs all inputs
output = activation(Z)               # Transform each result
```

These mathematical operations decompose into explicit computational steps during processor execution. Each output value requires a sequence of multiply-accumulate operations:

```python
# Computational Level: Implementation
for batch in range(32):              # Process 32 samples at once
    for out_neuron in range(512):    # Compute each output neuron
        sum = 0.0
        for in_feature in range(256): # Each output needs all inputs
            sum += input[batch, in_feature] * weights[out_neuron, in_feature]
        output[batch, out_neuron] = activation(sum + bias[out_neuron])
```

#### Sequential Execution on Scalar Processors

Traditional scalar processors execute these operations sequentially, processing individual values one at a time. For a batch of 32 samples, computing the layer outputs requires over 4 million multiply-accumulate operations. Each operation involves loading an input value and a weight value, multiplying them, and accumulating the result. This sequential approach becomes highly inefficient when processing the massive number of identical operations required by neural networks.

Vector processing units transform this execution pattern by operating on multiple data elements simultaneously. The following RISC-V assembly code demonstrates modern vector processing:

```assembly
# Vector hardware execution (RISC-V Vector Extension)
vsetvli t0, a0, e32   # Process 8 elements at once
loop_batch:
    loop_neuron:
        vxor.vv v0, v0, v0    # Clear 8 accumulators
        loop_feature:
            vle32.v v1, (in_ptr)   # Load 8 inputs together
            vle32.v v2, (wt_ptr)   # Load 8 weights together
            vfmacc.vv v0, v1, v2   # 8 multiply-adds at once
            add in_ptr, in_ptr, 32  # Move to next 8 inputs
            add wt_ptr, wt_ptr, 32  # Move to next 8 weights
            bnez feature_cnt, loop_feature
```

#### Parallel Execution with Vector Processing

This vector implementation processes eight data elements in parallel, reducing both computation time and energy consumption. Vector load instructions transfer eight values simultaneously, maximizing memory bandwidth utilization. The vector multiply-accumulate instruction processes eight pairs of values in parallel, dramatically reducing the total instruction count from over 4 million to approximately 500,000.

Modern vector processors support additional specialized operations that accelerate common neural network patterns. @tbl-vector summarizes key vector operations and their applications in neural network computation:

+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Vector Operation        | Description                                         | Neural Network Application                  |
+:========================+:====================================================+:============================================+
| Reduction               | Combines elements across a vector (e.g., sum, max)  | Pooling layers, attention score computation |
+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Gather                  | Loads multiple non-consecutive memory elements      | Embedding lookups, sparse operations        |
+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Scatter                 | Writes to multiple non-consecutive memory locations | Gradient updates for embeddings             |
+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Masked operations       | Selectively operates on vector elements             | Attention masks, padding handling           |
+-------------------------+-----------------------------------------------------+---------------------------------------------+
| Vector-scalar broadcast | Applies scalar to all vector elements               | Bias addition, scaling operations           |
+-------------------------+-----------------------------------------------------+---------------------------------------------+

: Vector operations and their neural network applications. {#tbl-vector .striped .hover}

The efficiency gains from vector processing extend beyond instruction count reduction. Memory bandwidth utilization improves as vector loads transfer multiple values per operation. Energy efficiency increases because control logic is shared across multiple operations. These improvements compound across the deep layers of modern neural networks, where billions of operations execute for each forward pass.

Vector operations establish the foundation for neural network acceleration by enabling efficient parallel processing of independent data elements. However, the core transformations in neural networks require coordinating computation across multiple dimensions simultaneously. This need for structured parallel computation leads to the next architectural primitive: matrix operations.

#### Historical Foundations of Vector Processing

The principles underlying vector operations have long played a central role in high-performance computing. In the 1970s and 1980s, vector processors emerged as a critical architectural solution for scientific computing, weather modeling, and physics simulations, where large arrays of data required efficient parallel processing. Early systems such as the Cray-1, one of the first commercially successful supercomputers, introduced dedicated vector units to perform arithmetic operations on entire data vectors in a single instruction. This approach dramatically improved computational throughput compared to traditional scalar execution.

These foundational concepts have reemerged in the context of machine learning, where neural networks exhibit an inherent structure well suited to vectorized execution. The same fundamental operations—vector addition, multiplication, and reduction—that once accelerated numerical simulations now drive the execution of deep learning workloads. While the scale and specialization of modern AI accelerators differ from their historical predecessors, the underlying architectural principles remain the same. The resurgence of vector processing in neural network acceleration highlights its enduring utility as a mechanism for achieving high computational efficiency.

Vector operations establish the foundation for neural network acceleration by enabling efficient parallel processing of independent data elements. However, the core transformations in neural networks require coordinating computation across multiple dimensions simultaneously. This need for structured parallel computation leads to the next architectural primitive: matrix operations.

### Matrix Operations

Matrix operations are the computational workhorse of neural networks, transforming high-dimensional data through structured patterns of weights, activations, and gradients. While vector operations process elements independently, matrix operations orchestrate computations across multiple dimensions simultaneously. Understanding these operations reveals fundamental patterns that drive hardware acceleration strategies.

#### Matrix Operations in Neural Networks

Neural network computations decompose into hierarchical matrix operations. A linear layer illustrates this hierarchy:

```python
# Framework Level: What ML developers write
layer = nn.Linear(256, 512)  # Layer transforms 256 inputs to 512 outputs
output = layer(input_batch)  # Process a batch of 32 samples

# Framework Internal: Core operations
Z = matmul(weights, input)   # Matrix: transforms [256 x 32] input to [512 x 32] output
Z = Z + bias                 # Vector: adds bias to each output independently
output = relu(Z)             # Vector: applies activation to each element independently
```

This computation illustrates the scale of matrix operations in neural networks. Each output neuron (512 total) must process all input features (256 total) for every sample in the batch (32 samples). The weight matrix alone contains 256 × 512 = 131,072 parameters that define these connections, demonstrating why efficient matrix multiplication becomes crucial for performance.

#### Types of Matrix Computations in Neural Networks

Matrix operations appear consistently across modern neural architectures. Consider these fundamental patterns:

```python
# Linear Layers - Direct matrix multiply
hidden = matmul(weights, inputs)    # weights: [out_dim x in_dim], inputs: [in_dim x batch]
                                   # Result combines all inputs for each output

# Attention Mechanisms - Multiple matrix operations
Q = matmul(Wq, inputs)       # Project inputs to query space [query_dim x batch]
K = matmul(Wk, inputs)       # Project inputs to key space [key_dim x batch]
attention = matmul(Q, K.T)   # Compare all queries with all keys [query_dim x key_dim]

# Convolutions - Matrix multiply after reshaping
patches = im2col(input)           # Convert [H x W x C] image to matrix of patches
output = matmul(kernel, patches)  # Apply kernels to all patches simultaneously
```

This pervasive pattern of matrix multiplication has direct implications for hardware design. Modern processors implement dedicated matrix units that extend beyond vector processing capabilities:

#### Hardware Acceleration of Matrix Operations

The computational demands of matrix operations have driven specialized hardware optimizations. Modern processors implement dedicated matrix units that extend beyond vector processing capabilities. Consider the following example of matrix acceleration in hardware:

```assembly
# Matrix processing unit operation for a block of the computation
mload mr1, (weight_ptr)     # Load e.g., 16x16 block of weight matrix 
mload mr2, (input_ptr)      # Load corresponding input block
matmul.mm mr3, mr1, mr2     # Multiply and accumulate entire blocks at once
mstore (output_ptr), mr3    # Store computed output block
```

These matrix operations complement vectorized computation by enabling structured many-to-many transformations.

#### Comparison of Matrix and Vector Operations

The interplay between matrix and vector operations shapes the efficiency of neural network execution. Table @tbl-matrix summarizes the distinct characteristics of these operations.

+-------------------+-------------------------+--------------------------+-------------------------------------------------+
| Operation Type    | Best For                | Examples                 | Key Characteristic                              |
+:==================+:========================+:=========================+:================================================+
|                   |                         | • Layer transformations  |                                                 |
| Matrix Operations | Many-to-many transforms | • Attention computation  | Each output depends on multiple inputs          |
|                   |                         | • Convolutions           |                                                 |
+-------------------+-------------------------+--------------------------+-------------------------------------------------+
|                   |                         | • Activation functions   |                                                 |
| Vector Operations | One-to-one transforms   | • Layer normalization    | Each output depends only on corresponding input |
|                   |                         | • Element-wise gradients |                                                 |
+-------------------+-------------------------+--------------------------+-------------------------------------------------+

: Comparison of matrix and vector operation characteristics. {#tbl-matrix .striped .hover}

This complementary relationship guides hardware design decisions. Accelerators provide dedicated matrix units for intensive many-to-many computations while maintaining efficient vector units for the numerous element-wise operations that connect them. However, achieving peak performance requires careful orchestration of data movement between these units, leading to our next consideration: dataflow patterns.

#### Historical Foundations of Matrix Compuation

Matrix operations have long served as a cornerstone of computational mathematics, with applications extending from numerical simulations to graphics processing. The structured nature of matrix multiplications and transformations made them a natural target for acceleration in early computing architectures. In the 1980s and 1990s, specialized digital signal processors (DSPs) and graphics processing units (GPUs) optimized for matrix computations played a critical role in accelerating workloads such as image processing, scientific computing, and 3D rendering.

The widespread adoption of machine learning has reinforced the importance of efficient matrix computation. Neural networks, fundamentally built on matrix multiplications and tensor operations, have driven the development of dedicated hardware architectures that extend beyond traditional vector processing. Modern tensor processing units (TPUs) and AI accelerators implement matrix multiplication at scale, reflecting the same architectural principles that once underpinned early scientific computing and graphics workloads. The resurgence of matrix-centric architectures highlights the deep connection between classical numerical computing and contemporary AI acceleration.

While vector and matrix operations form the computational backbone of neural networks, machine learning workloads also require efficient implementations of non-linear functions such as activation functions, normalization layers, and exponential transformations. These functions introduce computational challenges that demand dedicated hardware support. The next section examines special function units (SFUs) and their role in accelerating non-linear operations.

### Special Function Units

Neural networks combine linear transformations with non-linear functions to model complex patterns. While vector and matrix units efficiently handle linear operations, non-linear functions present unique computational challenges. This section examines how Special Function Units (SFUs) accelerate these essential non-linear computations through dedicated hardware implementations.

#### Non-Linear Functions

Non-linear functions play a fundamental role in deep learning by enabling neural networks to model complex relationships. Consider a typical neural network layer sequence:Edit

```python
# Framework Level Operation
layer = nn.Sequential(
    nn.Linear(256, 512),
    nn.ReLU(),
    nn.BatchNorm1d(512)
)
output = layer(input_tensor)
```

This sequence introduces multiple non-linear transformations. The framework decomposes it into mathematical operations:

```python
# Mathematical Operations
Z = matmul(weights, input) + bias    # Linear transformation
H = max(0, Z)                        # ReLU activation
mean = reduce_mean(H, axis=0)        # BatchNorm statistics
var = reduce_mean((H - mean)**2)     # Variance computation
output = gamma * (H - mean)/sqrt(var + eps) + beta  # Normalization
```

#### Implementing the Non-Linear Functions

On traditional processors, these seemingly simple mathematical operations translate into complex sequences of instructions. Consider the computation of batch normalization: calculating the square root requires multiple iterations of numerical approximation, while exponential functions in operations like softmax need series expansion or lookup tables. Even a simple ReLU activation requires conditional branching, which can disrupt instruction pipelining:

```python
# Traditional Implementation Overhead
for batch in range(32):
    for feature in range(512):
        # ReLU: Requires branch prediction and potential pipeline stalls
        z = matmul_output[batch, feature]
        h = max(0.0, z)    # Conditional operation
        
        # BatchNorm: Multiple passes over data
        mean_sum[feature] += h        # First pass for mean
        var_sum[feature] += h * h     # Additional pass for variance
        
        temp[batch, feature] = h      # Extra memory storage needed

# Normalization requires complex arithmetic
for feature in range(512):
    mean = mean_sum[feature] / batch_size
    var = (var_sum[feature] / batch_size) - mean * mean
    
    # Square root computation: Multiple iterations
    scale = gamma[feature] / sqrt(var + eps)  # Iterative approximation
    shift = beta[feature] - mean * scale
    
    # Additional pass over data for final computation
    for batch in range(32):
        output[batch, feature] = temp[batch, feature] * scale + shift
```

These operations introduce several key inefficiencies:

1. Multiple passes over data, increasing memory bandwidth requirements
2. Complex arithmetic requiring many instruction cycles
3. Conditional operations that can cause pipeline stalls
4. Additional memory storage for intermediate results
5. Poor utilization of vector processing units

More specfiically, the code must scan through data multiple times. For example, batch normalization requires one pass to compute the mean (`mean_sum[feature] += h`), another for variance computation (`var_sum[feature] += h * h`), and a final pass for the output transformation. Each pass requires loading and storing data through the memory hierarchy. Operations that appear simple in mathematical notation often expand into many instructions. The square root in `scale = gamma[feature] / sqrt(var + eps)` typically requires 10-20 iterations of numerical methods like Newton-Raphson approximation to achieve suitable precision. Conditional operations like ReLU's `h = max(0.0, z)` require branch instructions. Modern processors use pipelining to process multiple instructions simultaneously, but branches can force the pipeline to stall while waiting for the condition to be evaluated. The code requires temporary storage (like `temp[batch, feature] = h`) to hold intermediate values between passes. This not only increases memory usage but also adds additional load/store operations that consume bandwidth and energy. While vector units excel at regular, streaming computations, functions like exponentials and square roots often require scalar operations that cannot fully utilize vector processing capabilities. This leads to idle computational resources and reduced efficiency. 

#### Hardware Acceleration

Special Function Units (SFUs) address these inefficiencies through dedicated hardware implementation. Modern ML accelerators include specialized circuits that transform these complex operations into single-cycle or fixed-latency computations. The accelerator can load a vector of values and apply non-linear functions directly, eliminating the need for multiple passes and complex instruction sequences:

```assembly
# Example hardware execution with Special Function Units
vld.v v1, (input_ptr)      # Load vector of values
vrelu.v v2, v1             # Single-cycle ReLU on entire vector
vsigm.v v3, v1             # Fixed-latency sigmoid computation
vtanh.v v4, v1             # Direct hardware tanh implementation
vrsqrt.v v5, v1            # Fast reciprocal square root
```

Each SFU implements a specific function through specialized circuitry. For instance, a ReLU unit performs the comparison and selection in dedicated logic, eliminating branching overhead. Square root operations use hardware implementations of algorithms like Newton-Raphson with fixed iteration counts, providing guaranteed latency. Exponential and logarithmic functions often combine small lookup tables with hardware interpolation circuits. Using these custom instructions, the SFU implementation eliminates multiple passes over data, removes complex arithmetic sequences, and maintains high computational efficiency. @tbl-sfu shows the various hardware implementations and their typical latencies. 

+------------------+---------------------+---------------------------------------+-----------------+
| Function Unit    | Operation           | Implementation Strategy               | Typical Latency |
+:=================+:====================+:======================================+:================+
| Activation Unit  | ReLU, sigmoid, tanh | Piece-wise approximation circuits     | 1-2 cycles      |
+------------------+---------------------+---------------------------------------+-----------------+
| Statistics Unit  | Mean, variance      | Parallel reduction trees              | log(N) cycles   |
+------------------+---------------------+---------------------------------------+-----------------+
| Exponential Unit | exp, log            | Table lookup + hardware interpolation | 2-4 cycles      |
+------------------+---------------------+---------------------------------------+-----------------+
| Root/Power Unit  | sqrt, rsqrt         | Fixed-iteration Newton-Raphson        | 4-8 cycles      |
+------------------+---------------------+---------------------------------------+-----------------+

: Special function unit implementation. {#tbl-sfu .striped .hover}

#### Historical Foundations of SFUs

The need for efficient non-linear function evaluation has shaped computer architecture for decades. Early processors incorporated hardware support for complex mathematical functions, such as logarithms and trigonometric operations, to accelerate workloads in scientific computing and signal processing.

In the 1970s and 1980s, floating-point co-processors were introduced to handle complex mathematical operations separately from the main CPU. In the 1990s, instruction set extensions such as Intel’s SSE and ARM’s NEON provided dedicated hardware for vectorized mathematical transformations, improving efficiency for multimedia and signal processing applications.

Machine learning workloads have reintroduced a strong demand for specialized functional units, as activation functions, normalization layers, and exponential transformations are fundamental to neural network computations. Rather than relying on iterative software approximations, modern AI accelerators implement fast, fixed-latency SFUs for these operations, mirroring historical trends in scientific computing.

The reemergence of dedicated special function units underscores the ongoing cycle in hardware evolution, where domain-specific requirements drive the reinvention of classical architectural concepts in new computational paradigms.

While vector, matrix, and special function units provide the computational backbone of modern AI accelerators, their effectiveness depends on how efficiently data is moved and accessed. The next section explores dataflow architectures, memory hierarchies, and data reuse strategies that are critical for sustaining high performance in neural network execution.

### Parallel Execution

The vector, matrix, and SFU operations covered in previous sections work together as part of larger execution models in modern ML accelerators. The two primary execution paradigms are Single Instruction Multiple Data (SIMD) and Single Instruction Multiple Thread (SIMT), along with specialized matrix execution models. These models provide abstraction layers that both hide hardware complexity and expose computational efficiency to ML frameworks.

#### Single Instruction Multiple Data (SIMD)

SIMD enables parallel execution of vector operations by applying a single instruction across multiple data elements simultaneously. For example, when applying ReLU activation to a layer's outputs, SIMD can process multiple values in parallel with a single instruction. This model is ideal for element-wise operations commonly found in ML workloads, such as activations, normalization, and element-wise arithmetic. SIMD reduces instruction overhead and improves memory bandwidth utilization by keeping execution tightly packed across multiple data points. ML frameworks such as TensorFlow, PyTorch, and JAX automatically generate SIMD-optimized instructions when performing vectorized tensor operations.

#### Single Instruction Multiple Thread (SIMT)

While SIMD is efficient for executing operations within a single compute unit, SIMT scales execution across multiple independent threads. This model is the foundation of GPU computing, where thousands of threads execute in parallel, each handling a portion of the overall tensor computation. SIMT is particularly useful in CNNs (for parallelizing convolutions), Transformers (for attention mechanisms), and large-scale training workloads, where multiple independent data blocks can be processed concurrently.

#### Tensor Cores

Matrix execution models extend SIMD and SIMT principles to structured multi-dimensional computations. AI accelerators such as GPUs (via Tensor Cores), TPUs (via Systolic Arrays), and FPGAs implement specialized matrix execution engines to accelerate operations like matrix multiplication and convolution. These engines perform block-wise matrix operations, keeping intermediate values in local registers to minimize memory transfers. Since modern deep learning workloads are heavily matrix-multiplication dependent, these execution models are essential for achieving high throughput.

@tbl-primitives-summary summarizes how the processor primitives are mapped to the execution models used in modern AI accelerators. These execution models define how computation is structured in hardware and determine how efficiently ML workloads run across different architectures.

+-------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+
| Processor Primitive           | Execution Model                                                                      | Example Use in ML                                                             |
+:==============================+:=====================================================================================+:==============================================================================+
| Vector operations             | SIMD                                                                                 | Activations, normalization, element-wise computation                          |
+-------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+
| Matrix operations             | SIMT                                                                                 | CNN convolutions, transformer attention, parallel training                    |
+-------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+
| Matrix operations             | Matrix execution engines (tensor cores, systolic arrays)                             | General matrix multiplication (GEMM), high-throughput deep learning inference |
+-------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+
| Special function units (SFUs) | SIMD (for vectorized evaluation) or SIMT (for large-scale parallel SFU computations) | Softmax, batch normalization, non-linear activations                          |
+-------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+

: Mapping the processing primitives to execution models. {#tbl-primitives-summary .striped .hover}

Understanding processor primitives and their translation into execution models provides a foundation for reasoning about how computations are structured, parallelized, and optimized. This knowledge establishes the basis for identifying where optimizations can be applied and how performance is influenced purely from a compute perspective. As we move forward, we will examine how memory systems and data movement further shape execution efficiency and overall ML accelerator performance.

## Memory Systems for AI Acceleration

Machine learning accelerators are designed to maximize computational throughput, leveraging specialized primitives such as vector units, matrix engines, and systolic arrays. However, the efficiency of these compute units is fundamentally constrained by the availability of data. Unlike conventional workloads, ML models require frequent access to large volumes of parameters, activations, and intermediate results, leading to substantial memory bandwidth demands. If data cannot be delivered to the processing elements at the required rate, memory bottlenecks can significantly limit performance, regardless of the accelerator’s raw computational capability.

To address this challenge, modern AI hardware integrates sophisticated memory hierarchies, data movement strategies, and compression techniques to optimize performance. This section explores the interaction between ML workloads and memory systems, focusing on data access patterns, memory bandwidth constraints, and architectural innovations that enable efficient execution. Understanding these aspects will help us understand how to optimize AI acceleration and mitigate memory-related bottlenecks.

### Memory Intensity

#### ML Workloads Are Memory-Intensive  

Machine learning workloads place substantial demands on memory systems due to the large volume of data involved in computation. Unlike traditional compute-bound applications, where performance is often dictated by the speed of arithmetic operations, ML workloads are characterized by high data movement requirements. The efficiency of an accelerator is not solely determined by its computational throughput but also by its ability to continuously supply data to processing units without introducing stalls or delays.

A neural network processes multiple types of data throughout its execution, each with distinct memory access patterns:

- **Model parameters (weights and biases):** Deep learning models, particularly those used in large-scale applications such as natural language processing and computer vision, often contain millions to billions of parameters. Storing and accessing these weights efficiently is essential for maintaining throughput.
- **Intermediate activations:** During both training and inference, each layer produces intermediate results that must be temporarily stored and retrieved for subsequent operations. These activations can contribute significantly to memory overhead, particularly in deep architectures.
- **Gradients (during training):** Backpropagation requires storing and accessing gradients for every parameter, further increasing the volume of data movement between compute units and memory.

As models grow in size and complexity, the reliance on memory bandwidth increases proportionally. While specialized compute units accelerate operations such as matrix multiplications, their effectiveness is limited by how quickly data can be delivered. If the memory system cannot keep up with the demands of the compute hardware, performance bottlenecks emerge, leading to inefficient execution and increased energy consumption. Addressing these challenges requires an understanding of how memory systems are structured and how they interact with different ML workloads.

If the memory system cannot keep up with the demands of the compute hardware, performance bottlenecks emerge, leading to inefficient execution and increased energy consumption. The severity of this bottleneck can be quantified by comparing the time spent moving data to the time spent on computation:

$$
T_{\text{mem}} = \frac{M_{\text{total}}}{B_{\text{mem}}}, \quad T_{\text{compute}} = \frac{\text{FLOPs}}{P_{\text{peak}}}
$$

where $T_{\text{mem}}$ represents data transfer time, determined by the total data volume $M_{\text{total}}$ and available memory bandwidth $B_{\text{mem}}$. The compute time $T_{\text{compute}}$ depends on the number of floating-point operations and peak hardware throughput. When $T_{\text{mem}} > T_{\text{compute}}$, the system is memory-bound, meaning the accelerator spends more time waiting for data than performing useful computation. This imbalance highlights the need for memory-optimized architectures and efficient data movement strategies to sustain high performance.

#### The Compute-Memory Imbalance  

Neural networks rely on specialized computational primitives such as vector operations, matrix multiplications, and domain-specific functional units that accelerate key aspects of deep learning workloads. These operations are designed for highly parallel execution, enabling accelerators to perform vast amounts of computation in each cycle. Given this level of specialization, one might expect neural networks to execute efficiently without significant bottlenecks. However, the primary constraint is not the raw compute power but rather the ability to continuously supply data to these processing units.

While these compute units can execute millions of operations per second, they remain heavily dependent on memory bandwidth to sustain peak performance. Each matrix multiplication or vector operation requires a steady flow of weights, activations, and intermediate results, all of which must be fetched from memory. If data cannot be delivered at the required rate, memory stalls occur, leaving many compute units idle. This imbalance between computational capability and data availability is often referred to as the memory wall—a fundamental challenge in AI acceleration.

Over time, the gap between computation and memory performance has widened. While specialized accelerators continue to increase their ability to perform highly parallelized operations, memory bandwidth has not scaled at the same rate, leading to frequent stalls and reduced utilization of available compute resources. This inefficiency is particularly evident in deep learning models, where large parameter sizes, frequent memory accesses, and non-uniform data movement patterns exacerbate memory bottlenecks.

<!-- insert the memory and compute delta graph here -->

Beyond performance limitations, memory access also imposes a significant energy cost. Fetching data from off-chip DRAM, in particular, consumes far more energy than performing arithmetic operations. As a result, optimizing memory usage is critical not only for improving performance but also for enhancing the efficiency and sustainability of AI accelerators.

<!-- compute vs. data movement cost -->

### Data Movement Patterns

While vector and matrix operations provide the foundation for neural network computations, their efficiency is fundamentally constrained by **how data moves through the system**. Neural networks process millions—or even billions—of parameters and activations, requiring careful coordination of memory access to ensure that operands reach computational units **at the right time** and with minimal overhead. The challenge is that the sheer scale of these computations makes **data movement a dominant factor in performance and energy consumption**.  

Consider a typical matrix multiplication operation that illustrates these challenges:  

```python
# Matrix multiplication where:
# weights: [512 x 256] - model parameters
# input:   [256 x 32]  - batch of activations
# Z:       [512 x 32]  - output activations

# Computing each output element Z[i,j]:
for i in range(512):
    for j in range(32):
        # Each output requires:
        # - Full row of weights[i,:] (256 elements)
        # - Full column of input[:,j] (256 elements)
        for k in range(256):
            Z[i,j] += weights[i,k] * input[k,j]
```

This computation reveals several critical dataflow challenges:

1. Each output element requires accessing 512 values (256 weights and 256 inputs)
2. Weight parameters can be reused across multiple input batches
3. Each input value contributes to multiple output computations
4. Partial products must accumulate over hundreds of operations

ML accelerators address these challenges through different dataflow patterns, each optimizing across four key dimensions:

1. Data reuse: Maximizing computations per memory access
2. Storage hierarchy: Optimizing data placement across memory levels
3. Parallelism: Distributing computation effectively
4. Energy efficiency: Minimizing costly data movement

Three fundamental dataflow patterns have emerged as primary solutions, each offering distinct advantages:

#### Weight Stationary Pattern

This pattern maximizes weight reuse by maintaining weight parameters in local storage while streaming input activations. It particularly suits convolutional neural networks where the same weight kernel processes multiple input regions:

```python
# Weight stationary implementation pseudocode
for weight_block in weights:
    load_to_local(weight_block)     # Single weight load
    for input_block in inputs:
        compute(weight_block, input_block)  # Process multiple inputs with cached weights
```

Keeping weights local reduces the need to fetch the same weight values repeatedly, significantly lowering memory bandwidth requirements.

#### Output Stationary Pattern

This pattern minimizes output movement by accumulating partial sums locally while fetching required weights and inputs. It excels in fully connected layers where each output depends on many input-weight combinations:

```python
# Output stationary implementation pseudocode
for output_element in outputs:
    accumulator = 0                
    for w, x in zip(weights, inputs):
        accumulator += w * x        # Accumulate in place
    store_output(accumulator)       # Single write per output
```

Reducing the intermediate memory writes optimizes accumulation operations and minimizes memory access overhead.

#### Input Stationary Pattern

This pattern optimizes input reuse by keeping input activations in local storage while iterating through different weight values. It is especially useful in batch processing scenarios, where the same set of inputs contributes to multiple computations.  

```python
# Input stationary implementation pseudocode
for input_block in inputs:
    load_to_local(input_block)      # Single input load
    for weight_block in weights:
        compute(weight_block, input_block)  # Process cached input
```

This approach minimizes input reloads, making it highly efficient for workloads that involve large batch sizes or shared feature representations. @tbl-data-movement summarizes the characteristics and applications of these patterns:

| Pattern | Optimal Use Case | Data Reuse Strategy | Memory Access Pattern | Energy Profile |
|---------|-----------------|-------------------|---------------------|----------------|
| Weight Stationary | CNNs, kernel operations | High weight reuse | Sequential input access | Efficient for frequent weight access |
| Output Stationary | Fully connected layers | Minimized output movement | Random weight/input access | Optimal for long accumulation chains |
| Input Stationary | Batch processing | High input reuse | Sequential weight access | Efficient for large batches |

: Comparison of dataflow patterns {#tbl-data-movement .striped .hover}

Modern ML accelerators often implement multiple or hybrid dataflow strategies, selecting the most appropriate pattern based on the layer type and model structure. This flexibility allows for better resource utilization and reduced memory overhead, improving both performance and energy efficiency. However, optimizing execution at the dataflow level is only part of the challenge. To fully understand memory system performance, we must next examine how data is managed across different levels of storage hierarchy in an ML accelerator.

### Summary

The four fundamental architectural primitives---vector operations, matrix operations, dataflow patterns, and special function units---are the basis of modern machine learning accelerators. Each primitive addresses a critical aspect of neural network computation: vector operations enable efficient element-wise processing, matrix operations handle structured parallel transformations, dataflow patterns optimize data movement through memory hierarchies, and special function units accelerate essential non-linear computations.

While modern ML accelerators implement many specialized capabilities like quantization engines, sparsity accelerators, and advanced memory management units, which we will cover later in this chapter, these four primitives remain fundamental because they:

1. Address the core computational patterns present in virtually all neural networks
2. Maintain relevance across multiple hardware generations
3. Provide building blocks for more complex acceleration strategies
4. Enable systematic analysis of accelerator architectures

The following section build upon these fundamentals to explore advanced architectural features and complete accelerator designs, showing how these primitives combine with specialized units to create efficient ML hardware.
