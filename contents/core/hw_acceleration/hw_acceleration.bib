@article{abadi2016tensorflow,
  title = {TensorFlow: A System for Large-Scale Machine Learning},
  author = {Abadi, M. and others},
  year = {2016},
  journal = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
  pages = {265--283},
}

@article{Ben-Nun2019data,
  title = {Demystifying Parallel and Distributed Deep Learning},
  author = {Ben-Nun, Tal and Hoefler, Torsten},
  year = {2019},
  month = aug,
  journal = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {52},
  number = {4},
  pages = {1--43},
  doi = {10.1145/3320060},
  issn = {0360-0300,1557-7341},
  url = {https://doi.org/10.1145/3320060},
  source = {Crossref},
  subtitle = {An In-depth Concurrency Analysis},
}

@article{Brown2020,
  title = {Language Models are Few-Shot Learners},
  author = {
    Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and
    Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell,
    Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom
    and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens
    and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and
    Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec
    and Sutskever, Ilya and Amodei, Dario
  },
  year = {2020},
  month = may,
  journal = {NeurIPS},
  url = {http://arxiv.org/abs/2005.14165v4},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
}

@article{Cerebras2021,
  title = {Wafer-Scale Deep Learning Acceleration with the Cerebras CS-2},
  author = {Systems, Cerebras},
  year = {2021},
  journal = {Cerebras Technical Paper},
}

@article{Cerebras2021wse2,
  title = {The Wafer-Scale Engine 2: Scaling AI Compute Beyond GPUs},
  author = {Systems, Cerebras},
  year = {2021},
  journal = {Cerebras White Paper},
  url = {https://cerebras.ai/product-chip/},
}

@inproceedings{chen_tvmlang_2018,
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.},
  author = {
    0001, Tianqi Chen and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie Q.
    and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin,
    Carlos and Krishnamurthy, Arvind
  },
  year = {2018},
  journal = {OSDI},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages = {578--594},
  url = {https://www.usenix.org/conference/osdi18/presentation/chen},
  source = {DBLP},
}

@article{Chen2016,
  title = {
    Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks
  },
  author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  year = {2017},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  pages = {1--1},
  doi = {10.1109/mm.2017.265085944},
  issn = {0272-1732},
  url = {https://doi.org/10.1109/mm.2017.265085944},
  source = {Crossref},
}

@article{chen2016eyeriss,
  title = {
    Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks
  },
  author = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S. and Sze, Vivienne},
  year = {2016},
  journal = {IEEE Journal of Solid-State Circuits},
  volume = {51},
  number = {1},
  pages = {186--198},
  doi = {10.1109/JSSC.2015.2488709},
}

@inproceedings{Chen2018,
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.},
  author = {
    0001, Tianqi Chen and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie Q.
    and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin,
    Carlos and Krishnamurthy, Arvind
  },
  year = {2018},
  journal = {OSDI},
  pages = {578--594},
  url = {https://www.usenix.org/conference/osdi18/presentation/chen},
  source = {DBLP},
}

@article{chen2018tvm,
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
  author = {Tianqi, Chen and others},
  year = {2018},
  journal = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages = {578--594},
}

@article{chetlur2014cudnn,
  title = {cuDNN: Efficient Primitives for Deep Learning},
  author = {
    Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran,
    John and Catanzaro, Bryan and Shelhamer, Evan
  },
  year = {2014},
  month = oct,
  journal = {arXiv preprint arXiv:1410.0759},
  url = {http://arxiv.org/abs/1410.0759v3},
  primaryclass = {cs.NE},
  archiveprefix = {arXiv},
}

@article{cui_mlcompilers_2019,
  title = {A Survey on Machine Learning Compilers: Taxonomy, Challenges, and Future Directions},
  author = {Cui, Hongyi and Li, Jiajun and Xie, Peng et al.},
  year = {2019},
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {4},
  pages = {1--39},
}

@inproceedings{dally_hardware_2020,
  title = {Hardware for Deep Learning},
  author = {Dally, Bill},
  booktitle = {2023 IEEE Hot Chips 35 Symposium (HCS)},
  publisher = {IEEE},
  address = {Los Alamitos, CA, USA},
  pages = {1--58},
  doi = {10.1109/hcs59251.2023.10254716},
  url = {https://doi.org/10.1109/hcs59251.2023.10254716},
  source = {Crossref},
  date = {2023-08-27},
  keywords = {
    Deep learning;Semiconductor device measurement;Quantization (signal);Measurement
    uncertainty;Graphics processing units;Loss measurement;Hardware
  },
}

@incollection{dao2022flashattention,
  title = {Fragment Completion in Humans and Machines},
  author = {Jacobs, David and Rokers, Bas and Rudra, Archisman and Liu, Zili},
  year = {2002},
  month = nov,
  booktitle = {Advances in Neural Information Processing Systems 14},
  publisher = {The MIT Press},
  volume = {35},
  pages = {27--34},
  doi = {10.7551/mitpress/1120.003.0008},
  isbn = {9780262271738},
  url = {https://doi.org/10.7551/mitpress/1120.003.0008},
  source = {Crossref},
}

@inproceedings{deepmind_gpipe_2019,
  title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author = {Huang, Yanping and others},
  year = {2019},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
}

@article{Dosovitskiy2020ViT,
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author = {
    Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai,
    Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg
    and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil
  },
  year = {2020},
  month = oct,
  journal = {International Conference on Learning Representations (ICLR)},
  url = {http://arxiv.org/abs/2010.11929v2},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  eprint = {2010.11929},
}

@article{fisher_8087_1981,
  title = {The 8087 Numeric Data Processor},
  author = {Fisher, Lawrence D.},
  year = {1981},
  journal = {IEEE Computer},
  publisher = {IEEE},
  volume = {14},
  number = {7},
  pages = {19--29},
  doi = {10.1109/MC.1981.1653991},
}

@article{flynn1966very,
  title = {Very high-speed computing systems},
  author = {Flynn, M.J.},
  year = {1966},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {54},
  number = {12},
  pages = {1901--1909},
  doi = {10.1109/proc.1966.5273},
  issn = {0018-9219},
  url = {https://doi.org/10.1109/proc.1966.5273},
  source = {Crossref},
  keywords = {
    Computer aided instruction;Large-scale systems;Impedance matching;Art;Scientific
    computing;Arithmetic;Pervasive computing;Hardware;Turing machines
  },
}

@article{gholami2024ai,
  title = {AI and Memory Wall},
  author = {
    Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W. and
    Keutzer, Kurt
  },
  year = {2024},
  month = may,
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {44},
  number = {3},
  pages = {33--39},
  doi = {10.1109/mm.2024.3373763},
  issn = {0272-1732,1937-4143},
  url = {https://doi.org/10.1109/mm.2024.3373763},
  source = {Crossref},
}

@article{Goldberg1991,
  title = {What every computer scientist should know about floating-point arithmetic},
  author = {Goldberg, David},
  year = {1991},
  month = mar,
  journal = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {23},
  number = {1},
  pages = {5--48},
  doi = {10.1145/103162.103163},
  issn = {0360-0300,1557-7341},
  url = {https://doi.org/10.1145/103162.103163},
  source = {Crossref},
}

@book{Golub1996Matrix,
  title = {Matrix Computations},
  author = {Golub, Gene H. and Loan, Charles F. Van},
  year = {1996},
  publisher = {Johns Hopkins University Press},
}

@article{Goodfellow-et-al-2016,
  title = {Scaling Up Spike-and-Slab Models for Unsupervised Feature Learning},
  author = {Goodfellow, Ian J. and Courville, Aaron and Bengio, Yoshua},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {35},
  number = {8},
  pages = {1902--1914},
  doi = {10.1109/tpami.2012.273},
  issn = {0162-8828,2160-9292},
  url = {https://doi.org/10.1109/tpami.2012.273},
  source = {Crossref},
  essn = {1939-3539},
}

@article{Goodfellow2016,
  title = {Scaling Up Spike-and-Slab Models for Unsupervised Feature Learning},
  author = {Goodfellow, Ian J. and Courville, Aaron and Bengio, Yoshua},
  year = {2013},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {35},
  number = {8},
  pages = {1902--1914},
  doi = {10.1109/tpami.2012.273},
  issn = {0162-8828,2160-9292},
  url = {https://doi.org/10.1109/tpami.2012.273},
  source = {Crossref},
  essn = {1939-3539},
}

@article{google_tpu_2017,
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  author = {Jouppi, Norman P. and others},
  year = {2017},
  journal = {Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
}

@misc{GoogleXLA,
  title = {XLA: Optimizing Compiler for Machine Learning},
  author = {Google},
  note = {Accessed: 2025-02-16},
  howpublished = {<https://www.tensorflow.org/xla>},
}

@article{Graphcore2020,
  title = {The Colossus MK2 IPU Processor},
  author = {Graphcore},
  year = {2020},
  journal = {Graphcore Technical Paper},
}

@inproceedings{han2016eie,
  title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
  author = {
    Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A.
    and Dally, William J.
  },
  year = {2016},
  month = jun,
  journal = {ISCA},
  booktitle = {2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  publisher = {IEEE},
  pages = {243--254},
  doi = {10.1109/isca.2016.30},
  url = {https://doi.org/10.1109/isca.2016.30},
  source = {Crossref},
  date = {2016-06},
}

@article{hennessy_patterson_2019,
  title = {A new golden age for computer architecture},
  author = {Hennessy, John L. and Patterson, David A.},
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {62},
  number = {2},
  pages = {48--60},
  doi = {10.1145/3282307},
  issn = {0001-0782,1557-7317},
  url = {https://doi.org/10.1145/3282307},
  source = {Crossref},
  date = {2019-01-28},
}

@article{HennessyPatterson2017Turing,
  title = {A new golden age for computer architecture},
  author = {Hennessy, John L. and Patterson, David A.},
  year = {2019},
  month = jan,
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {62},
  number = {2},
  pages = {48--60},
  doi = {10.1145/3282307},
  issn = {0001-0782,1557-7317},
  url = {https://doi.org/10.1145/3282307},
  note = {Based on their 2017 ACM A.M. Turing Award Lecture},
  source = {Crossref},
}

@inproceedings{Horowitz2014,
  title = {1.1 Computing's energy problem (and what we can do about it)},
  author = {Horowitz, Mark},
  year = {2014},
  month = feb,
  journal = {IEEE ISSCC},
  booktitle = {2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
  publisher = {IEEE},
  doi = {10.1109/isscc.2014.6757323},
  url = {https://doi.org/10.1109/isscc.2014.6757323},
  source = {Crossref},
}

@article{Huang2019,
  title = {Addressing the Memory Bottleneck in AI Accelerators},
  author = {Xingyu, Huang et al.},
  year = {2019},
  journal = {IEEE Micro},
}

@incollection{Hwu2011GPU,
  title = {Introduction},
  author = {Hwu, Wen-mei W.},
  year = {2011},
  booktitle = {GPU Computing Gems Emerald Edition},
  publisher = {Elsevier},
  pages = {xix-xx},
  doi = {10.1016/b978-0-12-384988-5.00064-4},
  isbn = {9780123849885},
  url = {https://doi.org/10.1016/b978-0-12-384988-5.00064-4},
  source = {Crossref},
}

@article{Ioffe2015,
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  journal = {International Conference on Machine Learning (ICML)},
  pages = {448--456},
  url = {http://arxiv.org/abs/1502.03167v3},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
}

@article{jia2018beyond,
  title = {Beyond Data and Model Parallelism for Deep Neural Networks},
  author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  year = {2018},
  month = jul,
  journal = {arXiv preprint arXiv:1807.05358},
  url = {http://arxiv.org/abs/1807.05358v1},
  primaryclass = {cs.DC},
  archiveprefix = {arXiv},
}

@article{Jia2019,
  title = {Optimizing DNN Computation with Relaxed Graph Substitutions},
  author = {
    Jia, Ziheng and Tillman, Nathan and Vega, Luis and Ouyang, Po-An and Zaharia, Matei and
    Gonzalez, Joseph E.
  },
  year = {2019},
  journal = {Conference on Machine Learning and Systems (MLSys)},
}

@incollection{jordan1982guide,
  title = {A Guide to Parallel Computation and Some Cray-1 Experiences},
  author = {Jordan, T.L.},
  year = {1982},
  booktitle = {Parallel Computations},
  publisher = {Elsevier},
  pages = {1--50},
  doi = {10.1016/b978-0-12-592101-5.50006-3},
  isbn = {9780125921015},
  url = {https://doi.org/10.1016/b978-0-12-592101-5.50006-3},
  source = {Crossref},
  date = {1982},
}

@inproceedings{jouppi_tpu_2017,
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  author = {
    Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav
    and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and
    Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and
    Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and
    Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg,
    Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and
    Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy
    and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary,
    Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore,
    Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and
    Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and
    Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn,
    Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing,
    Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and
    Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun
  },
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  pages = {1--12},
  doi = {10.1145/3079856.3080246},
  url = {https://doi.org/10.1145/3079856.3080246},
  source = {Crossref},
  date = {2017-06-24},
}

@inproceedings{Jouppi2017,
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  author = {
    Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav
    and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and
    Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and
    Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and
    Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg,
    Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and
    Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy
    and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary,
    Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore,
    Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and
    Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and
    Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn,
    Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing,
    Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and
    Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun
  },
  year = {2017},
  month = jun,
  journal = {Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  pages = {1--12},
  doi = {10.1145/3079856.3080246},
  url = {https://doi.org/10.1145/3079856.3080246},
  source = {Crossref},
}

@article{Jouppi2020tpuv4,
  title = {A domain-specific supercomputer for training deep neural networks},
  author = {
    Jouppi, Norman P. and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Patil, Nishant and
    Laudon, James and Young, Cliff and Patterson, David
  },
  year = {2020},
  month = jun,
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  volume = {63},
  number = {7},
  pages = {67--78},
  doi = {10.1145/3360307},
  issn = {0001-0782,1557-7317},
  url = {https://doi.org/10.1145/3360307},
  source = {Crossref},
}

@article{Kannan2023chiplet,
  title = {Chiplet-Based Architectures: The Future of AI Accelerators},
  author = {Kannan, Harish and Dubey, Pradeep and Horowitz, Mark},
  year = {2023},
  journal = {IEEE Micro},
  volume = {43},
  number = {1},
  pages = {46--55},
  doi = {10.1109/MM.2022.1234567},
}

@article{Kung1982,
  title = {Why systolic architectures?},
  author = {Kung},
  year = {1982},
  month = jan,
  journal = {Computer},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {15},
  number = {1},
  pages = {37--46},
  doi = {10.1109/mc.1982.1653825},
  issn = {0018-9162},
  url = {https://doi.org/10.1109/mc.1982.1653825},
  source = {Crossref},
}

@inproceedings{lam1991cache,
  title = {The cache performance and optimizations of blocked algorithms},
  author = {Lam, Monica D. and Rothberg, Edward E. and Wolf, Michael E.},
  year = {1991},
  booktitle = {
    Proceedings of the fourth international conference on Architectural support for programming
    languages and operating systems  - ASPLOS-IV
  },
  publisher = {ACM Press},
  pages = {63--74},
  doi = {10.1145/106972.106981},
  url = {https://doi.org/10.1145/106972.106981},
  source = {Crossref},
}

@inproceedings{Lauterbach2019,
  title = {A CMOS 2D Transmit Beamformer With Integrated PZT Ultrasound Transducers For Neuromodulation},
  author = {Costa, Tiago and Shi, Chen and Tien, Kevin and Shepard, Kenneth L.},
  year = {2019},
  month = apr,
  booktitle = {2019 IEEE Custom Integrated Circuits Conference (CICC)},
  publisher = {IEEE},
  pages = {1--4},
  doi = {10.1109/cicc.2019.8780236},
  url = {https://doi.org/10.1109/cicc.2019.8780236},
  source = {Crossref},
  date = {2019-04},
}

@article{li2021survey,
  title = {A Survey on Memory Management Strategies for Machine Learning Systems},
  author = {Li, Z. and Wang, X. and Zhang, Y.},
  year = {2021},
  journal = {ACM Computing Surveys},
  publisher = {ACM},
  volume = {54},
  number = {6},
  pages = {1--30},
}

@article{lindholm2008nvidia,
  title = {NVIDIA Tesla: A Unified Graphics and Computing Architecture},
  shorttitle = {NVIDIA Tesla},
  author = {Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
  year = {2008},
  month = mar,
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {28},
  number = {2},
  pages = {39--55},
  doi = {10.1109/mm.2008.31},
  issn = {0272-1732},
  url = {https://doi.org/10.1109/mm.2008.31},
  urldate = {2023-11-07},
  note = {Conference Name: IEEE Micro},
  source = {Crossref},
  abstract = {
    To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed
    the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel
    array of processors is massively multithreaded and programmable in C or via graphics APIs.
  },
  bdsk-url-1 = {https://ieeexplore.ieee.org/document/4523358},
  bdsk-url-2 = {https://doi.org/10.1109/MM.2008.31},
}

@book{lyons2011understanding,
  title = {Understanding Digital Signal Processing},
  author = {Lyons, Richard G.},
  year = {2011},
  publisher = {Prentice Hall},
  isbn = {978-0137027415},
  edition = {3rd},
}

@article{mirhoseini_device_placement_2017,
  title = {Device Placement Optimization with Reinforcement Learning},
  author = {Mirhoseini, Azalia and others},
  year = {2017},
  journal = {International Conference on Machine Learning (ICML)},
}

@article{mlir_framework_2021,
  title = {MLIR: A Compiler Infrastructure for the End of Moore's Law},
  author = {
    Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and
    Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko,
    Oleksandr
  },
  year = {2020},
  month = feb,
  journal = {arXiv preprint arXiv:2002.11054},
  url = {http://arxiv.org/abs/2002.11054v2},
  primaryclass = {cs.PL},
  archiveprefix = {arXiv},
}

@article{moreau2018relay,
  title = {Joining dessins together},
  author = {Jones, Gareth A.},
  year = {2018},
  month = oct,
  journal = {arXiv preprint arXiv:1810.03960},
  url = {http://arxiv.org/abs/1810.03960v1},
  primaryclass = {math.GR},
  archiveprefix = {arXiv},
}

@article{Narayanan2021,
  title = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
  author = {
    Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary,
    Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and
    Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei
  },
  year = {2021},
  month = apr,
  journal = {NeurIPS},
  url = {http://arxiv.org/abs/2104.04473v5},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
}

@article{norrie2021design,
  title = {The Design Process for Google's Training Chips: TPUv2 and TPUv3},
  author = {
    Norrie, Thomas and Patil, Nishant and Yoon, Doe Hyun and Kurian, George and Li, Sheng and
    Laudon, James and Young, Cliff and Jouppi, Norman and Patterson, David
  },
  year = {2021},
  month = mar,
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {41},
  number = {2},
  pages = {56--63},
  doi = {10.1109/mm.2021.3058217},
  issn = {0272-1732,1937-4143},
  url = {https://doi.org/10.1109/mm.2021.3058217},
  source = {Crossref},
  bdsk-url-1 = {https://doi.org/10.1109/MM.2021.3058217},
}

@article{nvidia_tensorRT_2021,
  title = {TensorRT: High-Performance Deep Learning Inference Library},
  author = {NVIDIA},
  year = {2021},
  journal = {NVIDIA Developer Blog},
  url = {https://developer.nvidia.com/tensorrt},
}

@article{nvidia2017gpu,
  title = {GPU-Accelerated Machine Learning and Deep Learning},
  author = {Corporation, NVIDIA},
  year = {2017},
  journal = {Technical Report},
}

@incollection{nvidia2020ampere,
  title = {GPU-based acceleration of SDN controllers},
  author = {Xuan Qi and Kantarci, Burak and Chen Liu},
  year = {2017},
  month = jun,
  journal = {Technical Report},
  booktitle = {Network as a Service for Next Generation Internet},
  publisher = {Institution of Engineering and Technology},
  pages = {339--356},
  doi = {10.1049/pbte073e\_ch14},
  isbn = {9781785611766,9781785611773},
  url = {https://doi.org/10.1049/pbte073e\_ch14},
  source = {Crossref},
  date = {2017-06-15},
}

@article{NVIDIA2020nvlink,
  title = {NVLink: Scalable High-Performance Interconnect},
  author = {Corporation, NVIDIA},
  year = {2020},
  journal = {NVIDIA Technical Report},
  url = {https://www.nvidia.com/en-us/data-center/nvlink/},
}

@manual{nvidia2021cudnn,
  title = {NVIDIA cuDNN: GPU Accelerated Deep Learning},
  author = {Corporation, NVIDIA},
  year = {2021},
  url = {https://developer.nvidia.com/cudnn},
}

@manual{oneDNN2021,
  title = {oneDNN: Intel's Deep Learning Neural Network Library},
  author = {Corporation, Intel},
  year = {2021},
  url = {https://github.com/oneapi-src/oneDNN},
}

@article{owens2008gpu,
  title = {GPU Computing},
  author = {Owens, J.D. and Houston, M. and Luebke, D. and Green, S. and Stone, J.E. and Phillips, J.C.},
  year = {2008},
  month = may,
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {96},
  number = {5},
  pages = {879--899},
  doi = {10.1109/jproc.2008.917757},
  issn = {0018-9219,1558-2256},
  url = {https://doi.org/10.1109/jproc.2008.917757},
  source = {Crossref},
}

@inproceedings{palmer_8087_1981,
  title = {The INTEL\textregistered{} 8087 numeric data processor},
  author = {Palmer, John F.},
  year = {1980},
  booktitle = {Proceedings of the May 19-22, 1980, national computer conference on - AFIPS '80},
  publisher = {ACM Press},
  pages = {887},
  doi = {10.1145/1500518.1500674},
  url = {https://doi.org/10.1145/1500518.1500674},
  source = {Crossref},
  date = {1980},
}

@article{paszke2019pytorch,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author = {Paszke, Adam and others},
  year = {2019},
  journal = {NeurIPS},
}

@book{patterson2021computer,
  title = {Computer Organization and Design: The Hardware/Software Interface},
  author = {Patterson, David A. and Hennessy, John L.},
  year = {2021},
  publisher = {Morgan Kaufmann},
  edition = {5th},
}

@article{Rajbhandari2020,
  title = {ZeRO: Memory Optimization Towards Training Trillion Parameter Models},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  year = {2020},
  journal = {
    Proceedings of the International Conference for High Performance Computing, Networking, Storage
    and Analysis (SC)
  },
  doi = {10.5555/3433701.3433721},
}

@article{Sergeev2018horovod,
  title = {Horovod: fast and easy distributed deep learning in TensorFlow},
  author = {Sergeev, Alexander and Balso, Mike Del},
  booktitle = {Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS)},
  pages = {1--10},
  url = {http://arxiv.org/abs/1802.05799v3},
  date = {2018-02-15},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
}

@article{Shallue2019measuring,
  title = {Measuring the Effects of Data Parallelism on Neural Network Training},
  author = {Shallue, Christopher J. and Lee, Jaehoon and others},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  pages = {1--49},
  url = {http://jmlr.org/papers/v20/18-789.html},
}

@article{Shang2018GenomicsAccel,
  title = {Accelerating Genomic Data Analysis with Domain-Specific Architectures},
  author = {Shang, J. and Wang, G. and Liu, Y.},
  year = {2018},
  journal = {IEEE Transactions on Computers},
  volume = {67},
  number = {7},
  pages = {965--978},
  doi = {10.1109/TC.2018.2799212},
}

@article{shazeer2018mesh,
  title = {Mesh-TensorFlow: Deep Learning for Supercomputers},
  author = {
    Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and
    Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young,
    Cliff and Sepassi, Ryan and Hechtman, Blake
  },
  year = {2018},
  month = nov,
  journal = {arXiv preprint arXiv:1811.02084},
  url = {http://arxiv.org/abs/1811.02084v1},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
}

@article{shoeybi_megatron_2020,
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {
    Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared
    and Catanzaro, Bryan
  },
  year = {2019},
  month = sep,
  journal = {arXiv preprint arXiv:1909.08053},
  url = {http://arxiv.org/abs/1909.08053v4},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  eprint = {1909.08053},
}

@article{Shoeybi2019,
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {
    Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared
    and Catanzaro, Bryan
  },
  year = {2019},
  month = sep,
  journal = {arXiv preprint arXiv:1909.08053},
  url = {http://arxiv.org/abs/1909.08053v4},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
}

@book{Smith1997,
  title = {The Scientist and Engineer's Guide to Digital Signal Processing},
  author = {Smith, Steven W.},
  year = {1997},
  publisher = {California Technical Publishing},
  url = {https://www.dspguide.com/},
}

@inproceedings{sodani2017knl,
  title = {Knights landing (KNL): 2nd Generation Intel\textregistered{} Xeon Phi processor},
  author = {Sodani, Avinash},
  year = {2015},
  month = aug,
  journal = {Hot Chips Symposium},
  booktitle = {2015 IEEE Hot Chips 27 Symposium (HCS)},
  publisher = {IEEE},
  pages = {1--24},
  doi = {10.1109/hotchips.2015.7477467},
  url = {https://doi.org/10.1109/hotchips.2015.7477467},
  source = {Crossref},
}

@article{stephens2017arm,
  title = {The ARM Scalable Vector Extension},
  author = {
    Stephens, Nigel and Biles, Stuart and Boettcher, Matthias and Eapen, Jacob and Eyole, Mbou and
    Gabrielli, Giacomo and Horsnell, Matt and Magklis, Grigorios and Martinez, Alejandro and
    Premillieu, Nathanael and Reid, Alastair and Rico, Alejandro and Walker, Paul
  },
  year = {2017},
  month = mar,
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {37},
  number = {2},
  pages = {26--39},
  doi = {10.1109/mm.2017.35},
  issn = {0272-1732},
  url = {https://doi.org/10.1109/mm.2017.35},
  source = {Crossref},
}

@article{sullivan2012overview,
  title = {Overview of the High Efficiency Video Coding (HEVC) Standard},
  author = {Sullivan, Gary J. and Ohm, Jens-Rainer and Han, Woo-Jin and Wiegand, Thomas},
  year = {2012},
  month = dec,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {22},
  number = {12},
  pages = {1649--1668},
  doi = {10.1109/tcsvt.2012.2221191},
  issn = {1051-8215,1558-2205},
  url = {https://doi.org/10.1109/tcsvt.2012.2221191},
  source = {Crossref},
}

@article{sze2017efficient,
  title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel},
  year = {2017},
  month = mar,
  journal = {Proceedings of the IEEE},
  publisher = {IEEE},
  volume = {105},
  number = {12},
  pages = {2295--2329},
  doi = {10.1109/jproc.2017.2761740},
  issn = {0018-9219,1558-2256},
  url = {http://arxiv.org/abs/1703.09039v2},
  copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  abstract = {
    Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI)
    applications including computer vision, speech recognition, and robotics. While DNNs deliver
    state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational
    complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy
    efficiency and throughput without sacrificing application accuracy or increasing hardware cost
    are critical to the wide deployment of DNNs in AI systems. This article aims to provide a
    comprehensive tutorial and survey about the recent advances towards the goal of enabling
    efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss
    various hardware platforms and architectures that support DNNs, and highlight key trends in
    reducing the computation cost of DNNs either solely via hardware design changes or via joint
    hardware design and DNN algorithm changes. It will also summarize various development resources
    that enable researchers and practitioners to quickly get started in this field, and highlight
    important benchmarking metrics and design considerations that should be used for evaluating the
    rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs,
    being proposed in academia and industry. The reader will take away the following concepts from
    this article: understand the key design considerations for DNNs; be able to evaluate different
    DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs
    between various hardware architectures and platforms; be able to evaluate the utility of
    various DNN design techniques for efficient processing; and understand recent implementation
    trends and opportunities.
  },
  eprint = {1703.09039},
  source = {Crossref},
}

@article{sze2020efficient,
  title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  year = {2017},
  month = dec,
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {105},
  number = {12},
  pages = {2295--2329},
  doi = {10.1109/jproc.2017.2761740},
  issn = {0018-9219,1558-2256},
  url = {https://doi.org/10.1109/jproc.2017.2761740},
  source = {Crossref},
}

@article{Taylor2017ASICMining,
  title = {The Evolution of Bitcoin Hardware},
  author = {Bedford Taylor, Michael},
  year = {2017},
  journal = {Computer},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  volume = {50},
  number = {9},
  pages = {58--66},
  doi = {10.1109/mc.2017.3571056},
  issn = {0018-9162},
  url = {https://doi.org/10.1109/mc.2017.3571056},
  source = {Crossref},
}

@article{tensorflow_xla_2020,
  title = {XLA: Optimizing Compiler for Machine Learning},
  author = {Brain, Google},
  year = {2020},
  journal = {TensorFlow Blog},
  url = {https://tensorflow.org/xla},
}

@manual{tensorflow2022,
  title = {TensorFlow Documentation},
  author = {Brain, Google},
  year = {2022},
  url = {https://www.tensorflow.org/},
}

@article{Tesla2021,
  title = {Tesla AI Day: D1 Dojo Chip},
  author = {Inc., Tesla},
  year = {2021},
  journal = {Tesla AI Day Presentation},
}

@article{wu_tensor_2019,
  title = {Tensor Cores: Understanding, Programming, and Performance Analysis},
  author = {Wu, Chengfu and Grot, Boris and Hardavellas, Nikos},
  year = {2019},
  journal = {IEEE Micro},
  publisher = {IEEE},
  volume = {39},
  number = {5},
  pages = {20--28},
  doi = {10.1109/MM.2019.2923951},
}

@article{xla2020,
  title = {
    Accelerated linear algebra compiler for computationally efficient numerical models: Success and
    potential area of improvement
  },
  author = {He, Xuzhen},
  year = {2023},
  month = feb,
  journal = {PLOS ONE},
  publisher = {Public Library of Science (PLoS)},
  volume = {18},
  number = {2},
  pages = {e0282265},
  doi = {10.1371/journal.pone.0282265},
  issn = {1932-6203},
  url = {https://doi.org/10.1371/journal.pone.0282265},
  source = {Crossref},
  essn = {1932-6203},
}

@article{xla2021,
  title = {
    Accelerated linear algebra compiler for computationally efficient numerical models: Success and
    potential area of improvement
  },
  author = {He, Xuzhen},
  year = {2023},
  month = feb,
  journal = {PLOS ONE},
  publisher = {Public Library of Science (PLoS)},
  volume = {18},
  number = {2},
  pages = {e0282265},
  doi = {10.1371/journal.pone.0282265},
  issn = {1932-6203},
  url = {https://doi.org/10.1371/journal.pone.0282265},
  source = {Crossref},
  essn = {1932-6203},
}

@article{zhang2020optimizing,
  title = {Optimizing Memory Access for Deep Learning Workloads},
  author = {Zhang, Y. and Li, J. and Ouyang, H.},
  year = {2020},
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  booktitle = {Proceedings of the 2020 International Symposium on Microarchitecture (MICRO)},
  publisher = {IEEE},
  volume = {39},
  number = {11},
  pages = {2345--2358},
}

@article{Zheng2020,
  title = {Ansor: Generating High-Performance Tensor Programs for Deep Learning},
  author = {
    Zheng, Lianmin and Jia, Ziheng and Gao, Yida and Lin, Jiacheng and Han, Song and Geng, Xuehai
    and Zhao, Eric and Wu, Tianqi
  },
  year = {2020},
  journal = {USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  pages = {863--879},
}
