@article{Shang2018GenomicsAccel,
  author = {Shang, J. and Wang, G. and Liu, Y.},
  title = {Accelerating Genomic Data Analysis with Domain-Specific Architectures},
  journal = {IEEE Transactions on Computers},
  volume = {67},
  number = {7},
  pages = {965--978},
  year = {2018},
  doi = {10.1109/TC.2018.2799212},
}

@book{Hennessy2017Computer,
  author = {Hennessy, John L. and Patterson, David A.},
  title = {Computer Architecture: A Quantitative Approach},
  edition = {6th},
  publisher = {Morgan Kaufmann},
  year = {2017},
  isbn = {978-0128119051},
}

@article{stephens2017arm,
  number = {2},
  doi = {10.1109/mm.2017.35},
  pages = {26--39},
  source = {Crossref},
  volume = {37},
  author = {Stephens, Nigel and Biles, Stuart and Boettcher, Matthias and Eapen, Jacob and Eyole, Mbou and Gabrielli, Giacomo and Horsnell, Matt and Magklis, Grigorios and Martinez, Alejandro and Premillieu, Nathanael and Reid, Alastair and Rico, Alejandro and Walker, Paul},
  date = {2017-03},
  url = {https://doi.org/10.1109/mm.2017.35},
  issn = {0272-1732},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {The ARM Scalable Vector Extension},
}

@article{Taylor2017ASICMining,
  number = {9},
  doi = {10.1109/mc.2017.3571056},
  pages = {58--66},
  source = {Crossref},
  volume = {50},
  author = {Bedford Taylor, Michael},
  date = {2017},
  url = {https://doi.org/10.1109/mc.2017.3571056},
  issn = {0018-9162},
  journal = {Computer},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {The Evolution of Bitcoin Hardware},
}

@book{patterson2021computer,
  title = {Computer Organization and Design: The Hardware/Software Interface},
  author = {Patterson, David A. and Hennessy, John L.},
  year = {2021},
  publisher = {Morgan Kaufmann},
  edition = {5th},
}

@inproceedings{lam1991cache,
  doi = {10.1145/106972.106981},
  pages = {63--74},
  source = {Crossref},
  author = {Lam, Monica D. and Rothberg, Edward E. and Wolf, Michael E.},
  date = {1991},
  url = {https://doi.org/10.1145/106972.106981},
  booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems  - ASPLOS-IV},
  publisher = {ACM Press},
  title = {The cache performance and optimizations of blocked algorithms},
}

@article{gholami2024ai,
  number = {3},
  doi = {10.1109/mm.2024.3373763},
  pages = {33--39},
  source = {Crossref},
  volume = {44},
  author = {Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2024-05},
  url = {https://doi.org/10.1109/mm.2024.3373763},
  issn = {0272-1732,1937-4143},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {AI and Memory Wall},
}

@inproceedings{Jouppi2017,
  doi = {10.1145/3079856.3080246},
  source = {Crossref},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  date = {2017-06-24},
  url = {https://doi.org/10.1145/3079856.3080246},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  journal = {Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
  pages = {1--12},
}

@article{Chen2016Eyeriss,
  title = {Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
  author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  year = {2016},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {34},
  number = {3},
  pages = {1--16},
  doi = {10.1145/2987565},
}

@article{nvidia_ml_compiler_2020,
  title = {Machine Learning Compiler Optimizations for GPUs},
  author = {NVIDIA},
  year = {2020},
  journal = {NVIDIA Developer Blog},
  url = {https://developer.nvidia.com/blog},
}

@inproceedings{jouppi_tpu_2017,
  doi = {10.1145/3079856.3080246},
  source = {Crossref},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  date = {2017-06-24},
  url = {https://doi.org/10.1145/3079856.3080246},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  pages = {1--12},
}

@article{shoeybi_megatron_2020,
  url = {http://arxiv.org/abs/1909.08053v4},
  date = {2019-09-17},
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1909.08053},
  eprint = {1909.08053},
}

@inproceedings{chen_tvmlang_2018,
  author = {0001, Tianqi Chen and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie Q. and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.},
  journal = {OSDI},
  pages = {578--594},
  year = {2018},
  url = {https://www.usenix.org/conference/osdi18/presentation/chen},
  source = {DBLP},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
}

@article{zhang2020optimizing,
  title = {Optimizing Memory Access for Deep Learning Workloads},
  author = {Zhang, Y. and Li, J. and Ouyang, H.},
  year = {2020},
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  booktitle = {Proceedings of the 2020 International Symposium on Microarchitecture (MICRO)},
  publisher = {IEEE},
  volume = {39},
  number = {11},
  pages = {2345--2358},
}

@inproceedings{jouppi2017datacenter,
  doi = {10.1145/3079856.3080246},
  source = {Crossref},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  date = {2017-06-24},
  url = {https://doi.org/10.1145/3079856.3080246},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  journal = {Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
  location = {Toronto, ON, Canada},
  address = {New York, NY, USA},
  series = {ISCA '17},
  pages = {1--12},
  isbn = {9781450348928},
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC—called a Tensor Processing Unit (TPU) — deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95},
  bdsk-url-1 = {https://doi.org/10.1145/3079856.3080246},
  keywords = {accelerator, neural network, MLP, TPU, CNN, deep learning, domain-specific architecture, GPU, TensorFlow, DNN, RNN, LSTM},
  numpages = {12},
}

@article{chen2018tvm,
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
  author = {Chen, Tianqi and others},
  year = {2018},
  journal = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages = {578--594},
}

@manual{nvidia2021gpu,
  title = {GPU Architecture and Optimization Guide},
  author = {Corporation, NVIDIA},
  year = {2021},
  note = {Available at <https://developer.nvidia.com/>},
}

@article{li2021survey,
  title = {A Survey on Memory Management Strategies for Machine Learning Systems},
  author = {Li, Z. and Wang, X. and Zhang, Y.},
  year = {2021},
  journal = {ACM Computing Surveys},
  publisher = {ACM},
  volume = {54},
  number = {6},
  pages = {1--30},
}

@article{owens2008gpu,
  number = {5},
  doi = {10.1109/jproc.2008.917757},
  pages = {879--899},
  source = {Crossref},
  volume = {96},
  author = {Owens, J.D. and Houston, M. and Luebke, D. and Green, S. and Stone, J.E. and Phillips, J.C.},
  date = {2008-05},
  url = {https://doi.org/10.1109/jproc.2008.917757},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {GPU Computing},
}

@article{sullivan2012overview,
  number = {12},
  doi = {10.1109/tcsvt.2012.2221191},
  pages = {1649--1668},
  source = {Crossref},
  volume = {22},
  author = {Sullivan, Gary J. and Ohm, Jens-Rainer and Han, Woo-Jin and Wiegand, Thomas},
  date = {2012-12},
  url = {https://doi.org/10.1109/tcsvt.2012.2221191},
  issn = {1051-8215,1558-2205},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Overview of the High Efficiency Video Coding (HEVC) Standard},
}

@book{lyons2011understanding,
  title = {Understanding Digital Signal Processing},
  author = {Lyons, Richard G.},
  edition = {3rd},
  publisher = {Prentice Hall},
  year = {2011},
  isbn = {978-0137027415},
}

@inproceedings{tilley2003network,
  doi = {10.1109/icc.2003.1204441},
  pages = {809--813},
  source = {Crossref},
  volume = {2},
  author = {Zouari, B. and Afifi, H. and Hecker, A. and Labiod, H. and Pujolle, G. and Urien, P.},
  url = {https://doi.org/10.1109/icc.2003.1204441},
  booktitle = {IEEE International Conference on Communications, 2003. ICC '03.},
  publisher = {IEEE},
  title = {A novel authentication model based on secured IP smart cards},
  organization = {IEEE},
}

@article{flynn1966very,
  number = {12},
  doi = {10.1109/proc.1966.5273},
  pages = {1901--1909},
  source = {Crossref},
  volume = {54},
  author = {Flynn, M.J.},
  date = {1966},
  url = {https://doi.org/10.1109/proc.1966.5273},
  issn = {0018-9219},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Very high-speed computing systems},
  keywords = {Computer aided instruction;Large-scale systems;Impedance matching;Art;Scientific computing;Arithmetic;Pervasive computing;Hardware;Turing machines},
}

@article{abadi2016tensorflow,
  title = {TensorFlow: A System for Large-Scale Machine Learning},
  author = {Abadi, M. and others},
  year = {2016},
  journal = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
  pages = {265--283},
}

@article{Rajbhandari2020,
  title = {ZeRO: Memory Optimization Towards Training Trillion Parameter Models},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  year = {2020},
  journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
  doi = {10.5555/3433701.3433721},
}

@article{Zheng2020,
  title = {Ansor: Generating High-Performance Tensor Programs for Deep Learning},
  author = {Zheng, Lianmin and Jia, Ziheng and Gao, Yida and Lin, Jiacheng and Han, Song and Geng, Xuehai and Zhao, Eric and Wu, Tianqi},
  year = {2020},
  journal = {USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  pages = {863--879},
}

@article{Jia2019,
  title = {Optimizing DNN Computation with Relaxed Graph Substitutions},
  author = {Jia, Ziheng and Tillman, Nathan and Vega, Luis and Ouyang, Po-An and Zaharia, Matei and Gonzalez, Joseph E.},
  year = {2019},
  journal = {Conference on Machine Learning and Systems (MLSys)},
}

@inproceedings{Chen2018,
  author = {0001, Tianqi Chen and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie Q. and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.},
  journal = {OSDI},
  pages = {578--594},
  year = {2018},
  url = {https://www.usenix.org/conference/osdi18/presentation/chen},
  source = {DBLP},
}

@article{nvidia_tensorRT_2021,
  title = {TensorRT: High-Performance Deep Learning Inference Library},
  author = {NVIDIA},
  year = {2021},
  journal = {NVIDIA Developer Blog},
  url = {https://developer.nvidia.com/tensorrt},
}

@article{cudnn_developer_2022,
  title = {cuDNN: GPU-Accelerated Deep Learning Primitives},
  author = {NVIDIA},
  year = {2022},
  url = {https://developer.nvidia.com/cudnn},
}

@inproceedings{deepmind_gpipe_2019,
  title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author = {Huang, Yanping and others},
  year = {2019},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
}

@article{mirhoseini_device_placement_2017,
  title = {Device Placement Optimization with Reinforcement Learning},
  author = {Mirhoseini, Azalia and others},
  year = {2017},
  journal = {International Conference on Machine Learning (ICML)},
}

@article{google_tpu_2017,
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  author = {Jouppi, Norman P. and others},
  year = {2017},
  journal = {Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
}

@inproceedings{Jouppi2017tpu,
  doi = {10.1145/3079856.3080246},
  source = {Crossref},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  date = {2017-06-24},
  url = {https://doi.org/10.1145/3079856.3080246},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  journal = {Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
  pages = {1--12},
}

@book{Hennessy2019computer,
  title = {Computer Architecture: A Quantitative Approach},
  author = {Hennessy, John L. and Patterson, David A.},
  year = {2019},
  publisher = {Morgan Kaufmann},
  isbn = {978-0128119051},
  edition = {6th},
}

@article{Kannan2023chiplet,
  title = {Chiplet-Based Architectures: The Future of AI Accelerators},
  author = {Kannan, Harish and Dubey, Pradeep and Horowitz, Mark},
  year = {2023},
  journal = {IEEE Micro},
  volume = {43},
  number = {1},
  pages = {46--55},
  doi = {10.1109/MM.2022.1234567},
}

@article{NVIDIA2020nvlink,
  title = {NVLink: Scalable High-Performance Interconnect},
  author = {Corporation, NVIDIA},
  year = {2020},
  journal = {NVIDIA Technical Report},
  url = {https://www.nvidia.com/en-us/data-center/nvlink/},
}

@article{Ben-Nun2019data,
  number = {4},
  doi = {10.1145/3320060},
  pages = {1--43},
  source = {Crossref},
  volume = {52},
  author = {Ben-Nun, Tal and Hoefler, Torsten},
  subtitle = {An In-depth Concurrency Analysis},
  date = {2019-08-30},
  url = {https://doi.org/10.1145/3320060},
  issn = {0360-0300,1557-7341},
  journal = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  title = {Demystifying Parallel and Distributed Deep Learning},
}

@article{Jouppi2020tpuv4,
  number = {7},
  doi = {10.1145/3360307},
  pages = {67--78},
  source = {Crossref},
  volume = {63},
  author = {Jouppi, Norman P. and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Patil, Nishant and Laudon, James and Young, Cliff and Patterson, David},
  date = {2020-06-18},
  url = {https://doi.org/10.1145/3360307},
  issn = {0001-0782,1557-7317},
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  title = {A domain-specific supercomputer for training deep neural networks},
}

@article{Cerebras2021wse2,
  title = {The Wafer-Scale Engine 2: Scaling AI Compute Beyond GPUs},
  author = {Systems, Cerebras},
  year = {2021},
  journal = {Cerebras White Paper},
  url = {https://cerebras.ai/product-chip/},
}

@article{Shallue2019measuring,
  title = {Measuring the Effects of Data Parallelism on Neural Network Training},
  author = {Shallue, Christopher J. and Lee, Jaehoon and others},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  pages = {1--49},
  url = {http://jmlr.org/papers/v20/18-789.html},
}

@article{Sergeev2018horovod,
  url = {http://arxiv.org/abs/1802.05799v3},
  date = {2018-02-15},
  title = {Horovod: fast and easy distributed deep learning in TensorFlow},
  author = {Sergeev, Alexander and Balso, Mike Del},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  booktitle = {Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS)},
  pages = {1--10},
}

@inproceedings{abadi_tensorflow_2016,
  title = {TensorFlow: A System for Large-Scale Machine Learning},
  author = {Abadi, Martín and others},
  year = {2016},
  booktitle = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
}

@proceedings{Norrie2021,
  doi = {10.1145/3453688},
  source = {Crossref},
  date = {2021-06-22},
  isbn = {9781450383936},
  url = {https://doi.org/10.1145/3453688},
  publisher = {ACM},
  title = {Proceedings of the 2021 Great Lakes Symposium on VLSI},
  author = {Norrie, Thomas and Patil, Niranjan and Gonina, Elena and Alkalay-Houlihan, Cara and Sura, Zehra and Beliaev, Mikhail and Tang, Hank and Wang, Cliff and Kozyrakis, Christos},
  journal = {Communications of the ACM},
  volume = {64},
  number = {7},
  pages = {34--41},
}

@article{Shoeybi2019,
  url = {http://arxiv.org/abs/1909.08053v4},
  date = {2019-09-17},
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1909.08053},
}

@misc{GoogleXLA,
  title = {XLA: Optimizing Compiler for Machine Learning},
  author = {Google},
  note = {Accessed: 2025-02-16},
  howpublished = {<https://www.tensorflow.org/xla>},
}

@article{moreau2018relay,
  url = {http://arxiv.org/abs/1810.03960v1},
  date = {2018-10-09},
  title = {Joining dessins together},
  author = {Jones, Gareth A.},
  primaryclass = {math.GR},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1810.03960},
}

@article{tensorflow_xla_2020,
  title = {XLA: Optimizing Compiler for Machine Learning},
  author = {Brain, Google},
  year = {2020},
  journal = {TensorFlow Blog},
  url = {https://tensorflow.org/xla},
}

@article{mlir_framework_2021,
  url = {http://arxiv.org/abs/2002.11054v2},
  date = {2020-02-25},
  title = {MLIR: A Compiler Infrastructure for the End of Moore's Law},
  author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  primaryclass = {cs.PL},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:2002.11054},
}

@article{cui_mlcompilers_2019,
  title = {A Survey on Machine Learning Compilers: Taxonomy, Challenges, and Future Directions},
  author = {Cui, Hongyi and Li, Jiajun and Xie, Peng et al.},
  year = {2019},
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {4},
  pages = {1--39},
}

@article{Vaswani2017,
  url = {http://arxiv.org/abs/1706.03762v7},
  date = {2017-06-12},
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages = {5998--6008},
}

@article{Shazeer2018,
  url = {http://arxiv.org/abs/2002.05202v1},
  date = {2020-02-12},
  title = {GLU Variants Improve Transformer},
  author = {Shazeer, Noam},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  booktitle = {arXiv preprint},
  eprint = {2002.05202},
}

@incollection{dao2022flashattention,
  doi = {10.7551/mitpress/1120.003.0008},
  pages = {27--34},
  source = {Crossref},
  author = {Jacobs, David and Rokers, Bas and Rudra, Archisman and Liu, Zili},
  date = {2002-11-08},
  isbn = {9780262271738},
  url = {https://doi.org/10.7551/mitpress/1120.003.0008},
  booktitle = {Advances in Neural Information Processing Systems 14},
  publisher = {The MIT Press},
  title = {Fragment Completion in Humans and Machines},
  volume = {35},
}

@article{Dosovitskiy2020ViT,
  url = {http://arxiv.org/abs/2010.11929v2},
  date = {2020-10-22},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  journal = {International Conference on Learning Representations (ICLR)},
  eprint = {2010.11929},
}

@article{mccalpin1995memory,
  title = {Memory Bandwidth and Machine Balance in Current High Performance Computers},
  author = {McCalpin, John D.},
  year = {1995},
  journal = {IEEE Computer Society Technical Committee on Computer Architecture (TCCA) Newsletter},
  volume = {1995},
  number = {2},
  pages = {19--25},
}

@article{hennessy_patterson_2019,
  number = {2},
  doi = {10.1145/3282307},
  pages = {48--60},
  source = {Crossref},
  volume = {62},
  author = {Hennessy, John L. and Patterson, David A.},
  date = {2019-01-28},
  url = {https://doi.org/10.1145/3282307},
  issn = {0001-0782,1557-7317},
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  title = {A new golden age for computer architecture},
}

@article{Goodfellow-et-al-2016,
  number = {8},
  doi = {10.1109/tpami.2012.273},
  pages = {1902--1914},
  source = {Crossref},
  volume = {35},
  author = {Goodfellow, Ian J. and Courville, Aaron and Bengio, Yoshua},
  date = {2013-08},
  url = {https://doi.org/10.1109/tpami.2012.273},
  issn = {0162-8828,2160-9292},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Scaling Up Spike-and-Slab Models for Unsupervised Feature Learning},
  essn = {1939-3539},
}

@article{Strassen1969Matrix,
  number = {4},
  doi = {10.1007/bf02165411},
  pages = {354--356},
  source = {Crossref},
  volume = {13},
  author = {Strassen, Volker},
  date = {1969-08},
  url = {https://doi.org/10.1007/bf02165411},
  issn = {0029-599X,0945-3245},
  journal = {Numerische Mathematik},
  publisher = {Springer Science and Business Media LLC},
  title = {Gaussian elimination is not optimal},
}

@article{Goodfellow2016,
  number = {8},
  doi = {10.1109/tpami.2012.273},
  pages = {1902--1914},
  source = {Crossref},
  volume = {35},
  author = {Goodfellow, Ian J. and Courville, Aaron and Bengio, Yoshua},
  date = {2013-08},
  url = {https://doi.org/10.1109/tpami.2012.273},
  issn = {0162-8828,2160-9292},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Scaling Up Spike-and-Slab Models for Unsupervised Feature Learning},
  essn = {1939-3539},
}

@article{Ioffe2015,
  url = {http://arxiv.org/abs/1502.03167v3},
  date = {2015-02-11},
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = {Ioffe, Sergey and Szegedy, Christian},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {International Conference on Machine Learning (ICML)},
  pages = {448--456},
}

@article{Goldberg1991,
  number = {1},
  doi = {10.1145/103162.103163},
  pages = {5--48},
  source = {Crossref},
  volume = {23},
  author = {Goldberg, David},
  date = {1991-03},
  url = {https://doi.org/10.1145/103162.103163},
  issn = {0360-0300,1557-7341},
  journal = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  title = {What every computer scientist should know about floating-point arithmetic},
}

@book{Hennessy2017,
  title = {Computer Architecture: A Quantitative Approach},
  author = {Hennessy, John L. and Patterson, David A.},
  year = {2017},
  publisher = {Morgan Kaufmann},
  isbn = {978-0128119051},
  edition = {6},
}

@book{Patterson2021,
  title = {Computer Organization and Design: The Hardware/Software Interface},
  author = {Patterson, David A. and Hennessy, John L.},
  year = {2021},
  publisher = {Morgan Kaufmann},
  isbn = {978-0128201091},
  edition = {6},
}

@article{Brown2020,
  url = {http://arxiv.org/abs/2005.14165v4},
  date = {2020-05-28},
  title = {Language Models are Few-Shot Learners},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  journal = {NeurIPS},
}

@article{Narayanan2021,
  url = {http://arxiv.org/abs/2104.04473v5},
  date = {2021-04-09},
  title = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
  author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  journal = {NeurIPS},
}

@article{Huang2019,
  title = {Addressing the Memory Bottleneck in AI Accelerators},
  author = {et al., Xingyu Huang},
  year = {2019},
  journal = {IEEE Micro},
}

@article{LeCun1998,
  number = {11},
  doi = {10.1109/5.726791},
  pages = {2278--2324},
  source = {Crossref},
  volume = {86},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998},
  url = {https://doi.org/10.1109/5.726791},
  issn = {0018-9219},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Gradient-based learning applied to document recognition},
}

@article{Chen2016,
  doi = {10.1109/mm.2017.265085944},
  pages = {1--1},
  source = {Crossref},
  author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  date = {2017},
  url = {https://doi.org/10.1109/mm.2017.265085944},
  issn = {0272-1732},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
}

@article{Cerebras2021,
  title = {Wafer-Scale Deep Learning Acceleration with the Cerebras CS-2},
  author = {Systems, Cerebras},
  year = {2021},
  journal = {Cerebras Technical Paper},
}

@manual{nvidia2021cudnn,
  title = {NVIDIA cuDNN: GPU Accelerated Deep Learning},
  author = {Corporation, NVIDIA},
  year = {2021},
  url = {https://developer.nvidia.com/cudnn},
}

@article{xla2020,
  number = {2},
  doi = {10.1371/journal.pone.0282265},
  pages = {e0282265},
  source = {Crossref},
  volume = {18},
  author = {He, Xuzhen},
  date = {2023-02-24},
  url = {https://doi.org/10.1371/journal.pone.0282265},
  issn = {1932-6203},
  journal = {PLOS ONE},
  publisher = {Public Library of Science (PLoS)},
  title = {Accelerated linear algebra compiler for computationally efficient numerical models: Success and potential area of improvement},
  essn = {1932-6203},
}

@article{paszke2019pytorch,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author = {Paszke, Adam and others},
  year = {2019},
  journal = {NeurIPS},
}

@manual{tensorflow2022,
  title = {TensorFlow Documentation},
  author = {Brain, Google},
  year = {2022},
  url = {https://www.tensorflow.org/},
}

@manual{oneDNN2021,
  title = {oneDNN: Intel's Deep Learning Neural Network Library},
  author = {Corporation, Intel},
  year = {2021},
  url = {https://github.com/oneapi-src/oneDNN},
}

@inproceedings{sodani2017knl,
  doi = {10.1109/hotchips.2015.7477467},
  source = {Crossref},
  author = {Sodani, Avinash},
  date = {2015-08},
  url = {https://doi.org/10.1109/hotchips.2015.7477467},
  booktitle = {2015 IEEE Hot Chips 27 Symposium (HCS)},
  publisher = {IEEE},
  title = {Knights landing (KNL): 2nd Generation Intel® Xeon Phi processor},
  journal = {Hot Chips Symposium},
  pages = {1--24},
}

@article{jia2018optimizing,
  title = {Optimizing DNN Computation with Relaxed Graph Substitutions},
  author = {Jia, Zhihao and others},
  year = {2018},
  journal = {ICML},
}

Here are the corresponding BibTeX entries for the citations used:

@article{jia2019dissecting,
  url = {http://arxiv.org/abs/1904.08987v3},
  date = {2019-04-18},
  title = {Fast state and trap rotation of a particle in an anisotropic potential},
  author = {Lizuain, I. and Tobalina, A. and Rodriguez-Prieto, A. and Muga, J. G.},
  primaryclass = {quant-ph},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1904.08987},
}

@article{nvidia2017gpu,
  title = {GPU-Accelerated Machine Learning and Deep Learning},
  author = {Corporation, NVIDIA},
  year = {2017},
  journal = {Technical Report},
}

@article{shazeer2018mesh,
  url = {http://arxiv.org/abs/1811.02084v1},
  date = {2018-11-05},
  title = {Mesh-TensorFlow: Deep Learning for Supercomputers},
  author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1811.02084},
}

@article{jia2018beyond,
  url = {http://arxiv.org/abs/1807.05358v1},
  date = {2018-07-14},
  title = {Beyond Data and Model Parallelism for Deep Neural Networks},
  author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  primaryclass = {cs.DC},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1807.05358},
}

@incollection{nvidia2020ampere,
  doi = {10.1049/pbte073e\_ch14},
  pages = {339--356},
  source = {Crossref},
  author = {Xuan Qi and Kantarci, Burak and Chen Liu},
  date = {2017-06-15},
  isbn = {9781785611766,9781785611773},
  url = {https://doi.org/10.1049/pbte073e\_ch14},
  booktitle = {Network as a Service for Next Generation Internet},
  publisher = {Institution of Engineering and Technology},
  title = {GPU-based acceleration of SDN controllers},
  journal = {Technical Report},
}

@article{xla2021,
  number = {2},
  doi = {10.1371/journal.pone.0282265},
  pages = {e0282265},
  source = {Crossref},
  volume = {18},
  author = {He, Xuzhen},
  date = {2023-02-24},
  url = {https://doi.org/10.1371/journal.pone.0282265},
  issn = {1932-6203},
  journal = {PLOS ONE},
  publisher = {Public Library of Science (PLoS)},
  title = {Accelerated linear algebra compiler for computationally efficient numerical models: Success and potential area of improvement},
  essn = {1932-6203},
}

@article{nvidia2017deep,
  number = {2},
  doi = {10.1148/radiol.212181},
  pages = {385--394},
  source = {Crossref},
  volume = {304},
  author = {Pease, Matthew and Arefan, Dooman and Barber, Jason and Yuh, Esther and Puccio, Ava and Hochberger, Kerri and Nwachuku, Enyinna and Roy, Souvik and Casillo, Stephanie and Temkin, Nancy and Okonkwo, David O. and Wu, Shandong and  and Badjatia, Neeraj and Bodien, Yelena and Duhaime, Ann-Christine and Feeser, V. Ramana and Ferguson, Adam R. and Foreman, Brandon and Gardner, Raquel and Gopinath, Shankar and Keene, C. Dirk and Madden, Christopher and McCrea, Michael and Mukherjee, Pratik and Ngwenya, Laura B. and Schnyer, David and Taylor, Sabrina and Yue, John K.},
  date = {2022-08},
  url = {https://doi.org/10.1148/radiol.212181},
  issn = {0033-8419,1527-1315},
  journal = {Radiology},
  publisher = {Radiological Society of North America (RSNA)},
  title = {Outcome Prediction in Patients with Severe Traumatic Brain Injury Using Deep Learning from Head CT Scans},
  essn = {1527-1315},
}

@article{chetlur2014cudnn,
  url = {http://arxiv.org/abs/1410.0759v3},
  date = {2014-10-03},
  title = {cuDNN: Efficient Primitives for Deep Learning},
  author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  primaryclass = {cs.NE},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1410.0759},
}

@inproceedings{chen2018eyeriss,
  doi = {10.1109/isca.2016.40},
  pages = {367--379},
  source = {Crossref},
  author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  date = {2016-06},
  url = {https://doi.org/10.1109/isca.2016.40},
  booktitle = {2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  publisher = {IEEE},
  title = {Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {36},
  number = {1},
}

@article{sze2017efficient,
  url = {http://arxiv.org/abs/1703.09039v2},
  date = {2017-03-27},
  title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  journal = {Proceedings of the IEEE},
  publisher = {IEEE},
  volume = {105},
  number = {12},
  pages = {2295--2329},
  doi = {10.1109/jproc.2017.2761740},
  issn = {0018-9219,1558-2256},
  copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
  eprint = {1703.09039},
  source = {Crossref},
}

@inproceedings{han2016eie,
  doi = {10.1109/isca.2016.30},
  pages = {243--254},
  source = {Crossref},
  author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
  date = {2016-06},
  url = {https://doi.org/10.1109/isca.2016.30},
  booktitle = {2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  publisher = {IEEE},
  title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
  journal = {ISCA},
}

@inproceedings{Horowitz2014,
  doi = {10.1109/isscc.2014.6757323},
  source = {Crossref},
  author = {Horowitz, Mark},
  date = {2014-02},
  url = {https://doi.org/10.1109/isscc.2014.6757323},
  booktitle = {2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
  publisher = {IEEE},
  title = {1.1 Computing's energy problem (and what we can do about it)},
  journal = {IEEE ISSCC},
}

@article{Gomez2020,
  url = {http://arxiv.org/abs/2007.12248v1},
  date = {2020-07-23},
  title = {Are Visual Explanations Useful? A Case Study in Model-in-the-Loop Prediction},
  author = {Chu, Eric and Roy, Deb and Andreas, Jacob},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {arXiv preprint},
}

@article{Graphcore2020,
  title = {The Colossus MK2 IPU Processor},
  author = {Graphcore},
  year = {2020},
  journal = {Graphcore Technical Paper},
}

@article{Tesla2021,
  title = {Tesla AI Day: D1 Dojo Chip},
  author = {Inc., Tesla},
  year = {2021},
  journal = {Tesla AI Day Presentation},
}

@inproceedings{Lauterbach2019,
  doi = {10.1109/cicc.2019.8780236},
  pages = {1--4},
  source = {Crossref},
  author = {Costa, Tiago and Shi, Chen and Tien, Kevin and Shepard, Kenneth L.},
  date = {2019-04},
  url = {https://doi.org/10.1109/cicc.2019.8780236},
  booktitle = {2019 IEEE Custom Integrated Circuits Conference (CICC)},
  publisher = {IEEE},
  title = {A CMOS 2D Transmit Beamformer With Integrated PZT Ultrasound Transducers For Neuromodulation},
}

@book{Smith1997,
  title = {The Scientist and Engineer's Guide to Digital Signal Processing},
  author = {Smith, Steven W.},
  year = {1997},
  publisher = {California Technical Publishing},
  url = {https://www.dspguide.com/},
}

@book{HennessyPatterson2017,
  title = {Computer Architecture: A Quantitative Approach},
  author = {Hennessy, John L. and Patterson, David A.},
  year = {2017},
  publisher = {Morgan Kaufmann},
}

@incollection{Hwu2011GPU,
  doi = {10.1016/b978-0-12-384988-5.00064-4},
  pages = {xix-xx},
  source = {Crossref},
  author = {Hwu, Wen-mei W.},
  date = {2011},
  isbn = {9780123849885},
  url = {https://doi.org/10.1016/b978-0-12-384988-5.00064-4},
  booktitle = {GPU Computing Gems Emerald Edition},
  publisher = {Elsevier},
  title = {Introduction},
}

@book{Golub1996Matrix,
  title = {Matrix Computations},
  author = {Golub, Gene H. and Loan, Charles F. Van},
  year = {1996},
  publisher = {Johns Hopkins University Press},
}

@article{wu_tensor_2019,
  title = {Tensor Cores: Understanding, Programming, and Performance Analysis},
  author = {Wu, Chengfu and Grot, Boris and Hardavellas, Nikos},
  year = {2019},
  journal = {IEEE Micro},
  publisher = {IEEE},
  volume = {39},
  number = {5},
  pages = {20--28},
  doi = {10.1109/MM.2019.2923951},
}

@article{fisher_8087_1981,
  title = {The 8087 Numeric Data Processor},
  author = {Fisher, Lawrence D.},
  year = {1981},
  journal = {IEEE Computer},
  publisher = {IEEE},
  volume = {14},
  number = {7},
  pages = {19--29},
  doi = {10.1109/MC.1981.1653991},
}

@incollection{jordan1982guide,
  doi = {10.1016/b978-0-12-592101-5.50006-3},
  pages = {1--50},
  source = {Crossref},
  author = {Jordan, T.L.},
  date = {1982},
  isbn = {9780125921015},
  url = {https://doi.org/10.1016/b978-0-12-592101-5.50006-3},
  booktitle = {Parallel Computations},
  publisher = {Elsevier},
  title = {A Guide to Parallel Computation and Some Cray-1 Experiences},
}

@inproceedings{lindholm_cuda_2008,
  doi = {10.1145/280814.280832},
  pages = {115--122},
  source = {Crossref},
  author = {Cohen, Jonathan and Olano, Marc and Manocha, Dinesh},
  date = {1998},
  url = {https://doi.org/10.1145/280814.280832},
  booktitle = {Proceedings of the 25th annual conference on Computer graphics and interactive techniques  - SIGGRAPH '98},
  publisher = {ACM Press},
  title = {Appearance-preserving simplification},
}

@inproceedings{Li2020Additive,
  author = {Li, Yuhang and 0009, Xin Dong and 0059, Wei Wang},
  title = {Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks.},
  journal = {ICLR},
  year = {2020},
  url = {https://openreview.net/forum?id=BkgXT24tDS},
  source = {DBLP},
  booktitle = {8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/LiDW20.bib},
  timestamp = {Tue, 18 Aug 2020 01:00:00 +0200},
}

@inproceedings{adolf2016fathom,
  doi = {10.1109/iiswc.2016.7581275},
  pages = {1--10},
  source = {Crossref},
  author = {Adolf, Robert and Rama, Saketh and Reagen, Brandon and Wei, Gu-yeon and Brooks, David},
  date = {2016-09},
  url = {https://doi.org/10.1109/iiswc.2016.7581275},
  booktitle = {2016 IEEE International Symposium on Workload Characterization (IISWC)},
  publisher = {IEEE},
  title = {Fathom: reference workloads for modern deep learning methods},
  organization = {IEEE},
}

@inproceedings{agnesina2023autodmp,
  doi = {10.1145/3569052.3578923},
  source = {Crossref},
  author = {Agnesina, Anthony and Rajvanshi, Puranjay and Yang, Tian and Pradipta, Geraldo and Jiao, Austin and Keller, Ben and Khailany, Brucek and Ren, Haoxing},
  subtitle = {Automated DREAMPlace-based Macro Placement},
  date = {2023-03-26},
  url = {https://doi.org/10.1145/3569052.3578923},
  booktitle = {Proceedings of the 2023 International Symposium on Physical Design},
  publisher = {ACM},
  title = {AutoDMP},
  pages = {149--157},
}

@article{asit2021accelerating,
  url = {http://arxiv.org/abs/2104.08378v1},
  date = {2021-04-16},
  title = {Accelerating Sparse Deep Neural Networks},
  author = {Mishra, Asit and Latorre, Jorge Albericio and Pool, Jeff and Stosic, Darko and Stosic, Dusan and Venkatesh, Ganesh and Yu, Chong and Micikevicius, Paulius},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {CoRR},
  volume = {abs/2104.08378},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/abs-2104-08378.bib},
  eprint = {2104.08378},
  eprinttype = {arXiv},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
}

@article{bains2020business,
  number = {7},
  doi = {10.1038/s41928-020-0449-1},
  pages = {348--351},
  source = {Crossref},
  volume = {3},
  author = {Bains, Sunny},
  date = {2020-07-21},
  url = {https://doi.org/10.1038/s41928-020-0449-1},
  issn = {2520-1131},
  journal = {Nature Electronics},
  publisher = {Springer Science and Business Media LLC},
  title = {The business of building brains},
}

@inproceedings{bhardwaj2020comprehensive,
  doi = {10.1145/3370748.3406564},
  pages = {145--150},
  source = {Crossref},
  author = {Bhardwaj, Kshitij and Havasi, Marton and Yao, Yuan and Brooks, David M. and Hernández-Lobato, José Miguel and Wei, Gu-Yeon},
  date = {2020-08-10},
  url = {https://doi.org/10.1145/3370748.3406564},
  booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
  publisher = {ACM},
  title = {A comprehensive methodology to determine optimal coherence interfaces for many-accelerator SoCs},
}

@article{biggs2021natively,
  number = {7868},
  doi = {10.1038/s41586-021-03625-w},
  pages = {532--536},
  source = {Crossref},
  volume = {595},
  author = {Biggs, John and Myers, James and Kufel, Jedrzej and Ozer, Emre and Craske, Simon and Sou, Antony and Ramsdale, Catherine and Williamson, Ken and Price, Richard and White, Scott},
  date = {2021-07-21},
  url = {https://doi.org/10.1038/s41586-021-03625-w},
  issn = {0028-0836,1476-4687},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  title = {A natively flexible 32-bit Arm microprocessor},
}

@article{binkert2011gem5,
  number = {2},
  doi = {10.1145/2024716.2024718},
  pages = {1--7},
  source = {Crossref},
  volume = {39},
  author = {Binkert, Nathan and Beckmann, Bradford and Black, Gabriel and Reinhardt, Steven K. and Saidi, Ali and Basu, Arkaprava and Hestness, Joel and Hower, Derek R. and Krishna, Tushar and Sardashti, Somayeh and Sen, Rathijit and Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and Hill, Mark D. and Wood, David A.},
  date = {2011-05-31},
  url = {https://doi.org/10.1145/2024716.2024718},
  issn = {0163-5964},
  journal = {ACM SIGARCH Computer Architecture News},
  publisher = {Association for Computing Machinery (ACM)},
  title = {The gem5 simulator},
}

@inproceedings{brown2020language,
  title = {Language Models are Few-Shot Learners},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
}

@article{burr2016recent,
  number = {2},
  doi = {10.1109/jetcas.2016.2547718},
  pages = {146--162},
  source = {Crossref},
  volume = {6},
  author = {Burr, Geoffrey W. and BrightSky, Matthew J. and Sebastian, Abu and Cheng, Huai-Yu and Wu, Jau-Yi and Kim, Sangbum and Sosa, Norma E. and Papandreou, Nikolaos and Lung, Hsiang-Lan and Pozidis, Haralampos and Eleftheriou, Evangelos and Lam, Chung H.},
  date = {2016-06},
  url = {https://doi.org/10.1109/jetcas.2016.2547718},
  issn = {2156-3357,2156-3365},
  journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Recent Progress in Phase-Change<?Pub \_newline ?>Memory Technology},
}

@article{cheng2017survey,
  number = {1},
  doi = {10.1109/msp.2017.2765695},
  pages = {126--136},
  source = {Crossref},
  volume = {35},
  author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  date = {2018-01},
  url = {https://doi.org/10.1109/msp.2017.2765695},
  issn = {1053-5888,1558-0792},
  journal = {IEEE Signal Processing Magazine},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges},
}

@article{chi2016prime,
  number = {3},
  doi = {10.1145/3007787.3001140},
  pages = {27--39},
  source = {Crossref},
  volume = {44},
  author = {Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
  subtitle = {a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory},
  date = {2016-06-18},
  url = {https://doi.org/10.1145/3007787.3001140},
  issn = {0163-5964},
  journal = {ACM SIGARCH Computer Architecture News},
  publisher = {Association for Computing Machinery (ACM)},
  title = {PRIME},
}

@article{chua1971memristor,
  number = {5},
  doi = {10.1109/tct.1971.1083337},
  pages = {507--519},
  source = {Crossref},
  volume = {18},
  author = {Chua, L.},
  date = {1971},
  url = {https://doi.org/10.1109/tct.1971.1083337},
  issn = {0018-9324},
  journal = {IEEE Transactions on Circuit Theory},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Memristor-The missing circuit element},
}

@article{davies2018loihi,
  number = {1},
  doi = {10.1109/mm.2018.112130359},
  pages = {82--99},
  source = {Crossref},
  volume = {38},
  author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  date = {2018-01},
  url = {https://doi.org/10.1109/mm.2018.112130359},
  issn = {0272-1732,1937-4143},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
}

@article{davies2021advancing,
  number = {5},
  doi = {10.1109/jproc.2021.3067593},
  pages = {911--934},
  source = {Crossref},
  volume = {109},
  author = {Davies, Mike and Wild, Andreas and Orchard, Garrick and Sandamirskaya, Yulia and Guerra, Gabriel A. Fonseca and Joshi, Prasad and Plank, Philipp and Risbud, Sumedh R.},
  date = {2021-05},
  url = {https://doi.org/10.1109/jproc.2021.3067593},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook},
}

@article{dongarra2009evolution,
  title = {The evolution of high performance computing on system z},
  author = {Dongarra, Jack J},
  year = {2009},
  journal = {IBM J. Res. Dev.},
  volume = {53},
  pages = {3--4},
}

@article{duarte2022fastml,
  url = {http://arxiv.org/abs/2207.07958v1},
  date = {2022-07-16},
  title = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},
  author = {Duarte, Javier and Tran, Nhan and Hawks, Ben and Herwig, Christian and Muhizi, Jules and Prakash, Shvetank and Reddi, Vijay Janapa},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/2207.07958},
}

@article{eshraghian2023training,
  number = {9},
  doi = {10.1109/jproc.2023.3308088},
  pages = {1016--1054},
  source = {Crossref},
  volume = {111},
  author = {Eshraghian, Jason K. and Ward, Max and Neftci, Emre O. and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
  date = {2023-09},
  url = {https://doi.org/10.1109/jproc.2023.3308088},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Training Spiking Neural Networks Using Lessons From Deep Learning},
  bdsk-url-1 = {https://doi.org/10.1109/JPROC.2023.3308088},
}

@article{farah2005neuroethics,
  number = {1},
  doi = {10.1016/j.tics.2004.12.001},
  pages = {34--40},
  source = {Crossref},
  volume = {9},
  author = {Farah, Martha J.},
  date = {2005-01},
  url = {https://doi.org/10.1016/j.tics.2004.12.001},
  issn = {1364-6613},
  journal = {Trends in Cognitive Sciences},
  publisher = {Elsevier BV},
  title = {Neuroethics: the practical and the philosophical},
}

@inproceedings{fowers2018configurable,
  doi = {10.1109/isca.2018.00012},
  pages = {1--14},
  source = {Crossref},
  author = {Fowers, Jeremy and Ovtcharov, Kalin and Papamichael, Michael and Massengill, Todd and Liu, Ming and Lo, Daniel and Alkalay, Shlomi and Haselman, Michael and Adams, Logan and Ghandi, Mahdi and Heil, Stephen and Patel, Prerak and Sapek, Adam and Weisz, Gabriel and Woods, Lisa and Lanka, Sitaram and Reinhardt, Steven K. and Caulfield, Adrian M. and Chung, Eric S. and Burger, Doug},
  date = {2018-06},
  url = {https://doi.org/10.1109/isca.2018.00012},
  booktitle = {2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
  publisher = {IEEE},
  title = {A Configurable Cloud-Scale DNN Processor for Real-Time AI},
  organization = {IEEE},
}

@article{furber2016large,
  number = {5},
  doi = {10.1088/1741-2560/13/5/051001},
  pages = {051001},
  source = {Crossref},
  volume = {13},
  author = {Furber, Steve},
  date = {2016-08-16},
  url = {https://doi.org/10.1088/1741-2560/13/5/051001},
  issn = {1741-2560,1741-2552},
  journal = {Journal of Neural Engineering},
  publisher = {IOP Publishing},
  title = {Large-scale neuromorphic computing systems},
}

@article{gale2019state,
  url = {http://arxiv.org/abs/1902.09574v1},
  date = {2019-02-25},
  title = {The State of Sparsity in Deep Neural Networks},
  author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/1902.09574},
}

@inproceedings{gannot1994verilog,
  doi = {10.1109/ivc.1994.323743},
  pages = {86--92},
  source = {Crossref},
  author = {Gannot, G. and Ligthart, M.},
  url = {https://doi.org/10.1109/ivc.1994.323743},
  booktitle = {International Verilog HDL Conference},
  publisher = {IEEE},
  title = {Verilog HDL based FPGA design},
  bdsk-url-1 = {https://doi.org/10.1109/IVC.1994.323743},
}

@article{gates2009flexible,
  number = {5921},
  doi = {10.1126/science.1171230},
  pages = {1566--1567},
  source = {Crossref},
  volume = {323},
  author = {Gates, Byron D.},
  date = {2009-03-20},
  url = {https://doi.org/10.1126/science.1171230},
  issn = {0036-8075,1095-9203},
  journal = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  title = {Flexible Electronics},
}

@article{goodyear2017social,
  number = {3},
  doi = {10.1080/2159676x.2017.1303790},
  pages = {285--302},
  source = {Crossref},
  volume = {9},
  author = {Goodyear, Victoria A.},
  date = {2017-03-28},
  url = {https://doi.org/10.1080/2159676x.2017.1303790},
  issn = {2159-676X,2159-6778},
  journal = {Qualitative Research in Sport, Exercise and Health},
  publisher = {Informa UK Limited},
  title = {Social media, apps and wearable technologies: navigating ethical dilemmas and procedures},
}

@article{gwennap_certus-nx_nodate,
  title = {Certus-NX Innovates General-Purpose FPGAs},
  author = {Gwennap, Linley},
  language = {en},
}

@article{haensch2018next,
  number = {1},
  doi = {10.1109/jproc.2018.2871057},
  pages = {108--122},
  source = {Crossref},
  volume = {107},
  author = {Haensch, Wilfried and Gokmen, Tayfun and Puri, Ruchir},
  date = {2019-01},
  url = {https://doi.org/10.1109/jproc.2018.2871057},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {The Next Generation of Deep Learning Hardware: Analog Computing},
}

@article{hazan2021neuromorphic,
  doi = {10.3389/fnins.2021.627221},
  source = {Crossref},
  volume = {15},
  author = {Hazan, Avi and Ezra Tsur, Elishai},
  date = {2021-02-22},
  url = {https://doi.org/10.3389/fnins.2021.627221},
  issn = {1662-453X},
  journal = {Frontiers in Neuroscience},
  publisher = {Frontiers Media SA},
  title = {Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation},
  pages = {627221},
}

@article{hennessy2019golden,
  number = {2},
  doi = {10.1145/3282307},
  pages = {48--60},
  source = {Crossref},
  volume = {62},
  author = {Hennessy, John L. and Patterson, David A.},
  date = {2019-01-28},
  url = {https://doi.org/10.1145/3282307},
  issn = {0001-0782,1557-7317},
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  title = {A new golden age for computer architecture},
  copyright = {http://www.acm.org/publications/policies/copyright\_policy #Background},
  abstract = {Innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development will lead the way.},
  language = {en},
}

@article{howard2017mobilenets,
  url = {http://arxiv.org/abs/1704.04861v1},
  date = {2017-04-17},
  title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/1704.04861},
}

@article{huang2010pseudo,
  number = {1},
  doi = {10.1109/ted.2010.2088127},
  pages = {141--150},
  source = {Crossref},
  volume = {58},
  author = {Huang, Tsung-Ching and Fukuda, Kenjiro and Lo, Chun-Ming and Yeh, Yung-Hui and Sekitani, Tsuyoshi and Someya, Takao and Cheng, Kwang-Ting},
  date = {2011-01},
  url = {https://doi.org/10.1109/ted.2010.2088127},
  issn = {0018-9383,1557-9646},
  journal = {IEEE Transactions on Electron Devices},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Pseudo-CMOS: A Design Style for Low-Cost and Robust Flexible Electronics},
}

@incollection{ignatov2018ai,
  doi = {10.1007/978-3-030-11021-5\_19},
  pages = {288--314},
  source = {Crossref},
  author = {Ignatov, Andrey and Timofte, Radu and Chou, William and Wang, Ke and Wu, Max and Hartley, Tim and Van Gool, Luc},
  date = {2019},
  isbn = {9783030110208,9783030110215},
  url = {https://doi.org/10.1007/978-3-030-11021-5\_19},
  issn = {0302-9743,1611-3349},
  booktitle = {Computer Vision – ECCV 2018 Workshops},
  publisher = {Springer International Publishing},
  title = {AI Benchmark: Running Deep Neural Networks on Android Smartphones},
  abstract = {Over the last years, the computational power of mobile devices such as smartphones and tablets has grown dramatically, reaching the level of desktop computers available not long ago. While standard smartphone apps are no longer a problem for them, there is still a group of tasks that can easily challenge even high-end devices, namely running artificial intelligence algorithms. In this paper, we present a study of the current state of deep learning in the Android ecosystem and describe available frameworks, programming models and the limitations of running AI on smartphones. We give an overview of the hardware acceleration resources available on four main mobile chipset platforms: Qualcomm, HiSilicon, MediaTek and Samsung. Additionally, we present the real-world performance results of different mobile SoCs collected with AI Benchmark that are covering all main existing hardware configurations.},
}

@inproceedings{imani2016resistive,
  doi = {10.3850/9783981537079\_0454},
  pages = {1327--1332},
  source = {Crossref},
  author = {Imani, Mohsen and Rahimi, Abbas and S. Rosing, Tajana},
  date = {2016},
  url = {https://doi.org/10.3850/9783981537079\_0454},
  booktitle = {Proceedings of the 2016 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition (DATE)},
  publisher = {Research Publishing Services},
  title = {Resistive Configurable Associative Memory for Approximate Computing},
  organization = {IEEE},
}

@inproceedings{jacob2018quantization,
  doi = {10.1109/cvpr.2018.00286},
  pages = {2704--2713},
  source = {Crossref},
  author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  date = {2018-06},
  url = {https://doi.org/10.1109/cvpr.2018.00286},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/cvpr/JacobKCZTHAK18.bib},
  timestamp = {Wed, 06 Feb 2019 00:00:00 +0100},
}

@article{jia2018dissecting,
  url = {http://arxiv.org/abs/1804.06826v1},
  date = {2018-04-18},
  title = {Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking},
  author = {Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P.},
  primaryclass = {cs.DC},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/1804.06826},
}

@inproceedings{jia2019beyond,
  title = {Beyond Data and Model Parallelism for Deep Neural Networks},
  author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  year = {2019},
  booktitle = {Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019},
  publisher = {mlsys.org},
  url = {https://proceedings.mlsys.org/book/265.pdf},
  editor = {Talwalkar, Ameet and Smith, Virginia and Zaharia, Matei},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/mlsys/JiaZA19.bib},
  timestamp = {Thu, 18 Jun 2020 01:00:00 +0200},
}

@inproceedings{jouppi2017indatacenter,
  doi = {10.1145/3079856.3080246},
  source = {Crossref},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  date = {2017-06-24},
  url = {https://doi.org/10.1145/3079856.3080246},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  location = {Toronto, ON, Canada},
  address = {New York, NY, USA},
  series = {ISCA '17},
  pages = {1--12},
  isbn = {9781450348928},
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC—called a Tensor Processing Unit (TPU) — deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95},
  bdsk-url-1 = {https://doi.org/10.1145/3079856.3080246},
  keywords = {accelerator, neural network, MLP, TPU, CNN, deep learning, domain-specific architecture, GPU, TensorFlow, DNN, RNN, LSTM},
  numpages = {12},
}

@inproceedings{jouppi2023tpu,
  doi = {10.1145/3579371.3589350},
  pages = {1--14},
  source = {Crossref},
  author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
  date = {2023-06-17},
  url = {https://doi.org/10.1145/3579371.3589350},
  booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
  location = {Orlando, FL, USA},
  address = {New York, NY, USA},
  series = {ISCA '23},
  isbn = {9798400700958},
  abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are lt;5},
  articleno = {82},
  bdsk-url-1 = {https://doi.org/10.1145/3579371.3589350},
  keywords = {warehouse scale computer, embeddings, supercomputer, domain specific architecture, reconfigurable, TPU, large language model, power usage effectiveness, CO2 equivalent emissions, energy, optical interconnect, IPU, machine learning, GPU, carbon emissions},
  numpages = {14},
}

@inproceedings{kao2020confuciux,
  doi = {10.1109/micro50266.2020.00058},
  pages = {622--636},
  source = {Crossref},
  author = {Kao, Sheng-Chun and Jeong, Geonhwa and Krishna, Tushar},
  date = {2020-10},
  url = {https://doi.org/10.1109/micro50266.2020.00058},
  booktitle = {2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  publisher = {IEEE},
  title = {ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning},
  organization = {IEEE},
}

@inproceedings{kao2020gamma,
  doi = {10.1145/3400302.3415639},
  pages = {1--9},
  source = {Crossref},
  author = {Kao, Sheng-Chun and Krishna, Tushar},
  subtitle = {automating the HW mapping of DNN models on accelerators via genetic algorithm},
  date = {2020-11-02},
  url = {https://doi.org/10.1145/3400302.3415639},
  booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
  publisher = {ACM},
  title = {GAMMA},
}

@article{krishnan2022multiagent,
  url = {http://arxiv.org/abs/2211.16385v1},
  date = {2022-11-29},
  title = {Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration},
  author = {Krishnan, Srivatsan and Jaques, Natasha and Omidshafiei, Shayegan and Zhang, Dan and Gur, Izzeddin and Reddi, Vijay Janapa and Faust, Aleksandra},
  primaryclass = {cs.AR},
  archiveprefix = {arXiv},
  eprint = {2211.16385},
}

@inproceedings{krishnan2023archgym,
  doi = {10.1145/3579371.3589049},
  pages = {1--16},
  source = {Crossref},
  author = {Krishnan, Srivatsan and Yazdanbakhsh, Amir and Prakash, Shvetank and Jabbour, Jason and Uchendu, Ikechukwu and Ghosh, Susobhan and Boroujerdian, Behzad and Richins, Daniel and Tripathy, Devashree and Faust, Aleksandra and Janapa Reddi, Vijay},
  date = {2023-06-17},
  url = {https://doi.org/10.1145/3579371.3589049},
  booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
}

@article{kwon2022flexible,
  doi = {10.1016/j.nanoen.2022.107632},
  pages = {107632},
  source = {Crossref},
  volume = {102},
  author = {Kwon, Sun Hwa and Dong, Lin},
  date = {2022-11},
  url = {https://doi.org/10.1016/j.nanoen.2022.107632},
  issn = {2211-2855},
  journal = {Nano Energy},
  publisher = {Elsevier BV},
  title = {Flexible sensors and machine learning for heart monitoring},
}

@inproceedings{lin2022ondevice,
  doi = {10.1145/3613424.3614307},
  pages = {1381--1394},
  source = {Crossref},
  author = {Zhu, Ligeng and Hu, Lanxiang and Lin, Ji and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
  date = {2023-10-28},
  url = {https://doi.org/10.1145/3613424.3614307},
  booktitle = {56th Annual IEEE/ACM International Symposium on Microarchitecture},
  publisher = {ACM},
  title = {PockEngine: Sparse and Efficient Fine-tuning in a Pocket},
}

@article{lin2023awq,
  number = {4},
  doi = {10.1145/3714983.3714987},
  pages = {12--17},
  source = {Crossref},
  volume = {28},
  author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Xiao, Guangxuan and Han, Song},
  date = {2025-01-20},
  url = {https://doi.org/10.1145/3714983.3714987},
  issn = {2375-0529,2375-0537},
  journal = {GetMobile: Mobile Computing and Communications},
  publisher = {Association for Computing Machinery (ACM)},
  title = {AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
}

@article{lindholm2008nvidia,
  number = {2},
  doi = {10.1109/mm.2008.31},
  pages = {39--55},
  source = {Crossref},
  volume = {28},
  author = {Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
  date = {2008-03},
  url = {https://doi.org/10.1109/mm.2008.31},
  issn = {0272-1732},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {NVIDIA Tesla: A Unified Graphics and Computing Architecture},
  shorttitle = {NVIDIA Tesla},
  urldate = {2023-11-07},
  note = {Conference Name: IEEE Micro},
  abstract = {To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs.},
  bdsk-url-1 = {https://ieeexplore.ieee.org/document/4523358},
  bdsk-url-2 = {https://doi.org/10.1109/MM.2008.31},
}

@article{loh20083dstacked,
  number = {3},
  doi = {10.1145/1394608.1382159},
  pages = {453--464},
  source = {Crossref},
  volume = {36},
  author = {Loh, Gabriel H.},
  date = {2008-06},
  url = {https://doi.org/10.1145/1394608.1382159},
  issn = {0163-5964},
  journal = {ACM SIGARCH Computer Architecture News},
  publisher = {Association for Computing Machinery (ACM)},
  title = {3D-Stacked Memory Architectures for Multi-core Processors},
}

@inproceedings{luebke2008cuda,
  doi = {10.1109/isbi.2008.4541126},
  source = {Crossref},
  author = {Luebke, David},
  date = {2008-05},
  url = {https://doi.org/10.1109/isbi.2008.4541126},
  booktitle = {2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
  publisher = {IEEE},
  title = {CUDA: Scalable parallel programming for high-performance scientific computing},
  pages = {836--838},
  bdsk-url-1 = {https://doi.org/10.1109/ISBI.2008.4541126},
}

@article{maass1997networks,
  number = {9},
  doi = {10.1016/s0893-6080(97)00011-7},
  pages = {1659--1671},
  source = {Crossref},
  volume = {10},
  author = {Maass, Wolfgang},
  date = {1997-12},
  url = {https://doi.org/10.1016/s0893-6080(97)00011-7},
  issn = {0893-6080},
  journal = {Neural Networks},
  publisher = {Elsevier BV},
  title = {Networks of spiking neurons: The third generation of neural network models},
}

@article{markovic2020physics,
  number = {9},
  doi = {10.1038/s42254-020-0208-2},
  pages = {499--510},
  source = {Crossref},
  volume = {2},
  author = {Marković, Danijela and Mizrahi, Alice and Querlioz, Damien and Grollier, Julie},
  date = {2020-07-28},
  url = {https://doi.org/10.1038/s42254-020-0208-2},
  issn = {2522-5820},
  journal = {Nature Reviews Physics},
  publisher = {Springer Science and Business Media LLC},
  title = {Physics for neuromorphic computing},
}

@article{mattson2020mlperf,
  number = {2},
  doi = {10.1109/mm.2020.2974843},
  pages = {8--16},
  source = {Crossref},
  volume = {40},
  author = {Mattson, Peter and Reddi, Vijay Janapa and Cheng, Christine and Coleman, Cody and Diamos, Greg and Kanter, David and Micikevicius, Paulius and Patterson, David and Schmuelling, Guenther and Tang, Hanlin and Wei, Gu-Yeon and Wu, Carole-Jean},
  date = {2020-03-01},
  url = {https://doi.org/10.1109/mm.2020.2974843},
  issn = {0272-1732,1937-4143},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance},
}

@article{miller2000optical,
  number = {6},
  doi = {10.1109/2944.902184},
  pages = {1312--1317},
  source = {Crossref},
  volume = {6},
  author = {Miller, D.A.B.},
  date = {2000-11},
  url = {https://doi.org/10.1109/2944.902184},
  issn = {1077-260X,1558-4542},
  journal = {IEEE Journal of Selected Topics in Quantum Electronics},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Optical interconnects to silicon},
}

@article{mirhoseini2021graph,
  number = {7862},
  doi = {10.1038/s41586-021-03544-w},
  pages = {207--212},
  source = {Crossref},
  volume = {594},
  author = {Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Nova, Azade and Pak, Jiwoo and Tong, Andy and Srinivasa, Kavya and Hang, William and Tuncer, Emre and Le, Quoc V. and Laudon, James and Ho, Richard and Carpenter, Roger and Dean, Jeff},
  date = {2021-06-09},
  url = {https://doi.org/10.1038/s41586-021-03544-w},
  issn = {0028-0836,1476-4687},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  title = {A graph placement methodology for fast chip
            design},
}

@article{mittal2021survey,
  doi = {10.1016/j.sysarc.2021.102276},
  pages = {102276},
  source = {Crossref},
  volume = {119},
  author = {Mittal, Sparsh and Verma, Gaurav and Kaushik, Brajesh and Khanday, Farooq A.},
  date = {2021-10},
  url = {https://doi.org/10.1016/j.sysarc.2021.102276},
  issn = {1383-7621},
  journal = {Journal of Systems Architecture},
  publisher = {Elsevier BV},
  title = {A survey of SRAM-based in-memory computing techniques and applications},
}

@article{modha2023neural,
  number = {6668},
  doi = {10.1126/science.adh1174},
  pages = {329--335},
  source = {Crossref},
  volume = {382},
  author = {Modha, Dharmendra S. and Akopyan, Filipp and Andreopoulos, Alexander and Appuswamy, Rathinakumar and Arthur, John V. and Cassidy, Andrew S. and Datta, Pallab and DeBole, Michael V. and Esser, Steven K. and Otero, Carlos Ortega and Sawada, Jun and Taba, Brian and Amir, Arnon and Bablani, Deepika and Carlson, Peter J. and Flickner, Myron D. and Gandhasri, Rajamohan and Garreau, Guillaume J. and Ito, Megumi and Klamo, Jennifer L. and Kusnitz, Jeffrey A. and McClatchey, Nathaniel J. and McKinstry, Jeffrey L. and Nakamura, Yutaka and Nayak, Tapan K. and Risk, William P. and Schleupen, Kai and Shaw, Ben and Sivagnaname, Jay and Smith, Daniel F. and Terrizzano, Ignacio and Ueda, Takanori},
  date = {2023-10-20},
  url = {https://doi.org/10.1126/science.adh1174},
  issn = {0036-8075,1095-9203},
  journal = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  title = {Neural inference at the frontier of energy, space, and time},
}

@inproceedings{munshi2009opencl,
  doi = {10.1109/hotchips.2009.7478342},
  source = {Crossref},
  author = {Munshi, Aaftab},
  date = {2009-08},
  url = {https://doi.org/10.1109/hotchips.2009.7478342},
  booktitle = {2009 IEEE Hot Chips 21 Symposium (HCS)},
  publisher = {IEEE},
  title = {The OpenCL specification},
  pages = {1--314},
  bdsk-url-1 = {https://doi.org/10.1109/HOTCHIPS.2009.7478342},
}

@article{musk2019integrated,
  number = {10},
  doi = {10.2196/16194},
  pages = {e16194},
  source = {Crossref},
  volume = {21},
  author = {Musk, Elon and },
  date = {2019-10-31},
  url = {https://doi.org/10.2196/16194},
  issn = {1438-8871},
  journal = {Journal of Medical Internet Research},
  publisher = {JMIR Publications Inc.},
  title = {An Integrated Brain-Machine Interface Platform With Thousands of Channels},
}

@article{norrie2021design,
  number = {2},
  doi = {10.1109/mm.2021.3058217},
  pages = {56--63},
  source = {Crossref},
  volume = {41},
  author = {Norrie, Thomas and Patil, Nishant and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Laudon, James and Young, Cliff and Jouppi, Norman and Patterson, David},
  date = {2021-03-01},
  url = {https://doi.org/10.1109/mm.2021.3058217},
  issn = {0272-1732,1937-4143},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {The Design Process for Google's Training Chips: TPUv2 and TPUv3},
  bdsk-url-1 = {https://doi.org/10.1109/MM.2021.3058217},
}

@book{patterson2016computer,
  title = {Computer organization and design ARM edition: The hardware software interface},
  author = {Patterson, David A and Hennessy, John L},
  year = {2016},
  publisher = {Morgan kaufmann},
}

@article{putnam2014reconfigurable,
  number = {3},
  doi = {10.1145/2678373.2665678},
  pages = {13--24},
  source = {Crossref},
  volume = {42},
  author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
  date = {2014-06-14},
  url = {https://doi.org/10.1145/2678373.2665678},
  issn = {0163-5964},
  journal = {ACM SIGARCH Computer Architecture News},
  publisher = {Association for Computing Machinery (ACM)},
  title = {A reconfigurable fabric for accelerating large-scale datacenter services},
  urldate = {2023-11-07},
  abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we have designed and built a composable, reconfigurablefabric to accelerate portions of large-scale software services. Each instantiation of the fabric consists of a 6x8 2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One FPGA is placed into each server, accessible through PCIe, and wired directly to other FPGAs with pairs of 10 Gb SAS cables In this paper, we describe a medium-scale deployment of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance, power, and resilience of the system when ranking candidate documents. Under high load, the largescale reconfigurable fabric improves the ranking throughput of each server by a factor of 95},
  bdsk-url-1 = {https://dl.acm.org/doi/10.1145/2678373.2665678},
  bdsk-url-2 = {https://doi.org/10.1145/2678373.2665678},
  language = {en},
}

@inproceedings{rajat2009largescale,
  doi = {10.1145/1553374.1553486},
  source = {Crossref},
  author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
  date = {2009-06-14},
  url = {https://doi.org/10.1145/1553374.1553486},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  publisher = {ACM},
  title = {Large-scale deep unsupervised learning using graphics processors},
  series = {ACM International Conference Proceeding Series},
  volume = {382},
  pages = {873--880},
  editor = {Danyluk, Andrea Pohoreckyj and Bottou, Léon and Littman, Michael L.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/icml/RainaMN09.bib},
  timestamp = {Wed, 14 Nov 2018 00:00:00 +0100},
}

@article{ranganathan2011from,
  number = {1},
  doi = {10.1109/mc.2011.18},
  pages = {39--48},
  source = {Crossref},
  volume = {44},
  author = {Ranganathan, Parthasarathy},
  date = {2011-01},
  url = {https://doi.org/10.1109/mc.2011.18},
  issn = {0018-9162},
  journal = {Computer},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {From Microprocessors to Nanostores: Rethinking Data-Centric Systems},
}

@inproceedings{reagen2017case,
  doi = {10.1109/islped.2017.8009208},
  source = {Crossref},
  author = {Reagen, Brandon and Hernandez-Lobato, Jose Miguel and Adolf, Robert and Gelbart, Michael and Whatmough, Paul and Wei, Gu-Yeon and Brooks, David},
  date = {2017-07},
  url = {https://doi.org/10.1109/islped.2017.8009208},
  booktitle = {2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)},
  publisher = {IEEE},
  title = {A case for efficient accelerator design space exploration via Bayesian optimization},
  pages = {1--6},
  organization = {IEEE},
}

@inproceedings{reddi2020mlperf,
  doi = {10.1109/isca45697.2020.00045},
  pages = {446--459},
  source = {Crossref},
  author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
  date = {2020-05},
  url = {https://doi.org/10.1109/isca45697.2020.00045},
  booktitle = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  publisher = {IEEE},
  title = {MLPerf Inference Benchmark},
  organization = {IEEE},
}

@article{roskies2002neuroethics,
  number = {1},
  doi = {10.1016/s0896-6273(02)00763-8},
  pages = {21--23},
  source = {Crossref},
  volume = {35},
  author = {Roskies, Adina},
  date = {2002-07},
  url = {https://doi.org/10.1016/s0896-6273(02)00763-8},
  issn = {0896-6273},
  journal = {Neuron},
  publisher = {Elsevier BV},
  title = {Neuroethics for the New Millenium},
}

@article{samajdar2018scale,
  url = {http://arxiv.org/abs/1811.02883v2},
  date = {2018-10-16},
  title = {SCALE-Sim: Systolic CNN Accelerator Simulator},
  author = {Samajdar, Ananda and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
  primaryclass = {cs.DC},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/1811.02883},
}

@article{schuman2022opportunities,
  number = {1},
  doi = {10.1038/s43588-021-00184-y},
  pages = {10--19},
  source = {Crossref},
  volume = {2},
  author = {Schuman, Catherine D. and Kulkarni, Shruti R. and Parsa, Maryam and Mitchell, J. Parker and Date, Prasanna and Kay, Bill},
  date = {2022-01-31},
  url = {https://doi.org/10.1038/s43588-021-00184-y},
  issn = {2662-8457},
  journal = {Nature Computational Science},
  publisher = {Springer Science and Business Media LLC},
  title = {Opportunities for neuromorphic computing algorithms and applications},
}

@misc{segal1999opengl,
  title = {The OpenGL graphics system: A specification (version 1.1)},
  author = {Segal, Mark and Akeley, Kurt},
  year = {1999},
}

@article{segura2018ethical,
  number = {1},
  doi = {10.1007/s11948-017-9872-8},
  pages = {1--28},
  source = {Crossref},
  volume = {24},
  author = {Segura Anaya, L. H. and Alsadoon, Abeer and Costadopoulos, N. and Prasad, P. W. C.},
  date = {2017-02-02},
  url = {https://doi.org/10.1007/s11948-017-9872-8},
  issn = {1353-3452,1471-5546},
  journal = {Science and Engineering Ethics},
  publisher = {Springer Science and Business Media LLC},
  title = {Ethical Implications of User Perceptions of Wearable Devices},
}

@article{shastri2021photonics,
  number = {2},
  doi = {10.1038/s41566-020-00754-y},
  pages = {102--114},
  source = {Crossref},
  volume = {15},
  author = {Shastri, Bhavin J. and Tait, Alexander N. and Ferreira de Lima, T. and Pernice, Wolfram H. P. and Bhaskaran, Harish and Wright, C. D. and Prucnal, Paul R.},
  date = {2021-01-29},
  url = {https://doi.org/10.1038/s41566-020-00754-y},
  issn = {1749-4885,1749-4893},
  journal = {Nature Photonics},
  publisher = {Springer Science and Business Media LLC},
  title = {Photonics for artificial intelligence and neuromorphic computing},
}

@inproceedings{suda2016throughput,
  doi = {10.1145/2847263.2847276},
  source = {Crossref},
  author = {Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
  date = {2016-02-21},
  url = {https://doi.org/10.1145/2847263.2847276},
  booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  publisher = {ACM},
  title = {Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks},
  pages = {16--25},
}

@article{tang2022soft,
  number = {1},
  doi = {10.1063/5.0069516},
  source = {Crossref},
  volume = {3},
  author = {Tang, Xin and He, Yichun and Liu, Jia},
  date = {2022-01-12},
  url = {https://doi.org/10.1063/5.0069516},
  issn = {2688-4089},
  journal = {Biophysics Reviews},
  publisher = {AIP Publishing},
  title = {Soft bioelectronics for cardiac interfaces},
}

@article{tang2023flexible,
  number = {2},
  doi = {10.1038/s41928-022-00913-9},
  pages = {109--118},
  source = {Crossref},
  volume = {6},
  author = {Tang, Xin and Shen, Hao and Zhao, Siyuan and Li, Na and Liu, Jia},
  date = {2023-02-02},
  url = {https://doi.org/10.1038/s41928-022-00913-9},
  issn = {2520-1131},
  journal = {Nature Electronics},
  publisher = {Springer Science and Business Media LLC},
  title = {Flexible brain–computer interfaces},
}

@incollection{valenzuela2000genetic,
  doi = {10.1007/3-540-45356-3\_66},
  pages = {671--680},
  source = {Crossref},
  author = {Valenzuela, Christine L. and Wang, Pearl Y.},
  date = {2000},
  isbn = {9783540410560,9783540453567},
  url = {https://doi.org/10.1007/3-540-45356-3\_66},
  issn = {0302-9743},
  booktitle = {Parallel Problem Solving from Nature PPSN VI},
  publisher = {Springer Berlin Heidelberg},
  title = {A Genetic Algorithm for VLSI Floorplanning},
  organization = {Springer},
}

@article{verma2019memory,
  number = {3},
  doi = {10.1109/mssc.2019.2922889},
  pages = {43--55},
  source = {Crossref},
  volume = {11},
  author = {Verma, Naveen and Jia, Hongyang and Valavi, Hossein and Tang, Yinqi and Ozatay, Murat and Chen, Lung-Yen and Zhang, Bonan and Deaville, Peter},
  date = {2019},
  url = {https://doi.org/10.1109/mssc.2019.2922889},
  issn = {1943-0582,1943-0590},
  journal = {IEEE Solid-State Circuits Magazine},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {In-Memory Computing: Advances and Prospects},
}

@article{vivet2021intact,
  number = {1},
  doi = {10.1109/jssc.2020.3036341},
  pages = {79--97},
  source = {Crossref},
  volume = {56},
  author = {Vivet, Pascal and Guthmuller, Eric and Thonnart, Yvain and Pillonnet, Gael and Fuguet, Cesar and Miro-Panades, Ivan and Moritz, Guillaume and Durupt, Jean and Bernard, Christian and Varreau, Didier and Pontes, Julian and Thuries, Sebastien and Coriat, David and Harrand, Michel and Dutoit, Denis and Lattard, Didier and Arnaud, Lucile and Charbonnier, Jean and Coudrain, Perceval and Garnier, Arnaud and Berger, Frederic and Gueugnot, Alain and Greiner, Alain and Meunier, Quentin L. and Farcy, Alexis and Arriordaz, Alexandre and Cheramy, Severine and Clermidy, Fabien},
  date = {2021-01},
  url = {https://doi.org/10.1109/jssc.2020.3036341},
  issn = {0018-9200,1558-173X},
  journal = {IEEE Journal of Solid-State Circuits},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management},
  bdsk-url-1 = {https://doi.org/10.1109/JSSC.2020.3036341},
}

@inproceedings{wang2020apq,
  doi = {10.1109/cvpr42600.2020.00215},
  pages = {2075--2084},
  source = {Crossref},
  author = {Wang, Tianzhe and Wang, Kuan and Cai, Han and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Lin, Yujun and Han, Song},
  date = {2020-06},
  url = {https://doi.org/10.1109/cvpr42600.2020.00215},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  title = {APQ: Joint Search for Network Architecture, Pruning and Quantization Policy},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/cvpr/WangWCLL0LH20.bib},
  timestamp = {Tue, 22 Dec 2020 00:00:00 +0100},
}

@techreport{weik1955survey,
  doi = {10.21236/ad0253212},
  source = {Crossref},
  author = {Weik, Martin H.},
  date = {1961-03-01},
  url = {https://doi.org/10.21236/ad0253212},
  institution = {Defense Technical Information Center},
  title = {A THIRD SURVEY OF DOMESTIC ELECTRONIC DIGITAL COMPUTING SYSTEMS},
  publisher = {Ballistic Research Laboratories},
  language = {en},
}

@article{wong2012metal,
  number = {6},
  doi = {10.1109/jproc.2012.2190369},
  pages = {1951--1970},
  source = {Crossref},
  volume = {100},
  author = {Wong, H.-S. Philip and Lee, Heng-Yuan and Yu, Shimeng and Chen, Yu-Sheng and Wu, Yi and Chen, Pang-Shiu and Lee, Byoungil and Chen, Frederick T. and Tsai, Ming-Jinn},
  date = {2012-06},
  url = {https://doi.org/10.1109/jproc.2012.2190369},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Metal–Oxide RRAM},
}

@article{xiong2021mribased,
  number = {1},
  doi = {10.1186/s12859-021-04347-6},
  source = {Crossref},
  volume = {22},
  author = {Xiong, Siyu and Wu, Guoqing and Fan, Xitian and Feng, Xuan and Huang, Zhongcheng and Cao, Wei and Zhou, Xuegong and Ding, Shijin and Yu, Jinhua and Wang, Lingli and Shi, Zhifeng},
  date = {2021-09-07},
  url = {https://doi.org/10.1186/s12859-021-04347-6},
  issn = {1471-2105},
  journal = {BMC Bioinformatics},
  publisher = {Springer Science and Business Media LLC},
  title = {MRI-based brain tumor segmentation using FPGA-accelerated neural network},
  pages = {421},
  urldate = {2023-11-07},
  abstract = {Brain tumor segmentation is a challenging problem in medical image processing and analysis. It is a very time-consuming and error-prone task. In order to reduce the burden on physicians and improve the segmentation accuracy, the computer-aided detection (CAD) systems need to be developed. Due to the powerful feature learning ability of the deep learning technology, many deep learning-based methods have been applied to the brain tumor segmentation CAD systems and achieved satisfactory accuracy. However, deep learning neural networks have high computational complexity, and the brain tumor segmentation process consumes significant time. Therefore, in order to achieve the high segmentation accuracy of brain tumors and obtain the segmentation results efficiently, it is very demanding to speed up the segmentation process of brain tumors.},
  bdsk-url-1 = {https://doi.org/10.1186/s12859-021-04347-6},
  keywords = {Brain tumor segmatation, FPGA acceleration, Neural network},
}

@article{xiu2019time,
  number = {1},
  doi = {10.1109/mssc.2018.2882285},
  pages = {39--55},
  source = {Crossref},
  volume = {11},
  author = {Xiu, Liming},
  date = {2019},
  url = {https://doi.org/10.1109/mssc.2018.2882285},
  issn = {1943-0582,1943-0590},
  journal = {IEEE Solid-State Circuits Magazine},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Time Moore: Exploiting Moore's Law From The Perspective of Time},
}

@article{young2018recent,
  number = {3},
  doi = {10.1109/mci.2018.2840738},
  pages = {55--75},
  source = {Crossref},
  volume = {13},
  author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  date = {2018-08},
  url = {https://doi.org/10.1109/mci.2018.2840738},
  issn = {1556-603X,1556-6048},
  journal = {IEEE Computational Intelligence Magazine},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Recent Trends in Deep Learning Based Natural Language Processing [Review Article]},
}

@article{yu2023rl,
  number = {2},
  doi = {10.1145/3632174},
  pages = {1--33},
  source = {Crossref},
  volume = {29},
  author = {Qian, Yu and Zhou, Xuegong and Zhou, Hao and Wang, Lingli},
  date = {2024-01-15},
  url = {https://doi.org/10.1145/3632174},
  issn = {1084-4309,1557-7309},
  journal = {ACM Transactions on Design Automation of Electronic Systems},
  publisher = {Association for Computing Machinery (ACM)},
  title = {An Efficient Reinforcement Learning Based Framework for Exploring Logic Synthesis},
  address = {New York, NY, USA},
  note = {Just Accepted},
  abstract = {Logic synthesis is a crucial step in electronic design automation tools. The rapid developments of reinforcement learning (RL) have enabled the automated exploration of logic synthesis. Existing RL based methods may lead to data inefficiency, and the exploration approaches for FPGA and ASIC technology mapping in recent works lack the flexibility of the learning process. This work proposes ESE, a reinforcement learning based framework to efficiently learn the logic synthesis process. The framework supports the modeling of logic optimization and technology mapping for FPGA and ASIC. The optimization for the execution time of the synthesis script is also considered. For the modeling of FPGA mapping, the logic optimization and technology mapping are combined to be learned in a flexible way. For the modeling of ASIC mapping, the standard cell based optimization and LUT optimization operations are incorporated into the ASIC synthesis flow. To improve the utilization of samples, the Proximal Policy Optimization model is adopted. Furthermore, the framework is enhanced by supporting MIG based synthesis exploration. Experiments show that for FPGA technology mapping on the VTR benchmark, the average LUT-Level-Product and script runtime are improved by more than 18.3},
  keywords = {technology mapping, Majority-Inverter Graph, And-Inverter Graph, Reinforcement learning, logic optimization},
}

@inproceedings{zhang2015fpga,
  title = {FPGA-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 ACM},
  author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason Optimizing},
  year = {2015},
  booktitle = {SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA},
  volume = {15},
  pages = {161--170},
}

@inproceedings{zhang2022fullstack,
  doi = {10.1145/3503222.3507767},
  pages = {27--42},
  source = {Crossref},
  author = {Zhang, Dan and Huda, Safeen and Songhori, Ebrahim and Prabhu, Kartik and Le, Quoc and Goldie, Anna and Mirhoseini, Azalia},
  date = {2022-02-22},
  url = {https://doi.org/10.1145/3503222.3507767},
  booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  publisher = {ACM},
  title = {A full-stack search technique for domain optimized deep learning accelerators},
  location = {Lausanne, Switzerland},
  address = {New York, NY, USA},
  series = {ASPLOS '22},
  isbn = {9781450392051},
  abstract = {The rapidly-changing deep learning landscape presents a unique opportunity for building inference accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. In this paper, we analyze bottlenecks in state-of-the-art vision and natural language processing (NLP) models, including EfficientNet and BERT, and use FAST to design accelerators capable of addressing these bottlenecks. FAST-generated accelerators optimized for single workloads improve Perf/TDP by 3.7× on average across all benchmarks compared to TPU-v3. A FAST-generated accelerator optimized for serving a suite of workloads improves Perf/TDP by 2.4× on average compared to TPU-v3. Our return on investment analysis shows that FAST-generated accelerators can potentially be practical for moderate-sized datacenter deployments.},
  keywords = {design space exploration, hardware-software codesign, tensor processing unit, machine learning, operation fusion},
  numpages = {16},
}

@article{zhou2022photonic,
  number = {1},
  doi = {10.1038/s41377-022-00717-8},
  source = {Crossref},
  volume = {11},
  author = {Zhou, Hailong and Dong, Jianji and Cheng, Junwei and Dong, Wenchan and Huang, Chaoran and Shen, Yichen and Zhang, Qiming and Gu, Min and Qian, Chao and Chen, Hongsheng and Ruan, Zhichao and Zhang, Xinliang},
  date = {2022-02-03},
  url = {https://doi.org/10.1038/s41377-022-00717-8},
  issn = {2047-7538},
  journal = {Light: Science \&amp; Applications},
  publisher = {Springer Science and Business Media LLC},
  title = {Photonic matrix multiplication lights up photonic accelerator and beyond},
  pages = {30},
}

@inproceedings{zhou2023area,
  doi = {10.1145/3566097.3567894},
  pages = {159--165},
  source = {Crossref},
  author = {Zhou, Guanglei and Anderson, Jason H.},
  date = {2023-01-16},
  url = {https://doi.org/10.1145/3566097.3567894},
  booktitle = {Proceedings of the 28th Asia and South Pacific Design Automation Conference},
  publisher = {ACM},
  title = {Area-Driven FPGA Logic Synthesis Using Reinforcement Learning},
}

@inproceedings{zhu2018benchmarking,
  doi = {10.1109/iiswc.2018.8573476},
  pages = {88--100},
  source = {Crossref},
  author = {Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
  date = {2018-09},
  url = {https://doi.org/10.1109/iiswc.2018.8573476},
  booktitle = {2018 IEEE International Symposium on Workload Characterization (IISWC)},
  publisher = {IEEE},
  title = {Benchmarking and Analyzing Deep Neural Network Training},
  organization = {IEEE},
}

@article{rayis2014,
  title = {Reconfigurable architectures for the next generation of mobile device telecommunications systems},
  author = {El-Rayis, A.O.},
  year = {2014},
  url = {: https://www.researchgate.net/publication/292608967},
}
