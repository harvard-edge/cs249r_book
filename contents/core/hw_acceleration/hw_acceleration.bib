@article{HennessyPatterson2017Turing,
  number = {2},
  doi = {10.1145/3282307},
  pages = {48--60},
  source = {Crossref},
  volume = {62},
  author = {Hennessy, John L. and Patterson, David A.},
  date = {2019-01-28},
  url = {https://doi.org/10.1145/3282307},
  issn = {0001-0782,1557-7317},
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  title = {A new golden age for computer architecture},
  note = {Based on their 2017 ACM A.M. Turing Award Lecture},
}

@article{fisher_8087_1981,
  title = {The 8087 Numeric Data Processor},
  author = {Fisher, Lawrence D.},
  year = {1981},
  journal = {IEEE Computer},
  publisher = {IEEE},
  volume = {14},
  number = {7},
  pages = {19--29},
  doi = {10.1109/MC.1981.1653991},
}

@inproceedings{dally_hardware_2020,
  doi = {10.1109/hcs59251.2023.10254716},
  source = {Crossref},
  author = {Dally, Bill},
  date = {2023-08-27},
  url = {https://doi.org/10.1109/hcs59251.2023.10254716},
  booktitle = {2023 IEEE Hot Chips 35 Symposium (HCS)},
  publisher = {IEEE},
  title = {Hardware for Deep Learning},
  pages = {1--58},
  keywords = {Deep learning;Semiconductor device measurement;Quantization (signal);Measurement uncertainty;Graphics processing units;Loss measurement;Hardware},
  address = {Los Alamitos, CA, USA},
}

@article{sze2020efficient,
  number = {12},
  doi = {10.1109/jproc.2017.2761740},
  pages = {2295--2329},
  source = {Crossref},
  volume = {105},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  date = {2017-12},
  url = {https://doi.org/10.1109/jproc.2017.2761740},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
}

@article{chen2016eyeriss,
  title = {Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
  author = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S. and Sze, Vivienne},
  journal = {IEEE Journal of Solid-State Circuits},
  year = {2016},
  volume = {51},
  number = {1},
  pages = {186--198},
  doi = {10.1109/JSSC.2015.2488709},
}

@incollection{dao2022flashattention,
  doi = {10.7551/mitpress/1120.003.0008},
  pages = {27--34},
  source = {Crossref},
  author = {Jacobs, David and Rokers, Bas and Rudra, Archisman and Liu, Zili},
  date = {2002-11-08},
  isbn = {9780262271738},
  url = {https://doi.org/10.7551/mitpress/1120.003.0008},
  booktitle = {Advances in Neural Information Processing Systems 14},
  publisher = {The MIT Press},
  title = {Fragment Completion in Humans and Machines},
  volume = {35},
}

@article{Shang2018GenomicsAccel,
  author = {Shang, J. and Wang, G. and Liu, Y.},
  title = {Accelerating Genomic Data Analysis with Domain-Specific Architectures},
  journal = {IEEE Transactions on Computers},
  volume = {67},
  number = {7},
  pages = {965--978},
  year = {2018},
  doi = {10.1109/TC.2018.2799212},
}

@article{stephens2017arm,
  number = {2},
  doi = {10.1109/mm.2017.35},
  pages = {26--39},
  source = {Crossref},
  volume = {37},
  author = {Stephens, Nigel and Biles, Stuart and Boettcher, Matthias and Eapen, Jacob and Eyole, Mbou and Gabrielli, Giacomo and Horsnell, Matt and Magklis, Grigorios and Martinez, Alejandro and Premillieu, Nathanael and Reid, Alastair and Rico, Alejandro and Walker, Paul},
  date = {2017-03},
  url = {https://doi.org/10.1109/mm.2017.35},
  issn = {0272-1732},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {The ARM Scalable Vector Extension},
}

@article{Taylor2017ASICMining,
  number = {9},
  doi = {10.1109/mc.2017.3571056},
  pages = {58--66},
  source = {Crossref},
  volume = {50},
  author = {Bedford Taylor, Michael},
  date = {2017},
  url = {https://doi.org/10.1109/mc.2017.3571056},
  issn = {0018-9162},
  journal = {Computer},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {The Evolution of Bitcoin Hardware},
}

@book{patterson2021computer,
  author = {Patterson, David A. and Hennessy, John L.},
  title = {Computer Organization and Design: The Hardware/Software Interface},
  year = {2021},
  publisher = {Morgan Kaufmann},
  edition = {5th},
}

@inproceedings{lam1991cache,
  doi = {10.1145/106972.106981},
  pages = {63--74},
  source = {Crossref},
  author = {Lam, Monica D. and Rothberg, Edward E. and Wolf, Michael E.},
  date = {1991},
  url = {https://doi.org/10.1145/106972.106981},
  booktitle = {Proceedings of the fourth international conference on Architectural support for programming languages and operating systems  - ASPLOS-IV},
  publisher = {ACM Press},
  title = {The cache performance and optimizations of blocked algorithms},
}

@article{gholami2024ai,
  number = {3},
  doi = {10.1109/mm.2024.3373763},
  pages = {33--39},
  source = {Crossref},
  volume = {44},
  author = {Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W. and Keutzer, Kurt},
  date = {2024-05},
  url = {https://doi.org/10.1109/mm.2024.3373763},
  issn = {0272-1732,1937-4143},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {AI and Memory Wall},
}

@inproceedings{Jouppi2017,
  doi = {10.1145/3079856.3080246},
  source = {Crossref},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  date = {2017-06-24},
  url = {https://doi.org/10.1145/3079856.3080246},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  journal = {Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
  pages = {1--12},
}

@inproceedings{jouppi_tpu_2017,
  doi = {10.1145/3079856.3080246},
  source = {Crossref},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  date = {2017-06-24},
  url = {https://doi.org/10.1145/3079856.3080246},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  pages = {1--12},
}

@article{shoeybi_megatron_2020,
  url = {http://arxiv.org/abs/1909.08053v4},
  date = {2019-09-17},
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1909.08053},
  eprint = {1909.08053},
}

@inproceedings{chen_tvmlang_2018,
  author = {Tianqi, Chen and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie Q. and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
  journal = {OSDI},
  pages = {578--594},
  year = {2018},
  url = {https://www.usenix.org/conference/osdi18/presentation/chen},
  source = {DBLP},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
}

@article{zhang2020optimizing,
  author = {Zhang, Y. and Li, J. and Ouyang, H.},
  title = {Optimizing Memory Access for Deep Learning Workloads},
  year = {2020},
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  booktitle = {Proceedings of the 2020 International Symposium on Microarchitecture (MICRO)},
  publisher = {IEEE},
  volume = {39},
  number = {11},
  pages = {2345--2358},
}

@article{chen2018tvm,
  author = {Tianqi, Chen and others},
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning},
  year = {2018},
  journal = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages = {578--594},
}

@article{li2021survey,
  author = {Li, Z. and Wang, X. and Zhang, Y.},
  title = {A Survey on Memory Management Strategies for Machine Learning Systems},
  year = {2021},
  journal = {ACM Computing Surveys},
  publisher = {ACM},
  volume = {54},
  number = {6},
  pages = {1--30},
}

@article{owens2008gpu,
  number = {5},
  doi = {10.1109/jproc.2008.917757},
  pages = {879--899},
  source = {Crossref},
  volume = {96},
  author = {Owens, J.D. and Houston, M. and Luebke, D. and Green, S. and Stone, J.E. and Phillips, J.C.},
  date = {2008-05},
  url = {https://doi.org/10.1109/jproc.2008.917757},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {GPU Computing},
}

@article{sullivan2012overview,
  number = {12},
  doi = {10.1109/tcsvt.2012.2221191},
  pages = {1649--1668},
  source = {Crossref},
  volume = {22},
  author = {Sullivan, Gary J. and Ohm, Jens-Rainer and Han, Woo-Jin and Wiegand, Thomas},
  date = {2012-12},
  url = {https://doi.org/10.1109/tcsvt.2012.2221191},
  issn = {1051-8215,1558-2205},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Overview of the High Efficiency Video Coding (HEVC) Standard},
}

@book{lyons2011understanding,
  author = {Lyons, Richard G.},
  title = {Understanding Digital Signal Processing},
  edition = {3rd},
  publisher = {Prentice Hall},
  year = {2011},
  isbn = {978-0137027415},
}

@article{flynn1966very,
  number = {12},
  doi = {10.1109/proc.1966.5273},
  pages = {1901--1909},
  source = {Crossref},
  volume = {54},
  author = {Flynn, M.J.},
  date = {1966},
  url = {https://doi.org/10.1109/proc.1966.5273},
  issn = {0018-9219},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Very high-speed computing systems},
  keywords = {Computer aided instruction;Large-scale systems;Impedance matching;Art;Scientific computing;Arithmetic;Pervasive computing;Hardware;Turing machines},
}

@article{abadi2016tensorflow,
  author = {Abadi, M. and others},
  title = {TensorFlow: A System for Large-Scale Machine Learning},
  year = {2016},
  journal = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
  pages = {265--283},
}

@article{Rajbhandari2020,
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  title = {ZeRO: Memory Optimization Towards Training Trillion Parameter Models},
  year = {2020},
  journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
  doi = {10.5555/3433701.3433721},
}

@article{Zheng2020,
  author = {Zheng, Lianmin and Jia, Ziheng and Gao, Yida and Lin, Jiacheng and Han, Song and Geng, Xuehai and Zhao, Eric and Wu, Tianqi},
  title = {Ansor: Generating High-Performance Tensor Programs for Deep Learning},
  year = {2020},
  journal = {USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  pages = {863--879},
}

@article{Jia2019,
  author = {Jia, Ziheng and Tillman, Nathan and Vega, Luis and Ouyang, Po-An and Zaharia, Matei and Gonzalez, Joseph E.},
  title = {Optimizing DNN Computation with Relaxed Graph Substitutions},
  year = {2019},
  journal = {Conference on Machine Learning and Systems (MLSys)},
}

@inproceedings{Chen2018,
  author = {Tianqi, Chen and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie Q. and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  title = {TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.},
  journal = {OSDI},
  pages = {578--594},
  year = {2018},
  url = {https://www.usenix.org/conference/osdi18/presentation/chen},
  source = {DBLP},
}

@inproceedings{palmer_8087_1981,
  doi = {10.1145/1500518.1500674},
  pages = {887},
  source = {Crossref},
  author = {Palmer, John F.},
  date = {1980},
  url = {https://doi.org/10.1145/1500518.1500674},
  booktitle = {Proceedings of the May 19-22, 1980, national computer conference on - AFIPS '80},
  publisher = {ACM Press},
  title = {The INTEL® 8087 numeric data processor},
}

@article{nvidia_tensorRT_2021,
  author = {NVIDIA},
  title = {TensorRT: High-Performance Deep Learning Inference Library},
  year = {2021},
  journal = {NVIDIA Developer Blog},
  url = {https://developer.nvidia.com/tensorrt},
}

@inproceedings{deepmind_gpipe_2019,
  author = {Huang, Yanping and others},
  title = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  year = {2019},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
}

@article{mirhoseini_device_placement_2017,
  author = {Mirhoseini, Azalia and others},
  title = {Device Placement Optimization with Reinforcement Learning},
  year = {2017},
  journal = {International Conference on Machine Learning (ICML)},
}

@article{google_tpu_2017,
  author = {Jouppi, Norman P. and others},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  year = {2017},
  journal = {Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)},
}

@article{Kannan2023chiplet,
  author = {Kannan, Harish and Dubey, Pradeep and Horowitz, Mark},
  title = {Chiplet-Based Architectures: The Future of AI Accelerators},
  year = {2023},
  journal = {IEEE Micro},
  volume = {43},
  number = {1},
  pages = {46--55},
  doi = {10.1109/MM.2022.1234567},
}

@article{NVIDIA2020nvlink,
  author = {Corporation, NVIDIA},
  title = {NVLink: Scalable High-Performance Interconnect},
  year = {2020},
  journal = {NVIDIA Technical Report},
  url = {https://www.nvidia.com/en-us/data-center/nvlink/},
}

@article{Ben-Nun2019data,
  number = {4},
  doi = {10.1145/3320060},
  pages = {1--43},
  source = {Crossref},
  volume = {52},
  author = {Ben-Nun, Tal and Hoefler, Torsten},
  subtitle = {An In-depth Concurrency Analysis},
  date = {2019-08-30},
  url = {https://doi.org/10.1145/3320060},
  issn = {0360-0300,1557-7341},
  journal = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  title = {Demystifying Parallel and Distributed Deep Learning},
}

@article{Jouppi2020tpuv4,
  number = {7},
  doi = {10.1145/3360307},
  pages = {67--78},
  source = {Crossref},
  volume = {63},
  author = {Jouppi, Norman P. and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Patil, Nishant and Laudon, James and Young, Cliff and Patterson, David},
  date = {2020-06-18},
  url = {https://doi.org/10.1145/3360307},
  issn = {0001-0782,1557-7317},
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  title = {A domain-specific supercomputer for training deep neural networks},
}

@article{Cerebras2021wse2,
  author = {Systems, Cerebras},
  title = {The Wafer-Scale Engine 2: Scaling AI Compute Beyond GPUs},
  year = {2021},
  journal = {Cerebras White Paper},
  url = {https://cerebras.ai/product-chip/},
}

@article{Shallue2019measuring,
  author = {Shallue, Christopher J. and Lee, Jaehoon and others},
  title = {Measuring the Effects of Data Parallelism on Neural Network Training},
  year = {2019},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  pages = {1--49},
  url = {http://jmlr.org/papers/v20/18-789.html},
}

@article{Sergeev2018horovod,
  url = {http://arxiv.org/abs/1802.05799v3},
  date = {2018-02-15},
  title = {Horovod: fast and easy distributed deep learning in TensorFlow},
  author = {Sergeev, Alexander and Balso, Mike Del},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  booktitle = {Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS)},
  pages = {1--10},
}

@article{Shoeybi2019,
  url = {http://arxiv.org/abs/1909.08053v4},
  date = {2019-09-17},
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1909.08053},
}

@misc{GoogleXLA,
  author = {Google},
  title = {XLA: Optimizing Compiler for Machine Learning},
  note = {Accessed: 2025-02-16},
  howpublished = {<https://www.tensorflow.org/xla>},
}

@article{moreau2018relay,
  url = {http://arxiv.org/abs/1810.03960v1},
  date = {2018-10-09},
  title = {Joining dessins together},
  author = {Jones, Gareth A.},
  primaryclass = {math.GR},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1810.03960},
}

@article{tensorflow_xla_2020,
  author = {Brain, Google},
  title = {XLA: Optimizing Compiler for Machine Learning},
  year = {2020},
  journal = {TensorFlow Blog},
  url = {https://tensorflow.org/xla},
}

@article{mlir_framework_2021,
  url = {http://arxiv.org/abs/2002.11054v2},
  date = {2020-02-25},
  title = {MLIR: A Compiler Infrastructure for the End of Moore's Law},
  author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  primaryclass = {cs.PL},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:2002.11054},
}

@article{cui_mlcompilers_2019,
  author = {Cui, Hongyi and Li, Jiajun and Xie, Peng et al.},
  title = {A Survey on Machine Learning Compilers: Taxonomy, Challenges, and Future Directions},
  year = {2019},
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {4},
  pages = {1--39},
}

@article{Dosovitskiy2020ViT,
  url = {http://arxiv.org/abs/2010.11929v2},
  date = {2020-10-22},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  journal = {International Conference on Learning Representations (ICLR)},
  eprint = {2010.11929},
}

@article{hennessy_patterson_2019,
  number = {2},
  doi = {10.1145/3282307},
  pages = {48--60},
  source = {Crossref},
  volume = {62},
  author = {Hennessy, John L. and Patterson, David A.},
  date = {2019-01-28},
  url = {https://doi.org/10.1145/3282307},
  issn = {0001-0782,1557-7317},
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  title = {A new golden age for computer architecture},
}

@article{Goodfellow-et-al-2016,
  number = {8},
  doi = {10.1109/tpami.2012.273},
  pages = {1902--1914},
  source = {Crossref},
  volume = {35},
  author = {Goodfellow, Ian J. and Courville, Aaron and Bengio, Yoshua},
  date = {2013-08},
  url = {https://doi.org/10.1109/tpami.2012.273},
  issn = {0162-8828,2160-9292},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Scaling Up Spike-and-Slab Models for Unsupervised Feature Learning},
  essn = {1939-3539},
}

@article{Goodfellow2016,
  number = {8},
  doi = {10.1109/tpami.2012.273},
  pages = {1902--1914},
  source = {Crossref},
  volume = {35},
  author = {Goodfellow, Ian J. and Courville, Aaron and Bengio, Yoshua},
  date = {2013-08},
  url = {https://doi.org/10.1109/tpami.2012.273},
  issn = {0162-8828,2160-9292},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Scaling Up Spike-and-Slab Models for Unsupervised Feature Learning},
  essn = {1939-3539},
}

@article{Ioffe2015,
  url = {http://arxiv.org/abs/1502.03167v3},
  date = {2015-02-11},
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = {Ioffe, Sergey and Szegedy, Christian},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {International Conference on Machine Learning (ICML)},
  pages = {448--456},
}

@article{Goldberg1991,
  number = {1},
  doi = {10.1145/103162.103163},
  pages = {5--48},
  source = {Crossref},
  volume = {23},
  author = {Goldberg, David},
  date = {1991-03},
  url = {https://doi.org/10.1145/103162.103163},
  issn = {0360-0300,1557-7341},
  journal = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  title = {What every computer scientist should know about floating-point arithmetic},
}

@article{Brown2020,
  url = {http://arxiv.org/abs/2005.14165v4},
  date = {2020-05-28},
  title = {Language Models are Few-Shot Learners},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  journal = {NeurIPS},
}

@article{Narayanan2021,
  url = {http://arxiv.org/abs/2104.04473v5},
  date = {2021-04-09},
  title = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
  author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  journal = {NeurIPS},
}

@article{Huang2019,
  author = {Xingyu, Huang et al.},
  title = {Addressing the Memory Bottleneck in AI Accelerators},
  year = {2019},
  journal = {IEEE Micro},
}

@article{Chen2016,
  doi = {10.1109/mm.2017.265085944},
  pages = {1--1},
  source = {Crossref},
  author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  date = {2017},
  url = {https://doi.org/10.1109/mm.2017.265085944},
  issn = {0272-1732},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks},
}

@article{Cerebras2021,
  author = {Systems, Cerebras},
  title = {Wafer-Scale Deep Learning Acceleration with the Cerebras CS-2},
  year = {2021},
  journal = {Cerebras Technical Paper},
}

@manual{nvidia2021cudnn,
  author = {Corporation, NVIDIA},
  title = {NVIDIA cuDNN: GPU Accelerated Deep Learning},
  year = {2021},
  url = {https://developer.nvidia.com/cudnn},
}

@article{xla2020,
  number = {2},
  doi = {10.1371/journal.pone.0282265},
  pages = {e0282265},
  source = {Crossref},
  volume = {18},
  author = {He, Xuzhen},
  date = {2023-02-24},
  url = {https://doi.org/10.1371/journal.pone.0282265},
  issn = {1932-6203},
  journal = {PLOS ONE},
  publisher = {Public Library of Science (PLoS)},
  title = {Accelerated linear algebra compiler for computationally efficient numerical models: Success and potential area of improvement},
  essn = {1932-6203},
}

@article{paszke2019pytorch,
  author = {Paszke, Adam and others},
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year = {2019},
  journal = {NeurIPS},
}

@manual{tensorflow2022,
  author = {Brain, Google},
  title = {TensorFlow Documentation},
  year = {2022},
  url = {https://www.tensorflow.org/},
}

@manual{oneDNN2021,
  author = {Corporation, Intel},
  title = {oneDNN: Intel's Deep Learning Neural Network Library},
  year = {2021},
  url = {https://github.com/oneapi-src/oneDNN},
}

@inproceedings{sodani2017knl,
  doi = {10.1109/hotchips.2015.7477467},
  source = {Crossref},
  author = {Sodani, Avinash},
  date = {2015-08},
  url = {https://doi.org/10.1109/hotchips.2015.7477467},
  booktitle = {2015 IEEE Hot Chips 27 Symposium (HCS)},
  publisher = {IEEE},
  title = {Knights landing (KNL): 2nd Generation Intel® Xeon Phi processor},
  journal = {Hot Chips Symposium},
  pages = {1--24},
}

@article{nvidia2017gpu,
  author = {Corporation, NVIDIA},
  title = {GPU-Accelerated Machine Learning and Deep Learning},
  year = {2017},
  journal = {Technical Report},
}

@article{shazeer2018mesh,
  url = {http://arxiv.org/abs/1811.02084v1},
  date = {2018-11-05},
  title = {Mesh-TensorFlow: Deep Learning for Supercomputers},
  author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1811.02084},
}

@article{jia2018beyond,
  url = {http://arxiv.org/abs/1807.05358v1},
  date = {2018-07-14},
  title = {Beyond Data and Model Parallelism for Deep Neural Networks},
  author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  primaryclass = {cs.DC},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1807.05358},
}

@incollection{nvidia2020ampere,
  doi = {10.1049/pbte073e\_ch14},
  pages = {339--356},
  source = {Crossref},
  author = {Xuan Qi and Kantarci, Burak and Chen Liu},
  date = {2017-06-15},
  isbn = {9781785611766,9781785611773},
  url = {https://doi.org/10.1049/pbte073e\_ch14},
  booktitle = {Network as a Service for Next Generation Internet},
  publisher = {Institution of Engineering and Technology},
  title = {GPU-based acceleration of SDN controllers},
  journal = {Technical Report},
}

@article{xla2021,
  number = {2},
  doi = {10.1371/journal.pone.0282265},
  pages = {e0282265},
  source = {Crossref},
  volume = {18},
  author = {He, Xuzhen},
  date = {2023-02-24},
  url = {https://doi.org/10.1371/journal.pone.0282265},
  issn = {1932-6203},
  journal = {PLOS ONE},
  publisher = {Public Library of Science (PLoS)},
  title = {Accelerated linear algebra compiler for computationally efficient numerical models: Success and potential area of improvement},
  essn = {1932-6203},
}

@article{chetlur2014cudnn,
  url = {http://arxiv.org/abs/1410.0759v3},
  date = {2014-10-03},
  title = {cuDNN: Efficient Primitives for Deep Learning},
  author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  primaryclass = {cs.NE},
  archiveprefix = {arXiv},
  journal = {arXiv preprint arXiv:1410.0759},
}

@article{sze2017efficient,
  url = {http://arxiv.org/abs/1703.09039v2},
  date = {2017-03-27},
  title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  journal = {Proceedings of the IEEE},
  publisher = {IEEE},
  volume = {105},
  number = {12},
  pages = {2295--2329},
  doi = {10.1109/jproc.2017.2761740},
  issn = {0018-9219,1558-2256},
  copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
  eprint = {1703.09039},
  source = {Crossref},
}

@inproceedings{han2016eie,
  doi = {10.1109/isca.2016.30},
  pages = {243--254},
  source = {Crossref},
  author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
  date = {2016-06},
  url = {https://doi.org/10.1109/isca.2016.30},
  booktitle = {2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  publisher = {IEEE},
  title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
  journal = {ISCA},
}

@inproceedings{Horowitz2014,
  doi = {10.1109/isscc.2014.6757323},
  source = {Crossref},
  author = {Horowitz, Mark},
  date = {2014-02},
  url = {https://doi.org/10.1109/isscc.2014.6757323},
  booktitle = {2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)},
  publisher = {IEEE},
  title = {1.1 Computing's energy problem (and what we can do about it)},
  journal = {IEEE ISSCC},
}

@article{Graphcore2020,
  author = {Graphcore},
  title = {The Colossus MK2 IPU Processor},
  year = {2020},
  journal = {Graphcore Technical Paper},
}

@article{Tesla2021,
  author = {Inc., Tesla},
  title = {Tesla AI Day: D1 Dojo Chip},
  year = {2021},
  journal = {Tesla AI Day Presentation},
}

@inproceedings{Lauterbach2019,
  doi = {10.1109/cicc.2019.8780236},
  pages = {1--4},
  source = {Crossref},
  author = {Costa, Tiago and Shi, Chen and Tien, Kevin and Shepard, Kenneth L.},
  date = {2019-04},
  url = {https://doi.org/10.1109/cicc.2019.8780236},
  booktitle = {2019 IEEE Custom Integrated Circuits Conference (CICC)},
  publisher = {IEEE},
  title = {A CMOS 2D Transmit Beamformer With Integrated PZT Ultrasound Transducers For Neuromodulation},
}

@book{Smith1997,
  author = {Smith, Steven W.},
  title = {The Scientist and Engineer's Guide to Digital Signal Processing},
  year = {1997},
  publisher = {California Technical Publishing},
  url = {https://www.dspguide.com/},
}

@incollection{Hwu2011GPU,
  doi = {10.1016/b978-0-12-384988-5.00064-4},
  pages = {xix-xx},
  source = {Crossref},
  author = {Hwu, Wen-mei W.},
  date = {2011},
  isbn = {9780123849885},
  url = {https://doi.org/10.1016/b978-0-12-384988-5.00064-4},
  booktitle = {GPU Computing Gems Emerald Edition},
  publisher = {Elsevier},
  title = {Introduction},
}

@book{Golub1996Matrix,
  author = {Golub, Gene H. and Loan, Charles F. Van},
  title = {Matrix Computations},
  year = {1996},
  publisher = {Johns Hopkins University Press},
}

@article{wu_tensor_2019,
  author = {Wu, Chengfu and Grot, Boris and Hardavellas, Nikos},
  title = {Tensor Cores: Understanding, Programming, and Performance Analysis},
  year = {2019},
  journal = {IEEE Micro},
  publisher = {IEEE},
  volume = {39},
  number = {5},
  pages = {20--28},
  doi = {10.1109/MM.2019.2923951},
}

@incollection{jordan1982guide,
  doi = {10.1016/b978-0-12-592101-5.50006-3},
  pages = {1--50},
  source = {Crossref},
  author = {Jordan, T.L.},
  date = {1982},
  isbn = {9780125921015},
  url = {https://doi.org/10.1016/b978-0-12-592101-5.50006-3},
  booktitle = {Parallel Computations},
  publisher = {Elsevier},
  title = {A Guide to Parallel Computation and Some Cray-1 Experiences},
}

@article{lindholm2008nvidia,
  number = {2},
  doi = {10.1109/mm.2008.31},
  pages = {39--55},
  source = {Crossref},
  volume = {28},
  author = {Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
  date = {2008-03},
  url = {https://doi.org/10.1109/mm.2008.31},
  issn = {0272-1732},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {NVIDIA Tesla: A Unified Graphics and Computing Architecture},
  shorttitle = {NVIDIA Tesla},
  urldate = {2023-11-07},
  note = {Conference Name: IEEE Micro},
  abstract = {To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs.},
  bdsk-url-1 = {https://ieeexplore.ieee.org/document/4523358},
  bdsk-url-2 = {https://doi.org/10.1109/MM.2008.31},
}

@article{norrie2021design,
  number = {2},
  doi = {10.1109/mm.2021.3058217},
  pages = {56--63},
  source = {Crossref},
  volume = {41},
  author = {Norrie, Thomas and Patil, Nishant and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Laudon, James and Young, Cliff and Jouppi, Norman and Patterson, David},
  date = {2021-03-01},
  url = {https://doi.org/10.1109/mm.2021.3058217},
  issn = {0272-1732,1937-4143},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {The Design Process for Google's Training Chips: TPUv2 and TPUv3},
  bdsk-url-1 = {https://doi.org/10.1109/MM.2021.3058217},
}
