@inproceedings{Li2020Additive,
  author = {Li, Yuhang and 0009, Xin Dong and 0059, Wei Wang},
  title = {Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks.},
  journal = {ICLR},
  year = {2020},
  url = {https://openreview.net/forum?id=BkgXT24tDS},
  source = {DBLP},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/LiDW20.bib},
  booktitle = {8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  timestamp = {Tue, 18 Aug 2020 01:00:00 +0200},
}

@inproceedings{adolf2016fathom,
  doi = {10.1109/iiswc.2016.7581275},
  pages = {1--10},
  source = {Crossref},
  author = {Adolf, Robert and Rama, Saketh and Reagen, Brandon and Wei, Gu-yeon and Brooks, David},
  date = {2016-09},
  url = {https://doi.org/10.1109/iiswc.2016.7581275},
  booktitle = {2016 IEEE International Symposium on Workload Characterization (IISWC)},
  publisher = {IEEE},
  title = {Fathom: reference workloads for modern deep learning methods},
  organization = {IEEE},
}

@inproceedings{agnesina2023autodmp,
  doi = {10.1145/3569052.3578923},
  source = {Crossref},
  author = {Agnesina, Anthony and Rajvanshi, Puranjay and Yang, Tian and Pradipta, Geraldo and Jiao, Austin and Keller, Ben and Khailany, Brucek and Ren, Haoxing},
  subtitle = {Automated DREAMPlace-based Macro Placement},
  date = {2023-03-26},
  url = {https://doi.org/10.1145/3569052.3578923},
  booktitle = {Proceedings of the 2023 International Symposium on Physical Design},
  publisher = {ACM},
  title = {AutoDMP},
  pages = {149--157},
}

@article{asit2021accelerating,
  url = {http://arxiv.org/abs/2104.08378v1},
  date = {2021-04-16},
  title = {Accelerating Sparse Deep Neural Networks},
  author = {Mishra, Asit and Latorre, Jorge Albericio and Pool, Jeff and Stosic, Darko and Stosic, Dusan and Venkatesh, Ganesh and Yu, Chong and Micikevicius, Paulius},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/abs-2104-08378.bib},
  eprint = {2104.08378},
  eprinttype = {arXiv},
  journal = {CoRR},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  volume = {abs/2104.08378},
}

@article{bains2020business,
  number = {7},
  doi = {10.1038/s41928-020-0449-1},
  pages = {348--351},
  source = {Crossref},
  volume = {3},
  author = {Bains, Sunny},
  date = {2020-07-21},
  url = {https://doi.org/10.1038/s41928-020-0449-1},
  issn = {2520-1131},
  journal = {Nature Electronics},
  publisher = {Springer Science and Business Media LLC},
  title = {The business of building brains},
}

@inproceedings{bhardwaj2020comprehensive,
  doi = {10.1145/3370748.3406564},
  pages = {145--150},
  source = {Crossref},
  author = {Bhardwaj, Kshitij and Havasi, Marton and Yao, Yuan and Brooks, David M. and Hernández-Lobato, José Miguel and Wei, Gu-Yeon},
  date = {2020-08-10},
  url = {https://doi.org/10.1145/3370748.3406564},
  booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
  publisher = {ACM},
  title = {A comprehensive methodology to determine optimal coherence interfaces for many-accelerator SoCs},
}

@article{biggs2021natively,
  number = {7868},
  doi = {10.1038/s41586-021-03625-w},
  pages = {532--536},
  source = {Crossref},
  volume = {595},
  author = {Biggs, John and Myers, James and Kufel, Jedrzej and Ozer, Emre and Craske, Simon and Sou, Antony and Ramsdale, Catherine and Williamson, Ken and Price, Richard and White, Scott},
  date = {2021-07-21},
  url = {https://doi.org/10.1038/s41586-021-03625-w},
  issn = {0028-0836,1476-4687},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  title = {A natively flexible 32-bit Arm microprocessor},
}

@article{binkert2011gem5,
  number = {2},
  doi = {10.1145/2024716.2024718},
  pages = {1--7},
  source = {Crossref},
  volume = {39},
  author = {Binkert, Nathan and Beckmann, Bradford and Black, Gabriel and Reinhardt, Steven K. and Saidi, Ali and Basu, Arkaprava and Hestness, Joel and Hower, Derek R. and Krishna, Tushar and Sardashti, Somayeh and Sen, Rathijit and Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and Hill, Mark D. and Wood, David A.},
  date = {2011-05-31},
  url = {https://doi.org/10.1145/2024716.2024718},
  issn = {0163-5964},
  journal = {ACM SIGARCH Computer Architecture News},
  publisher = {Association for Computing Machinery (ACM)},
  title = {The gem5 simulator},
}

@inproceedings{brown2020language,
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
  timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
  title = {Language Models are Few-Shot Learners},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  year = {2020},
}

@article{burr2016recent,
  number = {2},
  doi = {10.1109/jetcas.2016.2547718},
  pages = {146--162},
  source = {Crossref},
  volume = {6},
  author = {Burr, Geoffrey W. and BrightSky, Matthew J. and Sebastian, Abu and Cheng, Huai-Yu and Wu, Jau-Yi and Kim, Sangbum and Sosa, Norma E. and Papandreou, Nikolaos and Lung, Hsiang-Lan and Pozidis, Haralampos and Eleftheriou, Evangelos and Lam, Chung H.},
  date = {2016-06},
  url = {https://doi.org/10.1109/jetcas.2016.2547718},
  issn = {2156-3357,2156-3365},
  journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Recent Progress in Phase-Change<?Pub _newline ?>Memory Technology},
}

@inproceedings{chen2018tvm,
  author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages = {578--594},
  title = {TVM: An automated End-to-End optimizing compiler for deep learning},
  year = {2018},
}

@article{cheng2017survey,
  number = {1},
  doi = {10.1109/msp.2017.2765695},
  pages = {126--136},
  source = {Crossref},
  volume = {35},
  author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  date = {2018-01},
  url = {https://doi.org/10.1109/msp.2017.2765695},
  issn = {1053-5888,1558-0792},
  journal = {IEEE Signal Processing Magazine},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges},
}

@article{chi2016prime,
  number = {3},
  doi = {10.1145/3007787.3001140},
  pages = {27--39},
  source = {Crossref},
  volume = {44},
  author = {Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
  subtitle = {a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory},
  date = {2016-06-18},
  url = {https://doi.org/10.1145/3007787.3001140},
  issn = {0163-5964},
  journal = {ACM SIGARCH Computer Architecture News},
  publisher = {Association for Computing Machinery (ACM)},
  title = {PRIME},
}

@article{chua1971memristor,
  number = {5},
  doi = {10.1109/tct.1971.1083337},
  pages = {507--519},
  source = {Crossref},
  volume = {18},
  author = {Chua, L.},
  date = {1971},
  url = {https://doi.org/10.1109/tct.1971.1083337},
  issn = {0018-9324},
  journal = {IEEE Transactions on Circuit Theory},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Memristor-The missing circuit element},
}

@article{davies2018loihi,
  number = {1},
  doi = {10.1109/mm.2018.112130359},
  pages = {82--99},
  source = {Crossref},
  volume = {38},
  author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  date = {2018-01},
  url = {https://doi.org/10.1109/mm.2018.112130359},
  issn = {0272-1732,1937-4143},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
}

@article{davies2021advancing,
  number = {5},
  doi = {10.1109/jproc.2021.3067593},
  pages = {911--934},
  source = {Crossref},
  volume = {109},
  author = {Davies, Mike and Wild, Andreas and Orchard, Garrick and Sandamirskaya, Yulia and Guerra, Gabriel A. Fonseca and Joshi, Prasad and Plank, Philipp and Risbud, Sumedh R.},
  date = {2021-05},
  url = {https://doi.org/10.1109/jproc.2021.3067593},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Advancing Neuromorphic Computing With Loihi: A Survey of Results and Outlook},
}

@article{dongarra2009evolution,
  author = {Dongarra, Jack J},
  journal = {IBM J. Res. Dev.},
  pages = {3--4},
  title = {The evolution of high performance computing on system z},
  volume = {53},
  year = {2009},
}

@article{duarte2022fastml,
  url = {http://arxiv.org/abs/2207.07958v1},
  date = {2022-07-16},
  title = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},
  author = {Duarte, Javier and Tran, Nhan and Hawks, Ben and Herwig, Christian and Muhizi, Jules and Prakash, Shvetank and Reddi, Vijay Janapa},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/2207.07958},
}

@article{eshraghian2023training,
  number = {9},
  doi = {10.1109/jproc.2023.3308088},
  pages = {1016--1054},
  source = {Crossref},
  volume = {111},
  author = {Eshraghian, Jason K. and Ward, Max and Neftci, Emre O. and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
  date = {2023-09},
  url = {https://doi.org/10.1109/jproc.2023.3308088},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Training Spiking Neural Networks Using Lessons From Deep Learning},
  bdsk-url-1 = {https://doi.org/10.1109/JPROC.2023.3308088},
}

@article{farah2005neuroethics,
  number = {1},
  doi = {10.1016/j.tics.2004.12.001},
  pages = {34--40},
  source = {Crossref},
  volume = {9},
  author = {Farah, Martha J.},
  date = {2005-01},
  url = {https://doi.org/10.1016/j.tics.2004.12.001},
  issn = {1364-6613},
  journal = {Trends in Cognitive Sciences},
  publisher = {Elsevier BV},
  title = {Neuroethics: the practical and the philosophical},
}

@inproceedings{fowers2018configurable,
  doi = {10.1109/isca.2018.00012},
  pages = {1--14},
  source = {Crossref},
  author = {Fowers, Jeremy and Ovtcharov, Kalin and Papamichael, Michael and Massengill, Todd and Liu, Ming and Lo, Daniel and Alkalay, Shlomi and Haselman, Michael and Adams, Logan and Ghandi, Mahdi and Heil, Stephen and Patel, Prerak and Sapek, Adam and Weisz, Gabriel and Woods, Lisa and Lanka, Sitaram and Reinhardt, Steven K. and Caulfield, Adrian M. and Chung, Eric S. and Burger, Doug},
  date = {2018-06},
  url = {https://doi.org/10.1109/isca.2018.00012},
  booktitle = {2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
  publisher = {IEEE},
  title = {A Configurable Cloud-Scale DNN Processor for Real-Time AI},
  organization = {IEEE},
}

@article{furber2016large,
  number = {5},
  doi = {10.1088/1741-2560/13/5/051001},
  pages = {051001},
  source = {Crossref},
  volume = {13},
  author = {Furber, Steve},
  date = {2016-08-16},
  url = {https://doi.org/10.1088/1741-2560/13/5/051001},
  issn = {1741-2560,1741-2552},
  journal = {Journal of Neural Engineering},
  publisher = {IOP Publishing},
  title = {Large-scale neuromorphic computing systems},
}

@article{gale2019state,
  url = {http://arxiv.org/abs/1902.09574v1},
  date = {2019-02-25},
  title = {The State of Sparsity in Deep Neural Networks},
  author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/1902.09574},
}

@inproceedings{gannot1994verilog,
  doi = {10.1109/ivc.1994.323743},
  pages = {86--92},
  source = {Crossref},
  author = {Gannot, G. and Ligthart, M.},
  url = {https://doi.org/10.1109/ivc.1994.323743},
  booktitle = {International Verilog HDL Conference},
  publisher = {IEEE},
  title = {Verilog HDL based FPGA design},
  bdsk-url-1 = {https://doi.org/10.1109/IVC.1994.323743},
}

@article{gates2009flexible,
  number = {5921},
  doi = {10.1126/science.1171230},
  pages = {1566--1567},
  source = {Crossref},
  volume = {323},
  author = {Gates, Byron D.},
  date = {2009-03-20},
  url = {https://doi.org/10.1126/science.1171230},
  issn = {0036-8075,1095-9203},
  journal = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  title = {Flexible Electronics},
}

@article{goodyear2017social,
  number = {3},
  doi = {10.1080/2159676x.2017.1303790},
  pages = {285--302},
  source = {Crossref},
  volume = {9},
  author = {Goodyear, Victoria A.},
  date = {2017-03-28},
  url = {https://doi.org/10.1080/2159676x.2017.1303790},
  issn = {2159-676X,2159-6778},
  journal = {Qualitative Research in Sport, Exercise and Health},
  publisher = {Informa UK Limited},
  title = {Social media, apps and wearable technologies: navigating ethical dilemmas and procedures},
}

@article{gwennap_certus-nx_nodate,
  author = {Gwennap, Linley},
  language = {en},
  title = {Certus-NX Innovates General-Purpose FPGAs},
}

@article{haensch2018next,
  number = {1},
  doi = {10.1109/jproc.2018.2871057},
  pages = {108--122},
  source = {Crossref},
  volume = {107},
  author = {Haensch, Wilfried and Gokmen, Tayfun and Puri, Ruchir},
  date = {2019-01},
  url = {https://doi.org/10.1109/jproc.2018.2871057},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {The Next Generation of Deep Learning Hardware: Analog Computing},
}

@article{hazan2021neuromorphic,
  doi = {10.3389/fnins.2021.627221},
  source = {Crossref},
  volume = {15},
  author = {Hazan, Avi and Ezra Tsur, Elishai},
  date = {2021-02-22},
  url = {https://doi.org/10.3389/fnins.2021.627221},
  issn = {1662-453X},
  journal = {Frontiers in Neuroscience},
  publisher = {Frontiers Media SA},
  title = {Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation},
  pages = {627221},
}

@article{hennessy2019golden,
  number = {2},
  doi = {10.1145/3282307},
  pages = {48--60},
  source = {Crossref},
  volume = {62},
  author = {Hennessy, John L. and Patterson, David A.},
  date = {2019-01-28},
  url = {https://doi.org/10.1145/3282307},
  issn = {0001-0782,1557-7317},
  journal = {Communications of the ACM},
  publisher = {Association for Computing Machinery (ACM)},
  title = {A new golden age for computer architecture},
  abstract = {Innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development will lead the way.},
  copyright = {http://www.acm.org/publications/policies/copyright_policy
#Background},
  language = {en},
}

@article{howard2017mobilenets,
  url = {http://arxiv.org/abs/1704.04861v1},
  date = {2017-04-17},
  title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/1704.04861},
}

@article{huang2010pseudo,
  number = {1},
  doi = {10.1109/ted.2010.2088127},
  pages = {141--150},
  source = {Crossref},
  volume = {58},
  author = {Huang, Tsung-Ching and Fukuda, Kenjiro and Lo, Chun-Ming and Yeh, Yung-Hui and Sekitani, Tsuyoshi and Someya, Takao and Cheng, Kwang-Ting},
  date = {2011-01},
  url = {https://doi.org/10.1109/ted.2010.2088127},
  issn = {0018-9383,1557-9646},
  journal = {IEEE Transactions on Electron Devices},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Pseudo-CMOS: A Design Style for Low-Cost and Robust Flexible Electronics},
}

@incollection{ignatov2018ai,
  doi = {10.1007/978-3-030-11021-5_19},
  pages = {288--314},
  source = {Crossref},
  author = {Ignatov, Andrey and Timofte, Radu and Chou, William and Wang, Ke and Wu, Max and Hartley, Tim and Van Gool, Luc},
  date = {2019},
  isbn = {9783030110208,9783030110215},
  url = {https://doi.org/10.1007/978-3-030-11021-5_19},
  issn = {0302-9743,1611-3349},
  booktitle = {Computer Vision – ECCV 2018 Workshops},
  publisher = {Springer International Publishing},
  title = {AI Benchmark: Running Deep Neural Networks on Android Smartphones},
  abstract = {Over the last years, the computational power of mobile devices such as smartphones and tablets has grown dramatically, reaching the level of desktop computers available not long ago. While standard smartphone apps are no longer a problem for them, there is still a group of tasks that can easily challenge even high-end devices, namely running artificial intelligence algorithms. In this paper, we present a study of the current state of deep learning in the Android ecosystem and describe available frameworks, programming models and the limitations of running AI on smartphones. We give an overview of the hardware acceleration resources available on four main mobile chipset platforms: Qualcomm, HiSilicon, MediaTek and Samsung. Additionally, we present the real-world performance results of different mobile SoCs collected with AI Benchmark that are covering all main existing hardware configurations.},
}

@inproceedings{imani2016resistive,
  doi = {10.3850/9783981537079_0454},
  pages = {1327--1332},
  source = {Crossref},
  author = {Imani, Mohsen and Rahimi, Abbas and S. Rosing, Tajana},
  date = {2016},
  url = {https://doi.org/10.3850/9783981537079_0454},
  booktitle = {Proceedings of the 2016 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition (DATE)},
  publisher = {Research Publishing Services},
  title = {Resistive Configurable Associative Memory for Approximate Computing},
  organization = {IEEE},
}

@inproceedings{jacob2018quantization,
  doi = {10.1109/cvpr.2018.00286},
  pages = {2704--2713},
  source = {Crossref},
  author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  date = {2018-06},
  url = {https://doi.org/10.1109/cvpr.2018.00286},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/cvpr/JacobKCZTHAK18.bib},
  timestamp = {Wed, 06 Feb 2019 00:00:00 +0100},
}

@article{jia2018dissecting,
  url = {http://arxiv.org/abs/1804.06826v1},
  date = {2018-04-18},
  title = {Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking},
  author = {Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P.},
  primaryclass = {cs.DC},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/1804.06826},
}

@inproceedings{jia2019beyond,
  author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  editor = {Talwalkar, Ameet and Smith, Virginia and Zaharia, Matei},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/mlsys/JiaZA19.bib},
  booktitle = {Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019},
  publisher = {mlsys.org},
  timestamp = {Thu, 18 Jun 2020 01:00:00 +0200},
  title = {Beyond Data and Model Parallelism for Deep Neural Networks},
  url = {https://proceedings.mlsys.org/book/265.pdf},
  year = {2019},
}

@inproceedings{jouppi2017datacenter,
  doi = {10.1145/3079856.3080246},
  source = {Crossref},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  date = {2017-06-24},
  url = {https://doi.org/10.1145/3079856.3080246},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC—called a Tensor Processing Unit (TPU) — deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95
},
  address = {New York, NY, USA},
  bdsk-url-1 = {https://doi.org/10.1145/3079856.3080246},
  isbn = {9781450348928},
  keywords = {accelerator, neural network, MLP, TPU, CNN, deep learning, domain-specific architecture, GPU, TensorFlow, DNN, RNN, LSTM},
  location = {Toronto, ON, Canada},
  numpages = {12},
  pages = {1--12},
  series = {ISCA '17},
}

@inproceedings{jouppi2017indatacenter,
  doi = {10.1145/3079856.3080246},
  source = {Crossref},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  date = {2017-06-24},
  url = {https://doi.org/10.1145/3079856.3080246},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC—called a Tensor Processing Unit (TPU) — deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95
},
  address = {New York, NY, USA},
  bdsk-url-1 = {https://doi.org/10.1145/3079856.3080246},
  isbn = {9781450348928},
  keywords = {accelerator, neural network, MLP, TPU, CNN, deep learning, domain-specific architecture, GPU, TensorFlow, DNN, RNN, LSTM},
  location = {Toronto, ON, Canada},
  numpages = {12},
  pages = {1--12},
  series = {ISCA '17},
}

@inproceedings{jouppi2023tpu,
  doi = {10.1145/3579371.3589350},
  pages = {1--14},
  source = {Crossref},
  author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
  date = {2023-06-17},
  url = {https://doi.org/10.1145/3579371.3589350},
  booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
  abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are lt;5
},
  address = {New York, NY, USA},
  articleno = {82},
  bdsk-url-1 = {https://doi.org/10.1145/3579371.3589350},
  isbn = {9798400700958},
  keywords = {warehouse scale computer, embeddings, supercomputer, domain specific architecture, reconfigurable, TPU, large language model, power usage effectiveness, CO2 equivalent emissions, energy, optical interconnect, IPU, machine learning, GPU, carbon emissions},
  location = {Orlando, FL, USA},
  numpages = {14},
  series = {ISCA '23},
}

@inproceedings{kao2020confuciux,
  doi = {10.1109/micro50266.2020.00058},
  pages = {622--636},
  source = {Crossref},
  author = {Kao, Sheng-Chun and Jeong, Geonhwa and Krishna, Tushar},
  date = {2020-10},
  url = {https://doi.org/10.1109/micro50266.2020.00058},
  booktitle = {2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  publisher = {IEEE},
  title = {ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning},
  organization = {IEEE},
}

@inproceedings{kao2020gamma,
  doi = {10.1145/3400302.3415639},
  pages = {1--9},
  source = {Crossref},
  author = {Kao, Sheng-Chun and Krishna, Tushar},
  subtitle = {automating the HW mapping of DNN models on accelerators via genetic algorithm},
  date = {2020-11-02},
  url = {https://doi.org/10.1145/3400302.3415639},
  booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
  publisher = {ACM},
  title = {GAMMA},
}

@article{krishnan2022multiagent,
  url = {http://arxiv.org/abs/2211.16385v1},
  date = {2022-11-29},
  title = {Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration},
  author = {Krishnan, Srivatsan and Jaques, Natasha and Omidshafiei, Shayegan and Zhang, Dan and Gur, Izzeddin and Reddi, Vijay Janapa and Faust, Aleksandra},
  primaryclass = {cs.AR},
  archiveprefix = {arXiv},
  eprint = {2211.16385},
}

@inproceedings{krishnan2023archgym,
  doi = {10.1145/3579371.3589049},
  pages = {1--16},
  source = {Crossref},
  author = {Krishnan, Srivatsan and Yazdanbakhsh, Amir and Prakash, Shvetank and Jabbour, Jason and Uchendu, Ikechukwu and Ghosh, Susobhan and Boroujerdian, Behzad and Richins, Daniel and Tripathy, Devashree and Faust, Aleksandra and Janapa Reddi, Vijay},
  date = {2023-06-17},
  url = {https://doi.org/10.1145/3579371.3589049},
  booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
  publisher = {ACM},
  title = {ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
}

@article{kwon2022flexible,
  doi = {10.1016/j.nanoen.2022.107632},
  pages = {107632},
  source = {Crossref},
  volume = {102},
  author = {Kwon, Sun Hwa and Dong, Lin},
  date = {2022-11},
  url = {https://doi.org/10.1016/j.nanoen.2022.107632},
  issn = {2211-2855},
  journal = {Nano Energy},
  publisher = {Elsevier BV},
  title = {Flexible sensors and machine learning for heart monitoring},
}

@inproceedings{lin2022ondevice,
  doi = {10.1145/3613424.3614307},
  pages = {1381--1394},
  source = {Crossref},
  author = {Zhu, Ligeng and Hu, Lanxiang and Lin, Ji and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
  date = {2023-10-28},
  url = {https://doi.org/10.1145/3613424.3614307},
  booktitle = {56th Annual IEEE/ACM International Symposium on Microarchitecture},
  publisher = {ACM},
  title = {PockEngine: Sparse and Efficient Fine-tuning in a Pocket},
}

@article{lin2023awq,
  number = {4},
  doi = {10.1145/3714983.3714987},
  pages = {12--17},
  source = {Crossref},
  volume = {28},
  author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Xiao, Guangxuan and Han, Song},
  date = {2025-01-20},
  url = {https://doi.org/10.1145/3714983.3714987},
  issn = {2375-0529,2375-0537},
  journal = {GetMobile: Mobile Computing and Communications},
  publisher = {Association for Computing Machinery (ACM)},
  title = {AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
}

@article{lindholm2008nvidia,
  number = {2},
  doi = {10.1109/mm.2008.31},
  pages = {39--55},
  source = {Crossref},
  volume = {28},
  author = {Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
  date = {2008-03},
  url = {https://doi.org/10.1109/mm.2008.31},
  issn = {0272-1732},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {NVIDIA Tesla: A Unified Graphics and Computing Architecture},
  abstract = {To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs.},
  bdsk-url-1 = {https://ieeexplore.ieee.org/document/4523358},
  bdsk-url-2 = {https://doi.org/10.1109/MM.2008.31},
  note = {Conference Name: IEEE Micro},
  shorttitle = {NVIDIA Tesla},
  urldate = {2023-11-07},
}

@article{loh20083dstacked,
  number = {3},
  doi = {10.1145/1394608.1382159},
  pages = {453--464},
  source = {Crossref},
  volume = {36},
  author = {Loh, Gabriel H.},
  date = {2008-06},
  url = {https://doi.org/10.1145/1394608.1382159},
  issn = {0163-5964},
  journal = {ACM SIGARCH Computer Architecture News},
  publisher = {Association for Computing Machinery (ACM)},
  title = {3D-Stacked Memory Architectures for Multi-core Processors},
}

@inproceedings{luebke2008cuda,
  doi = {10.1109/isbi.2008.4541126},
  source = {Crossref},
  author = {Luebke, David},
  date = {2008-05},
  url = {https://doi.org/10.1109/isbi.2008.4541126},
  booktitle = {2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
  publisher = {IEEE},
  title = {CUDA: Scalable parallel programming for high-performance scientific computing},
  bdsk-url-1 = {https://doi.org/10.1109/ISBI.2008.4541126},
  pages = {836--838},
}

@article{maass1997networks,
  number = {9},
  doi = {10.1016/s0893-6080(97)00011-7},
  pages = {1659--1671},
  source = {Crossref},
  volume = {10},
  author = {Maass, Wolfgang},
  date = {1997-12},
  url = {https://doi.org/10.1016/s0893-6080(97)00011-7},
  issn = {0893-6080},
  journal = {Neural Networks},
  publisher = {Elsevier BV},
  title = {Networks of spiking neurons: The third generation of neural network models},
}

@article{markovic2020physics,
  number = {9},
  doi = {10.1038/s42254-020-0208-2},
  pages = {499--510},
  source = {Crossref},
  volume = {2},
  author = {Marković, Danijela and Mizrahi, Alice and Querlioz, Damien and Grollier, Julie},
  date = {2020-07-28},
  url = {https://doi.org/10.1038/s42254-020-0208-2},
  issn = {2522-5820},
  journal = {Nature Reviews Physics},
  publisher = {Springer Science and Business Media LLC},
  title = {Physics for neuromorphic computing},
}

@article{mattson2020mlperf,
  number = {2},
  doi = {10.1109/mm.2020.2974843},
  pages = {8--16},
  source = {Crossref},
  volume = {40},
  author = {Mattson, Peter and Reddi, Vijay Janapa and Cheng, Christine and Coleman, Cody and Diamos, Greg and Kanter, David and Micikevicius, Paulius and Patterson, David and Schmuelling, Guenther and Tang, Hanlin and Wei, Gu-Yeon and Wu, Carole-Jean},
  date = {2020-03-01},
  url = {https://doi.org/10.1109/mm.2020.2974843},
  issn = {0272-1732,1937-4143},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance},
}

@article{miller2000optical,
  number = {6},
  doi = {10.1109/2944.902184},
  pages = {1312--1317},
  source = {Crossref},
  volume = {6},
  author = {Miller, D.A.B.},
  date = {2000-11},
  url = {https://doi.org/10.1109/2944.902184},
  issn = {1077-260X,1558-4542},
  journal = {IEEE Journal of Selected Topics in Quantum Electronics},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Optical interconnects to silicon},
}

@article{mirhoseini2021graph,
  number = {7862},
  doi = {10.1038/s41586-021-03544-w},
  pages = {207--212},
  source = {Crossref},
  volume = {594},
  author = {Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Nova, Azade and Pak, Jiwoo and Tong, Andy and Srinivasa, Kavya and Hang, William and Tuncer, Emre and Le, Quoc V. and Laudon, James and Ho, Richard and Carpenter, Roger and Dean, Jeff},
  date = {2021-06-09},
  url = {https://doi.org/10.1038/s41586-021-03544-w},
  issn = {0028-0836,1476-4687},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  title = {A graph placement methodology for fast chip
            design},
}

@article{mittal2021survey,
  doi = {10.1016/j.sysarc.2021.102276},
  pages = {102276},
  source = {Crossref},
  volume = {119},
  author = {Mittal, Sparsh and Verma, Gaurav and Kaushik, Brajesh and Khanday, Farooq A.},
  date = {2021-10},
  url = {https://doi.org/10.1016/j.sysarc.2021.102276},
  issn = {1383-7621},
  journal = {Journal of Systems Architecture},
  publisher = {Elsevier BV},
  title = {A survey of SRAM-based in-memory computing techniques and applications},
}

@article{modha2023neural,
  number = {6668},
  doi = {10.1126/science.adh1174},
  pages = {329--335},
  source = {Crossref},
  volume = {382},
  author = {Modha, Dharmendra S. and Akopyan, Filipp and Andreopoulos, Alexander and Appuswamy, Rathinakumar and Arthur, John V. and Cassidy, Andrew S. and Datta, Pallab and DeBole, Michael V. and Esser, Steven K. and Otero, Carlos Ortega and Sawada, Jun and Taba, Brian and Amir, Arnon and Bablani, Deepika and Carlson, Peter J. and Flickner, Myron D. and Gandhasri, Rajamohan and Garreau, Guillaume J. and Ito, Megumi and Klamo, Jennifer L. and Kusnitz, Jeffrey A. and McClatchey, Nathaniel J. and McKinstry, Jeffrey L. and Nakamura, Yutaka and Nayak, Tapan K. and Risk, William P. and Schleupen, Kai and Shaw, Ben and Sivagnaname, Jay and Smith, Daniel F. and Terrizzano, Ignacio and Ueda, Takanori},
  date = {2023-10-20},
  url = {https://doi.org/10.1126/science.adh1174},
  issn = {0036-8075,1095-9203},
  journal = {Science},
  publisher = {American Association for the Advancement of Science (AAAS)},
  title = {Neural inference at the frontier of energy, space, and time},
}

@inproceedings{munshi2009opencl,
  doi = {10.1109/hotchips.2009.7478342},
  source = {Crossref},
  author = {Munshi, Aaftab},
  date = {2009-08},
  url = {https://doi.org/10.1109/hotchips.2009.7478342},
  booktitle = {2009 IEEE Hot Chips 21 Symposium (HCS)},
  publisher = {IEEE},
  title = {The OpenCL specification},
  bdsk-url-1 = {https://doi.org/10.1109/HOTCHIPS.2009.7478342},
  pages = {1--314},
}

@article{musk2019integrated,
  number = {10},
  doi = {10.2196/16194},
  pages = {e16194},
  source = {Crossref},
  volume = {21},
  author = {Musk, Elon and },
  date = {2019-10-31},
  url = {https://doi.org/10.2196/16194},
  issn = {1438-8871},
  journal = {Journal of Medical Internet Research},
  publisher = {JMIR Publications Inc.},
  title = {An Integrated Brain-Machine Interface Platform With Thousands of Channels},
}

@article{norrie2021design,
  number = {2},
  doi = {10.1109/mm.2021.3058217},
  pages = {56--63},
  source = {Crossref},
  volume = {41},
  author = {Norrie, Thomas and Patil, Nishant and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Laudon, James and Young, Cliff and Jouppi, Norman and Patterson, David},
  date = {2021-03-01},
  url = {https://doi.org/10.1109/mm.2021.3058217},
  issn = {0272-1732,1937-4143},
  journal = {IEEE Micro},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {The Design Process for Google's Training Chips: TPUv2 and TPUv3},
  bdsk-url-1 = {https://doi.org/10.1109/MM.2021.3058217},
}

@book{patterson2016computer,
  author = {Patterson, David A and Hennessy, John L},
  publisher = {Morgan kaufmann},
  title = {Computer organization and design ARM edition: The hardware software interface},
  year = {2016},
}

@article{putnam2014reconfigurable,
  number = {3},
  doi = {10.1145/2678373.2665678},
  pages = {13--24},
  source = {Crossref},
  volume = {42},
  author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
  date = {2014-06-14},
  url = {https://doi.org/10.1145/2678373.2665678},
  issn = {0163-5964},
  journal = {ACM SIGARCH Computer Architecture News},
  publisher = {Association for Computing Machinery (ACM)},
  title = {A reconfigurable fabric for accelerating large-scale datacenter services},
  abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we have designed and built a composable, reconfigurablefabric to accelerate portions of large-scale software services. Each instantiation of the fabric consists of a 6x8 2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One FPGA is placed into each server, accessible through PCIe, and wired directly to other FPGAs with pairs of 10 Gb SAS cables In this paper, we describe a medium-scale deployment of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance, power, and resilience of the system when ranking candidate documents. Under high load, the largescale reconfigurable fabric improves the ranking throughput of each server by a factor of 95
},
  bdsk-url-1 = {https://dl.acm.org/doi/10.1145/2678373.2665678},
  bdsk-url-2 = {https://doi.org/10.1145/2678373.2665678},
  language = {en},
  urldate = {2023-11-07},
}

@inproceedings{rajat2009largescale,
  doi = {10.1145/1553374.1553486},
  source = {Crossref},
  author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
  date = {2009-06-14},
  url = {https://doi.org/10.1145/1553374.1553486},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  publisher = {ACM},
  title = {Large-scale deep unsupervised learning using graphics processors},
  editor = {Danyluk, Andrea Pohoreckyj and Bottou, Léon and Littman, Michael L.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/icml/RainaMN09.bib},
  pages = {873--880},
  series = {ACM International Conference Proceeding Series},
  timestamp = {Wed, 14 Nov 2018 00:00:00 +0100},
  volume = {382},
}

@article{ranganathan2011from,
  number = {1},
  doi = {10.1109/mc.2011.18},
  pages = {39--48},
  source = {Crossref},
  volume = {44},
  author = {Ranganathan, Parthasarathy},
  date = {2011-01},
  url = {https://doi.org/10.1109/mc.2011.18},
  issn = {0018-9162},
  journal = {Computer},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {From Microprocessors to Nanostores: Rethinking Data-Centric Systems},
}

@inproceedings{reagen2017case,
  doi = {10.1109/islped.2017.8009208},
  source = {Crossref},
  author = {Reagen, Brandon and Hernandez-Lobato, Jose Miguel and Adolf, Robert and Gelbart, Michael and Whatmough, Paul and Wei, Gu-Yeon and Brooks, David},
  date = {2017-07},
  url = {https://doi.org/10.1109/islped.2017.8009208},
  booktitle = {2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)},
  publisher = {IEEE},
  title = {A case for efficient accelerator design space exploration via Bayesian optimization},
  organization = {IEEE},
  pages = {1--6},
}

@inproceedings{reddi2020mlperf,
  doi = {10.1109/isca45697.2020.00045},
  pages = {446--459},
  source = {Crossref},
  author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
  date = {2020-05},
  url = {https://doi.org/10.1109/isca45697.2020.00045},
  booktitle = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  publisher = {IEEE},
  title = {MLPerf Inference Benchmark},
  organization = {IEEE},
}

@article{roskies2002neuroethics,
  number = {1},
  doi = {10.1016/s0896-6273(02)00763-8},
  pages = {21--23},
  source = {Crossref},
  volume = {35},
  author = {Roskies, Adina},
  date = {2002-07},
  url = {https://doi.org/10.1016/s0896-6273(02)00763-8},
  issn = {0896-6273},
  journal = {Neuron},
  publisher = {Elsevier BV},
  title = {Neuroethics for the New Millenium},
}

@article{samajdar2018scale,
  url = {http://arxiv.org/abs/1811.02883v2},
  date = {2018-10-16},
  title = {SCALE-Sim: Systolic CNN Accelerator Simulator},
  author = {Samajdar, Ananda and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
  primaryclass = {cs.DC},
  archiveprefix = {arXiv},
  journal = {ArXiv preprint},
  volume = {abs/1811.02883},
}

@article{schuman2022opportunities,
  number = {1},
  doi = {10.1038/s43588-021-00184-y},
  pages = {10--19},
  source = {Crossref},
  volume = {2},
  author = {Schuman, Catherine D. and Kulkarni, Shruti R. and Parsa, Maryam and Mitchell, J. Parker and Date, Prasanna and Kay, Bill},
  date = {2022-01-31},
  url = {https://doi.org/10.1038/s43588-021-00184-y},
  issn = {2662-8457},
  journal = {Nature Computational Science},
  publisher = {Springer Science and Business Media LLC},
  title = {Opportunities for neuromorphic computing algorithms and applications},
}

@misc{segal1999opengl,
  author = {Segal, Mark and Akeley, Kurt},
  title = {The OpenGL graphics system: A specification (version 1.1)},
  year = {1999},
}

@article{segura2018ethical,
  number = {1},
  doi = {10.1007/s11948-017-9872-8},
  pages = {1--28},
  source = {Crossref},
  volume = {24},
  author = {Segura Anaya, L. H. and Alsadoon, Abeer and Costadopoulos, N. and Prasad, P. W. C.},
  date = {2017-02-02},
  url = {https://doi.org/10.1007/s11948-017-9872-8},
  issn = {1353-3452,1471-5546},
  journal = {Science and Engineering Ethics},
  publisher = {Springer Science and Business Media LLC},
  title = {Ethical Implications of User Perceptions of Wearable Devices},
}

@article{shastri2021photonics,
  number = {2},
  doi = {10.1038/s41566-020-00754-y},
  pages = {102--114},
  source = {Crossref},
  volume = {15},
  author = {Shastri, Bhavin J. and Tait, Alexander N. and Ferreira de Lima, T. and Pernice, Wolfram H. P. and Bhaskaran, Harish and Wright, C. D. and Prucnal, Paul R.},
  date = {2021-01-29},
  url = {https://doi.org/10.1038/s41566-020-00754-y},
  issn = {1749-4885,1749-4893},
  journal = {Nature Photonics},
  publisher = {Springer Science and Business Media LLC},
  title = {Photonics for artificial intelligence and neuromorphic computing},
}

@inproceedings{suda2016throughput,
  doi = {10.1145/2847263.2847276},
  source = {Crossref},
  author = {Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
  date = {2016-02-21},
  url = {https://doi.org/10.1145/2847263.2847276},
  booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  publisher = {ACM},
  title = {Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks},
  pages = {16--25},
}

@article{sze2017efficient,
  url = {http://arxiv.org/abs/1703.09039v2},
  date = {2017-03-27},
  title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel},
  primaryclass = {cs.CV},
  archiveprefix = {arXiv},
  abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
  copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
  doi = {10.1109/jproc.2017.2761740},
  eprint = {1703.09039},
  issn = {0018-9219,1558-2256},
  journal = {Proc. IEEE},
  number = {12},
  pages = {2295--2329},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  source = {Crossref},
  volume = {105},
}

@article{tang2022soft,
  number = {1},
  doi = {10.1063/5.0069516},
  source = {Crossref},
  volume = {3},
  author = {Tang, Xin and He, Yichun and Liu, Jia},
  date = {2022-01-12},
  url = {https://doi.org/10.1063/5.0069516},
  issn = {2688-4089},
  journal = {Biophysics Reviews},
  publisher = {AIP Publishing},
  title = {Soft bioelectronics for cardiac interfaces},
}

@article{tang2023flexible,
  number = {2},
  doi = {10.1038/s41928-022-00913-9},
  pages = {109--118},
  source = {Crossref},
  volume = {6},
  author = {Tang, Xin and Shen, Hao and Zhao, Siyuan and Li, Na and Liu, Jia},
  date = {2023-02-02},
  url = {https://doi.org/10.1038/s41928-022-00913-9},
  issn = {2520-1131},
  journal = {Nature Electronics},
  publisher = {Springer Science and Business Media LLC},
  title = {Flexible brain–computer interfaces},
}

@incollection{valenzuela2000genetic,
  doi = {10.1007/3-540-45356-3_66},
  pages = {671--680},
  source = {Crossref},
  author = {Valenzuela, Christine L. and Wang, Pearl Y.},
  date = {2000},
  isbn = {9783540410560,9783540453567},
  url = {https://doi.org/10.1007/3-540-45356-3_66},
  issn = {0302-9743},
  booktitle = {Parallel Problem Solving from Nature PPSN VI},
  publisher = {Springer Berlin Heidelberg},
  title = {A Genetic Algorithm for VLSI Floorplanning},
  organization = {Springer},
}

@article{verma2019memory,
  number = {3},
  doi = {10.1109/mssc.2019.2922889},
  pages = {43--55},
  source = {Crossref},
  volume = {11},
  author = {Verma, Naveen and Jia, Hongyang and Valavi, Hossein and Tang, Yinqi and Ozatay, Murat and Chen, Lung-Yen and Zhang, Bonan and Deaville, Peter},
  date = {2019},
  url = {https://doi.org/10.1109/mssc.2019.2922889},
  issn = {1943-0582,1943-0590},
  journal = {IEEE Solid-State Circuits Magazine},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {In-Memory Computing: Advances and Prospects},
}

@article{vivet2021intact,
  number = {1},
  doi = {10.1109/jssc.2020.3036341},
  pages = {79--97},
  source = {Crossref},
  volume = {56},
  author = {Vivet, Pascal and Guthmuller, Eric and Thonnart, Yvain and Pillonnet, Gael and Fuguet, Cesar and Miro-Panades, Ivan and Moritz, Guillaume and Durupt, Jean and Bernard, Christian and Varreau, Didier and Pontes, Julian and Thuries, Sebastien and Coriat, David and Harrand, Michel and Dutoit, Denis and Lattard, Didier and Arnaud, Lucile and Charbonnier, Jean and Coudrain, Perceval and Garnier, Arnaud and Berger, Frederic and Gueugnot, Alain and Greiner, Alain and Meunier, Quentin L. and Farcy, Alexis and Arriordaz, Alexandre and Cheramy, Severine and Clermidy, Fabien},
  date = {2021-01},
  url = {https://doi.org/10.1109/jssc.2020.3036341},
  issn = {0018-9200,1558-173X},
  journal = {IEEE Journal of Solid-State Circuits},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management},
  bdsk-url-1 = {https://doi.org/10.1109/JSSC.2020.3036341},
}

@inproceedings{wang2020apq,
  doi = {10.1109/cvpr42600.2020.00215},
  pages = {2075--2084},
  source = {Crossref},
  author = {Wang, Tianzhe and Wang, Kuan and Cai, Han and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Lin, Yujun and Han, Song},
  date = {2020-06},
  url = {https://doi.org/10.1109/cvpr42600.2020.00215},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  title = {APQ: Joint Search for Network Architecture, Pruning and Quantization Policy},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/cvpr/WangWCLL0LH20.bib},
  timestamp = {Tue, 22 Dec 2020 00:00:00 +0100},
}

@techreport{weik1955survey,
  doi = {10.21236/ad0253212},
  source = {Crossref},
  author = {Weik, Martin H.},
  date = {1961-03-01},
  url = {https://doi.org/10.21236/ad0253212},
  institution = {Defense Technical Information Center},
  title = {A THIRD SURVEY OF DOMESTIC ELECTRONIC DIGITAL COMPUTING SYSTEMS},
  language = {en},
  publisher = {Ballistic Research Laboratories},
}

@article{wong2012metal,
  number = {6},
  doi = {10.1109/jproc.2012.2190369},
  pages = {1951--1970},
  source = {Crossref},
  volume = {100},
  author = {Wong, H.-S. Philip and Lee, Heng-Yuan and Yu, Shimeng and Chen, Yu-Sheng and Wu, Yi and Chen, Pang-Shiu and Lee, Byoungil and Chen, Frederick T. and Tsai, Ming-Jinn},
  date = {2012-06},
  url = {https://doi.org/10.1109/jproc.2012.2190369},
  issn = {0018-9219,1558-2256},
  journal = {Proceedings of the IEEE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Metal–Oxide RRAM},
}

@article{xiong2021mribased,
  number = {1},
  doi = {10.1186/s12859-021-04347-6},
  source = {Crossref},
  volume = {22},
  author = {Xiong, Siyu and Wu, Guoqing and Fan, Xitian and Feng, Xuan and Huang, Zhongcheng and Cao, Wei and Zhou, Xuegong and Ding, Shijin and Yu, Jinhua and Wang, Lingli and Shi, Zhifeng},
  date = {2021-09-07},
  url = {https://doi.org/10.1186/s12859-021-04347-6},
  issn = {1471-2105},
  journal = {BMC Bioinformatics},
  publisher = {Springer Science and Business Media LLC},
  title = {MRI-based brain tumor segmentation using FPGA-accelerated neural network},
  abstract = {Brain tumor segmentation is a challenging problem in medical image processing and analysis. It is a very time-consuming and error-prone task. In order to reduce the burden on physicians and improve the segmentation accuracy, the computer-aided detection (CAD) systems need to be developed. Due to the powerful feature learning ability of the deep learning technology, many deep learning-based methods have been applied to the brain tumor segmentation CAD systems and achieved satisfactory accuracy. However, deep learning neural networks have high computational complexity, and the brain tumor segmentation process consumes significant time. Therefore, in order to achieve the high segmentation accuracy of brain tumors and obtain the segmentation results efficiently, it is very demanding to speed up the segmentation process of brain tumors.},
  bdsk-url-1 = {https://doi.org/10.1186/s12859-021-04347-6},
  keywords = {Brain tumor segmatation, FPGA acceleration, Neural network},
  pages = {421},
  urldate = {2023-11-07},
}

@article{xiu2019time,
  number = {1},
  doi = {10.1109/mssc.2018.2882285},
  pages = {39--55},
  source = {Crossref},
  volume = {11},
  author = {Xiu, Liming},
  date = {2019},
  url = {https://doi.org/10.1109/mssc.2018.2882285},
  issn = {1943-0582,1943-0590},
  journal = {IEEE Solid-State Circuits Magazine},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Time Moore: Exploiting Moore's Law From The Perspective of Time},
}

@article{young2018recent,
  number = {3},
  doi = {10.1109/mci.2018.2840738},
  pages = {55--75},
  source = {Crossref},
  volume = {13},
  author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  date = {2018-08},
  url = {https://doi.org/10.1109/mci.2018.2840738},
  issn = {1556-603X,1556-6048},
  journal = {IEEE Computational Intelligence Magazine},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  title = {Recent Trends in Deep Learning Based Natural Language Processing [Review Article]},
}

@article{yu2023rl,
  number = {2},
  doi = {10.1145/3632174},
  pages = {1--33},
  source = {Crossref},
  volume = {29},
  author = {Qian, Yu and Zhou, Xuegong and Zhou, Hao and Wang, Lingli},
  date = {2024-01-15},
  url = {https://doi.org/10.1145/3632174},
  issn = {1084-4309,1557-7309},
  journal = {ACM Transactions on Design Automation of Electronic Systems},
  publisher = {Association for Computing Machinery (ACM)},
  title = {An Efficient Reinforcement Learning Based Framework for Exploring Logic Synthesis},
  abstract = {Logic synthesis is a crucial step in electronic design automation tools. The rapid developments of reinforcement learning (RL) have enabled the automated exploration of logic synthesis. Existing RL based methods may lead to data inefficiency, and the exploration approaches for FPGA and ASIC technology mapping in recent works lack the flexibility of the learning process. This work proposes ESE, a reinforcement learning based framework to efficiently learn the logic synthesis process. The framework supports the modeling of logic optimization and technology mapping for FPGA and ASIC. The optimization for the execution time of the synthesis script is also considered. For the modeling of FPGA mapping, the logic optimization and technology mapping are combined to be learned in a flexible way. For the modeling of ASIC mapping, the standard cell based optimization and LUT optimization operations are incorporated into the ASIC synthesis flow. To improve the utilization of samples, the Proximal Policy Optimization model is adopted. Furthermore, the framework is enhanced by supporting MIG based synthesis exploration. Experiments show that for FPGA technology mapping on the VTR benchmark, the average LUT-Level-Product and script runtime are improved by more than 18.3
},
  address = {New York, NY, USA},
  keywords = {technology mapping, Majority-Inverter Graph, And-Inverter Graph, Reinforcement learning, logic optimization},
  note = {Just Accepted},
}

@inproceedings{zhang2015fpga,
  author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason Optimizing},
  booktitle = {SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA},
  pages = {161--170},
  title = {FPGA-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 ACM},
  volume = {15},
  year = {2015},
}

@inproceedings{zhang2022fullstack,
  doi = {10.1145/3503222.3507767},
  pages = {27--42},
  source = {Crossref},
  author = {Zhang, Dan and Huda, Safeen and Songhori, Ebrahim and Prabhu, Kartik and Le, Quoc and Goldie, Anna and Mirhoseini, Azalia},
  date = {2022-02-22},
  url = {https://doi.org/10.1145/3503222.3507767},
  booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  publisher = {ACM},
  title = {A full-stack search technique for domain optimized deep learning accelerators},
  abstract = {The rapidly-changing deep learning landscape presents a unique opportunity for building inference accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. In this paper, we analyze bottlenecks in state-of-the-art vision and natural language processing (NLP) models, including EfficientNet and BERT, and use FAST to design accelerators capable of addressing these bottlenecks. FAST-generated accelerators optimized for single workloads improve Perf/TDP by 3.7× on average across all benchmarks compared to TPU-v3. A FAST-generated accelerator optimized for serving a suite of workloads improves Perf/TDP by 2.4× on average compared to TPU-v3. Our return on investment analysis shows that FAST-generated accelerators can potentially be practical for moderate-sized datacenter deployments.},
  address = {New York, NY, USA},
  isbn = {9781450392051},
  keywords = {design space exploration, hardware-software codesign, tensor processing unit, machine learning, operation fusion},
  location = {Lausanne, Switzerland},
  numpages = {16},
  series = {ASPLOS '22},
}

@article{zhou2022photonic,
  number = {1},
  doi = {10.1038/s41377-022-00717-8},
  source = {Crossref},
  volume = {11},
  author = {Zhou, Hailong and Dong, Jianji and Cheng, Junwei and Dong, Wenchan and Huang, Chaoran and Shen, Yichen and Zhang, Qiming and Gu, Min and Qian, Chao and Chen, Hongsheng and Ruan, Zhichao and Zhang, Xinliang},
  date = {2022-02-03},
  url = {https://doi.org/10.1038/s41377-022-00717-8},
  issn = {2047-7538},
  journal = {Light: Science \&amp; Applications},
  publisher = {Springer Science and Business Media LLC},
  title = {Photonic matrix multiplication lights up photonic accelerator and beyond},
  pages = {30},
}

@inproceedings{zhou2023area,
  doi = {10.1145/3566097.3567894},
  pages = {159--165},
  source = {Crossref},
  author = {Zhou, Guanglei and Anderson, Jason H.},
  date = {2023-01-16},
  url = {https://doi.org/10.1145/3566097.3567894},
  booktitle = {Proceedings of the 28th Asia and South Pacific Design Automation Conference},
  publisher = {ACM},
  title = {Area-Driven FPGA Logic Synthesis Using Reinforcement Learning},
}

@inproceedings{zhu2018benchmarking,
  doi = {10.1109/iiswc.2018.8573476},
  pages = {88--100},
  source = {Crossref},
  author = {Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
  date = {2018-09},
  url = {https://doi.org/10.1109/iiswc.2018.8573476},
  booktitle = {2018 IEEE International Symposium on Workload Characterization (IISWC)},
  publisher = {IEEE},
  title = {Benchmarking and Analyzing Deep Neural Network Training},
  organization = {IEEE},
}

@article{rayis2014,
  author = {El-Rayis, A.O.},
  title = {Reconfigurable architectures for the next generation of mobile device telecommunications systems},
  year = {2014},
  url = {: https://www.researchgate.net/publication/292608967},
}