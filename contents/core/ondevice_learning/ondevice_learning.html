<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/robust_ai/robust_ai.html" rel="next">
<link href="../../../contents/core/ops/ops.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-cbc843cc95873402613d6df7a37f2654.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
</style>
<style>
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/ops/ops.html">Robust Deployment</a></li><li class="breadcrumb-item"><a href="../../../contents/core/ondevice_learning/ondevice_learning.html">On-Device Learning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#on-device-learning" id="toc-on-device-learning" class="nav-link active" data-scroll-target="#on-device-learning">On-Device Learning</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-ondevice-learning-overview-3555" id="toc-sec-ondevice-learning-overview-3555" class="nav-link" data-scroll-target="#sec-ondevice-learning-overview-3555">Overview</a></li>
  <li><a href="#sec-ondevice-learning-deployment-drivers-2eb7" id="toc-sec-ondevice-learning-deployment-drivers-2eb7" class="nav-link" data-scroll-target="#sec-ondevice-learning-deployment-drivers-2eb7">Deployment Drivers</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-ondevice-learning-benefits-5ae7" id="toc-sec-ondevice-learning-ondevice-learning-benefits-5ae7" class="nav-link" data-scroll-target="#sec-ondevice-learning-ondevice-learning-benefits-5ae7">On-Device Learning Benefits</a></li>
  <li><a href="#sec-ondevice-learning-application-domains-1595" id="toc-sec-ondevice-learning-application-domains-1595" class="nav-link" data-scroll-target="#sec-ondevice-learning-application-domains-1595">Application Domains</a></li>
  <li><a href="#sec-ondevice-learning-training-paradigms-af01" id="toc-sec-ondevice-learning-training-paradigms-af01" class="nav-link" data-scroll-target="#sec-ondevice-learning-training-paradigms-af01">Training Paradigms</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-design-constraints-d887" id="toc-sec-ondevice-learning-design-constraints-d887" class="nav-link" data-scroll-target="#sec-ondevice-learning-design-constraints-d887">Design Constraints</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-model-constraints-b63e" id="toc-sec-ondevice-learning-model-constraints-b63e" class="nav-link" data-scroll-target="#sec-ondevice-learning-model-constraints-b63e">Model Constraints</a></li>
  <li><a href="#sec-ondevice-learning-data-constraints-f2ca" id="toc-sec-ondevice-learning-data-constraints-f2ca" class="nav-link" data-scroll-target="#sec-ondevice-learning-data-constraints-f2ca">Data Constraints</a></li>
  <li><a href="#sec-ondevice-learning-compute-constraints-4244" id="toc-sec-ondevice-learning-compute-constraints-4244" class="nav-link" data-scroll-target="#sec-ondevice-learning-compute-constraints-4244">Compute Constraints</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-model-adaptation-0b2b" id="toc-sec-ondevice-learning-model-adaptation-0b2b" class="nav-link" data-scroll-target="#sec-ondevice-learning-model-adaptation-0b2b">Model Adaptation</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-weight-freezing-797c" id="toc-sec-ondevice-learning-weight-freezing-797c" class="nav-link" data-scroll-target="#sec-ondevice-learning-weight-freezing-797c">Weight Freezing</a></li>
  <li><a href="#sec-ondevice-learning-residual-lowrank-updates-2add" id="toc-sec-ondevice-learning-residual-lowrank-updates-2add" class="nav-link" data-scroll-target="#sec-ondevice-learning-residual-lowrank-updates-2add">Residual and Low-Rank Updates</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-adapterbased-adaptation-04de" id="toc-sec-ondevice-learning-adapterbased-adaptation-04de" class="nav-link" data-scroll-target="#sec-ondevice-learning-adapterbased-adaptation-04de">Adapter-Based Adaptation</a></li>
  <li><a href="#sec-ondevice-learning-lowrank-techniques-2817" id="toc-sec-ondevice-learning-lowrank-techniques-2817" class="nav-link" data-scroll-target="#sec-ondevice-learning-lowrank-techniques-2817">Low-Rank Techniques</a></li>
  <li><a href="#sec-ondevice-learning-edge-personalization-a892" id="toc-sec-ondevice-learning-edge-personalization-a892" class="nav-link" data-scroll-target="#sec-ondevice-learning-edge-personalization-a892">Edge Personalization</a></li>
  <li><a href="#sec-ondevice-learning-tradeoffs-bb7b" id="toc-sec-ondevice-learning-tradeoffs-bb7b" class="nav-link" data-scroll-target="#sec-ondevice-learning-tradeoffs-bb7b">Tradeoffs</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-sparse-updates-eefe" id="toc-sec-ondevice-learning-sparse-updates-eefe" class="nav-link" data-scroll-target="#sec-ondevice-learning-sparse-updates-eefe">Sparse Updates</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-sparse-update-design-ecbc" id="toc-sec-ondevice-learning-sparse-update-design-ecbc" class="nav-link" data-scroll-target="#sec-ondevice-learning-sparse-update-design-ecbc">Sparse Update Design</a></li>
  <li><a href="#sec-ondevice-learning-layer-selection-c339" id="toc-sec-ondevice-learning-layer-selection-c339" class="nav-link" data-scroll-target="#sec-ondevice-learning-layer-selection-c339">Layer Selection</a></li>
  <li><a href="#sec-ondevice-learning-code-fragment-selective-layer-updating-pytorch-d094" id="toc-sec-ondevice-learning-code-fragment-selective-layer-updating-pytorch-d094" class="nav-link" data-scroll-target="#sec-ondevice-learning-code-fragment-selective-layer-updating-pytorch-d094">Code Fragment: Selective Layer Updating (PyTorch)</a></li>
  <li><a href="#sec-ondevice-learning-tinytrain-personalization-bdf2" id="toc-sec-ondevice-learning-tinytrain-personalization-bdf2" class="nav-link" data-scroll-target="#sec-ondevice-learning-tinytrain-personalization-bdf2">TinyTrain Personalization</a></li>
  <li><a href="#sec-ondevice-learning-tradeoffs-effd" id="toc-sec-ondevice-learning-tradeoffs-effd" class="nav-link" data-scroll-target="#sec-ondevice-learning-tradeoffs-effd">Tradeoffs</a></li>
  <li><a href="#sec-ondevice-learning-adaptation-strategy-comparison-8bd5" id="toc-sec-ondevice-learning-adaptation-strategy-comparison-8bd5" class="nav-link" data-scroll-target="#sec-ondevice-learning-adaptation-strategy-comparison-8bd5">Adaptation Strategy Comparison</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-data-efficiency-1cee" id="toc-sec-ondevice-learning-data-efficiency-1cee" class="nav-link" data-scroll-target="#sec-ondevice-learning-data-efficiency-1cee">Data Efficiency</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-fewshot-streaming-7b2b" id="toc-sec-ondevice-learning-fewshot-streaming-7b2b" class="nav-link" data-scroll-target="#sec-ondevice-learning-fewshot-streaming-7b2b">Few-Shot and Streaming</a></li>
  <li><a href="#sec-ondevice-learning-experience-replay-bc9e" id="toc-sec-ondevice-learning-experience-replay-bc9e" class="nav-link" data-scroll-target="#sec-ondevice-learning-experience-replay-bc9e">Experience Replay</a></li>
  <li><a href="#sec-ondevice-learning-data-compression-98a0" id="toc-sec-ondevice-learning-data-compression-98a0" class="nav-link" data-scroll-target="#sec-ondevice-learning-data-compression-98a0">Data Compression</a></li>
  <li><a href="#sec-ondevice-learning-tradeoffs-summary-b256" id="toc-sec-ondevice-learning-tradeoffs-summary-b256" class="nav-link" data-scroll-target="#sec-ondevice-learning-tradeoffs-summary-b256">Tradeoffs Summary</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-federated-learning-6534" id="toc-sec-ondevice-learning-federated-learning-6534" class="nav-link" data-scroll-target="#sec-ondevice-learning-federated-learning-6534">Federated Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-federated-learning-motivation-e16d" id="toc-sec-ondevice-learning-federated-learning-motivation-e16d" class="nav-link" data-scroll-target="#sec-ondevice-learning-federated-learning-motivation-e16d">Federated Learning Motivation</a></li>
  <li><a href="#sec-ondevice-learning-learning-protocols-2916" id="toc-sec-ondevice-learning-learning-protocols-2916" class="nav-link" data-scroll-target="#sec-ondevice-learning-learning-protocols-2916">Learning Protocols</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-local-training-3430" id="toc-sec-ondevice-learning-local-training-3430" class="nav-link" data-scroll-target="#sec-ondevice-learning-local-training-3430">Local Training</a></li>
  <li><a href="#sec-ondevice-learning-protocols-overview-64b2" id="toc-sec-ondevice-learning-protocols-overview-64b2" class="nav-link" data-scroll-target="#sec-ondevice-learning-protocols-overview-64b2">Protocols Overview</a></li>
  <li><a href="#sec-ondevice-learning-client-scheduling-aa1e" id="toc-sec-ondevice-learning-client-scheduling-aa1e" class="nav-link" data-scroll-target="#sec-ondevice-learning-client-scheduling-aa1e">Client Scheduling</a></li>
  <li><a href="#sec-ondevice-learning-efficient-communication-fb5c" id="toc-sec-ondevice-learning-efficient-communication-fb5c" class="nav-link" data-scroll-target="#sec-ondevice-learning-efficient-communication-fb5c">Efficient Communication</a></li>
  <li><a href="#sec-ondevice-learning-federated-personalization-4673" id="toc-sec-ondevice-learning-federated-personalization-4673" class="nav-link" data-scroll-target="#sec-ondevice-learning-federated-personalization-4673">Federated Personalization</a></li>
  <li><a href="#sec-ondevice-learning-federated-privacy-8320" id="toc-sec-ondevice-learning-federated-privacy-8320" class="nav-link" data-scroll-target="#sec-ondevice-learning-federated-privacy-8320">Federated Privacy</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-practical-system-design-3914" id="toc-sec-ondevice-learning-practical-system-design-3914" class="nav-link" data-scroll-target="#sec-ondevice-learning-practical-system-design-3914">Practical System Design</a></li>
  <li><a href="#sec-ondevice-learning-challenges-46f2" id="toc-sec-ondevice-learning-challenges-46f2" class="nav-link" data-scroll-target="#sec-ondevice-learning-challenges-46f2">Challenges</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-heterogeneity-ac2d" id="toc-sec-ondevice-learning-heterogeneity-ac2d" class="nav-link" data-scroll-target="#sec-ondevice-learning-heterogeneity-ac2d">Heterogeneity</a></li>
  <li><a href="#sec-ondevice-learning-data-fragmentation-378d" id="toc-sec-ondevice-learning-data-fragmentation-378d" class="nav-link" data-scroll-target="#sec-ondevice-learning-data-fragmentation-378d">Data Fragmentation</a></li>
  <li><a href="#sec-ondevice-learning-monitoring-validation-3151" id="toc-sec-ondevice-learning-monitoring-validation-3151" class="nav-link" data-scroll-target="#sec-ondevice-learning-monitoring-validation-3151">Monitoring and Validation</a></li>
  <li><a href="#sec-ondevice-learning-resource-management-4bbd" id="toc-sec-ondevice-learning-resource-management-4bbd" class="nav-link" data-scroll-target="#sec-ondevice-learning-resource-management-4bbd">Resource Management</a></li>
  <li><a href="#sec-ondevice-learning-deployment-risks-92aa" id="toc-sec-ondevice-learning-deployment-risks-92aa" class="nav-link" data-scroll-target="#sec-ondevice-learning-deployment-risks-92aa">Deployment Risks</a></li>
  <li><a href="#sec-ondevice-learning-challenges-summary-7983" id="toc-sec-ondevice-learning-challenges-summary-7983" class="nav-link" data-scroll-target="#sec-ondevice-learning-challenges-summary-7983">Challenges Summary</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-summary-ef13" id="toc-sec-ondevice-learning-summary-ef13" class="nav-link" data-scroll-target="#sec-ondevice-learning-summary-ef13">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/ops/ops.html">Robust Deployment</a></li><li class="breadcrumb-item"><a href="../../../contents/core/ondevice_learning/ondevice_learning.html">On-Device Learning</a></li></ol></nav></header>




<section id="on-device-learning" class="level1 page-columns page-full">
<h1>On-Device Learning</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: Drawing of a smartphone with its internal components exposed, revealing diverse miniature engineers of different genders and skin tones actively working on the ML model. The engineers, including men, women, and non-binary individuals, are tuning parameters, repairing connections, and enhancing the network on the fly. Data flows into the ML model, being processed in real-time, and generating output inferences.</em></p>
</div></div><p> <img src="images/png/cover_ondevice_learning.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>How does enabling learning directly on edge devices reshape machine learning system design, and what strategies support adaptation under resource constraints?</em></p>
<p>The shift toward on-device learning marks a significant evolution in the deployment and maintenance of machine learning systems. Rather than relying exclusively on centralized infrastructure, models are now increasingly expected to adapt in situ—updating and improving directly on the devices where they operate. This approach introduces a new design space, where training must occur within stringent constraints on memory, compute, energy, and data availability. In these settings, the balance between model adaptability, system efficiency, and deployment scalability becomes critical. This chapter examines the architectural, algorithmic, and infrastructure-level techniques that enable effective learning on the edge, and outlines the principles required to support autonomous model improvement in resource-constrained environments.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand on-device learning and how it differs from cloud-based training</p></li>
<li><p>Recognize the benefits and limitations of on-device learning</p></li>
<li><p>Examine strategies to adapt models through complexity reduction, optimization, and data compression</p></li>
<li><p>Understand related concepts like federated learning and transfer learning</p></li>
<li><p>Analyze the security implications of on-device learning and mitigation strategies</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-ondevice-learning-overview-3555" class="level2">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-overview-3555">Overview</h2>
<p>Machine learning systems have traditionally treated model training and model inference as distinct phases, often separated by both time and infrastructure. Training occurs in the cloud, leveraging large-scale compute clusters and curated datasets, while inference is performed downstream on deployed models—typically on user devices or edge servers. However, this separation is beginning to erode. Increasingly, devices are being equipped not just to run inference, but to adapt, personalize, and improve models locally.</p>
<p>On-device learning refers to the process of training or adapting machine learning models directly on the device where they are deployed. This capability opens the door to systems that can personalize models in response to user behavior, operate without cloud connectivity, and respect stringent privacy constraints by keeping data local. It also introduces a new set of challenges: devices have limited memory, computational power, and energy. Furthermore, training data is often sparse, noisy, or non-independent across users. These limitations necessitate a fundamental rethinking of training algorithms, system architecture, and deployment strategies.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition of On-Device Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of On-Device Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>On-Device Learning</strong> is the <em>local adaptation or training</em> of machine learning models directly on deployed hardware devices, without reliance on continuous connectivity to centralized servers. It enables <em>personalization, privacy preservation, and autonomous operation</em> by leveraging user-specific data collected in situ. On-device learning systems must operate under <em>tight constraints on compute, memory, energy, and data availability</em>, requiring specialized methods for model optimization, training efficiency, and data representation. As on-device learning matures, it increasingly incorporates <em>federated collaboration, lifelong adaptation, and secure execution</em>, expanding the frontier of intelligent edge computing.</p>
</div>
</div>
<p>This chapter explores the principles and systems design considerations underpinning on-device learning. It begins by examining the motivating applications that necessitate learning on the device, followed by a discussion of the unique hardware constraints introduced by embedded and mobile environments. The chapter then develops a taxonomy of strategies for adapting models, algorithms, and data pipelines to these constraints. Particular emphasis is placed on distributed and collaborative methods, such as federated learning, which enable decentralized training without direct data sharing. The chapter concludes with an analysis of outstanding challenges, including issues related to reliability, system validation, and the heterogeneity of deployment environments.</p>
</section>
<section id="sec-ondevice-learning-deployment-drivers-2eb7" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-deployment-drivers-2eb7">Deployment Drivers</h2>
<p>Machine learning systems have traditionally relied on centralized training pipelines, where models are developed and refined using large, curated datasets and powerful cloud-based infrastructure <span class="citation" data-cites="dean2012large">(<a href="#ref-dean2012large" role="doc-biblioref">Dean et al. 2012</a>)</span>. Once trained, these models are deployed to client devices for inference. While this separation has served most use cases well, it imposes limitations in settings where local data is dynamic, private, or personalized. On-device learning challenges this model by enabling systems to train or adapt directly on the device, without relying on constant connectivity to the cloud.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dean2012large" class="csl-entry" role="listitem">
Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, et al. 2012. <span>“Large Scale Distributed Deep Networks.”</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div></div><section id="sec-ondevice-learning-ondevice-learning-benefits-5ae7" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-ondevice-learning-benefits-5ae7">On-Device Learning Benefits</h3>
<p>Traditional machine learning systems rely on a clear division of labor between model training and inference. Training is performed in centralized environments with access to high-performance compute resources and large-scale datasets. Once trained, models are distributed to client devices, where they operate in a static inference-only mode. While this centralized paradigm has been effective in many deployments, it introduces limitations in settings where data is user-specific, behavior is dynamic, or connectivity is intermittent.</p>
<p>On-device learning refers to the capability of a deployed device to perform model adaptation using locally available data. This shift from centralized to decentralized learning is motivated by four key considerations: personalization, latency and availability, privacy, and infrastructure efficiency <span class="citation" data-cites="li2020federated">(<a href="#ref-li2020federated" role="doc-biblioref">Li et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2020federated" class="csl-entry" role="listitem">
Li, Tian, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020. <span>“Federated Learning: Challenges, Methods, and Future Directions.”</span> <em>IEEE Signal Processing Magazine</em> 37 (3): 50–60. <a href="https://doi.org/10.1109/msp.2020.2975749">https://doi.org/10.1109/msp.2020.2975749</a>.
</div></div><p>Personalization is a primary motivation. Deployed models often encounter usage patterns and data distributions that differ substantially from their training environments. Local adaptation enables models to refine behavior in response to user-specific data—capturing linguistic preferences, physiological baselines, sensor characteristics, or environmental conditions. This is particularly important in applications with high inter-user variability, where a single global model may fail to serve all users equally well.</p>
<p>Latency and availability further justify local learning. In edge computing scenarios, connectivity to centralized infrastructure may be unreliable, delayed, or intentionally limited to preserve bandwidth or reduce energy usage. On-device learning enables autonomous improvement of models even in fully offline or delay-sensitive contexts, where round-trip updates to the cloud are infeasible.</p>
<p>Privacy is another critical factor. Many applications involve sensitive or regulated data, including biometric measurements, typed input, location traces, or health information. Transmitting such data to the cloud introduces privacy risks and compliance burdens. Local learning mitigates these concerns by keeping raw data on the device and operating within privacy-preserving boundaries—potentially aiding adherence to regulations such as GDPR<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, HIPAA<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, or region-specific data sovereignty laws.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>GDPR:</strong> General Data Protection Regulation, a legal framework that sets guidelines for the collection and processing of personal information in the EU.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;<strong>HIPAA:</strong> Health Insurance Portability and Accountability Act, U.S. legislation that provides data privacy and security provisions for safeguarding medical information.</p></div></div><p>Infrastructure efficiency also plays a role. Centralized training pipelines require substantial backend infrastructure to collect, store, and process user data. At scale, this introduces bottlenecks in bandwidth, compute capacity, and energy consumption. By shifting learning to the edge, systems can reduce communication costs and distribute training workloads across the deployment fleet, relieving pressure on centralized resources.</p>
<p>These motivations are grounded in the broader concept of knowledge transfer, where a pretrained model transfers useful representations to a new task or domain. As depicted in <a href="#fig-transfer-conceptual" class="quarto-xref">Figure&nbsp;1</a>, knowledge transfer can occur between closely related tasks (e.g., playing different board games or musical instruments), or across domains that share structure (e.g., from riding a bicycle to driving a scooter). In the context of on-device learning, this means leveraging a model pretrained in the cloud and adapting it efficiently to a new context using only local data and limited updates. The figure highlights the key idea: pretrained knowledge enables fast adaptation without relearning from scratch, even when the new task diverges in input modality or goal.</p>
<div id="fig-transfer-conceptual" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transfer-conceptual-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/ondevice_transfer_learning_apps.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Knowledge Transfer: Pretrained models accelerate learning on new tasks by leveraging existing representations, as seen by adapting skills between related board games or musical instruments. This transfer extends across domains—like bicycle riding and scooter operation—where shared underlying structures enable efficient adaptation with limited new data."><img src="images/png/ondevice_transfer_learning_apps.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transfer-conceptual-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Knowledge Transfer</strong>: Pretrained models accelerate learning on new tasks by leveraging existing representations, as seen by adapting skills between related board games or musical instruments. This transfer extends across domains—like bicycle riding and scooter operation—where shared underlying structures enable efficient adaptation with limited new data.
</figcaption>
</figure>
</div>
<p>This conceptual shift, which is enabled by transfer learning and adaptation, is essential for real-world on-device applications. Whether adapting a language model for personal typing preferences, adjusting gesture recognition to an individual’s movement patterns, or recalibrating a sensor model in a changing environment, on-device learning allows systems to remain responsive, efficient, and user-aligned over time.</p>
</section>
<section id="sec-ondevice-learning-application-domains-1595" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-application-domains-1595">Application Domains</h3>
<p>The motivations for on-device learning are most clearly illustrated by examining the application domains where its benefits are both tangible and necessary. These domains span consumer technologies, healthcare, industrial systems, and embedded applications, each presenting scenarios where local adaptation is preferable, or even required, for effective machine learning deployment.</p>
<p>Mobile input prediction is a mature example of on-device learning in action. In systems such as smartphone keyboards, predictive text and autocorrect features benefit substantially from continuous local adaptation. User typing patterns are highly personalized and evolve dynamically, making centralized static models insufficient. On-device learning enables language models to finetune their predictions directly on the device, without transmitting keystroke data to external servers. This approach not only supports personalization but also aligns with privacy-preserving design principles.</p>
<p>For instance, Google’s Gboard employs federated learning to improve shared models across a large population of users while keeping raw data local to each device <span class="citation" data-cites="hard2018federated">(<a href="#ref-hard2018federated" role="doc-biblioref">Hard et al. 2018</a>)</span>. As shown in <a href="#fig-ondevice-gboard" class="quarto-xref">Figure&nbsp;2</a>, different prediction strategies illustrate how local adaptation can operate in real-time: next-word prediction (NWP) suggests likely continuations based on prior text, while Smart Compose leverages on-the-fly rescoring to offer dynamic completions, showcasing the sophistication of local inference mechanisms.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-ondevice-gboard" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ondevice-gboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/ondevice_gboard_example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: On-Device Prediction Strategies: Gboard employs both next-word prediction and smart compose with on-the-fly rescoring to adapt to user typing patterns locally, enhancing personalization and preserving privacy. These techniques demonstrate how machine learning models can refine predictions in real-time without transmitting data to a central server, enabling efficient and private mobile input experiences."><img src="images/png/ondevice_gboard_example.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ondevice-gboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>On-Device Prediction Strategies</strong>: Gboard employs both next-word prediction and smart compose with on-the-fly rescoring to adapt to user typing patterns locally, enhancing personalization and preserving privacy. These techniques demonstrate how machine learning models can refine predictions in real-time without transmitting data to a central server, enabling efficient and private mobile input experiences.
</figcaption>
</figure>
</div>
<p>Wearable and health monitoring devices also present strong use cases. These systems often rely on real-time data from accelerometers, heart rate sensors, or electrodermal activity monitors. However, physiological baselines vary significantly between individuals. On-device learning allows models to adapt to these baselines over time, improving the accuracy of activity recognition, stress detection, and sleep staging. Moreover, in regulated healthcare environments, patient data must remain localized due to privacy laws, further reinforcing the need for edge-local adaptation.</p>
<p>Wake-word detection and voice interfaces illustrate another critical scenario. Devices such as smart speakers and earbuds must recognize voice commands quickly and accurately, even in noisy or dynamic acoustic environments. Local training enables models to adapt to the user’s voice profile and ambient context, reducing false positives and missed detections. This kind of adaptation is particularly valuable in far-field audio settings, where microphone configurations and room acoustics vary widely across deployments.</p>
<p>Industrial IoT<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> and remote monitoring systems also benefit from local learning capabilities. In applications such as agricultural sensing, pipeline monitoring, or environmental surveillance, connectivity to centralized infrastructure may be limited or costly. On-device learning allows these systems to detect anomalies, adjust thresholds, or adapt to seasonal trends without continuous communication with the cloud. This capability is critical for maintaining autonomy and reliability in edge-deployed sensor networks.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Industrial Internet of Things (IoT):</strong> Network of physical objects, including devices, vehicles, and buildings, that use sensors and software to collect and exchange data.</p></div></div><p>Embedded computer vision systems, including those in robotics, AR/VR, and smart cameras, present additional opportunities. These systems often operate in novel or evolving environments that differ significantly from training conditions. On-device adaptation allows models to recalibrate to new lighting conditions, object appearances, or motion patterns, maintaining task accuracy over time.</p>
<p>Each of these domains highlights a common pattern: the deployment environment introduces variation or uncertainty that cannot be fully anticipated during centralized training. On-device learning offers a mechanism for adapting models in place, enabling systems to improve continuously in response to local conditions. These examples also reveal a critical design requirement: learning must be performed efficiently, privately, and reliably under significant resource constraints. The following section formalizes these constraints and outlines the system-level considerations that shape the design of on-device learning solutions.</p>
</section>
<section id="sec-ondevice-learning-training-paradigms-af01" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-training-paradigms-af01">Training Paradigms</h3>
<p>Most machine learning systems today follow a centralized learning paradigm. Models are trained in data centers using large-scale, curated datasets aggregated from many sources. Once trained, these models are deployed to client devices in a static form, where they perform inference without further modification. Updates to model parameters, either to incorporate new data or to improve generalization, are handled periodically through offline retraining, often using newly collected or labeled data sent back from the field.</p>
<p>This centralized model of learning offers numerous advantages: high-per­for­`mance computing infrastructure, access to diverse data distributions, and robust debugging and validation pipelines. However, it also depends on reliable data transfer, trust in data custodianship, and infrastructure capable of managing global updates across a fleet of devices. As machine learning is deployed into increasingly diverse and distributed environments, the limitations of this approach become more apparent.</p>
<p>In contrast, on-device learning is inherently decentralized. Each device maintains its own copy of a model and adapts it locally using data that is typically unavailable to centralized infrastructure. Training occurs on-device, often asynchronously and under varying resource conditions. Data never leaves the device, reducing exposure but also complicating coordination. Devices may differ substantially in their hardware capabilities, runtime environments, and patterns of use, making the learning process heterogeneous and difficult to standardize.</p>
<p>This decentralized nature introduces unique systems challenges. Devices may operate with different versions of the model, leading to inconsistencies in behavior. Evaluation and validation become more complex, as there is no central point from which to measure performance <span class="citation" data-cites="mcmahan2017communication">(<a href="#ref-mcmahan2017communication" role="doc-biblioref">McMahan et al. 2017</a>)</span>. Model updates must be carefully managed to prevent degradation, and safety guarantees become harder to enforce in the absence of centralized testing.</p>
<div class="no-row-height column-margin column-container"></div><p>At the same time, decentralization introduces opportunities. It allows for personalization without centralized oversight, supports learning in disconnected or bandwidth-limited environments, and reduces the cost of infrastructure for model updates. It also raises important questions of how to coordinate learning across devices, whether through periodic synchronization, federated aggregation, or hybrid approaches that combine local and global objectives.</p>
<p>The move from centralized to decentralized learning represents more than a shift in deployment architecture—it fundamentally reshapes the design space for machine learning systems. In centralized training, data is aggregated from many sources and processed in large-scale data centers, where models are trained, validated, and then deployed in a static form to edge devices. In contrast, on-device learning introduces a decentralized paradigm: models are updated directly on client devices using local data, often asynchronously and under diverse hardware conditions. This change reduces reliance on cloud infrastructure and enhances personalization and privacy, but it also introduces new coordination and validation challenges.</p>
<p>On-device learning emerges as a response to the limitations of centralized machine learning workflows. As illustrated in <a href="#fig-centralized-vs-decentralized" class="quarto-xref">Figure&nbsp;3</a>, the traditional paradigm (A) involves training a model on aggregated cloud-based data before pushing it to client devices for static inference. This architecture works well when centralized data collection is feasible, network connectivity is reliable, and model generalization across users is sufficient. However, it falls short in scenarios where data is highly personalized, privacy-sensitive, or collected in environments with limited connectivity.</p>
<p>In contrast, once the model is deployed, local differences begin to emerge. Region B depicts the process by which each device collects its own data stream, which is often non-IID<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and noisy, and adapts the model to better reflect its specific operating context. This marks the shift from global generalization to local specialization, highlighting the autonomy and variability introduced by decentralized learning.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Non-IID Data:</strong> Datasets where samples are not independently and identically distributed, often seen in personalized data streams.</p></div></div><p><a href="#fig-centralized-vs-decentralized" class="quarto-xref">Figure&nbsp;3</a> illustrates this shift. In region A, centralized learning begins with cloud-based training on aggregated data, followed by deployment to client devices. Region B marks the transition to local learning: devices begin collecting data, which is frequently non-IID, noisy, and unlabeled, and adapting their models based on individual usage patterns. Finally, region C depicts federated learning, in which client updates are periodically synchronized via aggregated model updates rather than raw data transfer, enabling privacy-preserving global refinement.</p>
<p>This shift from centralized training to decentralized, adaptive learning reshapes how ML systems are designed and deployed. It enables learning in settings where connectivity is intermittent, data is user-specific, and personalization is essential—while introducing new challenges in update coordination, evaluation, and system robustness.</p>
<div id="fig-centralized-vs-decentralized" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-centralized-vs-decentralized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="cee3e9981ee09d7c65e151839ec0b5d45964502d.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Decentralized vs.&nbsp;Centralized Learning: On-device learning shifts model training from aggregated cloud data to individual devices, enabling personalization and reducing reliance on network connectivity. This paradigm contrasts with centralized training, where a single global model is deployed to all devices after cloud-based optimization."><img src="ondevice_learning_files/mediabag/cee3e9981ee09d7c65e151839ec0b5d45964502d.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-centralized-vs-decentralized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Decentralized vs.&nbsp;Centralized Learning</strong>: On-device learning shifts model training from aggregated cloud data to individual devices, enabling personalization and reducing reliance on network connectivity. This paradigm contrasts with centralized training, where a single global model is deployed to all devices after cloud-based optimization.
</figcaption>
</figure>
</div>
<div id="quiz-question-sec-ondevice-learning-deployment-drivers-2eb7" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Which of the following is NOT a primary motivation for on-device learning?</p>
<ol type="a">
<li>Personalization</li>
<li>Latency and availability</li>
<li>Centralized data aggregation</li>
<li>Privacy</li>
</ol></li>
<li><p>Explain how on-device learning addresses privacy concerns that are present in centralized machine learning systems.</p></li>
<li><p>On-device learning can reduce infrastructure costs by decreasing the need for centralized data processing. (True/False)</p></li>
<li><p>In which scenario is on-device learning particularly beneficial?</p>
<ol type="a">
<li>When data is uniform and consistent across all users</li>
<li>In environments with reliable and high-speed internet connectivity</li>
<li>When user data is highly personalized and varies significantly</li>
<li>For applications that require real-time global data synchronization</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-deployment-drivers-2eb7" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ondevice-learning-design-constraints-d887" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-design-constraints-d887">Design Constraints</h2>
<p>Enabling learning on the device requires rethinking conventional assumptions about where and how machine learning systems operate. In centralized environments, models are trained with access to extensive compute infrastructure, large and curated datasets, and generous memory and energy budgets. At the edge, none of these assumptions hold. Instead, on-device learning must navigate a constrained design space shaped by the structure of the model, the nature of the available data, and the computational capabilities of the deployment platform.</p>
<p>These three dimensions, the model, the data, and the computational resources, form the foundation of any on-device learning system. Each imposes distinct limitations that influence algorithmic design and system architecture. The model must be compact enough to fit within memory and storage bounds, yet expressive enough to support adaptation. The data is local, often sparse, unlabeled, and non-IID, requiring robust and efficient learning procedures. The compute environment is resource-constrained, often lacking support for floating-point operations or backpropagation primitives. These constraints are not merely technical—they reflect the realities of deploying machine learning systems in the wild. Devices may be battery-powered, have limited connectivity, and operate in unpredictable environments. They may also be heterogeneous, with different hardware capabilities and software stacks. As a result, on-device learning must be designed to accommodate these variations while still delivering reliable performance.</p>
<p><a href="#fig-ondevice-pretraining" class="quarto-xref">Figure&nbsp;4</a> illustrates a pipeline that combines offline pre-training with online adaptive learning on resource-constrained IoT devices. The system first undergoes meta-training with generic data. During deployment, device-specific constraints such as data availability, compute, and memory shape the adaptation strategy by ranking and selecting layers and channels to update. This enables efficient on-device learning within limited resource envelopes.</p>
<div id="fig-ondevice-pretraining" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ondevice-pretraining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c42ceee9cd4bc594efd92af96f7dfce696ac282b.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: On-Device Adaptation Pipeline: Resource-constrained devices leverage a two-stage learning process: offline pre-training establishes initial model weights, followed by online adaptation that selectively updates layers based on available data, compute, and memory. This approach balances model performance with the practical limitations of edge deployment, enabling continuous learning in real-world environments."><img src="ondevice_learning_files/mediabag/c42ceee9cd4bc594efd92af96f7dfce696ac282b.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ondevice-pretraining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>On-Device Adaptation Pipeline</strong>: Resource-constrained devices leverage a two-stage learning process: offline pre-training establishes initial model weights, followed by online adaptation that selectively updates layers based on available data, compute, and memory. This approach balances model performance with the practical limitations of edge deployment, enabling continuous learning in real-world environments.
</figcaption>
</figure>
</div>
<section id="sec-ondevice-learning-model-constraints-b63e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-model-constraints-b63e">Model Constraints</h3>
<p>The structure and size of the machine learning model directly influence the feasibility of on-device training. Unlike cloud-deployed models that can span billions of parameters and rely on multi-gigabyte memory budgets, models intended for on-device learning must conform to tight constraints on memory, storage, and computational complexity. These constraints apply not only at inference time, but also during training, where additional resources are needed for gradient computation, parameter updates, and optimizer state.</p>
<p>For example, the MobileNetV2 architecture, commonly used in mobile vision tasks, requires approximately 14 MB of storage in its standard configuration. While this is feasible for modern smartphones, it far exceeds the memory available on embedded microcontrollers such as the Arduino Nano 33 BLE Sense, which provides only 256 KB of SRAM and 1 MB of flash storage. In such platforms, even a single layer of a typical convolutional neural network may exceed available RAM during training due to the need to store intermediate feature maps.</p>
<p>In addition to storage constraints, the training process itself expands the effective memory footprint. Standard backpropagation requires caching activations for each layer during the forward pass, which are then reused during gradient computation in the backward pass. For a 10-layer convolutional model processing <span class="math inline">\(64 \times 64\)</span> images, the required memory may exceed 1–2 MB—well beyond the SRAM capacity of most embedded systems.</p>
<p>Model complexity also affects runtime energy consumption and thermal limits. In systems such as smartwatches or battery-powered wearables, sustained model training can deplete energy reserves or trigger thermal throttling<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. Training a full model using floating-point operations on these devices is often infeasible. This limitation has motivated the development of ultra-lightweight model variants, such as MLPerf Tiny<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> benchmark networks <span class="citation" data-cites="banbury2021mlperf">(<a href="#ref-banbury2021mlperf" role="doc-biblioref">Banbury et al. 2021</a>)</span>, which fit within 100–200 KB and can be adapted using only partial gradient updates.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Reduction in computing performance to prevent overheating in electronic devices.</p></div><div id="fn6"><p><sup>6</sup>&nbsp;<strong>MLPerf Tiny:</strong> A benchmark suite for evaluating the performance of ultra-low power machine learning systems in real-world scenarios.</p></div><div id="ref-banbury2021mlperf" class="csl-entry" role="listitem">
Banbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, et al. 2021. <span>“MLPerf Tiny Benchmark.”</span> <em>arXiv Preprint arXiv:2106.07597</em>, June. <a href="http://arxiv.org/abs/2106.07597v4">http://arxiv.org/abs/2106.07597v4</a>.
</div></div><p>The model architecture itself must also be designed with on-device learning in mind. Many conventional architectures, such as transformers or large convolutional networks, are not well-suited for on-device adaptation due to their size and complexity. Instead, lightweight architectures such as MobileNets, SqueezeNet, and EfficientNet have been developed specifically for resource-constrained environments. These models use techniques such as depthwise separable convolutions, bottleneck layers, and quantization to reduce memory and compute requirements while maintaining performance.</p>
<p>These architectures are often designed to be modular, allowing for easy adaptation and fine-tuning. For example, MobileNets <span class="citation" data-cites="howard2017mobilenets">(<a href="#ref-howard2017mobilenets" role="doc-biblioref">Howard et al. 2017</a>)</span> can be configured with different width multipliers and resolution settings to balance performance and resource usage. This flexibility is critical for on-device learning, where the model must adapt to the specific constraints of the deployment environment.</p>
<div class="no-row-height column-margin column-container"><div id="ref-howard2017mobilenets" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. <span>“MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.”</span> <em>CoRR</em> abs/1704.04861 (April). <a href="http://arxiv.org/abs/1704.04861v1">http://arxiv.org/abs/1704.04861v1</a>.
</div></div></section>
<section id="sec-ondevice-learning-data-constraints-f2ca" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-data-constraints-f2ca">Data Constraints</h3>
<p>The nature of data available to on-device learning systems differs significantly from the large, curated, and centrally managed datasets typically used in cloud-based training. At the edge, data is locally collected, temporally sparse, and often unstructured or unlabeled. These characteristics introduce challenges in volume, quality, and statistical distribution, all of which affect the reliability and generalizability of learning on the device.</p>
<p>Data volume is typically limited due to storage constraints and the nature of user interaction. For example, a smart fitness tracker may collect motion data only during physical activity, generating relatively few labeled samples per day. If a user wears the device for just 30 minutes of exercise, only a few hundred data points might be available for training, compared to the thousands typically required for supervised learning in controlled environments.</p>
<p>Moreover, on-device data is frequently non-IID (non-independent and identically distributed) <span class="citation" data-cites="zhao2018federated">(<a href="#ref-zhao2018federated" role="doc-biblioref">Zhao et al. 2018</a>)</span>. Consider a voice assistant deployed in different households: one user may issue commands in English with a strong regional accent, while another might speak a different language entirely. The local data distribution is highly user-specific and may differ substantially from the training distribution of the initial model. This heterogeneity complicates both model convergence and the design of update mechanisms that generalize well across devices.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhao2018federated" class="csl-entry" role="listitem">
Zhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. 2018. <span>“Federated Learning with Non-IID Data.”</span> <em>CoRR</em> abs/1806.00582 (June). <a href="http://arxiv.org/abs/1806.00582v2">http://arxiv.org/abs/1806.00582v2</a>.
</div></div><p>Label scarcity presents an additional obstacle. Most edge-collected data is unlabeled by default. In a smartphone camera, for instance, the device may capture thousands of images, but only a few are associated with user actions (e.g., tagging or favoriting), which could serve as implicit labels. In many applications, including detecting anomalies in sensor data and adapting gesture recognition models, labels may be entirely unavailable, making traditional supervised learning infeasible without additional methods.</p>
<p>Noise and variability further degrade data quality. Embedded systems such as environmental sensors or automotive ECUs<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> may experience fluctuations in sensor calibration, environmental interference, or mechanical wear, leading to corrupted or drifting input signals over time. Without centralized validation, these errors may silently degrade learning performance if not detected and filtered appropriately.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Electronic Control Unit (ECU):</strong> A device that controls one or more of the electrical systems or subsystems in a vehicle.</p></div></div><p>Finally, data privacy and security concerns are paramount in many on-device learning applications. Sensitive information, such as health data or user interactions, must be protected from unauthorized access. This requirement often precludes the use of traditional data-sharing methods, such as uploading raw data to a central server for training. Instead, on-device learning must rely on techniques that allow for local adaptation without exposing sensitive information.</p>
</section>
<section id="sec-ondevice-learning-compute-constraints-4244" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-compute-constraints-4244">Compute Constraints</h3>
<p>On-device learning must operate within the computational envelope of the target hardware platform, which ranges from low-power embedded microcontrollers to mobile-class processors found in smartphones and wearables. These systems differ substantially from the large-scale GPU or TPU infrastructure used in cloud-based training. They impose strict limits on instruction throughput, parallelism, and architectural support for training-specific operations, all of which shape the design of feasible learning strategies.</p>
<p>On the embedded end of the spectrum, devices such as the STM32F4 or ESP32 microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-point operations <span class="citation" data-cites="lai2020tinyml">(<a href="#ref-lai2020tinyml" role="doc-biblioref">Lai 2020</a>)</span>. These constraints preclude the use of conventional deep learning libraries and require models to be carefully designed for integer arithmetic and minimal runtime memory allocation. In such cases, even small models require tailored techniques, including quantization-aware training and selective parameter updates, to execute training loops without exceeding memory or power budgets. For example, the STM32F4 microcontroller can run a simple linear regression model with a few hundred parameters, but training even a small convolutional neural network would exceed its memory capacity. In these environments, training is often limited to simple algorithms such as stochastic gradient descent (SGD) or <span class="math inline">\(k\)</span>-means clustering, which can be implemented using integer arithmetic and minimal memory overhead.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lai2020tinyml" class="csl-entry" role="listitem">
Lai, Pete Warden Daniel Situnayake. 2020. <em>TinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers</em>. O’Reilly Media.
</div></div><p>In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine, and Google Tensor SoC, provides significantly more compute power, often with dedicated AI accelerators and optimized support for 8-bit or mixed-precision matrix operations. These platforms can support more complex training routines, including full backpropagation over compact models, though they still fall short of the computational throughput and memory bandwidth available in centralized data centers. For instance, training a lightweight transformer on a smartphone is feasible but must be tightly bounded in both time and energy consumption to avoid degrading the user experience.</p>
<p>Compute constraints are especially salient in real-time or battery-operated systems. In a smartphone-based speech recognizer, on-device adaptation must not interfere with inference latency or system responsiveness. Similarly, in wearable medical monitors, training must occur opportunistically, during periods of low activity or charging, to preserve battery life and avoid thermal issues.</p>
<div id="quiz-question-sec-ondevice-learning-design-constraints-d887" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes a key challenge of on-device learning compared to cloud-based learning?</p>
<ol type="a">
<li>Access to extensive compute infrastructure</li>
<li>Availability of large, curated datasets</li>
<li>Constraints on memory and computational resources</li>
<li>Ability to perform floating-point operations</li>
</ol></li>
<li><p>Explain why model complexity is a critical consideration for on-device learning and how it affects energy consumption.</p></li>
<li><p>In on-device learning, data is often ______, which presents challenges for model convergence and generalization.</p></li>
<li><p>True or False: On-device learning systems can easily implement conventional deep learning libraries due to their advanced hardware capabilities.</p></li>
<li><p>Describe a scenario where on-device learning must adapt to compute constraints without degrading user experience.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-design-constraints-d887" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ondevice-learning-model-adaptation-0b2b" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-model-adaptation-0b2b">Model Adaptation</h2>
<p>Adapting a machine learning model on the device requires revisiting a core assumption of conventional training: that the entire model must be updated. In resource-constrained environments, this assumption becomes infeasible due to memory, compute, and energy limitations. Instead, modern approaches to on-device learning often focus on minimizing the scope of adaptation, updating only a subset of model parameters while reusing the majority of the pretrained architecture. These approaches leverage the power of transfer learning, starting with a model pretrained (usually offline on large datasets) and efficiently specializing it using the limited local data and compute resources available at the edge. This strategy is particularly effective when the pretrained model has already learned useful representations that can be adapted to new tasks or domains. By freezing most of the model parameters and only updating a small subset, we can achieve significant reductions in memory and compute requirements while still allowing for meaningful adaptation.</p>
<p>This strategy reduces both computational overhead and memory usage during training, enabling efficient local updates on devices ranging from smartphones to embedded microcontrollers. The central idea is to retain most of the model as a frozen backbone, while introducing lightweight, adaptable components, including bias-only updates, residual adapters, or task-specific layers, that can capture local variations in data. These techniques enable personalized or environment-aware learning without incurring the full cost of end-to-end finetuning.</p>
<p>In the sections that follow, we examine how minimal adaptation strategies are designed, the tradeoffs they introduce, and their role in enabling practical on-device learning.</p>
<section id="sec-ondevice-learning-weight-freezing-797c" class="level3">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-weight-freezing-797c">Weight Freezing</h3>
<p>One of the simplest and most effective strategies for reducing the cost of on-device learning is to freeze the majority of a model’s parameters and adapt only a minimal subset. A widely used approach is bias-only adaptation, in which all weights are fixed and only the bias terms, which are typically scalar offsets applied after linear or convolutional layers, are updated during training. This significantly reduces the number of trainable parameters, simplifies memory management during backpropagation, and helps mitigate overfitting when data is sparse or noisy.</p>
<p>Consider a standard neural network layer: <span class="math display">\[
y = W x + b
\]</span> where <span class="math inline">\(W \in \mathbb{R}^{m \times n}\)</span> is the weight matrix, <span class="math inline">\(b \in \mathbb{R}^m\)</span> is the bias vector, and <span class="math inline">\(x \in \mathbb{R}^n\)</span> is the input. In full training, gradients are computed for both <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span>. In bias-only adaptation, we constrain: <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial W} = 0, \quad \frac{\partial \mathcal{L}}{\partial b} \neq 0
\]</span> so that only the bias is updated via gradient descent: <span class="math display">\[
b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b}
\]</span></p>
<p>This drastically reduces the number of stored gradients and optimizer states, enabling training to proceed even under memory-constrained conditions. On embedded devices that lack floating-point units, this reduction can be critical to enabling on-device learning at all.</p>
<p>The code snippet in <a href="#lst-bias-adaptation" class="quarto-xref">Listing&nbsp;1</a> demonstrates how to implement bias-only adaptation in PyTorch.</p>
<div id="lst-bias-adaptation" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-bias-adaptation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: <strong>Bias-Only Adaptation</strong>: Freezes model parameters except for biases to reduce memory usage and enable on-device learning.
</figcaption>
<div aria-describedby="lst-bias-adaptation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze all parameters</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Enable gradients for bias parameters only</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'bias'</span> <span class="kw">in</span> name:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This pattern ensures that only bias terms participate in the backward pass and optimizer update. It is particularly useful when adapting pretrained models to user-specific or device-local data.</p>
<p>This technique underpins TinyTL, a framework explicitly designed to enable efficient adaptation of deep neural networks on microcontrollers and other memory-limited platforms. Rather than updating all network parameters during training, TinyTL freezes both the convolutional weights and the batch normalization statistics, training only the bias terms and, in some cases, lightweight residual components. This architectural shift drastically reduces memory usage during backpropagation, since the largest tensors, which are intermediate activations, no longer need to be stored for gradient computation.</p>
<p><a href="#fig-tinytl-architecture" class="quarto-xref">Figure&nbsp;5</a> illustrates the architectural differences between a standard model and the TinyTL approach. In the conventional baseline architecture, all layers are trainable, and backpropagation requires storing intermediate activations for the full network. This significantly increases the memory footprint, which quickly becomes infeasible on edge devices with only a few hundred kilobytes of SRAM.</p>
<div id="fig-tinytl-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tinytl-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/ondevice_transfer_tinytl.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Memory-Efficient Adaptation: Tinytl reduces on-device training costs by freezing convolutional weights and batch normalization, updating only bias terms and lightweight residual connections to minimize memory usage during backpropagation. This approach enables deployment of deep neural networks on resource-constrained edge devices with limited SRAM, facilitating efficient model adaptation without requiring full parameter updates."><img src="images/png/ondevice_transfer_tinytl.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tinytl-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Memory-Efficient Adaptation</strong>: Tinytl reduces on-device training costs by freezing convolutional weights and batch normalization, updating only bias terms and lightweight residual connections to minimize memory usage during backpropagation. This approach enables deployment of deep neural networks on resource-constrained edge devices with limited SRAM, facilitating efficient model adaptation without requiring full parameter updates.
</figcaption>
</figure>
</div>
<p>In contrast, the TinyTL architecture freezes all weights and updates only the bias terms inserted after convolutional layers. These bias modules are lightweight and require minimal memory, enabling efficient training with a drastically reduced memory footprint. The frozen convolutional layers act as a fixed feature extractor, and only the trainable bias components are involved in adaptation. By avoiding storage of full activation maps and limiting the number of updated parameters, TinyTL enables on-device training under severe resource constraints.</p>
<p>Because the base model remains unchanged, TinyTL assumes that the pretrained features are sufficiently expressive for downstream tasks. The bias terms allow for minor but meaningful shifts in model behavior, particularly for personalization tasks. When domain shift is more significant, TinyTL can optionally incorporate small residual adapters to improve expressivity, all while preserving the system’s tight memory and energy profile.</p>
<p>These design choices allow TinyTL to reduce training memory usage by more than 10×. For instance, adapting a MobileNetV2 model using TinyTL can reduce the number of updated parameters from over 3 million to fewer than 50,000. Combined with quantization, this enables local adaptation on devices with only a few hundred kilobytes of memory—making on-device learning truly feasible in constrained environments.</p>
</section>
<section id="sec-ondevice-learning-residual-lowrank-updates-2add" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-residual-lowrank-updates-2add">Residual and Low-Rank Updates</h3>
<p>Bias-only updates offer a lightweight path for on-device learning, but they are limited in representational flexibility. When the frozen model does not align well with the target distribution, it may be necessary to allow more expressive adaptation—without incurring the full cost of weight updates. One solution is to introduce residual adaptation modules <span class="citation" data-cites="houlsby2019parameter">(<a href="#ref-houlsby2019parameter" role="doc-biblioref">Houlsby et al. 2019</a>)</span>,<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> or low-rank parameterizations<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, which provide a middle ground between static backbones<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> and full fine-tuning <span class="citation" data-cites="hu2021lora">(<a href="#ref-hu2021lora" role="doc-biblioref">Hu et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-houlsby2019parameter" class="csl-entry" role="listitem">
Houlsby, Neil, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Chloé de Laroussilhe, Andrea Gesmundo, Mohammad Attariyan, and Sylvain Gelly. 2019. <span>“Parameter-Efficient Transfer Learning for NLP.”</span> In <em>International Conference on Machine Learning</em>, 2790–99. PMLR.
</div><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Residual Adaptation Modules:</strong> Layers added to existing networks to improve adaptability without extensive retraining.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Low-rank Parameterizations:</strong> Techniques that decompose parameters into low-rank matrices to save computation.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;<strong>Static Backbones:</strong> Unchangeable core parts of a neural network model, typically pre-trained.</p></div></div><p>These methods extend a frozen model by adding trainable layers, which are typically small and computationally inexpensive, that allow the network to respond to new data. The main body of the network remains fixed, while only the added components are optimized. This modularity makes the approach well-suited for on-device adaptation in constrained settings, where small updates must deliver meaningful changes.</p>
<section id="sec-ondevice-learning-adapterbased-adaptation-04de" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-adapterbased-adaptation-04de">Adapter-Based Adaptation</h4>
<p>A common implementation involves inserting adapters, which are small residual bottleneck layers, between existing layers in a pretrained model. Consider a hidden representation <span class="math inline">\(h\)</span> passed between layers. A residual adapter introduces a transformation:</p>
<p><span class="math display">\[
h' = h + A(h)
\]</span></p>
<p>where <span class="math inline">\(A(\cdot)\)</span> is a trainable function, typically composed of two linear layers with a nonlinearity:</p>
<p><span class="math display">\[
A(h) = W_2 \, \sigma(W_1 h)
\]</span></p>
<p>with <span class="math inline">\(W_1 \in \mathbb{R}^{r \times d}\)</span> and <span class="math inline">\(W_2 \in \mathbb{R}^{d \times r}\)</span>, where <span class="math inline">\(r \ll d\)</span>. This bottleneck design ensures that only a small number of parameters are introduced per layer.</p>
<p>The adapters act as learnable perturbations on top of a frozen backbone. Because they are small and sparsely applied, they add negligible memory overhead, yet they allow the model to shift its predictions in response to new inputs.</p>
</section>
<section id="sec-ondevice-learning-lowrank-techniques-2817" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-lowrank-techniques-2817">Low-Rank Techniques</h4>
<p>Another efficient strategy is to constrain weight updates themselves to a low-rank structure. Rather than updating a full matrix <span class="math inline">\(W\)</span>, we approximate the update as: <span class="math display">\[
\Delta W \approx U V^\top
\]</span> where <span class="math inline">\(U \in \mathbb{R}^{m \times r}\)</span> and <span class="math inline">\(V \in \mathbb{R}^{n \times r}\)</span>, with <span class="math inline">\(r \ll \min(m,n)\)</span>. This reduces the number of trainable parameters from <span class="math inline">\(mn\)</span> to <span class="math inline">\(r(m + n)\)</span>. During adaptation, the new weight is computed as: <span class="math display">\[
W_{\text{adapted}} = W_{\text{frozen}} + U V^\top
\]</span></p>
<p>This formulation is commonly used in LoRA (Low-Rank Adaptation) techniques, originally developed for transformer models <span class="citation" data-cites="hu2021lora">(<a href="#ref-hu2021lora" role="doc-biblioref">Hu et al. 2021</a>)</span> but broadly applicable across architectures. Low-rank updates can be implemented efficiently on edge devices, particularly when <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are small and fixed-point representations are supported (<a href="#lst-lowrank-adapter" class="quarto-xref">Listing&nbsp;2</a>).</p>
<div class="no-row-height column-margin column-container"><div id="ref-hu2021lora" class="csl-entry" role="listitem">
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>“LoRA: Low-Rank Adaptation of Large Language Models.”</span> <em>arXiv Preprint arXiv:2106.09685</em>, June. <a href="http://arxiv.org/abs/2106.09685v2">http://arxiv.org/abs/2106.09685v2</a>.
</div></div><div id="lst-lowrank-adapter" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-lowrank-adapter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: <strong>Low-Rank Adapter</strong>: The code implements a low-rank adapter module by approximating weight updates using matrices (u) and (v), reducing parameter count while enabling efficient model adaptation on edge devices.
</figcaption>
<div aria-describedby="lst-lowrank-adapter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Adapter(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, bottleneck_dim):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down <span class="op">=</span> nn.Linear(dim, bottleneck_dim)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up <span class="op">=</span> nn.Linear(bottleneck_dim, dim)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.ReLU()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> <span class="va">self</span>.up(<span class="va">self</span>.activation(<span class="va">self</span>.down(x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This adapter adds a small residual transformation to a frozen layer. When inserted into a larger model, only the adapter parameters are trained.</p>
</section>
<section id="sec-ondevice-learning-edge-personalization-a892" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-edge-personalization-a892">Edge Personalization</h4>
<p>Adapters are especially useful when a global model is deployed to many devices and must adapt to device-specific input distributions. For instance, in smartphone camera pipelines, environmental lighting, user preferences, or lens distortion may vary between users <span class="citation" data-cites="rebuffi2017learning">(<a href="#ref-rebuffi2017learning" role="doc-biblioref">Rebuffi, Bilen, and Vedaldi 2017</a>)</span>. A shared model can be frozen and fine-tuned per-device using a few residual modules, allowing lightweight personalization without risking catastrophic forgetting<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. In voice-based systems, adapter modules have been shown to reduce word error rates in personalized speech recognition without retraining the full acoustic model. They also allow easy rollback or switching between user-specific versions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rebuffi2017learning" class="csl-entry" role="listitem">
Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. 2017. <span>“Learning Multiple Visual Domains with Residual Adapters.”</span> In <em>Advances in Neural Information Processing Systems</em>. Vol. 30.
</div><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Catastrophic Forgetting:</strong> A phenomenon where a neural network forgets previously learned information upon learning new data.</p></div></div></section>
<section id="sec-ondevice-learning-tradeoffs-bb7b" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-tradeoffs-bb7b">Tradeoffs</h4>
<p>Residual and low-rank updates strike a balance between expressivity and efficiency. Compared to bias-only learning, they can model more substantial deviations from the pretrained task. However, they require more memory and compute—both for training and inference.</p>
<p>When considering residual and low-rank updates for on-device learning, several important tradeoffs emerge. First, these methods consistently demonstrate superior adaptation quality compared to bias-only approaches, particularly when deployed in scenarios involving significant distribution shifts<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> from the original training data <span class="citation" data-cites="quinonero2009dataset">(<a href="#ref-quinonero2009dataset" role="doc-biblioref">Quiñonero-Candela et al. 2008</a>)</span>. This improved adaptability stems from their increased parameter capacity and ability to learn more complex transformations.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;Distribution shifts refer to changes in the input data’s characteristics, which can affect model performance when different from the training data.</p></div><div id="ref-quinonero2009dataset" class="csl-entry" role="listitem">
Quiñonero-Candela, Joaquin, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. 2008. <span>“Dataset Shift in Machine Learning.”</span> <em>The MIT Press</em>. The MIT Press. <a href="https://doi.org/10.7551/mitpress/7921.003.0002">https://doi.org/10.7551/mitpress/7921.003.0002</a>.
</div></div><p>However, this enhanced adaptability comes at a cost. The introduction of additional layers or parameters inevitably increases both memory requirements and computational latency during forward and backward passes. While these increases are modest compared to full model training, they must be carefully considered when deploying to resource-constrained devices.</p>
<p>Additionally, implementing these adaptation techniques requires system-level support for dynamic computation graphs<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> and the ability to selectively inject trainable parameters. Not all deployment environments or inference engines may support such capabilities out of the box.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;<strong>Dynamic Computation Graphs:</strong> Structures that allow changes during runtime, enabling models to adapt structures based on input data.</p></div></div><p>Despite these considerations, residual adaptation techniques have proven particularly valuable in mobile and edge computing scenarios where devices have sufficient computational resources. For instance, modern smartphones and tablets can readily accommodate these adaptations while maintaining acceptable performance characteristics. This makes residual adaptation a practical choice for applications requiring personalization without the overhead of full model retraining.</p>
</section>
</section>
<section id="sec-ondevice-learning-sparse-updates-eefe" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-sparse-updates-eefe">Sparse Updates</h3>
<p>Even when adaptation is restricted to a small number of parameters, including biases or adapter modules, training remains resource-intensive on constrained devices. One promising approach is to selectively update only a task-relevant subset of model parameters, rather than modifying the entire network or introducing new modules. This approach is known as task-adaptive sparse updating <span class="citation" data-cites="zhang2020efficient">(<a href="#ref-zhang2020efficient" role="doc-biblioref">Zhang, Song, and Tao 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2020efficient" class="csl-entry" role="listitem">
Zhang, Xitong, Jialin Song, and Dacheng Tao. 2020. <span>“Efficient Task-Specific Adaptation for Deep Models.”</span> In <em>International Conference on Learning Representations (ICLR)</em>.
</div></div><p>The key insight is that not all layers of a deep model contribute equally to performance gains on a new task or dataset. If we can identify a <em>minimal subset of parameters</em> that are most impactful for adaptation, we can train only those, reducing memory and compute costs while still achieving meaningful personalization.</p>
<section id="sec-ondevice-learning-sparse-update-design-ecbc" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-sparse-update-design-ecbc">Sparse Update Design</h4>
<p>Let a neural network be defined by parameters <span class="math inline">\(\theta = \{\theta_1, \theta_2, \ldots, \theta_L\}\)</span> across <span class="math inline">\(L\)</span> layers. In standard fine-tuning, we compute gradients and perform updates on all parameters: <span class="math display">\[
\theta_i \leftarrow \theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, \quad \text{for } i = 1, \ldots, L
\]</span></p>
<p>In task-adaptive sparse updates, we select a small subset <span class="math inline">\(\mathcal{S} \subset \{1, \ldots, L\}\)</span> such that only parameters in <span class="math inline">\(\mathcal{S}\)</span> are updated: <span class="math display">\[
\theta_i \leftarrow
\begin{cases}
\theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, &amp; \text{if } i \in \mathcal{S} \\
\theta_i, &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>The challenge lies in selecting the optimal subset <span class="math inline">\(\mathcal{S}\)</span> given memory and compute constraints.</p>
</section>
<section id="sec-ondevice-learning-layer-selection-c339" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-layer-selection-c339">Layer Selection</h4>
<p>A principled strategy for selecting <span class="math inline">\(\mathcal{S}\)</span> is to use contribution analysis—an empirical method that estimates how much each layer contributes to downstream performance improvement. For example, one can measure the marginal gain from updating each layer independently:</p>
<ol type="1">
<li>Freeze the entire model.</li>
<li>Unfreeze one candidate layer.</li>
<li>Finetune briefly and evaluate improvement in validation accuracy.</li>
<li>Rank layers by performance gain per unit cost (e.g., per KB of trainable memory).</li>
</ol>
<p>This layer-wise profiling yields a ranking from which <span class="math inline">\(\mathcal{S}\)</span> can be constructed subject to a memory budget.</p>
<p>A concrete example is TinyTrain, a method designed to enable rapid adaptation on-device <span class="citation" data-cites="deng2022tinytrain">(<a href="#ref-deng2022tinytrain" role="doc-biblioref">C. Deng, Zhang, and Wu 2022</a>)</span>. TinyTrain pretrains a model along with meta-gradients that capture which layers are most sensitive to new tasks. At runtime, the system dynamically selects layers to update based on task characteristics and available resources.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deng2022tinytrain" class="csl-entry" role="listitem">
Deng, Chulin, Yujun Zhang, and Yanzhi Wu. 2022. <span>“TinyTrain: Learning to Train Compact Neural Networks on the Edge.”</span> In <em>Proceedings of the 39th International Conference on Machine Learning (ICML)</em>.
</div></div></section>
<section id="sec-ondevice-learning-code-fragment-selective-layer-updating-pytorch-d094" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-code-fragment-selective-layer-updating-pytorch-d094">Code Fragment: Selective Layer Updating (PyTorch)</h4>
<div id="lst-selective-update" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-selective-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: <strong>Selective Layer Updating</strong>: This technique enables fine-tuning specific layers of a pre-trained model while keeping others frozen, optimizing computational resources for targeted improvements. <em>Source: PyTorch Documentation</em>
</figcaption>
<div aria-describedby="lst-selective-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Assume model has named layers: ['conv1', 'conv2', 'fc']</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">## We selectively update only conv2 and fc</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'conv2'</span> <span class="kw">in</span> name <span class="kw">or</span> <span class="st">'fc'</span> <span class="kw">in</span> name:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This pattern can be extended with profiling logic to select layers based on contribution scores or hardware profiles, as shown in <a href="#lst-selective-update" class="quarto-xref">Listing&nbsp;3</a>.</p>
</section>
<section id="sec-ondevice-learning-tinytrain-personalization-bdf2" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-tinytrain-personalization-bdf2">TinyTrain Personalization</h4>
<p>Consider a scenario where a user wears an augmented reality headset that performs real-time object recognition. As lighting and environments shift, the system must adapt to maintain accuracy—but training must occur during brief idle periods or while charging.</p>
<p>TinyTrain enables this by using meta-training during offline preparation: the model learns not only to perform the task, but also which parameters are most important to adapt. Then, at deployment, the device performs task-adaptive sparse updates, modifying only a few layers that are most relevant for its current environment. This keeps adaptation fast, energy-efficient, and memory-aware.</p>
</section>
<section id="sec-ondevice-learning-tradeoffs-effd" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-tradeoffs-effd">Tradeoffs</h4>
<p>Task-adaptive sparse updates introduce several important system-level considerations that must be carefully balanced. First, the overhead of contribution analysis, although primarily incurred during pretraining or initial profiling, represents a non-trivial computational cost. This overhead is typically acceptable since it occurs offline, but it must be factored into the overall system design and deployment pipeline.</p>
<p>Second, the stability of the adaptation process becomes critical when working with sparse updates. If too few parameters are selected for updating, the model may underfit the target distribution, failing to capture important local variations. This suggests the need for careful validation of the selected parameter subset before deployment, potentially incorporating minimum thresholds for adaptation capacity.</p>
<p>Third, the selection of updateable parameters must account for hardware-specific characteristics of the target platform. Beyond just considering gradient magnitudes, the system must evaluate the actual execution cost of updating specific layers on the deployed hardware. Some parameters might show high contribution scores but prove expensive to update on certain architectures, requiring a more nuanced selection strategy that balances statistical utility with runtime efficiency.</p>
<p>Despite these tradeoffs, task-adaptive sparse updates provide a powerful mechanism to scale adaptation to diverse deployment contexts, from microcontrollers to mobile devices <span class="citation" data-cites="diao2023sparse">(<a href="#ref-diao2023sparse" role="doc-biblioref">Levy et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-diao2023sparse" class="csl-entry" role="listitem">
Levy, Orin, Alon Cohen, Asaf Cassel, and Yishay Mansour. 2023. <span>“Efficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation.”</span> <em>arXiv Preprint arXiv:2303.01464</em>, March. <a href="http://arxiv.org/abs/2303.01464v2">http://arxiv.org/abs/2303.01464v2</a>.
</div></div></section>
<section id="sec-ondevice-learning-adaptation-strategy-comparison-8bd5" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-adaptation-strategy-comparison-8bd5">Adaptation Strategy Comparison</h4>
<p>Each adaptation strategy for on-device learning offers a distinct balance between expressivity, resource efficiency, and implementation complexity. Understanding these tradeoffs is essential when designing systems for diverse deployment targets—from ultra-low-power microcontrollers to feature-rich mobile processors.</p>
<p>Bias-only adaptation is the most lightweight approach, updating only scalar offsets in each layer while freezing all other parameters. This significantly reduces memory requirements and computational burden, making it suitable for devices with tight memory and energy budgets. However, its limited expressivity means it is best suited to applications where the pretrained model already captures most of the relevant task features and only minor local calibration is required.</p>
<p>Residual adaptation, often implemented via adapter modules, introduces a small number of trainable parameters into the frozen backbone of a neural network. This allows for greater flexibility than bias-only updates, while still maintaining control over the adaptation cost. Because the backbone remains fixed, training can be performed efficiently and safely under constrained conditions. This method supports modular personalization across tasks and users, making it a favorable choice for mobile settings where moderate adaptation capacity is needed.</p>
<p>Task-adaptive sparse updates offer the greatest potential for task-specific finetuning by selectively updating only a subset of layers or parameters based on their contribution to downstream performance. While this method enables expressive local adaptation, it requires a mechanism for layer selection, through profiling, contribution analysis, or meta-training, which introduces additional complexity. Nonetheless, when deployed carefully, it allows for dynamic tradeoffs between accuracy and efficiency, particularly in systems that experience large domain shifts or evolving input conditions.</p>
<p>These three approaches form a spectrum of tradeoffs. Their relative suitability depends on application domain, available hardware, latency constraints, and expected distribution shift. <a href="#tbl-adaptation-strategies" class="quarto-xref">Table&nbsp;1</a> summarizes their characteristics:</p>
<div id="tbl-adaptation-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-adaptation-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Adaptation Strategy Trade-Offs</strong>: Table entries characterize three approaches to model adaptation—bias-only updates, selective layer updates, and full finetuning—by quantifying their impact on trainable parameters, memory overhead, expressivity, suitability for different use cases, and system requirements. These characteristics reveal the inherent trade-offs between model flexibility, computational cost, and performance when deploying machine learning systems in dynamic environments.
</figcaption>
<div aria-describedby="tbl-adaptation-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 15%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 21%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Technique</th>
<th style="text-align: left;">Trainable Parameters</th>
<th style="text-align: left;">Memory Overhead</th>
<th style="text-align: left;">Expressivity</th>
<th style="text-align: left;">Use Case Suitability</th>
<th style="text-align: left;">System Requirements</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bias-Only Updates</td>
<td style="text-align: left;">Bias terms only</td>
<td style="text-align: left;">Minimal</td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Simple personalization; low variance</td>
<td style="text-align: left;">Extreme memory/compute limits</td>
</tr>
<tr class="even">
<td style="text-align: left;">Residual Adapters</td>
<td style="text-align: left;">Adapter modules</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">Moderate to High</td>
<td style="text-align: left;">User-specific tuning on mobile</td>
<td style="text-align: left;">Mobile-class SoCs with runtime support</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sparse Layer Updates</td>
<td style="text-align: left;">Selective parameter subsets</td>
<td style="text-align: left;">Variable</td>
<td style="text-align: left;">High (task-adaptive)</td>
<td style="text-align: left;">Real-time adaptation; domain shift</td>
<td style="text-align: left;">Requires profiling or meta-training</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="quiz-question-sec-ondevice-learning-model-adaptation-0b2b" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary advantage of bias-only adaptation in on-device learning?</p>
<ol type="a">
<li>High expressivity and flexibility</li>
<li>Minimal memory and compute requirements</li>
<li>Ability to handle significant domain shifts</li>
<li>Increased training complexity</li>
</ol></li>
<li><p>Explain why residual adapters are beneficial in on-device learning for mobile devices.</p></li>
<li><p>In task-adaptive sparse updates, only a ______ subset of model parameters is updated to achieve meaningful personalization.</p></li>
<li><p>True or False: Low-rank parameterizations in on-device learning are primarily used to increase the number of trainable parameters.</p></li>
<li><p>Order the following steps involved in task-adaptive sparse updates: 1) Evaluate improvement in validation accuracy, 2) Freeze the entire model, 3) Rank layers by performance gain per unit cost, 4) Unfreeze one candidate layer, 5) Finetune briefly.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-model-adaptation-0b2b" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-ondevice-learning-data-efficiency-1cee" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-data-efficiency-1cee">Data Efficiency</h2>
<p>On-device learning systems operate in environments where data is scarce, noisy, and highly individualized. Unlike centralized machine learning pipelines that rely on large, curated datasets, edge devices typically observe only small volumes of task-relevant data—collected incrementally over time and rarely labeled in a supervised manner <span class="citation" data-cites="chen2019closer">(<a href="#ref-chen2019closer" role="doc-biblioref">Chen et al. 2019</a>)</span>. This constraint fundamentally reshapes the learning process. Algorithms must extract value from minimal supervision, generalize from sparse observations, and remain robust to distributional shift. In many cases, the available data may be insufficient to train a model from scratch or even to finetune all parameters of a pretrained network. Instead, practical on-device learning relies on data-efficient techniques: few-shot adaptation, streaming updates, memory-based replay, and compressed supervision. These approaches enable models to improve over time without requiring extensive labeled datasets or centralized aggregation, making them well-suited to mobile, wearable, and embedded platforms where data acquisition is constrained by power, storage, and privacy considerations.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chen2019closer" class="csl-entry" role="listitem">
Chen, Wei-Yu, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. 2019. <span>“A Closer Look at Few-Shot Classification.”</span> In <em>International Conference on Learning Representations (ICLR)</em>.
</div></div><section id="sec-ondevice-learning-fewshot-streaming-7b2b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-fewshot-streaming-7b2b">Few-Shot and Streaming</h3>
<p>In conventional machine learning workflows, effective training typically requires large labeled datasets, carefully curated and preprocessed to ensure sufficient diversity and balance. On-device learning, by contrast, must often proceed from only a handful of local examples—collected passively through user interaction or ambient sensing, and rarely labeled in a supervised fashion. These constraints motivate two complementary adaptation strategies: few-shot learning, in which models generalize from a small, static set of examples, and streaming adaptation, where updates occur continuously as data arrives.</p>
<p>Few-shot adaptation is particularly relevant when the device observes a small number of labeled or weakly labeled instances for a new task or user condition <span class="citation" data-cites="wang2020generalizing">(<a href="#ref-wang2020generalizing" role="doc-biblioref">Wang et al. 2020</a>)</span>. In such settings, it is often infeasible to perform full finetuning of all model parameters without overfitting. Instead, methods such as bias-only updates, adapter modules, or prototype-based classification are employed to make use of limited data while minimizing capacity for memorization. Let <span class="math inline">\(D = \{(x_i, y_i)\}_{i=1}^K\)</span> denote a <span class="math inline">\(K\)</span>-shot dataset of labeled examples collected on-device. The goal is to update the model parameters <span class="math inline">\(\theta\)</span> to improve task performance under constraints such as:</p>
<div class="no-row-height column-margin column-container"><div id="ref-wang2020generalizing" class="csl-entry" role="listitem">
Wang, Yaqing, Quanming Yao, James T. Kwok, and Lionel M. Ni. 2020. <span>“Generalizing from a Few Examples: A Survey on Few-Shot Learning.”</span> <em>ACM Computing Surveys</em> 53 (3): 1–34. <a href="https://doi.org/10.1145/3386252">https://doi.org/10.1145/3386252</a>.
</div></div><ul>
<li>Limited number of gradient steps: <span class="math inline">\(T \ll 100\)</span></li>
<li>Constrained memory footprint: <span class="math inline">\(\|\theta_{\text{updated}}\| \ll \|\theta\|\)</span></li>
<li>Preservation of prior task knowledge (to avoid catastrophic forgetting)</li>
</ul>
<p>Keyword spotting (KWS) systems offer a concrete example of few-shot adaptation in a real-world, on-device deployment <span class="citation" data-cites="warden2018speech">(<a href="#ref-warden2018speech" role="doc-biblioref">Warden 2018</a>)</span>. These models are used to detect fixed phrases, including phrases like “Hey Siri” or “OK Google”, with low latency and high reliability. A typical KWS model consists of a pretrained acoustic encoder (e.g., a small convolutional or recurrent network that transforms input audio into an embedding space) followed by a lightweight classifier. In commercial systems, the encoder is trained centrally using thousands of hours of labeled speech across multiple languages and speakers. However, supporting custom wake words (e.g., “Hey Jarvis”) or adapting to underrepresented accents and dialects is often infeasible via centralized training due to data scarcity and privacy concerns.</p>
<div class="no-row-height column-margin column-container"><div id="ref-warden2018speech" class="csl-entry" role="listitem">
Warden, Pete. 2018. <span>“Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.”</span> <em>arXiv Preprint arXiv:1804.03209</em>, April. <a href="http://arxiv.org/abs/1804.03209v1">http://arxiv.org/abs/1804.03209v1</a>.
</div></div><p>Few-shot adaptation solves this problem by finetuning only the output classifier or a small subset of parameters, including bias terms, using just a few example utterances collected directly on the device. For example, a user might provide 5–10 recordings of their custom wake word. These samples are then used to update the model locally, while the main encoder remains frozen to preserve generalization and reduce memory overhead. This enables personalization without requiring additional labeled data or transmitting private audio to the cloud.</p>
<p>Such an approach is not only computationally efficient, but also aligned with privacy-preserving design principles. Because only the output layer is updated, often involving a simple gradient step or prototype computation, the total memory footprint and runtime compute are compatible with mobile-class devices or even microcontrollers. This makes KWS a canonical case study for few-shot learning at the edge, where the system must operate under tight constraints while delivering user-specific performance.</p>
<p>Beyond static few-shot learning, many on-device scenarios benefit from streaming adaptation, where models must learn incrementally as new data arrives <span class="citation" data-cites="hayes2020remind">(<a href="#ref-hayes2020remind" role="doc-biblioref">Hayes et al. 2020</a>)</span>. Streaming adaptation generalizes this idea to continuous, asynchronous settings where data arrives incrementally over time. Let <span class="math inline">\(\{x_t\}_{t=1}^{\infty}\)</span> represent a stream of observations. In streaming settings, the model must update itself after observing each new input, typically without access to prior data, and under bounded memory and compute. The model update can be written generically as: <span class="math display">\[
\theta_{t+1} = \theta_t - \eta_t \nabla \mathcal{L}(x_t; \theta_t)
\]</span> where <span class="math inline">\(\eta_t\)</span> is the learning rate at time <span class="math inline">\(t\)</span>. This form of adaptation is sensitive to noise and drift in the input distribution, and thus often incorporates mechanisms such as learning rate decay, meta-learned initialization, or update gating to improve stability.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hayes2020remind" class="csl-entry" role="listitem">
Hayes, Tyler L., Kushal Kafle, Robik Shrestha, Manoj Acharya, and Christopher Kanan. 2020. <span>“REMIND Your Neural Network to Prevent Catastrophic Forgetting.”</span> In <em>Computer Vision – ECCV 2020</em>, 466–83. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-58598-3\_28">https://doi.org/10.1007/978-3-030-58598-3\_28</a>.
</div></div><p>Aside from KWS, practical examples of these strategies abound. In wearable health devices, a model that classifies physical activities may begin with a generic classifier and adapt to user-specific motion patterns using only a few labeled activity segments. In smart assistants, user voice profiles are finetuned over time using ongoing speech input, even when explicit supervision is unavailable. In such cases, local feedback, including correction, repetition, or downstream task success, can serve as implicit signals to guide learning.</p>
<p>Few-shot and streaming adaptation highlight the shift from traditional training pipelines to data-efficient, real-time learning under uncertainty. They form a foundation for more advanced memory and replay strategies, which we turn to next.</p>
</section>
<section id="sec-ondevice-learning-experience-replay-bc9e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-experience-replay-bc9e">Experience Replay</h3>
<p>On-device learning systems face a fundamental tension between continuous adaptation and limited data availability. One common approach to alleviating this tension is experience replay—a memory-based strategy that enables models to retrain on past examples. Originally developed in the context of reinforcement learning and continual learning, replay buffers help prevent catastrophic forgetting and stabilize training in non-stationary environments.</p>
<p>Unlike server-side replay strategies that rely on large datasets and extensive compute, on-device replay must operate with extremely limited capacity, often with tens or hundreds of samples, and must avoid interfering with user experience <span class="citation" data-cites="rolnick2019experience">(<a href="#ref-rolnick2019experience" role="doc-biblioref">Rolnick et al. 2019</a>)</span>. Buffers may store only compressed features or distilled summaries, and updates must occur opportunistically (e.g., during idle cycles or charging). These system-level constraints reshape how replay is implemented and evaluated in the context of embedded ML.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rolnick2019experience" class="csl-entry" role="listitem">
Rolnick, David, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Greg Wayne. 2019. <span>“Experience Replay for Continual Learning.”</span> In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>.
</div></div><p>Let <span class="math inline">\(\mathcal{M}\)</span> represent a memory buffer that retains a fixed-size subset of training examples. At time step <span class="math inline">\(t\)</span>, the model receives a new data point <span class="math inline">\((x_t, y_t)\)</span> and appends it to <span class="math inline">\(\mathcal{M}\)</span>. A replay-based update then samples a batch <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^{k}\)</span> from <span class="math inline">\(\mathcal{M}\)</span> and applies a gradient step: <span class="math display">\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \left[ \frac{1}{k} \sum_{i=1}^{k} \mathcal{L}(x_i, y_i; \theta_t) \right]
\]</span> where <span class="math inline">\(\theta_t\)</span> are the model parameters, <span class="math inline">\(\eta\)</span> is the learning rate, and <span class="math inline">\(\mathcal{L}\)</span> is the loss function. Over time, this replay mechanism allows the model to reinforce prior knowledge while incorporating new information.</p>
<p>A practical on-device implementation might use a ring buffer<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> to store a small set of compressed feature vectors rather than full input examples. The pseudocode as shown in <a href="#lst-replay-buffer" class="quarto-xref">Listing&nbsp;4</a> illustrates a minimal replay buffer designed for constrained environments.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Ring Buffer:</strong> A circular buffer that efficiently manages data by overwriting old entries with new ones as space requires.</p></div></div><div id="lst-replay-buffer" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-replay-buffer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: <strong>Replay Buffer</strong>: Implements a circular storage mechanism for efficient memory management in constrained environments. This approach allows models to efficiently retain and sample from recent data points, balancing the need to leverage historical information while incorporating new insights.
</figcaption>
<div aria-describedby="lst-replay-buffer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Replay Buffer Techniques</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ReplayBuffer:</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, capacity):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.capacity <span class="op">=</span> capacity</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">buffer</span> <span class="op">=</span> []</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> store(<span class="va">self</span>, feature_vec, label):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.<span class="bu">buffer</span>) <span class="op">&lt;</span> <span class="va">self</span>.capacity:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.<span class="bu">buffer</span>.append((feature_vec, label))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.<span class="bu">buffer</span>[<span class="va">self</span>.index] <span class="op">=</span> (feature_vec, label)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.index <span class="op">=</span> (<span class="va">self</span>.index <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.capacity</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, k):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> random.sample(</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">buffer</span>,</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">min</span>(k, <span class="bu">len</span>(<span class="va">self</span>.<span class="bu">buffer</span>))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This implementation maintains a fixed-capacity cyclic buffer, storing compressed representations (e.g., last-layer embeddings) and associated labels. Such buffers are useful for replaying adaptation updates without violating memory or energy budgets.</p>
<p>In TinyML applications, experience replay has been applied to problems such as gesture recognition, where devices must continuously improve predictions while observing a small number of events per day. Instead of training directly on the streaming data, the device stores representative feature vectors from recent gestures and uses them to finetune classification boundaries periodically. Similarly, in on-device keyword spotting, replaying past utterances can improve wake-word detection accuracy without the need to transmit audio data off-device.</p>
<p>While experience replay improves stability in data-sparse or non-stationary environments, it introduces several tradeoffs. Storing raw inputs may breach privacy constraints or exceed storage budgets, especially in vision and audio applications. Replaying from feature vectors reduces memory usage but may limit the richness of gradients for upstream layers. Write cycles to persistent flash memory, which are frequently necessary for long-term storage on embedded devices, can also raise wear-leveling concerns<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>. These constraints require careful co-design of memory usage policies, replay frequency, and feature selection strategies, particularly in continuous deployment scenarios.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;Wear leveling is a technique used in flash memory management to distribute data writes evenly across the memory, prolonging lifespan.</p></div></div></section>
<section id="sec-ondevice-learning-data-compression-98a0" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-data-compression-98a0">Data Compression</h3>
<p>In many on-device learning scenarios, the raw training data may be too large, noisy, or redundant to store and process effectively. This motivates the use of compressed data representations, where the original inputs are transformed into lower-dimensional embeddings or compact encodings that preserve salient information while minimizing memory and compute costs.</p>
<p>Compressed representations serve two complementary goals. First, they reduce the footprint of stored data, allowing devices to maintain longer histories or replay buffers under tight memory budgets <span class="citation" data-cites="sanh2019distilbert">(<a href="#ref-sanh2019distilbert" role="doc-biblioref">Sanh et al. 2019</a>)</span>. Second, they simplify the learning task by projecting raw inputs into more structured feature spaces, often learned via pretraining or meta-learning, in which efficient adaptation is possible with minimal supervision.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sanh2019distilbert" class="csl-entry" role="listitem">
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. <span>“DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.”</span> <em>arXiv Preprint arXiv:1910.01108</em>, October. <a href="http://arxiv.org/abs/1910.01108v4">http://arxiv.org/abs/1910.01108v4</a>.
</div></div><p>One common approach is to encode data points using a pretrained feature extractor and discard the original high-dimensional input. For example, an image <span class="math inline">\(x_i\)</span> might be passed through a convolutional neural network (CNN) to produce an embedding vector <span class="math inline">\(z_i = f(x_i)\)</span>, where <span class="math inline">\(f(\cdot)\)</span> is a fixed feature encoder. This embedding captures visual structure (e.g., shape, texture, or spatial layout) in a compact representation, usually ranging from 64 to 512 dimensions, suitable for lightweight downstream adaptation.</p>
<p>Mathematically, training can proceed over compressed samples <span class="math inline">\((z_i, y_i)\)</span> using a lightweight decoder or projection head. Let <span class="math inline">\(\theta\)</span> represent the trainable parameters of this decoder model, which is typically a small neural network that maps from compressed representations to output predictions. As each example is presented, the model parameters are updated using gradient descent: <span class="math display">\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}\big(g(z_i; \theta), y_i\big)
\]</span> Here:</p>
<ul>
<li><span class="math inline">\(z_i\)</span> is the compressed representation of the <span class="math inline">\(i\)</span>-th input,</li>
<li><span class="math inline">\(y_i\)</span> is the corresponding label or supervision signal,</li>
<li><span class="math inline">\(g(z_i; \theta)\)</span> is the decoder’s prediction,</li>
<li><span class="math inline">\(\mathcal{L}\)</span> is the loss function measuring prediction error,</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate, and</li>
<li><span class="math inline">\(\nabla_\theta\)</span> denotes the gradient with respect to the parameters <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>This formulation highlights how only a compact decoder model, which has the parameter set <span class="math inline">\(\theta\)</span>, needs to be trained, making the learning process feasible even when memory and compute are limited.</p>
<p>Advanced approaches go beyond fixed encoders by learning discrete or sparse dictionaries that represent data using low-rank or sparse coefficient matrices. For instance, a dataset of sensor traces can be factorized as <span class="math inline">\(X \approx DC\)</span>, where <span class="math inline">\(D\)</span> is a dictionary of basis patterns and <span class="math inline">\(C\)</span> is a block-sparse coefficient matrix indicating which patterns are active in each example. By updating only a small number of dictionary atoms or coefficients, the model can adapt with minimal overhead.</p>
<p>Compressed representations are particularly useful in privacy-sensitive settings, as they allow raw data to be discarded or obfuscated after encoding. Furthermore, compression acts as an implicit regularizer, smoothing the learning process and mitigating overfitting when only a few training examples are available.</p>
<p>In practice, these strategies have been applied in domains such as keyword spotting, where raw audio signals are first transformed into Mel-frequency cepstral coefficients (MFCCs)—a compact, lossy representation of the power spectrum of speech. These MFCC vectors serve as compressed inputs for downstream models, enabling local adaptation using only a few kilobytes of memory. Instead of storing raw audio waveforms, which are large and computationally expensive to process, devices store and learn from these compressed feature vectors directly. Similarly, in low-power computer vision systems, embeddings extracted from lightweight CNNs are retained and reused for few-shot learning. These examples illustrate how representation learning and compression serve as foundational tools for scaling on-device learning to memory- and bandwidth-constrained environments.</p>
</section>
<section id="sec-ondevice-learning-tradeoffs-summary-b256" class="level3">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-tradeoffs-summary-b256">Tradeoffs Summary</h3>
<p>Each of the techniques introduced in this section, few-shot learning, experience replay, and compressed data representations, offers a strategy for adapting models on-device when data is scarce or streaming. However, they operate under different assumptions and constraints, and their effectiveness depends on system-level factors such as memory capacity, data availability, task structure, and privacy requirements.</p>
<p>Few-shot adaptation excels when a small but informative set of labeled examples is available, especially when personalization or rapid task-specific tuning is required. It minimizes compute and data needs, but its effectiveness hinges on the quality of pretrained representations and the alignment between the initial model and the local task.</p>
<p>Experience replay addresses continual adaptation by mitigating forgetting and improving stability, especially in non-stationary environments. It enables reuse of past data, but requires memory to store examples and compute cycles for periodic updates. Replay buffers may also raise privacy or longevity concerns, especially on devices with limited storage or flash write cycles.</p>
<p>Compressed data representations reduce the footprint of learning by transforming raw data into compact feature spaces. This approach supports longer retention of experience and efficient finetuning, particularly when only lightweight heads are trainable. However, compression can introduce information loss, and fixed encoders may fail to capture task-relevant variability if they are not well-aligned with deployment conditions. <a href="#tbl-ondevice-techniques" class="quarto-xref">Table&nbsp;2</a> summarizes key tradeoffs:</p>
<div id="tbl-ondevice-techniques" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ondevice-techniques-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>On-Device Learning Trade-Offs</strong>: Few-shot adaptation balances data efficiency with model personalization by leveraging small labeled datasets, but requires careful consideration of memory and compute constraints for deployment on resource-limited devices. The table summarizes key considerations for selecting appropriate on-device learning techniques based on application requirements and available resources.
</figcaption>
<div aria-describedby="tbl-ondevice-techniques-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 24%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Technique</th>
<th style="text-align: left;">Data Requirements</th>
<th style="text-align: left;">Memory/Compute Overhead</th>
<th style="text-align: left;">Use Case Fit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Few-Shot Adaptation</td>
<td style="text-align: left;">Small labeled set (K-shots)</td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Personalization, quick on-device finetuning</td>
</tr>
<tr class="even">
<td style="text-align: left;">Experience Replay</td>
<td style="text-align: left;">Streaming data</td>
<td style="text-align: left;">Moderate (buffer &amp; update)</td>
<td style="text-align: left;">Non-stationary data, stability under drift</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Compressed Representations</td>
<td style="text-align: left;">Unlabeled or encoded data</td>
<td style="text-align: left;">Low to Moderate</td>
<td style="text-align: left;">Memory-limited devices, privacy-sensitive contexts</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In practice, these methods are not mutually exclusive. Many real-world systems combine them to achieve robust, efficient adaptation. For example, a keyword spotting system may use compressed audio features (e.g., MFCCs), finetune a few parameters from a small support set, and maintain a replay buffer of past embeddings for continual refinement.</p>
<p>Together, these strategies embody the core challenge of on-device learning: achieving reliable model improvement under persistent constraints on data, compute, and memory.</p>
<div id="quiz-question-sec-ondevice-learning-data-efficiency-1cee" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Which of the following techniques is most suitable for adapting a model on a device with limited memory and compute capacity?</p>
<ol type="a">
<li>Full model finetuning</li>
<li>Few-shot adaptation</li>
<li>Centralized training</li>
<li>Batch processing</li>
</ol></li>
<li><p>Explain how experience replay helps mitigate catastrophic forgetting in on-device learning systems.</p></li>
<li><p>In on-device learning, ______ representations are used to reduce data footprint and support efficient adaptation.</p></li>
<li><p>True or False: Few-shot adaptation is primarily effective when large, labeled datasets are available on-device.</p></li>
<li><p>Consider a replay buffer with a capacity of 100 samples. If the buffer currently holds 80 samples and a new sample arrives, how many samples will the buffer contain after storing the new sample? Explain the significance of maintaining a fixed-capacity buffer in on-device learning.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-data-efficiency-1cee" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ondevice-learning-federated-learning-6534" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-federated-learning-6534">Federated Learning</h2>
<p>On-device learning enables models to adapt locally using data generated on the device, but doing so in isolation limits a system’s ability to generalize across users and tasks. In many applications, learning must occur not just within a single device, but across a fleet of heterogeneous, intermittently connected systems. This calls for a distributed coordination framework that supports collective model improvement without violating the constraints of privacy, limited connectivity, and device autonomy. Federated learning (FL) is one such framework.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition of Federated Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of Federated Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Federated Learning</strong> is a <em>decentralized machine learning approach</em> in which training occurs across a population of distributed devices, each using its <em>private, locally collected data</em>. Rather than transmitting raw data to a central server, devices share only <em>model updates</em>, including gradients and weight changes, which are then aggregated to improve a shared global model. This approach <em>preserves data privacy</em> while enabling <em>collective intelligence across diverse environments</em>. As federated learning matures, it integrates <em>privacy-enhancing technologies, communication-efficient protocols, and personalization strategies</em>, making it foundational for scalable, privacy-conscious ML systems.</p>
</div>
</div>
<p>To better understand the role of federated learning, it is useful to contrast it with other learning paradigms. <a href="#fig-learning-paradigms" class="quarto-xref">Figure&nbsp;6</a> illustrates the distinction between offline learning, on-device learning, and federated learning. In traditional offline learning, all data is collected and processed centrally. The model is trained in the cloud using curated datasets and is then deployed to edge devices without further adaptation. In contrast, on-device learning enables local model adaptation using data generated on the device itself, supporting personalization but in isolation—without sharing insights across users. Federated learning bridges these two extremes by enabling localized training while coordinating updates globally. It retains data privacy by keeping raw data local, yet benefits from distributed model improvements by aggregating updates from many devices.</p>
<div id="fig-learning-paradigms" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-learning-paradigms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="db3b3c77490433e99581d89b70e6b76a5008efbd.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Learning Paradigm Comparison: Federated learning balances data privacy with collective model improvement by coordinating local training across distributed devices, unlike offline learning’s centralized approach or on-device learning’s isolated adaptation. This figure contrasts how each paradigm handles data location and model update strategies, revealing the trade-offs between personalization, data security, and global knowledge sharing."><img src="ondevice_learning_files/mediabag/db3b3c77490433e99581d89b70e6b76a5008efbd.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-learning-paradigms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Learning Paradigm Comparison</strong>: Federated learning balances data privacy with collective model improvement by coordinating local training across distributed devices, unlike offline learning’s centralized approach or on-device learning’s isolated adaptation. This figure contrasts how each paradigm handles data location and model update strategies, revealing the trade-offs between personalization, data security, and global knowledge sharing.
</figcaption>
</figure>
</div>
<p>This section explores the principles and practical considerations of federated learning in the context of mobile and embedded systems. It begins by outlining the canonical FL protocols and their system implications. It then discusses device participation constraints, communication-efficient update mechanisms, and strategies for personalized learning. Throughout, the emphasis remains on how federated methods can extend the reach of on-device learning by enabling distributed model training across diverse and resource-constrained hardware platforms.</p>
<section id="sec-ondevice-learning-federated-learning-motivation-e16d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-federated-learning-motivation-e16d">Federated Learning Motivation</h3>
<p>Federated learning (FL) is a decentralized paradigm for training machine learning models across a population of devices without transferring raw data to a central server <span class="citation" data-cites="mcmahan2017communication">(<a href="#ref-mcmahan2017communication" role="doc-biblioref">McMahan et al. 2017</a>)</span>. Unlike traditional centralized training pipelines, which require aggregating all training data in a single location, federated learning distributes the training process itself. Each participating device computes updates based on its local data and contributes to a global model through an aggregation protocol, typically coordinated by a central server. This shift in training architecture aligns closely with the needs of mobile, edge, and embedded systems, where privacy, communication cost, and system heterogeneity impose significant constraints on centralized approaches.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hard2018federated" class="csl-entry" role="listitem">
Hard, Andrew, Kanishka Rao, Rajiv Mathews, Saurabh Ramaswamy, Françoise Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage. 2018. <span>“Federated Learning for Mobile Keyboard Prediction.”</span> In <em>International Conference on Learning Representations (ICLR)</em>.
</div></div><p>The relevance of federated learning becomes apparent in several practical domains. In mobile keyboard applications, such as Google’s Gboard, the system must continuously improve text prediction models based on user-specific input patterns <span class="citation" data-cites="hard2018federated">(<a href="#ref-hard2018federated" role="doc-biblioref">Hard et al. 2018</a>)</span>. Federated learning allows the system to train on device-local keystroke data, while maintaining privacy, while still contributing to a shared model that benefits all users. Similarly, wearable health monitors often collect biometric signals that vary greatly between individuals. Training models centrally on such data would require uploading sensitive physiological traces, raising both ethical and regulatory concerns. FL mitigates these issues by enabling model updates to be computed directly on the wearable device.</p>
<p>In the context of smart assistants and voice interfaces, devices must adapt to individual voice profiles while minimizing false activations. Wake-word models, for instance, can be personalized locally and periodically synchronized through federated updates, avoiding the need to transmit raw voice recordings. Industrial and environmental sensors, deployed in remote locations or operating under severe bandwidth limitations, benefit from federated learning by enabling local adaptation and global coordination without constant connectivity.</p>
<p>These examples illustrate how federated learning bridges the gap between model improvement and system-level constraints. It enables personalization without compromising user privacy, supports learning under limited connectivity, and distributes computation across a diverse and heterogeneous device fleet. However, these benefits come with new challenges. Federated learning systems must account for client variability, communication efficiency, and the non-IID nature of local data distributions. Furthermore, they must ensure robustness to adversarial behavior and provide guarantees on model performance despite partial participation or dropout.</p>
<p>The remainder of this section explores the key techniques and tradeoffs that define federated learning in on-device settings. We begin by examining the core learning protocols that govern coordination across devices, and proceed to investigate strategies for scheduling, communication efficiency, and personalization.</p>
</section>
<section id="sec-ondevice-learning-learning-protocols-2916" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-learning-protocols-2916">Learning Protocols</h3>
<p>Federated learning protocols define the rules and mechanisms by which devices collaborate to train a shared model. These protocols govern how local updates are computed, aggregated, and communicated, as well as how devices participate in the training process. The choice of protocol has significant implications for system performance, communication overhead, and model convergence.</p>
<p>In this section, we outline the core components of federated learning protocols, including local training, aggregation methods, and communication strategies. We also discuss the tradeoffs associated with different approaches and their implications for on-device learning systems.</p>
<section id="sec-ondevice-learning-local-training-3430" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-local-training-3430">Local Training</h4>
<p>Local training refers to the process by which individual devices compute model updates based on their local data. This step is important in federated learning, as it allows devices to adapt the shared model to their specific contexts without transferring raw data. The local training process typically involves the following steps:</p>
<ol type="1">
<li><strong>Model Initialization</strong>: Each device initializes its local model parameters, often by downloading the latest global model from the server.</li>
<li><strong>Local Data Sampling</strong>: The device samples a subset of its local data for training. This data may be non-IID, meaning that it may not be uniformly distributed across devices.</li>
<li><strong>Local Training</strong>: The device performs a number of training iterations on its local data, updating the model parameters based on the computed gradients.</li>
<li><strong>Model Update</strong>: After local training, the device computes a model update (e.g., the difference between the updated and initial parameters) and prepares to send it to the server.</li>
<li><strong>Communication</strong>: The device transmits the model update to the server, typically using a secure communication channel to protect user privacy.</li>
<li><strong>Model Aggregation</strong>: The server aggregates the updates from multiple devices to produce a new global model, which is then distributed back to the participating devices.</li>
</ol>
<p>This process is repeated iteratively, with devices periodically downloading the latest global model and performing local training. The frequency of these updates can vary based on system constraints, device availability, and communication costs.</p>
</section>
<section id="sec-ondevice-learning-protocols-overview-64b2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-protocols-overview-64b2">Protocols Overview</h4>
<p>At the heart of federated learning is a coordination mechanism that enables many devices, each having access to only a small, local dataset, to collaboratively train a shared model. This is achieved through a protocol in which client devices perform local training and periodically transmit model updates to a central server. The server aggregates these updates to refine a global model, which is then redistributed to clients for the next training round. This cyclical procedure decouples the learning process from centralized data collection, making it especially well-suited to mobile and edge environments where user data is private, bandwidth is constrained, and device participation is sporadic.</p>
<p>The most widely used baseline for this process is Federated Averaging (FedAvg), which has become a canonical algorithm for federated learning <span class="citation" data-cites="mcmahan2017communication">(<a href="#ref-mcmahan2017communication" role="doc-biblioref">McMahan et al. 2017</a>)</span>. In FedAvg, each device trains its local copy of the model using stochastic gradient descent (SGD) on its private data. After a fixed number of local steps, each device sends its updated model parameters to the server. The server computes a weighted average of these parameters, which are weighted according to the number of data samples on each device, and updates the global model accordingly. This updated model is then sent back to the devices, completing one round of training.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mcmahan2017communication" class="csl-entry" role="listitem">
McMahan, H Brendan, Eider Moore, Daniel Ramage, Seth Hampson, et al. 2017. <span>“Communication-Efficient Learning of Deep Networks from Decentralized Data.”</span> In <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 1273–82.
</div></div><p>Formally, let <span class="math inline">\(\mathcal{D}_k\)</span> denote the local dataset on client <span class="math inline">\(k\)</span>, and let <span class="math inline">\(\theta_k^t\)</span> be the parameters of the model on client <span class="math inline">\(k\)</span> at round <span class="math inline">\(t\)</span>. Each client performs <span class="math inline">\(E\)</span> steps of SGD on its local data, yielding an update <span class="math inline">\(\theta_k^{t+1}\)</span>. The central server then aggregates these updates as: <span class="math display">\[
\theta^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} \theta_k^{t+1}
\]</span> where <span class="math inline">\(n_k = |\mathcal{D}_k|\)</span> is the number of samples on device <span class="math inline">\(k\)</span>, <span class="math inline">\(n = \sum_k n_k\)</span> is the total number of samples across participating clients, and <span class="math inline">\(K\)</span> is the number of active devices in the current round.</p>
<p>This basic structure introduces a number of design choices and tradeoffs. The number of local steps <span class="math inline">\(E\)</span> impacts the balance between computation and communication: larger <span class="math inline">\(E\)</span> reduces communication frequency but risks divergence if local data distributions vary too much. Similarly, the selection of participating clients affects convergence stability and fairness. In real-world deployments, not all devices are available at all times, and hardware capabilities may differ substantially, requiring robust participation scheduling and failure tolerance.</p>
</section>
<section id="sec-ondevice-learning-client-scheduling-aa1e" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-client-scheduling-aa1e">Client Scheduling</h4>
<p>Federated learning operates under the assumption that clients, devices, which hold local data, periodically become available for participation in training rounds. However, in real-world systems, client availability is intermittent and highly variable. Devices may be turned off, disconnected from power, lacking network access, or otherwise unable to participate at any given time. As a result, client scheduling plays a central role in the effectiveness and efficiency of distributed learning.</p>
<p>At a baseline level, federated learning systems define eligibility criteria for participation. Devices must meet minimum requirements such as being plugged in, connected to Wi-Fi, and idle, to avoid interfering with user experience or depleting battery resources. These criteria determine which subset of the total population is considered “available” for any given training round.</p>
<p>Beyond these operational filters, devices also differ in their hardware capabilities, data availability, and network conditions. For example, some smartphones may contain many recent examples relevant to the current task, while others may have outdated or irrelevant data. Network bandwidth and upload speed may vary widely depending on geography and carrier infrastructure. As a result, selecting clients at random can lead to poor coverage of the underlying data distribution and unstable model convergence.</p>
<p>Moreover, availability-driven selection introduces participation bias: clients with favorable conditions, including frequent charging, high-end hardware, and consistent connectivity, are more likely to participate repeatedly, while others are systematically underrepresented. This can skew the resulting model toward behaviors and preferences of a privileged subset of the population, raising both fairness and generalization concerns.</p>
<p>To address these challenges, systems must carefully balance scheduling efficiency with client diversity. A key approach involves using stratified or quota-based sampling to ensure representative client participation across different groups. For instance, asynchronous buffer-based techniques allow participating clients to contribute model updates independently, without requiring synchronized coordination in every round <span class="citation" data-cites="fedbuff">(<a href="#ref-fedbuff" role="doc-biblioref">Nguyen et al. 2021</a>)</span>. This model has been extended to incorporate staleness awareness <span class="citation" data-cites="fedstale">(<a href="#ref-fedstale" role="doc-biblioref">Rodio and Neglia 2024</a>)</span> and fairness mechanisms <span class="citation" data-cites="fedstaleweight">(<a href="#ref-fedstaleweight" role="doc-biblioref">Ma et al. 2024</a>)</span>, preventing bias from over-active clients who might otherwise dominate the training process.</p>
<div class="no-row-height column-margin column-container"><div id="ref-fedbuff" class="csl-entry" role="listitem">
Nguyen, John, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Michael Rabbat, Mani Malek, and Dzmitry Huba. 2021. <span>“Federated Learning with Buffered Asynchronous Aggregation,”</span> June. <a href="http://arxiv.org/abs/2106.06639v4">http://arxiv.org/abs/2106.06639v4</a>.
</div><div id="ref-fedstale" class="csl-entry" role="listitem">
Rodio, Angelo, and Giovanni Neglia. 2024. <span>“FedStale: Leveraging Stale Client Updates in Federated Learning,”</span> May. <a href="http://arxiv.org/abs/2405.04171v1">http://arxiv.org/abs/2405.04171v1</a>.
</div><div id="ref-fedstaleweight" class="csl-entry" role="listitem">
Ma, Jeffrey, Alan Tu, Yiling Chen, and Vijay Janapa Reddi. 2024. <span>“FedStaleWeight: Buffered Asynchronous Federated Learning with Fair Aggregation via Staleness Reweighting,”</span> June. <a href="http://arxiv.org/abs/2406.02877v1">http://arxiv.org/abs/2406.02877v1</a>.
</div></div><p>To address these challenges, federated learning systems implement adaptive client selection strategies. These include prioritizing clients with underrepresented data types, targeting geographies or demographics that are less frequently sampled, and using historical participation data to enforce fairness constraints. Systems may also incorporate predictive modeling to anticipate future client availability or success rates, improving training throughput.</p>
<p>Selected clients perform one or more local training steps on their private data and transmit their model updates to a central server. These updates are aggregated to form a new global model. Typically, this aggregation is weighted, where the contributions of each client are scaled, for example, by the number of local examples used during training, before averaging. This ensures that clients with more representative or larger datasets exert proportional influence on the global model.</p>
<p>These scheduling decisions directly impact system performance. They affect convergence rate, model generalization, energy consumption, and overall user experience. Poor scheduling can result in excessive stragglers, overfitting to narrow client segments, or wasted computation. As a result, client scheduling is not merely a logistical concern—it is a core component of system design in federated learning, demanding both algorithmic insight and infrastructure-level coordination.</p>
</section>
<section id="sec-ondevice-learning-efficient-communication-fb5c" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-efficient-communication-fb5c">Efficient Communication</h4>
<p>One of the principal bottlenecks in federated learning systems is the cost of communication between edge clients and the central server. Transmitting full model weights or gradients after every training round can quickly overwhelm bandwidth and energy budgets—particularly for mobile or embedded devices operating over constrained wireless links. To address this, a range of techniques have been developed to reduce communication overhead while preserving learning efficacy.</p>
<p>These techniques fall into three primary categories: model compression, selective update sharing, and architectural partitioning.</p>
<p>Model compression methods aim to reduce the size of transmitted updates through quantization, sparsification, or subsampling. For instance, instead of sending full-precision gradients, a client may transmit 8-bit quantized updates or communicate only the top-<span class="math inline">\(k\)</span> gradient elements with highest magnitude. These techniques significantly reduce transmission size with limited impact on convergence when applied carefully.</p>
<p>Selective update sharing further reduces communication by transmitting only subsets of model parameters or updates. In layer-wise selective sharing, clients may update only certain layers, typically, the final classifier or adapter modules, while keeping the majority of the backbone frozen. This reduces both upload cost and the risk of overfitting shared representations to non-representative client data.</p>
<p>Split models and architectural partitioning divide the model into a shared global component and a private local component. Clients train and maintain their private modules independently while synchronizing only the shared parts with the server. This allows for user-specific personalization with minimal communication and privacy leakage.</p>
<p>All of these approaches operate within the context of a federated aggregation protocol. A standard baseline for aggregation is Federated Averaging (FedAvg), in which the server updates the global model by computing a weighted average of the client updates received in a given round. Let <span class="math inline">\(\mathcal{K}_t\)</span> denote the set of participating clients in round <span class="math inline">\(t\)</span>, and let <span class="math inline">\(\theta_k^t\)</span> represent the locally updated model parameters from client <span class="math inline">\(k\)</span>. The server computes the new global model <span class="math inline">\(\theta^{t+1}\)</span> as: <span class="math display">\[
\theta^{t+1} = \sum_{k \in \mathcal{K}_t} \frac{n_k}{n_{\mathcal{K}_t}} \theta_k^t
\]</span></p>
<p>Here, <span class="math inline">\(n_k\)</span> is the number of local training examples at client <span class="math inline">\(k\)</span>, and <span class="math inline">\(n_{\mathcal{K}_t} = \sum_{k \in \mathcal{K}_t} n_k\)</span> is the total number of training examples across all participating clients. This data-weighted aggregation ensures that clients with more training data exert a proportionally larger influence on the global model, while also accounting for partial participation and heterogeneous data volumes.</p>
<p>However, communication-efficient updates can introduce tradeoffs. Compression may degrade gradient fidelity, selective updates can limit model capacity, and split architectures may complicate coordination. As a result, effective federated learning requires careful balancing of bandwidth constraints, privacy concerns, and convergence dynamics—a balance that depends heavily on the capabilities and variability of the client population.</p>
</section>
<section id="sec-ondevice-learning-federated-personalization-4673" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-federated-personalization-4673">Federated Personalization</h4>
<p>While compression and communication strategies improve scalability, they do not address a critical limitation of the global federated learning paradigm—its inability to capture user-specific variation. In real-world deployments, devices often observe distinct and heterogeneous data distributions. A one-size-fits-all global model may underperform when applied uniformly across diverse users. This motivates the need for personalized federated learning, where local models are adapted to user-specific data without compromising the benefits of global coordination.</p>
<p>Let <span class="math inline">\(\theta_k\)</span> denote the model parameters on client <span class="math inline">\(k\)</span>, and <span class="math inline">\(\theta_{\text{global}}\)</span> the aggregated global model. Traditional FL seeks to minimize a global objective: <span class="math display">\[
\min_\theta \sum_{k=1}^K w_k \mathcal{L}_k(\theta)
\]</span> where <span class="math inline">\(\mathcal{L}_k(\theta)\)</span> is the local loss on client <span class="math inline">\(k\)</span>, and <span class="math inline">\(w_k\)</span> is a weighting factor (e.g., proportional to local dataset size). However, this formulation assumes that a single model <span class="math inline">\(\theta\)</span> can serve all users well. In practice, local loss landscapes <span class="math inline">\(\mathcal{L}_k\)</span> often differ significantly across clients, reflecting non-IID data distributions and varying task requirements.</p>
<p>Personalization modifies this objective to allow each client to maintain its own adapted parameters <span class="math inline">\(\theta_k\)</span>, optimized with respect to both the global model and local data: <span class="math display">\[
\min_{\theta_1, \ldots, \theta_K} \sum_{k=1}^K \left( \mathcal{L}_k(\theta_k) + \lambda \cdot \mathcal{R}(\theta_k, \theta_{\text{global}}) \right)
\]</span></p>
<p>Here, <span class="math inline">\(\mathcal{R}\)</span> is a regularization term that penalizes deviation from the global model, and <span class="math inline">\(\lambda\)</span> controls the strength of this penalty. This formulation enables local models to deviate as needed, while still benefiting from global coordination.</p>
<p>Real-world use cases illustrate the importance of this approach. Consider a wearable health monitor that tracks physiological signals to classify physical activities. While a global model may perform reasonably well across the population, individual users exhibit unique motion patterns, gait signatures, or sensor placements. Personalized finetuning of the final classification layer or low-rank adapters enables improved accuracy, particularly for rare or user-specific classes.</p>
<p>Several personalization strategies have emerged to address the tradeoffs between compute overhead, privacy, and adaptation speed. One widely used approach is local finetuning, in which each client downloads the latest global model and performs a small number of gradient steps using its private data. While this method is simple and preserves privacy, it may yield suboptimal results when the global model is poorly aligned with the client’s data distribution or when the local dataset is extremely limited.</p>
<p>Another effective technique involves personalization layers, where the model is partitioned into a shared backbone and a lightweight, client-specific head—typically the final classification layer <span class="citation" data-cites="arivazhagan2019federated">(<a href="#ref-arivazhagan2019federated" role="doc-biblioref">Arivazhagan et al. 2019</a>)</span>. Only the head is updated on-device, significantly reducing memory usage and training time. This approach is particularly well-suited for scenarios in which the primary variation across clients lies in output categories or decision boundaries.</p>
<div class="no-row-height column-margin column-container"><div id="ref-arivazhagan2019federated" class="csl-entry" role="listitem">
Arivazhagan, Manoj Ghuhan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. 2019. <span>“Federated Learning with Personalization Layers.”</span> <em>CoRR</em> abs/1912.00818 (December). <a href="http://arxiv.org/abs/1912.00818v1">http://arxiv.org/abs/1912.00818v1</a>.
</div></div><p>Clustered federated learning offers an alternative by grouping clients according to similarities in their data or performance characteristics, and training separate models for each cluster. This strategy can enhance accuracy within homogeneous subpopulations but introduces additional system complexity and may require exchanging metadata to determine group membership.</p>
<p>Finally, meta-learning approaches, such as Model-Agnostic Meta-Learning (MAML), aim to produce a global model initialization that can be quickly adapted to new tasks with just a few local updates <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>. This technique is especially useful when clients have limited data or operate in environments with frequent distributional shifts. Each of these strategies reflects a different point in the tradeoff space. These strategies vary in their system implications, including compute overhead, privacy guarantees, and adaptation latency. <a href="#tbl-personalization-strategies" class="quarto-xref">Table&nbsp;3</a> summarizes the tradeoffs.</p>
<div class="no-row-height column-margin column-container"><div id="ref-finn2017model" class="csl-entry" role="listitem">
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. <span>“Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.”</span> In <em>Proceedings of the 34th International Conference on Machine Learning (ICML)</em>.
</div></div><div id="tbl-personalization-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-personalization-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: <strong>Personalization Trade-Offs</strong>: Federated learning strategies balance personalization with system costs, impacting compute overhead, privacy preservation, and adaptation speed for diverse client populations. This table summarizes how local finetuning, clustered learning, and meta-learning each navigate this trade-off space, enabling tailored models while considering practical deployment constraints.
</figcaption>
<div aria-describedby="tbl-personalization-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 15%">
<col style="width: 19%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Strategy</th>
<th style="text-align: left;">Personalization Mechanism</th>
<th style="text-align: left;">Compute Overhead</th>
<th style="text-align: left;">Privacy Preservation</th>
<th style="text-align: left;">Adaptation Speed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Local Finetuning</td>
<td style="text-align: left;">Gradient descent on local loss post-aggregation</td>
<td style="text-align: left;">Low to Moderate</td>
<td style="text-align: left;">High (no data sharing)</td>
<td style="text-align: left;">Fast (few steps)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Personalization Layers</td>
<td style="text-align: left;">Split model: shared base + user-specific head</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Fast (train small head)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Clustered FL</td>
<td style="text-align: left;">Group clients by data similarity, train per group</td>
<td style="text-align: left;">Moderate to High</td>
<td style="text-align: left;">Medium (group metadata)</td>
<td style="text-align: left;">Medium</td>
</tr>
<tr class="even">
<td style="text-align: left;">Meta-Learning</td>
<td style="text-align: left;">Train for fast adaptation across tasks/devices</td>
<td style="text-align: left;">High (meta-objective)</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Very Fast (few-shot)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Selecting the appropriate personalization method depends on deployment constraints, data characteristics, and the desired balance between accuracy, privacy, and computational efficiency. In practice, hybrid approaches that combine elements of multiple strategies, including local finetuning atop a personalized head, are often employed to achieve robust performance across heterogeneous devices.</p>
</section>
<section id="sec-ondevice-learning-federated-privacy-8320" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-federated-privacy-8320">Federated Privacy</h4>
<p>While federated learning is often motivated by privacy concerns, as it involves keeping raw data localized instead of transmitting it to a central server, the paradigm introduces its own set of security and privacy risks. Although devices do not share their raw data, the transmitted model updates (such as gradients or weight changes) can inadvertently leak information about the underlying private data. Techniques such as model inversion attacks and membership inference attacks demonstrate that adversaries may partially reconstruct or infer properties of local datasets by analyzing these updates.</p>
<p>To mitigate such risks, modern federated learning systems commonly employ protective measures. Secure Aggregation protocols ensure that individual model updates are encrypted and aggregated in a way that the server only observes the combined result, not any individual client’s contribution. Differential Privacy techniques inject carefully calibrated noise into updates to mathematically bound the information that can be inferred about any single client’s data.</p>
<p>While these techniques enhance privacy, they introduce additional system complexity and tradeoffs between model utility, communication cost, and robustness. A deeper exploration of these attacks, defenses, and their implications for federated and on-device learning is provided in a later security and privacy chapter.</p>
<div id="quiz-question-sec-ondevice-learning-federated-learning-6534" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a key advantage of federated learning over traditional centralized learning?</p>
<ol type="a">
<li>Reduced model complexity</li>
<li>Enhanced data privacy</li>
<li>Lower computational requirements</li>
<li>Simplified model training</li>
</ol></li>
<li><p>Explain the role of client scheduling in federated learning and its impact on model convergence and fairness.</p></li>
<li><p>In federated learning, the process of combining model updates from multiple devices is known as ______.</p></li>
<li><p>True or False: Federated learning can completely eliminate the risk of data leakage.</p></li>
<li><p>Discuss the tradeoffs involved in using model compression techniques to reduce communication overhead in federated learning.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-federated-learning-6534" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-ondevice-learning-practical-system-design-3914" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-practical-system-design-3914">Practical System Design</h2>
<p>On-device learning presents opportunities for personalization, privacy preservation, and autonomous adaptation, but realizing these benefits in practice requires disciplined system design. Constraints on memory, compute, energy, and observability necessitate careful selection of adaptation mechanisms, training strategies, and deployment safeguards.</p>
<p>A key principle in building practical systems is to minimize the adaptation footprint. Full-model fine-tuning is typically infeasible on edge platforms, instead, localized update strategies, including bias-only optimization, residual adapters, and lightweight task-specific heads, should be prioritized. These approaches enable model specialization under resource constraints while mitigating the risks of overfitting or instability.</p>
<p>The feasibility of lightweight adaptation depends critically on the strength of offline pretraining <span class="citation" data-cites="bommasani2021opportunities">(<a href="#ref-bommasani2021opportunities" role="doc-biblioref">Bommasani et al. 2021</a>)</span>. Pretrained models should encapsulate generalizable feature representations that allow efficient adaptation from limited local data. Shifting the burden of feature extraction to centralized training reduces the complexity and energy cost of on-device updates, while improving convergence stability in data-sparse environments.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bommasani2021opportunities" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2021. <span>“On the Opportunities and Risks of Foundation Models.”</span> <em>arXiv Preprint arXiv:2108.07258</em>, August. <a href="http://arxiv.org/abs/2108.07258v3">http://arxiv.org/abs/2108.07258v3</a>.
</div></div><p>Even when adaptation is lightweight, opportunistic scheduling remains essential to preserve system responsiveness and user experience. Local updates should be deferred to periods when the device is idle, connected to external power, and operating on a reliable network. Such policies minimize the impact of background training on latency, battery consumption, and thermal performance.</p>
<p>The sensitivity of local training artifacts necessitates careful data security measures. Replay buffers, support sets, adaptation logs, and model update metadata must be protected against unauthorized access or tampering. Lightweight encryption or hardware-backed secure storage can mitigate these risks without imposing prohibitive resource costs on edge platforms.</p>
<p>However, security measures alone do not guarantee model robustness. As models adapt locally, monitoring adaptation dynamics becomes critical. Lightweight validation techniques, including confidence scoring, drift detection heuristics, and shadow model evaluation, can help identify divergence early, enabling systems to trigger rollback mechanisms before severe degradation occurs <span class="citation" data-cites="gama2014survey">(<a href="#ref-gama2014survey" role="doc-biblioref">Gama et al. 2014</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gama2014survey" class="csl-entry" role="listitem">
Gama, João, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. 2014. <span>“A Survey on Concept Drift Adaptation.”</span> <em>ACM Computing Surveys</em> 46 (4): 1–37. <a href="https://doi.org/10.1145/2523813">https://doi.org/10.1145/2523813</a>.
</div></div><p>Robust rollback procedures depend on retaining trusted model checkpoints. Every deployment should preserve a known-good baseline version of the model that can be restored if adaptation leads to unacceptable behavior. This principle is especially important in safety-critical and regulated domains, where failure recovery must be provable and rapid.</p>
<p>In decentralized or federated learning contexts, communication efficiency becomes a first-order design constraint. Compression techniques such as quantized gradient updates, sparsified parameter sets, and selective model transmission must be employed to enable scalable coordination across large, heterogeneous fleets of devices without overwhelming bandwidth or energy budgets <span class="citation" data-cites="konevcny2016federated">(<a href="#ref-konevcny2016federated" role="doc-biblioref">Konečný et al. 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-konevcny2016federated" class="csl-entry" role="listitem">
Konečný, Jakub, H. Brendan McMahan, Daniel Ramage, and Peter Richtárik. 2016. <span>“Federated Optimization: Distributed Machine Learning for on-Device Intelligence.”</span> <em>CoRR</em> abs/1610.02527 (October). <a href="http://arxiv.org/abs/1610.02527v1">http://arxiv.org/abs/1610.02527v1</a>.
</div></div><p>Moreover, when personalization is required, systems should aim for localized adaptation wherever possible. Restricting updates to lightweight components, including final classification heads or modular adapters, constrains the risk of catastrophic forgetting, reduces memory overhead, and accelerates adaptation without destabilizing core model representations.</p>
<p>Finally, throughout the system lifecycle, privacy and compliance requirements must be architected into adaptation pipelines. Mechanisms to support user consent, data minimization, retention limits, and the right to erasure must be considered fundamental aspects of model design, not post-hoc adjustments. Meeting regulatory obligations at scale demands that on-device learning workflows align inherently with principles of auditable autonomy.</p>
<p>The flowchart in <a href="#fig-odl-design-flow" class="quarto-xref">Figure&nbsp;7</a> summarizes key decision points in designing practical, scalable, and resilient on-device learning systems.</p>
<div id="fig-odl-design-flow" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-odl-design-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="3b9efdf1c76a5cc6b25fc2e7103af44de20c5a40.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: On-Device Learning Design: This flowchart guides the systematic development of practical on-device learning systems by outlining key decision points related to data management, model selection, and privacy considerations throughout the system lifecycle. Integrating privacy and compliance requirements—such as user consent and data minimization—into the design process ensures auditable autonomy and scalable deployment of on-device intelligence."><img src="ondevice_learning_files/mediabag/3b9efdf1c76a5cc6b25fc2e7103af44de20c5a40.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-odl-design-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>On-Device Learning Design</strong>: This flowchart guides the systematic development of practical on-device learning systems by outlining key decision points related to data management, model selection, and privacy considerations throughout the system lifecycle. Integrating privacy and compliance requirements—such as user consent and data minimization—into the design process ensures auditable autonomy and scalable deployment of on-device intelligence.
</figcaption>
</figure>
</div>
<div id="quiz-question-sec-ondevice-learning-practical-system-design-3914" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>Which of the following strategies is most effective for minimizing the adaptation footprint in on-device learning systems?</p>
<ol type="a">
<li>Full-model fine-tuning</li>
<li>Bias-only optimization</li>
<li>Increasing model complexity</li>
<li>Centralized data processing</li>
</ol></li>
<li><p>Explain why opportunistic scheduling is crucial for maintaining system responsiveness in on-device learning.</p></li>
<li><p>In on-device learning, ______ techniques such as quantized gradient updates and sparsified parameter sets are used to improve communication efficiency.</p></li>
<li><p>True or False: Security measures alone are sufficient to guarantee model robustness in on-device learning systems.</p></li>
<li><p>Discuss how privacy and compliance requirements should be integrated into the design of on-device learning systems.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-practical-system-design-3914" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="sec-ondevice-learning-challenges-46f2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-challenges-46f2">Challenges</h2>
<p>While on-device learning holds significant promise for enabling adaptive, private, and efficient machine learning at the edge, its practical deployment introduces a range of challenges that extend beyond algorithm design. Unlike conventional centralized systems, where training occurs in controlled environments with uniform hardware and curated datasets, edge systems must contend with heterogeneity in devices, fragmentation in data, and the absence of centralized validation infrastructure. These factors give rise to new systems-level tradeoffs and open questions concerning reliability, safety, and maintainability. Moreover, regulatory and operational constraints complicate the deployment of self-updating models in real-world applications. This section explores these limitations, emphasizing the systemic barriers that must be addressed to make on-device learning robust, scalable, and trustworthy.</p>
<section id="sec-ondevice-learning-heterogeneity-ac2d" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-heterogeneity-ac2d">Heterogeneity</h4>
<p>Federated and on-device learning systems must operate across a vast and diverse ecosystem of devices, ranging from smartphones and wearables to IoT sensors and microcontrollers. This heterogeneity spans multiple dimensions: hardware capabilities, software stacks, network connectivity, and power availability. Unlike cloud-based systems, where environments can be standardized and controlled, edge deployments encounter a wide distribution of system configurations and constraints. These variations introduce significant complexity in algorithm design, resource scheduling, and model deployment.</p>
<p>At the hardware level, devices differ in terms of memory capacity, processor architecture (e.g., ARM Cortex-M vs.&nbsp;A-series), instruction set support (e.g., availability of SIMD or floating-point units), and the presence or absence of AI accelerators. Some clients may possess powerful NPUs capable of running small training loops, while others may rely solely on low-frequency CPUs with minimal RAM. These differences affect the feasible size of models, the choice of training algorithm, and the frequency of updates.</p>
<p>Software heterogeneity compounds the challenge. Devices may run different versions of operating systems, kernel-level drivers, and runtime libraries. Some environments support optimized ML runtimes like TensorFlow Lite Micro or ONNX Runtime Mobile, while others rely on custom inference stacks or restricted APIs. These discrepancies can lead to subtle inconsistencies in behavior, especially when models are compiled differently or when floating-point precision varies across platforms.</p>
<p>In addition to computational heterogeneity, devices exhibit variation in connectivity and uptime. Some are intermittently connected, plugged in only occasionally, or operate under strict bandwidth constraints. Others may have continuous power and reliable networking, but still prioritize user-facing responsiveness over background learning. These differences complicate the orchestration of coordinated learning and the scheduling of updates.</p>
<p>Finally, system fragmentation affects reproducibility and testing. With such a wide range of execution environments, it is difficult to ensure consistent model behavior or to debug failures reliably. This makes monitoring, validation, and rollback mechanisms more critical—but also more difficult to implement uniformly across the fleet.</p>
<p>Consider a federated learning deployment for mobile keyboards. A high-end smartphone might feature 8 GB of RAM, a dedicated AI accelerator, and continuous Wi-Fi access. In contrast, a budget device may have just 2 GB of RAM, no hardware acceleration, and rely on intermittent mobile data. These disparities influence how long training runs can proceed, how frequently models can be updated, and even whether training is feasible at all. To support such a range, the system must dynamically adjust training schedules, model formats, and compression strategies—ensuring equitable model improvement across users while respecting each device’s limitations.</p>
</section>
<section id="sec-ondevice-learning-data-fragmentation-378d" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-data-fragmentation-378d">Data Fragmentation</h4>
<p>In centralized machine learning, data can be aggregated, shuffled, and curated to approximate independent and identically distributed (IID) samples—a key assumption underlying many learning algorithms. In contrast, on-device and federated learning systems must contend with highly fragmented and non-IID data. Each device collects data specific to its user, context, and usage patterns. These data distributions are often skewed, sparse, and dynamically shifting over time.</p>
<p>From a statistical standpoint, the non-IID nature of on-device data leads to challenges in both optimization and generalization. Gradients computed on one device may conflict with those from another, slowing convergence or destabilizing training. Local updates can cause models to overfit to the idiosyncrasies of individual clients, reducing performance when aggregated globally. Moreover, the diversity of data across clients complicates evaluation and model validation: there is no single test set that reflects the true deployment distribution.</p>
<p>The fragmentation also limits the representativeness of any single client’s data. Many clients may observe only a narrow slice of the input space or task distribution, making it difficult to learn robust or generalizable representations. Devices might also encounter new classes or tasks not seen during centralized pretraining, requiring mechanisms for out-of-distribution detection and continual adaptation.</p>
<p>These challenges demand algorithms that are robust to heterogeneity and resilient to imbalanced participation. Techniques such as personalization layers, importance weighting, and adaptive aggregation schemes attempt to mitigate these issues, but there is no universally optimal solution. The degree and nature of non-IID data varies widely across applications, making this one of the most persistent and fundamental challenges in decentralized learning.</p>
<p>A common example of data fragmentation arises in speech recognition systems deployed on personal assistants. Each user exhibits a unique voice profile, accent, and speaking style, which results in significant differences across local datasets. Some users may issue frequent, clearly enunciated commands, while others speak infrequently or in noisy environments. These variations cause device-specific gradients to diverge, especially when training wake-word detectors or adapting language models locally.</p>
<p>In federated learning deployments for virtual keyboards, the problem is further amplified. One user might primarily type in English, another in Hindi, and a third may switch fluidly between multiple languages. The resulting training data is highly non-IID—not only in language but also in vocabulary, phrasing, and typing cadence. A global model trained on aggregated updates may degrade if it fails to capture these localized differences, highlighting the need for adaptive, data-aware strategies that accommodate heterogeneity without sacrificing collective performance.</p>
</section>
<section id="sec-ondevice-learning-monitoring-validation-3151" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-monitoring-validation-3151">Monitoring and Validation</h4>
<p>Unlike centralized machine learning systems, where model updates can be continuously evaluated against a held-out validation set, on-device learning introduces a fundamental shift in visibility and observability. Once deployed, models operate in highly diverse and often disconnected environments, where internal updates may proceed without external monitoring. This creates significant challenges for ensuring that model adaptation is both beneficial and safe.</p>
<p>A core difficulty lies in the absence of centralized validation data. In traditional workflows, models are trained and evaluated using curated datasets that serve as proxies for deployment conditions. On-device learners, by contrast, adapt in response to local inputs, which are rarely labeled and may not be systematically collected. As a result, the quality and direction of updates, whether they enhance generalization or cause drift, are difficult to assess without interfering with the user experience or violating privacy constraints.</p>
<p>The risk of model drift is especially pronounced in streaming settings, where continual adaptation may cause a slow degradation in performance. For instance, a voice recognition model that adapts too aggressively to background noise may eventually overfit to transient acoustic conditions, reducing accuracy on the target task. Without visibility into the evolution of model parameters or outputs, such degradations can remain undetected until they become severe.</p>
<p>Mitigating this problem requires mechanisms for on-device validation and update gating. One approach is to interleave adaptation steps with lightweight performance checks—using proxy objectives or self-supervised signals to approximate model confidence <span class="citation" data-cites="deng2021adaptive">(<a href="#ref-deng2021adaptive" role="doc-biblioref">Y. Deng, Mokhtari, and Ozdaglar 2021</a>)</span>. For example, a keyword spotting system might track detection confidence across recent utterances and suspend updates if confidence consistently drops below a threshold. Alternatively, shadow evaluation can be employed, where multiple model variants are maintained on the device and evaluated in parallel on incoming data streams, allowing the system to compare the adapted model’s behavior against a stable baseline.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deng2021adaptive" class="csl-entry" role="listitem">
Deng, Yuzhe, Aryan Mokhtari, and Asuman Ozdaglar. 2021. <span>“Adaptive Federated Optimization.”</span> In <em>Proceedings of the 38th International Conference on Machine Learning (ICML)</em>.
</div></div><p>Another strategy involves periodic checkpointing and rollback, where snapshots of the model state are saved before adaptation. If subsequent performance degrades, as determined by downstream metrics or user feedback, the system can revert to a known good state. This approach has been used in health monitoring devices, where incorrect predictions could lead to user distrust or safety concerns. However, it introduces storage and compute overhead, especially in memory-constrained environments.</p>
<p>In some cases, federated validation offers a partial solution. Devices can share anonymized model updates or summary statistics with a central server, which aggregates them across users to identify global patterns of drift or failure. While this preserves some degree of privacy, it introduces communication overhead and may not capture rare or user-specific failures.</p>
<p>Ultimately, update monitoring and validation in on-device learning require a rethinking of traditional evaluation practices. Instead of centralized test sets, systems must rely on implicit signals, runtime feedback, and conservative adaptation policies to ensure robustness. The absence of global observability is not merely a technical limitation—it reflects a deeper systems challenge in aligning local adaptation with global reliability.</p>
</section>
<section id="sec-ondevice-learning-resource-management-4bbd" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-resource-management-4bbd">Resource Management</h4>
<p>On-device learning introduces new modes of resource contention that are not present in conventional inference-only deployments. While many edge devices are provisioned to run pretrained models efficiently, they are rarely designed with training workloads in mind. Local adaptation therefore competes for scarce resources, including compute cycles, memory bandwidth, energy, and thermal headroom, with other system processes and user-facing applications.</p>
<p>The most direct constraint is compute availability. Training involves additional forward and backward passes through the model, which can significantly exceed the cost of inference. Even when only a small subset of parameters is updated, for instance, in bias-only or head-only adaptation, backpropagation must still traverse the relevant layers, triggering increased instruction counts and memory traffic. On devices with shared compute units (e.g., mobile SoCs or embedded CPUs), this demand can delay interactive tasks, reduce frame rates, or impair sensor processing.</p>
<p>Energy consumption compounds this problem. Adaptation typically involves sustained computation over multiple input samples, which taxes battery-powered systems and may lead to rapid energy depletion. For instance, performing a single epoch of adaptation on a microcontroller-class device can consume several millijoules—an appreciable fraction of the energy budget for a duty-cycled system operating on harvested power. This necessitates careful scheduling, such that learning occurs only during idle periods, when energy reserves are high and user latency constraints are relaxed.</p>
<p>From a memory perspective, training incurs higher peak usage than inference, due to the need to cache intermediate activations, gradients, and optimizer state <span class="citation" data-cites="lin2020mcunet">(<a href="#ref-lin2020mcunet" role="doc-biblioref">Lin et al. 2020</a>)</span>. These requirements may exceed the static memory footprint anticipated during model deployment, particularly when adaptation involves multiple layers or gradient accumulation. In highly constrained systems, for example, systems with less than 512 KB of RAM, this may preclude certain types of adaptation altogether, unless additional optimization techniques (e.g., checkpointing or low-rank updates) are employed.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2020mcunet" class="csl-entry" role="listitem">
Lin, Ji, Wei-Ming Chen, Yujun Lin, and Song Han. 2020. <span>“MCUNet: Tiny Deep Learning on IoT Devices.”</span> In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>.
</div></div><p>These resource demands must also be balanced against quality of service (QoS) goals. Users expect edge devices to respond reliably and consistently, regardless of whether learning is occurring in the background. Any observable degradation, including dropped audio in a wake-word detector or lag in a wearable display, can erode user trust. As such, many systems adopt opportunistic learning policies, where adaptation is suspended during foreground activity and resumed only when system load is low.</p>
<p>In some deployments, adaptation is further gated by cost constraints imposed by networked infrastructure. For instance, devices may offload portions of the learning workload to nearby gateways or cloudlets<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, introducing bandwidth and communication trade-offs. These hybrid models raise additional questions of task placement and scheduling: should the update occur locally, or be deferred until a high-throughput link is available?</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;<strong>Cloudlets:</strong> Smaller-scale cloud datacenters located at the edge of the internet to decrease latency for mobile and wearable devices.</p></div></div><p>In summary, the cost of on-device learning is not solely measured in FLOPs or memory usage. It manifests as a complex interplay of system load, user experience, energy availability, and infrastructure capacity. Addressing these challenges requires co-design across algorithmic, runtime, and hardware layers, ensuring that adaptation remains unobtrusive, efficient, and sustainable under real-world constraints.</p>
</section>
<section id="sec-ondevice-learning-deployment-risks-92aa" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-deployment-risks-92aa">Deployment Risks</h4>
<p>The deployment of adaptive models on edge devices introduces challenges that extend beyond technical feasibility. In domains where compliance, auditability, and regulatory approval are necessary, including healthcare, finance, and safety-critical systems, on-device learning poses a fundamental tension between system autonomy and control.</p>
<p>In traditional machine learning pipelines, all model updates are centrally managed, versioned, and validated. The training data, model checkpoints, and evaluation metrics are typically recorded in reproducible workflows that support traceability. When learning occurs on the device itself, however, this visibility is lost. Each device may independently evolve its model parameters, influenced by unique local data streams that are never observed by the developer or system maintainer.</p>
<p>This autonomy creates a validation gap. Without access to the input data or the exact update trajectory, it becomes difficult to verify that the learned model still adheres to its original specification or performance guarantees. This is especially problematic in regulated industries, where certification depends on demonstrating that a system behaves consistently across defined operational boundaries. A device that updates itself in response to real-world usage may drift outside those bounds, triggering compliance violations without any external signal.</p>
<p>Moreover, the lack of centralized oversight complicates rollback and failure recovery. If a model update degrades performance, it may not be immediately detectable—particularly in offline scenarios or systems without telemetry. By the time failure is observed, the system’s internal state may have diverged significantly from any known checkpoint, making diagnosis and recovery more complex than in static deployments. This necessitates robust safety mechanisms, such as conservative update thresholds, rollback caches, or dual-model architectures<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> that retain a verified baseline.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;System designs that employ two separate models, typically to enhance reliability and safety by providing a fallback.</p></div></div><p>In addition to compliance challenges, on-device learning introduces new security vulnerabilities. Because model adaptation occurs locally and relies on device-specific, potentially untrusted data streams, adversaries may attempt to manipulate the learning process, by tampering with stored data, such as replay buffers, or by injecting poisoned examples during adaptation, to degrade model performance or introduce vulnerabilities. Furthermore, any locally stored adaptation data, such as feature embeddings or few-shot examples, must be secured against unauthorized access to prevent unintended information leakage.</p>
<p>Maintaining model integrity over time is particularly difficult in decentralized settings, where central monitoring and validation are limited. Autonomous updates could, without external visibility, cause models to drift into unsafe or biased states. These risks are compounded by compliance obligations such as the GDPR’s right to erasure: if user data subtly influences a model through adaptation, tracking and reversing that influence becomes complex.</p>
<p>The security and integrity of self-adapting models, particularly at the edge, pose critical open challenges. A comprehensive treatment of these threats and corresponding mitigation strategies, including attack models and edge-specific defenses, is presented in Chapter 15: Security and Privacy.</p>
<p>Privacy regulations also interact with on-device learning in nontrivial ways. While local adaptation can reduce the need to transmit sensitive data, it may still require storage and processing of personal information, including sensor traces or behavioral logs, on the device itself. Depending on jurisdiction, this may invoke additional requirements for data retention, user consent, and auditability. Systems must be designed to satisfy these requirements without compromising adaptation effectiveness, which often involves encrypting stored data, enforcing retention limits, or implementing user-controlled reset mechanisms.</p>
<p>Lastly, the emergence of edge learning raises open questions about accountability and liability <span class="citation" data-cites="brakerski2022federated">(<a href="#ref-brakerski2022federated" role="doc-biblioref">Brakerski et al. 2022</a>)</span>. When a model adapts autonomously, who is responsible for its behavior? If an adapted model makes a faulty decision, for instance, misdiagnosing a health condition or misinterpreting a voice command, the root cause may lie in local data drift, poor initialization, or insufficient safeguards. Without standardized mechanisms for capturing and analyzing these failure modes, responsibility may be difficult to assign, and regulatory approval harder to obtain.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brakerski2022federated" class="csl-entry" role="listitem">
Brakerski, Zvika et al. 2022. <span>“Federated Learning and the Rise of Edge Intelligence: Challenges and Opportunities.”</span> <em>Communications of the ACM</em> 65 (8): 54–63.
</div></div><p>Addressing these deployment and compliance risks requires new tooling, protocols, and design practices that support auditable autonomy—the ability of a system to adapt in place while still satisfying external requirements for traceability, reproducibility, and user protection. As on-device learning becomes more prevalent, these challenges will become central to both system architecture and governance frameworks.</p>
</section>
<section id="sec-ondevice-learning-challenges-summary-7983" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-challenges-summary-7983">Challenges Summary</h4>
<p>Designing on-device learning systems involves navigating a complex landscape of technical and practical constraints. While localized adaptation enables personalization, privacy, and responsiveness, it also introduces a range of challenges that span hardware heterogeneity, data fragmentation, observability, and regulatory compliance.</p>
<p>System heterogeneity complicates deployment and optimization by introducing variation in compute, memory, and runtime environments. Non-IID data distributions challenge learning stability and generalization, especially when models are trained on-device without access to global context. The absence of centralized monitoring makes it difficult to validate updates or detect performance regressions, and training activity must often compete with core device functionality for energy and compute. Finally, post-deployment learning introduces complications in model governance, from auditability and rollback to privacy assurance.</p>
<p>These challenges are not isolated—they interact in ways that influence the viability of different adaptation strategies. <a href="#tbl-ondevice-challenges" class="quarto-xref">Table&nbsp;4</a> summarizes the primary challenges and their implications for ML systems deployed at the edge.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 38%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Challenge</th>
<th style="text-align: left;">Root Cause</th>
<th style="text-align: left;">System-Level Implications</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">System Heterogeneity</td>
<td style="text-align: left;">Diverse hardware, software, and toolchains</td>
<td style="text-align: left;">Limits portability; requires platform-specific tuning</td>
</tr>
<tr class="even">
<td style="text-align: left;">Non-IID and Fragmented Data</td>
<td style="text-align: left;">Localized, user-specific data distributions</td>
<td style="text-align: left;">Hinders generalization; increases risk of drift</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Limited Observability and Feedback</td>
<td style="text-align: left;">No centralized testing or logging</td>
<td style="text-align: left;">Makes update validation and debugging difficult</td>
</tr>
</tbody>
</table>
<div id="tbl-ondevice-challenges" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ondevice-challenges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: <strong>On-Device Learning Challenges</strong>: System heterogeneity, non-IID data, and limited resources introduce unique challenges for deploying and adapting machine learning models on edge devices, impacting portability, stability, and governance. The table details root causes of these challenges and their system-level implications, highlighting trade-offs between model performance and resource constraints.
</figcaption>
<div aria-describedby="tbl-ondevice-challenges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 38%">
<col style="width: 35%">
</colgroup>
<tbody>
<tr class="odd">
<td>Deployment and Compliance Risk</td>
<td>Learning continues post-deployment</td>
<td>Complicates model versioning, auditing, and rollback</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="quiz-question-sec-ondevice-learning-challenges-46f2" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes a challenge introduced by system heterogeneity in on-device learning?</p>
<ol type="a">
<li>Standardized deployment across all devices</li>
<li>Uniform data distribution across devices</li>
<li>Consistent model behavior across diverse hardware</li>
<li>Centralized validation of model updates</li>
</ol></li>
<li><p>Explain how non-IID data distributions in on-device learning systems affect model generalization and stability.</p></li>
<li><p>True or False: On-device learning systems can easily validate updates using centralized test sets.</p></li>
<li><p>In on-device learning, ______ is a significant challenge due to the competing demands for memory, compute, and battery resources.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-challenges-46f2" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ondevice-learning-summary-ef13" class="level2">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-summary-ef13">Summary</h2>
<p>On-device learning is a major shift in the design and operation of machine learning systems. Rather than relying exclusively on centralized training and static model deployment, this paradigm enables systems to adapt dynamically to local data and usage conditions. This shift is motivated by a confluence of factors—ranging from the need for personalization and privacy preservation to latency constraints and infrastructure efficiency. However, it also introduces a new set of challenges tied to the constrained nature of edge computing platforms.</p>
<p>Throughout this chapter, we explored the architectural and algorithmic strategies that make on-device learning feasible under tight compute, memory, energy, and data constraints. We began by establishing the motivation for moving learning to the edge, followed by a discussion of the system-level limitations that shape practical design choices. A core insight is that no single solution suffices across all use cases. Instead, effective on-device learning systems combine multiple techniques: minimizing the number of trainable parameters, reducing runtime costs, leveraging memory-based adaptation, and compressing data representations for efficient supervision.</p>
<p>We also examined federated learning as a key enabler of decentralized model refinement, particularly when coordination across many heterogeneous devices is required. While federated approaches provide strong privacy guarantees and infrastructure scalability, they introduce new concerns around client scheduling, communication efficiency, and personalization—all of which must be addressed to ensure robust real-world deployments.</p>
<p>Finally, we turned a critical eye toward the limitations of on-device learning, including system heterogeneity, non-IID data distributions, and the absence of reliable evaluation mechanisms in the field. These challenges underscore the importance of co-designing learning algorithms with hardware, runtime, and privacy constraints in mind.</p>
<p>As machine learning continues to expand into mobile, embedded, and wearable environments, the ability to adapt locally, while ensuring responsibility, efficiency, and reliability, will be essential to the next generation of intelligent systems.</p>


<div id="quiz-question-sec-ondevice-learning-summary-ef13" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Explain why it is important to combine multiple techniques, such as minimizing trainable parameters and compressing data representations, in on-device learning systems.</p></li>
<li><p>True or False: On-device learning systems can rely solely on federated learning to address all challenges related to privacy, personalization, and system heterogeneity.</p></li>
<li><p>Which of the following best describes a challenge that arises from the system heterogeneity in on-device learning?</p>
<ol type="a">
<li>Uniform data distribution across devices</li>
<li>Consistent hardware capabilities</li>
<li>Variable compute and memory resources</li>
<li>Standardized evaluation metrics</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-summary-ef13" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-ondevice-learning-deployment-drivers-2eb7" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is NOT a primary motivation for on-device learning?</strong></p>
<ol type="a">
<li>Personalization</li>
<li>Latency and availability</li>
<li>Centralized data aggregation</li>
<li>Privacy</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Centralized data aggregation. On-device learning is motivated by personalization, latency, availability, and privacy, while centralized data aggregation is a characteristic of traditional centralized learning systems.</p>
<p><em>Learning Objective</em>: Understand the primary motivations driving the adoption of on-device learning.</p></li>
<li><p><strong>Explain how on-device learning addresses privacy concerns that are present in centralized machine learning systems.</strong></p>
<p><em>Answer</em>: On-device learning mitigates privacy concerns by keeping data local to the device, thus reducing the need to transmit sensitive information, such as biometric data, to the cloud. This approach helps comply with privacy regulations like GDPR and HIPAA, as it minimizes data exposure and potential breaches.</p>
<p><em>Learning Objective</em>: Analyze how on-device learning enhances privacy compared to centralized systems.</p></li>
<li><p><strong>On-device learning can reduce infrastructure costs by decreasing the need for centralized data processing. (True/False)</strong></p>
<p><em>Answer</em>: True. On-device learning reduces the reliance on centralized infrastructure by distributing the training workload across devices, which decreases the need for extensive data processing and storage in centralized data centers.</p>
<p><em>Learning Objective</em>: Evaluate the impact of on-device learning on infrastructure costs.</p></li>
<li><p><strong>In which scenario is on-device learning particularly beneficial?</strong></p>
<ol type="a">
<li>When data is uniform and consistent across all users</li>
<li>In environments with reliable and high-speed internet connectivity</li>
<li>When user data is highly personalized and varies significantly</li>
<li>For applications that require real-time global data synchronization</li>
</ol>
<p><em>Answer</em>: The correct answer is C. When user data is highly personalized and varies significantly. On-device learning is beneficial in scenarios where data is user-specific and personalized, allowing models to adapt to individual user patterns.</p>
<p><em>Learning Objective</em>: Identify scenarios where on-device learning provides significant advantages over centralized learning.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-deployment-drivers-2eb7" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-design-constraints-d887" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes a key challenge of on-device learning compared to cloud-based learning?</strong></p>
<ol type="a">
<li>Access to extensive compute infrastructure</li>
<li>Availability of large, curated datasets</li>
<li>Constraints on memory and computational resources</li>
<li>Ability to perform floating-point operations</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Constraints on memory and computational resources are a key challenge of on-device learning, as edge devices often have limited capabilities compared to cloud-based environments.</p>
<p><em>Learning Objective</em>: Understand the primary constraints that differentiate on-device learning from cloud-based learning.</p></li>
<li><p><strong>Explain why model complexity is a critical consideration for on-device learning and how it affects energy consumption.</strong></p>
<p><em>Answer</em>: Model complexity affects energy consumption because complex models require more computations, increasing power usage and potentially leading to thermal throttling. On-device learning must balance model expressiveness with energy efficiency to operate within the constraints of battery-powered devices.</p>
<p><em>Learning Objective</em>: Analyze the impact of model complexity on energy consumption in on-device learning scenarios.</p></li>
<li><p><strong>In on-device learning, data is often ______, which presents challenges for model convergence and generalization.</strong></p>
<p><em>Answer</em>: non-IID. Non-independent and identically distributed data can lead to challenges in ensuring that models generalize well across different devices and user scenarios.</p>
<p><em>Learning Objective</em>: Recognize the implications of non-IID data on the generalization of on-device learning models.</p></li>
<li><p><strong>True or False: On-device learning systems can easily implement conventional deep learning libraries due to their advanced hardware capabilities.</strong></p>
<p><em>Answer</em>: False. On-device learning systems often lack the hardware capabilities to support conventional deep learning libraries, requiring specialized techniques like quantization and integer arithmetic.</p>
<p><em>Learning Objective</em>: Understand the hardware limitations of on-device learning systems and the need for specialized techniques.</p></li>
<li><p><strong>Describe a scenario where on-device learning must adapt to compute constraints without degrading user experience.</strong></p>
<p><em>Answer</em>: In a smartphone-based speech recognizer, on-device learning must adapt to compute constraints by performing training during low activity periods or charging times, ensuring that inference latency and system responsiveness are not affected.</p>
<p><em>Learning Objective</em>: Apply understanding of compute constraints to real-world scenarios in on-device learning.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-design-constraints-d887" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-model-adaptation-0b2b" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary advantage of bias-only adaptation in on-device learning?</strong></p>
<ol type="a">
<li>High expressivity and flexibility</li>
<li>Minimal memory and compute requirements</li>
<li>Ability to handle significant domain shifts</li>
<li>Increased training complexity</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Bias-only adaptation minimizes memory and compute requirements by updating only scalar offsets, making it suitable for devices with tight resource constraints.</p>
<p><em>Learning Objective</em>: Understand the advantages of bias-only adaptation in resource-constrained environments.</p></li>
<li><p><strong>Explain why residual adapters are beneficial in on-device learning for mobile devices.</strong></p>
<p><em>Answer</em>: Residual adapters introduce a small number of trainable parameters, allowing for greater flexibility and personalization without significantly increasing memory or compute requirements. This makes them suitable for mobile devices where moderate adaptation capacity is needed.</p>
<p><em>Learning Objective</em>: Analyze the benefits of using residual adapters in mobile device contexts.</p></li>
<li><p><strong>In task-adaptive sparse updates, only a ______ subset of model parameters is updated to achieve meaningful personalization.</strong></p>
<p><em>Answer</em>: minimal. This approach reduces memory and compute costs by focusing updates on the most impactful parameters for the task.</p>
<p><em>Learning Objective</em>: Understand the concept and benefits of task-adaptive sparse updates.</p></li>
<li><p><strong>True or False: Low-rank parameterizations in on-device learning are primarily used to increase the number of trainable parameters.</strong></p>
<p><em>Answer</em>: False. Low-rank parameterizations reduce the number of trainable parameters by approximating updates with smaller matrices, thus saving computation and memory.</p>
<p><em>Learning Objective</em>: Clarify misconceptions about the purpose of low-rank parameterizations.</p></li>
<li><p><strong>Order the following steps involved in task-adaptive sparse updates: 1) Evaluate improvement in validation accuracy, 2) Freeze the entire model, 3) Rank layers by performance gain per unit cost, 4) Unfreeze one candidate layer, 5) Finetune briefly.</strong></p>
<p><em>Answer</em>: 2, 4, 5, 1, 3. First, freeze the entire model, then unfreeze one candidate layer, finetune briefly, evaluate improvement in validation accuracy, and finally rank layers by performance gain per unit cost.</p>
<p><em>Learning Objective</em>: Understand the process of implementing task-adaptive sparse updates.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-model-adaptation-0b2b" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-data-efficiency-1cee" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following techniques is most suitable for adapting a model on a device with limited memory and compute capacity?</strong></p>
<ol type="a">
<li>Full model finetuning</li>
<li>Few-shot adaptation</li>
<li>Centralized training</li>
<li>Batch processing</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Few-shot adaptation is designed to work with limited data and compute resources, making it suitable for on-device learning where memory and compute capacity are constrained.</p>
<p><em>Learning Objective</em>: Identify appropriate on-device learning techniques for constrained environments.</p></li>
<li><p><strong>Explain how experience replay helps mitigate catastrophic forgetting in on-device learning systems.</strong></p>
<p><em>Answer</em>: Experience replay mitigates catastrophic forgetting by storing past examples in a replay buffer, allowing the model to reinforce prior knowledge while learning from new data. This helps stabilize training in non-stationary environments by periodically revisiting past experiences.</p>
<p><em>Learning Objective</em>: Understand the role of experience replay in preventing catastrophic forgetting.</p></li>
<li><p><strong>In on-device learning, ______ representations are used to reduce data footprint and support efficient adaptation.</strong></p>
<p><em>Answer</em>: compressed. Compressed representations reduce the data footprint by transforming raw inputs into lower-dimensional embeddings, enabling efficient adaptation under memory and compute constraints.</p>
<p><em>Learning Objective</em>: Recall the use of compressed representations in on-device learning.</p></li>
<li><p><strong>True or False: Few-shot adaptation is primarily effective when large, labeled datasets are available on-device.</strong></p>
<p><em>Answer</em>: False. Few-shot adaptation is effective when only a small set of labeled examples is available, making it suitable for on-device learning where data is scarce.</p>
<p><em>Learning Objective</em>: Challenge misconceptions about the data requirements for few-shot adaptation.</p></li>
<li><p><strong>Consider a replay buffer with a capacity of 100 samples. If the buffer currently holds 80 samples and a new sample arrives, how many samples will the buffer contain after storing the new sample? Explain the significance of maintaining a fixed-capacity buffer in on-device learning.</strong></p>
<p><em>Answer</em>: The buffer will contain 81 samples after storing the new sample. Maintaining a fixed-capacity buffer ensures that memory usage remains constant, which is crucial for on-device learning where resources are limited. It also allows for efficient overwriting of old samples, ensuring that the most recent data is prioritized.</p>
<p><em>Learning Objective</em>: Calculate and understand the operational implications of maintaining a fixed-capacity replay buffer.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-data-efficiency-1cee" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-federated-learning-6534" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a key advantage of federated learning over traditional centralized learning?</strong></p>
<ol type="a">
<li>Reduced model complexity</li>
<li>Enhanced data privacy</li>
<li>Lower computational requirements</li>
<li>Simplified model training</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Federated learning enhances data privacy by keeping raw data localized on devices and only sharing model updates, which reduces the risk of data breaches associated with centralized data collection.</p>
<p><em>Learning Objective</em>: Understand the privacy benefits of federated learning compared to centralized learning.</p></li>
<li><p><strong>Explain the role of client scheduling in federated learning and its impact on model convergence and fairness.</strong></p>
<p><em>Answer</em>: Client scheduling in federated learning determines which devices participate in training rounds based on availability and resource criteria. It impacts model convergence by ensuring diverse data representation and fairness by preventing bias towards frequently available clients. Effective scheduling balances these factors to improve model generalization and system performance.</p>
<p><em>Learning Objective</em>: Analyze the importance of client scheduling in federated learning systems.</p></li>
<li><p><strong>In federated learning, the process of combining model updates from multiple devices is known as ______.</strong></p>
<p><em>Answer</em>: aggregation. Aggregation combines model updates from multiple devices to form a new global model, ensuring that the learning process benefits from diverse data without compromising individual privacy.</p>
<p><em>Learning Objective</em>: Recall the terminology and process of aggregating model updates in federated learning.</p></li>
<li><p><strong>True or False: Federated learning can completely eliminate the risk of data leakage.</strong></p>
<p><em>Answer</em>: False. While federated learning reduces the risk of data leakage by keeping raw data on devices, it does not completely eliminate it. Model updates can still leak information, necessitating additional privacy-preserving techniques.</p>
<p><em>Learning Objective</em>: Evaluate the privacy limitations of federated learning.</p></li>
<li><p><strong>Discuss the tradeoffs involved in using model compression techniques to reduce communication overhead in federated learning.</strong></p>
<p><em>Answer</em>: Model compression techniques, such as quantization and sparsification, reduce communication overhead by decreasing the size of model updates. However, they can degrade gradient fidelity, potentially affecting model convergence and accuracy. Balancing these tradeoffs requires careful consideration of system constraints and the variability of client data.</p>
<p><em>Learning Objective</em>: Analyze the tradeoffs of communication-efficient strategies in federated learning.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-federated-learning-6534" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-practical-system-design-3914" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following strategies is most effective for minimizing the adaptation footprint in on-device learning systems?</strong></p>
<ol type="a">
<li>Full-model fine-tuning</li>
<li>Bias-only optimization</li>
<li>Increasing model complexity</li>
<li>Centralized data processing</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Bias-only optimization is effective for minimizing the adaptation footprint as it allows for model specialization under resource constraints without the need for full-model fine-tuning, which is typically infeasible on edge platforms.</p>
<p><em>Learning Objective</em>: Understand strategies for minimizing adaptation footprint in on-device learning.</p></li>
<li><p><strong>Explain why opportunistic scheduling is crucial for maintaining system responsiveness in on-device learning.</strong></p>
<p><em>Answer</em>: Opportunistic scheduling ensures that local updates occur during periods when the device is idle, connected to external power, and on a reliable network. This minimizes the impact of background training on latency, battery consumption, and thermal performance, thereby maintaining system responsiveness.</p>
<p><em>Learning Objective</em>: Analyze the role of opportunistic scheduling in maintaining system responsiveness.</p></li>
<li><p><strong>In on-device learning, ______ techniques such as quantized gradient updates and sparsified parameter sets are used to improve communication efficiency.</strong></p>
<p><em>Answer</em>: compression. Compression techniques like quantized gradient updates and sparsified parameter sets help improve communication efficiency, enabling scalable coordination across devices without overwhelming bandwidth or energy budgets.</p>
<p><em>Learning Objective</em>: Identify techniques used to improve communication efficiency in on-device learning.</p></li>
<li><p><strong>True or False: Security measures alone are sufficient to guarantee model robustness in on-device learning systems.</strong></p>
<p><em>Answer</em>: False. Security measures are important, but they do not guarantee model robustness. Monitoring adaptation dynamics and employing rollback mechanisms are also crucial to prevent severe degradation in model performance.</p>
<p><em>Learning Objective</em>: Understand the limitations of security measures in ensuring model robustness.</p></li>
<li><p><strong>Discuss how privacy and compliance requirements should be integrated into the design of on-device learning systems.</strong></p>
<p><em>Answer</em>: Privacy and compliance requirements should be architected into adaptation pipelines from the outset. This includes mechanisms for user consent, data minimization, retention limits, and the right to erasure. These elements should be fundamental to model design to meet regulatory obligations at scale.</p>
<p><em>Learning Objective</em>: Evaluate the integration of privacy and compliance requirements in on-device learning system design.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-practical-system-design-3914" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-challenges-46f2" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes a challenge introduced by system heterogeneity in on-device learning?</strong></p>
<ol type="a">
<li>Standardized deployment across all devices</li>
<li>Uniform data distribution across devices</li>
<li>Consistent model behavior across diverse hardware</li>
<li>Centralized validation of model updates</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Consistent model behavior across diverse hardware is a challenge due to differences in memory, processor architecture, and software environments, which require platform-specific tuning.</p>
<p><em>Learning Objective</em>: Understand the impact of system heterogeneity on model deployment and behavior in on-device learning.</p></li>
<li><p><strong>Explain how non-IID data distributions in on-device learning systems affect model generalization and stability.</strong></p>
<p><em>Answer</em>: Non-IID data distributions lead to challenges in model generalization and stability because each device collects unique, user-specific data, which can cause models to overfit to local idiosyncrasies and diverge from global performance expectations.</p>
<p><em>Learning Objective</em>: Analyze the effects of non-IID data on learning stability and model generalization in decentralized systems.</p></li>
<li><p><strong>True or False: On-device learning systems can easily validate updates using centralized test sets.</strong></p>
<p><em>Answer</em>: False. On-device learning systems lack centralized test sets for validation, making it difficult to assess updates without interfering with user experience or violating privacy constraints.</p>
<p><em>Learning Objective</em>: Understand the challenges of validating model updates in on-device learning environments.</p></li>
<li><p><strong>In on-device learning, ______ is a significant challenge due to the competing demands for memory, compute, and battery resources.</strong></p>
<p><em>Answer</em>: resource contention. This challenge arises because adaptation workloads compete with other system processes, requiring careful scheduling to maintain user experience.</p>
<p><em>Learning Objective</em>: Recognize the impact of resource contention on the performance and scheduling of on-device learning tasks.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-challenges-46f2" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-summary-ef13" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Explain why it is important to combine multiple techniques, such as minimizing trainable parameters and compressing data representations, in on-device learning systems.</strong></p>
<p><em>Answer</em>: Combining multiple techniques is crucial in on-device learning systems due to the constrained nature of edge devices. Minimizing trainable parameters reduces computational load and energy consumption, while compressing data representations allows for efficient use of limited memory and bandwidth. This multi-faceted approach ensures that the system can adapt dynamically to local conditions without overwhelming the device’s resources.</p>
<p><em>Learning Objective</em>: Understand the necessity of integrating various techniques to address the constraints of on-device learning environments.</p></li>
<li><p><strong>True or False: On-device learning systems can rely solely on federated learning to address all challenges related to privacy, personalization, and system heterogeneity.</strong></p>
<p><em>Answer</em>: False. While federated learning provides privacy and scalability benefits, it does not address all challenges in on-device learning. Issues such as client scheduling, communication efficiency, and personalization require additional strategies beyond federated learning to ensure robust deployment.</p>
<p><em>Learning Objective</em>: Recognize the limitations of federated learning and the need for complementary strategies in on-device learning systems.</p></li>
<li><p><strong>Which of the following best describes a challenge that arises from the system heterogeneity in on-device learning?</strong></p>
<ol type="a">
<li>Uniform data distribution across devices</li>
<li>Consistent hardware capabilities</li>
<li>Variable compute and memory resources</li>
<li>Standardized evaluation metrics</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Variable compute and memory resources. System heterogeneity in on-device learning refers to the diverse range of hardware capabilities across devices, which affects how models are trained and deployed. This variability requires adaptive strategies to ensure models can perform effectively across different environments.</p>
<p><em>Learning Objective</em>: Identify challenges related to system heterogeneity in on-device learning and understand their implications.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-summary-ef13" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/ops/ops.html" class="pagination-link" aria-label="ML Operations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">ML Operations</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/robust_ai/robust_ai.html" class="pagination-link" aria-label="Robust AI">
        <span class="nav-page-text">Robust AI</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>