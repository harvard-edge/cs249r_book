<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/privacy_security/privacy_security.html" rel="next">
<link href="../../../contents/core/ops/ops.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ae75ed80ef5b3e74590777de1ac3d8c3.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-fb9419aa16e212485fb71aeffa090357.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-88c09ca42f84778652146dd5e6ffaef1.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-fb9419aa16e212485fb71aeffa090357.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script type="module" src="../../../tools/scripts/socratiQ/bundle.js" defer=""></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<script src="../../../assets/scripts/version-link.js" defer=""></script>
<script src="../../../assets/scripts/subscribe-modal.js" defer=""></script>
<style>
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-colab {
  --color1: #FFF5E6;
  --color2: #FF6B35;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
</style>
<style>
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_chapter_connection.png");
}
details.callout-colab > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_colab.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_videos.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_exercises.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_quiz_question.png");
}
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_code.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_quiz_answer.png");
}
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_example.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_definition.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_slides.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/tinytorch" target="_blank"> <i class="bi bi-fire" role="img">
</i> 
<span class="menu-text">TinyTorch</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-hands-on" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-code" role="img">
</i> 
 <span class="menu-text">Hands-on</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-hands-on">    
        <li>
    <a class="dropdown-item" href="../../../labs"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">Labs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../kits"><i class="bi bi-box" role="img">
</i> 
 <span class="dropdown-text">Kits</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-downloads" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-download" role="img">
</i> 
 <span class="menu-text">Downloads</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-downloads">    
        <li>
    <a class="dropdown-item" href="../../../assets/downloads/Machine-Learning-Systems.pdf" target="_blank"><i class="bi bi-file-pdf" role="img">
</i> 
 <span class="dropdown-text">PDF</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../assets/downloads/Machine-Learning-Systems.epub" target="_blank"><i class="bi bi-book" role="img">
</i> 
 <span class="dropdown-text">EPUB</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book#support-this-work" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../#subscribe"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text">Subscribe</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/ops/ops.html">Robust Deployment</a></li><li class="breadcrumb-item"><a href="../../../contents/core/ondevice_learning/ondevice_learning.html">On-Device Learning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="fce6c4125da43eee52b57ef05deaae8b" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>üåü <strong>2025 Goal achieved:</strong> 10,000 GitHub stars. Thank you! üôè <a href="https://www.linkedin.com/events/7402387002232520704/">Celebrate on Dec 17 ‚Üí</a><br> üéÑ <strong>Upcoming Release:</strong> Tinyüî•Torch ML framework. Don‚Äôt import it. <a href="https://mlsysbook.ai/tinytorch">Build it ‚Üí</a><br> üéâ <strong>Coming 2026:</strong> Textbook (Volume 1) published by The MIT Press. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news ‚Üí</a></p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-ondevice-learning" id="toc-sec-ondevice-learning" class="nav-link active" data-scroll-target="#sec-ondevice-learning">On-Device Learning</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-ondevice-learning-distributed-learning-paradigm-shift-883d" id="toc-sec-ondevice-learning-distributed-learning-paradigm-shift-883d" class="nav-link" data-scroll-target="#sec-ondevice-learning-distributed-learning-paradigm-shift-883d">Distributed Learning Paradigm Shift</a></li>
  <li><a href="#sec-ondevice-learning-motivations-benefits-37c3" id="toc-sec-ondevice-learning-motivations-benefits-37c3" class="nav-link" data-scroll-target="#sec-ondevice-learning-motivations-benefits-37c3">Motivations and Benefits</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-ondevice-learning-benefits-3256" id="toc-sec-ondevice-learning-ondevice-learning-benefits-3256" class="nav-link" data-scroll-target="#sec-ondevice-learning-ondevice-learning-benefits-3256">On-Device Learning Benefits</a></li>
  <li><a href="#sec-ondevice-learning-alternative-approaches-decision-criteria-bd36" id="toc-sec-ondevice-learning-alternative-approaches-decision-criteria-bd36" class="nav-link" data-scroll-target="#sec-ondevice-learning-alternative-approaches-decision-criteria-bd36">Alternative Approaches and Decision Criteria</a></li>
  <li><a href="#sec-ondevice-learning-realworld-application-domains-c4e4" id="toc-sec-ondevice-learning-realworld-application-domains-c4e4" class="nav-link" data-scroll-target="#sec-ondevice-learning-realworld-application-domains-c4e4">Real-World Application Domains</a></li>
  <li><a href="#sec-ondevice-learning-architectural-tradeoffs-centralized-vs-decentralized-training-d420" id="toc-sec-ondevice-learning-architectural-tradeoffs-centralized-vs-decentralized-training-d420" class="nav-link" data-scroll-target="#sec-ondevice-learning-architectural-tradeoffs-centralized-vs-decentralized-training-d420">Architectural Trade-offs: Centralized vs.&nbsp;Decentralized Training</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-design-constraints-c776" id="toc-sec-ondevice-learning-design-constraints-c776" class="nav-link" data-scroll-target="#sec-ondevice-learning-design-constraints-c776">Design Constraints</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-quantifying-training-overhead-edge-devices-3e4c" id="toc-sec-ondevice-learning-quantifying-training-overhead-edge-devices-3e4c" class="nav-link" data-scroll-target="#sec-ondevice-learning-quantifying-training-overhead-edge-devices-3e4c">Quantifying Training Overhead on Edge Devices</a></li>
  <li><a href="#sec-ondevice-learning-model-constraints-9232" id="toc-sec-ondevice-learning-model-constraints-9232" class="nav-link" data-scroll-target="#sec-ondevice-learning-model-constraints-9232">Model Constraints</a></li>
  <li><a href="#sec-ondevice-learning-data-constraints-303e" id="toc-sec-ondevice-learning-data-constraints-303e" class="nav-link" data-scroll-target="#sec-ondevice-learning-data-constraints-303e">Data Constraints</a></li>
  <li><a href="#sec-ondevice-learning-compute-constraints-4d6d" id="toc-sec-ondevice-learning-compute-constraints-4d6d" class="nav-link" data-scroll-target="#sec-ondevice-learning-compute-constraints-4d6d">Compute Constraints</a></li>
  <li><a href="#sec-ondevice-learning-edge-hardware-integration-challenges-a240" id="toc-sec-ondevice-learning-edge-hardware-integration-challenges-a240" class="nav-link" data-scroll-target="#sec-ondevice-learning-edge-hardware-integration-challenges-a240">Edge Hardware Integration Challenges</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-energy-thermal-constraint-analysis-3133" id="toc-sec-ondevice-learning-energy-thermal-constraint-analysis-3133" class="nav-link" data-scroll-target="#sec-ondevice-learning-energy-thermal-constraint-analysis-3133">Energy and Thermal Constraint Analysis</a></li>
  <li><a href="#sec-ondevice-learning-memory-hierarchy-optimization-8396" id="toc-sec-ondevice-learning-memory-hierarchy-optimization-8396" class="nav-link" data-scroll-target="#sec-ondevice-learning-memory-hierarchy-optimization-8396">Memory Hierarchy Optimization</a></li>
  <li><a href="#sec-ondevice-learning-mobile-ai-accelerator-optimization-67fe" id="toc-sec-ondevice-learning-mobile-ai-accelerator-optimization-67fe" class="nav-link" data-scroll-target="#sec-ondevice-learning-mobile-ai-accelerator-optimization-67fe">Mobile AI Accelerator Optimization</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-holistic-resource-management-strategies-9e4b" id="toc-sec-ondevice-learning-holistic-resource-management-strategies-9e4b" class="nav-link" data-scroll-target="#sec-ondevice-learning-holistic-resource-management-strategies-9e4b">Holistic Resource Management Strategies</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-model-adaptation-6a82" id="toc-sec-ondevice-learning-model-adaptation-6a82" class="nav-link" data-scroll-target="#sec-ondevice-learning-model-adaptation-6a82">Model Adaptation</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-weight-freezing-3407" id="toc-sec-ondevice-learning-weight-freezing-3407" class="nav-link" data-scroll-target="#sec-ondevice-learning-weight-freezing-3407">Weight Freezing</a></li>
  <li><a href="#sec-ondevice-learning-structured-parameter-updates-f1a1" id="toc-sec-ondevice-learning-structured-parameter-updates-f1a1" class="nav-link" data-scroll-target="#sec-ondevice-learning-structured-parameter-updates-f1a1">Structured Parameter Updates</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-adapterbased-adaptation-6a9a" id="toc-sec-ondevice-learning-adapterbased-adaptation-6a9a" class="nav-link" data-scroll-target="#sec-ondevice-learning-adapterbased-adaptation-6a9a">Adapter-Based Adaptation</a></li>
  <li><a href="#sec-ondevice-learning-lowrank-techniques-4570" id="toc-sec-ondevice-learning-lowrank-techniques-4570" class="nav-link" data-scroll-target="#sec-ondevice-learning-lowrank-techniques-4570">Low-Rank Techniques</a></li>
  <li><a href="#sec-ondevice-learning-edge-personalization-a511" id="toc-sec-ondevice-learning-edge-personalization-a511" class="nav-link" data-scroll-target="#sec-ondevice-learning-edge-personalization-a511">Edge Personalization</a></li>
  <li><a href="#sec-ondevice-learning-performance-vs-resource-tradeoffs-2be2" id="toc-sec-ondevice-learning-performance-vs-resource-tradeoffs-2be2" class="nav-link" data-scroll-target="#sec-ondevice-learning-performance-vs-resource-tradeoffs-2be2">Performance vs.&nbsp;Resource Trade-offs</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-sparse-updates-879b" id="toc-sec-ondevice-learning-sparse-updates-879b" class="nav-link" data-scroll-target="#sec-ondevice-learning-sparse-updates-879b">Sparse Updates</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-sparse-update-design-ee7c" id="toc-sec-ondevice-learning-sparse-update-design-ee7c" class="nav-link" data-scroll-target="#sec-ondevice-learning-sparse-update-design-ee7c">Sparse Update Design</a></li>
  <li><a href="#sec-ondevice-learning-layer-selection-ab3c" id="toc-sec-ondevice-learning-layer-selection-ab3c" class="nav-link" data-scroll-target="#sec-ondevice-learning-layer-selection-ab3c">Layer Selection</a></li>
  <li><a href="#sec-ondevice-learning-selective-layer-update-implementation-eed2" id="toc-sec-ondevice-learning-selective-layer-update-implementation-eed2" class="nav-link" data-scroll-target="#sec-ondevice-learning-selective-layer-update-implementation-eed2">Selective Layer Update Implementation</a></li>
  <li><a href="#sec-ondevice-learning-tinytrain-personalization-66f2" id="toc-sec-ondevice-learning-tinytrain-personalization-66f2" class="nav-link" data-scroll-target="#sec-ondevice-learning-tinytrain-personalization-66f2">TinyTrain Personalization</a></li>
  <li><a href="#sec-ondevice-learning-adaptation-strategy-tradeoffs-cb23" id="toc-sec-ondevice-learning-adaptation-strategy-tradeoffs-cb23" class="nav-link" data-scroll-target="#sec-ondevice-learning-adaptation-strategy-tradeoffs-cb23">Adaptation Strategy Trade-offs</a></li>
  <li><a href="#sec-ondevice-learning-adaptation-strategy-comparison-1faf" id="toc-sec-ondevice-learning-adaptation-strategy-comparison-1faf" class="nav-link" data-scroll-target="#sec-ondevice-learning-adaptation-strategy-comparison-1faf">Adaptation Strategy Comparison</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-data-efficiency-c701" id="toc-sec-ondevice-learning-data-efficiency-c701" class="nav-link" data-scroll-target="#sec-ondevice-learning-data-efficiency-c701">Data Efficiency</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-fewshot-learning-data-streaming-566e" id="toc-sec-ondevice-learning-fewshot-learning-data-streaming-566e" class="nav-link" data-scroll-target="#sec-ondevice-learning-fewshot-learning-data-streaming-566e">Few-Shot Learning and Data Streaming</a></li>
  <li><a href="#sec-ondevice-learning-experience-replay-737e" id="toc-sec-ondevice-learning-experience-replay-737e" class="nav-link" data-scroll-target="#sec-ondevice-learning-experience-replay-737e">Experience Replay</a></li>
  <li><a href="#sec-ondevice-learning-data-compression-8b40" id="toc-sec-ondevice-learning-data-compression-8b40" class="nav-link" data-scroll-target="#sec-ondevice-learning-data-compression-8b40">Data Compression</a></li>
  <li><a href="#sec-ondevice-learning-data-efficiency-strategy-comparison-4f92" id="toc-sec-ondevice-learning-data-efficiency-strategy-comparison-4f92" class="nav-link" data-scroll-target="#sec-ondevice-learning-data-efficiency-strategy-comparison-4f92">Data Efficiency Strategy Comparison</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-federated-learning-6e7e" id="toc-sec-ondevice-learning-federated-learning-6e7e" class="nav-link" data-scroll-target="#sec-ondevice-learning-federated-learning-6e7e">Federated Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-privacypreserving-collaborative-learning-ecf9" id="toc-sec-ondevice-learning-privacypreserving-collaborative-learning-ecf9" class="nav-link" data-scroll-target="#sec-ondevice-learning-privacypreserving-collaborative-learning-ecf9">Privacy-Preserving Collaborative Learning</a></li>
  <li><a href="#sec-ondevice-learning-learning-protocols-139a" id="toc-sec-ondevice-learning-learning-protocols-139a" class="nav-link" data-scroll-target="#sec-ondevice-learning-learning-protocols-139a">Learning Protocols</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-local-training-e73d" id="toc-sec-ondevice-learning-local-training-e73d" class="nav-link" data-scroll-target="#sec-ondevice-learning-local-training-e73d">Local Training</a></li>
  <li><a href="#sec-ondevice-learning-federated-aggregation-protocols-5d37" id="toc-sec-ondevice-learning-federated-aggregation-protocols-5d37" class="nav-link" data-scroll-target="#sec-ondevice-learning-federated-aggregation-protocols-5d37">Federated Aggregation Protocols</a></li>
  <li><a href="#sec-ondevice-learning-client-scheduling-f675" id="toc-sec-ondevice-learning-client-scheduling-f675" class="nav-link" data-scroll-target="#sec-ondevice-learning-client-scheduling-f675">Client Scheduling</a></li>
  <li><a href="#sec-ondevice-learning-bandwidthaware-update-compression-7730" id="toc-sec-ondevice-learning-bandwidthaware-update-compression-7730" class="nav-link" data-scroll-target="#sec-ondevice-learning-bandwidthaware-update-compression-7730">Bandwidth-Aware Update Compression</a></li>
  <li><a href="#sec-ondevice-learning-federated-personalization-3c73" id="toc-sec-ondevice-learning-federated-personalization-3c73" class="nav-link" data-scroll-target="#sec-ondevice-learning-federated-personalization-3c73">Federated Personalization</a></li>
  <li><a href="#sec-ondevice-learning-federated-privacy-a1ed" id="toc-sec-ondevice-learning-federated-privacy-a1ed" class="nav-link" data-scroll-target="#sec-ondevice-learning-federated-privacy-a1ed">Federated Privacy</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-largescale-device-orchestration-1360" id="toc-sec-ondevice-learning-largescale-device-orchestration-1360" class="nav-link" data-scroll-target="#sec-ondevice-learning-largescale-device-orchestration-1360">Large-Scale Device Orchestration</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-network-bandwidth-optimization-53da" id="toc-sec-ondevice-learning-network-bandwidth-optimization-53da" class="nav-link" data-scroll-target="#sec-ondevice-learning-network-bandwidth-optimization-53da">Network and Bandwidth Optimization</a></li>
  <li><a href="#sec-ondevice-learning-asynchronous-device-synchronization-348e" id="toc-sec-ondevice-learning-asynchronous-device-synchronization-348e" class="nav-link" data-scroll-target="#sec-ondevice-learning-asynchronous-device-synchronization-348e">Asynchronous Device Synchronization</a></li>
  <li><a href="#sec-ondevice-learning-managing-milliondevice-heterogeneity-86c1" id="toc-sec-ondevice-learning-managing-milliondevice-heterogeneity-86c1" class="nav-link" data-scroll-target="#sec-ondevice-learning-managing-milliondevice-heterogeneity-86c1">Managing Million-Device Heterogeneity</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-production-integration-beb5" id="toc-sec-ondevice-learning-production-integration-beb5" class="nav-link" data-scroll-target="#sec-ondevice-learning-production-integration-beb5">Production Integration</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-mlops-integration-challenges-bb4e" id="toc-sec-ondevice-learning-mlops-integration-challenges-bb4e" class="nav-link" data-scroll-target="#sec-ondevice-learning-mlops-integration-challenges-bb4e">MLOps Integration Challenges</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-deployment-pipeline-transformations-431d" id="toc-sec-ondevice-learning-deployment-pipeline-transformations-431d" class="nav-link" data-scroll-target="#sec-ondevice-learning-deployment-pipeline-transformations-431d">Deployment Pipeline Transformations</a></li>
  <li><a href="#sec-ondevice-learning-monitoring-system-evolution-94c9" id="toc-sec-ondevice-learning-monitoring-system-evolution-94c9" class="nav-link" data-scroll-target="#sec-ondevice-learning-monitoring-system-evolution-94c9">Monitoring System Evolution</a></li>
  <li><a href="#sec-ondevice-learning-continuous-training-orchestration-8974" id="toc-sec-ondevice-learning-continuous-training-orchestration-8974" class="nav-link" data-scroll-target="#sec-ondevice-learning-continuous-training-orchestration-8974">Continuous Training Orchestration</a></li>
  <li><a href="#sec-ondevice-learning-validation-strategy-adaptation-ab3d" id="toc-sec-ondevice-learning-validation-strategy-adaptation-ab3d" class="nav-link" data-scroll-target="#sec-ondevice-learning-validation-strategy-adaptation-ab3d">Validation Strategy Adaptation</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-bioinspired-learning-efficiency-55ad" id="toc-sec-ondevice-learning-bioinspired-learning-efficiency-55ad" class="nav-link" data-scroll-target="#sec-ondevice-learning-bioinspired-learning-efficiency-55ad">Bio-Inspired Learning Efficiency</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-learning-biological-neural-efficiency-74b5" id="toc-sec-ondevice-learning-learning-biological-neural-efficiency-74b5" class="nav-link" data-scroll-target="#sec-ondevice-learning-learning-biological-neural-efficiency-74b5">Learning from Biological Neural Efficiency</a></li>
  <li><a href="#sec-ondevice-learning-unlabeled-data-exploitation-strategies-cded" id="toc-sec-ondevice-learning-unlabeled-data-exploitation-strategies-cded" class="nav-link" data-scroll-target="#sec-ondevice-learning-unlabeled-data-exploitation-strategies-cded">Unlabeled Data Exploitation Strategies</a></li>
  <li><a href="#sec-ondevice-learning-lifelong-adaptation-without-forgetting-9200" id="toc-sec-ondevice-learning-lifelong-adaptation-without-forgetting-9200" class="nav-link" data-scroll-target="#sec-ondevice-learning-lifelong-adaptation-without-forgetting-9200">Lifelong Adaptation Without Forgetting</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-systems-integration-production-deployment-c6bb" id="toc-sec-ondevice-learning-systems-integration-production-deployment-c6bb" class="nav-link" data-scroll-target="#sec-ondevice-learning-systems-integration-production-deployment-c6bb">Systems Integration for Production Deployment</a></li>
  <li><a href="#sec-ondevice-learning-persistent-technical-operational-challenges-8c12" id="toc-sec-ondevice-learning-persistent-technical-operational-challenges-8c12" class="nav-link" data-scroll-target="#sec-ondevice-learning-persistent-technical-operational-challenges-8c12">Persistent Technical and Operational Challenges</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-device-data-heterogeneity-management-a789" id="toc-sec-ondevice-learning-device-data-heterogeneity-management-a789" class="nav-link" data-scroll-target="#sec-ondevice-learning-device-data-heterogeneity-management-a789">Device and Data Heterogeneity Management</a></li>
  <li><a href="#sec-ondevice-learning-noniid-data-distribution-challenges-42cd" id="toc-sec-ondevice-learning-noniid-data-distribution-challenges-42cd" class="nav-link" data-scroll-target="#sec-ondevice-learning-noniid-data-distribution-challenges-42cd">Non-IID Data Distribution Challenges</a></li>
  <li><a href="#sec-ondevice-learning-distributed-system-observability-2270" id="toc-sec-ondevice-learning-distributed-system-observability-2270" class="nav-link" data-scroll-target="#sec-ondevice-learning-distributed-system-observability-2270">Distributed System Observability</a>
  <ul class="collapse">
  <li><a href="#sec-ondevice-learning-performance-evaluation-dynamic-environments-82a9" id="toc-sec-ondevice-learning-performance-evaluation-dynamic-environments-82a9" class="nav-link" data-scroll-target="#sec-ondevice-learning-performance-evaluation-dynamic-environments-82a9">Performance Evaluation in Dynamic Environments</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-resource-management-691a" id="toc-sec-ondevice-learning-resource-management-691a" class="nav-link" data-scroll-target="#sec-ondevice-learning-resource-management-691a">Resource Management</a></li>
  <li><a href="#sec-ondevice-learning-identifying-preventing-system-failures-ee88" id="toc-sec-ondevice-learning-identifying-preventing-system-failures-ee88" class="nav-link" data-scroll-target="#sec-ondevice-learning-identifying-preventing-system-failures-ee88">Identifying and Preventing System Failures</a></li>
  <li><a href="#sec-ondevice-learning-production-deployment-risk-assessment-db49" id="toc-sec-ondevice-learning-production-deployment-risk-assessment-db49" class="nav-link" data-scroll-target="#sec-ondevice-learning-production-deployment-risk-assessment-db49">Production Deployment Risk Assessment</a></li>
  <li><a href="#sec-ondevice-learning-engineering-challenge-synthesis-92d4" id="toc-sec-ondevice-learning-engineering-challenge-synthesis-92d4" class="nav-link" data-scroll-target="#sec-ondevice-learning-engineering-challenge-synthesis-92d4">Engineering Challenge Synthesis</a></li>
  <li><a href="#sec-ondevice-learning-foundations-robust-ai-systems-3fde" id="toc-sec-ondevice-learning-foundations-robust-ai-systems-3fde" class="nav-link" data-scroll-target="#sec-ondevice-learning-foundations-robust-ai-systems-3fde">Foundations for Robust AI Systems</a></li>
  </ul></li>
  <li><a href="#sec-ondevice-learning-fallacies-pitfalls-6c6d" id="toc-sec-ondevice-learning-fallacies-pitfalls-6c6d" class="nav-link" data-scroll-target="#sec-ondevice-learning-fallacies-pitfalls-6c6d">Fallacies and Pitfalls</a></li>
  <li><a href="#sec-ondevice-learning-summary-0af9" id="toc-sec-ondevice-learning-summary-0af9" class="nav-link" data-scroll-target="#sec-ondevice-learning-summary-0af9">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/ops/ops.html">Robust Deployment</a></li><li class="breadcrumb-item"><a href="../../../contents/core/ondevice_learning/ondevice_learning.html">On-Device Learning</a></li></ol></nav></header>





<section id="sec-ondevice-learning" class="level1 page-columns page-full">
<h1>On-Device Learning</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL¬∑E 3 Prompt: Drawing of a smartphone with its internal components exposed, revealing diverse miniature engineers of different genders and skin tones actively working on the ML model. The engineers, including men, women, and non-binary individuals, are tuning parameters, repairing connections, and enhancing the network on the fly. Data flows into the ML model, being processed in real-time, and generating output inferences.</em></p>
</div></div><p> <img src="images/png/cover_ondevice_learning.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>Why does on-device learning represent the most fundamental architectural shift in machine learning systems since the separation of training and inference, and what makes this capability essential for the future of intelligent systems?</em></p>
<p>On-device learning dismantles the assumption governing machine learning architecture for decades: the separation between where models are trained and where they operate. This redefines what systems can become by enabling continuous adaptation in the real world rather than static deployment of pre-trained models. The shift from centralized training to distributed, adaptive learning transforms systems from passive inference engines into intelligent agents capable of personalization, privacy preservation, and autonomous improvement in disconnected environments. This architectural revolution becomes essential as AI systems move beyond controlled data centers into unpredictable environments where pre-training cannot anticipate every scenario or deployment condition. Understanding on-device learning principles enables engineers to design systems that break free from static model limitations, creating adaptive intelligence that learns and evolves at the point of human interaction.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Distinguish on-device learning from centralized training approaches by comparing computational distribution, data locality, and coordination mechanisms</p></li>
<li><p>Identify key motivational drivers (personalization, latency, privacy, infrastructure efficiency) and evaluate when on-device learning is appropriate versus alternative approaches</p></li>
<li><p>Analyze how training amplifies resource constraints compared to inference, quantifying memory (3-5<span class="math inline">\(\times\)</span>), computational (2-3<span class="math inline">\(\times\)</span>), and energy overhead impacts on system design</p></li>
<li><p>Evaluate adaptation strategies including weight freezing, residual updates, and sparse updates by comparing their resource consumption, expressivity, and suitability for different device classes</p></li>
<li><p>Examine data efficiency techniques for learning with limited local datasets, including few-shot learning, experience replay, and data compression methods</p></li>
<li><p>Apply federated learning protocols to coordinate privacy-preserving model updates across heterogeneous device populations while managing communication efficiency and convergence challenges</p></li>
<li><p>Design on-device learning systems that integrate thermal management, memory hierarchy optimization, and power budgeting to maintain acceptable user experience</p></li>
<li><p>Implement practical deployment strategies that address MLOps integration challenges including device-aware pipelines, distributed monitoring, and heterogeneous update coordination</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-ondevice-learning-distributed-learning-paradigm-shift-883d" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-distributed-learning-paradigm-shift-883d">Distributed Learning Paradigm Shift</h2>
<p>Operational frameworks (<strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>) establish the foundation for managing machine learning systems at scale through centralized orchestration, monitoring, and deployment pipelines. These frameworks assume controlled cloud environments where computational resources are abundant, network connectivity is reliable, and system behavior is predictable. However, as machine learning systems increasingly move beyond data centers to edge devices, these fundamental assumptions begin to break down.</p>
<p>A smartphone learning to predict user text input, a smart home device adapting to household routines, or an autonomous vehicle updating its perception models based on local driving conditions exemplify scenarios where traditional centralized training approaches prove inadequate. The smartphone encounters linguistic patterns unique to individual users that were not present in global training data. The smart home device must adapt to seasonal changes and family dynamics that vary dramatically across households. The autonomous vehicle faces local road conditions, weather patterns, and traffic behaviors that differ from its original training environment.</p>
<p>These scenarios exemplify on-device learning, where models must train and adapt directly on the devices where they operate<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. This paradigm transforms machine learning from a centralized discipline to a distributed ecosystem where learning occurs across millions of heterogeneous devices, each operating under unique constraints and local conditions.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>A11 Bionic Breakthrough</strong>: Apple‚Äôs A11 Bionic (2017) was the first mobile chip with sufficient computational power for on-device training, delivering 0.6 TOPS compared to the previous A10‚Äôs 0.2 TOPS. This <span class="math inline">\(3\times\)</span> improvement, combined with 4.3 billion transistors and a dual-core Neural Engine, allowed gradient computation for the first time on mobile devices. Google‚Äôs Pixel Visual Core achieved similar capabilities with 8 custom Image Processing Units optimized for machine learning workloads.</p></div></div><p>The transition to on-device learning introduces fundamental tension in machine learning systems design. While cloud-based architectures leverage abundant computational resources and controlled operational environments, edge devices must function within severely constrained resource envelopes characterized by limited memory capacity, restricted computational throughput, constrained energy budgets, and intermittent network connectivity. These constraints that make on-device learning technically challenging simultaneously enable its most significant advantages: personalized adaptation through localized data processing, privacy preservation through data locality, and operational autonomy through independence from centralized infrastructure.</p>
<p>This chapter examines the theoretical foundations and practical methodologies necessary to navigate this architectural tension. Building on computational efficiency principles (<strong><a href="../efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 9: Efficient AI</a></strong>) and operational frameworks (<strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>), we investigate the specialized algorithmic techniques, architectural design patterns, and system-level principles that enable effective learning under extreme resource constraints. The challenge extends beyond conventional optimization of training algorithms, requiring reconceptualization of the entire machine learning pipeline for deployment environments where traditional computational assumptions fail.</p>
<div id="callout-definition*-1.1" class="callout callout-definition" title="On-Device Learning">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>On-Device Learning</summary><div><strong><em>On-Device Learning</em></strong> is the local training or adaptation of machine learning models directly on deployed hardware without server connectivity, enabling <em>personalization</em>, <em>privacy preservation</em>, and <em>autonomous operation</em> under severe resource constraints.<p></p>
</div></details>
</div>
<p>The implications of this paradigm extend far beyond technical optimization, challenging established assumptions regarding machine learning system development, deployment, and maintenance lifecycles. Models transition from following predictable versioning patterns to exhibiting continuous divergence and adaptation trajectories. Performance evaluation methodologies shift from centralized monitoring dashboards to distributed assessment across heterogeneous user populations. Privacy preservation evolves from a regulatory compliance consideration to a core architectural requirement that shapes system design decisions.</p>
<p>Understanding these systemic implications requires examining both the compelling motivations driving organizational adoption of on-device learning and the substantial technical challenges that must be addressed. This analysis establishes the theoretical foundations and practical methodologies required to architect systems capable of effective learning at the network edge while operating within stringent constraints.</p>
<div id="quiz-question-sec-ondevice-learning-distributed-learning-paradigm-shift-883d" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>What is a primary advantage of on-device learning in machine learning systems?</p>
<ol type="a">
<li>Increased reliance on centralized servers</li>
<li>Simplified system architecture</li>
<li>Unlimited computational resources</li>
<li>Improved privacy through data locality</li>
</ol></li>
<li><p>True or False: On-device learning eliminates the need for computational efficiency in machine learning models.</p></li>
<li><p>Explain how the transition from centralized to on-device learning affects the deployment and maintenance lifecycles of machine learning models.</p></li>
<li><p>Which of the following is a challenge faced by on-device learning compared to centralized learning?</p>
<ol type="a">
<li>Abundant computational resources</li>
<li>Limited memory capacity</li>
<li>Reliable network connectivity</li>
<li>Predictable system behavior</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-distributed-learning-paradigm-shift-883d" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
<section id="sec-ondevice-learning-motivations-benefits-37c3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-motivations-benefits-37c3">Motivations and Benefits</h2>
<p>Machine learning systems have traditionally relied on centralized training pipelines, where models are developed and refined using large, curated datasets and powerful cloud-based infrastructure <span class="citation" data-cites="dean2012large">(<a href="#ref-dean2012large" role="doc-biblioref">Dean et al. 2012</a>)</span>. Once trained, these models are deployed to client devices for inference, creating a clear separation between the training and deployment phases. While this architectural separation has served most use cases well, it imposes significant limitations in modern applications where local data is dynamic, private, or highly personalized.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dean2012large" class="csl-entry" role="listitem">
Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, et al. 2012. <span>‚ÄúLarge Scale Distributed Deep Networks.‚Äù</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div></div><p>On-device learning challenges this established model by enabling systems to train or adapt directly on the device, without relying on constant connectivity to the cloud. This shift represents more than a technological advancement, it reflects changing application requirements and user expectations that demand responsive, personalized, and privacy-preserving machine learning systems.</p>
<p>Consider a smartphone keyboard adapting to a user‚Äôs unique vocabulary and typing patterns. To personalize predictions, the system must perform gradient updates on a compact language model using locally observed text input. A single gradient update for even a minimal language model requires 50-100&nbsp;MB of memory for activations and optimizer state. Modern smartphones typically allocate 200-300&nbsp;MB to background applications like keyboards (varies by OS and device generation). This razor-thin margin, where a single training step consumes 25% of available memory, exemplifies the central engineering challenge of on-device learning. The system must achieve meaningful personalization while operating within constraints so severe that traditional training approaches become architecturally infeasible. This quantitative reality drives the need for specialized techniques that make adaptation possible within extreme resource limitations.</p>
<section id="sec-ondevice-learning-ondevice-learning-benefits-3256" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-ondevice-learning-benefits-3256">On-Device Learning Benefits</h3>
<p>Understanding the driving forces behind on-device learning adoption requires examining the inherent limitations of traditional centralized approaches. Traditional machine learning systems rely on a clear division of labor between model training and inference. Training is performed in centralized environments with access to high-performance compute resources and large-scale datasets. Once trained, models are distributed to client devices, where they operate in a static inference-only mode.</p>
<p>While this centralized paradigm has proven effective in many deployments, it introduces fundamental limitations in scenarios where data is user-specific, behavior is dynamic, or connectivity is intermittent. These limitations become particularly acute as machine learning moves beyond controlled environments into real-world applications with diverse user populations and deployment contexts.</p>
<p>On-device learning addresses these limitations by enabling deployed devices to perform model adaptation using locally available data. On-device learning is not merely an efficiency optimization; it serves as a cornerstone of building trustworthy AI systems, opening Part IV: Trustworthy Systems. By keeping data local, it provides a powerful foundation for privacy. By adapting to individual users, it enhances fairness and utility. By enabling offline operation, it improves robustness against network failures and infrastructure dependencies. This chapter explores the engineering required to build these trustworthy, adaptive systems.</p>
<p>This shift from centralized to decentralized learning is motivated by four key considerations that reflect both technological capabilities and changing application requirements: personalization, latency and availability, privacy, and infrastructure efficiency <span class="citation" data-cites="li2020federated">(<a href="#ref-li2020federated" role="doc-biblioref">Li et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>Personalization represents the most compelling motivation, as deployed models often encounter usage patterns and data distributions that differ substantially from their training environments. Local adaptation allows models to refine behavior in response to user-specific data, capturing linguistic preferences, physiological baselines, sensor characteristics, or environmental conditions. This capability proves essential in applications with high inter-user variability, where a single global model cannot serve all users effectively.</p>
<p>Latency and availability constraints provide additional justification for local learning. In edge computing scenarios, connectivity to centralized infrastructure may be unreliable, delayed, or intentionally limited to preserve bandwidth or reduce energy consumption. On-device learning enables autonomous improvement of models even in fully offline or delay-sensitive contexts, where round-trip updates to the cloud are architecturally infeasible.</p>
<p>Privacy considerations provide a third compelling driver. Many modern applications involve sensitive or regulated data, including biometric measurements, typed input, location traces, or health information. Local learning mitigates privacy concerns by keeping raw data on the device and operating within privacy-preserving boundaries, potentially aiding adherence to regulations such as GDPR<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, HIPAA <span class="citation" data-cites="hipaa1996health">(<a href="#ref-hipaa1996health" role="doc-biblioref">Tomes 1996</a>)</span>, or region-specific data sovereignty laws.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<strong>GDPR‚Äôs ML Impact</strong>: When GDPR took effect in May 2018 <span class="citation" data-cites="gdpr2016regulation">(<a href="#ref-gdpr2016regulation" role="doc-biblioref">Rasmussen et al. 2024</a>)</span>, it made centralized ML training illegal for personal data without explicit consent. The ‚Äúright to be forgotten‚Äù also meant models trained on personal data could be legally required to ‚Äúunlearn‚Äù specific users, technically impossible with traditional training. This drove massive investment in privacy-preserving ML techniques.</p><div id="ref-gdpr2016regulation" class="csl-entry" role="listitem">
Rasmussen, Torben Riis, Anja Gouliaev, Erik Jakobsen, Karin Hjorthaug, Lene Unmack Larsen, Peter Meldgaard, Jesper Thygesen, et al. 2024. <span>‚ÄúImpact of Multidisciplinary Team Discrepancies on Comparative Lung Cancer Outcome Analyses and Treatment Equality.‚Äù</span> <em>BMC Cancer</em> 24 (1): 1423. <a href="https://doi.org/10.1186/s12885-024-13188-4">https://doi.org/10.1186/s12885-024-13188-4</a>.
</div></div><div id="ref-hipaa1996health" class="csl-entry" role="listitem">
Tomes, JP. 1996. <span>‚ÄúThe Health Insurance Portability and Accountability Act of 1996: Understanding the Anti-Kickback Laws.‚Äù</span> <em>Journal of Health Care Finance</em> 25 (2): 55‚Äì62. <a href="https://www.hhs.gov/hipaa/index.html">https://www.hhs.gov/hipaa/index.html</a>.
</div></div><p>Infrastructure efficiency provides economic motivation for distributed learning approaches. Centralized training pipelines require substantial backend infrastructure to collect, store, and process user data from potentially millions of devices. By shifting learning to the edge, systems reduce communication costs and distribute training workloads across the deployment fleet, relieving pressure on centralized resources while improving scalability.</p>
</section>
<section id="sec-ondevice-learning-alternative-approaches-decision-criteria-bd36" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-alternative-approaches-decision-criteria-bd36">Alternative Approaches and Decision Criteria</h3>
<p>On-device learning represents a significant engineering investment with inherent complexity that may not be justified by the benefits. Before committing to this approach, teams should carefully evaluate whether simpler alternatives can achieve comparable results with lower operational overhead. Understanding when not to implement on-device learning is as important as understanding its benefits, as premature adoption can introduce unnecessary complexity without proportional value.</p>
<p>Several alternative approaches often suffice for personalization and adaptation requirements without local training complexity:</p>
<ul>
<li><p><strong>Feature-based Personalization</strong>: Provides effective customization by storing user preferences, interaction history, and behavioral features locally. Rather than adapting model weights, the system feeds these stored features into a static model to achieve personalization. News recommendation systems exemplify this approach by storing user topic preferences and reading patterns locally, then combining these features with a centralized content model to provide personalized recommendations without model updates.</p></li>
<li><p><strong>Cloud-based Fine-tuning with Privacy Controls</strong>: Enables personalization through centralized adaptation with appropriate privacy safeguards. User data is processed in batches during off-peak hours using privacy-preserving techniques such as differential privacy<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> or federated analytics. This approach often achieves superior accuracy compared to resource-constrained on-device updates while maintaining acceptable privacy properties for many applications.</p></li>
<li><p><strong>User-specific Lookup Tables</strong>: Combine global models with personalized retrieval mechanisms. The system maintains a lightweight, user-specific lookup table for frequently accessed patterns while using a shared global model for generalization. This hybrid approach provides personalization benefits with minimal computational and storage overhead.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Differential Privacy</strong>: Mathematical framework that provides quantifiable privacy guarantees by adding carefully calibrated noise to computations. In federated learning, DP ensures that individual user data cannot be inferred from model updates, even by aggregators. Key parameter Œµ controls privacy-utility tradeoff: smaller Œµ means stronger privacy but lower model accuracy. Typical deployments use Œµ=1-8, requiring noise addition that can increase communication overhead by 2-10<span class="math inline">\(\times\)</span> and reduce model accuracy by 1-5%. Essential for regulatory compliance and user trust in distributed learning systems.</p></div></div><p>The decision to implement on-device learning should be driven by quantifiable requirements that preclude these simpler alternatives. True data privacy constraints that legally prohibit cloud processing, genuine network limitations that prevent reliable connectivity, quantitative latency budgets that preclude cloud round-trips, or demonstrable performance improvements that justify the operational complexity represent legitimate drivers for on-device learning adoption.</p>
<p>For applications with critical timing requirements (camera processing under 33&nbsp;ms, voice response under 500&nbsp;ms, AR/VR motion-to-photon latency under 20&nbsp;ms, or safety-critical control under 10&nbsp;ms), network round-trip times (typically 50-200&nbsp;ms) make cloud-based alternatives architecturally infeasible. In such scenarios, on-device learning becomes necessary regardless of complexity considerations. Teams should thoroughly evaluate simpler solutions before committing to the significant engineering investment that on-device learning requires.</p>
<p>These motivations are grounded in the broader concept of knowledge transfer, where a pretrained model transfers useful representations to a new task or domain. This foundational principle makes on-device learning both feasible and effective, enabling sophisticated adaptation with minimal local resources. As depicted in <a href="#fig-transfer-conceptual" class="quarto-xref">Figure&nbsp;1</a>, knowledge transfer can occur between closely related tasks (e.g., playing different board games or musical instruments), or across domains that share structure (e.g., from riding a bicycle to driving a scooter). In the context of on-device learning, this means leveraging a model pretrained in the cloud and adapting it efficiently to a new context using only local data and limited updates. The figure highlights the key idea: pretrained knowledge allows fast adaptation without relearning from scratch, even when the new task diverges in input modality or goal.</p>
<div id="fig-transfer-conceptual" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transfer-conceptual-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/ondevice_transfer_learning_apps.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Knowledge Transfer: Pretrained models accelerate learning on new tasks by leveraging existing representations, as seen by adapting skills between related board games or musical instruments. This transfer extends across domains like bicycle riding and scooter operation, where shared underlying structures allow efficient adaptation with limited new data."><img src="images/png/ondevice_transfer_learning_apps.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transfer-conceptual-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Knowledge Transfer: Pretrained models accelerate learning on new tasks by leveraging existing representations, as seen by adapting skills between related board games or musical instruments. This transfer extends across domains like bicycle riding and scooter operation, where shared underlying structures allow efficient adaptation with limited new data.
</figcaption>
</figure>
</div>
<p>This conceptual shift, enabled by transfer learning and adaptation, enables real-world on-device applications. Whether adapting a language model for personal typing preferences, adjusting gesture recognition to individual movement patterns, or recalibrating a sensor model in changing environments, on-device learning allows systems to remain responsive, efficient, and user-aligned over time.</p>
</section>
<section id="sec-ondevice-learning-realworld-application-domains-c4e4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-realworld-application-domains-c4e4">Real-World Application Domains</h3>
<p>Building on these established motivations (personalization, latency, privacy, and infrastructure efficiency), real-world deployments demonstrate the practical impact of on-device learning across diverse application domains. These domains span consumer technologies, healthcare, industrial systems, and embedded applications, each showcasing scenarios where the benefits outlined above become essential for effective machine learning deployment.</p>
<p>Mobile input prediction represents the most mature and widely deployed example of on-device learning. In systems such as smartphone keyboards, predictive text and autocorrect features benefit substantially from continuous local adaptation. User typing patterns are highly personalized and evolve dynamically, making centralized static models insufficient for optimal user experience. On-device learning allows language models to fine-tune their predictions directly on the device, achieving personalization while maintaining data locality.</p>
<p>For instance, Google‚Äôs Gboard employs federated learning to improve shared models across a large population of users while keeping raw data local to each device <span class="citation" data-cites="hard2018federated">(<a href="#ref-hard2018federated" role="doc-biblioref">Hard et al. 2018</a>)</span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hard2018federated" class="csl-entry" role="listitem">
Hard, Andrew, Kanishka Rao, Rajiv Mathews, Saurabh Ramaswamy, Fran√ßoise Beaufays, Sean Augenstein, Hubert Eichner, Chlo√© Kiddon, and Daniel Ramage. 2018. <span>‚ÄúFederated Learning for Mobile Keyboard Prediction.‚Äù</span> In <em>International Conference on Learning Representations (ICLR)</em>.
</div><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Gboard Federated Pioneer</strong>: Gboard became the first major commercial federated learning deployment in 2017, processing updates from over 1 billion devices. The technical challenge was immense: aggregating model updates while ensuring no individual user‚Äôs typing patterns could be inferred. Google‚Äôs success with Gboard proved federated learning could work at planetary scale, demonstrating 10-20% accuracy improvements over static models while maintaining strict differential privacy guarantees.</p></div></div><p>As shown in <a href="#fig-ondevice-gboard" class="quarto-xref">Figure&nbsp;2</a>, different prediction strategies illustrate how local adaptation operates in real time: next-word prediction (NWP) suggests likely continuations based on prior text, while Smart Compose uses on-the-fly rescoring to offer dynamic completions, demonstrating the sophistication of local inference mechanisms.</p>
<div id="fig-ondevice-gboard" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ondevice-gboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/ondevice_gboard_example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: On-Device Prediction Strategies: Gboard employs both next-word prediction and smart compose with on-the-fly rescoring to adapt to user typing patterns locally, enhancing personalization and preserving privacy. These techniques demonstrate how machine learning models can refine predictions in real time without transmitting data to a central server, enabling efficient and private mobile input experiences."><img src="images/png/ondevice_gboard_example.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ondevice-gboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: On-Device Prediction Strategies: Gboard employs both next-word prediction and smart compose with on-the-fly rescoring to adapt to user typing patterns locally, enhancing personalization and preserving privacy. These techniques demonstrate how machine learning models can refine predictions in real time without transmitting data to a central server, enabling efficient and private mobile input experiences.
</figcaption>
</figure>
</div>
<p>Building on the consumer applications, wearable and health monitoring devices present equally compelling use cases with additional regulatory constraints. These systems rely on real-time data from accelerometers, heart rate sensors, and electrodermal activity monitors to track user health and fitness. Physiological baselines vary dramatically between individuals, creating a personalization challenge that static models cannot address effectively. On-device learning allows models to adapt to these individual baselines over time, substantially improving the accuracy of activity recognition, stress detection, and sleep staging while meeting regulatory requirements for data localization.</p>
<p>Voice interaction technologies present another important application domain with unique acoustic challenges. Wake-word detection<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> and voice interfaces in devices such as smart speakers and earbuds must recognize voice commands quickly and accurately, even in noisy or dynamic acoustic environments.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Wake-Word Detection</strong>: Always-listening keyword spotting that activates voice assistants (‚ÄúHey Siri,‚Äù ‚ÄúOK Google,‚Äù ‚ÄúAlexa‚Äù). These systems run continuously at ~1&nbsp;mW power consumption, roughly 1000<span class="math inline">\(\times\)</span> less than full speech recognition. They use tiny neural networks (~100&nbsp;KB) with specialized architectures optimized for sub-100&nbsp;ms latency and minimal false positive rates (&lt;0.1 activations per hour). Modern systems achieve 95%+ accuracy while processing 16&nbsp;kHz audio in real-time, making on-device personalization critical for adapting to individual voice characteristics and reducing false activations.</p></div></div><p>These systems face strict latency requirements: voice interfaces must maintain end-to-end response times under 500&nbsp;ms to preserve natural conversation flow, with wake-word detection requiring sub-100&nbsp;ms response times to avoid user frustration. Local training allows models to adapt to the user‚Äôs unique voice profile and changing ambient context, reducing false positives and missed detections while meeting these demanding performance constraints. This adaptation proves particularly valuable in far-field audio settings, where microphone configurations and room acoustics vary dramatically across deployments.</p>
<p>Beyond consumer applications, industrial IoT and remote monitoring systems demonstrate the value of on-device learning in resource-constrained environments. In applications such as agricultural sensing, pipeline monitoring, or environmental surveillance, connectivity to centralized infrastructure may be limited, expensive, or entirely unavailable. On-device learning allows these systems to detect anomalies, adjust thresholds, or adapt to seasonal trends without continuous communication with the cloud. This capability proves necessary for maintaining autonomy and reliability in edge-deployed sensor networks, where system downtime or missed detections can have significant economic or safety consequences.</p>
<p>The most demanding applications emerge in embedded computer vision systems, including those in robotics, AR/VR, and smart cameras, which combine complex visual processing with extreme timing constraints. Camera applications must process frames within 33&nbsp;ms to maintain 30 FPS real-time performance, while AR/VR systems demand motion-to-photon latencies under 20&nbsp;ms to prevent nausea and maintain immersion. Safety-critical control systems require even tighter bounds, typically under 10&nbsp;ms, where delayed decisions can have severe consequences. These systems operate in novel or rapidly changing environments that differ substantially from their original training conditions. On-device adaptation allows models to recalibrate to new lighting conditions, object appearances, or motion patterns while meeting these critical latency budgets that fundamentally drive the architectural decision between on-device versus cloud-based processing.</p>
<p>Each domain reveals a common pattern: deployment environments introduce variation and context-specific requirements that cannot be anticipated during centralized training. These applications demonstrate how the motivational drivers (personalization, latency, privacy, and infrastructure efficiency) manifest as concrete engineering constraints. Mobile keyboards face memory limitations for storing user-specific patterns, wearable devices encounter energy budgets that restrict training frequency, voice interfaces must meet sub-100&nbsp;ms latency requirements that preclude cloud coordination, and industrial IoT systems operate in network-constrained environments that demand autonomous adaptation. This pattern illuminates the fundamental design requirement shaping all subsequent technical decisions: learning must be performed efficiently, privately, and reliably under significant resource constraints that we examine through constraint analysis (<a href="#sec-ondevice-learning-design-constraints-c776" class="quarto-xref">Section&nbsp;1.3</a>), adaptation techniques (<a href="#sec-ondevice-learning-model-adaptation-6a82" class="quarto-xref">Section&nbsp;1.4</a>), and federated coordination (<a href="#sec-ondevice-learning-federated-learning-6e7e" class="quarto-xref">Section&nbsp;1.6</a>).</p>
</section>
<section id="sec-ondevice-learning-architectural-tradeoffs-centralized-vs-decentralized-training-d420" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-architectural-tradeoffs-centralized-vs-decentralized-training-d420">Architectural Trade-offs: Centralized vs.&nbsp;Decentralized Training</h3>
<p>These applications demonstrate the practical value of on-device learning across diverse domains. Building on this foundation, we now examine how on-device learning differs from traditional ML architectures, revealing a complete reimagining of the training lifecycle that extends far beyond simple deployment choices.</p>
<p>Understanding the shift that on-device learning represents requires examining how traditional machine learning systems are structured and where their limitations become apparent. Most machine learning systems today follow a centralized learning paradigm that has served the field well but increasingly shows strain under modern deployment requirements. Models are trained in data centers using large-scale, curated datasets aggregated from many sources. Once trained, these models are deployed to client devices in a static form, where they perform inference without further modification. Updates to model parameters, either to incorporate new data or to improve generalization, are handled periodically through offline retraining, often using newly collected or labeled data sent back from the field.</p>
<p>This established centralized model offers numerous proven advantages: high-performance computing infrastructure, access to diverse data distributions, and robust debugging and validation pipelines. It also depends on several assumptions that may not hold in modern deployment scenarios: reliable data transfer, trust in data custodianship, and infrastructure capable of managing global updates across device fleets. As machine learning is deployed into increasingly diverse and distributed environments, the limitations of this approach become more apparent and often prohibitive.</p>
<p>In contrast to this centralized approach, on-device learning embraces an inherently decentralized paradigm that challenges many traditional assumptions. Each device maintains its own copy of a model and adapts it locally using data that is typically unavailable to centralized infrastructure. Training occurs on-device, often asynchronously and under varying resource conditions that change based on device usage patterns, battery levels, and thermal states. Data never leaves the device, reducing privacy exposure but also complicating coordination between devices. Devices may differ dramatically in their hardware capabilities, runtime environments, and patterns of use, making the learning process heterogeneous and difficult to standardize. These hardware variations create significant system design challenges.</p>
<p>This decentralized architecture introduces a new class of systems challenges that extend well beyond traditional machine learning concerns. Devices may operate with different versions of the model, leading to inconsistencies in behavior across the deployment fleet. Evaluation and validation become significantly more complex, as there is no central point from which to measure performance across all devices <span class="citation" data-cites="mcmahan2017communication">(<a href="#ref-mcmahan2017communication" role="doc-biblioref">McMahan et al. 2017</a>)</span>. Model updates must be carefully managed to prevent degradation, and safety guarantees become substantially harder to enforce in the absence of centralized testing and validation infrastructure.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bonawitz2019towards" class="csl-entry" role="listitem">
Bonawitz, Keith, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, et al. 2019. <span>‚ÄúTowards Federated Learning at Scale: System Design,‚Äù</span> February. <a href="http://arxiv.org/abs/1902.01046v2">http://arxiv.org/abs/1902.01046v2</a>.
</div></div><p>Managing thousands of heterogeneous edge devices exceeds typical distributed systems complexity. Device heterogeneity extends beyond hardware differences to include varying operating system versions, security patches, network configurations, and power management policies. At any given time, 20-40% of devices are offline <span class="citation" data-cites="bonawitz2019towards">(<a href="#ref-bonawitz2019towards" role="doc-biblioref">Bonawitz et al. 2019</a>)</span>, while others have been disconnected for weeks or months, creating persistent coordination challenges.</p>
<p>When disconnected devices reconnect, they require state reconciliation to avoid version conflicts. Update verification becomes critical as devices can silently fail to apply updates or report success while running outdated models. Robust systems implement multi-stage verification: cryptographic signatures confirm update integrity, functional tests validate model behavior, and telemetry confirms deployment success. Rollback strategies must handle partial deployments where some devices received updates while others remain on previous versions, requiring sophisticated orchestration to maintain system consistency during failure recovery.</p>
<p>These challenges require different approaches to system design and operational management compared to centralized ML systems, building on the distributed systems principles from <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong> while introducing edge-specific complexities.</p>
<p>Despite these challenges, decentralization introduces opportunities that often justify the additional complexity. It allows for deep personalization without centralized oversight, supports robust learning in disconnected or bandwidth-limited environments, and reduces the operational cost and infrastructure burden for model updates. Realizing these benefits raises questions of how to effectively coordinate learning across devices, whether through periodic synchronization, federated aggregation, or hybrid approaches that balance local and global objectives.</p>
<p>The move from centralized to decentralized learning represents more than a shift in deployment architecture. It reshapes the entire design space for machine learning systems, requiring new approaches to model architecture, training algorithms, data management, and system validation. In centralized training, data is aggregated from many sources and processed in large-scale data centers, where models are trained, validated, and then deployed in a static form to edge devices. In contrast, on-device learning introduces a fundamentally different paradigm: models are updated directly on client devices using local data, often asynchronously and under diverse hardware conditions. This architectural transformation introduces coordination challenges while enabling autonomous local adaptation, requiring careful consideration of validation, system reliability, and update orchestration across heterogeneous device populations.</p>
<p>On-device learning responds to the limitations of centralized machine learning workflows. The transformation from centralized to decentralized learning creates three distinct operational phases, each with different characteristics and challenges.</p>
<p>The traditional centralized paradigm begins with cloud-based training on aggregated data, followed by static model deployment to client devices. This approach works well when data collection is feasible, network connectivity is reliable, and a single global model can serve all users effectively. However, it breaks down when data becomes personalized, privacy-sensitive, or collected in environments with limited connectivity.</p>
<p>Once deployed, local differences begin to emerge as each device encounters its own unique data distribution. Devices collect data that reflects individual user patterns, environmental conditions, and usage contexts. This data is often non-IID (non-independent and identically distributed)<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> and noisy, requiring local model adaptation to maintain performance. This transition marks the shift from global generalization to local specialization.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Non-IID (Non-Independent and Identically Distributed)</strong>: In machine learning, data is IID when samples are drawn independently from the same distribution. Non-IID violates this assumption, common in federated learning where each device collects data from different users, environments, or use cases. For example, smartphone keyboard data varies dramatically between users (languages, writing styles, autocorrect needs), making personalized model training essential but challenging for convergence.</p></div></div><p>The final phase introduces federated coordination, where devices periodically synchronize their local adaptations through aggregated model updates rather than raw data sharing. This enables privacy-preserving global refinement while maintaining the benefits of local personalization.</p>
<p>These three distinct phases (centralized training, local adaptation, and federated coordination) represent an architectural evolution that reshapes every aspect of the machine learning lifecycle. <a href="#fig-centralized-vs-decentralized" class="quarto-xref">Figure&nbsp;3</a> illustrates how data flow, computational distribution, and coordination mechanisms differ across these phases, highlighting the increasing complexity but also the enhanced capabilities that emerge at each stage. Understanding this progression helps frame the challenges that on-device learning systems must address.</p>
<div id="fig-centralized-vs-decentralized" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-centralized-vs-decentralized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="cee3e9981ee09d7c65e151839ec0b5d45964502d.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: The evolution from centralized cloud training (region A) through local device adaptation (region B) to federated coordination (region C) represents a fundamental shift in machine learning architecture. Each phase introduces distinct operational characteristics, from uniform global models to personalized local adaptations to coordinated distributed learning."><img src="ondevice_learning_files/mediabag/cee3e9981ee09d7c65e151839ec0b5d45964502d.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-centralized-vs-decentralized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The evolution from centralized cloud training (region A) through local device adaptation (region B) to federated coordination (region C) represents a fundamental shift in machine learning architecture. Each phase introduces distinct operational characteristics, from uniform global models to personalized local adaptations to coordinated distributed learning.
</figcaption>
</figure>
</div>
<div id="quiz-question-sec-ondevice-learning-motivations-benefits-37c3" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary benefit of on-device learning compared to centralized learning?</p>
<ol type="a">
<li>Increased computational power</li>
<li>Simplified model management</li>
<li>Enhanced personalization and privacy</li>
<li>Lower development costs</li>
</ol></li>
<li><p>Describe a scenario where on-device learning is more advantageous than centralized learning, considering privacy and latency.</p></li>
<li><p>True or False: On-device learning eliminates the need for centralized model updates.</p></li>
<li><p>On-device learning allows for model adaptation using ____ data, enhancing personalization and privacy.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-motivations-benefits-37c3" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ondevice-learning-design-constraints-c776" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-design-constraints-c776">Design Constraints</h2>
<p>Part III established efficiency principles that shape all machine learning systems. <strong><a href="../efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 9: Efficient AI</a></strong> introduced three efficiency dimensions (algorithmic, compute, and data efficiency) and revealed through scaling laws why brute-force approaches hit fundamental limits. <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong> developed compression techniques including quantization, pruning, and knowledge distillation that enable deployment on resource-constrained devices. <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong> characterized edge hardware capabilities from microcontrollers to mobile accelerators, as detailed in the hardware discussions. These chapters focused primarily on inference workloads: running pre-trained models efficiently.</p>
<p>On-device learning operates under these same efficiency constraints but with training-specific amplifications that make optimization dramatically more challenging. Where inference requires a single forward pass through the network, training demands forward propagation, gradient computation through backpropagation, and weight updates, increasing memory requirements by 3-5<span class="math inline">\(\times\)</span> and computational costs by 2-3<span class="math inline">\(\times\)</span>. The model compression techniques that enable efficient inference become baseline requirements rather than optimizations, as training within edge device constraints would be impossible without aggressive compression.</p>
<p>Given the established motivations for on-device learning, we now examine the fundamental engineering challenges that shape its implementation. Enabling learning on the device requires completely rethinking conventional assumptions about where and how machine learning systems operate. In centralized environments, models are trained with access to extensive compute infrastructure, large and curated datasets, and generous memory and energy budgets. At the edge, none of these assumptions hold, creating a fundamentally different design space.</p>
<p>On-device learning constraints fall into three critical dimensions that parallel but extend the efficiency framework from Part III: model compression requirements (extending algorithmic efficiency), sparse and non-uniform data characteristics (extending data efficiency), and severely limited computational resources (extending compute efficiency). These three dimensions form an interconnected constraint space that defines the feasible region for on-device learning systems, with each dimension imposing distinct limitations that influence algorithmic choices, system architecture, and deployment strategies.</p>
<section id="sec-ondevice-learning-quantifying-training-overhead-edge-devices-3e4c" class="level3">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-quantifying-training-overhead-edge-devices-3e4c">Quantifying Training Overhead on Edge Devices</h3>
<p>The transition from inference-only deployment to on-device training creates multiplicative rather than additive complexity. These constraints interact and amplify each other in ways that reshape system design requirements, building on the resource optimization principles from <strong><a href="../efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 9: Efficient AI</a></strong> while introducing new challenges specific to distributed learning environments.</p>
<p>The efficiency constraints introduced in Part III apply to both inference and training, but training amplifies each constraint dimension by 3-10<span class="math inline">\(\times\)</span>. <a href="#tbl-training-amplification" class="quarto-xref">Table&nbsp;1</a> quantifies how training workloads intensify the challenges established in <strong><a href="../efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 9: Efficient AI</a></strong>, <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong>, and <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong>.</p>
<p>These amplifications explain why simply applying Part III optimization techniques to training workloads proves insufficient. Each constraint category shapes on-device learning system design, requiring approaches that build on but extend beyond the inference-focused methods from earlier chapters.</p>
<div id="tbl-training-amplification" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-training-amplification-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Training Amplifies Inference Constraints</strong>: On-device learning operates under the same efficiency constraints as inference (Part III) but with training-specific amplifications that make optimization dramatically more challenging. This table quantifies how each constraint dimension intensifies when transitioning from running pre-trained models to adapting them locally. Amplification factors assume standard backpropagation without optimizations like gradient checkpointing.
</figcaption>
<div aria-describedby="tbl-training-amplification-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 25%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Constraint Dimension</strong></th>
<th style="text-align: left;"><strong>Inference (Part III)</strong></th>
<th style="text-align: left;"><strong>Training Amplification</strong></th>
<th style="text-align: left;"><strong>Impact on Design</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Memory Footprint</strong></td>
<td style="text-align: left;">Model weights + single activation map</td>
<td style="text-align: left;">Weights + full activation cache + gradients + optimizer state</td>
<td style="text-align: left;">3-5<span class="math inline">\(\times\)</span> increase; forces aggressive compression</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Compute Operations</strong></td>
<td style="text-align: left;">Forward pass only</td>
<td style="text-align: left;">Forward + backward + weight update</td>
<td style="text-align: left;">2-3<span class="math inline">\(\times\)</span> increase; limits model complexity</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Memory Bandwidth</strong></td>
<td style="text-align: left;">Sequential weight reads</td>
<td style="text-align: left;">Bidirectional data flow for gradients</td>
<td style="text-align: left;">5-10<span class="math inline">\(\times\)</span> increase; creates bottlenecks</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Energy per Sample</strong></td>
<td style="text-align: left;">Single inference operation</td>
<td style="text-align: left;">Multiple gradient steps with convergence</td>
<td style="text-align: left;">10-50<span class="math inline">\(\times\)</span> increase; requires opportunistic scheduling</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Data Requirements</strong></td>
<td style="text-align: left;">Pre-collected, curated datasets</td>
<td style="text-align: left;">Sparse, noisy, streaming local data</td>
<td style="text-align: left;">Necessitates sample-efficient methods</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Hardware Utilization</strong></td>
<td style="text-align: left;">Optimized for forward passes</td>
<td style="text-align: left;">Different access patterns for backprop</td>
<td style="text-align: left;">Inference accelerators may not help training</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#fig-ondevice-pretraining" class="quarto-xref">Figure&nbsp;4</a> illustrates a pipeline that combines offline pre-training with online adaptive learning on resource-constrained IoT devices. The system first undergoes meta-training with generic data. During deployment, device-specific constraints such as data availability, compute, and memory shape the adaptation strategy by ranking and selecting layers and channels to update. This allows efficient on-device learning within limited resource envelopes.</p>
<div id="fig-ondevice-pretraining" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ondevice-pretraining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c42ceee9cd4bc594efd92af96f7dfce696ac282b.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Resource-constrained devices use a two-stage learning process: offline pre-training establishes initial model weights, followed by online adaptation that selectively updates layers based on available data, compute, and memory. This approach balances model performance with the practical limitations of edge deployment, enabling continuous learning in real-world environments."><img src="ondevice_learning_files/mediabag/c42ceee9cd4bc594efd92af96f7dfce696ac282b.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ondevice-pretraining-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Resource-constrained devices use a two-stage learning process: offline pre-training establishes initial model weights, followed by online adaptation that selectively updates layers based on available data, compute, and memory. This approach balances model performance with the practical limitations of edge deployment, enabling continuous learning in real-world environments.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ondevice-learning-model-constraints-9232" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-model-constraints-9232">Model Constraints</h3>
<p>The first dimension of on-device learning constraints centers on the model itself. Its structure, size, and computational requirements determine deployment feasibility. The structure and size of the machine learning model directly influence whether on-device training is even possible, let alone practical. Unlike cloud-deployed models that can span billions of parameters and rely on multi-gigabyte memory budgets, models intended for on-device learning must conform to tight constraints on memory, storage, and computational complexity. These constraints apply not only at inference time, but become even more restrictive during training, where additional resources are needed for gradient computation, parameter updates, and optimizer state management.</p>
<p>The scale of these constraints becomes apparent when examining specific examples across the device spectrum. The MobileNetV2 architecture, commonly used in mobile vision tasks, requires approximately 14 MB of storage in its standard configuration. While this memory requirement is entirely feasible for modern smartphones with gigabytes of available RAM, it far exceeds the memory available on embedded microcontrollers such as the Arduino Nano 33 BLE Sense<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>, which provides only 256 KB of SRAM and 1 MB of flash storage. This dramatic difference in available resources necessitates aggressive model compression techniques. In such severely constrained platforms, even a single layer of a typical convolutional neural network may exceed available RAM during training due to the need to store intermediate feature maps and gradient information.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Arduino Edge Computing Reality</strong>: The Arduino Nano 33 BLE Sense represents typical microcontroller constraints: 256&nbsp;KB SRAM is roughly 65,000 times smaller than a modern smartphone‚Äôs 16&nbsp;GB RAM (flagship devices). To put this in perspective, storing just one 224√ó224√ó3 RGB image (150&nbsp;KB) would consume 60% of available memory. Training requires 3-5<span class="math inline">\(\times\)</span> more memory for gradients and activations, making even tiny models challenging. The 1&nbsp;MB flash storage can hold only the smallest quantized models, forcing designers to use 8-bit or even 4-bit representations.</p></div></div><p>Beyond static storage requirements, the training process itself dramatically expands the effective memory footprint, creating an additional layer of constraint. Standard backpropagation requires caching activations for each layer during the forward pass, which are then reused during gradient computation in the backward pass. As established in the amplification analysis above, this activation caching multiplies memory requirements compared to inference-only deployment. For a seemingly modest 10-layer convolutional model processing <span class="math inline">\(64 \times 64\)</span> images, the required memory may exceed 1 to 2 MB, well beyond the SRAM capacity of most embedded systems and highlighting the fundamental tension between model expressiveness and resource availability.</p>
<p>Compounding these memory constraints, model complexity directly affects runtime energy consumption and thermal limits, introducing additional practical barriers to deployment. In systems such as smartwatches or battery-powered wearables, sustained model training can rapidly deplete energy reserves or trigger thermal throttling that degrades performance. Training a full model using floating-point operations on these devices is often infeasible from an energy perspective, even when memory constraints are satisfied. These practical limitations have motivated the development of ultra-lightweight model variants, such as MLPerf Tiny benchmark networks <span class="citation" data-cites="banbury2021mlperf">(<a href="#ref-banbury2021mlperf" role="doc-biblioref">Banbury et al. 2021</a>)</span>, which fit within 100‚Äì200 KB and can be adapted using only partial gradient updates. These specialized models employ aggressive quantization and pruning strategies to achieve such compact representations while maintaining sufficient expressiveness for meaningful adaptation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-banbury2021mlperf" class="csl-entry" role="listitem">
Banbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat Jeffries, Csaba Kiraly, Pietro Montino, et al. 2021. <span>‚ÄúMLPerf Tiny Benchmark.‚Äù</span> <em>arXiv Preprint arXiv:2106.07597</em>, June. <a href="http://arxiv.org/abs/2106.07597v4">http://arxiv.org/abs/2106.07597v4</a>.
</div></div><p>The practical implications of battery and thermal constraints extend beyond just limiting training duration. Mobile devices must carefully balance training opportunities with user experience. Aggressive on-device training can cause noticeable device heating and rapid battery drain, leading to user dissatisfaction and potential app uninstalls. Modern smartphones typically limit sustained processing to 2-3&nbsp;W for ML workloads to prevent thermal discomfort, though they can burst to 5-10&nbsp;W for brief periods before thermal throttling kicks in. Training even modest models can easily exceed these sustainable power limits. This reality necessitates intelligent scheduling strategies: training during charging periods when thermal dissipation is improved, utilizing low-power cores for gradient computation when possible, and implementing thermal-aware duty cycling that pauses training when temperature thresholds are exceeded. Some systems even leverage device usage patterns, scheduling intensive adaptation only during overnight charging when the device is idle and connected to power.</p>
<p>Given these multifaceted constraints, the model architecture itself must be fundamentally designed with on-device learning capabilities in mind from the outset. Many conventional architectures, such as large transformers or deep convolutional networks, are simply not viable for on-device adaptation due to their inherent size and computational complexity. Instead, specialized lightweight architectures such as MobileNets<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, SqueezeNet <span class="citation" data-cites="iandola2016squeezenet">(<a href="#ref-iandola2016squeezenet" role="doc-biblioref">Iandola et al. 2016</a>)</span>, and EfficientNet <span class="citation" data-cites="tan2019efficientnet">(<a href="#ref-tan2019efficientnet" role="doc-biblioref">Tan and Le 2019</a>)</span> have been developed specifically for resource-constrained environments. These architectures leverage efficiency principles and architectural optimizations, rethinking how neural networks can be structured. These specialized models employ techniques such as depthwise separable convolutions<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, bottleneck layers, and aggressive quantization to dramatically reduce memory and compute requirements while maintaining sufficient performance for practical applications.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;<strong>MobileNet Innovation</strong>: Google‚Äôs MobileNet family revolutionized mobile AI by achieving 10-20<span class="math inline">\(\times\)</span> parameter reduction compared to traditional CNNs. MobileNetV1 (2017) used depthwise separable convolutions to reduce floating-point operations (FLOPs) by 8-9<span class="math inline">\(\times\)</span>, while MobileNetV2 (2018) added inverted residuals and linear bottlenecks. The breakthrough allowed real-time inference on smartphones: MobileNetV2 runs ImageNet classification in ~75&nbsp;ms on a Pixel phone versus 1.8 seconds for ResNet-50 <span class="citation" data-cites="he2016deep">(<a href="#ref-he2016deep" role="doc-biblioref">He et al. 2016</a>)</span>.</p><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>‚ÄúDeep Residual Learning for Image Recognition.‚Äù</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770‚Äì78. IEEE. <a href="https://doi.org/10.1109/cvpr.2016.90">https://doi.org/10.1109/cvpr.2016.90</a>.
</div></div><div id="ref-iandola2016squeezenet" class="csl-entry" role="listitem">
Iandola, Forrest N., Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. 2016. <span>‚ÄúSqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and &lt;0.5MB Model Size,‚Äù</span> February. <a href="http://arxiv.org/abs/1602.07360v4">http://arxiv.org/abs/1602.07360v4</a>.
</div><div id="ref-tan2019efficientnet" class="csl-entry" role="listitem">
Tan, Mingxing, and Quoc V. Le. 2019. <span>‚ÄúEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.‚Äù</span> In <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 6105‚Äì14.
</div><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Depthwise Separable Convolutions</strong>: This technique decomposes standard convolution into two operations: depthwise convolution (applies single filter per input channel) and pointwise convolution (1√ó1 conv to combine channels). For a 3√ó3 conv with 512 input/output channels, standard convolution requires 2.4&nbsp;M parameters while depthwise separable needs only 13.8&nbsp;K, a 174<span class="math inline">\(\times\)</span> reduction. The computational savings are similarly dramatic, making real-time inference possible on mobile CPUs.</p></div><div id="ref-howard2017mobilenets" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. <span>‚ÄúMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.‚Äù</span> <em>CoRR</em> abs/1704.04861 (April). <a href="http://arxiv.org/abs/1704.04861v1">http://arxiv.org/abs/1704.04861v1</a>.
</div></div><p>These architectures are often designed to be modular, allowing for easy adaptation and fine-tuning. For example, MobileNets <span class="citation" data-cites="howard2017mobilenets">(<a href="#ref-howard2017mobilenets" role="doc-biblioref">Howard et al. 2017</a>)</span> can be configured with different width multipliers and resolution settings to balance performance and resource usage. Concretely, MobileNetV2 with Œ±=1.0 requires 3.4&nbsp;M parameters (13.6&nbsp;MB in FP32), but with Œ±=0.5 this drops to 0.7&nbsp;M parameters (2.8&nbsp;MB), enabling deployment on devices with just 4&nbsp;MB available RAM. This flexibility is important for on-device learning, where the model must adapt to the specific constraints of the deployment environment.</p>
<p>While model architecture determines the memory and computational baseline for on-device learning, the characteristics of available training data introduce equally fundamental limitations that shape every aspect of the learning process.</p>
</section>
<section id="sec-ondevice-learning-data-constraints-303e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-data-constraints-303e">Data Constraints</h3>
<p>The second dimension of on-device learning constraints centers on data availability and quality. The nature of data available to on-device ML systems differs dramatically from the large, curated, and centrally managed datasets used in cloud-based training. At the edge, data is locally collected, temporally sparse, and often unstructured or unlabeled, creating a different learning environment. These characteristics introduce multifaceted challenges in volume, quality, and statistical distribution, all of which directly affect the reliability and generalizability of learning on the device.</p>
<p>Data volume represents the first major constraint, severely limited by both storage constraints and the sporadic nature of user interaction. For example, a smart fitness tracker may collect motion data only during physical activity, generating relatively few labeled samples per day. If a user wears the device for just 30 minutes of exercise, only a few hundred data points might be available for training, compared to the thousands or millions typically required for effective supervised learning in controlled environments. This scarcity changes the learning paradigm from data-rich to data-efficient algorithms.</p>
<p>Beyond volume limitations, on-device data is frequently non-IID (non-independent and identically distributed) <span class="citation" data-cites="zhao2018federated">(<a href="#ref-zhao2018federated" role="doc-biblioref">Zhao et al. 2018</a>)</span>, creating statistical challenges that cloud-based systems rarely encounter. This heterogeneity manifests across multiple dimensions: user behavior patterns, environmental conditions, linguistic preferences, and usage contexts. A voice assistant deployed across households encounters dramatic variation in accents, languages, speaking styles, and command patterns. Similarly, smartphone keyboards adapt to individual typing patterns, autocorrect preferences, and multilingual usage that varies dramatically between users. This data heterogeneity complicates both model convergence and the design of update mechanisms that must generalize across devices while maintaining personalization.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhao2018federated" class="csl-entry" role="listitem">
Zhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. 2018. <span>‚ÄúFederated Learning with Non-IID Data.‚Äù</span> <em>CoRR</em> abs/1806.00582 (June). <a href="http://arxiv.org/abs/1806.00582v2">http://arxiv.org/abs/1806.00582v2</a>.
</div></div><p>Compounding these distribution challenges, label scarcity presents an additional critical obstacle that severely limits traditional learning approaches. Most edge-collected data is unlabeled by default, requiring systems to learn from weak or implicit supervision signals. In a smartphone camera, for instance, the device may capture thousands of images throughout the day, but only a few are associated with meaningful user actions (e.g., tagging, favoriting, or sharing), which could serve as implicit labels. In many applications, including detecting anomalies in sensor data and adapting gesture recognition models, explicit labels may be entirely unavailable, making traditional supervised learning infeasible without developing alternative methods for weak supervision or unsupervised adaptation.</p>
<p>Data quality issues add another layer of complexity to the on-device learning challenge. Noise and variability further degrade the already limited data available for training. Embedded systems such as environmental sensors or automotive ECUs may experience fluctuations in sensor calibration, environmental interference, or mechanical wear, leading to corrupted or drifting input signals over time. Without centralized validation systems to detect and filter these errors, they may silently degrade learning performance, creating a reliability challenge that cloud-based systems can more easily address through data preprocessing pipelines.</p>
<p>Finally, data privacy and security concerns impose the most restrictive constraints of all, often making data sharing architecturally impossible rather than merely undesirable. Sensitive information, such as health data, personal communications, or user behavioral patterns, must be protected from unauthorized access under legal and ethical requirements. This constraint often completely precludes the use of traditional data-sharing methods, such as uploading raw data to a central server for training. Instead, on-device learning must rely on sophisticated techniques that enable local adaptation without ever exposing sensitive information, changing how learning systems can be designed and validated.</p>
</section>
<section id="sec-ondevice-learning-compute-constraints-4d6d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-compute-constraints-4d6d">Compute Constraints</h3>
<p><strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong> characterized the edge hardware landscape that provides computational substrate for machine learning: microcontrollers like STM32F4 and ESP32 at the most constrained end, mobile-class processors with dedicated AI accelerators (Apple Neural Engine, Qualcomm Hexagon, Google Tensor) in the middle, and high-capability edge devices at the upper end. That chapter focused on inference capabilities‚Äîthe computational throughput, memory bandwidth, and energy efficiency achievable when executing pre-trained models.</p>
<p>Training workloads exhibit fundamentally different computational characteristics that reshape hardware utilization patterns. Building on the edge hardware landscape characterized in <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong>, from microcontrollers to mobile AI accelerators, on-device learning must operate within severely constrained computational envelopes that differ dramatically from cloud-based training infrastructure by factors of hundreds or thousands in raw computational capacity.</p>
<p>The key difference: backpropagation requires significantly higher memory bandwidth than inference due to gradient computation and activation caching, weight updates create write-heavy access patterns unlike inference‚Äôs read-only operations, and optimizer state management demands additional memory allocation that inference never encounters. These training-specific demands mean hardware perfectly adequate for inference may prove entirely inadequate for adaptation, even when updating only a small parameter subset.</p>
<p>At the most constrained end of the spectrum, devices such as the STM32F4<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> or ESP32<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> microcontrollers offer only a few hundred kilobytes of SRAM and completely lack hardware support for floating-point operations <span class="citation" data-cites="lai2020tinyml">(<a href="#ref-lai2020tinyml" role="doc-biblioref">Warden and Situnayake 2020</a>)</span>. These extreme constraints represent the fundamental limitations of edge hardware (<strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong>). Such severe limitations preclude the use of conventional deep learning libraries and require models to be meticulously designed for integer arithmetic and minimal runtime memory allocation. In these environments, even apparently simple models require highly specialized techniques, including quantization-aware training<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> and selective parameter updates, to execute training loops without exceeding memory or power budgets.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<strong>STM32F4 Microcontroller Reality</strong>: The STM32F4 represents the harsh reality of embedded computing: 192&nbsp;KB SRAM (roughly the size of a small JPEG image) and 1&nbsp;MB flash storage, running at 168&nbsp;MHz without floating-point hardware acceleration. Integer arithmetic is 10-100<span class="math inline">\(\times\)</span> slower than dedicated floating-point units found in mobile chips. Power consumption is ~100&nbsp;mW during active processing, requiring careful duty-cycling to preserve battery life. These constraints make even simple neural networks challenging: a 10-neuron hidden layer requires ~40&nbsp;KB for weights alone in FP32.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;<strong>ESP32 Edge Computing</strong>: The ESP32 provides 520&nbsp;KB SRAM and dual-core processing at 240&nbsp;MHz, making it more capable than STM32F4 but still severely constrained. Its key advantage is built-in WiFi and Bluetooth for federated learning scenarios. However, the lack of hardware floating-point support means all ML operations must use integer quantization. Real-world deployments show 8-bit quantized models can achieve 95% of FP32 accuracy while fitting in ~50&nbsp;KB memory, enabling basic on-device training for simple tasks like sensor anomaly detection.</p></div><div id="ref-lai2020tinyml" class="csl-entry" role="listitem">
Warden, Pete, and Daniel Situnayake. 2020. <em>TinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers</em>. O‚ÄôReilly Media.
</div><div id="fn12"><p><sup>12</sup>&nbsp;<strong>Quantization-Aware Training</strong>: Unlike post-training quantization which converts trained FP32 models to INT8, quantization-aware training simulates low-precision arithmetic during training itself. This allows the model to learn robust representations despite reduced precision. Critical for edge devices where INT8 operations consume <span class="math inline">\(4\times\)</span> less power and enable <span class="math inline">\(4\times\)</span> faster inference compared to FP32, while maintaining 95-99% of original accuracy.</p></div><div id="fn13"><p><sup>13</sup>&nbsp;<strong>Stochastic Gradient Descent (SGD)</strong>: The fundamental optimization algorithm for neural networks, updating parameters using gradients computed on small batches (or single samples). Unlike full-batch gradient descent, SGD‚Äôs randomness helps escape local minima while requiring minimal memory, storing only current parameters and gradients. This simplicity makes SGD ideal for microcontrollers where advanced optimizers like Adam would exceed memory budgets.</p></div></div><p>The practical implications are stark: while the STM32F4 microcontroller can run a simple linear regression model with a few hundred parameters, training even a small convolutional neural network would immediately exceed its memory capacity. In these severely constrained environments, training is often limited to simple algorithms such as stochastic gradient descent (SGD)<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> or <span class="math inline">\(k\)</span>-means clustering, which can be implemented using integer arithmetic and minimal memory overhead, representing a fundamental departure from modern machine learning practice.</p>
<p>Moving up the computational hierarchy, mobile-class hardware represents a significant improvement but still operates under substantial constraints. Platforms including the Qualcomm Snapdragon, Apple Neural Engine<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>, and Google Tensor SoC<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> provide significantly more compute power than microcontrollers, often featuring dedicated AI accelerators and optimized support for 8-bit or mixed-precision<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> matrix operations. These accelerators, their capabilities, and their programming models are detailed in <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong>. While these platforms can support more sophisticated training routines, including full backpropagation over compact models, they still fall dramatically short of the computational throughput and memory bandwidth available in centralized data centers. For instance, training a lightweight transformer<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> on a smartphone is technically feasible but must be tightly bounded in both time and energy consumption to avoid degrading the user experience, highlighting the persistent tension between learning capabilities and practical deployment constraints.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Apple Neural Engine Evolution</strong>: Apple‚Äôs Neural Engine has evolved dramatically since the A11 Bionic. The A17 Pro (2023) features a 16-core Neural Engine delivering 35 TOPS, roughly equivalent to an NVIDIA GTX 1080 Ti. This represents a 58<span class="math inline">\(\times\)</span> improvement over the original A11. The Neural Engine specializes in matrix operations with dedicated 8-bit and 16-bit arithmetic units, enabling efficient on-device training. Real-world performance: fine-tuning a MobileNet classifier takes ~2 seconds versus 45 seconds on CPU alone, while consuming only ~500&nbsp;mW additional power.</p></div><div id="fn15"><p><sup>15</sup>&nbsp;<strong>Google Tensor SoC Architecture</strong>: Google‚Äôs Tensor chips (starting with Pixel 6 in 2021) feature a custom TPU v1-derived Edge TPU optimized for ML workloads. Unlike Apple‚Äôs Neural Engine, Tensor optimizes for Google‚Äôs specific models (speech recognition, computational photography). The TPU provides efficient 8-bit integer operations while consuming only 2&nbsp;W, making it highly efficient for federated learning scenarios where devices train locally on speech or image data.</p></div><div id="fn16"><p><sup>16</sup>&nbsp;<strong>Mixed-Precision Training</strong>: Uses different numerical precisions for different operations, typically FP16 for forward/backward passes and FP32 for parameter updates. This halves memory usage and doubles throughput on modern hardware with Tensor Cores, while maintaining training stability through automatic loss scaling. Mobile implementations often use INT8 for inference and FP16 for gradient computation, balancing accuracy with hardware constraints.</p></div><div id="fn17"><p><sup>17</sup>&nbsp;<strong>Lightweight Transformers</strong>: Mobile-optimized transformer architectures like MobileBERT <span class="citation" data-cites="sun2020mobilebert">(<a href="#ref-sun2020mobilebert" role="doc-biblioref">Sun et al. 2020</a>)</span> and DistilBERT <span class="citation" data-cites="sanh2019distilbert">(<a href="#ref-sanh2019distilbert" role="doc-biblioref">Sanh et al. 2019</a>)</span> achieve 4-6<span class="math inline">\(\times\)</span> speedup over full models through techniques like knowledge distillation, layer reduction, and attention head pruning. MobileBERT retains 97% of BERT-base accuracy while running inference in ~40&nbsp;ms on mobile CPUs versus 160&nbsp;ms for full BERT. Key optimizations include bottleneck attention mechanisms and specialized mobile-friendly layer configurations.</p><div id="ref-sun2020mobilebert" class="csl-entry" role="listitem">
Sun, Zhiqing, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. <span>‚ÄúMobileBERT: A Compact Task-Agnostic BERT for Resource-Limited Devices.‚Äù</span> In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, 2158‚Äì70. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.195">https://doi.org/10.18653/v1/2020.acl-main.195</a>.
</div><div id="ref-sanh2019distilbert" class="csl-entry" role="listitem">
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. <span>‚ÄúDistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.‚Äù</span> <em>arXiv Preprint arXiv:1910.01108</em>, October. <a href="http://arxiv.org/abs/1910.01108v4">http://arxiv.org/abs/1910.01108v4</a>.
</div></div></div><p>These computational limitations become especially acute in real-time or battery-operated systems, as demonstrated in camera processing requirements, where specific latency budgets create hard architectural constraints. Camera applications processing at 30 FPS cannot exceed 33&nbsp;ms per frame, voice interfaces require rapid response times for natural interaction, AR/VR systems demand sub-20&nbsp;ms motion-to-photon latency to prevent user discomfort, and safety-critical control systems must respond within 10&nbsp;ms to ensure operational safety. These quantitative constraints determine whether on-device learning is feasible or whether cloud-based alternatives become architecturally necessary. In a smartphone-based speech recognizer, on-device adaptation must seamlessly coexist with primary inference workloads without interfering with response latency or system responsiveness. Similarly, in wearable medical monitors, training must occur opportunistically during carefully managed windows‚Äîtypically during periods of low activity or charging‚Äîto preserve battery life and avoid thermal management issues.</p>
<p>Beyond raw computational capacity, the architectural implications of these hardware constraints extend into fundamental system design choices. Training operations exhibit fundamentally different memory access patterns than inference workloads: backpropagation requires 3-5<span class="math inline">\(\times\)</span> higher memory bandwidth due to gradient computation and activation caching, creating bottlenecks that pure computational metrics don‚Äôt capture. Modern edge accelerators attempt to address these challenges through increasingly specialized hardware features. Adaptive precision datapaths allow dynamic switching between INT4 for forward passes and FP16 for gradient computation, optimizing both accuracy and efficiency within power budgets. Sparse computation units accelerate selective parameter updates by skipping zero gradients‚Äîa capability critical for efficient bias-only and LoRA adaptations. Near-memory compute architectures<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> reduce data movement costs by performing gradient updates directly adjacent to weight storage, addressing the memory bandwidth bottleneck. However, most current edge accelerators remain fundamentally optimized for inference workloads, creating significant hardware-software co-design opportunities for future generations of on-device training accelerators specifically designed to handle the unique demands of local adaptation.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Near-Memory Computing</strong>: Places processing units directly adjacent to or within memory arrays, dramatically reducing data movement costs. Traditional von Neumann architectures spend 100-1000<span class="math inline">\(\times\)</span> more energy moving data than computing on it. Near-memory designs can perform matrix operations with 10-100<span class="math inline">\(\times\)</span> better energy efficiency by eliminating costly memory bus transfers. Critical for edge training where gradient computations require intensive memory access patterns that overwhelm traditional cache hierarchies.</p></div></div></section>
<section id="sec-ondevice-learning-edge-hardware-integration-challenges-a240" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-edge-hardware-integration-challenges-a240">Edge Hardware Integration Challenges</h3>
<p>Beyond the individual constraints of models, data, and computation, on-device learning systems must navigate the complex interactions between these elements and the underlying physics of mobile computing: power dissipation, thermal limits, and energy budgets. These physical constraints are not mere engineering details; they are fundamental design drivers that determine the entire feasible space of on-device learning algorithms. Understanding these quantitative constraints enables informed design decisions that balance learning capabilities with long-term system sustainability and user acceptance.</p>
<section id="sec-ondevice-learning-energy-thermal-constraint-analysis-3133" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-energy-thermal-constraint-analysis-3133">Energy and Thermal Constraint Analysis</h4>
<p>Energy and thermal management represent perhaps the most challenging aspect of on-device learning system design, as they directly impact user experience and device longevity. Mobile devices operate under strict power budgets that fundamentally determine feasible model complexity and training schedules. The thermal design power (TDP) of mobile processors creates hard constraints that shape every aspect of on-device learning strategies. Modern smartphones typically maintain sustained processing at 2-3&nbsp;W for ML workloads to prevent thermal discomfort, but can burst to 5-10&nbsp;W for brief periods before thermal throttling dramatically reduces performance by 50% or more. This thermal cycling behavior forces training algorithms to operate in carefully managed burst modes, utilizing peak performance for only 10-30 seconds before backing off to sustainable power levels, a constraint that fundamentally changes how training algorithms must be designed.</p>
<p>The mobile power budget hierarchy reveals the tight constraints under which on-device learning must operate. Smartphone sustained processing is limited to 2-3&nbsp;W to prevent user-noticeable heating and maintain acceptable battery life throughout the day. Peak training burst mode can reach 10&nbsp;W, but this power level is sustainable for only 10-30 seconds before thermal throttling kicks in to protect the hardware. Dedicated neural processing units consume 0.5-2&nbsp;W for AI workloads, offering optimized power efficiency compared to general-purpose processors. CPU-based AI processing requires 3-5&nbsp;W and demands aggressive thermal management with duty cycling to prevent overheating, making it the least power-efficient option for sustained on-device learning.</p>
<p>The power consumption characteristics of training workloads create additional layers of constraint that extend beyond simple computational capacity. Power consumption scales superlinearly with model size and training complexity, with training operations consuming 10-50<span class="math inline">\(\times\)</span> more power than equivalent inference workloads due to the substantial computational overhead of gradient computation (consuming 40-70% of training power), weight updates (20-30%), and dramatically increased data movement between memory hierarchies (10-30%). To maintain acceptable user experience, mobile devices typically budget only 500-1000&nbsp;mW for sustained ML training, effectively limiting practical training sessions to 10-100 minutes daily under normal usage patterns. This severe power constraint fundamentally shifts the design priority from maximizing computational throughput to optimizing power efficiency, requiring careful co-optimization of algorithms and hardware utilization patterns.</p>
<p>The thermal management challenges extend far beyond simple power limits, creating complex dynamic constraints that vary with environmental conditions and usage patterns. Training workloads generate localized heat that can trigger protective throttling in specific processor cores or accelerator units, often in unpredictable ways that depend on ambient temperature and device design. Modern mobile SoCs implement sophisticated thermal management systems, including dynamic voltage and frequency scaling (DVFS)<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>, core migration between efficiency and performance clusters, and selective shutdown of non-essential processing units. Successfully deployed on-device learning systems must intimately integrate with these thermal management frameworks, intelligently scheduling training bursts during optimal thermal windows and gracefully degrading performance when thermal limits are approached, rather than simply failing or causing user-visible performance problems.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<strong>Dynamic Voltage and Frequency Scaling (DVFS)</strong>: Modern mobile processors continuously adjust operating voltage and clock frequency based on workload and thermal conditions. During ML training, DVFS can reduce clock speeds by 30-50% when temperature exceeds 70¬∞C, directly impacting training throughput. Effective on-device learning systems monitor thermal state and proactively reduce batch sizes or training intensity to maintain consistent performance rather than experiencing sudden throttling events.</p></div></div></section>
<section id="sec-ondevice-learning-memory-hierarchy-optimization-8396" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-memory-hierarchy-optimization-8396">Memory Hierarchy Optimization</h4>
<p>Complementing the thermal and power challenges, memory hierarchy constraints create another fundamental bottleneck that shapes on-device learning system design. As established in the constraint amplification analysis above, these limitations affect both static model storage and the dynamic memory requirements during training, often pushing systems beyond their practical limits.</p>
<p>The device memory hierarchy spans several orders of magnitude across different device classes, each presenting distinct constraints for on-device learning. The iPhone 15 Pro provides 8&nbsp;GB total system memory, but only approximately 2-4&nbsp;GB remains available for application workloads after accounting for operating system requirements and background processes. Budget Android devices operate with 4&nbsp;GB total system memory, leaving just 1-2&nbsp;GB available for ML workloads after OS overhead consumes significant resources. IoT embedded systems provide 64&nbsp;MB-1&nbsp;GB total memory that must be shared between system tasks and application data, creating severe constraints for any learning algorithms. Microcontrollers offer only 256&nbsp;KB-2&nbsp;MB SRAM, requiring extreme optimization and careful memory management that fundamentally limits the complexity of models that can adapt on such platforms.</p>
<p>The memory expansion during training creates particularly acute challenges that often determine system feasibility. Standard backpropagation requires caching intermediate activations for each layer during the forward pass, which are then reused during gradient computation in the backward pass, creating substantial memory overhead. A MobileNetV2 model requiring just 14&nbsp;MB for inference balloons to 50-70&nbsp;MB during training, often exceeding the available memory budget on many mobile devices and making training impossible without aggressive optimization. This dramatic expansion necessitates sophisticated model compression techniques that must compound multiplicatively: INT8 quantization provides <span class="math inline">\(4\times\)</span> memory reduction, structured pruning achieves <span class="math inline">\(10\times\)</span> parameter reduction, and knowledge distillation enables <span class="math inline">\(5\times\)</span> model size reduction while maintaining accuracy within 2-5% of the original model. These techniques must be carefully combined to achieve the aggressive compression ratios required for practical deployment.</p>
<p>Given these memory constraints, cache optimization becomes absolutely critical for achieving acceptable performance with constrained memory pools. Modern mobile SoCs feature complex memory hierarchies with L1 cache (32-64&nbsp;KB), L2 cache (1-8&nbsp;MB), and system memory (4-16&nbsp;GB) that exhibit 10-100<span class="math inline">\(\times\)</span> latency differences between levels, creating severe performance cliffs when working sets exceed cache capacity. Training workloads that exceed cache capacity face dramatic performance degradation due to memory bandwidth bottlenecks that can slow training by orders of magnitude. Successful on-device learning systems must carefully design data access patterns to maximize cache hit rates, often requiring specialized memory layouts that group related parameters for spatial locality, carefully sized mini-batches that fit entirely within cache constraints, and sophisticated gradient accumulation strategies that minimize expensive memory bus traffic.</p>
<p>The memory bandwidth limitations become particularly acute during training. While inference workloads primarily read model weights sequentially, training requires bidirectional data flow for gradient computation and weight updates. This increased memory traffic can saturate the memory subsystem, creating bottlenecks that limit training throughput regardless of computational capacity. Advanced implementations employ techniques such as gradient checkpointing<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> to trade computation for memory, and mixed-precision training to reduce bandwidth requirements while maintaining numerical stability.</p>
<div class="no-row-height column-margin column-container"><div id="fn20"><p><sup>20</sup>&nbsp;<strong>Gradient Checkpointing</strong>: A memory optimization technique that trades computation for memory by recomputing intermediate activations during the backward pass instead of storing them. This can reduce memory requirements by 50-80% at the cost of 20-30% additional computation. Particularly valuable for on-device training where memory is more constrained than compute capacity, enabling training of larger models within fixed memory budgets.</p></div></div></section>
<section id="sec-ondevice-learning-mobile-ai-accelerator-optimization-67fe" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-mobile-ai-accelerator-optimization-67fe">Mobile AI Accelerator Optimization</h4>
<p>Different mobile platforms provide distinct acceleration capabilities that determine not only achievable model complexity but also feasible learning paradigms. The architectural differences between these accelerators fundamentally shape the design space for on-device training algorithms, influencing everything from numerical precision choices to gradient computation strategies.</p>
<p>Current generation mobile accelerators demonstrate remarkable diversity in their capabilities and optimization focus. Apple‚Äôs Neural Engine in the A17 Pro delivers 35 TOPS peak performance specialized for 8-bit and 16-bit operations, optimized primarily for CoreML inference patterns with limited training support, making it ideal for inference-heavy adaptation techniques. Qualcomm‚Äôs Hexagon DSP in the Snapdragon 8 Gen 3 achieves 45 TOPS with flexible precision support and programmable vector units, enabling mixed-precision training workflows that can adapt precision dynamically based on training phase and memory constraints. Google‚Äôs Tensor TPU in the Pixel 8 is optimized specifically for TensorFlow Lite operations with strong INT8 performance and tight integration with federated learning frameworks, reflecting Google‚Äôs strategic focus on distributed learning scenarios. The energy efficiency comparison reveals why dedicated neural processing units are essential: NPUs achieve 1-5 TOPS per watt versus general-purpose CPUs at just 0.1-0.2 TOPS per watt, representing a 5-50<span class="math inline">\(\times\)</span> efficiency advantage that makes the difference between feasible and infeasible on-device training.</p>
<p>These accelerators determine not just raw performance but feasible learning paradigms and algorithmic approaches. Apple‚Äôs Neural Engine excels at fixed-precision inference workloads but provides limited support for the dynamic precision requirements of gradient computation, making it more suitable for inference-heavy adaptation techniques like few-shot learning. Qualcomm‚Äôs Hexagon DSP offers greater training flexibility through its programmable vector units and support for mixed-precision arithmetic, enabling more sophisticated on-device training including full backpropagation on compact models. Google‚Äôs Tensor TPU integrates tightly with federated learning frameworks and provides optimized communication primitives for distributed training scenarios.</p>
<p>The architectural implications extend beyond computational throughput to memory access patterns and data flow optimization. Training workloads exhibit fundamentally different characteristics than inference: gradient computation requires significantly higher memory bandwidth due to the amplification effects discussed above, weight updates create write-heavy access patterns, and optimizer state management demands additional memory allocation. Modern edge accelerators are beginning to address these challenges through specialized hardware features including adaptive precision datapaths that dynamically switch between INT4 for forward passes and FP16 for gradient computation, sparse computation units that accelerate selective parameter updates by skipping zero gradients, and near-memory compute architectures that reduce data movement costs by performing gradient updates directly adjacent to weight storage.</p>
<p>However, most current edge accelerators remain primarily optimized for inference workloads, creating a significant hardware-software co-design opportunity. Future on-device training accelerators will need to efficiently handle the unique demands of local adaptation, including support for dynamic precision scaling, efficient gradient accumulation, and specialized memory hierarchies optimized for the bidirectional data flow patterns characteristic of training workloads. Architecture selection influences everything from model quantization strategies and gradient computation approaches to federated communication protocols and thermal management policies.</p>
</section>
</section>
<section id="sec-ondevice-learning-holistic-resource-management-strategies-9e4b" class="level3">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-holistic-resource-management-strategies-9e4b">Holistic Resource Management Strategies</h3>
<p>The constraint analysis above reveals three fundamental challenge categories that define the on-device learning design space. Each constraint category directly drives a corresponding solution pillar, creating a systematic engineering approach to this complex systems problem. The constraint-to-solution mapping follows naturally from understanding how specific limitations necessitate particular technical responses.</p>
<p>The resource amplification effects‚Äîwhere training increases memory requirements by 3-10<span class="math inline">\(\times\)</span>, computational costs by 2-3<span class="math inline">\(\times\)</span>, and energy consumption proportionally‚Äîdirectly necessitate Model Adaptation approaches. When traditional training becomes impossible due to resource constraints, systems must fundamentally reduce the scope of parameter updates while preserving learning capability.</p>
<p>The information scarcity constraints‚Äîlimited local datasets, non-IID distributions, privacy restrictions on data sharing, and minimal supervision‚Äîdirectly drive Data Efficiency solutions. When conventional data-hungry approaches fail due to insufficient local information, systems must extract maximum learning signal from minimal examples.</p>
<p>The coordination challenges‚Äîdevice heterogeneity, intermittent connectivity, distributed validation complexity, and scalability requirements‚Äîdirectly motivate Federated Coordination mechanisms. When isolated on-device learning limits collective intelligence, systems must enable privacy-preserving collaboration across device populations.</p>
<p>This constraint-to-solution mapping, illustrated in <a href="#tbl-constraint-solution-mapping" class="quarto-xref">Table&nbsp;2</a>, creates a systematic engineering framework where each pillar addresses specific aspects of the deployment challenge while integrating with the others. Rather than viewing these as independent techniques, successful on-device learning systems orchestrate all three approaches to create coherent adaptive systems that operate effectively within edge constraints.</p>
<div id="tbl-constraint-solution-mapping" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-constraint-solution-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Constraint-Solution Mapping</strong>: The three fundamental constraint categories in on-device learning each drive corresponding solution approaches through direct necessity.
</figcaption>
<div aria-describedby="tbl-constraint-solution-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 37%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Constraint Category</strong></th>
<th style="text-align: left;"><strong>Key Challenges</strong></th>
<th style="text-align: left;"><strong>Solution Approach</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Resource Amplification</strong></td>
<td style="text-align: left;">‚Ä¢&nbsp;Training workloads (3-10√ó memory) ‚Ä¢&nbsp;Memory&nbsp;limitations ‚Ä¢&nbsp;Power&nbsp;constraints</td>
<td style="text-align: left;"><strong>Model Adaptation</strong> ‚Ä¢&nbsp;Parameter-efficient updates ‚Ä¢&nbsp;Selective layer fine-tuning ‚Ä¢&nbsp;Low-rank&nbsp;adaptations</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Information Scarcity</strong></td>
<td style="text-align: left;">‚Ä¢&nbsp;Limited local datasets ‚Ä¢&nbsp;Non-IID&nbsp;distributions ‚Ä¢&nbsp;Privacy&nbsp;restrictions</td>
<td style="text-align: left;"><p><strong>Data Efficiency</strong></p>
<p>‚Ä¢&nbsp;Few-shot learning</p>
<p>‚Ä¢&nbsp;Meta-learning ‚Ä¢&nbsp;Transfer learning</p></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Coordination Challenges</strong></td>
<td style="text-align: left;">‚Ä¢&nbsp;Device heterogeneity ‚Ä¢&nbsp;Intermittent&nbsp;connectivity ‚Ä¢&nbsp;Distributed&nbsp;validation</td>
<td style="text-align: left;"><strong>Federated Coordination</strong> ‚Ä¢&nbsp;Privacy-preserving aggregation ‚Ä¢&nbsp;Robust communication protocols ‚Ä¢&nbsp;Asynchronous participation</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The subsequent sections examine each solution pillar systematically, building on the optimization principles from <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong> and the distributed systems frameworks from <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>. Each pillar provides essential capabilities that the others cannot deliver alone, but their integration creates systems capable of meaningful adaptation within the severe constraints of edge deployment environments.</p>
<div id="quiz-question-sec-ondevice-learning-design-constraints-c776" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes a challenge of on-device learning compared to cloud-based training?</p>
<ol type="a">
<li>Access to large, curated datasets</li>
<li>Higher computational capacity</li>
<li>Limited memory and computational resources</li>
<li>Centralized model updates</li>
</ol></li>
<li><p>Explain how model compression techniques are essential for on-device learning, particularly during training.</p></li>
<li><p>On-device learning requires careful management of ____ due to increased memory and computational demands during training.</p></li>
<li><p>True or False: On-device learning systems can use the same model architectures as cloud-based systems without modification.</p></li>
<li><p>Order the following steps in the on-device learning process: (1) Meta-training with generic data, (2) Online adaptive learning, (3) Ranking and selecting layers to update.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-design-constraints-c776" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ondevice-learning-model-adaptation-6a82" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-model-adaptation-6a82">Model Adaptation</h2>
<p>The computational and memory constraints outlined above create a challenging environment for model training, but they also reveal clear solution pathways when approached systematically. Model adaptation represents the first pillar of on-device learning systems engineering: reducing the scope of parameter updates to make training feasible within edge constraints while maintaining sufficient model expressivity for meaningful personalization.</p>
<p>The engineering challenge centers on navigating a fundamental trade-off space: adaptation expressivity versus resource consumption. At one extreme, updating all parameters provides maximum flexibility but exceeds edge device capabilities. At the other extreme, no adaptation preserves resources but fails to capture user-specific patterns. Effective on-device learning systems must operate in the middle ground, selecting adaptation strategies based on three key engineering criteria.</p>
<p>First, available memory, compute, and energy determine which adaptation approaches are feasible. A smartwatch with 1&nbsp;MB RAM requires fundamentally different strategies than a smartphone with 8&nbsp;GB. Second, the degree of user-specific variation drives adaptation complexity needs. Simple preference learning may require only bias updates, while complex domain shifts demand more sophisticated approaches. Third, adaptation techniques must integrate with existing inference pipelines, federated coordination protocols, and operational monitoring systems established in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>.</p>
<p>This systems perspective guides the selection and combination of techniques starting with lightweight approaches (<a href="#sec-ondevice-learning-weight-freezing-3407" class="quarto-xref">Section&nbsp;1.4.1</a>) and progressing to more sophisticated methods (<a href="#sec-ondevice-learning-sparse-updates-879b" class="quarto-xref">Section&nbsp;1.4.3</a>), moving from lightweight bias-only approaches through progressively more expressive but resource-intensive methods. Each technique represents a different point in the engineering trade-off space rather than an isolated algorithmic solution.</p>
<p>Building on the compression techniques from <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong>, on-device learning transforms compression from a one-time optimization into an ongoing constraint. The central insight driving all model adaptation approaches is that complete model retraining is neither necessary nor feasible for on-device learning scenarios. Instead, systems can strategically leverage pre-trained representations and adapt only the minimal parameter subset required to capture local variations, operating on the principle: preserve what works globally, adapt what matters locally.</p>
<p>This section systematically examines three complementary adaptation strategies, each specifically designed to address different device constraint profiles and application requirements. Weight freezing addresses severe memory limitations by updating only bias terms or final layers, enabling learning even on severely constrained microcontrollers that would otherwise lack the resources for any form of adaptation. Structured updates use low-rank and residual adaptations to balance model expressiveness with computational efficiency, enabling more sophisticated learning than bias-only approaches while maintaining manageable resource requirements. Sparse updates enable selective parameter modification based on gradient importance or layer criticality, concentrating learning capacity on the most impactful parameters while leaving less important weights frozen.</p>
<p>These approaches build on established architectural principles while strategically applying optimization strategies to the unique challenges of edge deployment. Each technique represents a carefully considered point in the fundamental accuracy-efficiency tradeoff space, enabling practical deployment across the full spectrum of edge hardware capabilities‚Äîfrom ultra-constrained microcontrollers to capable mobile processors.</p>
<section id="sec-ondevice-learning-weight-freezing-3407" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-weight-freezing-3407">Weight Freezing</h3>
<p>The most straightforward approach to making on-device learning feasible is to dramatically reduce the number of parameters that require updating. One of the simplest and most effective strategies for achieving this reduction is to freeze the majority of a model‚Äôs parameters and adapt only a carefully chosen minimal subset. The most widely used approach within this family is bias-only adaptation, in which all weights are held fixed and only the bias terms (typically scalar offsets applied after linear or convolutional layers) are updated during training. This simple constraint creates significant benefits: it reduces the number of trainable parameters (often by 100-1000<span class="math inline">\(\times\)</span>), simplifies memory management during backpropagation, and helps mitigate overfitting when training data is sparse or noisy.</p>
<p>Consider a standard neural network layer: <span class="math display">\[
y = W x + b
\]</span> where <span class="math inline">\(W \in \mathbb{R}^{m \times n}\)</span> is the weight matrix, <span class="math inline">\(b \in \mathbb{R}^m\)</span> is the bias vector, and <span class="math inline">\(x \in \mathbb{R}^n\)</span> is the input. In full training, gradients are computed for both <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span>. In bias-only adaptation, we constrain: <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial W} = 0, \quad \frac{\partial \mathcal{L}}{\partial b} \neq 0
\]</span> so that only the bias is updated via gradient descent: <span class="math display">\[
b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b}
\]</span></p>
<p>This reduces the number of stored gradients and optimizer states, enabling training to proceed under memory-constrained conditions. On embedded devices that lack floating-point units, this reduction enables on-device learning.</p>
<p>The code snippet in <a href="#lst-bias-adaptation" class="quarto-xref">Listing&nbsp;1</a> demonstrates how to implement bias-only adaptation in PyTorch.</p>
<div id="lst-bias-adaptation" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-bias-adaptation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: <strong>Bias-Only Adaptation</strong>: Freezes model parameters except for biases to reduce memory usage and allow on-device learning.
</figcaption>
<div aria-describedby="lst-bias-adaptation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze all parameters</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Enable gradients for bias parameters only</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"bias"</span> <span class="kw">in</span> name:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This pattern ensures that only bias terms participate in the backward pass and optimizer update, simplifying the training process while maintaining adaptation capability. It proves valuable when adapting pretrained models to user-specific or device-local data where the core representations remain relevant but require calibration.</p>
<p>The practical effectiveness of this approach is demonstrated by TinyTL, a framework explicitly designed to enable efficient adaptation of deep neural networks on microcontrollers and other severely memory-limited platforms. Rather than updating all network parameters during training (impossible on such constrained devices), TinyTL strategically freezes both the convolutional weights and the batch normalization statistics, training only the bias terms and, in some cases, lightweight residual components. This architectural constraint creates a profound shift in memory requirements during backpropagation, since the largest memory consumers (intermediate activations) no longer need to be stored for gradient computation across frozen layers.</p>
<p>The architectural impact of this approach becomes clear when comparing standard training with the TinyTL approach. <a href="#fig-tinytl-architecture" class="quarto-xref">Figure&nbsp;5</a> illustrates the fundamental differences between a conventional model and the TinyTL approach to on-device adaptation. Given the edge device memory constraints established earlier, the TinyTL approach fundamentally changes the memory equation by eliminating the need to store activations for frozen layers, making adaptation possible within the severe memory constraints of edge devices.</p>
<div id="fig-tinytl-architecture" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tinytl-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4dd56f31327163c092f937596ccb4692bd02639e.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: TinyTL reduces on-device training costs by freezing convolutional weights and batch normalization, updating only bias terms and lightweight residual connections to minimize memory usage during backpropagation. This approach allows deployment of deep neural networks on resource-constrained edge devices with limited SRAM, facilitating efficient model adaptation without requiring full parameter updates."><img src="ondevice_learning_files/mediabag/4dd56f31327163c092f937596ccb4692bd02639e.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tinytl-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: TinyTL reduces on-device training costs by freezing convolutional weights and batch normalization, updating only bias terms and lightweight residual connections to minimize memory usage during backpropagation. This approach allows deployment of deep neural networks on resource-constrained edge devices with limited SRAM, facilitating efficient model adaptation without requiring full parameter updates.
</figcaption>
</figure>
</div>
<p>In contrast, the TinyTL architecture freezes all weights and updates only the bias terms inserted after convolutional layers. These bias modules are lightweight and require minimal memory, enabling efficient training with a drastically reduced memory footprint. The frozen convolutional layers act as a fixed feature extractor, and only the trainable bias components are involved in adaptation. By avoiding storage of full activation maps and limiting the number of updated parameters, TinyTL allows on-device training under severe resource constraints.</p>
<p>Because the base model remains unchanged, TinyTL assumes that the pretrained features are sufficiently expressive for downstream tasks. The bias terms allow for minor but meaningful shifts in model behavior, particularly for personalization tasks. When domain shift is more significant, TinyTL can optionally incorporate small residual adapters to improve expressivity, all while preserving the system‚Äôs tight memory and energy profile.</p>
<p>These design choices allow TinyTL to reduce training memory usage by 10√ó. For instance, adapting a MobileNetV2 model using TinyTL can reduce the number of updated parameters from over 3 million to fewer than 50,000<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>. Combined with quantization, this allows local adaptation on devices with only a few hundred kilobytes of memory‚Äîmaking on-device learning truly feasible in constrained environments.</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;<strong>TinyTL Memory Breakthrough</strong>: TinyTL‚Äôs 60<span class="math inline">\(\times\)</span> parameter reduction (3.4&nbsp;M to 50&nbsp;K) translates to dramatic memory savings. In FP32, MobileNetV2 requires ~12&nbsp;MB for weights plus ~8&nbsp;MB for activation caching during training‚Äîexceeding most microcontroller capabilities. TinyTL reduces this to ~200&nbsp;KB weights plus ~400&nbsp;KB activations, fitting comfortably within a 1&nbsp;MB memory budget. Real deployments on STM32H7 achieve 85% of full fine-tuning accuracy while using 15<span class="math inline">\(\times\)</span> less memory and completing updates in ~30 seconds versus 8 minutes for full training.</p></div></div></section>
<section id="sec-ondevice-learning-structured-parameter-updates-f1a1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-structured-parameter-updates-f1a1">Structured Parameter Updates</h3>
<p>While weight freezing provides computational efficiency and clear memory bounds, it severely limits model expressivity by constraining adaptation to a small parameter subset. When bias-only updates prove insufficient for capturing complex domain shifts or user-specific patterns, residual and low-rank techniques provide increased adaptation capability while maintaining computational tractability. These approaches represent a middle ground between the extreme efficiency of weight freezing and the full expressivity of unrestricted fine-tuning.</p>
<p>Rather than modifying existing parameters, these methods extend frozen models by adding trainable components‚Äîresidual adaptation modules <span class="citation" data-cites="houlsby2019parameter">(<a href="#ref-houlsby2019parameter" role="doc-biblioref">Houlsby et al. 2019</a>)</span> or low-rank parameterizations <span class="citation" data-cites="hu2021lora">(<a href="#ref-hu2021lora" role="doc-biblioref">Hu et al. 2021</a>)</span>‚Äîthat provide controlled increases in model capacity. This architectural approach enables more sophisticated adaptation while preserving the computational benefits that make on-device learning feasible.</p>
<div class="no-row-height column-margin column-container"><div id="ref-houlsby2019parameter" class="csl-entry" role="listitem">
Houlsby, Neil, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Chlo√© de Laroussilhe, Andrea Gesmundo, Mohammad Attariyan, and Sylvain Gelly. 2019. <span>‚ÄúParameter-Efficient Transfer Learning for NLP.‚Äù</span> In <em>International Conference on Machine Learning</em>, 2790‚Äì99. PMLR.
</div></div><p>These methods extend a frozen model by adding trainable layers, which are typically small and computationally inexpensive, that allow the network to respond to new data. The main body of the network remains fixed, while only the added components are optimized. This modularity makes the approach well-suited for on-device adaptation in constrained settings, where small updates must deliver meaningful changes.</p>
<section id="sec-ondevice-learning-adapterbased-adaptation-6a9a" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-adapterbased-adaptation-6a9a">Adapter-Based Adaptation</h4>
<p>A common implementation involves inserting adapters, which are small residual bottleneck layers, between existing layers in a pretrained model. Consider a hidden representation <span class="math inline">\(h\)</span> passed between layers. A residual adapter introduces a transformation: <span class="math display">\[
h' = h + A(h)
\]</span> where <span class="math inline">\(A(\cdot)\)</span> is a trainable function, typically composed of two linear layers with a nonlinearity: <span class="math display">\[
A(h) = W_2 \, \sigma(W_1 h)
\]</span> with <span class="math inline">\(W_1 \in \mathbb{R}^{r \times d}\)</span> and <span class="math inline">\(W_2 \in \mathbb{R}^{d \times r}\)</span>, where <span class="math inline">\(r \ll d\)</span>. This bottleneck design ensures that only a small number of parameters are introduced per layer.</p>
<p>The adapters act as learnable perturbations on top of a frozen backbone. Because they are small and sparsely applied, they add negligible memory overhead, yet they allow the model to shift its predictions in response to new inputs.</p>
</section>
<section id="sec-ondevice-learning-lowrank-techniques-4570" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-lowrank-techniques-4570">Low-Rank Techniques</h4>
<p>Another efficient strategy is to constrain weight updates themselves to a low-rank structure. Rather than updating a full matrix <span class="math inline">\(W\)</span>, we approximate the update as: <span class="math display">\[
\Delta W \approx U V^\top
\]</span> where <span class="math inline">\(U \in \mathbb{R}^{m \times r}\)</span> and <span class="math inline">\(V \in \mathbb{R}^{n \times r}\)</span>, with <span class="math inline">\(r \ll \min(m,n)\)</span>. This reduces the number of trainable parameters from <span class="math inline">\(mn\)</span> to <span class="math inline">\(r(m + n)\)</span>.</p>
<p>The mathematical intuition behind this decomposition connects to fundamental linear algebra principles: any matrix can be expressed as a sum of rank-one matrices through singular value decomposition. By constraining our updates to low rank (typically <span class="math inline">\(r = 4\)</span> to <span class="math inline">\(16\)</span>), we capture the most significant modes of variation while reducing parameters. For a typical transformer layer with dimensions <span class="math inline">\(768 \times 768\)</span>, full fine-tuning requires updating 589,824 parameters. With rank-4 decomposition, we update only <span class="math inline">\(768 \times 4 \times 2 = 6,144\)</span> parameters, a 96% reduction, while empirically retaining 85-90% of the adaptation quality.</p>
<p>During adaptation, the new weight is computed as: <span class="math display">\[
W_{\text{adapted}} = W_{\text{frozen}} + U V^\top
\]</span></p>
<p>This formulation is commonly used in LoRA (Low-Rank Adaptation)<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> techniques, originally developed for transformer models <span class="citation" data-cites="hu2021lora">(<a href="#ref-hu2021lora" role="doc-biblioref">Hu et al. 2021</a>)</span> but broadly applicable across architectures. From a systems engineering perspective, LoRA addresses critical connectivity and resource trade-offs in on-device learning deployment.</p>
<div class="no-row-height column-margin column-container"><div id="fn22"><p><sup>22</sup>&nbsp;<strong>LoRA (Low-Rank Adaptation)</strong>: Introduced by Microsoft in 2021, LoRA enables efficient fine-tuning by learning low-rank decomposition matrices rather than updating full weight matrices. For a weight matrix W, LoRA learns rank-r matrices A and B such that the update is BA (where r &lt;&lt; original dimensions). This reduces trainable parameters by 100-10000<span class="math inline">\(\times\)</span> while maintaining 90-95% adaptation quality. LoRA has become the standard for parameter-efficient fine-tuning in large language models.</p></div><div id="ref-hu2021lora" class="csl-entry" role="listitem">
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>‚ÄúLoRA: Low-Rank Adaptation of Large Language Models.‚Äù</span> <em>arXiv Preprint arXiv:2106.09685</em>, June. <a href="http://arxiv.org/abs/2106.09685v2">http://arxiv.org/abs/2106.09685v2</a>.
</div></div><p>Consider a mobile deployment where a 7B parameter language model requires 14&nbsp;GB for full fine-tuning‚Äîimpossible on typical smartphones with 6-8&nbsp;GB total memory. LoRA with rank-16 reduces this to ~100&nbsp;MB of trainable parameters (0.7% of original), enabling local adaptation within mobile memory constraints.</p>
<p>LoRA‚Äôs efficiency becomes critical in intermittent connectivity scenarios. A full model update over cellular networks would require 14&nbsp;GB download (potential cost $140+ in mobile data charges), while LoRA adapter updates are typically 10-50&nbsp;MB. For periodic federated coordination, devices can synchronize LoRA adapters in under 30 seconds on 3G networks, compared to hours for full model transfers. This enables practical federated learning even with poor network conditions.</p>
<p>Systems typically deploy different LoRA configurations based on device capabilities‚Äîflagship phones use rank-32 adapters for higher expressivity, mid-range devices use rank-16 for balanced performance, and budget devices use rank-8 to stay within 2&nbsp;GB memory limits. Low-rank updates can be implemented efficiently on edge devices, particularly when <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are small and fixed-point representations are supported (<a href="#lst-lowrank-adapter" class="quarto-xref">Listing&nbsp;2</a>).</p>
<div id="lst-lowrank-adapter" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-lowrank-adapter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: <strong>Low-Rank Adapter</strong>: The code implements a low-rank adapter module by approximating weight updates using matrices (u) and (v), reducing parameter count while enabling efficient model adaptation on edge devices.
</figcaption>
<div aria-describedby="lst-lowrank-adapter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Adapter(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, bottleneck_dim):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down <span class="op">=</span> nn.Linear(dim, bottleneck_dim)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up <span class="op">=</span> nn.Linear(bottleneck_dim, dim)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.ReLU()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> <span class="va">self</span>.up(<span class="va">self</span>.activation(<span class="va">self</span>.down(x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This adapter adds a small residual transformation to a frozen layer. When inserted into a larger model, only the adapter parameters are trained.</p>
</section>
<section id="sec-ondevice-learning-edge-personalization-a511" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-edge-personalization-a511">Edge Personalization</h4>
<p>Adapters are useful when a global model is deployed to many devices and must adapt to device-specific input distributions. In smartphone camera pipelines, environmental lighting, user preferences, or lens distortion vary between users <span class="citation" data-cites="rebuffi2017learning">(<a href="#ref-rebuffi2017learning" role="doc-biblioref">Rebuffi, Bilen, and Vedaldi 2017</a>)</span>. A shared model can be frozen and fine-tuned per-device using a few residual modules, allowing lightweight personalization without risking catastrophic forgetting. In voice-based systems, adapter modules have been shown to reduce word error rates in personalized speech recognition without retraining the full acoustic model. They also allow easy rollback or switching between user-specific versions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rebuffi2017learning" class="csl-entry" role="listitem">
Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. 2017. <span>‚ÄúLearning Multiple Visual Domains with Residual Adapters.‚Äù</span> In <em>Advances in Neural Information Processing Systems</em>. Vol. 30.
</div></div></section>
<section id="sec-ondevice-learning-performance-vs-resource-tradeoffs-2be2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-performance-vs-resource-tradeoffs-2be2">Performance vs.&nbsp;Resource Trade-offs</h4>
<p>Residual and low-rank updates strike a balance between expressivity and efficiency. Compared to bias-only learning, they can model more substantial deviations from the pretrained task. However, they require more memory and compute for training and inference.</p>
<p>When considering residual and low-rank updates for on-device learning, several important tradeoffs emerge. First, these methods consistently demonstrate superior adaptation quality compared to bias-only approaches, particularly when deployed in scenarios involving significant distribution shifts from the original training data <span class="citation" data-cites="quinonero2009dataset">(<a href="#ref-quinonero2009dataset" role="doc-biblioref">Qui√±onero-Candela et al. 2008</a>)</span>. This improved adaptability stems from their increased parameter capacity and ability to learn more complex transformations.</p>
<div class="no-row-height column-margin column-container"><div id="ref-quinonero2009dataset" class="csl-entry" role="listitem">
Qui√±onero-Candela, Joaquin, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. 2008. <span>‚ÄúDataset Shift in Machine Learning.‚Äù</span> <em>The MIT Press</em>. The MIT Press. <a href="https://doi.org/10.7551/mitpress/7921.003.0002">https://doi.org/10.7551/mitpress/7921.003.0002</a>.
</div></div><p>This enhanced adaptability comes at a cost. The introduction of additional layers or parameters inevitably increases both memory requirements and computational latency during forward and backward passes. While these increases are modest compared to full model training, they must be considered when deploying to resource-constrained devices.</p>
<p>Implementing these adaptation techniques requires system-level support for dynamic computation graphs and the ability to selectively inject trainable parameters. Not all deployment environments or inference engines support such capabilities out of the box.</p>
<p>Residual adaptation techniques have proven valuable in mobile and edge computing scenarios where devices have sufficient computational resources. Modern smartphones and tablets can accommodate these adaptations while maintaining acceptable performance characteristics. This makes residual adaptation a practical choice for applications requiring personalization without the overhead of full model retraining.</p>
</section>
</section>
<section id="sec-ondevice-learning-sparse-updates-879b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-sparse-updates-879b">Sparse Updates</h3>
<p>As we progress from bias-only updates through low-rank adaptations to more sophisticated techniques, sparse updates represent the most advanced approach in our model adaptation hierarchy. While the previous techniques add new parameters or restrict updates to specific subsets, sparse updates dynamically identify which existing parameters provide the greatest adaptation benefit for each specific task or user. This approach maximizes adaptation expressivity while maintaining the computational efficiency essential for edge deployment.</p>
<p>Even when adaptation is restricted to a small number of parameters through the techniques discussed above, training remains resource-intensive on constrained devices. Sparse updates address this challenge by selectively updating only task-relevant subsets of model parameters, rather than modifying entire networks or introducing new modules. This approach, known as task-adaptive sparse updating <span class="citation" data-cites="zhang2020efficient">(<a href="#ref-zhang2020efficient" role="doc-biblioref">Zhang, Song, and Tao 2020</a>)</span>, represents the culmination of principled parameter selection strategies.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2020efficient" class="csl-entry" role="listitem">
Zhang, Xitong, Jialin Song, and Dacheng Tao. 2020. <span>‚ÄúEfficient Task-Specific Adaptation for Deep Models.‚Äù</span> In <em>International Conference on Learning Representations (ICLR)</em>.
</div></div><p>The key insight is that not all layers of a deep model contribute equally to performance gains on a new task or dataset. If we can identify a <em>minimal subset of parameters</em> that are most impactful for adaptation, we can train only those, reducing memory and compute costs while still achieving meaningful personalization.</p>
<section id="sec-ondevice-learning-sparse-update-design-ee7c" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-sparse-update-design-ee7c">Sparse Update Design</h4>
<p>Let a neural network be defined by parameters <span class="math inline">\(\theta = \{\theta_1, \theta_2, \ldots, \theta_L\}\)</span> across <span class="math inline">\(L\)</span> layers. In standard fine-tuning, we compute gradients and perform updates on all parameters: <span class="math display">\[
\theta_i \leftarrow \theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, \quad \text{for } i = 1, \ldots, L
\]</span></p>
<p>In task-adaptive sparse updates, we select a small subset <span class="math inline">\(\mathcal{S} \subset \{1, \ldots, L\}\)</span> such that only parameters in <span class="math inline">\(\mathcal{S}\)</span> are updated: <span class="math display">\[
\theta_i \leftarrow
\begin{cases}
\theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, &amp; \text{if } i \in \mathcal{S} \\
\theta_i, &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>The challenge lies in selecting the optimal subset <span class="math inline">\(\mathcal{S}\)</span> given memory and compute constraints.</p>
</section>
<section id="sec-ondevice-learning-layer-selection-ab3c" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-layer-selection-ab3c">Layer Selection</h4>
<p>A principled strategy for selecting <span class="math inline">\(\mathcal{S}\)</span> is to use contribution analysis‚Äîan empirical method that estimates how much each layer contributes to downstream performance improvement. For example, one can measure the marginal gain from updating each layer independently:</p>
<ol type="1">
<li>Freeze the entire model.</li>
<li>Unfreeze one candidate layer.</li>
<li>Finetune briefly and evaluate improvement in validation accuracy.</li>
<li>Rank layers by performance gain per unit cost (e.g., per KB of trainable memory).</li>
</ol>
<p>This layer-wise profiling yields a ranking from which <span class="math inline">\(\mathcal{S}\)</span> can be constructed subject to a memory budget.</p>
<p>A concrete example is TinyTrain, a method designed to allow rapid adaptation on-device <span class="citation" data-cites="deng2022tinytrain">(<a href="#ref-deng2022tinytrain" role="doc-biblioref">C. Deng, Zhang, and Wu 2022</a>)</span>. TinyTrain pretrains a model along with meta-gradients that capture which layers are most sensitive to new tasks. At runtime, the system dynamically selects layers to update based on task characteristics and available resources.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deng2022tinytrain" class="csl-entry" role="listitem">
Deng, Chulin, Yujun Zhang, and Yanzhi Wu. 2022. <span>‚ÄúTinyTrain: Learning to Train Compact Neural Networks on the Edge.‚Äù</span> In <em>Proceedings of the 39th International Conference on Machine Learning (ICML)</em>.
</div></div></section>
<section id="sec-ondevice-learning-selective-layer-update-implementation-eed2" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-selective-layer-update-implementation-eed2">Selective Layer Update Implementation</h4>
<p>This pattern can be extended with profiling logic to select layers based on contribution scores or hardware profiles, as shown in <a href="#lst-selective-update" class="quarto-xref">Listing&nbsp;3</a>.</p>
<div id="lst-selective-update" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-selective-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: <strong>Selective Layer Updating</strong>: This technique allows fine-tuning specific layers of a pre-trained model while keeping others frozen, optimizing computational resources for targeted improvements. <em>Source: PyTorch Documentation</em>
</figcaption>
<div aria-describedby="lst-selective-update-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assume model has named layers: ['conv1', 'conv2', 'fc']</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We selectively update only conv2 and fc</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"conv2"</span> <span class="kw">in</span> name <span class="kw">or</span> <span class="st">"fc"</span> <span class="kw">in</span> name:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
</section>
<section id="sec-ondevice-learning-tinytrain-personalization-66f2" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-tinytrain-personalization-66f2">TinyTrain Personalization</h4>
<p>Consider a scenario where a user wears an augmented reality headset that performs real-time object recognition. As lighting and environments shift, the system must adapt to maintain accuracy‚Äîbut training must occur during brief idle periods or while charging.</p>
<p>TinyTrain allows this by using meta-training during offline preparation: the model learns not only to perform the task, but also which parameters are most important to adapt. Then, at deployment, the device performs task-adaptive sparse updates, modifying only a few layers that are most relevant for its current environment. This keeps adaptation fast, energy-efficient, and memory-aware.</p>
</section>
<section id="sec-ondevice-learning-adaptation-strategy-tradeoffs-cb23" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-adaptation-strategy-tradeoffs-cb23">Adaptation Strategy Trade-offs</h4>
<p>Task-adaptive sparse updates introduce several important system-level considerations that must be carefully balanced. First, the overhead of contribution analysis, although primarily incurred during pretraining or initial profiling, represents a non-trivial computational cost. This overhead is typically acceptable since it occurs offline, but it must be factored into the overall system design and deployment pipeline.</p>
<p>Second, the stability of the adaptation process becomes important when working with sparse updates. If too few parameters are selected for updating, the model may underfit the target distribution, failing to capture important local variations. This suggests the need for careful validation of the selected parameter subset before deployment, potentially incorporating minimum thresholds for adaptation capacity.</p>
<p>Third, the selection of updatable parameters must account for hardware-specific characteristics of the target platform. Beyond just considering gradient magnitudes, the system must evaluate the actual execution cost of updating specific layers on the deployed hardware. Some parameters might show high contribution scores but prove expensive to update on certain architectures, requiring a more nuanced selection strategy that balances statistical utility with runtime efficiency.</p>
<p>Despite these tradeoffs, task-adaptive sparse updates provide a powerful mechanism to scale adaptation to diverse deployment contexts, from microcontrollers to mobile devices <span class="citation" data-cites="diao2023sparse">(<a href="#ref-diao2023sparse" role="doc-biblioref">Levy et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-diao2023sparse" class="csl-entry" role="listitem">
Levy, Orin, Alon Cohen, Asaf Cassel, and Yishay Mansour. 2023. <span>‚ÄúEfficient Rate Optimal Regret for Adversarial Contextual MDPs Using Online Function Approximation.‚Äù</span> <em>arXiv Preprint arXiv:2303.01464</em>, March. <a href="http://arxiv.org/abs/2303.01464v2">http://arxiv.org/abs/2303.01464v2</a>.
</div></div></section>
<section id="sec-ondevice-learning-adaptation-strategy-comparison-1faf" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-adaptation-strategy-comparison-1faf">Adaptation Strategy Comparison</h4>
<p>Each adaptation strategy for on-device learning offers a distinct balance between expressivity, resource efficiency, and implementation complexity. Understanding these tradeoffs is important when designing systems for diverse deployment targets‚Äîfrom ultra-low-power microcontrollers to feature-rich mobile processors.</p>
<p>Bias-only adaptation is the most lightweight approach, updating only scalar offsets in each layer while freezing all other parameters. This significantly reduces memory requirements and computational burden, making it suitable for devices with tight memory and energy budgets. However, its limited expressivity means it is best suited to applications where the pretrained model already captures most of the relevant task features and only minor local calibration is required.</p>
<p>Residual adaptation, often implemented via adapter modules, introduces a small number of trainable parameters into the frozen backbone of a neural network. This allows for greater flexibility than bias-only updates, while still maintaining control over the adaptation cost. Because the backbone remains fixed, training can be performed efficiently and safely under constrained conditions. This method supports modular personalization across tasks and users, making it a favorable choice for mobile settings where moderate adaptation capacity is needed.</p>
<p>Task-adaptive sparse updates offer the greatest potential for task-specific finetuning by selectively updating only a subset of layers or parameters based on their contribution to downstream performance. While this method allows expressive local adaptation, it requires a mechanism for layer selection, through profiling, contribution analysis, or meta-training, which introduces additional complexity. Nonetheless, when deployed carefully, it allows for dynamic tradeoffs between accuracy and efficiency, particularly in systems that experience large domain shifts or evolving input conditions.</p>
<p>These three approaches form a spectrum of tradeoffs. Their relative suitability depends on application domain, available hardware, latency constraints, and expected distribution shift. <a href="#tbl-adaptation-strategies" class="quarto-xref">Table&nbsp;3</a> summarizes their characteristics:</p>
<div id="tbl-adaptation-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-adaptation-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: <strong>Adaptation Strategy Trade-Offs</strong>: Table entries characterize three approaches to model adaptation‚Äîbias-only updates, selective layer updates, and full finetuning‚Äîby quantifying their impact on trainable parameters, memory overhead, expressivity, suitability for different use cases, and system requirements. These characteristics reveal the inherent trade-offs between model flexibility, computational cost, and performance when deploying machine learning systems in dynamic environments.
</figcaption>
<div aria-describedby="tbl-adaptation-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 16%">
<col style="width: 11%">
<col style="width: 12%">
<col style="width: 20%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Technique</strong></th>
<th style="text-align: left;"><strong>Trainable Parameters</strong></th>
<th style="text-align: left;"><strong>Memory Overhead</strong></th>
<th style="text-align: left;"><strong>Expressivity</strong></th>
<th style="text-align: left;"><strong>Use Case Suitability</strong></th>
<th style="text-align: left;"><strong>System Requirements</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Bias-Only Updates</strong></td>
<td style="text-align: left;">Bias terms only</td>
<td style="text-align: left;">Minimal</td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Simple personalization; low variance</td>
<td style="text-align: left;">Extreme memory/compute limits</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Residual Adapters</strong></td>
<td style="text-align: left;">Adapter modules</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">Moderate to High</td>
<td style="text-align: left;">User-specific tuning on mobile</td>
<td style="text-align: left;">Mobile-class SoCs with runtime support</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Sparse Layer Updates</strong></td>
<td style="text-align: left;">Selective parameter subsets</td>
<td style="text-align: left;">Variable</td>
<td style="text-align: left;">High (task-adaptive)</td>
<td style="text-align: left;">Real-time adaptation; domain shift</td>
<td style="text-align: left;">Requires profiling or meta-training</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="quiz-question-sec-ondevice-learning-model-adaptation-6a82" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Which of the following adaptation strategies is most suitable for devices with extreme memory and compute constraints?</p>
<ol type="a">
<li>Sparse layer updates</li>
<li>Residual adapters</li>
<li>Bias-only updates</li>
<li>Full model retraining</li>
</ol></li>
<li><p>Explain the trade-offs involved in using residual adapters for on-device learning.</p></li>
<li><p>In task-adaptive sparse updates, only a subset of parameters is updated based on their ____ to downstream performance.</p></li>
<li><p>Order the following adaptation strategies from least to most expressive: (1) Residual adapters, (2) Bias-only updates, (3) Sparse layer updates.</p></li>
<li><p>In a production system with limited memory, which adaptation strategy would enable efficient personalization without full retraining?</p>
<ol type="a">
<li>Full model retraining</li>
<li>Low-rank updates</li>
<li>Sparse layer updates</li>
<li>Bias-only updates</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-model-adaptation-6a82" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-ondevice-learning-data-efficiency-c701" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-data-efficiency-c701">Data Efficiency</h2>
<p>Having established resource-efficient adaptation through model techniques, we encounter the second pillar of on-device learning systems engineering: maximizing learning signal from severely constrained data. This represents a fundamental shift from the data-abundant environments assumed by traditional ML systems to the information-scarce reality of edge deployment.</p>
<p>The systems engineering challenge centers on a critical trade-off: data collection cost versus adaptation quality. Edge devices face severe data acquisition constraints that reshape learning system design in ways not encountered in centralized training. Understanding and navigating these constraints requires systematic analysis of four interconnected engineering dimensions.</p>
<p>First, every data point has acquisition costs in terms of user friction, energy consumption, storage overhead, and privacy risk. A voice assistant learning from audio samples must balance improvement potential against battery drain and user comfort with always-on recording. Second, limited data collection capacity forces systems to choose between broad coverage and deep examples. A mobile keyboard can collect many shallow typing patterns or fewer detailed interaction sequences, each strategy implying different learning approaches. Third, some applications demand rapid learning from minimal examples (emergency response scenarios), while others can accumulate data over time (user preference learning). This temporal dimension drives fundamental architectural choices. Fourth, data efficiency techniques must integrate with the model adaptation approaches from <a href="#sec-ondevice-learning-model-adaptation-6a82" class="quarto-xref">Section&nbsp;1.4</a>, federated coordination (<a href="#sec-ondevice-learning-federated-learning-6e7e" class="quarto-xref">Section&nbsp;1.6</a>), and the operational monitoring established in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>.</p>
<p>These engineering constraints create a systematic trade-off space where different data efficiency approaches serve different combinations of constraints. Rather than choosing a single technique, successful on-device learning systems typically combine multiple approaches, each addressing specific aspects of the data scarcity challenge.</p>
<p>This section examines four complementary data efficiency strategies that address different facets of the data scarcity challenge. Few-shot learning enables adaptation from minimal labeled examples, allowing systems to personalize based on just a handful of user-provided samples rather than requiring extensive training datasets. Streaming updates accommodate data that arrives incrementally over time, enabling continuous adaptation as devices encounter new patterns during normal operation without needing to collect and store large batches. Experience replay maximizes learning from limited data through intelligent reuse, replaying important examples multiple times to extract maximum learning signal from scarce training data. Data compression reduces memory requirements while preserving learning signals, enabling systems to maintain replay buffers and training histories within the tight memory constraints of edge devices.</p>
<p>Each technique addresses different aspects of the data constraint problem, enabling robust learning even when traditional supervised learning would fail.</p>
<section id="sec-ondevice-learning-fewshot-learning-data-streaming-566e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-fewshot-learning-data-streaming-566e">Few-Shot Learning and Data Streaming</h3>
<p>In conventional machine learning workflows, effective training typically requires large labeled datasets, carefully curated and preprocessed to ensure sufficient diversity and balance. On-device learning, by contrast, must often proceed from only a handful of local examples‚Äîcollected passively through user interaction or ambient sensing, and rarely labeled in a supervised fashion. These constraints motivate two complementary adaptation strategies: few-shot learning, in which models generalize from a small, static set of examples, and streaming adaptation, where updates occur continuously as data arrives.</p>
<p>Few-shot adaptation is particularly relevant when the device observes a small number of labeled or weakly labeled instances for a new task or user condition <span class="citation" data-cites="wang2020generalizing">(<a href="#ref-wang2020generalizing" role="doc-biblioref">Y. Wang et al. 2020</a>)</span>. In such settings, it is often infeasible to perform full finetuning of all model parameters without overfitting. Instead, methods such as bias-only updates, adapter modules, or prototype-based classification are employed to make use of limited data while minimizing capacity for memorization. Let <span class="math inline">\(D = \{(x_i, y_i)\}_{i=1}^K\)</span> denote a <span class="math inline">\(K\)</span>-shot dataset of labeled examples collected on-device. The goal is to update the model parameters <span class="math inline">\(\theta\)</span> to improve task performance under constraints such as:</p>
<div class="no-row-height column-margin column-container"><div id="ref-wang2020generalizing" class="csl-entry" role="listitem">
Wang, Yaqing, Quanming Yao, James T. Kwok, and Lionel M. Ni. 2020. <span>‚ÄúGeneralizing from a Few Examples: A Survey on Few-Shot Learning.‚Äù</span> <em>ACM Computing Surveys</em> 53 (3): 1‚Äì34. <a href="https://doi.org/10.1145/3386252">https://doi.org/10.1145/3386252</a>.
</div></div><ul>
<li>Limited number of gradient steps: <span class="math inline">\(T \ll 100\)</span></li>
<li>Constrained memory footprint: <span class="math inline">\(\|\theta_{\text{updated}}\| \ll \|\theta\|\)</span></li>
<li>Preservation of prior task knowledge (to avoid catastrophic forgetting)</li>
</ul>
<p>Keyword spotting (KWS) systems offer a concrete example of few-shot adaptation in a real-world, on-device deployment <span class="citation" data-cites="warden2018speech">(<a href="#ref-warden2018speech" role="doc-biblioref">Warden 2018</a>)</span>. These models are used to detect fixed phrases, including phrases like ‚ÄúHey Siri‚Äù<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> or ‚ÄúOK Google‚Äù, with low latency and high reliability. A typical KWS model consists of a pretrained acoustic encoder (e.g., a small convolutional or recurrent network that transforms input audio into an embedding space) followed by a lightweight classifier. In commercial systems, the encoder is trained centrally using thousands of hours of labeled speech across multiple languages and speakers. However, supporting custom wake words (e.g., ‚ÄúHey Jarvis‚Äù) or adapting to underrepresented accents and dialects is often infeasible via centralized training due to data scarcity and privacy concerns.</p>
<div class="no-row-height column-margin column-container"><div id="ref-warden2018speech" class="csl-entry" role="listitem">
Warden, Pete. 2018. <span>‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition.‚Äù</span> <em>arXiv Preprint arXiv:1804.03209</em>, April. <a href="http://arxiv.org/abs/1804.03209v1">http://arxiv.org/abs/1804.03209v1</a>.
</div><div id="fn23"><p><sup>23</sup>&nbsp;<strong>‚ÄúHey Siri‚Äù Technical Reality</strong>: Apple‚Äôs ‚ÄúHey Siri‚Äù system operates under extreme constraints‚Äîdetection must complete within 100&nbsp;ms to feel responsive, while consuming less than 1&nbsp;mW power when listening continuously. The always-on processor monitors audio using a 192&nbsp;KB model running at ~0.5 TOPS. False positive rate must be under 0.001% of audio frames processed (equivalent to &lt;0.1 activations per hour, or less than once per day under typical usage) while maintaining &gt;95% true positive rate across accents, background noise, and speaking styles. The system processes 16&nbsp;kHz audio in 200&nbsp;ms windows, extracting Mel-frequency features for classification.</p></div></div><p>Few-shot adaptation solves this problem by finetuning only the output classifier or a small subset of parameters, including bias terms, using just a few example utterances collected directly on the device. For example, a user might provide 5‚Äì10 recordings of their custom wake word. These samples are then used to update the model locally, while the main encoder remains frozen to preserve generalization and reduce memory overhead. This allows personalization without requiring additional labeled data or transmitting private audio to the cloud.</p>
<p>Such an approach is not only computationally efficient, but also aligned with privacy-preserving design principles. Because only the output layer is updated, often involving a simple gradient step or prototype computation, the total memory footprint and runtime compute are compatible with mobile-class devices or even microcontrollers. This makes KWS a canonical case study for few-shot learning at the edge, where the system must operate under tight constraints while delivering user-specific performance.</p>
<p>Beyond static few-shot learning, many on-device scenarios benefit from streaming adaptation, where models must learn incrementally as new data arrives <span class="citation" data-cites="hayes2020remind">(<a href="#ref-hayes2020remind" role="doc-biblioref">Hayes et al. 2020</a>)</span>. Streaming adaptation generalizes this idea to continuous, asynchronous settings where data arrives incrementally over time. Let <span class="math inline">\(\{x_t\}_{t=1}^{\infty}\)</span> represent a stream of observations. In streaming settings, the model must update itself after observing each new input, typically without access to prior data, and under bounded memory and compute. The model update can be written generically as: <span class="math display">\[
\theta_{t+1} = \theta_t - \eta_t \nabla \mathcal{L}(x_t; \theta_t)
\]</span> where <span class="math inline">\(\eta_t\)</span> is the learning rate at time <span class="math inline">\(t\)</span>. This form of adaptation is sensitive to noise and drift in the input distribution, and thus often incorporates mechanisms such as learning rate decay, meta-learned initialization, or update gating to improve stability.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hayes2020remind" class="csl-entry" role="listitem">
Hayes, Tyler L., Kushal Kafle, Robik Shrestha, Manoj Acharya, and Christopher Kanan. 2020. <span>‚ÄúREMIND Your Neural Network to Prevent Catastrophic Forgetting.‚Äù</span> In <em>Computer Vision ‚Äì ECCV 2020</em>, 466‚Äì83. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-58598-3\_28">https://doi.org/10.1007/978-3-030-58598-3\_28</a>.
</div></div><p>Aside from KWS, practical examples of these strategies abound. In wearable health devices, a model that classifies physical activities may begin with a generic classifier and adapt to user-specific motion patterns using only a few labeled activity segments. In smart assistants, user voice profiles are fine-tuned over time using ongoing speech input, even when explicit supervision is unavailable. In such cases, local feedback, including correction, repetition, or downstream task success, can serve as implicit signals to guide learning.</p>
<p>Few-shot and streaming adaptation highlight the shift from traditional training pipelines to data-efficient, real-time learning under uncertainty. They form a foundation for more advanced memory and replay strategies, which we turn to next.</p>
</section>
<section id="sec-ondevice-learning-experience-replay-737e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-experience-replay-737e">Experience Replay</h3>
<p>Experience replay addresses the challenge of catastrophic forgetting‚Äîwhere learning new tasks causes models to forget previously learned information‚Äîin continuous learning scenarios by maintaining a buffer of representative examples from previous learning episodes. This technique, originally developed for reinforcement learning <span class="citation" data-cites="mnih2015human">(<a href="#ref-mnih2015human" role="doc-biblioref">Mnih et al. 2015</a>)</span>, proves essential in on-device learning where sequential data streams can cause models to overfit to recent examples.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mnih2015human" class="csl-entry" role="listitem">
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. 2015. <span>‚ÄúHuman-Level Control Through Deep Reinforcement Learning.‚Äù</span> <em>Nature</em> 518 (7540): 529‚Äì33. <a href="https://doi.org/10.1038/nature14236">https://doi.org/10.1038/nature14236</a>.
</div><div id="ref-rolnick2019experience" class="csl-entry" role="listitem">
Rolnick, David, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Greg Wayne. 2019. <span>‚ÄúExperience Replay for Continual Learning.‚Äù</span> In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>.
</div></div><p>Unlike server-side replay strategies that rely on large datasets and extensive compute, on-device replay must operate with extremely limited capacity, often with tens or hundreds of samples, and must avoid interfering with user experience <span class="citation" data-cites="rolnick2019experience">(<a href="#ref-rolnick2019experience" role="doc-biblioref">Rolnick et al. 2019</a>)</span>. Buffers may store only compressed features or distilled summaries, and updates must occur opportunistically (e.g., during idle cycles or charging). These system-level constraints reshape how replay is implemented and evaluated in the context of embedded ML.</p>
<p>Let <span class="math inline">\(\mathcal{M}\)</span> represent a memory buffer that retains a fixed-size subset of training examples. At time step <span class="math inline">\(t\)</span>, the model receives a new data point <span class="math inline">\((x_t, y_t)\)</span> and appends it to <span class="math inline">\(\mathcal{M}\)</span>. A replay-based update then samples a batch <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^{k}\)</span> from <span class="math inline">\(\mathcal{M}\)</span> and applies a gradient step: <span class="math display">\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \left[ \frac{1}{k} \sum_{i=1}^{k} \mathcal{L}(x_i, y_i; \theta_t) \right]
\]</span> where <span class="math inline">\(\theta_t\)</span> are the model parameters, <span class="math inline">\(\eta\)</span> is the learning rate, and <span class="math inline">\(\mathcal{L}\)</span> is the loss function. Over time, this replay mechanism allows the model to reinforce prior knowledge while incorporating new information.</p>
<p>A practical on-device implementation might use a ring buffer to store a small set of compressed feature vectors rather than full input examples. The pseudocode as shown in <a href="#lst-replay-buffer" class="quarto-xref">Listing&nbsp;4</a> illustrates a minimal replay buffer designed for constrained environments.</p>
<div id="lst-replay-buffer" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-replay-buffer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: <strong>Replay Buffer</strong>: Implements a circular storage mechanism for efficient memory management in constrained environments. This approach allows models to efficiently retain and sample from recent data points, balancing the need to use historical information while incorporating new insights.
</figcaption>
<div aria-describedby="lst-replay-buffer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Replay Buffer Techniques</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ReplayBuffer:</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, capacity):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.capacity <span class="op">=</span> capacity</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">buffer</span> <span class="op">=</span> []</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> store(<span class="va">self</span>, feature_vec, label):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.<span class="bu">buffer</span>) <span class="op">&lt;</span> <span class="va">self</span>.capacity:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.<span class="bu">buffer</span>.append((feature_vec, label))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.<span class="bu">buffer</span>[<span class="va">self</span>.index] <span class="op">=</span> (feature_vec, label)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.index <span class="op">=</span> (<span class="va">self</span>.index <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.capacity</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, k):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.sample(<span class="va">self</span>.<span class="bu">buffer</span>, <span class="bu">min</span>(k, <span class="bu">len</span>(<span class="va">self</span>.<span class="bu">buffer</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This implementation maintains a fixed-capacity cyclic buffer, storing compressed representations (e.g., last-layer embeddings) and associated labels. Such buffers are useful for replaying adaptation updates without violating memory or energy budgets.</p>
<p>In TinyML applications<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>, experience replay has been applied to problems such as gesture recognition, where devices must continuously improve predictions while observing a small number of events per day. Instead of training directly on the streaming data, the device stores representative feature vectors from recent gestures and uses them to finetune classification boundaries periodically. Similarly, in on-device keyword spotting, replaying past utterances can improve wake-word detection accuracy without the need to transmit audio data off-device.</p>
<div class="no-row-height column-margin column-container"><div id="fn24"><p><sup>24</sup>&nbsp;<strong>TinyML Market Reality</strong>: The TinyML market reached $2.4 billion in 2023 and is projected to grow to $23.3 billion by 2030. Over 100 billion microcontrollers ship annually, but fewer than 1% currently support on-device learning due to memory and power constraints. Successful TinyML deployments typically consume &lt;1&nbsp;mW power, use &lt;256&nbsp;KB memory, and cost under $1 per chip. Applications include predictive maintenance (vibration sensors), health monitoring (heart rate variability), and smart agriculture (soil moisture prediction).</p></div></div><p>While experience replay improves stability in data-sparse or non-stationary environments, it introduces several tradeoffs. Storing raw inputs may breach privacy constraints or exceed storage budgets, especially in vision and audio applications. Replaying from feature vectors reduces memory usage but may limit the richness of gradients for upstream layers. Write cycles to persistent flash memory, which are frequently necessary for long-term storage on embedded devices, can also raise wear-leveling concerns. These constraints require careful co-design of memory usage policies, replay frequency, and feature selection strategies, particularly in continuous deployment scenarios.</p>
</section>
<section id="sec-ondevice-learning-data-compression-8b40" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-data-compression-8b40">Data Compression</h3>
<p>In many on-device learning scenarios, the raw training data may be too large, noisy, or redundant to store and process effectively. This motivates the use of compressed data representations, where the original inputs are transformed into lower-dimensional embeddings or compact encodings that preserve salient information while minimizing memory and compute costs.</p>
<p>Compressed representations serve two complementary goals. First, they reduce the footprint of stored data, allowing devices to maintain longer histories or replay buffers under tight memory budgets <span class="citation" data-cites="sanh2019distilbert">(<a href="#ref-sanh2019distilbert" role="doc-biblioref">Sanh et al. 2019</a>)</span>. Second, they simplify the learning task by projecting raw inputs into more structured feature spaces, often learned via pretraining or meta-learning, in which efficient adaptation is possible with minimal supervision.</p>
<div class="no-row-height column-margin column-container"></div><p>One common approach is to encode data points using a pretrained feature extractor and discard the original high-dimensional input. For example, an image <span class="math inline">\(x_i\)</span> might be passed through a CNN to produce an embedding vector <span class="math inline">\(z_i = f(x_i)\)</span>, where <span class="math inline">\(f(\cdot)\)</span> is a fixed feature encoder. This embedding captures visual structure (e.g., shape, texture, or spatial layout) in a compact representation, usually ranging from 64 to 512 dimensions, suitable for lightweight downstream adaptation.</p>
<p>Mathematically, training can proceed over compressed samples <span class="math inline">\((z_i, y_i)\)</span> using a lightweight decoder or projection head. Let <span class="math inline">\(\theta\)</span> represent the trainable parameters of this decoder model, which is typically a small neural network that maps from compressed representations to output predictions. As each example is presented, the model parameters are updated using gradient descent: <span class="math display">\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}\big(g(z_i; \theta), y_i\big)
\]</span> Here:</p>
<ul>
<li><span class="math inline">\(z_i\)</span> is the compressed representation of the <span class="math inline">\(i\)</span>-th input,</li>
<li><span class="math inline">\(y_i\)</span> is the corresponding label or supervision signal,</li>
<li><span class="math inline">\(g(z_i; \theta)\)</span> is the decoder‚Äôs prediction,</li>
<li><span class="math inline">\(\mathcal{L}\)</span> is the loss function measuring prediction error,</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate, and</li>
<li><span class="math inline">\(\nabla_\theta\)</span> denotes the gradient with respect to the parameters <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>This formulation highlights how only a compact decoder model, which has the parameter set <span class="math inline">\(\theta\)</span>, needs to be trained, making the learning process feasible even when memory and compute are limited.</p>
<p>Advanced approaches extend beyond fixed encoders by learning discrete or sparse dictionaries that represent data using low-rank or sparse coefficient matrices. A dataset of sensor traces can be factorized as <span class="math inline">\(X \approx DC\)</span>, where <span class="math inline">\(D\)</span> is a dictionary of basis patterns and <span class="math inline">\(C\)</span> is a block-sparse coefficient matrix indicating which patterns are active in each example. By updating only a small number of dictionary atoms or coefficients, the model adapts with minimal overhead.</p>
<p>Compressed representations prove useful in privacy-sensitive settings, as they allow raw data to be discarded or obfuscated after encoding. Compression acts as an implicit regularizer, smoothing the learning process and mitigating overfitting when only a few training examples are available.</p>
<p>In practice, these strategies have been applied in domains such as keyword spotting, where raw audio signals are first transformed into Mel-frequency cepstral coefficients (MFCCs)<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>‚Äîa compact, lossy representation of the power spectrum of speech. These MFCC vectors serve as compressed inputs for downstream models, enabling local adaptation using only a few kilobytes of memory.</p>
<div class="no-row-height column-margin column-container"><div id="fn25"><p><sup>25</sup>&nbsp;<strong>Mel-Frequency Cepstral Coefficients (MFCCs)</strong>: Audio features that mimic human auditory perception by applying mel-scale frequency warping (emphasizing lower frequencies where speech information concentrates) followed by cepstral analysis. A typical MFCC extraction converts 16&nbsp;kHz audio windows into 12-13 coefficients, reducing a 320-sample window (20&nbsp;ms) from 640 bytes to ~50 bytes while preserving speech intelligibility. Widely used in speech recognition since the 1980s due to robustness against noise and computational efficiency. Instead of storing raw audio waveforms, which are large and computationally expensive to process, devices store and learn from these compressed feature vectors directly. Similarly, in low-power computer vision systems, embeddings extracted from lightweight CNNs are retained and reused for few-shot learning. These examples illustrate how representation learning and compression serve as foundational tools for scaling on-device learning to memory- and bandwidth-constrained environments.</p></div></div></section>
<section id="sec-ondevice-learning-data-efficiency-strategy-comparison-4f92" class="level3">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-data-efficiency-strategy-comparison-4f92">Data Efficiency Strategy Comparison</h3>
<p>The techniques introduced in this section (few-shot learning, experience replay, and compressed data representations) offer strategies for adapting models on-device when data is scarce or streaming. They operate under different assumptions and constraints, and their effectiveness depends on system-level factors such as memory capacity, data availability, task structure, and privacy requirements.</p>
<p>Few-shot adaptation excels when a small but informative set of labeled examples is available, particularly when personalization or rapid task-specific tuning is required. It minimizes compute and data needs, but its effectiveness depends on the quality of pretrained representations and the alignment between the initial model and the local task.</p>
<p>Experience replay addresses continual adaptation by mitigating forgetting and improving stability, especially in non-stationary environments. It allows reuse of past data, but requires memory to store examples and compute cycles for periodic updates. Replay buffers may also raise privacy or longevity concerns, especially on devices with limited storage or flash write cycles.</p>
<p>Compressed data representations reduce the footprint of learning by transforming raw data into compact feature spaces. This approach supports longer retention of experience and efficient finetuning, particularly when only lightweight heads are trainable. Compression can introduce information loss, and fixed encoders may fail to capture task-relevant variability if they are not well-aligned with deployment conditions. <a href="#tbl-ondevice-techniques" class="quarto-xref">Table&nbsp;4</a> summarizes key tradeoffs:</p>
<div id="tbl-ondevice-techniques" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ondevice-techniques-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: <strong>On-Device Learning Trade-Offs</strong>: Few-shot adaptation balances data efficiency with model personalization by leveraging small labeled datasets, but requires careful consideration of memory and compute constraints for deployment on resource-limited devices. The table summarizes key considerations for selecting appropriate on-device learning techniques based on application requirements and available resources.
</figcaption>
<div aria-describedby="tbl-ondevice-techniques-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 21%">
<col style="width: 27%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Technique</strong></th>
<th style="text-align: left;"><strong>Data Requirements</strong></th>
<th style="text-align: left;"><strong>Memory/Compute Overhead</strong></th>
<th style="text-align: left;"><strong>Use Case Fit</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Few-Shot Adaptation</strong></td>
<td style="text-align: left;">Small labeled set (K-shots)</td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Personalization, quick on-device finetuning</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Experience Replay</strong></td>
<td style="text-align: left;">Streaming data</td>
<td style="text-align: left;">Moderate (buffer &amp; update)</td>
<td style="text-align: left;">Non-stationary data, stability under drift</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Compressed</strong> <strong>Representations</strong></td>
<td style="text-align: left;">Unlabeled or encoded data</td>
<td style="text-align: left;">Low to Moderate</td>
<td style="text-align: left;">Memory-limited devices, privacy-sensitive contexts</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In practice, these methods are not mutually exclusive. Many real-world systems combine them to achieve robust, efficient adaptation. For example, a keyword spotting system may use compressed audio features (e.g., MFCCs), finetune a few parameters from a small support set, and maintain a replay buffer of past embeddings for continual refinement.</p>
<p>Together, these strategies embody the core challenge of on-device learning: achieving reliable model improvement under persistent constraints on data, compute, and memory.</p>
<div id="quiz-question-sec-ondevice-learning-data-efficiency-c701" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following strategies is most suitable for adapting models on-device when only a few labeled examples are available?</p>
<ol type="a">
<li>Experience Replay</li>
<li>Batch Training</li>
<li>Data Compression</li>
<li>Few-Shot Learning</li>
</ol></li>
<li><p>Explain how experience replay can mitigate the issue of catastrophic forgetting in on-device learning systems.</p></li>
<li><p>In on-device learning, data compression is used to reduce the memory footprint by transforming raw data into ____ representations.</p></li>
<li><p>What is a primary trade-off when using compressed data representations in on-device learning?</p>
<ol type="a">
<li>Loss of task-specific variability</li>
<li>Increased data acquisition costs</li>
<li>Higher energy consumption</li>
<li>Reduced model personalization</li>
</ol></li>
<li><p>Consider a scenario where a wearable device must adapt to user-specific motion patterns. How might few-shot learning and experience replay be combined to improve the device‚Äôs performance?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-data-efficiency-c701" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ondevice-learning-federated-learning-6e7e" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-federated-learning-6e7e">Federated Learning</h2>
<p>The individual device techniques examined above‚Äîfrom bias-only updates to sophisticated adapter modules‚Äîcreate powerful personalization capabilities but reveal a fundamental limitation when deployed at scale. While each device can adapt effectively to local conditions, these isolated improvements cannot benefit the broader device population. Valuable insights about model robustness, adaptation strategies, and failure modes remain trapped on individual devices, losing the collective intelligence that makes centralized training effective.</p>
<p>This limitation becomes apparent in scenarios requiring both personalization and population-scale learning. The model adaptation and data efficiency techniques enable individual devices to learn effectively within resource constraints, but they also reveal a fundamental coordination challenge that emerges when sophisticated local learning meets the realities of distributed deployment.</p>
<p>Consider a voice assistant deployed to 10 million homes. Each device adapts locally to its user‚Äôs voice, accent, and vocabulary. Device A learns that ‚Äúdata‚Äù is pronounced /Ààde…™t…ô/, Device B learns /Ààd√¶t…ô/. Device C encounters the rare phrase ‚Äúmachine learning‚Äù frequently (tech household), while Device D never sees it (non-tech household). After six months of local adaptation:</p>
<ul>
<li>Each device excels at its specific user‚Äôs patterns but only its patterns</li>
<li>Rare vocabulary gets learned on some devices, forgotten on others</li>
<li>Local biases accumulate without correction from broader population</li>
<li>Valuable insights discovered on one device benefit no others</li>
</ul>
<p>Individual on-device learning, while powerful, faces fundamental limitations when devices operate in isolation. Each device observes only a narrow slice of the full data distribution, limiting generalization. Device capabilities vary dramatically, creating learning imbalances across the population. Valuable insights learned on one device cannot benefit others, reducing overall system intelligence. Without coordination, models may diverge or degrade over time due to local biases.</p>
<p>Federated learning emerges as the solution to distributed coordination constraints. It enables privacy-preserving collaboration where devices contribute to collective intelligence without sharing raw data. Rather than viewing individual device learning and coordinated learning as separate paradigms, federated learning represents the natural evolution when on-device systems deploy at scale. This approach transforms the constraint of data locality from a limitation into a privacy feature, allowing systems to learn from population-scale data while keeping individual information secure.</p>
<p>The privacy requirements here directly connect to security and privacy principles that become crucial in production deployments. Rather than viewing individual device learning and coordinated learning as separate paradigms, federated learning represents the natural evolution of on-device systems when deployed at scale.</p>
<div id="callout-definition*-1.2" class="callout callout-definition" title="Federated Learning">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Federated Learning</summary><div><strong><em>Federated Learning</em></strong> is a decentralized training approach in which distributed devices collaboratively train a <em>shared model</em> using <em>local data</em> while exchanging only <em>model updates</em>, preserving <em>privacy</em> through data localization.<p></p>
</div></details>
</div>
<p>To better understand the role of federated learning, it is useful to contrast it with other learning paradigms. <a href="#fig-learning-paradigms" class="quarto-xref">Figure&nbsp;6</a> illustrates the distinction between offline learning, on-device learning, and federated learning. In traditional offline learning, all data is collected and processed centrally. The model is trained in the cloud using curated datasets and is then deployed to edge devices without further adaptation. In contrast, on-device learning allows local model adaptation using data generated on the device itself, supporting personalization but in isolation‚Äîwithout sharing insights across users. Federated learning bridges these two extremes by enabling localized training while coordinating updates globally. It retains data privacy by keeping raw data local, yet benefits from distributed model improvements by aggregating updates from many devices.</p>
<div id="fig-learning-paradigms" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-learning-paradigms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="005a806feb88db3c537093715f5a1a98861dcb04.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Federated learning balances data privacy with collective model improvement by coordinating local training across distributed devices, unlike offline learning‚Äôs centralized approach or on-device learning‚Äôs isolated adaptation. This figure contrasts how each paradigm handles data location and model update strategies, revealing the trade-offs between personalization, data security, and global knowledge sharing."><img src="ondevice_learning_files/mediabag/005a806feb88db3c537093715f5a1a98861dcb04.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-learning-paradigms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Federated learning balances data privacy with collective model improvement by coordinating local training across distributed devices, unlike offline learning‚Äôs centralized approach or on-device learning‚Äôs isolated adaptation. This figure contrasts how each paradigm handles data location and model update strategies, revealing the trade-offs between personalization, data security, and global knowledge sharing.
</figcaption>
</figure>
</div>
<p>This section explores the principles and practical considerations of federated learning in the context of mobile and embedded systems. It begins by outlining the canonical FL protocols and their system implications. It then discusses device participation constraints, communication-efficient update mechanisms, and strategies for personalized learning. Throughout, the emphasis remains on how federated methods can extend the reach of on-device learning by enabling distributed model training across diverse and resource-constrained hardware platforms.</p>
<section id="sec-ondevice-learning-privacypreserving-collaborative-learning-ecf9" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-privacypreserving-collaborative-learning-ecf9">Privacy-Preserving Collaborative Learning</h3>
<p>Federated learning (FL) is a decentralized paradigm for training machine learning models across a population of devices without transferring raw data to a central server <span class="citation" data-cites="mcmahan2017communication">(<a href="#ref-mcmahan2017communication" role="doc-biblioref">McMahan et al. 2017</a>)</span>. Unlike traditional centralized training pipelines, which require aggregating all training data in a single location, federated learning distributes the training process itself. Each participating device computes updates based on its local data and contributes to a global model through an aggregation protocol, typically coordinated by a central server. This shift in training architecture aligns closely with the needs of mobile, edge, and embedded systems, where privacy, communication cost, and system heterogeneity impose significant constraints on centralized approaches.</p>
<div class="no-row-height column-margin column-container"></div><p>As demonstrated across the application domains discussed earlier‚Äîfrom Gboard‚Äôs keyboard personalization to wearable health monitoring to voice interfaces‚Äîfederated learning bridges the gap between model improvement and the system-level constraints established throughout this chapter. It enables the personalization, privacy, and connectivity benefits motivating on-device learning while addressing the resource constraints through coordinated but distributed training. However, these benefits introduce new challenges including client variability, communication efficiency, and non-IID data distributions that require specialized protocols and coordination mechanisms.</p>
<p>Building on this foundation, the remainder of this section explores the key techniques and tradeoffs that define federated learning in on-device settings, examining the core learning protocols that govern coordination across devices and investigating strategies for scheduling, communication efficiency, and personalization.</p>
</section>
<section id="sec-ondevice-learning-learning-protocols-139a" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-learning-protocols-139a">Learning Protocols</h3>
<p>Federated learning protocols define the rules and mechanisms by which devices collaborate to train a shared model. These protocols govern how local updates are computed, aggregated, and communicated, as well as how devices participate in the training process. The choice of protocol has significant implications for system performance, communication overhead, and model convergence.</p>
<p>In this section, we outline the core components of federated learning protocols, including local training, aggregation methods, and communication strategies. We also discuss the tradeoffs associated with different approaches and their implications for on-device ML systems.</p>
<section id="sec-ondevice-learning-local-training-e73d" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-local-training-e73d">Local Training</h4>
<p>Local training refers to the process by which individual devices compute model updates based on their local data. This step is critical in federated learning, as it allows devices to adapt the shared model to their specific contexts without transferring raw data. The local training process involves the following steps:</p>
<ol type="1">
<li><strong>Model Initialization</strong>: Each device initializes its local model parameters, often by downloading the latest global model from the server.</li>
<li><strong>Local Data Sampling</strong>: The device samples a subset of its local data for training. This data may be non-IID, meaning that it may not be uniformly distributed across devices.</li>
<li><strong>Local Training</strong>: The device performs a number of training iterations on its local data, updating the model parameters based on the computed gradients.</li>
<li><strong>Model Update</strong>: After local training, the device computes a model update (e.g., the difference between the updated and initial parameters) and prepares to send it to the server.</li>
<li><strong>Communication</strong>: The device transmits the model update to the server, typically using a secure communication channel to protect user privacy.</li>
<li><strong>Model Aggregation</strong>: The server aggregates the updates from multiple devices to produce a new global model, which is then distributed back to the participating devices.</li>
</ol>
<p>This process is repeated iteratively, with devices periodically downloading the latest global model and performing local training. The frequency of these updates can vary based on system constraints, device availability, and communication costs.</p>
</section>
<section id="sec-ondevice-learning-federated-aggregation-protocols-5d37" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-federated-aggregation-protocols-5d37">Federated Aggregation Protocols</h4>
<p>At the heart of federated learning is a coordination mechanism that allows many devices, each having access to only a small, local dataset, to collaboratively train a shared model. This is achieved through a protocol where client devices perform local training and transmit model updates to a central server. The server aggregates these updates to refine a global model, which is then redistributed to clients for the next training round. This cyclical procedure decouples the learning process from centralized data collection, making it well-suited to the mobile and edge environments characterized throughout this chapter where user data is private, bandwidth is constrained, and device participation is sporadic.</p>
<p>The most widely used baseline for this process is Federated Averaging (FedAvg)<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>, which has become a canonical algorithm for federated learning <span class="citation" data-cites="mcmahan2017communication">(<a href="#ref-mcmahan2017communication" role="doc-biblioref">McMahan et al. 2017</a>)</span>. In FedAvg, each device trains its local copy of the model using stochastic gradient descent (SGD) on its private data.</p>
<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;<strong>Federated Averaging (FedAvg)</strong>: Introduced by Google in 2017, FedAvg revolutionized distributed ML by averaging model weights rather than gradients. Each client performs multiple local SGD steps (typically 1-20) before sending weights to the server, reducing communication by 10-100<span class="math inline">\(\times\)</span> compared to distributed SGD. The key insight: local updates contain richer information than single gradients, enabling convergence with far fewer communication rounds. FedAvg powers production systems like Gboard, processing billions of devices. After a fixed number of local steps, each device sends its updated model parameters to the server. The server computes a weighted average of these parameters, which are weighted according to the number of data samples on each device, and updates the global model accordingly. This updated model is then sent back to the devices, completing one round of training.</p></div><div id="ref-mcmahan2017communication" class="csl-entry" role="listitem">
McMahan, H Brendan, Eider Moore, Daniel Ramage, Seth Hampson, et al. 2017. <span>‚ÄúCommunication-Efficient Learning of Deep Networks from Decentralized Data.‚Äù</span> In <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, 1273‚Äì82.
</div></div><p>Formally, let <span class="math inline">\(\mathcal{D}_k\)</span> denote the local dataset on client <span class="math inline">\(k\)</span>, and let <span class="math inline">\(\theta_k^t\)</span> be the parameters of the model on client <span class="math inline">\(k\)</span> at round <span class="math inline">\(t\)</span>. Each client performs <span class="math inline">\(E\)</span> steps of SGD on its local data, yielding an update <span class="math inline">\(\theta_k^{t+1}\)</span>. The central server then aggregates these updates as: <span class="math display">\[
\theta^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} \theta_k^{t+1}
\]</span> where <span class="math inline">\(n_k = |\mathcal{D}_k|\)</span> is the number of samples on device <span class="math inline">\(k\)</span>, <span class="math inline">\(n = \sum_k n_k\)</span> is the total number of samples across participating clients, and <span class="math inline">\(K\)</span> is the number of active devices in the current round.</p>
<p>This cyclical coordination protocol forms the foundation of federated learning, as illustrated in <a href="#fig-federated-averaging-cycle" class="quarto-xref">Figure&nbsp;7</a> that clarifies the core FedAvg process:</p>
<div id="fig-federated-averaging-cycle" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-federated-averaging-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="449666b7514656ab9621b390145b031832e8dd4e.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Federated Averaging Cycle: The four-step coordination protocol that enables distributed training while preserving data privacy. (1) Server distributes global model to participating clients, (2) Clients train locally on their private data using multiple SGD steps, (3) Clients send updated model weights (not raw data) back to the server, (4) Server performs weighted averaging of client updates to create new global model."><img src="ondevice_learning_files/mediabag/449666b7514656ab9621b390145b031832e8dd4e.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-federated-averaging-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Federated Averaging Cycle</strong>: The four-step coordination protocol that enables distributed training while preserving data privacy. (1) Server distributes global model to participating clients, (2) Clients train locally on their private data using multiple SGD steps, (3) Clients send updated model weights (not raw data) back to the server, (4) Server performs weighted averaging of client updates to create new global model.
</figcaption>
</figure>
</div>
<p>This basic structure introduces a number of design choices and tradeoffs. The number of local steps <span class="math inline">\(E\)</span> impacts the balance between computation and communication: larger <span class="math inline">\(E\)</span> reduces communication frequency but risks divergence if local data distributions vary too much. The selection of participating clients affects convergence stability and fairness. In real-world deployments, not all devices are available at all times, and hardware capabilities may differ substantially, requiring robust participation scheduling and failure tolerance.</p>
</section>
<section id="sec-ondevice-learning-client-scheduling-f675" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-client-scheduling-f675">Client Scheduling</h4>
<p>Federated learning operates under the assumption that clients, devices, which hold local data, periodically become available for participation in training rounds. In real-world systems, client availability is intermittent and variable. Devices may be turned off, disconnected from power, lacking network access, or otherwise unable to participate at any given time. As a result, client scheduling plays a central role in the effectiveness and efficiency of distributed learning.</p>
<p>At a baseline level, federated ML systems define eligibility criteria for participation. Devices must meet minimum requirements such as being plugged in, connected to Wi-Fi, and idle, to avoid interfering with user experience or depleting battery resources. These criteria determine which subset of the total population is considered ‚Äúavailable‚Äù for any given training round.</p>
<p>Beyond these operational filters, devices also differ in their hardware capabilities, data availability, and network conditions. Some smartphones contain many recent examples relevant to the current task, while others have outdated or irrelevant data. Network bandwidth and upload speed may vary widely depending on geography and carrier infrastructure. As a result, selecting clients at random can lead to poor coverage of the underlying data distribution and unstable model convergence.</p>
<p>Availability-driven selection introduces participation bias: clients with favorable conditions, including frequent charging, high-end hardware, and consistent connectivity, are more likely to participate repeatedly, while others are systematically underrepresented. This can skew the resulting model toward behaviors and preferences of a privileged subset of the population, raising both fairness and generalization concerns.</p>
<p>The severity of participation bias becomes apparent when examining real deployment statistics. Studies of federated learning deployments show that the most active 10% of devices can contribute to over 50% of training rounds, while the bottom 50% of devices may never participate at all. This creates a feedback loop: models become increasingly optimized for users with high-end devices and stable connectivity, potentially degrading performance for resource-constrained users who need adaptation the most. A keyboard prediction model might become biased toward the typing patterns of users with flagship phones who charge overnight, missing important linguistic variations from users with budget devices or irregular charging patterns.</p>
<p>To address these challenges, systems must balance scheduling efficiency with client diversity. A key approach involves using stratified or quota-based sampling to ensure representative client participation across different groups. Some systems implement ‚Äúfairness budgets‚Äù that track cumulative participation and actively prioritize underrepresented devices when they become available. Others use importance sampling techniques to reweight contributions based on estimated population statistics rather than raw participation rates. For instance, asynchronous buffer-based techniques allow participating clients to contribute model updates independently, without requiring synchronized coordination in every round <span class="citation" data-cites="fedbuff">(<a href="#ref-fedbuff" role="doc-biblioref">Nguyen et al. 2021</a>)</span>. This model has been extended to incorporate staleness awareness <span class="citation" data-cites="fedstale">(<a href="#ref-fedstale" role="doc-biblioref">Rodio and Neglia 2024</a>)</span> and fairness mechanisms <span class="citation" data-cites="fedstaleweight">(<a href="#ref-fedstaleweight" role="doc-biblioref">Ma et al. 2024</a>)</span>, preventing bias from over-active clients who might otherwise dominate the training process.</p>
<div class="no-row-height column-margin column-container"><div id="ref-fedbuff" class="csl-entry" role="listitem">
Nguyen, John, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Michael Rabbat, Mani Malek, and Dzmitry Huba. 2021. <span>‚ÄúFederated Learning with Buffered Asynchronous Aggregation,‚Äù</span> June. <a href="http://arxiv.org/abs/2106.06639v4">http://arxiv.org/abs/2106.06639v4</a>.
</div><div id="ref-fedstale" class="csl-entry" role="listitem">
Rodio, Angelo, and Giovanni Neglia. 2024. <span>‚ÄúFedStale: Leveraging Stale Client Updates in Federated Learning,‚Äù</span> May. <a href="http://arxiv.org/abs/2405.04171v1">http://arxiv.org/abs/2405.04171v1</a>.
</div><div id="ref-fedstaleweight" class="csl-entry" role="listitem">
Ma, Jeffrey, Alan Tu, Yiling Chen, and Vijay Janapa Reddi. 2024. <span>‚ÄúFedStaleWeight: Buffered Asynchronous Federated Learning with Fair Aggregation via Staleness Reweighting,‚Äù</span> June. <a href="http://arxiv.org/abs/2406.02877v1">http://arxiv.org/abs/2406.02877v1</a>.
</div></div><p>To address these challenges, federated ML systems implement adaptive client selection strategies. These include prioritizing clients with underrepresented data types, targeting geographies or demographics that are less frequently sampled, and using historical participation data to enforce fairness constraints. Systems incorporate predictive modeling to anticipate future client availability or success rates, improving training throughput.</p>
<p>Selected clients perform one or more local training steps on their private data and transmit their model updates to a central server. These updates are aggregated to form a new global model. Typically, this aggregation is weighted, where the contributions of each client are scaled, for example, by the number of local examples used during training, before averaging. This ensures that clients with more representative or larger datasets exert proportional influence on the global model.</p>
<p>These scheduling decisions directly impact system performance. They affect convergence rate, model generalization, energy consumption, and overall user experience. Poor scheduling can result in excessive stragglers, overfitting to narrow client segments, or wasted computation. As a result, client scheduling is not merely a logistical concern; it is a core component of system design in federated learning, demanding both algorithmic insight and infrastructure-level coordination.</p>
</section>
<section id="sec-ondevice-learning-bandwidthaware-update-compression-7730" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-bandwidthaware-update-compression-7730">Bandwidth-Aware Update Compression</h4>
<p>One of the principal bottlenecks in federated ML systems is the cost of communication between edge clients and the central server. Transmitting full model weights or gradients after every training round can overwhelm bandwidth and energy budgets, particularly for mobile or embedded devices operating over constrained wireless links<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>. To address this, a range of techniques have been developed to reduce communication overhead while preserving learning efficacy.</p>
<div class="no-row-height column-margin column-container"><div id="fn27"><p><sup>27</sup>&nbsp;<strong>Wireless Communication Reality</strong>: Mobile devices face severe bandwidth and energy constraints for federated learning. LTE uploads average 5-10 Mbps versus 50+ Mbps downloads, creating asymmetric bottlenecks. Transmitting a 50&nbsp;MB model update consumes approximately 100&nbsp;mAh battery (2-3% of typical capacity, varies by radio efficiency and signal strength) and takes 40-80 seconds. WiFi improves throughput but isn‚Äôt always available. Low-power devices using LoRaWAN or NB-IoT face even harsher limits‚ÄîLoRaWAN maxes at 50&nbsp;kbps with 1% duty cycle restrictions, making frequent updates impractical without aggressive compression.</p></div></div><p>These techniques fall into three primary categories: model compression, selective update sharing, and architectural partitioning.</p>
<p>Model compression methods aim to reduce the size of transmitted updates through quantization<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a>, sparsification, or subsampling. Instead of sending full-precision gradients, a client transmits 8-bit quantized updates or communicates only the top-<span class="math inline">\(k\)</span> gradient elements<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> with highest magnitude.</p>
<div class="no-row-height column-margin column-container"><div id="fn28"><p><sup>28</sup>&nbsp;<strong>Gradient Quantization</strong>: Reduces communication by converting FP32 gradients to lower precision (INT8, INT4, or even 1-bit). Advanced techniques like signSGD use only gradient signs, achieving 32<span class="math inline">\(\times\)</span> compression. Error compensation methods accumulate quantization errors for later transmission, maintaining convergence quality. Real deployments achieve 8-16<span class="math inline">\(\times\)</span> communication reduction with &lt;1% accuracy loss.</p></div><div id="fn29"><p><sup>29</sup>&nbsp;<strong>Gradient Sparsification</strong>: Transmits only the largest gradients by magnitude (typically top 1-10%), dramatically reducing communication. Gradient accumulation stores untransmitted gradients locally until they become large enough to send. This technique exploits the observation that most gradients are small and contribute minimally to convergence, achieving 10-100<span class="math inline">\(\times\)</span> compression ratios while maintaining training effectiveness. These techniques reduce transmission size with limited impact on convergence when applied carefully.</p></div></div><p>Selective update sharing further reduces communication by transmitting only subsets of model parameters or updates. In layer-wise selective sharing, clients update only certain layers, typically the final classifier or adapter modules, while keeping the majority of the backbone frozen. This reduces both upload cost and the risk of overfitting shared representations to non-representative client data.</p>
<p>Split models and architectural partitioning divide the model into a shared global component and a private local component. Clients train and maintain their private modules independently while synchronizing only the shared parts with the server. This allows for user-specific personalization with minimal communication and privacy leakage.</p>
<p>All of these approaches operate within the context of a federated aggregation protocol. A standard baseline for aggregation is Federated Averaging (FedAvg), in which the server updates the global model by computing a weighted average of the client updates received in a given round. Let <span class="math inline">\(\mathcal{K}_t\)</span> denote the set of participating clients in round <span class="math inline">\(t\)</span>, and let <span class="math inline">\(\theta_k^t\)</span> represent the locally updated model parameters from client <span class="math inline">\(k\)</span>. The server computes the new global model <span class="math inline">\(\theta^{t+1}\)</span> as: <span class="math display">\[
\theta^{t+1} = \sum_{k \in \mathcal{K}_t} \frac{n_k}{n_{\mathcal{K}_t}} \theta_k^t
\]</span></p>
<p>Here, <span class="math inline">\(n_k\)</span> is the number of local training examples at client <span class="math inline">\(k\)</span>, and <span class="math inline">\(n_{\mathcal{K}_t} = \sum_{k \in \mathcal{K}_t} n_k\)</span> is the total number of training examples across all participating clients. This data-weighted aggregation ensures that clients with more training data exert a proportionally larger influence on the global model, while also accounting for partial participation and heterogeneous data volumes.</p>
<p>However, communication-efficient updates can introduce tradeoffs. Compression may degrade gradient fidelity, selective updates can limit model capacity, and split architectures may complicate coordination. As a result, effective federated learning requires careful balancing of bandwidth constraints, privacy concerns, and convergence dynamics‚Äîa balance that depends heavily on the capabilities and variability of the client population.</p>
</section>
<section id="sec-ondevice-learning-federated-personalization-3c73" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-federated-personalization-3c73">Federated Personalization</h4>
<p>While compression and communication strategies improve scalability, they do not address a important limitation of the global federated learning paradigm‚Äîits inability to capture user-specific variation. In real-world deployments, devices often observe distinct and heterogeneous data distributions. A one-size-fits-all global model may underperform when applied uniformly across diverse users. This motivates the need for personalized federated learning, where local models are adapted to user-specific data without compromising the benefits of global coordination.</p>
<p>Let <span class="math inline">\(\theta_k\)</span> denote the model parameters on client <span class="math inline">\(k\)</span>, and <span class="math inline">\(\theta_{\text{global}}\)</span> the aggregated global model. Traditional FL seeks to minimize a global objective: <span class="math display">\[
\min_\theta \sum_{k=1}^K w_k \mathcal{L}_k(\theta)
\]</span> where <span class="math inline">\(\mathcal{L}_k(\theta)\)</span> is the local loss on client <span class="math inline">\(k\)</span>, and <span class="math inline">\(w_k\)</span> is a weighting factor (e.g., proportional to local dataset size). However, this formulation assumes that a single model <span class="math inline">\(\theta\)</span> can serve all users well. In practice, local loss landscapes <span class="math inline">\(\mathcal{L}_k\)</span> often differ significantly across clients, reflecting non-IID data distributions and varying task requirements.</p>
<p>Personalization modifies this objective to allow each client to maintain its own adapted parameters <span class="math inline">\(\theta_k\)</span>, optimized with respect to both the global model and local data: <span class="math display">\[
\min_{\theta_1, \ldots, \theta_K} \sum_{k=1}^K \left( \mathcal{L}_k(\theta_k) + \lambda \cdot \mathcal{R}(\theta_k, \theta_{\text{global}}) \right)
\]</span></p>
<p>Here, <span class="math inline">\(\mathcal{R}\)</span> is a regularization term that penalizes deviation from the global model, and <span class="math inline">\(\lambda\)</span> controls the strength of this penalty. This formulation allows local models to deviate as needed, while still benefiting from global coordination.</p>
<p>Real-world use cases illustrate the importance of this approach. Consider a wearable health monitor that tracks physiological signals to classify physical activities. While a global model may perform reasonably well across the population, individual users exhibit unique motion patterns, gait signatures, or sensor placements. Personalized finetuning of the final classification layer or low-rank adapters allows improved accuracy, particularly for rare or user-specific classes.</p>
<p>Several personalization strategies have emerged to address the tradeoffs between compute overhead, privacy, and adaptation speed. One widely used approach is local finetuning, in which each client downloads the latest global model and performs a small number of gradient steps using its private data. While this method is simple and preserves privacy, it may yield suboptimal results when the global model is poorly aligned with the client‚Äôs data distribution or when the local dataset is extremely limited.</p>
<p>Another effective technique involves personalization layers, where the model is partitioned into a shared backbone and a lightweight, client-specific head‚Äîtypically the final classification layer <span class="citation" data-cites="arivazhagan2019federated">(<a href="#ref-arivazhagan2019federated" role="doc-biblioref">Arivazhagan et al. 2019</a>)</span>. Only the head is updated on-device, significantly reducing memory usage and training time. This approach is particularly well-suited for scenarios in which the primary variation across clients lies in output categories or decision boundaries.</p>
<div class="no-row-height column-margin column-container"><div id="ref-arivazhagan2019federated" class="csl-entry" role="listitem">
Arivazhagan, Manoj Ghuhan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. 2019. <span>‚ÄúFederated Learning with Personalization Layers.‚Äù</span> <em>CoRR</em> abs/1912.00818 (December). <a href="http://arxiv.org/abs/1912.00818v1">http://arxiv.org/abs/1912.00818v1</a>.
</div></div><p>Clustered federated learning offers an alternative by grouping clients according to similarities in their data or performance characteristics, and training separate models for each cluster. This strategy can enhance accuracy within homogeneous subpopulations but introduces additional system complexity and may require exchanging metadata to determine group membership.</p>
<p>Finally, meta-learning approaches, such as Model-Agnostic Meta-Learning (MAML), aim to produce a global model initialization that can be quickly adapted to new tasks with just a few local updates <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>. This technique is especially useful when clients have limited data or operate in environments with frequent distributional shifts. Each of these strategies reflects a different point in the tradeoff space. These strategies vary in their system implications, including compute overhead, privacy guarantees, and adaptation latency. <a href="#tbl-personalization-strategies" class="quarto-xref">Table&nbsp;5</a> summarizes the tradeoffs.</p>
<div class="no-row-height column-margin column-container"><div id="ref-finn2017model" class="csl-entry" role="listitem">
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. <span>‚ÄúModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.‚Äù</span> In <em>Proceedings of the 34th International Conference on Machine Learning (ICML)</em>.
</div></div><div id="tbl-personalization-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-personalization-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: <strong>Personalization Trade-Offs</strong>: Federated learning strategies balance personalization with system costs, impacting compute overhead, privacy preservation, and adaptation speed for diverse client populations. This table summarizes how local finetuning, clustered learning, and meta-learning each navigate this trade-off space, enabling tailored models while considering practical deployment constraints.
</figcaption>
<div aria-describedby="tbl-personalization-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 24%">
<col style="width: 17%">
<col style="width: 14%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Strategy</strong></th>
<th style="text-align: left;"><strong>Personalization Mechanism</strong></th>
<th style="text-align: left;"><strong>Compute Overhead</strong></th>
<th style="text-align: left;"><strong>Privacy</strong> <strong>Preservation</strong></th>
<th style="text-align: left;"><strong>Adaptation Speed</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Local Finetuning</strong></td>
<td style="text-align: left;">Gradient descent on local loss post-aggregation</td>
<td style="text-align: left;">Low to Moderate</td>
<td style="text-align: left;">High (no data sharing)</td>
<td style="text-align: left;">Fast (few steps)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Personalization Layers</strong></td>
<td style="text-align: left;">Split model: shared base + user-specific head</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Fast (train small head)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Clustered FL</strong></td>
<td style="text-align: left;">Group clients by data similarity, train per group</td>
<td style="text-align: left;">Moderate to High</td>
<td style="text-align: left;">Medium (group metadata)</td>
<td style="text-align: left;">Medium</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Meta-Learning</strong></td>
<td style="text-align: left;">Train for fast adaptation across tasks/devices</td>
<td style="text-align: left;">High (meta-objective)</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Very Fast (few-shot)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Selecting the appropriate personalization method depends on deployment constraints, data characteristics, and the desired balance between accuracy, privacy, and computational efficiency. In practice, hybrid approaches that combine elements of multiple strategies, including local finetuning atop a personalized head, are often employed to achieve robust performance across heterogeneous devices.</p>
</section>
<section id="sec-ondevice-learning-federated-privacy-a1ed" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-federated-privacy-a1ed">Federated Privacy</h4>
<p>While federated learning is often motivated by privacy concerns, as it involves keeping raw data localized instead of transmitting it to a central server, the paradigm introduces its own set of security and privacy risks. Although devices do not share their raw data, the transmitted model updates (such as gradients or weight changes) can inadvertently leak information about the underlying private data. Techniques such as model inversion attacks and membership inference attacks demonstrate that adversaries may partially reconstruct or infer properties of local datasets by analyzing these updates.</p>
<p>To mitigate such risks, modern federated ML systems commonly employ protective measures. Secure Aggregation protocols ensure that individual model updates are encrypted and aggregated in a way that the server only observes the combined result, not any individual client‚Äôs contribution. Differential Privacy<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> techniques inject carefully calibrated noise into updates to mathematically bound the information that can be inferred about any single client‚Äôs data.</p>
<div class="no-row-height column-margin column-container"><div id="fn30"><p><sup>30</sup>&nbsp;<strong>Differential Privacy</strong>: Mathematical framework that provides quantifiable privacy guarantees by adding carefully calibrated noise to computations. In federated learning, DP ensures that individual user data cannot be inferred from model updates, even by aggregators. Key parameter Œµ controls privacy-utility tradeoff: smaller Œµ means stronger privacy but lower model accuracy. Typical deployments use Œµ=1-8, requiring noise addition that can increase communication overhead by 2-10<span class="math inline">\(\times\)</span> and reduce model accuracy by 1-5%. Essential for regulatory compliance and user trust in distributed learning systems.</p></div></div><p>While these techniques enhance privacy, they introduce additional system complexity and tradeoffs between model utility, communication cost, and robustness. A deeper exploration of these attacks, defenses, and their implications requires dedicated coverage of security principles in distributed ML systems.</p>
</section>
</section>
<section id="sec-ondevice-learning-largescale-device-orchestration-1360" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-largescale-device-orchestration-1360">Large-Scale Device Orchestration</h3>
<p>Federated learning transforms machine learning into a massive distributed systems challenge that extends far beyond traditional algorithmic considerations. Coordinating thousands or millions of heterogeneous devices with intermittent connectivity requires sophisticated distributed systems protocols that handle Byzantine failures, network partitions, and communication efficiency at unprecedented scale. These challenges fundamentally differ from the controlled environments of data center distributed training, where high-bandwidth networks and reliable infrastructure enable straightforward coordination protocols.</p>
<section id="sec-ondevice-learning-network-bandwidth-optimization-53da" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-network-bandwidth-optimization-53da">Network and Bandwidth Optimization</h4>
<p>The communication bottleneck represents the primary scalability constraint in federated learning systems. Understanding the quantitative transfer requirements enables principled design decisions about model architectures, update compression strategies, and client participation policies that determine system viability.</p>
<p>The federated communication hierarchy reveals the severe bandwidth constraints under which distributed learning must operate. Full model synchronization requires 10-500&nbsp;MB per training round for typical deep learning models‚Äîprohibitive for mobile networks with limited upload bandwidth that averages just 5-50 Mbps in practice. Gradient compression achieves 10-100<span class="math inline">\(\times\)</span> reduction through quantization (reducing FP32 to INT8), sparsification (transmitting only non-zero gradients), and selective gradient transmission (sending only the most significant updates). Practical deployments demand even more aggressive 100-1000<span class="math inline">\(\times\)</span> compression ratios, reducing 100&nbsp;MB models to manageable 100&nbsp;KB-1&nbsp;MB updates that mobile devices can transmit within reasonable timeframes and without exhausting data plans. Communication frequency introduces a critical trade-off between model update freshness‚Äîmore frequent updates enable faster adaptation to changing conditions‚Äîand network efficiency constraints that limit sustainable bandwidth consumption.</p>
<p>Network infrastructure constraints directly impact participation rates and overall system viability. Modern 4G networks typically provide upload speeds ranging from 5-50 Mbps under optimal conditions (with significant geographic and carrier variation), meaning an 8&nbsp;MB model update requires 1.3-13 seconds of sustained transmission. However, real-world mobile networks exhibit extreme variability: rural areas may experience 1 Mbps upload speeds while urban 5G deployments enable 100+ Mbps. This 100<span class="math inline">\(\times\)</span> variance in network capability necessitates adaptive communication strategies that optimize for lowest-common-denominator connectivity while enabling high-capability devices to contribute more effectively.</p>
<p>The relationship between communication requirements and participation rates exhibits sharp threshold effects. Empirical studies demonstrate that federated learning systems requiring model transfers exceeding 10&nbsp;MB achieve less than 10% sustained client participation, while systems maintaining updates below 1&nbsp;MB can sustain 40-60% participation rates across diverse mobile populations. This communication efficiency directly translates to model quality improvements: higher participation rates provide better statistical diversity and more robust gradient estimates for global model updates.</p>
<p>Advanced compression techniques become essential for practical deployment. Gradient quantization reduces precision from FP32 to INT8 or even binary representations, achieving 4-32<span class="math inline">\(\times\)</span> compression with minimal accuracy loss. Sparsification techniques transmit only the largest gradient components, leveraging the natural sparsity in neural network updates. Top-k gradient selection further reduces communication by transmitting only the most significant parameter updates, while error accumulation ensures that small gradients are not permanently lost.</p>
</section>
<section id="sec-ondevice-learning-asynchronous-device-synchronization-348e" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-asynchronous-device-synchronization-348e">Asynchronous Device Synchronization</h4>
<p>Federated learning operates at the complex intersection of distributed systems and machine learning, inheriting fundamental challenges from both domains while introducing unique complications that arise from the mobile, heterogeneous, and unreliable nature of edge devices.</p>
<p>Federated learning must contend with Byzantine fault tolerance requirements that extend beyond typical distributed systems challenges. Device failures occur frequently as clients crash, lose power, or disconnect during training rounds due to battery depletion or network connectivity issues‚Äîfar more common than server failures in traditional distributed training. Malicious updates present security concerns as adversarial clients can provide corrupted gradients deliberately designed to degrade global model performance or extract private information from the aggregation process. Robust aggregation protocols implementing Byzantine-resilient averaging ensure system reliability despite the presence of compromised or unreliable participants, though these protocols introduce significant computational overhead. Consensus mechanisms must coordinate millions of unreliable participants without the overhead of traditional distributed consensus protocols like Paxos or Raft, which were designed for small clusters of reliable servers.</p>
<p>Network partitions pose particularly acute challenges for federated coordination protocols. Unlike traditional distributed systems operating within reliable data center networks, federated learning must gracefully handle prolonged client disconnection events where devices may remain offline for hours or days while traveling, in poor coverage areas, or simply powered down. Asynchronous coordination protocols enable continued training progress despite missing participants, but must carefully balance staleness (accepting potentially outdated contributions) against freshness (prioritizing recent but potentially sparse updates).</p>
<p>Fault recovery and resilience strategies form an essential layer of federated learning infrastructure. Checkpoint synchronization through periodic global model snapshots enables recovery from server failures and provides rollback points when corrupted training rounds are detected, though checkpointing large models across millions of devices introduces substantial storage and communication overhead. Partial update handling ensures systems gracefully handle incomplete training rounds when significant subsets of clients fail or disconnect mid-training, requiring careful weighting strategies to prevent bias toward more reliable device cohorts. State reconciliation protocols enable clients rejoining after extended offline periods‚Äîpotentially days or weeks‚Äîto efficiently resynchronize with the current global model while minimizing communication overhead that could overwhelm bandwidth-constrained devices. Dynamic load balancing addresses uneven client availability patterns that create computational hotspots, requiring intelligent load redistribution across available participants to maintain training throughput despite time-varying participation rates.</p>
<p>The asynchronous nature of federated coordination introduces additional complexity in maintaining training convergence guarantees. Traditional synchronous training assumes all participants complete each round, but federated systems must handle stragglers and dropouts gracefully. Techniques such as FedAsync<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> enable asynchronous aggregation where the server continuously updates the global model as client updates arrive, while bounded staleness mechanisms prevent extremely outdated updates from corrupting recent progress.</p>
<div class="no-row-height column-margin column-container"><div id="fn31"><p><sup>31</sup>&nbsp;<strong>Asynchronous Federated Learning (FedAsync)</strong>: Enables continuous model updates without waiting for slow or unreliable clients. The server maintains a global model that gets updated immediately when client contributions arrive, using staleness-aware weighting to reduce the influence of outdated updates. This approach can improve convergence speed by 2-5<span class="math inline">\(\times\)</span> in heterogeneous environments while maintaining model quality within 1-3% of synchronous training.</p></div></div></section>
<section id="sec-ondevice-learning-managing-milliondevice-heterogeneity-86c1" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-managing-milliondevice-heterogeneity-86c1">Managing Million-Device Heterogeneity</h4>
<p>Real-world federated learning deployments exhibit extreme heterogeneity across multiple dimensions simultaneously: hardware capabilities, network conditions, data distributions, and availability patterns. This multi-dimensional heterogeneity fundamentally challenges traditional distributed machine learning assumptions about homogeneous participants operating under similar conditions.</p>
<p>Real-world federated learning deployments face multi-dimensional device heterogeneity that creates extreme variation across every system dimension. Computational variation spans 1000<span class="math inline">\(\times\)</span> differences in processing power between flagship smartphones running at 35 TOPS and IoT microcontrollers operating at just 0.03 TOPS, fundamentally limiting what models can train on different device tiers. Memory constraints exhibit even more dramatic 100-10,000<span class="math inline">\(\times\)</span> differences in available RAM across device categories, ranging from 256KB on microcontrollers to 16&nbsp;GB on premium smartphones, determining whether devices can perform any local training at all or must rely purely on inference. Energy limitations force training sessions to be carefully scheduled around charging patterns, thermal constraints, and battery preservation requirements, with mobile devices typically limiting ML workloads to 500-1000&nbsp;mW sustained power consumption. Network diversity introduces orders-of-magnitude performance differences as WiFi, 4G, 5G, and satellite connectivity exhibit vastly different bandwidth (ranging from 1 Mbps to 1 Gbps), latency (10&nbsp;ms to 600&nbsp;ms), and reliability characteristics that determine feasible update frequencies and compression requirements.</p>
<p>Adaptive coordination protocols address this heterogeneity through sophisticated tiered participation strategies that optimize resource utilization across the device spectrum. High-capability devices such as flagship smartphones can perform complex local training with large batch sizes and multiple epochs, while resource-constrained IoT devices contribute through lightweight updates, specialized subtasks, or even simple data aggregation. This creates a natural computational hierarchy where powerful devices act as ‚Äúsuper-peers‚Äù performing disproportionate computation, while edge devices contribute specialized local knowledge and coverage.</p>
<p>The scale challenges extend far beyond device heterogeneity to fundamental coordination overhead limitations. Traditional distributed consensus algorithms such as Raft or PBFT are designed for dozens of nodes in controlled environments, but federated learning requires coordination among millions of participants across unreliable networks. This necessitates hierarchical coordination architectures where regional aggregation servers reduce communication overhead by performing local consensus before contributing to global aggregation. Edge computing infrastructure provides natural hierarchical coordination points, enabling federated learning systems to leverage existing content delivery networks (CDNs) and mobile edge computing (MEC) deployments for efficient gradient aggregation.</p>
<p>Modern federated systems implement sophisticated client selection strategies that balance statistical diversity with practical constraints. Random sampling ensures unbiased representation but may select many low-capability devices, while capability-based selection improves training efficiency but risks statistical bias. Hybrid approaches use stratified sampling across device tiers, ensuring both statistical representativeness and computational efficiency. These selection strategies must also consider temporal patterns: office workers‚Äô devices may be available during specific hours, while IoT sensors provide continuous but limited computational resources.</p>
<div id="quiz-question-sec-ondevice-learning-federated-learning-6e7e" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>What is a primary challenge of federated learning compared to centralized learning?</p>
<ol type="a">
<li>Increased computational power required on the server</li>
<li>Lack of personalization for individual device users</li>
<li>Difficulty in coordinating updates from distributed devices</li>
<li>Higher data storage requirements on individual devices</li>
</ol></li>
<li><p>Explain how federated learning addresses privacy concerns while enabling collective intelligence.</p></li>
<li><p>Order the following steps in the federated learning cycle: (1) Local training on device, (2) Aggregation of model updates, (3) Distribution of global model to devices, (4) Transmission of model updates to server.</p></li>
<li><p>In a production system using federated learning, what trade-off must be considered when choosing the number of local training steps?</p>
<ol type="a">
<li>Balancing communication frequency and model divergence</li>
<li>Balancing model accuracy and computational cost</li>
<li>Balancing data privacy and model size</li>
<li>Balancing server load and energy consumption</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-federated-learning-6e7e" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-ondevice-learning-production-integration-beb5" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-production-integration-beb5">Production Integration</h2>
<p>The theoretical foundation established earlier‚Äîmodel adaptation strategies, data efficiency techniques, and federated coordination algorithms‚Äîprovides the building blocks for on-device learning systems. However, translating these individual components into production-ready systems requires addressing integration challenges that cut across all constraint dimensions simultaneously.</p>
<p>Real-world deployment introduces systemic complexity that exceeds the sum of individual techniques. Model adaptation, data efficiency, and federated coordination must work together seamlessly rather than as independent optimizations. Different learning strategies have varying computational and memory profiles that must be coordinated within overall device budgets. Training, inference, and communication must be scheduled carefully to avoid interference with user experience and system stability. Unlike centralized systems with observable training loops, on-device learning requires distributed validation and failure detection mechanisms that operate across heterogeneous device populations.</p>
<p>This transition from theory to practice requires systematic engineering approaches that balance competing constraints while maintaining system reliability. Successful on-device learning deployments depend not on individual algorithmic improvements but on holistic system designs that orchestrate multiple techniques within operational constraints. The subsequent sections examine how production systems address these integration challenges through principled design patterns, operational practices, and monitoring strategies that enable scalable, reliable on-device learning deployment.</p>
<section id="sec-ondevice-learning-mlops-integration-challenges-bb4e" class="level3">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-mlops-integration-challenges-bb4e">MLOps Integration Challenges</h3>
<p>Integrating on-device learning into existing MLOps workflows requires extending the operational frameworks established in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong> to handle distributed training, heterogeneous devices, and privacy-preserving coordination. The continuous integration pipelines, model versioning systems, and monitoring infrastructure discussed in the preceding chapter provide essential foundations, but must be adapted to address unique edge deployment challenges. Standard MLOps pipelines assume centralized data access, controlled deployment environments, and unified monitoring capabilities that do not directly apply to edge learning scenarios, requiring new approaches to the technical debt management and operational excellence principles established earlier.</p>
<section id="sec-ondevice-learning-deployment-pipeline-transformations-431d" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-deployment-pipeline-transformations-431d">Deployment Pipeline Transformations</h4>
<p>Traditional MLOps deployment pipelines from <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong> follow a standardized CI/CD process: model training, validation, staging, and production deployment of a single model artifact to uniform infrastructure. On-device learning requires device-aware deployment pipelines that distribute different adaptation strategies across heterogeneous device tiers. Microcontrollers receive bias-only updates, mid-range phones use LoRA adapters, and flagship devices perform selective layer updates. The deployment artifact evolves from a static model file to a collection of adaptation policies, initial model weights, and device-specific optimization configurations.</p>
<p>This architectural shift necessitates extending traditional deployment pipelines with device capability detection, strategy selection logic, and tiered deployment orchestration that maintains the reliability guarantees of conventional MLOps while accommodating unprecedented deployment diversity.</p>
<p>This transformation introduces new complexity in version management. While centralized systems maintain a single model version, on-device learning systems must simultaneously track multiple versioning dimensions. The pre-trained backbone distributed to all devices represents the base model version, which serves as the foundation for all local adaptations. Different update mechanisms deployed per device class constitute adaptation strategies, varying from simple bias adjustments on microcontrollers to full layer fine-tuning on flagship devices. Local model states naturally diverge from the base as devices encounter unique data distributions, creating device-specific checkpoints that reflect individual adaptation histories. Finally, federated learning rounds that periodically synchronize device populations establish aggregation epochs, marking discrete points where distributed knowledge converges into updated global models. Successful deployments implement tiered versioning schemes where base models evolve slowly‚Äîtypically through monthly updates‚Äîwhile local adaptations occur continuously, creating a hierarchical version space rather than the linear version history familiar from traditional deployments.</p>
</section>
<section id="sec-ondevice-learning-monitoring-system-evolution-94c9" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-monitoring-system-evolution-94c9">Monitoring System Evolution</h4>
<p><strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong> established monitoring practices that aggregate metrics from centralized inference servers. On-device learning monitoring must operate within fundamentally different constraints that reshape how systems observe, measure, and respond to model behavior across distributed device populations.</p>
<p>Privacy-preserving telemetry represents the first fundamental departure from traditional monitoring. Collecting performance metrics without compromising user privacy requires federated analytics where devices share only aggregate statistics or differentially private summaries. Systems cannot simply log individual predictions or training samples as centralized systems do. Instead, devices report distribution summaries such as mean accuracy and confidence histograms rather than per-example metrics. All reported statistics must include differential privacy guarantees that bound information leakage through carefully calibrated noise addition. Secure aggregation protocols prevent the server from observing individual device contributions, ensuring that even the aggregation process itself cannot reconstruct private information from any single device‚Äôs data.</p>
<p>Drift detection presents additional challenges without access to ground truth labels. Traditional monitoring compares model predictions against labeled validation sets maintained on centralized infrastructure. On-device systems must detect drift using only local signals available during deployment. Confidence calibration tracks whether predicted probabilities match empirical frequencies, detecting degradation when the model‚Äôs confidence estimates become poorly calibrated to actual outcomes. Input distribution monitoring detects when feature distributions shift from training data through statistical techniques that require no labels. Task performance proxies leverage implicit feedback such as user corrections or task abandonment as quality signals that indicate when the model fails to meet user needs. Shadow baseline comparison runs a frozen base model alongside the adapted model to measure divergence, flagging cases where local adaptation degrades rather than improves performance relative to the known-good baseline.</p>
<p>Heterogeneous performance tracking addresses a third critical challenge: global averages mask critical failures when device populations exhibit high variance. Monitoring systems must segment performance across multiple dimensions to identify systematic issues that affect specific device cohorts. Capability-based performance gaps reveal when flagship devices achieve substantially better results than budget devices, indicating that adaptation strategies may need adjustment for resource-constrained hardware. Regional bias issues surface when models perform well in some geographic markets but poorly in others, potentially reflecting data distribution shifts or cultural factors not captured during initial training. Temporal patterns emerge when performance degrades for devices running stale base models that have not received recent updates from federated aggregation. Participation inequality becomes visible when comparing devices that adapt frequently against those that rarely participate in training, revealing potential fairness issues in how learning benefits are distributed across the user population.</p>
</section>
<section id="sec-ondevice-learning-continuous-training-orchestration-8974" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-continuous-training-orchestration-8974">Continuous Training Orchestration</h4>
<p>Traditional continuous training covered in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong> executes scheduled retraining jobs on centralized infrastructure with predictable resource availability and coordinated execution. On-device learning transforms this into continuous distributed training where millions of devices train independently without global synchronization, creating orchestration challenges that require fundamentally different coordination strategies.</p>
<p>Asynchronous device coordination represents the first major departure from centralized training. Millions of devices train independently on their local data, but the orchestration system cannot rely on synchronized participation. Only 20-40% of devices are typically available in any training round due to network connectivity limitations, battery constraints, and varying usage patterns. The system must exhibit straggler tolerance, ensuring that slow devices on limited hardware or poor network connections cannot block faster devices from progressing with their local adaptations. Devices often operate on different base model versions simultaneously, creating version skew that the aggregation protocol must handle gracefully without forcing all devices to maintain identical model states. State reconciliation becomes necessary when devices reconnect after extended offline periods‚Äîpotentially days or weeks‚Äîrequiring the system to integrate their accumulated local adaptations despite having missed multiple federated aggregation rounds.</p>
<p>Resource-aware scheduling ensures that training respects both device constraints and user experience. Orchestration policies implement opportunistic training windows that execute adaptation only when the device is idle, charging, and connected to WiFi, avoiding interference with active user tasks or consuming metered cellular data. Thermal budgets suspend training when device temperature exceeds manufacturer-specified thresholds, preventing user discomfort and hardware damage from sustained computational loads. Battery preservation policies limit training energy consumption to less than 5% of battery capacity per day, ensuring that on-device learning does not noticeably impact device runtime from the user‚Äôs perspective. Network-aware communication compresses model updates aggressively when devices must use metered connections, trading computational overhead for reduced bandwidth consumption to minimize user data charges.</p>
<p>Convergence assessment without global visibility poses the final orchestration challenge. Traditional training monitors loss curves on centralized validation sets, providing clear signals about training progress and convergence. Distributed training must assess convergence through indirect signals aggregated across the device population. Federated evaluation aggregates validation metrics from devices that maintain local held-out sets, providing approximate measures of global model quality despite incomplete device participation. Update magnitude tracking monitors how much local gradients change the global model in each aggregation round, with diminishing update sizes signaling potential convergence. Participation diversity ensures broad device representation in aggregated updates, preventing convergence metrics from reflecting only a narrow subset of the deployment environment. Temporal consistency detects when model improvements plateau across multiple aggregation rounds, indicating that the current adaptation strategy has exhausted its potential gains and may require adjustment.</p>
</section>
<section id="sec-ondevice-learning-validation-strategy-adaptation-ab3d" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-validation-strategy-adaptation-ab3d">Validation Strategy Adaptation</h4>
<p>The validation approaches from <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong> assume access to held-out test sets and centralized evaluation infrastructure where model quality can be measured directly against known ground truth. On-device learning requires distributed validation that respects privacy and resource constraints while still providing reliable quality signals across heterogeneous device populations.</p>
<p>Shadow model evaluation provides the primary validation mechanism by maintaining multiple model variants on each device and comparing their behavior. Devices simultaneously run a baseline shadow model‚Äîa frozen copy of the last known-good base model that provides a stable reference point‚Äîalongside the current locally-adapted version that reflects recent on-device training. Many systems also maintain the latest federated aggregation result as a global model variant, enabling comparison between individual device adaptations and the collective knowledge aggregated from the entire device population. By comparing predictions across these variants on incoming data streams, systems detect when local adaptation degrades performance relative to established baselines. This comparison occurs continuously during normal operation, requiring no additional labeled validation data. When the adapted model consistently underperforms the baseline shadow, the system triggers automatic rollback to the known-good version, preventing performance degradation from persisting in production.</p>
<p>Confidence-based quality gates provide an additional validation signal when labeled validation data is unavailable. Without ground truth labels, systems use prediction confidence as a quality proxy that correlates with model performance. Well-calibrated models should exhibit high confidence on in-distribution samples that resemble their training data, with confidence scores that accurately reflect the probability of correct predictions. Confidence drops indicate either distributional shift‚Äîwhere input data no longer matches training distributions‚Äîor model degradation from problematic local adaptations. Threshold-based gating implements this validation mechanism by continuously monitoring average prediction confidence and suspending adaptation when confidence falls below baseline levels established during initial deployment. This approach catches many failure modes without requiring labeled validation data, though it cannot detect all performance issues since overconfident but incorrect predictions can maintain high confidence scores.</p>
<p>Federated A/B testing enables validation of new adaptation strategies or model architectures across distributed device populations. To validate proposed changes, systems implement distributed experiments that randomly assign devices to treatment and control groups while maintaining statistical balance across device tiers and usage patterns. Both groups collect federated metrics using privacy-preserving aggregation protocols that prevent individual device data from being exposed while enabling population-level comparisons. The system compares adaptation success rates‚Äîmeasuring how frequently local adaptations improve over baseline models‚Äîalong with convergence speed that indicates how quickly devices reach optimal performance, and final performance metrics that reflect ultimate model quality after adaptation completes. Successful strategies demonstrating clear improvements in treatment groups are rolled out gradually across the device population, starting with small percentages and expanding only after confirming that benefits generalize beyond the experimental cohort.</p>
<p>These operational transformations necessitate new tooling and infrastructure that systematically extends traditional MLOps practices from <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>. The CI/CD pipelines, monitoring dashboards, A/B testing frameworks, and incident response procedures established for centralized deployments form the foundation for on-device learning operations. The federated learning protocols (<a href="#sec-ondevice-learning-federated-learning-6e7e" class="quarto-xref">Section&nbsp;1.6</a>) provide coordination mechanisms for distributed training, while monitoring challenges (<a href="#sec-ondevice-learning-distributed-system-observability-2270" class="quarto-xref">Section&nbsp;1.9.3</a>) address observability gaps created by decentralized adaptation.</p>
<p>Successful on-device learning deployments build upon proven MLOps methodologies while adapting them to the unique challenges of distributed, heterogeneous learning environments. This evolutionary approach ensures operational reliability while enabling the benefits of edge learning.</p>
</section>
</section>
<section id="sec-ondevice-learning-bioinspired-learning-efficiency-55ad" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-bioinspired-learning-efficiency-55ad">Bio-Inspired Learning Efficiency</h3>
<p>The constraints of on-device learning mirror fundamental challenges solved by biological intelligence systems, offering theoretical insights into efficient learning design. Understanding these connections enables principled approaches to resource-constrained machine learning that leverage billions of years of evolutionary optimization.</p>
<section id="sec-ondevice-learning-learning-biological-neural-efficiency-74b5" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-learning-biological-neural-efficiency-74b5">Learning from Biological Neural Efficiency</h4>
<p>The human brain operates at approximately 20 watts while continuously learning from limited supervision‚Äîprecisely the efficiency target for on-device learning systems<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a>. This remarkable efficiency emerges from several architectural principles that directly inform edge learning design, demonstrating what is theoretically achievable with highly optimized learning systems.</p>
<div class="no-row-height column-margin column-container"><div id="fn32"><p><sup>32</sup>&nbsp;<strong>Brain Power Efficiency</strong>: The human brain‚Äôs 20&nbsp;W power consumption (equivalent to a bright LED bulb) enables processing 10^15 operations per second‚Äîroughly 50,000<span class="math inline">\(\times\)</span> more efficient than current AI accelerators per operation. This efficiency comes from ~86 billion neurons with only 1-2% active simultaneously, massive parallelism with 10^14 synapses, and adaptive precision where important computations use more resources. Modern edge AI targets similar efficiency: sparse activation patterns, adaptive precision (INT8 to FP16), and event-driven processing that activates only when needed.</p></div></div><p>The brain‚Äôs efficiency characteristics reveal multiple dimensions of optimization that on-device systems should target. From a power perspective, the brain consumes just 20&nbsp;W total, with approximately 10&nbsp;W dedicated to active learning and memory consolidation‚Äîan energy budget comparable to what mobile devices can sustainably allocate to on-device learning during charging periods. Memory efficiency comes from sparse, distributed representations where only 1-2% of neurons activate simultaneously during any cognitive task, dramatically reducing the computational and storage requirements compared to dense neural networks. Learning efficiency manifests through few-shot learning capabilities that enable adaptation from single exposures, along with continuous adaptation mechanisms that avoid catastrophic forgetting when integrating new knowledge. Hierarchical processing organizes information across multiple scales, from low-level sensory inputs to high-level abstract reasoning, enabling efficient reuse of learned features across different tasks and contexts.</p>
<p>Biological learning exhibits several features that on-device systems must replicate to achieve similar efficiency. Sparse representations ensure efficient use of limited neural resources‚Äîonly a tiny fraction of brain neurons fire during any cognitive task. This sparsity directly parallels the selective parameter updates and pruned architectures essential for mobile deployment. Event-driven processing minimizes energy consumption by activating computation only when sensory input changes, analogous to opportunistic training during device idle periods.</p>
</section>
<section id="sec-ondevice-learning-unlabeled-data-exploitation-strategies-cded" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-unlabeled-data-exploitation-strategies-cded">Unlabeled Data Exploitation Strategies</h4>
<p>Mobile devices continuously collect rich sensor streams ideal for self-supervised learning: visual data from cameras, temporal patterns from accelerometers, spatial patterns from GPS, and interaction patterns from touchscreen usage. This abundant unlabeled data enables sophisticated representation learning without external supervision.</p>
<p>The scale of sensor data generation on mobile devices creates unprecedented opportunities for self-supervised learning. Visual streams from cameras operating at 30 frames per second provide approximately 2.6 million frames daily, offering abundant data for contrastive learning approaches that learn visual representations by comparing augmented versions of the same image<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a>. Motion data from accelerometers sampling at 100&nbsp;Hz generates 8.6 million data points daily, capturing temporal patterns suitable for learning representations of human activities and device movement. Location traces from GPS sensors enable spatial representation learning and behavioral prediction by capturing movement patterns and frequently visited locations without requiring explicit labels. Interaction patterns from touch events, typing dynamics, and app usage sequences create rich behavioral embeddings that reveal user preferences and habits, enabling personalized model adaptation without manual annotation.</p>
<div class="no-row-height column-margin column-container"><div id="fn33"><p><sup>33</sup>&nbsp;<strong>Mobile Data Generation Scale</strong>: A typical smartphone generates ~2-4&nbsp;GB of sensor data daily from cameras (1-2&nbsp;GB), accelerometers (~50&nbsp;MB), GPS traces (~10&nbsp;MB), and touch interactions (~5&nbsp;MB). This massive data stream offers unprecedented self-supervised learning opportunities‚Äîmodern contrastive learning can extract useful representations from just 1% of this data, making effective on-device learning feasible without external labels or cloud processing.</p></div></div><p>Contrastive learning from temporal correlations offers particularly promising opportunities for leveraging this sensor data. Consecutive frames from mobile cameras naturally provide positive pairs for visual representation learning‚Äîimages captured milliseconds apart typically show the same scene from slightly different perspectives‚Äîwhile augmentation techniques such as color jittering and random cropping create negative examples. Audio streams from microphones enable self-supervised speech representation learning through masking and prediction tasks, where the model learns to predict masked portions of audio spectrograms. Even device orientation and motion data can be used for self-supervised pretraining of activity recognition models, learning representations that capture the temporal structure of human movement without requiring labeled activity annotations.</p>
<p>The biological inspiration extends to continual learning without forgetting. Brains continuously integrate new experiences while retaining decades of memories through mechanisms like synaptic consolidation and replay. On-device systems must implement analogous mechanisms: elastic weight consolidation prevents catastrophic forgetting by protecting weights important for previous tasks, experience replay maintains stability during adaptation by interleaving new training with replayed examples from previous tasks, and progressive neural architectures expand model capacity as new tasks emerge rather than forcing all knowledge into fixed-capacity networks.</p>
</section>
<section id="sec-ondevice-learning-lifelong-adaptation-without-forgetting-9200" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-lifelong-adaptation-without-forgetting-9200">Lifelong Adaptation Without Forgetting</h4>
<p>Real-world on-device deployment demands continual adaptation to changing environments, user behavior, and task requirements. This presents the fundamental challenge of the stability-plasticity tradeoff: models must remain stable enough to preserve existing knowledge while plastic enough to learn new patterns.</p>
<p>Continual learning on edge devices faces several interconnected challenges that compound the difficulty of distributed adaptation. Catastrophic forgetting occurs when new learning overwrites previously acquired knowledge, causing models to lose performance on earlier tasks as they adapt to new ones‚Äîa particularly severe problem when devices cannot access historical training data. Task interference emerges when multiple learning objectives compete for limited model capacity, forcing difficult tradeoffs between different capabilities that the model must maintain simultaneously. Data distribution shift manifests as deployment environments differ significantly from training conditions, requiring models to adapt to new patterns while maintaining performance on the original distribution. Resource constraints fundamentally limit the available solutions, as limited memory prevents storing all historical data for replay-based approaches that work well in centralized settings but exceed edge device capabilities.</p>
<p>Meta-learning approaches address these challenges by learning learning algorithms themselves rather than just learning specific tasks. Model-Agnostic Meta-Learning (MAML) trains models to quickly adapt to new tasks with minimal data‚Äîexactly the capability required for personalized on-device adaptation where collecting large user-specific datasets is impractical. Few-shot learning techniques enable rapid specialization from small user-specific datasets, allowing models to personalize based on just a handful of examples while maintaining general capabilities learned during pretraining.</p>
<p>The theoretical foundation suggests that optimal on-device learning systems will combine sparse representations, self-supervised pretraining on sensor data, and meta-learning for rapid adaptation. These principles directly influence practical system design: sparse model architectures reduce memory and compute requirements, self-supervised objectives utilize abundant unlabeled sensor data, and meta-learning enables efficient personalization from limited user interactions.</p>
<p>A key principle in building practical systems is to minimize the adaptation footprint. Full-model fine-tuning is typically infeasible on edge platforms, instead, localized update strategies, including bias-only optimization, residual adapters, and lightweight task-specific heads, should be prioritized. These approaches allow model specialization under resource constraints while mitigating the risks of overfitting or instability.</p>
<p>The feasibility of lightweight adaptation depends importantly on the strength of offline pretraining <span class="citation" data-cites="bommasani2021opportunities">(<a href="#ref-bommasani2021opportunities" role="doc-biblioref">Bommasani et al. 2021</a>)</span>. Pretrained models should encapsulate generalizable feature representations that allow efficient adaptation from limited local data. Shifting the burden of feature extraction to centralized training reduces the complexity and energy cost of on-device updates, while improving convergence stability in data-sparse environments.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bommasani2021opportunities" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2021. <span>‚ÄúOn the Opportunities and Risks of Foundation Models.‚Äù</span> <em>arXiv Preprint arXiv:2108.07258</em>, August. <a href="http://arxiv.org/abs/2108.07258v3">http://arxiv.org/abs/2108.07258v3</a>.
</div></div><p>Even when adaptation is lightweight, opportunistic scheduling remains important to preserve system responsiveness and user experience. Local updates should be deferred to periods when the device is idle, connected to external power, and operating on a reliable network. Such policies minimize the impact of background training on latency, battery consumption, and thermal performance.</p>
<p>The sensitivity of local training artifacts necessitates careful data security measures. Replay buffers, support sets, adaptation logs, and model update metadata must be protected against unauthorized access or tampering. Lightweight encryption or hardware-backed secure storage can mitigate these risks without imposing prohibitive resource costs on edge platforms.</p>
<p>However, security measures alone do not guarantee model robustness. As models adapt locally, monitoring adaptation dynamics becomes important. Lightweight validation techniques, including confidence scoring, drift detection heuristics, and shadow model evaluation, can help identify divergence early, enabling systems to trigger rollback mechanisms before severe degradation occurs <span class="citation" data-cites="gama2014survey">(<a href="#ref-gama2014survey" role="doc-biblioref">Gama et al. 2014</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gama2014survey" class="csl-entry" role="listitem">
Gama, Jo√£o, Indrƒó ≈Ωliobaitƒó, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. 2014. <span>‚ÄúA Survey on Concept Drift Adaptation.‚Äù</span> <em>ACM Computing Surveys</em> 46 (4): 1‚Äì37. <a href="https://doi.org/10.1145/2523813">https://doi.org/10.1145/2523813</a>.
</div></div><p>Robust rollback procedures depend on retaining trusted model checkpoints. Every deployment should preserve a known-good baseline version of the model that can be restored if adaptation leads to unacceptable behavior. This principle is especially important in safety-important and regulated domains, where failure recovery must be provable and rapid.</p>
<p>In decentralized or federated learning contexts, communication efficiency becomes a first-order design constraint. Compression techniques such as quantized gradient updates, sparsified parameter sets, and selective model transmission must be employed to allow scalable coordination across large, heterogeneous fleets of devices without overwhelming bandwidth or energy budgets <span class="citation" data-cites="konevcny2016federated">(<a href="#ref-konevcny2016federated" role="doc-biblioref">Koneƒçn√Ω et al. 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-konevcny2016federated" class="csl-entry" role="listitem">
Koneƒçn√Ω, Jakub, H. Brendan McMahan, Daniel Ramage, and Peter Richt√°rik. 2016. <span>‚ÄúFederated Optimization: Distributed Machine Learning for on-Device Intelligence.‚Äù</span> <em>CoRR</em> abs/1610.02527 (October). <a href="http://arxiv.org/abs/1610.02527v1">http://arxiv.org/abs/1610.02527v1</a>.
</div></div><p>When personalization is required, systems should aim for localized adaptation wherever possible. Restricting updates to lightweight components, including final classification heads or modular adapters, constrains the risk of catastrophic forgetting, reduces memory overhead, and accelerates adaptation without destabilizing core model representations.</p>
<p>Finally, throughout the system lifecycle, privacy and compliance requirements must be architected into adaptation pipelines. Mechanisms to support user consent, data minimization, retention limits, and the right to erasure must be considered core aspects of model design, not post-hoc adjustments. Meeting regulatory obligations at scale demands that on-device learning workflows align inherently with principles of auditable autonomy.</p>
<p>The flowchart in <a href="#fig-odl-design-flow" class="quarto-xref">Figure&nbsp;8</a> summarizes key decision points in designing practical, scalable, and resilient on-device ML systems.</p>
<div id="fig-odl-design-flow" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-odl-design-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="9ffea3341c84d553792847a1715a8d5b06738ff7.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: This flowchart guides the systematic development of practical on-device ML systems by outlining key decision points related to data management, model selection, and privacy considerations throughout the system lifecycle. Integrating privacy and compliance requirements‚Äîsuch as user consent and data minimization‚Äîinto the design process ensures auditable autonomy and scalable deployment of on-device intelligence."><img src="ondevice_learning_files/mediabag/9ffea3341c84d553792847a1715a8d5b06738ff7.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-odl-design-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: This flowchart guides the systematic development of practical on-device ML systems by outlining key decision points related to data management, model selection, and privacy considerations throughout the system lifecycle. Integrating privacy and compliance requirements‚Äîsuch as user consent and data minimization‚Äîinto the design process ensures auditable autonomy and scalable deployment of on-device intelligence.
</figcaption>
</figure>
</div>
<div id="quiz-question-sec-ondevice-learning-production-integration-beb5" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes a key challenge in deploying on-device learning systems compared to centralized systems?</p>
<ol type="a">
<li>Centralized data access</li>
<li>Uniform monitoring capabilities</li>
<li>Device-aware deployment pipelines</li>
<li>Single model version management</li>
</ol></li>
<li><p>Explain how privacy-preserving telemetry differs from traditional monitoring in on-device learning systems.</p></li>
<li><p>Order the following steps in an on-device learning deployment pipeline: (1) Device capability detection, (2) Strategy selection logic, (3) Tiered deployment orchestration.</p></li>
<li><p>Discuss the trade-offs involved in using federated learning for on-device systems, focusing on resource constraints and privacy.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-production-integration-beb5" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-ondevice-learning-systems-integration-production-deployment-c6bb" class="level2">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-systems-integration-production-deployment-c6bb">Systems Integration for Production Deployment</h2>
<p>Real-world on-device learning systems achieve effectiveness by systematically combining all three solution pillars rather than relying on isolated techniques. This integration requires careful systems engineering to manage interactions, resolve conflicts, and optimize the overall system performance within deployment constraints.</p>
<p>Consider a production voice assistant deployment across 50 million heterogeneous devices. The system architecture demonstrates systematic integration across three complementary layers that work together to enable effective learning under diverse constraints.</p>
<p>The model adaptation layer stratifies techniques by device capability, matching sophistication to available resources. Flagship phones representing the top 20% of the deployment use LoRA rank-32 adapters that enable sophisticated voice pattern learning through high-dimensional parameter updates. Mid-tier devices comprising 60% of the fleet employ rank-16 adapters that balance adaptation expressiveness with the tighter memory constraints typical of mainstream smartphones. Budget devices making up the remaining 20% rely on bias-only updates that stay comfortably within 1&nbsp;GB memory limits while still enabling basic personalization.</p>
<p>The data efficiency layer implements adaptive strategies across the entire device population while respecting individual resource constraints. All devices implement experience replay, but with device-appropriate buffer sizes‚Äî10&nbsp;MB on budget devices versus 100&nbsp;MB on flagship models‚Äîensuring that memory-constrained devices can still benefit from replay-based learning. Few-shot learning enables rapid adaptation to new users within their first 10 interactions, reducing the cold-start problem that plagues systems requiring extensive training data. Streaming updates accommodate continuous voice pattern evolution as users‚Äô speaking styles naturally change over time or as they use the assistant in new acoustic environments.</p>
<p>The federated coordination layer orchestrates privacy-preserving collaboration across the device population. Devices participate in federated training rounds opportunistically based on connectivity status and battery level, ensuring that coordination does not degrade user experience. LoRA adapters aggregate efficiently with just 50&nbsp;MB per update compared to 14&nbsp;GB for full model synchronization, making federated learning practical over mobile networks. Privacy-preserving aggregation protocols ensure that individual voice patterns never leave devices while still enabling population-scale improvements in accent recognition and language understanding that benefit all users.</p>
<p>Effective systems integration requires adherence to key engineering principles that ensure robust operation across heterogeneous device populations:</p>
<ol type="1">
<li><p><strong>Hierarchical Capability Matching</strong>: Deploy more sophisticated techniques on capable devices while ensuring basic functionality across the device spectrum. Never assume uniform capabilities.</p></li>
<li><p><strong>Graceful Degradation</strong>: Systems must operate effectively when individual components fail. Poor connectivity should not prevent local adaptation; low battery should trigger minimal adaptation modes.</p></li>
<li><p><strong>Conflict Resolution</strong>: Model adaptation and data efficiency techniques can conflict (limited memory vs buffer size). Systematic resource allocation prevents these conflicts through predefined priority hierarchies.</p></li>
<li><p><strong>Performance Validation</strong>: Integration creates emergent behaviors that individual techniques don‚Äôt exhibit. Systems require comprehensive testing across device combinations and network conditions.</p></li>
</ol>
<p>This integrated approach transforms on-device learning from a collection of techniques into a coherent systems capability that provides robust personalization within real-world deployment constraints.</p>
<div id="quiz-question-sec-ondevice-learning-systems-integration-production-deployment-c6bb" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Which layer in the adaptive systems integration is responsible for ensuring privacy-preserving collaboration across devices?</p>
<ol type="a">
<li>Model adaptation layer</li>
<li>Data efficiency layer</li>
<li>Hierarchical capability matching</li>
<li>Federated coordination layer</li>
</ol></li>
<li><p>Explain the concept of hierarchical capability matching in the context of adaptive systems integration.</p></li>
<li><p>Order the following steps in the adaptive systems integration process: (1) Implement experience replay, (2) Deploy LoRA adapters, (3) Ensure privacy-preserving aggregation.</p></li>
<li><p>What is a key trade-off when implementing streaming updates in on-device learning systems?</p>
<ol type="a">
<li>Higher computational cost versus improved privacy</li>
<li>Increased memory usage versus faster adaptation</li>
<li>Larger buffer sizes versus reduced personalization</li>
<li>More frequent updates versus network congestion</li>
</ol></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-systems-integration-production-deployment-c6bb" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
<section id="sec-ondevice-learning-persistent-technical-operational-challenges-8c12" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-persistent-technical-operational-challenges-8c12">Persistent Technical and Operational Challenges</h2>
<p>The solution techniques explored above‚Äîmodel adaptation, data efficiency, and federated coordination‚Äîaddress many fundamental constraints of on-device learning but also reveal persistent challenges that emerge from their interaction in real-world deployments. These challenges represent the current frontiers of on-device learning research and highlight areas where the techniques discussed earlier reach their limits or create new operational complexities. Understanding these challenges provides critical context for evaluating when on-device learning approaches are appropriate and where alternative strategies may be necessary.</p>
<p>Unlike conventional centralized systems, where training occurs in controlled environments with uniform hardware and curated datasets, edge systems must contend with heterogeneity in devices, fragmentation in data, and the absence of centralized validation infrastructure. These factors give rise to new systems-level tradeoffs that test the boundaries of the adaptation strategies, data efficiency methods, and coordination mechanisms we have examined.</p>
<section id="sec-ondevice-learning-device-data-heterogeneity-management-a789" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-device-data-heterogeneity-management-a789">Device and Data Heterogeneity Management</h3>
<p>Federated and on-device ML systems must operate across a vast and diverse ecosystem of devices, ranging from smartphones and wearables to IoT sensors and microcontrollers. This heterogeneity spans multiple dimensions: hardware capabilities, software stacks, network connectivity, and power availability. Unlike cloud-based systems, where environments can be standardized and controlled, edge deployments encounter a wide distribution of system configurations and constraints. These variations introduce significant complexity in algorithm design, resource scheduling, and model deployment.</p>
<p>At the hardware level, devices differ in terms of memory capacity, processor architecture (e.g., ARM Cortex-M vs.&nbsp;A-series)<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a>, instruction set support (e.g., availability of SIMD or floating-point units), and the presence or absence of AI accelerators. Some clients may possess powerful NPUs capable of running small training loops, while others may rely solely on low-frequency CPUs with minimal RAM. These differences affect the feasible size of models, the choice of training algorithm, and the frequency of updates.</p>
<div class="no-row-height column-margin column-container"><div id="fn34"><p><sup>34</sup>&nbsp;<strong>ARM Cortex Architecture Spectrum</strong>: The ARM Cortex family spans 6 orders of magnitude in capabilities. Cortex-M0+ (IoT sensors) runs at 48&nbsp;MHz with 32&nbsp;KB RAM and no floating-point, consuming ~10¬µW. Cortex-M7 (embedded systems) reaches 400&nbsp;MHz with 1&nbsp;MB RAM and single-precision FPU, consuming ~100&nbsp;mW. Cortex-A78 (smartphones) delivers 3&nbsp;GHz performance with multi-core processing, NEON SIMD, and advanced branch prediction, consuming 1-5&nbsp;W. This diversity means federated learning must adapt algorithms dynamically‚Äîquantized inference on M0+, lightweight training on M7, and full backpropagation on A78.</p></div><div id="fn35"><p><sup>35</sup>&nbsp;<strong>TensorFlow Lite</strong>: Google‚Äôs framework for mobile and embedded ML inference, optimized for ARM processors and mobile GPUs. TFLite reduces model size by 75% through quantization and pruning, while achieving <span class="math inline">\(3\times\)</span> faster inference than full TensorFlow. The framework supports 16-bit and 8-bit quantization, with specialized kernels for mobile CPUs and GPUs. TFLite Micro targets microcontrollers with &lt;1&nbsp;MB memory, enabling ML on Arduino and other embedded platforms.</p></div></div><p>Software heterogeneity compounds the challenge. Devices may run different versions of operating systems, kernel-level drivers, and runtime libraries. Some environments support optimized ML runtimes like TensorFlow Lite<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> Micro or ONNX Runtime Mobile, while others rely on custom inference stacks or restricted APIs. These discrepancies can lead to subtle inconsistencies in behavior, especially when models are compiled differently or when floating-point precision varies across platforms.</p>
<p>In addition to computational heterogeneity, devices exhibit variation in connectivity and uptime. Some are intermittently connected, plugged in only occasionally, or operate under strict bandwidth constraints. Others may have continuous power and reliable networking, but still prioritize user-facing responsiveness over background learning. These differences complicate the orchestration of coordinated learning and the scheduling of updates.</p>
<p>Finally, system fragmentation affects reproducibility and testing. With such a wide range of execution environments, it is difficult to ensure consistent model behavior or to debug failures reliably. This makes monitoring, validation, and rollback mechanisms more important‚Äîbut also more difficult to implement uniformly across the fleet.</p>
<p>Consider a federated learning deployment for mobile keyboards. A high-end smartphone might feature 8 GB of RAM, a dedicated AI accelerator, and continuous Wi-Fi access. In contrast, a budget device may have just 2 GB of RAM, no hardware acceleration, and rely on intermittent mobile data. These disparities influence how long training runs can proceed, how frequently models can be updated, and even whether training is feasible at all. To support such a range, the system must dynamically adjust training schedules, model formats, and compression strategies‚Äîensuring equitable model improvement across users while respecting each device‚Äôs limitations.</p>
</section>
<section id="sec-ondevice-learning-noniid-data-distribution-challenges-42cd" class="level3">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-noniid-data-distribution-challenges-42cd">Non-IID Data Distribution Challenges</h3>
<p>In centralized machine learning, data can be aggregated, shuffled, and curated to approximate independent and identically distributed (IID) samples‚Äîa key assumption underlying many learning algorithms. On-device and federated learning systems fundamentally challenge this assumption, requiring algorithms that can handle highly fragmented and non-IID data across diverse devices and contexts.</p>
<p>The statistical implications of this fragmentation create cascading challenges throughout the learning process. Gradients computed on different devices may conflict, slowing convergence or destabilizing training. Local updates risk overfitting to individual client idiosyncrasies, reducing performance when aggregated globally. The diversity of data across clients also complicates evaluation, as no single test set can represent the true deployment distribution.</p>
<p>These challenges necessitate robust algorithms that can handle heterogeneity and imbalanced participation. Techniques such as personalization layers, importance weighting, and adaptive aggregation schemes provide partial solutions, but the optimal approach varies with application context and the specific nature of data fragmentation. As established in <a href="#sec-ondevice-learning-data-constraints-303e" class="quarto-xref">Section&nbsp;1.3.3</a>, this statistical heterogeneity represents one of the core challenges distinguishing on-device learning from traditional centralized approaches.</p>
</section>
<section id="sec-ondevice-learning-distributed-system-observability-2270" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-distributed-system-observability-2270">Distributed System Observability</h3>
<p>The monitoring and observability frameworks from <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong> must be fundamentally reimagined for distributed edge environments. Traditional centralized monitoring approaches that rely on unified data collection and real-time visibility become impractical when devices operate intermittently connected and data cannot be centralized. The drift detection and performance monitoring techniques established in MLOps provide conceptual foundations, but require adaptation to handle the distributed, privacy-preserving nature of on-device learning systems.</p>
<p>Unlike centralized machine learning systems, where model updates can be continuously evaluated against held-out validation sets, on-device learning introduces a core shift in visibility and observability. Once deployed, models operate in highly diverse and often disconnected environments, where internal updates may proceed without external monitoring. This creates significant challenges for ensuring that model adaptation is both beneficial and safe.</p>
<p>A core difficulty lies in the absence of centralized validation data. In traditional workflows, models are trained and evaluated using curated datasets that serve as proxies for deployment conditions. On-device learners, by contrast, adapt in response to local inputs, which are rarely labeled and may not be systematically collected. As a result, the quality and direction of updates, whether they enhance generalization or cause drift, are difficult to assess without interfering with the user experience or violating privacy constraints.</p>
<p>The risk of model drift is especially pronounced in streaming settings, where continual adaptation may cause a slow degradation in performance. For instance, a voice recognition model that adapts too aggressively to background noise may eventually overfit to transient acoustic conditions, reducing accuracy on the target task. Without visibility into the evolution of model parameters or outputs, such degradations can remain undetected until they become severe.</p>
<p>Mitigating this problem requires mechanisms for on-device validation and update gating. One approach is to interleave adaptation steps with lightweight performance checks‚Äîusing proxy objectives or self-supervised signals to approximate model confidence <span class="citation" data-cites="deng2021adaptive">(<a href="#ref-deng2021adaptive" role="doc-biblioref">Y. Deng, Mokhtari, and Ozdaglar 2021</a>)</span>. For example, a keyword spotting system might track detection confidence across recent utterances and suspend updates if confidence consistently drops below a threshold. Alternatively, shadow evaluation can be employed, where multiple model variants are maintained on the device and evaluated in parallel on incoming data streams, allowing the system to compare the adapted model‚Äôs behavior against a stable baseline.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deng2021adaptive" class="csl-entry" role="listitem">
Deng, Yuzhe, Aryan Mokhtari, and Asuman Ozdaglar. 2021. <span>‚ÄúAdaptive Federated Optimization.‚Äù</span> In <em>Proceedings of the 38th International Conference on Machine Learning (ICML)</em>.
</div></div><p>Another strategy involves periodic checkpointing and rollback, where snapshots of the model state are saved before adaptation. If subsequent performance degrades, as determined by downstream metrics or user feedback, the system can revert to a known good state. This approach has been used in health monitoring devices, where incorrect predictions could lead to user distrust or safety concerns. However, it introduces storage and compute overhead, especially in memory-constrained environments.</p>
<p>In some cases, federated validation offers a partial solution. Devices can share anonymized model updates or summary statistics with a central server, which aggregates them across users to identify global patterns of drift or failure. While this preserves some degree of privacy, it introduces communication overhead and may not capture rare or user-specific failures.</p>
<p>Ultimately, update monitoring and validation in on-device learning require a rethinking of traditional evaluation practices. Instead of centralized test sets, systems must rely on implicit signals, runtime feedback, and conservative adaptation policies to ensure robustness. The absence of global observability is not merely a technical limitation‚Äîit reflects a deeper systems challenge in aligning local adaptation with global reliability.</p>
<section id="sec-ondevice-learning-performance-evaluation-dynamic-environments-82a9" class="level4">
<h4 class="anchored" data-anchor-id="sec-ondevice-learning-performance-evaluation-dynamic-environments-82a9">Performance Evaluation in Dynamic Environments</h4>
<p><strong><a href="../benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 12: Benchmarking AI</a></strong> established systematic approaches for measuring ML system performance: inference latency, throughput, energy efficiency, and accuracy metrics. These benchmarking methodologies provide foundations for characterizing model performance, but they were designed for static inference workloads. On-device learning requires extending these metrics to capture adaptation quality and training efficiency through training-specific benchmarks.</p>
<p>Beyond the inference metrics from <strong><a href="../benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 12: Benchmarking AI</a></strong>, adaptive systems require specialized training metrics that capture learning efficiency under edge constraints. Adaptation efficiency measures accuracy improvement per training sample consumed, quantified as the slope of the learning curve under resource constraints‚Äîa system achieving 2% accuracy gain per 100 training samples demonstrates higher adaptation efficiency than one requiring 500 samples for the same improvement, directly translating to faster personalization and reduced data collection requirements. Memory-constrained convergence evaluates the validation loss achieved within specified RAM budgets, such as ‚Äúconvergence within 512&nbsp;KB training footprint,‚Äù capturing how effectively systems learn given fixed memory allocations‚Äîcritical for comparing adaptation strategies across device classes from microcontrollers to smartphones. Energy-per-update quantifies millijoules consumed per gradient update, a metric critical for battery-powered devices where training energy directly impacts user experience‚Äîmobile devices typically budget 500-1000&nbsp;mW for sustained ML workloads, translating to just 1.8-3.6 joules per hour of adaptation before noticeably affecting battery life. Time-to-adaptation measures wall-clock time from receiving new data to achieving measurable improvement, accounting for opportunistic scheduling constraints that defer training to idle periods‚Äîthis metric captures real-world adaptation speed including waiting for device idleness, charging status, and thermal headroom rather than just raw computational throughput.</p>
<p>Evaluating whether local adaptation actually improves over global models requires personalization gain metrics that justify the overhead of on-device learning. Per-user performance delta measures accuracy improvement for the adapted model versus the global baseline on user-specific holdout data‚Äîsystems should demonstrate statistically significant improvements, typically exceeding 2% accuracy gains, to justify the computational overhead, energy consumption, and complexity that adaptation introduces. Personalization-privacy tradeoff quantifies accuracy gain per unit of local data exposure, measuring the value extracted from privacy-sensitive information‚Äîthis metric helps assess whether adaptation benefits outweigh the privacy costs of retaining user data locally, particularly important for applications handling sensitive information like health data or personal communications. Catastrophic forgetting rate measures degradation on the original task as the model adapts to local distributions through retention testing‚Äîacceptable forgetting rates depend on the application domain but typically should remain below 5% accuracy loss on original tasks to ensure that personalization does not come at the expense of the model‚Äôs general capabilities.</p>
<p>When devices coordinate through federated learning (<a href="#sec-ondevice-learning-federated-learning-6e7e" class="quarto-xref">Section&nbsp;1.6</a>), federated coordination cost metrics become critical for assessing system viability. Communication efficiency measures model accuracy improvement per byte transmitted, capturing the effectiveness of gradient compression and selective update strategies‚Äîmodern federated systems achieve 10-100<span class="math inline">\(\times\)</span> compression through quantization and sparsification techniques while maintaining 95% or more of uncompressed accuracy, making the difference between practical and impractical mobile deployment. Stragglers impact quantifies convergence delay caused by slow or unreliable devices, measured as the difference in convergence time with versus without participation filters‚Äîeffective straggler mitigation through asynchronous aggregation and selective participation reduces convergence time by 30-50% compared to synchronous approaches that wait for all devices. Aggregation quality evaluates global model performance as a function of device participation rate, revealing minimum viable participation thresholds below which federated learning fails to converge effectively‚Äîmost federated systems require 10-20% device participation per round to maintain stable convergence, establishing clear requirements for client selection and availability management strategies.</p>
<p>These training-specific benchmarks complement the inference metrics from <strong><a href="../benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 12: Benchmarking AI</a></strong>, creating complete performance characterization for adaptive systems. Practical benchmarking must measure both dimensions: a system that achieves fast inference but slow adaptation, or efficient adaptation but poor final accuracy, fails to meet real-world requirements. The integration of inference and training benchmarks enables holistic evaluation of on-device learning systems across their full operational lifecycle.</p>
</section>
</section>
<section id="sec-ondevice-learning-resource-management-691a" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-resource-management-691a">Resource Management</h3>
<p>On-device learning introduces resource contention modes absent in conventional inference-only deployments. Many edge devices are provisioned to run pretrained models efficiently but are rarely designed with training workloads in mind. Local adaptation therefore competes for scarce resources, including compute cycles, memory bandwidth, energy, and thermal headroom, with other system processes and user-facing applications.</p>
<p>The most direct constraint is compute availability. Training involves additional forward and backward passes through the model, which can exceed the cost of inference. Even when only a small subset of parameters is updated, for instance, in bias-only or head-only adaptation, backpropagation must still traverse the relevant layers, triggering increased instruction counts and memory traffic. On devices with shared compute units (e.g., mobile SoCs or embedded CPUs), this demand can delay interactive tasks, reduce frame rates, or impair sensor processing.</p>
<p>Energy consumption compounds this problem. Adaptation typically involves sustained computation over multiple input samples, which taxes battery-powered systems and may lead to rapid energy depletion. For instance, performing a single epoch of adaptation on a microcontroller-class device can consume several millijoules<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a>‚Äîan appreciable fraction of the energy budget for a duty-cycled system operating on harvested power. This necessitates careful scheduling, such that learning occurs only during idle periods, when energy reserves are high and user latency constraints are relaxed.</p>
<div class="no-row-height column-margin column-container"><div id="fn36"><p><sup>36</sup>&nbsp;<strong>Microcontroller Power Budget Reality</strong>: A typical microcontroller consuming 10&nbsp;mW during training exhausts 3.6 joules per hour, equivalent to a 1000&nbsp;mAh battery in 2.8 hours. Energy harvesting systems collect only 10-100&nbsp;mW continuously (solar panels in indoor light), making sustained training impossible. Real deployments use duty cycling: train for 10 seconds every hour, consuming ~1 joule total. This constrains training to 100-1000 gradient steps maximum, requiring extremely efficient algorithms and careful energy budgeting between sensing, computation, and communication.</p></div><div id="fn37"><p><sup>37</sup>&nbsp;<strong>Activation Caching</strong>: During backpropagation, forward pass activations must be stored to compute gradients, dramatically increasing memory usage. For a typical CNN, activation memory can be 3-5<span class="math inline">\(\times\)</span> larger than model weights. Modern techniques like gradient checkpointing trade computation for memory by recomputing activations during backward pass, reducing memory by 80% at the cost of ~30% more compute time. Critical for training on memory-constrained devices where activation storage often exceeds available RAM. These requirements may exceed the static memory footprint anticipated during model deployment, particularly when adaptation involves multiple layers or gradient accumulation. In highly constrained systems, for example, systems with less than 512 KB of RAM, this may preclude certain types of adaptation altogether, unless additional optimization techniques (e.g., checkpointing or low-rank updates) are employed.</p></div><div id="ref-lin2020mcunet" class="csl-entry" role="listitem">
Lin, Ji, Wei-Ming Chen, Yujun Lin, and Song Han. 2020. <span>‚ÄúMCUNet: Tiny Deep Learning on IoT Devices.‚Äù</span> In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>.
</div></div><p>From a memory perspective, training incurs higher peak usage than inference, due to the need to cache intermediate activations<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a>, gradients, and optimizer state <span class="citation" data-cites="lin2020mcunet">(<a href="#ref-lin2020mcunet" role="doc-biblioref">Lin et al. 2020</a>)</span>.</p>
<p>These resource demands must also be balanced against quality of service (QoS) goals. Users expect edge devices to respond reliably and consistently, regardless of whether learning is occurring in the background. Any observable degradation, including dropped audio in a wake-word detector or lag in a wearable display, can erode user trust. These system reliability concerns parallel the operational challenges discussed in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>. As such, many systems adopt opportunistic learning policies, where adaptation is suspended during foreground activity and resumed only when system load is low.</p>
<p>In some deployments, adaptation is further gated by cost constraints imposed by networked infrastructure. For instance, devices may offload portions of the learning workload to nearby gateways or cloudlets, introducing bandwidth and communication trade-offs. These hybrid models raise additional questions of task placement and scheduling: should the update occur locally, or be deferred until a high-throughput link is available?</p>
<p>In summary, the cost of on-device learning is not solely measured in FLOPs or memory usage. It manifests as a complex interplay of system load, user experience, energy availability, and infrastructure capacity. Addressing these challenges requires co-design across algorithmic, runtime, and hardware layers, ensuring that adaptation remains unobtrusive, efficient, and sustainable under real-world constraints.</p>
</section>
<section id="sec-ondevice-learning-identifying-preventing-system-failures-ee88" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-identifying-preventing-system-failures-ee88">Identifying and Preventing System Failures</h3>
<p>Understanding potential failure modes in on-device learning helps prevent costly deployment mistakes. Based on documented challenges in federated learning research <span class="citation" data-cites="kairouz2021advances">(<a href="#ref-kairouz2021advances" role="doc-biblioref">Kairouz et al. 2021</a>)</span> and known risks in adaptive systems, several categories of failures warrant careful consideration.</p>
<div class="no-row-height column-margin column-container"></div><p>The most fundamental risk in on-device learning is unbounded adaptation drift, where continuous learning without constraints causes models to gradually diverge from their intended behavior. Consider a hypothetical keyboard prediction system that learns from all user inputs including corrections‚Äîit might begin incorporating typos as valid suggestions, leading to progressively degraded predictions. This risk becomes acute in health monitoring applications where gradual changes in user baselines could be learned as ‚Äúnormal,‚Äù potentially causing the system to miss important anomalies that would have been detected by a static model. The insidious nature of this drift is that it occurs slowly and locally, making detection difficult without proper monitoring infrastructure.</p>
<p>Beyond individual device drift, federated learning systems face the challenge of participation bias amplification at the population level. Devices with reliable power and connectivity participate more frequently in federated rounds <span class="citation" data-cites="li2020federated">(<a href="#ref-li2020federated" role="doc-biblioref">Li et al. 2020</a>)</span>. This uneven participation creates scenarios where models become increasingly optimized for users with high-end devices while performance degrades for those with limited resources. The resulting feedback loop exacerbates digital inequality: better-served users receive increasingly better models, while underserved populations experience declining performance, reducing their engagement and further diminishing their representation in training rounds <span class="citation" data-cites="wang2021field">(<a href="#ref-wang2021field" role="doc-biblioref">J. Wang et al. 2021</a>)</span>. These fairness and bias amplification concerns highlight the ethical implications of distributed learning systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wang2021field" class="csl-entry" role="listitem">
Wang, Jianyu, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Aguera y Arcas, Maruan Al-Shedivat, et al. 2021. <span>‚ÄúA Field Guide to Federated Optimization,‚Äù</span> July. <a href="http://arxiv.org/abs/2107.06917v1">http://arxiv.org/abs/2107.06917v1</a>.
</div></div><p>These systematic biases interact with data quality issues to create autocorrection feedback loops, particularly in text-based applications. When systems cannot distinguish between intended inputs and corrections, they may develop unexpected behaviors. Frequently corrected domain-specific terminology might be incorrectly learned as errors, leading to inappropriate suggestions in professional contexts. This problem compounds the drift issue: not only do models adapt to individual quirks, but they may also learn from their own mistakes when users accept autocorrections without realizing the system is learning from these interactions.</p>
<p>The interconnected nature of these failure modes, from individual drift to population bias to data quality degradation, underscores the importance of implementing comprehensive safety mechanisms. Successful deployments require bounded adaptation ranges to prevent unbounded drift, stratified sampling to address participation bias, careful data filtering to avoid learning from corrections as ground truth, and shadow evaluation against static baselines to detect degradation. While specific production incidents are rarely publicized due to competitive and privacy concerns, the research community has identified these patterns as critical areas requiring systematic mitigation strategies <span class="citation" data-cites="li2020federated kairouz2021advances">(<a href="#ref-li2020federated" role="doc-biblioref">Li et al. 2020</a>; <a href="#ref-kairouz2021advances" role="doc-biblioref">Kairouz et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2020federated" class="csl-entry" role="listitem">
Li, Tian, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020. <span>‚ÄúFederated Learning: Challenges, Methods, and Future Directions.‚Äù</span> <em>IEEE Signal Processing Magazine</em> 37 (3): 50‚Äì60. <a href="https://doi.org/10.1109/msp.2020.2975749">https://doi.org/10.1109/msp.2020.2975749</a>.
</div><div id="ref-kairouz2021advances" class="csl-entry" role="listitem">
Kairouz, Peter, H. Brendan McMahan, Brendan Avent, Aur√©lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, et al. 2021. <span>‚ÄúAdvances and Open Problems in Federated Learning.‚Äù</span> <em>Foundations and Trends in Machine Learning</em> 14 (1‚Äì2): 1‚Äì210. <a href="https://doi.org/10.1561/2200000083">https://doi.org/10.1561/2200000083</a>.
</div></div></section>
<section id="sec-ondevice-learning-production-deployment-risk-assessment-db49" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-production-deployment-risk-assessment-db49">Production Deployment Risk Assessment</h3>
<p>The deployment of adaptive models on edge devices introduces challenges that extend beyond technical feasibility. In domains where compliance, auditability, and regulatory approval are necessary, including healthcare, finance, and safety-important systems, on-device learning poses a core tension between system autonomy and control.</p>
<p>In traditional machine learning pipelines, all model updates are centrally managed, versioned, and validated. The training data, model checkpoints, and evaluation metrics are typically recorded in reproducible workflows that support traceability. When learning occurs on the device itself, however, this visibility is lost. Each device may independently evolve its model parameters, influenced by unique local data streams that are never observed by the developer or system maintainer.</p>
<p>This autonomy creates a validation gap. Without access to the input data or the exact update trajectory, it becomes difficult to verify that the learned model still adheres to its original specification or performance guarantees. This is especially problematic in regulated industries, where certification depends on demonstrating that a system behaves consistently across defined operational boundaries. A device that updates itself in response to real-world usage may drift outside those bounds, triggering compliance violations without any external signal.</p>
<p>The lack of centralized oversight complicates rollback and failure recovery. If a model update degrades performance, it may not be immediately detectable, particularly in offline scenarios or systems without telemetry. By the time failure is observed, the system‚Äôs internal state may have diverged significantly from any known checkpoint, making diagnosis and recovery more complex than in static deployments. This necessitates robust safety mechanisms, such as conservative update thresholds, rollback caches, or dual-model architectures that retain a verified baseline.</p>
<p>In addition to compliance challenges, on-device learning introduces new security vulnerabilities. Because model adaptation occurs locally and relies on device-specific, potentially untrusted data streams, adversaries may attempt to manipulate the learning process by tampering with stored data, such as replay buffers, or by injecting poisoned examples during adaptation, to degrade model performance or introduce vulnerabilities. Any locally stored adaptation data, such as feature embeddings or few-shot examples, must be secured against unauthorized access to prevent unintended information leakage.</p>
<p>Maintaining model integrity over time is particularly difficult in decentralized settings, where central monitoring and validation are limited. Autonomous updates could, without external visibility, cause models to drift into unsafe or biased states. These risks are compounded by compliance obligations such as the GDPR‚Äôs right to erasure: if user data subtly influences a model through adaptation, tracking and reversing that influence becomes complex.</p>
<p>The security and integrity of self-adapting models, particularly at the edge, pose important open challenges. A comprehensive treatment of these threats and corresponding mitigation strategies requires specialized security frameworks for distributed ML systems.</p>
<p>Privacy regulations also interact with on-device learning in nontrivial ways. While local adaptation can reduce the need to transmit sensitive data, it may still require storage and processing of personal information, including sensor traces or behavioral logs, on the device itself. These privacy considerations require careful attention to security frameworks and regulatory compliance. Depending on jurisdiction, this may invoke additional requirements for data retention, user consent, and auditability. Systems must be designed to satisfy these requirements without compromising adaptation effectiveness, which often involves encrypting stored data, enforcing retention limits, or implementing user-controlled reset mechanisms.</p>
<p>Lastly, the emergence of edge learning raises open questions about accountability and liability <span class="citation" data-cites="brakerski2022federated">(<a href="#ref-brakerski2022federated" role="doc-biblioref">Brakerski et al. 2022</a>)</span>. When a model adapts autonomously, who is responsible for its behavior? If an adapted model makes a faulty decision, such as misdiagnosing a health condition or misinterpreting a voice command, the root cause may lie in local data drift, poor initialization, or insufficient safeguards. Without standardized mechanisms for capturing and analyzing these failure modes, responsibility may be difficult to assign, and regulatory approval harder to obtain.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brakerski2022federated" class="csl-entry" role="listitem">
Brakerski, Zvika et al. 2022. <span>‚ÄúFederated Learning and the Rise of Edge Intelligence: Challenges and Opportunities.‚Äù</span> <em>Communications of the ACM</em> 65 (8): 54‚Äì63.
</div></div><p>Addressing these deployment and compliance risks requires new tooling, protocols, and design practices that support auditable autonomy‚Äîthe ability of a system to adapt in place while still satisfying external requirements for traceability, reproducibility, and user protection. As on-device learning becomes more prevalent, these challenges will become central to both system architecture and governance frameworks.</p>
</section>
<section id="sec-ondevice-learning-engineering-challenge-synthesis-92d4" class="level3">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-engineering-challenge-synthesis-92d4">Engineering Challenge Synthesis</h3>
<p>Designing on-device ML systems involves navigating a complex landscape of technical and practical constraints. While localized adaptation allows personalization, privacy, and responsiveness, it also introduces a range of challenges that span hardware heterogeneity, data fragmentation, observability, and regulatory compliance.</p>
<p>System heterogeneity complicates deployment and optimization by introducing variation in compute, memory, and runtime environments. Non-IID data distributions challenge learning stability and generalization, especially when models are trained on-device without access to global context. The absence of centralized monitoring makes it difficult to validate updates or detect performance regressions, and training activity must often compete with core device functionality for energy and compute. Finally, post-deployment learning introduces complications in model governance, from auditability and rollback to privacy assurance.</p>
<p>These challenges are not isolated‚Äîthey interact in ways that influence the viability of different adaptation strategies. <a href="#tbl-ondevice-challenges" class="quarto-xref">Table&nbsp;6</a> summarizes the primary challenges and their implications for ML systems deployed at the edge.</p>
<div id="tbl-ondevice-challenges" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ondevice-challenges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: <strong>On-Device Learning Challenges</strong>: System heterogeneity, non-IID data, and limited resources introduce unique challenges for deploying and adapting machine learning models on edge devices, impacting portability, stability, and governance. The table details root causes of these challenges and their system-level implications, highlighting trade-offs between model performance and resource constraints.
</figcaption>
<div aria-describedby="tbl-ondevice-challenges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 35%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Challenge</strong></th>
<th style="text-align: left;"><strong>Root Cause</strong></th>
<th style="text-align: left;"><strong>System-Level Implications</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>System Heterogeneity</strong></td>
<td style="text-align: left;">Diverse hardware, software, and toolchains</td>
<td style="text-align: left;">Limits portability; requires platform-specific tuning</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Non-IID and Fragmented Data</strong></td>
<td style="text-align: left;">Localized, user-specific data distributions</td>
<td style="text-align: left;">Hinders generalization; increases risk of drift</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Limited Observability and Feedback</strong></td>
<td style="text-align: left;">No centralized testing or logging</td>
<td style="text-align: left;">Makes update validation and debugging difficult</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Resource Contention and Scheduling</strong></td>
<td style="text-align: left;">Competing demands for memory, compute, and battery</td>
<td style="text-align: left;">Requires dynamic scheduling and budget-aware learning</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Deployment and Compliance Risk</strong></td>
<td style="text-align: left;">Learning continues post-deployment</td>
<td style="text-align: left;">Complicates model versioning, auditing, and rollback</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="sec-ondevice-learning-foundations-robust-ai-systems-3fde" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-ondevice-learning-foundations-robust-ai-systems-3fde">Foundations for Robust AI Systems</h3>
<p>The operational challenges and failure modes explored in the preceding sections reveal vulnerabilities that extend beyond deployment concerns into fundamental system reliability. When models adapt autonomously across millions of heterogeneous devices, three categories of threats emerge that traditional centralized training never encounters.</p>
<p>First, unlike centralized systems where failures are localized and observable (as discussed in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>), on-device learning creates scenarios where local failures can propagate silently across device populations. A corrupted adaptation on one device, if aggregated through federated learning, can poison the global model. Hardware faults that would trigger errors in centralized infrastructure may silently corrupt gradients on edge devices with minimal error detection capabilities.</p>
<p>Second, the federated coordination mechanisms that enable collaborative learning also create new attack surfaces. Adversarial clients can inject poisoned gradients<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a> designed to degrade global model performance. Model inversion attacks can extract private information from shared updates despite aggregation. The distributed nature of on-device learning makes these attacks both easier to execute (compromising client devices) and harder to detect (no centralized validation).</p>
<div class="no-row-height column-margin column-container"><div id="fn38"><p><sup>38</sup>&nbsp;<strong>Byzantine Fault Tolerance in FL</strong>: Distributed systems property that enables correct operation despite some participants being malicious or faulty (named after the Byzantine Generals Problem). In federated learning, up to f malicious clients can be tolerated among n participants using algorithms like Krum or trimmed mean aggregation, which requires n ‚â• 3f + 1 total participants. These robust aggregation methods increase communication costs by 2-5<span class="math inline">\(\times\)</span> and computational overhead by 3-10<span class="math inline">\(\times\)</span>, but prevent poisoning attacks where malicious clients could degrade global model performance by injecting adversarial gradients.</p></div></div><p>Third, on-device systems must handle distribution shifts and environmental changes without access to labeled validation data. Models may confidently drift into failure modes, adapting to local biases or temporary anomalies. The non-IID data distributions across devices mean that local drift on individual devices may not trigger global alarms, allowing silent degradation.</p>
<p>These reliability threats demand systematic approaches that ensure on-device learning systems remain robust despite autonomous adaptation, malicious manipulation, and environmental uncertainty. <strong><a href="../robust_ai/robust_ai.html#sec-robust-ai">Chapter 16: Robust AI</a></strong> examines these challenges comprehensively, establishing principles for fault-tolerant AI systems that can maintain reliability despite hardware faults, adversarial attacks, and distribution shifts. The techniques developed there‚ÄîByzantine-resilient aggregation, adversarial training, and drift detection‚Äîbecome essential components of production-ready on-device learning systems rather than optional enhancements.</p>
<p>The privacy-preserving aspects of these robustness mechanisms, including secure aggregation and differential privacy, connect directly to <strong><a href="../privacy_security/privacy_security.html#sec-security-privacy">Chapter 15: Security & Privacy</a></strong>, which establishes the cryptographic foundations and privacy guarantees necessary for deploying self-learning systems at scale while maintaining user trust and regulatory compliance.</p>
<div id="quiz-question-sec-ondevice-learning-persistent-technical-operational-challenges-8c12" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.9</strong></summary><div>
<ol type="1">
<li><p>What is a primary challenge of deploying machine learning models on heterogeneous devices in on-device learning systems?</p>
<ol type="a">
<li>Ensuring uniform hardware capabilities across devices</li>
<li>Centralizing data collection for model training</li>
<li>Managing diverse software stacks and runtime environments</li>
<li>Standardizing network connectivity across all devices</li>
</ol></li>
<li><p>Explain how data fragmentation in on-device learning systems affects model training and evaluation.</p></li>
<li><p>True or False: In on-device learning, the absence of centralized validation data makes it easier to ensure model updates are beneficial.</p></li>
<li><p>Order the following challenges in on-device learning from most to least impacted by system heterogeneity: (1) Model deployment, (2) Algorithm design, (3) Resource scheduling.</p></li>
<li><p>Consider a scenario where an on-device learning system must adapt to user-specific data while maintaining privacy. What trade-offs might you encounter, and how could they be addressed?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-persistent-technical-operational-challenges-8c12" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-ondevice-learning-fallacies-pitfalls-6c6d" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-fallacies-pitfalls-6c6d">Fallacies and Pitfalls</h2>
<p>On-device learning operates in a fundamentally different environment from cloud-based training, with severe resource constraints and privacy requirements that challenge traditional machine learning assumptions. The appeal of local adaptation and privacy preservation can obscure the significant technical limitations and implementation challenges that determine whether on-device learning provides net benefits over simpler alternatives.</p>
<p><strong>Fallacy:</strong> <em>On-device learning provides the same adaptation capabilities as cloud-based training.</em></p>
<p>This misconception leads teams to expect that local learning can achieve the same model improvements as centralized training with abundant computational resources. On-device learning operates under severe constraints including limited memory, restricted computational power, and minimal energy budgets that fundamentally limit adaptation capabilities. Local datasets are typically small, biased, and non-representative, making it impossible to achieve the same generalization performance as centralized training. Effective on-device learning requires accepting these limitations and designing adaptation strategies that provide meaningful improvements within practical constraints rather than attempting to replicate cloud-scale learning capabilities. This necessitates an efficiency-first mindset and careful optimization techniques.</p>
<p><strong>Pitfall:</strong> <em>Assuming that federated learning automatically preserves privacy without additional safeguards.</em></p>
<p>Many practitioners believe that keeping data on local devices inherently provides privacy protection without considering the information that can be inferred from model updates. Gradient and parameter updates can leak significant information about local training data through various inference attacks. Device participation patterns, update frequencies, and model convergence behaviors can reveal sensitive information about users and their activities. True privacy preservation requires additional mechanisms like differential privacy (mathematical guarantees that individual data points cannot be inferred from model outputs), secure aggregation protocols that prevent parameter inspection, and careful communication protocols rather than relying solely on data locality.</p>
<p><strong>Fallacy:</strong> <em>Resource-constrained adaptation always produces better personalized models than generic models.</em></p>
<p>This belief assumes that any local adaptation is beneficial regardless of the quality or quantity of local data available. On-device learning with insufficient, noisy, or biased local data can actually degrade model performance compared to well-trained generic models. Small datasets may not provide enough signal for meaningful learning, while adaptation to local noise can harm generalization. Effective on-device learning systems must include mechanisms to detect when local adaptation is beneficial and fall back to generic models when local data is inadequate for reliable learning.</p>
<p><strong>Pitfall:</strong> <em>Ignoring the heterogeneity challenges across different device types and capabilities.</em></p>
<p>Teams often design on-device learning systems assuming uniform hardware capabilities across deployment devices. Real-world deployments span diverse hardware with varying computational power, memory capacity, energy constraints, and networking capabilities. A learning algorithm that works well on high-end smartphones may fail catastrophically on resource-constrained IoT devices<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn39"><p><sup>39</sup>&nbsp;<strong>System Heterogeneity Reality</strong>: Edge device capabilities span 6+ orders of magnitude‚Äîfrom 32&nbsp;KB RAM microcontrollers to 16&nbsp;GB smartphones. Processing power varies from 48&nbsp;MHz ARM Cortex-M0+ (~10 MIPS) to 3&nbsp;GHz A-series processors (~100,000 MIPS). Power budgets range from 10&nbsp;ŒºW (sensor nodes) to 5&nbsp;W (flagship phones). This extreme diversity means federated learning algorithms must dynamically adapt: quantized inference on low-end devices, selective participation based on capability, and tiered aggregation strategies that account for the 10,000<span class="math inline">\(\times\)</span> performance differences within a single deployment. This heterogeneity affects not only individual device performance but also federated learning coordination where slow or unreliable devices can bottleneck the entire system. Successful on-device learning requires adaptive algorithms that adjust to device capabilities and robust coordination mechanisms that handle device heterogeneity gracefully. The development and deployment of such systems benefits from robust engineering practices that handle uncertainty and failure gracefully.</p></div></div><p><strong>Pitfall:</strong> <em>Underestimating the complexity of orchestrating learning across distributed edge systems.</em></p>
<p>Many teams focus on individual device optimization without considering the system-level challenges of coordinating learning across thousands or millions of edge devices. Edge systems orchestration must handle intermittent connectivity, varying power states, different time zones, and unpredictable device availability patterns that create complex scheduling and synchronization challenges. Device clustering, federated rounds coordination, model versioning across diverse deployment contexts, and handling partial participation from unreliable devices require sophisticated infrastructure beyond simple aggregation servers. Additionally, real-world edge deployments involve multiple stakeholders with different incentives, security requirements, and operational procedures that must be balanced against learning objectives. Effective edge learning systems require robust orchestration frameworks that can maintain system coherence despite constant device churn, network partitions, and operational disruptions.</p>
<div id="quiz-question-sec-ondevice-learning-fallacies-pitfalls-6c6d" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.10</strong></summary><div>
<ol type="1">
<li><p>True or False: On-device learning can achieve the same generalization performance as cloud-based training with sufficient local data.</p></li>
<li><p>Which of the following is a misconception about federated learning?</p>
<ol type="a">
<li>Federated learning improves model performance by utilizing diverse data from multiple sources.</li>
<li>Federated learning automatically preserves privacy without additional safeguards.</li>
<li>Federated learning reduces the need for centralized data storage.</li>
<li>Federated learning requires robust orchestration frameworks to handle device heterogeneity.</li>
</ol></li>
<li><p>Explain why resource-constrained adaptation might not always produce better personalized models than generic models.</p></li>
<li><p>Order the following challenges in on-device learning from most to least impacted by system heterogeneity: (1) Model versioning, (2) Federated learning coordination, (3) Device capability detection.</p></li>
<li><p>Consider a scenario where an on-device learning system must adapt to user-specific data while maintaining privacy. What trade-offs would you consider in implementing such a system?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-fallacies-pitfalls-6c6d" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
<section id="sec-ondevice-learning-summary-0af9" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-ondevice-learning-summary-0af9">Summary</h2>
<p>On-device learning represents a fundamental shift from static, centralized training to dynamic, local adaptation directly on deployment devices. This paradigm enables machine learning systems to personalize experiences while preserving privacy, reduce network dependencies, and respond rapidly to changing local conditions. Success requires integrating optimization principles, understanding hardware constraints, and applying sound operational practices. The transition from traditional cloud-based training to edge-based learning requires overcoming severe computational, memory, and energy constraints that fundamentally reshape how models are designed and adapted.</p>
<p>The technical strategies that enable practical on-device learning span multiple dimensions of system design. Adaptation techniques range from lightweight bias-only updates to selective parameter tuning, each offering different tradeoffs between expressivity and resource efficiency. Data efficiency becomes paramount when learning from limited local examples, driving innovations in few-shot learning<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a>, streaming adaptation, and memory-based replay mechanisms<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn40"><p><sup>40</sup>&nbsp;<strong>Few-Shot Learning</strong>: Machine learning paradigm that learns new concepts from only a few (typically 1-10) labeled examples. Originally inspired by human learning capabilities‚Äîhumans can recognize new objects from just one or two examples. In ML, few-shot learning leverages pre-trained representations and meta-learning to quickly adapt to new tasks. Critical for on-device scenarios where collecting large labeled datasets is impractical. Techniques include prototypical networks, model-agnostic meta-learning (MAML), and metric learning approaches that achieve 80-90% accuracy with just 5 examples per class. Federated learning emerges as a crucial coordination mechanism, allowing devices to collaborate while maintaining data locality and privacy guarantees.</p></div><div id="fn41"><p><sup>41</sup>&nbsp;<strong>Catastrophic Forgetting</strong>: When neural networks learn new tasks, they tend to ‚Äúforget‚Äù previously learned information as new gradients overwrite old weights. This is a fundamental challenge for on-device learning where models must continuously adapt without losing prior knowledge. Solutions include elastic weight consolidation (EWC), gradient episodic memory (GEM), and replay buffers that store representative samples. In resource-constrained devices, rehearsal strategies must balance memory overhead (storing old examples) with computational cost (retraining on mixed data), directly impacting system design decisions.</p></div></div><div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>On-device learning shifts machine learning from static deployment to dynamic local adaptation, enabling personalization while preserving privacy</li>
<li>Resource constraints drive specialized techniques: bias-only updates, adapter modules, sparse parameter updates, and compressed data representations</li>
<li>Federated learning coordinates distributed training across heterogeneous devices while maintaining privacy and handling non-IID data distributions</li>
<li>Success requires co-designing algorithms with hardware constraints, balancing adaptation capability against memory, energy, and computational limitations</li>
</ul>
</div>
</div>
<p>Real-world applications demonstrate both the potential and challenges of on-device learning, from keyword spotting systems that adapt to user voices to recommendation engines that personalize without transmitting user data. As machine learning expands into mobile, embedded, and wearable environments, the ability to learn locally while maintaining efficiency and reliability becomes essential for next-generation intelligent systems that operate seamlessly across diverse deployment contexts.</p>
<p>The distributed nature of on-device learning introduces new vulnerabilities that extend beyond individual device constraints. The very capabilities that make these systems powerful‚Äîlearning from user data, adapting to local patterns, coordinating across devices‚Äîalso create new attack surfaces and privacy risks. These adaptive systems must not only function correctly but also protect sensitive user information and defend against adversarial manipulation. Security and privacy frameworks (<strong><a href="../privacy_security/privacy_security.html#sec-security-privacy">Chapter 15: Security & Privacy</a></strong>) address these critical concerns, showing how to protect on-device learning systems from both privacy breaches and adversarial attacks. Subsequently, the robust AI principles (<strong><a href="../robust_ai/robust_ai.html#sec-robust-ai">Chapter 16: Robust AI</a></strong>) extend these protections to encompass system-wide reliability challenges including hardware failures and software faults, while ML Operations (<strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>) provides the comprehensive framework for deploying and maintaining these complex adaptive systems in production.</p>


<div id="quiz-question-sec-ondevice-learning-summary-0af9" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.11</strong></summary><div>
<ol type="1">
<li><p>Which of the following adaptation techniques in on-device learning offers the best balance between expressivity and resource efficiency?</p>
<ol type="a">
<li>Bias-only updates</li>
<li>Full model retraining</li>
<li>Selective parameter tuning</li>
<li>Data augmentation</li>
</ol></li>
<li><p>Discuss the trade-offs involved in using few-shot learning for on-device systems with limited data availability.</p></li>
<li><p>In on-device learning, the challenge of ____ arises when models forget previously learned information as they adapt to new tasks.</p></li>
<li><p>How might federated learning be used to enhance privacy in on-device learning systems?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ondevice-learning-summary-0af9" class="question-label">See Answers ‚Üí</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-ondevice-learning-distributed-learning-paradigm-shift-883d" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>What is a primary advantage of on-device learning in machine learning systems?</strong></p>
<ol type="a">
<li>Increased reliance on centralized servers</li>
<li>Simplified system architecture</li>
<li>Unlimited computational resources</li>
<li>Improved privacy through data locality</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Improved privacy through data locality. On-device learning processes data locally, reducing the need to transfer sensitive information to centralized servers, thus enhancing privacy.</p>
<p><em>Learning Objective</em>: Understand the privacy advantages of on-device learning.</p></li>
<li><p><strong>True or False: On-device learning eliminates the need for computational efficiency in machine learning models.</strong></p>
<p><em>Answer</em>: False. On-device learning requires significant computational efficiency due to the limited resources available on edge devices, such as memory and energy constraints.</p>
<p><em>Learning Objective</em>: Recognize the importance of computational efficiency in on-device learning.</p></li>
<li><p><strong>Explain how the transition from centralized to on-device learning affects the deployment and maintenance lifecycles of machine learning models.</strong></p>
<p><em>Answer</em>: The transition to on-device learning changes deployment and maintenance lifecycles by requiring models to adapt continuously to local conditions, rather than following predictable versioning patterns. This necessitates new strategies for model updates and performance evaluation across diverse environments. For example, an autonomous vehicle‚Äôs model must adapt to local driving conditions in real-time, which contrasts with periodic updates in a centralized system. This ensures models remain relevant and effective in dynamic environments.</p>
<p><em>Learning Objective</em>: Analyze the impact of on-device learning on ML model lifecycles.</p></li>
<li><p><strong>Which of the following is a challenge faced by on-device learning compared to centralized learning?</strong></p>
<ol type="a">
<li>Abundant computational resources</li>
<li>Limited memory capacity</li>
<li>Reliable network connectivity</li>
<li>Predictable system behavior</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Limited memory capacity. On-device learning must operate within the constraints of edge devices, which often have limited memory and computational resources.</p>
<p><em>Learning Objective</em>: Identify the challenges associated with on-device learning.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-distributed-learning-paradigm-shift-883d" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-motivations-benefits-37c3" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary benefit of on-device learning compared to centralized learning?</strong></p>
<ol type="a">
<li>Increased computational power</li>
<li>Simplified model management</li>
<li>Enhanced personalization and privacy</li>
<li>Lower development costs</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Enhanced personalization and privacy. On-device learning allows models to adapt to user-specific data locally, preserving privacy and providing personalized experiences. Options A, B, and D do not align with the primary benefits discussed in the section.</p>
<p><em>Learning Objective</em>: Understand the key benefits of on-device learning over centralized learning.</p></li>
<li><p><strong>Describe a scenario where on-device learning is more advantageous than centralized learning, considering privacy and latency.</strong></p>
<p><em>Answer</em>: On-device learning is advantageous in scenarios like mobile input prediction, where user data is sensitive, and immediate response is required. For example, a smartphone keyboard adapting to a user‚Äôs typing style benefits from local data processing, ensuring privacy and reducing latency. This approach meets user expectations for privacy and responsiveness without relying on cloud connectivity.</p>
<p><em>Learning Objective</em>: Apply the concept of on-device learning to real-world scenarios, focusing on privacy and latency benefits.</p></li>
<li><p><strong>True or False: On-device learning eliminates the need for centralized model updates.</strong></p>
<p><em>Answer</em>: False. While on-device learning allows local model adaptation, centralized updates may still be necessary to incorporate global improvements and ensure consistency across devices. This is important for maintaining overall system performance and reliability.</p>
<p><em>Learning Objective</em>: Challenge misconceptions about the role of centralized updates in on-device learning systems.</p></li>
<li><p><strong>On-device learning allows for model adaptation using ____ data, enhancing personalization and privacy.</strong></p>
<p><em>Answer</em>: local. On-device learning leverages data available on the device itself, ensuring that sensitive information does not need to be transmitted to the cloud.</p>
<p><em>Learning Objective</em>: Recall the type of data used in on-device learning to enhance personalization and privacy.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-motivations-benefits-37c3" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-design-constraints-c776" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes a challenge of on-device learning compared to cloud-based training?</strong></p>
<ol type="a">
<li>Access to large, curated datasets</li>
<li>Higher computational capacity</li>
<li>Limited memory and computational resources</li>
<li>Centralized model updates</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Limited memory and computational resources. On-device learning faces challenges due to constrained resources, unlike cloud-based training which benefits from extensive infrastructure.</p>
<p><em>Learning Objective</em>: Understand the constraints of on-device learning compared to centralized environments.</p></li>
<li><p><strong>Explain how model compression techniques are essential for on-device learning, particularly during training.</strong></p>
<p><em>Answer</em>: Model compression techniques, such as quantization and pruning, are essential for on-device learning because they reduce memory and computational requirements, enabling models to fit within the limited resources of edge devices. For example, aggressive compression allows training on devices with minimal RAM by reducing the model size and complexity, which is crucial since training amplifies resource demands.</p>
<p><em>Learning Objective</em>: Analyze the role of model compression in enabling on-device learning under resource constraints.</p></li>
<li><p><strong>On-device learning requires careful management of ____ due to increased memory and computational demands during training.</strong></p>
<p><em>Answer</em>: resources. On-device learning amplifies resource demands, making efficient management of memory and computation crucial.</p>
<p><em>Learning Objective</em>: Recall the importance of resource management in on-device learning.</p></li>
<li><p><strong>True or False: On-device learning systems can use the same model architectures as cloud-based systems without modification.</strong></p>
<p><em>Answer</em>: False. On-device learning systems require specialized model architectures that are optimized for limited resources, unlike cloud-based systems that can use larger and more complex models.</p>
<p><em>Learning Objective</em>: Challenge the misconception that on-device and cloud-based systems can use identical model architectures.</p></li>
<li><p><strong>Order the following steps in the on-device learning process: (1) Meta-training with generic data, (2) Online adaptive learning, (3) Ranking and selecting layers to update.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Meta-training with generic data, (3) Ranking and selecting layers to update, (2) Online adaptive learning. The process begins with meta-training to establish initial weights, followed by ranking to determine which layers to update, and concludes with adaptive learning based on device-specific constraints.</p>
<p><em>Learning Objective</em>: Understand the sequence of steps in the on-device learning process.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-design-constraints-c776" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-model-adaptation-6a82" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following adaptation strategies is most suitable for devices with extreme memory and compute constraints?</strong></p>
<ol type="a">
<li>Sparse layer updates</li>
<li>Residual adapters</li>
<li>Bias-only updates</li>
<li>Full model retraining</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Bias-only updates. This strategy significantly reduces memory and computational requirements by updating only scalar offsets, making it ideal for devices with tight resource constraints.</p>
<p><em>Learning Objective</em>: Understand which adaptation strategies are suitable for different levels of device constraints.</p></li>
<li><p><strong>Explain the trade-offs involved in using residual adapters for on-device learning.</strong></p>
<p><em>Answer</em>: Residual adapters offer greater flexibility than bias-only updates by introducing small trainable modules into a frozen model. This allows for more expressive adaptation but increases memory and computational requirements compared to bias-only updates. They are suitable for devices with moderate resources, balancing personalization needs with resource constraints.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs of using residual adapters in model adaptation.</p></li>
<li><p><strong>In task-adaptive sparse updates, only a subset of parameters is updated based on their ____ to downstream performance.</strong></p>
<p><em>Answer</em>: contribution. This approach focuses on updating the most impactful parameters, optimizing resource use while maintaining adaptation quality.</p>
<p><em>Learning Objective</em>: Recall the criteria for selecting parameters in sparse updates.</p></li>
<li><p><strong>Order the following adaptation strategies from least to most expressive: (1) Residual adapters, (2) Bias-only updates, (3) Sparse layer updates.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Bias-only updates, (1) Residual adapters, (3) Sparse layer updates. Bias-only updates are the least expressive, while sparse updates allow for the most task-specific adaptation.</p>
<p><em>Learning Objective</em>: Understand the expressivity hierarchy of different adaptation strategies.</p></li>
<li><p><strong>In a production system with limited memory, which adaptation strategy would enable efficient personalization without full retraining?</strong></p>
<ol type="a">
<li>Full model retraining</li>
<li>Low-rank updates</li>
<li>Sparse layer updates</li>
<li>Bias-only updates</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Bias-only updates. This approach allows for efficient personalization by updating only the bias terms, avoiding the need for full retraining.</p>
<p><em>Learning Objective</em>: Apply knowledge of adaptation strategies to real-world system constraints.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-model-adaptation-6a82" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-data-efficiency-c701" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following strategies is most suitable for adapting models on-device when only a few labeled examples are available?</strong></p>
<ol type="a">
<li>Experience Replay</li>
<li>Batch Training</li>
<li>Data Compression</li>
<li>Few-Shot Learning</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Few-Shot Learning. This strategy allows models to personalize based on a small number of labeled examples, making it ideal for on-device learning with limited data.</p>
<p><em>Learning Objective</em>: Understand the suitability of few-shot learning for personalization with minimal labeled data.</p></li>
<li><p><strong>Explain how experience replay can mitigate the issue of catastrophic forgetting in on-device learning systems.</strong></p>
<p><em>Answer</em>: Experience replay mitigates catastrophic forgetting by maintaining a buffer of past examples, allowing the model to reinforce prior knowledge while learning new information. This is crucial in non-stationary environments where data streams continuously, helping to stabilize learning and prevent overfitting to recent data.</p>
<p><em>Learning Objective</em>: Analyze the role of experience replay in preventing catastrophic forgetting in continuous learning scenarios.</p></li>
<li><p><strong>In on-device learning, data compression is used to reduce the memory footprint by transforming raw data into ____ representations.</strong></p>
<p><em>Answer</em>: compressed. Compressed representations allow devices to store and process data more efficiently, supporting longer retention of experience under memory constraints.</p>
<p><em>Learning Objective</em>: Recall the purpose of data compression in reducing memory usage in on-device learning.</p></li>
<li><p><strong>What is a primary trade-off when using compressed data representations in on-device learning?</strong></p>
<ol type="a">
<li>Loss of task-specific variability</li>
<li>Increased data acquisition costs</li>
<li>Higher energy consumption</li>
<li>Reduced model personalization</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Loss of task-specific variability. Compression can introduce information loss, limiting the model‚Äôs ability to capture variability specific to the deployment conditions.</p>
<p><em>Learning Objective</em>: Evaluate the trade-offs associated with using compressed data representations in on-device learning.</p></li>
<li><p><strong>Consider a scenario where a wearable device must adapt to user-specific motion patterns. How might few-shot learning and experience replay be combined to improve the device‚Äôs performance?</strong></p>
<p><em>Answer</em>: Few-shot learning can quickly personalize the model using a small set of labeled activity segments, while experience replay maintains a buffer of past motion patterns to reinforce learning and prevent forgetting. This combination allows the device to adapt efficiently to user-specific behaviors while ensuring stability over time.</p>
<p><em>Learning Objective</em>: Integrate few-shot learning and experience replay strategies to enhance on-device adaptation in a practical scenario.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-data-efficiency-c701" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-federated-learning-6e7e" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>What is a primary challenge of federated learning compared to centralized learning?</strong></p>
<ol type="a">
<li>Increased computational power required on the server</li>
<li>Lack of personalization for individual device users</li>
<li>Difficulty in coordinating updates from distributed devices</li>
<li>Higher data storage requirements on individual devices</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Difficulty in coordinating updates from distributed devices. This is correct because federated learning involves aggregating model updates from many devices, which can be challenging due to network variability and device availability. Options A, B, and D are incorrect because they do not address the core challenge of distributed coordination.</p>
<p><em>Learning Objective</em>: Understand the coordination challenges in federated learning systems.</p></li>
<li><p><strong>Explain how federated learning addresses privacy concerns while enabling collective intelligence.</strong></p>
<p><em>Answer</em>: Federated learning addresses privacy concerns by keeping raw data localized on individual devices, only transmitting model updates like gradients to a central server. This preserves data privacy while enabling collective intelligence by aggregating updates to improve a shared global model. This approach allows systems to learn from population-scale data without compromising individual privacy.</p>
<p><em>Learning Objective</em>: Explain the privacy-preserving mechanisms of federated learning.</p></li>
<li><p><strong>Order the following steps in the federated learning cycle: (1) Local training on device, (2) Aggregation of model updates, (3) Distribution of global model to devices, (4) Transmission of model updates to server.</strong></p>
<p><em>Answer</em>: The correct order is: (3) Distribution of global model to devices, (1) Local training on device, (4) Transmission of model updates to server, (2) Aggregation of model updates. This sequence reflects the typical federated learning process where a global model is first distributed, then locally trained on devices, updates are sent back to the server, and finally aggregated to form a new global model.</p>
<p><em>Learning Objective</em>: Understand the cyclical process of federated learning.</p></li>
<li><p><strong>In a production system using federated learning, what trade-off must be considered when choosing the number of local training steps?</strong></p>
<ol type="a">
<li>Balancing communication frequency and model divergence</li>
<li>Balancing model accuracy and computational cost</li>
<li>Balancing data privacy and model size</li>
<li>Balancing server load and energy consumption</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Balancing communication frequency and model divergence. This is correct because increasing the number of local steps reduces communication frequency but can lead to model divergence if local data distributions vary significantly. Options B, C, and D do not directly address the trade-off related to local training steps.</p>
<p><em>Learning Objective</em>: Analyze trade-offs in federated learning related to local training steps.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-federated-learning-6e7e" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-production-integration-beb5" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes a key challenge in deploying on-device learning systems compared to centralized systems?</strong></p>
<ol type="a">
<li>Centralized data access</li>
<li>Uniform monitoring capabilities</li>
<li>Device-aware deployment pipelines</li>
<li>Single model version management</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Device-aware deployment pipelines. On-device learning requires pipelines that account for heterogeneous device capabilities, unlike centralized systems that deploy to uniform infrastructure.</p>
<p><em>Learning Objective</em>: Understand the unique challenges in deploying on-device learning systems.</p></li>
<li><p><strong>Explain how privacy-preserving telemetry differs from traditional monitoring in on-device learning systems.</strong></p>
<p><em>Answer</em>: Privacy-preserving telemetry in on-device learning involves collecting aggregate statistics or differentially private summaries instead of individual predictions or training samples. This approach ensures user privacy by preventing the reconstruction of private information from any single device‚Äôs data. For example, devices report mean accuracy rather than per-example metrics. This enables monitoring across distributed devices without compromising user privacy.</p>
<p><em>Learning Objective</em>: Analyze the differences and implications of privacy-preserving telemetry in on-device learning.</p></li>
<li><p><strong>Order the following steps in an on-device learning deployment pipeline: (1) Device capability detection, (2) Strategy selection logic, (3) Tiered deployment orchestration.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Device capability detection, (2) Strategy selection logic, (3) Tiered deployment orchestration. This sequence ensures that the system first identifies device capabilities, then selects appropriate strategies, and finally orchestrates deployments across different device tiers.</p>
<p><em>Learning Objective</em>: Understand the sequence of steps in deploying on-device learning systems.</p></li>
<li><p><strong>Discuss the trade-offs involved in using federated learning for on-device systems, focusing on resource constraints and privacy.</strong></p>
<p><em>Answer</em>: Federated learning for on-device systems offers privacy benefits by keeping data local, but it introduces trade-offs such as increased computational load on devices and the need for efficient communication protocols to manage resource constraints. For example, devices must perform local training, which can strain battery and processing resources. Balancing these trade-offs is crucial for maintaining user experience and system reliability.</p>
<p><em>Learning Objective</em>: Evaluate the trade-offs of implementing federated learning in resource-constrained environments.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-production-integration-beb5" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-systems-integration-production-deployment-c6bb" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Which layer in the adaptive systems integration is responsible for ensuring privacy-preserving collaboration across devices?</strong></p>
<ol type="a">
<li>Model adaptation layer</li>
<li>Data efficiency layer</li>
<li>Hierarchical capability matching</li>
<li>Federated coordination layer</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Federated coordination layer. This layer orchestrates privacy-preserving collaboration across devices, ensuring that individual voice patterns remain on the device while enabling population-scale improvements.</p>
<p><em>Learning Objective</em>: Understand the role of the federated coordination layer in adaptive systems integration.</p></li>
<li><p><strong>Explain the concept of hierarchical capability matching in the context of adaptive systems integration.</strong></p>
<p><em>Answer</em>: Hierarchical capability matching involves deploying more sophisticated techniques on capable devices while ensuring basic functionality across all devices. For example, flagship phones use LoRA (Low-Rank Adaptation) rank-32 adapters, while budget devices rely on bias-only updates. This approach ensures that each device operates optimally within its constraints.</p>
<p><em>Learning Objective</em>: Describe hierarchical capability matching and its importance in system integration.</p></li>
<li><p><strong>Order the following steps in the adaptive systems integration process: (1) Implement experience replay, (2) Deploy LoRA adapters, (3) Ensure privacy-preserving aggregation.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Deploy LoRA (Low-Rank Adaptation) adapters, (1) Implement experience replay, (3) Ensure privacy-preserving aggregation. LoRA adapters are deployed first to enable model adaptation, followed by experience replay for data efficiency, and finally privacy-preserving aggregation for federated learning.</p>
<p><em>Learning Objective</em>: Sequence the integration steps in adaptive systems to understand their interdependencies.</p></li>
<li><p><strong>What is a key trade-off when implementing streaming updates in on-device learning systems?</strong></p>
<ol type="a">
<li>Higher computational cost versus improved privacy</li>
<li>Increased memory usage versus faster adaptation</li>
<li>Larger buffer sizes versus reduced personalization</li>
<li>More frequent updates versus network congestion</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Increased memory usage versus faster adaptation. Streaming updates allow continuous adaptation to changing user patterns, but they require more memory to store and process data.</p>
<p><em>Learning Objective</em>: Identify trade-offs associated with streaming updates in adaptive systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-systems-integration-production-deployment-c6bb" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-persistent-technical-operational-challenges-8c12" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.9</strong></summary><div>
<ol type="1">
<li><p><strong>What is a primary challenge of deploying machine learning models on heterogeneous devices in on-device learning systems?</strong></p>
<ol type="a">
<li>Ensuring uniform hardware capabilities across devices</li>
<li>Centralizing data collection for model training</li>
<li>Managing diverse software stacks and runtime environments</li>
<li>Standardizing network connectivity across all devices</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Managing diverse software stacks and runtime environments. This is a challenge because devices may run different operating systems and libraries, leading to inconsistencies in model behavior. Options A, B, and D are incorrect as they focus on uniformity and centralization, which are not applicable in heterogeneous environments.</p>
<p><em>Learning Objective</em>: Understand the challenges posed by hardware and software heterogeneity in on-device learning.</p></li>
<li><p><strong>Explain how data fragmentation in on-device learning systems affects model training and evaluation.</strong></p>
<p><em>Answer</em>: Data fragmentation leads to non-IID data distributions, which can slow convergence and destabilize training. It complicates evaluation because no single test set represents the deployment distribution. For example, gradients computed on different devices may conflict due to non-IID data distributions, where each device sees different types of data that lead to conflicting optimization directions, and local updates may overfit to client-specific data. This challenges the stability and generalization of models trained on-device.</p>
<p><em>Learning Objective</em>: Analyze the impact of data fragmentation on training and evaluation in on-device learning systems.</p></li>
<li><p><strong>True or False: In on-device learning, the absence of centralized validation data makes it easier to ensure model updates are beneficial.</strong></p>
<p><em>Answer</em>: False. This is false because the lack of centralized validation data makes it difficult to assess the quality and direction of model updates, potentially leading to drift or performance degradation.</p>
<p><em>Learning Objective</em>: Challenge misconceptions about the ease of validating model updates in decentralized environments.</p></li>
<li><p><strong>Order the following challenges in on-device learning from most to least impacted by system heterogeneity: (1) Model deployment, (2) Algorithm design, (3) Resource scheduling.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Algorithm design, (1) Model deployment, (3) Resource scheduling. Algorithm design is most impacted because it must account for diverse hardware capabilities. Model deployment follows, as it requires adaptation to different environments. Resource scheduling is least impacted, as it primarily deals with optimizing available resources.</p>
<p><em>Learning Objective</em>: Understand the relative impact of system heterogeneity on various aspects of on-device learning.</p></li>
<li><p><strong>Consider a scenario where an on-device learning system must adapt to user-specific data while maintaining privacy. What trade-offs might you encounter, and how could they be addressed?</strong></p>
<p><em>Answer</em>: Trade-offs include balancing personalization with privacy, as user-specific data can enhance model accuracy but risks privacy breaches. Techniques like differential privacy and federated learning can address these by allowing updates without exposing raw data. This enables personalized models while respecting user privacy, which is crucial for sensitive applications like health monitoring.</p>
<p><em>Learning Objective</em>: Evaluate trade-offs in on-device learning related to personalization and privacy, and propose solutions.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-persistent-technical-operational-challenges-8c12" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-fallacies-pitfalls-6c6d" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.10</strong></summary><div>
<ol type="1">
<li><p><strong>True or False: On-device learning can achieve the same generalization performance as cloud-based training with sufficient local data.</strong></p>
<p><em>Answer</em>: False. On-device learning typically operates with limited, biased, and non-representative local datasets, making it impossible to achieve the same generalization performance as centralized training.</p>
<p><em>Learning Objective</em>: Understand the limitations of on-device learning compared to cloud-based training in terms of generalization performance.</p></li>
<li><p><strong>Which of the following is a misconception about federated learning?</strong></p>
<ol type="a">
<li>Federated learning improves model performance by utilizing diverse data from multiple sources.</li>
<li>Federated learning automatically preserves privacy without additional safeguards.</li>
<li>Federated learning reduces the need for centralized data storage.</li>
<li>Federated learning requires robust orchestration frameworks to handle device heterogeneity.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Federated learning automatically preserves privacy without additional safeguards. This is a misconception because model updates can leak significant information, and additional mechanisms like differential privacy are needed.</p>
<p><em>Learning Objective</em>: Identify misconceptions about privacy in federated learning and understand the need for additional privacy-preserving mechanisms.</p></li>
<li><p><strong>Explain why resource-constrained adaptation might not always produce better personalized models than generic models.</strong></p>
<p><em>Answer</em>: Resource-constrained adaptation might not always produce better personalized models because local data can be insufficient, noisy, or biased, leading to degraded model performance. For example, small datasets may not provide enough signal for meaningful learning, and adaptation to local noise can harm generalization. Effective on-device learning systems must detect when local adaptation is beneficial and fall back to generic models when local data is inadequate.</p>
<p><em>Learning Objective</em>: Analyze the conditions under which local adaptation may not be beneficial and understand the importance of fallback mechanisms in on-device learning.</p></li>
<li><p><strong>Order the following challenges in on-device learning from most to least impacted by system heterogeneity: (1) Model versioning, (2) Federated learning coordination, (3) Device capability detection.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Federated learning coordination, (3) Device capability detection, (1) Model versioning. Federated learning coordination is most impacted due to the need to handle diverse device capabilities and participation patterns. Device capability detection is next, as it requires adapting algorithms to different hardware. Model versioning is least impacted, though it still requires careful management across diverse contexts.</p>
<p><em>Learning Objective</em>: Understand the impact of system heterogeneity on various aspects of on-device learning and prioritize challenges accordingly.</p></li>
<li><p><strong>Consider a scenario where an on-device learning system must adapt to user-specific data while maintaining privacy. What trade-offs would you consider in implementing such a system?</strong></p>
<p><em>Answer</em>: In implementing an on-device learning system that adapts to user-specific data while maintaining privacy, trade-offs include balancing model accuracy with privacy guarantees, managing computational and memory constraints, and ensuring robust coordination across heterogeneous devices. For example, employing differential privacy may reduce model accuracy, but it is crucial for privacy preservation. Achieving effective personalization without compromising privacy or system performance requires careful design and optimization.</p>
<p><em>Learning Objective</em>: Evaluate the trade-offs involved in designing on-device learning systems that balance personalization and privacy.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-fallacies-pitfalls-6c6d" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ondevice-learning-summary-0af9" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.11</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following adaptation techniques in on-device learning offers the best balance between expressivity and resource efficiency?</strong></p>
<ol type="a">
<li>Bias-only updates</li>
<li>Full model retraining</li>
<li>Selective parameter tuning</li>
<li>Data augmentation</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Selective parameter tuning. This approach allows for targeted updates that maximize expressivity while minimizing resource usage, unlike full model retraining which is resource-intensive.</p>
<p><em>Learning Objective</em>: Evaluate adaptation techniques in on-device learning for their resource efficiency and expressivity.</p></li>
<li><p><strong>Discuss the trade-offs involved in using few-shot learning for on-device systems with limited data availability.</strong></p>
<p><em>Answer</em>: Few-shot learning allows rapid adaptation with minimal data, crucial for on-device systems with limited data. However, it may require complex algorithms and pre-trained models, increasing computational overhead. This trade-off affects system design, balancing adaptation speed against resource constraints.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs of few-shot learning in resource-constrained environments.</p></li>
<li><p><strong>In on-device learning, the challenge of ____ arises when models forget previously learned information as they adapt to new tasks.</strong></p>
<p><em>Answer</em>: catastrophic forgetting. This challenge occurs as new learning tasks overwrite existing knowledge, necessitating strategies like replay buffers to retain important information.</p>
<p><em>Learning Objective</em>: Understand the concept of catastrophic forgetting and its impact on on-device learning.</p></li>
<li><p><strong>How might federated learning be used to enhance privacy in on-device learning systems?</strong></p>
<p><em>Answer</em>: Federated learning enhances privacy by keeping data localized on devices and aggregating model updates centrally. This process prevents raw data from being exposed, maintaining user privacy while enabling collaborative model improvements.</p>
<p><em>Learning Objective</em>: Explain how federated learning contributes to privacy in on-device learning systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-ondevice-learning-summary-0af9" class="answer-label">‚Üê Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/ops/ops.html" class="pagination-link" aria-label="ML Operations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">ML Operations</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/privacy_security/privacy_security.html" class="pagination-link" aria-label="Security &amp; Privacy">
        <span class="nav-page-text">Security &amp; Privacy</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>¬© 2024-2025 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://www.netlify.com">
<p><img src="https://www.netlify.com/v3/img/components/netlify-color-accent.svg" alt="Deploys by Netlify" style="height: 15px; vertical-align: middle; margin-left: 3px;"></p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>