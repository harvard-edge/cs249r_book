---
bibliography: ops.bib
---

# ML Operations {#sec-mlops}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-embedded-aiops-resource), [Videos](#sec-embedded-aiops-resource), [Exercises](#sec-embedded-aiops-resource)
:::

![_DALL·E 3 Prompt: Create a detailed, wide rectangular illustration of an AI workflow. The image should showcase the process across six stages, with a flow from left to right: 1. Data collection, with diverse individuals of different genders and descents using a variety of devices like laptops, smartphones, and sensors to gather data. 2. Data processing, displaying a data center with active servers and databases with glowing lights. 3. Model training, represented by a computer screen with code, neural network diagrams, and progress indicators. 4. Model evaluation, featuring people examining data analytics on large monitors. 5. Deployment, where the AI is integrated into robotics, mobile apps, and industrial equipment. 6. Monitoring, showing professionals tracking AI performance metrics on dashboards to check for accuracy and concept drift over time. Each stage should be distinctly marked and the style should be clean, sleek, and modern with a dynamic and informative color scheme._](images/png/cover_ml_ops.png)

## Purpose {.unnumbered}

_What systematic principles enable continuous evolution of machine learning systems in production, and how do operational requirements shape deployment architecture?_

The transition from experimental to production environments represents a fundamental challenge in scaling machine learning systems. Operational practices reveal essential patterns for maintaining reliability and adaptability throughout the system lifecycle, highlighting critical relationships between deployment architecture and continuous delivery. The implementation of MLOps frameworks draws out the key trade-offs between automation, monitoring, and governance that shape system evolution. Understanding these operational dynamics provides insights into managing complex AI workflows, establishing core principles for creating systems that remain robust and responsive in dynamic production environments.

::: {.callout-tip title="Learning Objectives"}

* Understand what MLOps is and why it is needed

* Learn the architectural patterns for traditional MLOps

* Contrast traditional vs. embedded MLOps across the ML lifecycle

* Identify key constraints of embedded environments

* Learn strategies to mitigate embedded ML challenges

* Examine real-world case studies demonstrating embedded MLOps principles

* Appreciate the need for holistic technical and human approaches

:::

## Overview

Machine Learning Operations (MLOps) is a systematic discipline that integrates machine learning, data science, and software engineering practices to automate and streamline the end-to-end ML lifecycle. This lifecycle encompasses data preparation, model training, evaluation, deployment, monitoring, and ongoing maintenance. The goal of MLOps is to ensure that ML models are developed, deployed, and operated reliably, efficiently, and at scale.

::: {.callout-note title="Definition of MLOps"}

**Machine Learning Operations (MLOps)** is the _engineering discipline_ focused on managing the full lifecycle of machine learning systems—from data and model development to deployment, monitoring, and ongoing maintenance in production environments. It applies principles from DevOps while addressing the _unique challenges of ML_, including data and model versioning, continuous retraining, and system behavior under uncertainty. MLOps emphasizes _collaborative workflows_, _infrastructure automation_, and _governance_ to ensure that ML systems are _reliable, scalable, and auditable_ throughout their operational lifespan.

:::

To ground the discussion, consider a conventional ML application involving centralized infrastructure. A ridesharing company may aim to predict real-time rider demand using a machine learning model. The data science team might invest significant time designing and training the model. However, when it comes time to deploy it, the model often needs to be reengineered to align with the engineering team’s production requirements. This disconnect can introduce weeks of delay and engineering overhead. MLOps addresses this gap.

By establishing standard protocols, tools, and workflows, MLOps enables models developed during experimentation to transition seamlessly into production. It promotes collaboration across traditionally siloed roles—such as data scientists, ML engineers, and DevOps professionals—by defining interfaces and responsibilities. MLOps also supports continuous integration and delivery for ML, allowing teams to retrain, validate, and redeploy models frequently in response to new data or system conditions.

Returning to the ridesharing example, a mature MLOps practice would allow the company to continuously retrain its demand forecasting model as new ridership data becomes available. It would also make it easier to evaluate alternative model architectures, deploy experimental updates, and monitor system performance in production—all without disrupting live operations. This agility is critical for maintaining model relevance in dynamic environments.

Beyond operational efficiency, MLOps brings important benefits for governance and accountability. It standardizes the tracking of model versions, data lineage, and configuration parameters, creating a reproducible and auditable trail of ML artifacts. This is essential in highly regulated industries such as healthcare and finance, where model explainability and provenance are fundamental requirements.

Organizations across sectors are adopting MLOps to increase team productivity, reduce time-to-market, and improve the reliability of ML systems. The adoption of MLOps not only enhances model performance and robustness but also enables a sustainable approach to managing ML systems at scale.

This chapter introduces the core motivations and foundational components of MLOps, traces its historical development from DevOps, and outlines the key challenges and practices that guide its adoption in modern ML system design.

## Historical Context

MLOps has its roots in DevOps, a set of practices that combines software development (Dev) and IT operations (Ops) to shorten the development lifecycle and enable the continuous delivery of high-quality software. Both DevOps and MLOps emphasize automation, collaboration, and iterative improvement. However, while DevOps emerged to address challenges in software deployment and operational management, MLOps evolved in response to the unique complexities of machine learning workflows—especially those involving data-driven components. Understanding this evolution is essential for appreciating the motivations and structure of modern ML systems.

### DevOps

The term DevOps was coined in 2009 by [Patrick Debois](https://www.jedi.be/), a consultant and Agile practitioner who organized the first [DevOpsDays](https://www.devopsdays.org/) conference in Ghent, Belgium. DevOps extended the principles of the [Agile](https://agilemanifesto.org/) movement—which had emphasized close collaboration among development teams and rapid, iterative releases—by bringing IT operations into the fold.

In traditional software pipelines, development and operations teams often worked in silos, leading to inefficiencies, delays, and misaligned priorities. DevOps emerged as a response, advocating for shared ownership, infrastructure as code, and the use of automation to streamline deployment pipelines. Tools such as [Jenkins](https://www.jenkins.io/), [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/) became foundational to implementing continuous integration and continuous delivery (CI/CD) practices.

DevOps promotes collaboration through automation and feedback loops, aiming to reduce time-to-release and improve software reliability. It established the cultural and technical groundwork for extending similar principles to the ML domain.

### MLOps

MLOps builds on the DevOps foundation but adapts it to the specific demands of ML system development and deployment. While DevOps focuses on integrating and delivering deterministic software, MLOps must manage non-deterministic, data-dependent workflows. These workflows span data acquisition, preprocessing, model training, evaluation, deployment, and continuous monitoring.

Several recurring challenges in operationalizing machine learning motivated the emergence of MLOps as a distinct discipline. One major concern is data drift, where shifts in input data distributions over time degrade model accuracy. This necessitates continuous monitoring and automated retraining procedures. Equally critical is reproducibility—ML workflows often lack standardized mechanisms to track code, datasets, configurations, and environments, making it difficult to reproduce past experiments. The lack of explainability in complex models has further driven demand for tools that increase model transparency and interpretability, particularly in regulated domains.

Post-deployment, many organizations struggle with monitoring model performance in production, especially in detecting silent failures or changes in user behavior. Additionally, the manual overhead involved in retraining and redeploying models creates friction in experimentation and iteration. Finally, configuring and maintaining ML infrastructure is complex and error-prone, highlighting the need for platforms that offer optimized, modular, and reusable infrastructure. Together, these challenges form the foundation for MLOps practices that focus on automation, collaboration, and lifecycle management.

These challenges introduced the need for a new set of tools and workflows tailored to the ML lifecycle. While DevOps primarily unifies software development and IT operations, MLOps requires coordination across a broader set of stakeholders—data scientists, ML engineers, data engineers, and operations teams.

MLOps introduces specialized practices such as [data versioning](https://dvc.org/), [model versioning](https://dvc.org/), and [model monitoring](https://www.fiddler.ai/) that go beyond the scope of DevOps. It emphasizes scalable experimentation, reproducibility, governance, and responsiveness to evolving data conditions.

Table @tbl-mlops summarizes key similarities and differences between DevOps and MLOps:

+----------------------+--------------------------------------------+-------------------------------------------------------+
| Aspect               | DevOps                                     | MLOps                                                 |
+:=====================+:===========================================+:======================================================+
| Objective            | Streamlining software development          | Optimizing the lifecycle of machine learning models   |
|                      | and operations processes                   |                                                       |
+----------------------+--------------------------------------------+-------------------------------------------------------+
| Methodology          | Continuous Integration and Continuous      | Similar to CI/CD but focuses on machine learning      |
|                      | Delivery (CI/CD) for software development  | workflows                                             |
+----------------------+--------------------------------------------+-------------------------------------------------------+
| Primary Tools        | Version control (Git), CI/CD tools         | Data versioning tools, Model training and deployment  |
|                      | (Jenkins, Travis CI), Configuration        | tools, CI/CD pipelines tailored for ML                |
|                      | management (Ansible, Puppet)               |                                                       |
+----------------------+--------------------------------------------+-------------------------------------------------------+
| Primary Concerns     | Code integration, Testing, Release         | Data management, Model versioning, Experiment         |
|                      | management, Automation, Infrastructure     | tracking, Model deployment, Scalability of ML         |
|                      | as code                                    | workflows                                             |
+----------------------+--------------------------------------------+-------------------------------------------------------+
| Typical Outcomes     | Faster and more reliable software releases,| Efficient management and deployment of machine        |
|                      | Improved collaboration between development | learning models, Enhanced collaboration between       |
|                      | and operations teams                       | data scientists and engineers                         |
+----------------------+--------------------------------------------+-------------------------------------------------------+

: Comparison of DevOps and MLOps. {#tbl-mlops .striped .hover}

These distinctions become clearer when examined through practical examples. One such case study—focused on speech recognition—demonstrates the lifecycle of ML deployment and monitoring in action.

:::{#vid-mlops .callout-important title="MLOps in Practice"}

{{< video https://www.youtube.com/watch?v=YJsRD_hU4tc&list=PLkDaE6sCZn6GMoA0wbpJLi3t34Gd8l0aK&index=3 >}}

:::

## MLOps Key Components

The core components of MLOps form an integrated framework that supports the full machine learning lifecycle—from initial development through deployment and long-term maintenance in production. This section synthesizes key ideas such as automation, reproducibility, and monitoring introduced earlier in the book, while also introducing critical new practices, including governance, model evaluation, and cross-team collaboration. Each component plays a distinct role in creating scalable, reliable, and maintainable ML systems. Together, they form a layered architecture—illustrated in @fig-ops-layers—that supports everything from low-level infrastructure to high-level application logic. By understanding how these components interact, practitioners can design systems that are not only performant but also transparent, auditable, and adaptable to changing conditions.

![The MLOps stack, including ML Models, Frameworks, Model Orchestration, Infrastructure, and Hardware, illustrates the end-to-end workflow of MLOps.](images/png/mlops_overview_layers.png){#fig-ops-layers}

### Data Management

In earlier chapters, we examined how data is collected, preprocessed, and transformed into features suitable for model training and inference. Within the context of MLOps, these tasks are formalized and scaled into systematic, repeatable processes that ensure data reliability, traceability, and operational efficiency. Data management, in this setting, extends beyond initial preparation to encompass the continuous handling of data artifacts throughout the lifecycle of a machine learning system.

A foundational aspect of MLOps data management is dataset versioning. Machine learning systems often evolve in tandem with the data on which they are trained. Therefore, it is essential to maintain a clear mapping between specific versions of data and corresponding model iterations. Tools such as [DVC](https://dvc.org/) enable teams to version large datasets alongside code repositories managed by [Git](https://git-scm.com/), ensuring that data lineage is preserved and that experiments are reproducible.

Supervised learning pipelines also require consistent and well-managed annotation workflows. Labeling tools such as [Label Studio](https://labelstud.io/) support scalable, team-based annotation with integrated audit trails and version histories. These capabilities are particularly important in production settings, where labeling conventions may evolve over time or require refinement across multiple iterations of a project.

In operational environments, data must also be stored in a manner that supports secure, scalable, and collaborative access. Cloud-based object storage systems such as [Amazon S3](https://aws.amazon.com/s3/) and [Google Cloud Storage](https://cloud.google.com/storage) offer durability and fine-grained access control, making them well-suited for managing both raw and processed data artifacts. These systems frequently serve as the foundation for downstream analytics, model development, and deployment workflows.

To transition from raw data to analysis- or inference-ready formats, MLOps teams construct automated data pipelines. These pipelines perform structured tasks such as data ingestion, schema validation, deduplication, transformation, and loading. Orchestration tools including [Apache Airflow](https://airflow.apache.org/), [Prefect](https://www.prefect.io/), and [dbt](https://www.getdbt.com/) are commonly used to define and manage these workflows. When managed as code, pipelines support versioning, modularity, and integration with CI/CD systems.

An increasingly important element of the MLOps data infrastructure is the feature store. Feature stores, such as [Feast](https://feast.dev/) and [Tecton](https://www.tecton.ai/), provide a centralized repository for storing and retrieving engineered features. These systems serve both batch and online use cases, ensuring that models access the same feature definitions during training and inference, thereby improving consistency and reducing data leakage.

Consider a predictive maintenance application in an industrial setting. A continuous stream of sensor data is ingested and joined with historical maintenance logs through a scheduled pipeline managed in Airflow. The resulting features—such as rolling averages or statistical aggregates—are stored in a feature store for both retraining and low-latency inference. This pipeline is versioned, monitored, and integrated with the model registry, enabling full traceability from data to deployed model predictions.

Effective data management in MLOps is not limited to ensuring data quality. It also establishes the operational backbone that enables model reproducibility, auditability, and sustained deployment at scale. Without robust data management, the integrity of downstream training, evaluation, and serving processes cannot be maintained.

:::{#vid-datapipe .callout-important title="Data Pipelines"}

{{< video https://www.youtube.com/watch?v=gz-44N3MMOA&list=PLkDaE6sCZn6GMoA0wbpJLi3t34Gd8l0aK&index=33 >}}

:::

### CI/CD Pipelines

In conventional software systems, continuous integration and continuous delivery (CI/CD) pipelines are essential for ensuring that code changes can be tested, validated, and deployed efficiently. In the context of machine learning systems, CI/CD pipelines are adapted to handle additional complexities introduced by data dependencies, model training workflows, and artifact versioning. These pipelines provide a structured mechanism to transition ML models from development into production in a reproducible, scalable, and automated manner.

A typical ML CI/CD pipeline consists of several coordinated stages, including: checking out updated code, preprocessing input data, training a candidate model, validating its performance, packaging the model, and deploying it to a serving environment. In some cases, pipelines also include triggers for automatic retraining based on data drift or performance degradation. By codifying these steps, CI/CD pipelines reduce manual intervention, enforce quality checks, and support continuous improvement of deployed systems.

A wide range of tools is available for implementing ML-focused CI/CD workflows. General-purpose CI/CD orchestrators such as [Jenkins](https://www.jenkins.io/), [CircleCI](https://circleci.com/), and [GitHub Actions](https://github.com/features/actions) are commonly used to manage version control events and execution logic. These tools are frequently integrated with domain-specific platforms such as [Kubeflow](https://www.kubeflow.org/), [Metaflow](https://metaflow.org/), and [Prefect](https://www.prefect.io/), which offer higher-level abstractions for managing ML tasks and workflows.

Figure @fig-ops-cicd illustrates a representative CI/CD pipeline for machine learning systems. The process begins with a dataset and feature repository, from which data is ingested and validated. Validated data is then transformed for model training. A retraining trigger, such as a scheduled job or performance threshold, may initiate this process automatically. Once training and hyperparameter tuning are complete, the resulting model undergoes evaluation against predefined criteria. If the model satisfies the required thresholds, it is registered in a model repository along with metadata, performance metrics, and lineage information. Finally, the model is deployed back into the production system, closing the loop and enabling continuous delivery of updated models.

![MLOps CI/CD diagram. Source: HarvardX.](images/png/cicd_pipelines.png){#fig-ops-cicd}

As a practical example, consider an image classification model under active development. When a data scientist commits changes to a [GitHub](https://github.com/) repository, a Jenkins pipeline is triggered. The pipeline fetches the latest data, performs preprocessing, and initiates model training. Experiments are tracked using [MLflow](https://mlflow.org/), which logs metrics and stores model artifacts. After passing automated evaluation tests, the model is containerized and deployed to a staging environment using [Kubernetes](https://kubernetes.io/). If the model meets validation criteria in staging, the pipeline orchestrates a [canary deployment](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments), gradually routing production traffic to the new model while monitoring key metrics for anomalies. In case of performance regressions, the system can automatically revert to a previous model version.

CI/CD pipelines play a central role in enabling scalable, repeatable, and safe deployment of machine learning models. By unifying the disparate stages of the ML workflow under continuous automation, these pipelines support faster iteration, improved reproducibility, and greater resilience in production systems. In mature MLOps environments, CI/CD is not an optional layer, but a foundational capability that transforms ad hoc experimentation into a structured and operationally sound development process.

### Training Pipelines

Model training is a central phase in the machine learning lifecycle, where algorithms are optimized to learn patterns from data. In prior chapters, we introduced the fundamentals of model development and training workflows, including architecture selection, hyperparameter tuning, and evaluation. Within an MLOps context, these activities are reframed as part of a reproducible, scalable, and automated pipeline that supports continual experimentation and reliable production deployment.

Modern machine learning frameworks such as [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), and [Keras](https://keras.io/) provide modular components for building and training models. These libraries include high-level abstractions for layers, activation functions, loss metrics, and optimizers, enabling practitioners to prototype and iterate efficiently. When embedded into MLOps pipelines, these frameworks serve as the foundation for training processes that can be systematically scaled, tracked, and retrained.

Reproducibility is a key objective of MLOps. Training scripts and configurations are version-controlled using tools like [Git](https://git-scm.com/) and hosted on platforms such as [GitHub](https://github.com/). Interactive development environments, including [Jupyter](https://jupyter.org/) notebooks, are commonly used to encapsulate data ingestion, feature engineering, training routines, and evaluation logic in a unified format. These notebooks can be integrated into automated pipelines, allowing the same logic used for local experimentation to be reused for scheduled retraining in production systems.

Automation further enhances model training by reducing manual effort and standardizing critical steps. MLOps workflows often incorporate techniques such as [hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview), [neural architecture search](https://arxiv.org/abs/1808.05377), and [automatic feature selection](https://scikit-learn.org/stable/modules/feature_selection.html) to explore the design space efficiently. These tasks are orchestrated using CI/CD pipelines, which automate data preprocessing, model training, evaluation, registration, and deployment. For instance, a Jenkins pipeline may trigger a retraining job when new labeled data becomes available. The resulting model is evaluated against baseline metrics, and if performance thresholds are met, it is deployed automatically.

The increasing availability of cloud-based infrastructure has further expanded the reach of model training. Cloud providers offer managed services that provision high-performance computing resources—including GPU and TPU accelerators—on demand. Depending on the platform, teams may construct their own training workflows or rely on fully managed services such as [Vertex AI Fine Tuning](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models), which support automated adaptation of foundation models to new tasks. Nonetheless, hardware availability, regional access restrictions, and cost constraints remain important considerations when designing cloud-based training systems.

As an illustrative example, consider a data scientist developing a convolutional neural network (CNN) for image classification using a PyTorch notebook. The [fastai](https://www.fast.ai/) library is used to simplify model construction and training. The notebook trains the model on a labeled dataset, computes performance metrics, and tunes hyperparameters such as learning rate and architecture depth. Once validated, the training script is version-controlled and incorporated into a retraining pipeline that is periodically triggered based on data updates or model performance monitoring.

Through standardized workflows, versioned environments, and automated orchestration, MLOps enables the model training process to transition from ad hoc experimentation to a robust, repeatable, and scalable system. This not only accelerates development but also ensures that trained models meet production standards for reliability, traceability, and performance.

### Model Validation

Before a machine learning model is deployed into production, it must undergo rigorous evaluation to ensure that it meets predefined performance, robustness, and reliability criteria. While earlier chapters discussed evaluation in the context of model development, MLOps reframes evaluation as a structured and repeatable process for validating operational readiness. It incorporates practices that support pre-deployment assessment, post-deployment monitoring, and automated regression testing.

The evaluation process typically begins with performance testing against a holdout test set—a dataset not used during training or validation. This dataset is sampled from the same distribution as production data and is used to measure generalization. Core metrics such as [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision), [area under the curve (AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve), [precision](https://en.wikipedia.org/wiki/Precision_and_recall), [recall](https://en.wikipedia.org/wiki/Precision_and_recall), and [F1 score](https://en.wikipedia.org/wiki/F1_score) are computed to quantify model performance. These metrics are not only used at a single point in time but also tracked longitudinally to detect degradation, such as that caused by [data drift](https://www.ibm.com/cloud/learn/data-drift), where shifts in input distributions can reduce model accuracy over time.

Beyond static evaluation, MLOps encourages controlled deployment strategies that simulate production conditions while minimizing risk. One widely adopted method is [canary testing](https://martinfowler.com/bliki/CanaryRelease.html), in which the new model is deployed to a small fraction of users or queries. During this limited rollout, live performance metrics are monitored to assess system stability and user impact. For instance, an e-commerce platform may deploy a new recommendation model to 5% of web traffic and observe metrics such as click-through rate, latency, and prediction accuracy. Only after the model demonstrates consistent and reliable performance is it promoted to full production.

Cloud-based ML platforms further support model evaluation by enabling experiment logging, request replay, and synthetic test case generation. These capabilities allow teams to evaluate different models under identical conditions, facilitating comparisons and root-cause analysis. Tools such as [Weights and Biases](https://wandb.ai/) automate aspects of this process by capturing training artifacts, recording hyperparameter configurations, and visualizing performance metrics across experiments. These tools integrate directly into training and deployment pipelines, improving transparency and traceability.

While automation is central to MLOps evaluation practices, human oversight remains essential. Automated tests may fail to capture nuanced performance issues, such as poor generalization on rare subpopulations or shifts in user behavior. Therefore, teams often combine quantitative evaluation with qualitative review, particularly for models deployed in high-stakes or regulated environments.

In summary, model evaluation within MLOps is a multi-stage process that bridges offline testing and live system monitoring. It ensures that models not only meet technical benchmarks but also behave predictably and responsibly under real-world conditions. These evaluation practices reduce deployment risk and help maintain the reliability of machine learning systems over time.

### Model Deployment

Teams need to properly package, test, and track ML models to reliably deploy them to production. MLOps introduces frameworks and procedures for actively versioning, deploying, monitoring, and updating models in sustainable ways.

One common approach to deployment involves containerizing models using tools like [Docker](https://www.docker.com/), which package code, libraries, and dependencies into standardized units. Containers ensure smooth portability across environments, making deployment consistent and predictable. Frameworks like [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) and [BentoML](https://bentoml.org/) help serve predictions from deployed models via performance-optimized APIs. These frameworks handle versioning, scaling, and monitoring.

Before full-scale rollout, teams deploy updated models to staging or QA environments to rigorously test performance. Techniques such as shadow or canary deployments are used to validate new models incrementally. For instance, canary deployments route a small percentage of traffic to the new model while closely monitoring performance. If no issues arise, traffic to the new model gradually increases. Robust rollback procedures are essential to handle unexpected issues, reverting systems to the previous stable model version to ensure minimal disruption. Integration with CI/CD pipelines further automates the deployment and rollback process, enabling efficient iteration cycles.

To maintain lineage and auditability, teams track model artifacts, including scripts, weights, logs, and metrics, using tools like [MLflow](https://mlflow.org/). Model registries, such as [Vertex AI's model registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction), act as centralized repositories for storing and managing trained models. These registries not only facilitate version comparisons but also often include access to base models, which may be open source, proprietary, or a hybrid (e.g., [LLAMA](https://ai.meta.com/llama/)). Deploying a model from the registry to an inference endpoint is streamlined, handling resource provisioning, model weight downloads, and hosting.

Inference endpoints typically expose the deployed model via REST APIs for real-time predictions. Depending on performance requirements, teams can configure resources, such as GPU accelerators, to meet latency and throughput targets. Some providers also offer flexible options like serverless or batch inference, eliminating the need for persistent endpoints and enabling cost-efficient, scalable deployments. For example, [AWS SageMaker Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) supports such configurations. By leveraging these tools and practices, teams can deploy ML models resiliently, ensuring smooth transitions between versions, maintaining production stability, and optimizing performance across diverse use cases.

### Inference Serving

Once a model has been deployed, the final stage in operationalizing machine learning is to make it accessible to downstream applications or end-users. Serving infrastructure provides the interface between trained models and real-world systems, enabling predictions to be delivered reliably and efficiently. In large-scale settings, such as social media platforms or e-commerce services, serving systems may process tens of trillions of inference queries per day [@wu2019machine]. Meeting such demand requires careful design to balance latency, scalability, and robustness.

To address these challenges, production-grade serving frameworks have emerged. Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving), [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server), and [KServe](https://kserve.github.io/website/latest/) provide standardized mechanisms for deploying, versioning, and scaling machine learning models across heterogeneous infrastructure. These frameworks abstract many of the lower-level concerns, allowing teams to focus on system behavior, integration, and performance targets.

Model serving architectures are typically designed around three broad paradigms:

1. Online Serving, which provides low-latency, real-time predictions for interactive systems such as recommendation engines or fraud detection.
2. Offline Serving, which processes large batches of data asynchronously, typically in scheduled jobs used for reporting or model retraining.
3. Near-Online (Semi-Synchronous) Serving, which offers a balance between latency and throughput, appropriate for scenarios like chatbots or semi-interactive analytics.

Each of these approaches introduces different constraints in terms of availability, responsiveness, and throughput. Serving systems are therefore constructed to meet specific Service Level Agreements (SLAs) and Service Level Objectives (SLOs), which quantify acceptable performance boundaries along dimensions such as latency, error rates, and uptime. Achieving these goals requires a range of optimizations in request handling, scheduling, and resource allocation.

A number of serving system design strategies are commonly employed to meet these requirements. Request scheduling and batching aggregate inference requests to improve throughput and hardware utilization. For instance, Clipper [@crankshaw2017clipper] applies batching and caching to reduce response times in online settings. Model instance selection and routing dynamically assign requests to model variants based on system load or user-defined constraints; INFaaS [@romero2021infaas] illustrates this approach by optimizing accuracy-latency trade-offs across variant models.

To ensure high availability, load balancing distributes inference traffic evenly across replicas, as demonstrated in MArk [@zhang2019mark], which improves utilization in heterogeneous environments. In addition, autoscaling mechanisms adjust the number of active model instances based on real-time demand, helping systems like INFaaS and MArk respond to workload fluctuations without overprovisioning.

In more complex inference scenarios, model orchestration coordinates the execution of multi-stage models or distributed components. AlpaServe [@li2023alpaserve] exemplifies this by enabling efficient serving of large foundation models through coordinated resource allocation. Finally, execution time prediction enables systems to anticipate latency for individual requests. Clockwork [@gujarati2020serving] uses this capability to reduce tail latency and improve scheduling efficiency under high load.

While these systems differ in implementation, they collectively illustrate the critical techniques that underpin scalable and responsive ML-as-a-Service infrastructure. @tbl-serving-techniques summarizes these strategies and highlights representative systems that implement them.

+------------------------------+--------------------------------------------------------------------------------+------------------------+
| Technique                | Description                                                                | Example System     |
+:=============================+:===============================================================================+:=======================+
| Request Scheduling & Batching| Groups inference requests to improve throughput and reduce overhead            | Clipper                |
+------------------------------+--------------------------------------------------------------------------------+------------------------+
| Instance Selection & Routing | Dynamically assigns requests to model variants based on constraints            | INFaaS                 |
+------------------------------+--------------------------------------------------------------------------------+------------------------+
| Load Balancing               | Distributes traffic across replicas to prevent bottlenecks                     | MArk                   |
+------------------------------+--------------------------------------------------------------------------------+------------------------+
| Autoscaling                  | Adjusts model instances to match workload demands                              | INFaaS, MArk           |
+------------------------------+--------------------------------------------------------------------------------+------------------------+
| Model Orchestration          | Coordinates execution across model components or pipelines                     | AlpaServe              |
+------------------------------+--------------------------------------------------------------------------------+------------------------+
| Execution Time Prediction    | Forecasts latency to optimize request scheduling                               | Clockwork              |
+------------------------------+--------------------------------------------------------------------------------+------------------------+

: Serving system techniques and example implementations. {#tbl-serving-techniques .striped .hover}

Together, these strategies form the foundation of robust model serving systems. When effectively integrated, they enable machine learning applications to meet performance targets while maintaining system-level efficiency and scalability.

### Infrastructure Management

Scalable, resilient infrastructure is a foundational requirement for operationalizing machine learning systems. As models move from experimentation to production, MLOps teams must ensure that the underlying computational resources can support continuous integration, large-scale training, automated deployment, and real-time inference. This requires managing infrastructure not as static hardware, but as a dynamic, programmable, and versioned system.

To achieve this, teams adopt the practice of Infrastructure as Code (IaC), which allows infrastructure to be defined, deployed, and maintained using declarative configuration files. Tools such as [Terraform](https://www.terraform.io/), [AWS CloudFormation](https://aws.amazon.com/cloudformation/), and [Ansible](https://www.ansible.com/) support this paradigm by enabling teams to version infrastructure definitions alongside application code. In MLOps settings, Terraform is widely used to provision and manage resources across public cloud platforms such as [AWS](https://aws.amazon.com/), [Google Cloud Platform](https://cloud.google.com/), and [Microsoft Azure](https://azure.microsoft.com/).

Infrastructure management spans the full lifecycle of ML systems. During model training, teams use IaC scripts to allocate compute instances with GPU or TPU accelerators, configure distributed storage, and deploy container clusters. These configurations ensure that data scientists and ML engineers can access reproducible environments with the required computational capacity. Because infrastructure definitions are stored as code, they can be audited, reused, and integrated into CI/CD pipelines to ensure consistency across environments.

Containerization plays a critical role in making ML workloads portable and consistent. Tools like [Docker](https://www.docker.com/) encapsulate models and their dependencies into isolated units, while orchestration systems such as [Kubernetes](https://kubernetes.io/) manage containerized workloads across clusters. These systems enable rapid deployment, resource allocation, and scaling—capabilities that are essential in production environments where workloads can vary dynamically.

To handle changes in workload intensity—such as spikes during hyperparameter tuning or surges in prediction traffic—teams rely on cloud elasticity and autoscaling. Cloud platforms support on-demand provisioning and horizontal scaling of infrastructure resources. [Autoscaling mechanisms](https://aws.amazon.com/autoscaling/) automatically adjust compute capacity based on usage metrics, enabling teams to optimize for both performance and cost-efficiency.

Importantly, infrastructure in MLOps is not limited to the cloud. Many deployments span on-premises, cloud, and edge environments, depending on latency, privacy, or regulatory constraints. A robust infrastructure management strategy must accommodate this diversity by offering flexible deployment targets and consistent configuration management across environments.

To illustrate, consider a scenario in which a team uses Terraform to deploy a Kubernetes cluster on Google Cloud Platform. The cluster is configured to host containerized TensorFlow models that serve predictions via HTTP APIs. As user demand increases, Kubernetes automatically scales the number of pods to handle the load. Meanwhile, CI/CD pipelines update the model containers based on retraining cycles, and monitoring tools track cluster performance, latency, and resource utilization. All infrastructure components—from network configurations to compute quotas—are managed as version-controlled code, ensuring reproducibility and auditability.

By adopting Infrastructure as Code, leveraging cloud-native orchestration, and supporting automated scaling, MLOps teams gain the ability to provision and maintain the resources required for machine learning at production scale. This infrastructure layer underpins the entire MLOps stack, enabling reliable training, deployment, and serving workflows.

### Monitoring Systems

Monitoring is a critical function in MLOps, enabling teams to maintain operational visibility over machine learning systems deployed in production. Once a model is live, it becomes exposed to real-world inputs, evolving data distributions, and shifting user behavior. Without continuous monitoring, it becomes difficult to detect performance degradation, data quality issues, or system failures in a timely manner.

Effective monitoring spans both model behavior and infrastructure performance. On the model side, teams track metrics such as accuracy, precision, recall, and the [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) using live or sampled predictions. By evaluating these metrics over time, they can detect whether the model's performance remains stable or begins to drift.

One of the primary risks in production ML systems is model drift—a gradual decline in model performance as the input data distribution or the relationship between inputs and outputs changes. Drift manifests in two main forms:

- Concept drift occurs when the underlying relationship between features and targets evolves. For example, during the COVID-19 pandemic, purchasing behavior shifted dramatically, invalidating many previously accurate recommendation models.
- Data drift refers to shifts in the input data distribution itself. In applications such as self-driving cars, this may result from seasonal changes in weather, lighting, or road conditions, all of which affect the model's inputs.

In addition to model-level monitoring, infrastructure-level monitoring tracks indicators such as CPU and GPU utilization, memory and disk consumption, network latency, and service availability. These signals help ensure that the system remains performant and responsive under varying load conditions. Tools such as [Prometheus](https://prometheus.io/), [Grafana](https://grafana.com/), and [Elastic](https://www.elastic.co/) are widely used to collect, aggregate, and visualize operational metrics. These tools often integrate into dashboards that offer real-time and historical views of system behavior.

Proactive alerting mechanisms are configured to notify teams when anomalies or threshold violations occur. For example, a sustained drop in model accuracy may trigger an alert to investigate potential drift, prompting retraining with updated data. Similarly, infrastructure alerts can signal memory saturation or degraded network performance, allowing engineers to take corrective action before failures propagate.

Ultimately, robust monitoring enables teams to detect problems before they escalate, maintain high service availability, and preserve the reliability and trustworthiness of machine learning systems. In the absence of such practices, models may silently degrade or systems may fail under load, undermining the effectiveness of the ML pipeline as a whole.

:::{#vid-monitoring .callout-important title="Model Monitoring"}

{{< video https://www.youtube.com/watch?v=hq_XyP9y0xg&list=PLkDaE6sCZn6GMoA0wbpJLi3t34Gd8l0aK&index=7 >}}

:::

### Model Governance

As machine learning systems become increasingly embedded in decision-making processes, governance has emerged as a critical pillar of MLOps. Governance refers to the policies, practices, and tools used to ensure that models are transparent, fair, accountable, and compliant with ethical standards and regulatory requirements. Without proper governance, deployed models may produce biased or opaque decisions, leading to significant legal, reputational, and societal risks.

Governance begins during the model development phase, where teams implement techniques to increase transparency and explainability. For example, methods such as [SHAP](https://github.com/slundberg/shap) and [LIME](https://github.com/marcotcr/lime) offer post hoc explanations of model predictions by identifying which input features were most influential in a particular decision. These techniques allow auditors, developers, and non-technical stakeholders to better understand how and why a model behaves the way it does.

In addition to interpretability, fairness is a central concern in governance. Bias detection tools analyze model outputs across different demographic groups—such as those defined by age, gender, or ethnicity—to identify disparities in performance. For instance, a model used for loan approval should not systematically disadvantage certain populations. MLOps teams employ pre-deployment audits on curated, representative datasets to evaluate fairness, robustness, and overall model behavior before a system is put into production.

Governance also extends into the post-deployment phase. As introduced in the previous section on monitoring, teams must track for concept drift, where the statistical relationships between features and labels evolve over time. Such drift can undermine the fairness or accuracy of a model, particularly if the shift disproportionately affects a specific subgroup. By analyzing logs and user feedback, teams can identify recurring failure modes, unexplained model outputs, or emerging disparities in treatment across user segments.

Supporting this lifecycle approach to governance are platforms and toolkits that integrate governance functions into the broader MLOps stack. For example, [Watson OpenScale](https://www.ibm.com/cloud/watson-openscale) provides built-in modules for explainability, bias detection, and monitoring. These tools allow governance policies to be encoded as part of automated pipelines, ensuring that checks are consistently applied throughout development, evaluation, and production.

Ultimately, governance focuses on three core objectives: transparency, fairness, and compliance. Transparency ensures that models are interpretable and auditable. Fairness promotes equitable treatment across user groups. Compliance ensures alignment with legal and organizational policies. Embedding governance practices throughout the MLOps lifecycle transforms machine learning from a technical artifact into a trustworthy system capable of serving societal and organizational goals.

### Cross-Functional Collaboration

Machine learning systems are developed and maintained by multidisciplinary teams, including data scientists, ML engineers, software developers, infrastructure specialists, product managers, and compliance officers. As these roles span different domains of expertise, effective communication and collaboration are essential to ensure alignment, efficiency, and system reliability. MLOps fosters this cross-functional integration by introducing shared tools, processes, and artifacts that promote transparency and coordination across the machine learning lifecycle.

Collaboration begins with consistent tracking of experiments, model versions, and metadata. Tools such as [MLflow](https://mlflow.org/) provide a structured environment for logging experiments, capturing parameters, recording evaluation metrics, and managing trained models through a centralized registry. This registry serves as a shared reference point for all team members, enabling reproducibility and easing handoff between roles. Integration with version control systems such as [GitHub](https://github.com/) and [GitLab](https://about.gitlab.com/) further streamlines collaboration by linking code changes with model updates and pipeline triggers.

In addition to tracking infrastructure, teams benefit from platforms that support exploratory collaboration. [Weights & Biases](https://wandb.ai/) is one such platform that allows data scientists to visualize experiment metrics, compare training runs, and share insights with peers. Features such as live dashboards and experiment timelines facilitate discussion and decision-making around model improvements, hyperparameter tuning, or dataset refinements. These collaborative environments reduce friction in model development by making results interpretable and reproducible across the team.

Beyond model tracking, collaboration also depends on shared understanding of data semantics and usage. Establishing common data contexts—through glossaries, [data dictionaries](https://en.wikipedia.org/wiki/Data_dictionary), schema references, and lineage documentation—ensures that all stakeholders interpret features, labels, and statistics consistently. This is particularly important in large organizations, where data pipelines may evolve independently across teams or departments.

For example, a data scientist working on an anomaly detection model may use Weights & Biases to log experiment results and visualize performance trends. These insights are shared with the broader team to inform feature engineering decisions. Once the model reaches an acceptable performance threshold, it is registered in MLflow along with its metadata and training lineage. This allows an ML engineer to pick up the model for deployment without ambiguity about its provenance or configuration.

By integrating collaborative tools, standardized documentation, and transparent experiment tracking, MLOps removes communication barriers that have traditionally slowed down ML workflows. It enables distributed teams to operate cohesively, accelerating iteration cycles and improving the reliability of deployed systems.

:::{#vid-deploy .callout-important title="Deployment Challenges"}

{{< video https://www.youtube.com/watch?v=UyEtTyeahus&list=PLkDaE6sCZn6GMoA0wbpJLi3t34Gd8l0aK&index=5 >}}

:::

## Hidden Technical Debt

As machine learning systems mature and scale, they often accumulate technical debt—the long-term cost of expedient design decisions made during development. Originally proposed in software engineering in the 1990s, the technical debt metaphor compares shortcuts in implementation to financial debt: it may enable short-term velocity, but requires ongoing interest payments in the form of maintenance, refactoring, and systemic risk. While some debt is strategic and manageable, uncontrolled technical debt can inhibit flexibility, slow iteration, and introduce brittleness into production systems.

In machine learning, technical debt takes on new and less visible forms, arising not only from software abstractions but also from data dependencies, model entanglement, feedback loops, and evolving operational environments. The complexity of ML systems—spanning data ingestion, feature extraction, training pipelines, and deployment infrastructure—makes them especially prone to hidden forms of debt.

@fig-technical-debt provides a conceptual overview of the relative size and interdependence of components in an ML system. The small black box in the center represents the model code itself—a surprisingly small portion of the overall system. Surrounding it are much larger components: configuration, data collection, and feature engineering. These areas, though often overlooked, are critical to system functionality and are major sources of technical debt when poorly designed or inconsistently maintained.

![ML system components. Source: @sculley2015hidden](images/png/hidden_debt.png){#fig-technical-debt}

The sections that follow describe key categories of technical debt unique to ML systems. Each subsection highlights common sources, illustrative examples, and potential mitigations. While some forms of debt may be unavoidable during early development, understanding their causes and impact is essential for building robust and maintainable ML systems.

### Boundary Erosion

In traditional software systems, modularity and abstraction provide clear boundaries between components, allowing changes to be isolated and behavior to remain predictable. Machine learning systems, in contrast, tend to blur these boundaries. The interactions between data pipelines, feature engineering, model training, and downstream consumption often lead to tightly coupled components with poorly defined interfaces.

This erosion of boundaries makes ML systems particularly vulnerable to cascading effects from even minor changes. A seemingly small update to a preprocessing step or feature transformation can propagate through the system in unexpected ways, breaking assumptions made elsewhere in the pipeline. This lack of encapsulation increases the risk of entanglement, where dependencies between components become so intertwined that local modifications require global understanding and coordination.

One manifestation of this problem is known as CACE---“Changing Anything Changes Everything.” When systems are built without strong boundaries, adjusting a feature encoding, model hyperparameter, or data selection criterion can affect downstream behavior in unpredictable ways. This inhibits iteration and makes testing and validation more complex. For example, changing the binning strategy of a numerical feature may cause a previously tuned model to underperform, triggering retraining and downstream evaluation changes.

To mitigate boundary erosion, teams should prioritize architectural practices that support modularity and encapsulation. Designing components with well-defined interfaces allows teams to isolate faults, reason about changes, and reduce the risk of system-wide regressions. For instance, clearly separating data ingestion from feature engineering, and feature engineering from modeling logic, introduces layers that can be independently validated, monitored, and maintained.

Boundary erosion is often invisible in early development but becomes a significant burden as systems scale or require adaptation. Proactive design decisions that preserve abstraction and limit interdependencies are essential to managing complexity and avoiding long-term maintenance costs.

### Correction Cascades

As machine learning systems evolve, they often undergo iterative refinement to address performance issues, accommodate new requirements, or adapt to environmental changes. In well-engineered systems, such updates are localized and managed through modular changes. However, in ML systems, even small adjustments can trigger **correction cascades**—a sequence of dependent fixes that propagate backward and forward through the workflow.

@fig-correction-cascades-flowchart illustrates how these cascades emerge across different stages of the ML lifecycle, from problem definition and data collection to model development and deployment. Each arc represents a corrective action, and the colors indicate different sources of instability, including inadequate domain expertise, brittle real-world interfaces, misaligned incentives, and insufficient documentation. The red arrows represent cascading revisions, while the dotted arrow at the bottom highlights a full system restart—a drastic but sometimes necessary outcome.

![Correction cascades flowchart. Source: @sculley2015hidden.](images/png/data_cascades.png){#fig-correction-cascades-flowchart}

One common source of correction cascades is **sequential model development**—reusing or fine-tuning existing models to accelerate development for new tasks. While this strategy is often efficient, it can introduce hidden dependencies that are difficult to unwind later. Assumptions baked into earlier models become implicit constraints for future models, limiting flexibility and increasing the cost of downstream corrections.

Consider a scenario where a team fine-tunes a customer churn prediction model for a new product. The original model may embed product-specific behaviors or feature encodings that are not valid in the new setting. As performance issues emerge, teams may attempt to patch the model, only to discover that the true problem lies several layers upstream—perhaps in the original feature selection or labeling criteria.

To avoid or reduce the impact of correction cascades, teams must make careful tradeoffs between reuse and redesign. Several factors influence this decision. For small, static datasets, fine-tuning may be appropriate. For large or rapidly evolving datasets, retraining from scratch provides greater control and adaptability. Fine-tuning also requires fewer computational resources, making it attractive in constrained settings. However, modifying foundational components later becomes extremely costly due to these cascading effects.

Therefore, careful consideration should be given to introducing fresh model architectures, even if resource-intensive, to avoid correction cascades down the line. This approach may help mitigate the amplifying effects of issues downstream and reduce technical debt. However, there are still scenarios where sequential model building makes sense, necessitating a thoughtful balance between efficiency, flexibility, and long-term maintainability in the ML development process.

These early forms of technical debt—particularly boundary erosion and correction cascades—often trigger broader system-wide fragility. The resulting dependencies, inconsistencies, and feedback dynamics tend to accumulate as systems evolve. To better understand how different categories of technical debt interact and propagate across the ML lifecycle, @fig-techdebt-flow provides a conceptual overview of the flow of debt in machine learning systems.

::: {#fig-techdebt-flow fig-env="figure" fig-pos="htb"}

```{.tikz}

\begin{tikzpicture}[
  box/.style={rectangle, minimum width=2.6cm, minimum height=1cm, text centered, draw, thick},
  floatbox/.style={box, draw=violet!60, fill=violet!5},
  quantbox/.style={box, draw=gray!60, fill=gray!20},
  multbox/.style={box, draw=cyan!60, fill=cyan!15},
  intbox/.style={box, draw=pink!80!red, fill=pink!10},
  greenbox/.style={box, draw=green!60!black, fill=green!10},
  actbox/.style={box, draw=brown!70!black, fill=brown!10},
  arrow/.style={thick, -{Latex[length=2mm]}}
]

% Nodes
\node[floatbox] (floatin) {Float input};
\node[quantbox, right=1.5cm of floatin] (q1) {Quantization};
\node[multbox, below=1.2cm of q1] (matmul) {Matrix\\Multiplication};
\node[intbox, right=2.5cm of matmul] (int32) {Int32 Output};
\node[greenbox, right=1.5cm of int32] (q2) {Quantization};
\node[actbox, right=1.5cm of q2] (act) {Activation};
\node[floatbox, right=1.5cm of act] (floatout) {Float 16 Output};

% Arrows
\draw[arrow] (floatin) -- (q1);
\draw[arrow] (q1) -- node[right]{\small Int8} (matmul);
\draw[arrow] (matmul) -- (int32);
\draw[arrow] (int32) -- (q2);
\draw[arrow] (q2) -- node[above]{\small Float 16} (act);
\draw[arrow] (act) -- (floatout);

\end{tikzpicture}

```

:::

As the figure illustrates, decisions made during early exploration and system assembly can affect later stages such as data pipeline stability, configuration management, and real-world system behavior. Many of these issues are interdependent: for example, data dependency debt can amplify feedback loop risk, and poor configuration hygiene can lead to fragile adaptation mechanisms. The remainder of this section will follow the flow of this diagram to examine how debt evolves, interacts, and can ultimately be mitigated.

### Undeclared Consumers

Machine learning systems often provide predictions or outputs that serve as inputs to other services, pipelines, or downstream models. In traditional software, these connections are typically made explicit through APIs, service contracts, or documented dependencies. In ML systems, however, it is common for model outputs to be consumed by undeclared consumers—downstream components that rely on predictions without being formally tracked or validated.

This lack of visibility introduces a subtle but serious form of technical debt. Because these consumers are not declared or governed by explicit interfaces, updates to the model—such as changes in output format, semantics, or feature behavior—can silently break downstream functionality. The original model was not designed with these unknown consumers in mind, so its evolution risks unintended consequences across the broader system.

The situation becomes more problematic when these downstream consumers feed back into the original model's training data. This introduces feedback loops that are difficult to detect and nearly impossible to reason about analytically. For instance, if a model’s output is used in a recommendation system and user behavior is influenced by those recommendations, future training data becomes contaminated by earlier predictions. Such loops can distort model behavior, create self-reinforcing biases, and mask performance regressions.

One example might involve a credit scoring model whose outputs are consumed by a downstream eligibility engine. If the eligibility system later influences which applicants are accepted—and that in turn affects the label distribution in the next training cycle—the model is now shaping the very data on which it will be retrained.

To mitigate the risks associated with undeclared consumers, teams should begin by implementing strict access controls to limit who or what can consume model outputs. Rather than making predictions broadly available, systems should expose outputs only through well-defined interfaces, ensuring that their use can be monitored and audited. In addition, establishing formal interface contracts—including documented schemas, value ranges, and semantic expectations—helps enforce consistent behavior across components and reduces the likelihood of misinterpretation. Monitoring and logging mechanisms can provide visibility into where and how predictions are used, revealing dependencies that may not have been anticipated during development. Finally, architectural decisions should emphasize system boundaries that encapsulate model behavior, thereby isolating changes and minimizing the risk of downstream entanglement. Together, these practices support a more disciplined and transparent approach to system integration, reducing the likelihood of costly surprises as ML systems evolve.

### Data Dependency Debt

Machine learning systems rely heavily on data pipelines that ingest, transform, and deliver training and inference inputs. Over time, these pipelines often develop implicit and unstable dependencies that become difficult to trace, validate, or manage—leading to what is known as data dependency debt. This form of debt is particularly challenging because it tends to accumulate silently and may only become visible when a downstream model fails unexpectedly due to changes in upstream data.

In traditional software systems, compilers, static analysis tools, and dependency checkers help engineers track and manage code-level dependencies. These tools enable early detection of unused imports, broken interfaces, and type mismatches. However, ML systems typically lack equivalent tooling for analyzing data dependencies, which include everything from feature generation scripts and data joins to external data sources and labeling conventions. Without such tools, changes to even a single feature or schema can ripple across a system without warning.

Two common forms of data dependency debt are unstable inputs and underutilized inputs. Unstable inputs refer to data sources that change over time—either in content, structure, or availability—leading to inconsistent model behavior. A model trained on one version of a feature may produce unexpected results when that feature's distribution or encoding changes. Underutilized inputs refer to data elements included in training pipelines that have little or no impact on model performance. These features increase complexity, slow down processing, and increase the surface area for bugs, yet provide little return on investment.

One approach to managing unstable dependencies is to implement robust data versioning. By tracking which data snapshot was used for training a given model, teams can reproduce results and isolate regressions. However, versioning also introduces overhead: multiple versions must be stored, managed, and tested for staleness. For underutilized inputs, a common strategy is to run leave-one-feature-out evaluations, where features are systematically removed to assess their contribution to model performance. This analysis can guide decisions about whether to simplify the feature set or deprecate unused data streams.

Addressing data dependency debt requires both architectural discipline and appropriate tooling. ML systems must be designed with traceability in mind—recording not just what data was used, but where it came from, how it was transformed, and how it affected model behavior. For example, consider an e-commerce platform that includes a "days since last login" feature in its churn prediction model. If the meaning of this feature changes—say, due to a platform redesign that automatically logs users in through a companion app—the input distribution will shift, potentially degrading model performance. Without explicit tracking and validation of this data dependency, the issue might go unnoticed until accuracy metrics decline in production. As systems scale, unexamined data dependencies like these become a major source of brittleness and drift. Investing in structured data practices early in the lifecycle—such as schema validation, lineage tracking, and dependency testing—can help prevent these issues from compounding over time.

### Feedback Loops

Unlike traditional software systems, machine learning models have the capacity to influence their own future behavior through the data they help generate. This dynamic creates feedback loops, where model predictions shape future inputs, often in subtle and difficult-to-detect ways. When unaddressed, these loops introduce a unique form of technical debt: the inability to analyze and reason about model behavior over time, leading to what is known as feedback loop analysis debt.

Feedback loops in ML systems can be either direct or indirect. A direct feedback loop occurs when a model’s outputs directly affect future training data. For example, in an online recommendation system, the items a model suggests may strongly influence user clicks and, consequently, the labeled data used for retraining. If the model consistently promotes a narrow subset of items, it may bias the training set over time, reinforcing its own behavior and reducing exposure to alternative signals.

Indirect or hidden feedback loops arise when two or more systems interact with one another—often through real-world processes—without clear visibility into their mutual influence. For instance, consider two separate ML models deployed by a financial institution: one predicts credit risk, and the other recommends credit offers. If the output of the second model implicitly affects the population that is later scored by the first, a feedback loop is created without any explicit connection between the two systems. These loops are especially dangerous because they bypass traditional validation frameworks and may take weeks or months to manifest.

Feedback loops undermine assumptions about data independence and stationarity. They can mask model degradation, introduce long-term bias, and lead to unanticipated performance failures. Because most ML validation is performed offline with static datasets, these dynamic interactions are difficult to detect before deployment.

Several mitigation strategies exist, though none are comprehensive. Careful monitoring of model performance across cohorts and over time can help reveal the emergence of loop-induced drift. Canary deployments allow teams to test new models on a small subset of traffic and observe behavior before full rollout. More fundamentally, architectural practices that reduce coupling between system components—such as isolating decision-making logic from user-facing outcomes—can help minimize the propagation of influence.

Ultimately, feedback loops reflect a deeper challenge in ML system design: models do not operate in isolation, but in dynamic environments where their outputs alter future inputs. Reducing analysis debt requires designing systems with these dynamics in mind and embedding mechanisms to detect and manage self-influencing behavior over time.

### Pipeline Debt

As machine learning workflows grow in scope, teams often assemble pipelines that stitch together multiple components—data ingestion, feature extraction, model training, evaluation, and deployment. In the absence of standard interfaces or modular abstractions, these pipelines tend to evolve into ad hoc constructions of custom scripts, manual processes, and undocumented assumptions. Over time, this leads to pipeline debt: a form of technical debt arising from complexity, fragility, and a lack of reusability in ML workflows.

This problem is often described as the emergence of a “pipeline jungle,” where modifications become difficult, and experimentation is constrained by brittle interdependencies. When teams are reluctant to refactor fragile pipelines, they resort to building alternate versions for new use cases or experiments. As these variations accumulate, so do inconsistencies in data processing, metric computation, and configuration management. The result is duplication, reduced efficiency, and a growing risk of errors.

Consider a real-world scenario where a team maintains multiple models that rely on different but overlapping preprocessing pipelines. One model applies text normalization using simple lowercasing, while another uses a custom tokenization library. Over time, discrepancies emerge in behavior, leading to conflicting evaluation metrics and unexpected model drift. As new models are introduced, developers are unsure which pipeline to reuse or modify, and duplications multiply.

Pipeline debt also limits collaboration across teams. Without well-defined interfaces or shared abstractions, it becomes difficult to exchange components or adopt best practices. Team members often need to reverse-engineer pipeline logic, slowing onboarding and increasing the risk of introducing regressions.

The most effective way to manage pipeline debt is to embrace modularity and encapsulation. Well-architected pipelines define clear inputs, outputs, and transformation logic, often expressed through workflow orchestration tools such as [Apache Airflow](https://airflow.apache.org/), [Prefect](https://www.prefect.io/), or [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/). These tools help teams formalize processing steps, track lineage, and monitor execution.

In addition, the adoption of shared libraries for feature engineering, transformation functions, and evaluation metrics promotes consistency and reuse. Teams can isolate logic into composable units that can be independently tested, versioned, and integrated across models. This reduces the risk of technical lock-in and enables more agile development as systems evolve.

Ultimately, pipeline debt reflects a breakdown in software engineering rigor applied to ML workflows. Investing in interfaces, documentation, and shared tooling not only improves maintainability but also unlocks faster experimentation and system scalability.

### Configuration Debt

Configuration is a critical yet often undervalued component of machine learning systems. Tuning parameters such as learning rates, regularization strengths, model architectures, feature processing options, and evaluation thresholds all require deliberate management. However, in practice, configurations are frequently introduced in an ad hoc manner—manually adjusted during experimentation, inconsistently documented, and rarely versioned. This leads to the accumulation of configuration debt: the technical burden resulting from fragile, opaque, and outdated settings that undermine system reliability and reproducibility.

When configuration debt accumulates, several challenges emerge. Fragile configurations may contain implicit assumptions about data distributions, training schedules, or pipeline structure that no longer hold as the system evolves. In the absence of proper documentation, these assumptions become embedded in silent defaults—settings that function in development but fail in production. Teams may hesitate to modify these configurations out of fear of introducing regressions, further entrenching the problem. Additionally, when configurations are not centrally tracked, knowledge about what parameters work well becomes siloed within individuals or specific notebooks, leading to redundant experimentation and slowed iteration.

For example, consider a team deploying a neural network for customer segmentation. During development, one data scientist improves performance by tweaking several architectural parameters—adding layers, changing activation functions, and adjusting batch sizes—but these changes are stored locally and never committed to the shared configuration repository. Months later, the model is retrained on new data, but the performance degrades unexpectedly. Without a consistent record of previous configurations, the team struggles to identify what changed. The lack of traceability not only delays debugging but also undermines confidence in the reproducibility of prior results.

Mitigating configuration debt requires integrating configuration management into the ML system lifecycle. Teams should adopt structured formats—such as YAML, JSON, or domain-specific configuration frameworks—and store them in version-controlled repositories alongside model code. Validating configurations as part of the training and deployment process ensures that unexpected or invalid parameter settings are caught early. Automated tools for hyperparameter optimization and neural architecture search further reduce reliance on manual tuning and help standardize configuration discovery.

Above all, ML systems benefit when configuration is treated not as a side effect of experimentation, but as a first-class system component. Like code, configurations must be tested, documented, and maintained. Doing so enables faster iteration, easier debugging, and more reliable system behavior over time.

### Early-Stage Debt

In the early phases of machine learning development, teams often move quickly to prototype models, experiment with data sources, and explore modeling approaches. During this stage, speed and flexibility are critical, and some level of technical debt is expected and even necessary to support rapid iteration. However, the decisions made in these early stages—especially if driven by urgency rather than design—can introduce early-stage debt that becomes increasingly difficult to manage as the system matures.

This form of debt often stems from shortcuts in code organization, data preprocessing, feature engineering, or model packaging. Pipelines may be built without clear abstractions, evaluation scripts may lack reproducibility, and configuration files may be undocumented or fragmented. While such practices may be justified in the exploratory phase, they become liabilities once the system enters production or needs to scale across teams and use cases.

For example, a startup team developing a minimum viable product (MVP) might embed core business logic directly into the model training code—such as applying customer-specific rules or filters during preprocessing. This expedites initial experimentation but creates a brittle system in which modifying the business logic or model behavior requires untangling deeply intertwined code. As the company grows and multiple teams begin working on the system, these decisions limit flexibility, slow iteration, and increase the risk of breaking core functionality during updates.

Despite these risks, not all early-stage debt is harmful. The key distinction lies in whether the system is designed to support evolution. Techniques such as using modular code, isolating configuration from logic, and containerizing experimental environments allow teams to move quickly without sacrificing future maintainability. Abstractions—such as shared data access layers or feature transformation modules—can be introduced incrementally as patterns stabilize.

To manage early-stage debt effectively, teams should adopt the principle of flexible foundations: designing for change without over-engineering. This means identifying which components are likely to evolve and introducing appropriate boundaries and interfaces early on. As the system matures, natural inflection points emerge—opportunities to refactor or re-architect without disrupting existing workflows.

Accepting some technical debt in the short term is often a rational tradeoff. The challenge is ensuring that such debt is intentional, tracked, and revisited before it becomes entrenched. By investing in adaptability from the beginning, ML teams can balance early innovation with long-term sustainability.

### Summary

Technical debt in machine learning systems is both pervasive and distinct from debt encountered in traditional software engineering. While the original metaphor of financial debt highlights the tradeoff between speed and long-term cost, the analogy falls short in capturing the full complexity of ML systems. In machine learning, debt often arises not only from code shortcuts but also from entangled data dependencies, poorly understood feedback loops, fragile pipelines, and configuration sprawl. Unlike financial debt, which can be explicitly quantified, ML technical debt is largely hidden, emerging only as systems scale, evolve, or fail.

This chapter has outlined several forms of ML-specific technical debt, each rooted in different aspects of the system lifecycle. Boundary erosion undermines modularity and makes systems difficult to reason about. Correction cascades illustrate how local fixes can ripple through a tightly coupled workflow. Undeclared consumers and feedback loops introduce invisible dependencies that challenge traceability and reproducibility. Data and configuration debt reflect the fragility of inputs and parameters that are poorly managed, while pipeline and change adaptation debt expose the risks of inflexible architectures. Early-stage debt reminds us that even in the exploratory phase, decisions should be made with an eye toward future extensibility.

The common thread across all these debt types is the need for system-level thinking. ML systems are not just code—they are evolving ecosystems of data, models, infrastructure, and teams. Managing technical debt requires architectural discipline, robust tooling, and a culture that values maintainability alongside innovation. It also requires humility: acknowledging that today’s solutions may become tomorrow’s constraints if not designed with care.

As machine learning becomes increasingly central to production systems, understanding and addressing hidden technical debt is essential. Doing so not only improves reliability and scalability, but also empowers teams to iterate faster, collaborate more effectively, and sustain the long-term evolution of their systems.

## Roles and Responsibilities

Given the vastness of MLOps, successfully implementing ML systems requires diverse skills and close collaboration between people with different areas of expertise. While data scientists build the core ML models, it takes cross-functional teamwork to successfully deploy these models into production environments and enable them to deliver sustainable business value.

MLOps provides the framework and practices for coordinating the efforts of various roles involved in developing, deploying, and running MLG systems. Bridging traditional silos between data, engineering, and operations teams is key to MLOp's success. Enabling seamless collaboration through the machine learning lifecycle accelerates benefit realization while ensuring ML models' long-term reliability and performance.

We will look at some key roles involved in MLOps and their primary responsibilities. Understanding the breadth of skills needed to operationalize ML models guides assembling MLOps teams. It also clarifies how the workflows between roles fit under the overarching MLOps methodology.

### Data Engineers

Data engineers are responsible for building and maintaining the data infrastructure and pipelines that feed data to ML models. They ensure data is smoothly moved from source systems into the storage, processing, and feature engineering environments needed for ML model development and deployment. Their main responsibilities include:

* Migrating raw data from on-prem databases, sensors, and apps into cloud-based data lakes like Amazon S3 or Google Cloud Storage. This provides cost-efficient, scalable storage.
* Building data pipelines with workflow schedulers like Apache Airflow, Prefect, and dbt. These extract data from sources, transform and validate data, and load it into destinations like data warehouses, feature stores, or directly for model training.
* Transforming messy, raw data into structured, analysis-ready datasets. This includes handling null or malformed values, deduplicating, joining disparate data sources, aggregating data, and engineering new features.
* Maintaining data infrastructure components like cloud data warehouses ([Snowflake](https://www.snowflake.com/en/data-cloud/workloads/data-warehouse/), [Redshift](https://aws.amazon.com/redshift/), [BigQuery](https://cloud.google.com/bigquery?hl=en)), data lakes, and metadata management systems. Provisioning and optimizing data processing systems.
* Provisioning and optimizing data processing systems for efficient, scalable data handling and analysis.
* Establishing data versioning, backup, and archival processes for ML datasets and features and enforcing data governance policies.

For example, a manufacturing firm may use Apache Airflow pipelines to extract sensor data from PLCs on the factory floor into an Amazon S3 data lake. The data engineers would then process this raw data to filter, clean, and join it with product metadata. These pipeline outputs would then load into a Snowflake data warehouse from which features can be read for model training and prediction.

The data engineering team builds and sustains the data foundation for reliable model development and operations. Their work enables data scientists and ML engineers to focus on building, training, and deploying ML models at scale.

### Data Scientists

The job of the data scientists is to focus on the research, experimentation, development, and continuous improvement of ML models. They leverage their expertise in statistics, modeling, and algorithms to create high-performing models. Their main responsibilities include:

* Working with business and data teams to identify opportunities where ML can add value, framing the problem, and defining success metrics.
* Performing exploratory data analysis to understand relationships in data, derive insights, and identify relevant features for modeling.
* Researching and experimenting with different ML algorithms and model architectures based on the problem and data characteristics and leveraging libraries like TensorFlow, PyTorch, and Keras.
* To maximize performance, train and fine-tune models by tuning hyperparameters, adjusting neural network architectures, feature engineering, etc.
* Evaluating model performance through metrics like accuracy, AUC, and F1 scores and performing error analysis to identify areas for improvement.
* Developing new model versions by incorporating new data, testing different approaches, optimizing model behavior, and maintaining documentation and lineage for models.

For example, a data scientist may leverage TensorFlow and [TensorFlow Probability](https://www.tensorflow.org/probability) to develop a demand forecasting model for retail inventory planning. They would iterate on different sequence models like LSTMs and experiment with features derived from product, sales, and seasonal data. The model would be evaluated based on error metrics versus actual demand before deployment. The data scientist monitors performance and retrains/enhances the model as new data comes in.

Data scientists drive model creation, improvement, and innovation through their expertise in ML techniques. They collaborate closely with other roles to ensure models create maximum business impact.

### ML Engineers

ML engineers enable models data scientists develop to be productized and deployed at scale. Their expertise makes models reliably serve predictions in applications and business processes. Their main responsibilities include:

* Taking prototype models from data scientists and hardening them for production environments through coding best practices.
* Building APIs and microservices for model deployment using tools like [Flask](https://flask.palletsprojects.com/en/3.0.x/), [FastAPI](https://fastapi.tiangolo.com/). Containerizing models with Docker.
* Manage model versions, sync new models into production using CI/CD pipelines, and implement canary releases, A/B tests, and rollback procedures.
* Optimizing model performance for high scalability, low latency, and cost efficiency. Leveraging compression, quantization, and multi-model serving.
* Monitor models once in production and ensure continued reliability and accuracy. Retraining models periodically.

For example, an ML engineer may take a TensorFlow fraud detection model developed by data scientists and containerize it using TensorFlow Serving for scalable deployment. The model would be integrated into the company's transaction processing pipeline via APIs. The ML engineer implements a model registry and CI/CD pipeline using MLFlow and Jenkins to deploy model updates reliably. The ML engineers then monitor the running model for continued performance using tools like Prometheus and Grafana. If model accuracy drops, they initiate retraining and deployment of a new model version.

The ML engineering team enables data science models to progress smoothly into sustainable and robust production systems. Their expertise in building modular, monitored systems delivers continuous business value.

### DevOps Engineers

DevOps engineers enable MLOps by building and managing the underlying infrastructure for developing, deploying, and monitoring ML models. As a specialized branch of software engineering, DevOps focuses on creating automation pipelines, cloud architecture, and operational frameworks. Their main responsibilities include:

* Provisioning and managing cloud infrastructure for ML workflows using IaC tools like Terraform, Docker, and Kubernetes.
* Developing CI/CD pipelines for model retraining, validation, and deployment. Integrating ML tools into the pipeline, such as MLflow and Kubeflow.
* Monitoring model and infrastructure performance using tools like [Prometheus](https://prometheus.io/), [Grafana](https://grafana.com/), [ELK stack](https://aws.amazon.com/what-is/elk-stack/). Building alerts and dashboards.
* Implement governance practices around model development, testing, and promotion to enable reproducibility and traceability.
* Embedding ML models within applications. They are exposing models via APIs and microservices for integration.
* Optimizing infrastructure performance and costs and leveraging autoscaling, spot instances, and availability across regions.

For example, a DevOps engineer provisions a Kubernetes cluster on AWS using Terraform to run ML training jobs and online deployment. The engineer builds a CI/CD pipeline in Jenkins, which triggers model retraining when new data becomes available. After automated testing, the model is registered with MLflow and deployed in the Kubernetes cluster. The engineer then monitors cluster health, container resource usage, and API latency using Prometheus and Grafana.

The DevOps team enables rapid experimentation and reliable deployments for ML through cloud, automation, and monitoring expertise. Their work maximizes model impact while minimizing technical debt.

### Project Managers

Project managers play a vital role in MLOps by coordinating the activities between the teams involved in delivering ML projects. They help drive alignment, accountability, and accelerated results. Their main responsibilities include:

* Working with stakeholders to define project goals, success metrics, timelines, and budgets; outlining specifications and scope.
* Creating a project plan spanning data acquisition, model development, infrastructure setup, deployment, and monitoring.
* Coordinating design, development, and testing efforts between data engineers, data scientists, ML engineers, and DevOps roles.
* Tracking progress and milestones, identifying roadblocks and resolving them through corrective actions, and managing risks and issues.
* Facilitating communication through status reports, meetings, workshops, and documentation and enabling seamless collaboration.
* Driving adherence to timelines and budget and escalating anticipated overruns or shortfalls for mitigation.

For example, a project manager would create a project plan for developing and enhancing a customer churn prediction model. They coordinate between data engineers building data pipelines, data scientists experimenting with models, ML engineers productizing models, and DevOps setting up deployment infrastructure. The project manager tracks progress via milestones like dataset preparation, model prototyping, deployment, and monitoring. To enact preventive solutions, they surface any risks, delays, or budget issues.

Skilled project managers enable MLOps teams to work synergistically to rapidly deliver maximum business value from ML investments. Their leadership and organization align with diverse teams.

## Traditional vs. Embedded MLOps

Building on our discussion of [On-device Learning](../ondevice_learning/ondevice_learning.qmd) in the previous chapter, we now turn our attention to the broader context of embedded systems in MLOps. The unique constraints and requirements of embedded environments significantly impact the implementation of machine learning models and operations. As we have discussed in previous chapters, embedded systems introduce unique challenges to MLOps due to their constrained resources, intermittent connectivity, and the need for efficient, power-aware computation. Unlike cloud environments with abundant compute and storage, embedded devices often operate with limited memory, power, and processing capabilities, requiring careful optimization of workflows. These limitations influence all aspects of MLOps, from deployment and data collection to monitoring and updates.

In traditional MLOps, ML models are typically deployed in cloud-based or server environments, with abundant resources like computing power and memory. These environments facilitate the smooth operation of complex models that require significant computational resources. For instance, a cloud-based image recognition model might be used by a social media platform to tag photos with relevant labels automatically. In this case, the model can leverage the extensive resources available in the cloud to efficiently process vast amounts of data.

On the other hand, embedded MLOps involves deploying ML models on embedded systems, specialized computing systems designed to perform specific functions within larger systems. Embedded systems are typically characterized by their limited computational resources and power. For example, an ML model might be embedded in a smart thermostat to optimize heating and cooling based on the user's preferences and habits. The model must be optimized to run efficiently on the thermostat's limited hardware without compromising its performance or accuracy.

The key difference between traditional and embedded MLOps lies in the embedded system's resource constraints. While traditional MLOps can leverage abundant cloud or server resources, embedded MLOps must contend with the hardware limitations on which the model is deployed. This requires careful optimization and fine-tuning of the model to ensure it can deliver accurate and valuable insights within the embedded system's constraints.

Furthermore, embedded MLOps must consider the unique challenges posed by integrating ML models with other embedded system components. For example, the model must be compatible with the system's software and hardware and must be able to interface seamlessly with other components, such as sensors or actuators. This requires a deep understanding of both ML and embedded systems and close collaboration between data scientists, engineers, and other stakeholders.

So, while traditional MLOps and embedded MLOps share the common goal of deploying and maintaining ML models in production environments, the unique challenges posed by embedded systems require a specialized approach. Embedded MLOps must carefully balance the need for model accuracy and performance with the constraints of the hardware on which the model is deployed. This requires a deep understanding of both ML and embedded systems and close collaboration between various stakeholders to ensure the successful integration of ML models into embedded systems.

This time, we will group the subtopics under broader categories to streamline the structure of our thought process on MLOps. This structure will help you understand how different aspects of MLOps are interconnected and why each is important for the efficient operation of ML systems as we discuss the challenges in the context of embedded systems.

* Model Lifecycle Management
  * Data Management: Handling data ingestion, validation, and version control.
  * Model Training: Techniques and practices for effective and scalable model training.
  * Model Evaluation: Strategies for testing and validating model performance.
  * Model Deployment: Approaches for deploying models into production environments.

* Development and Operations Integration
  * CI/CD Pipelines: Integrating ML models into continuous integration and deployment pipelines.
  * Infrastructure Management: Setting up and maintaining the infrastructure required for training and deploying models.
  * Communication & Collaboration: Ensuring smooth communication and collaboration between data scientists, ML engineers, and operations teams.

* Operational Excellence
  * Monitoring: Techniques for monitoring model performance, data drift, and operational health.
  * Governance: Implementing policies for model auditability, compliance, and ethical considerations.

### Model Lifecycle Management

#### Data Management

In traditional centralized MLOps, data is aggregated into large datasets and data lakes, then processed on cloud or on-prem servers. However, embedded MLOps relies on decentralized data from local on-device sensors. Devices collect smaller batches of incremental data, often noisy and unstructured. With connectivity constraints, this data cannot always be instantly transmitted to the cloud and needs to be intelligently cached and processed at the edge.

Due to limited on-device computing, embedded devices can only preprocess and clean data minimally before transmission. Early filtering and processing occur at edge gateways to reduce transmission loads. While leveraging cloud storage, more processing and storage happen at the edge to account for intermittent connectivity. Devices identify and transmit only the most critical subsets of data to the cloud.

Labeling also needs centralized data access, requiring more automated techniques like federated learning, where devices collaboratively label peers' data. With personal edge devices, data privacy and regulations are critical concerns. Data collection, transmission, and storage must be secure and compliant.

For instance, a smartwatch may collect the day's step count, heart rate, and GPS coordinates. This data is cached locally and transmitted to an edge gateway when WiFi is available—the gateway processes and filters data before syncing relevant subsets with the cloud platform to retrain models.

#### Model Training

In traditional centralized MLOps, models are trained using abundant data via deep learning on high-powered cloud GPU servers. However, embedded MLOps need more support in model complexity, data availability, and computing resources for training.

The volume of aggregated data is much lower, often requiring techniques like federated learning across devices to create training sets. The specialized nature of edge data also limits public datasets for pre-training. With privacy concerns, data samples must be tightly controlled and anonymized where possible.

Furthermore, the models must use simplified architectures optimized for low-power edge hardware. Given the computing limitations, high-end GPUs are inaccessible for intensive deep learning. Training leverages lower-powered edge servers and clusters with distributed approaches to spread load.

Transfer learning emerges as a crucial strategy to address data scarcity and irregularity in machine learning, particularly in edge computing scenarios. As illustrated in @fig-transfer-learning-mlops, this approach involves pre-training models on large public datasets and then fine-tuning them on limited domain-specific edge data. The figure depicts a neural network where initial layers ($W_{A1}$ to $W_{A4}$), responsible for general feature extraction, are frozen (indicated by a green dashed line). These layers retain knowledge from previous tasks, accelerating learning and reducing resource requirements. The latter layers ($W_{A5}$ to $W_{A7}$), beyond the blue dashed line, are fine-tuned for the specific task, focusing on task-specific feature learning.

![Transfer learning in MLOps. Source: HarvardX.](images/png/transfer_learning.png){#fig-transfer-learning-mlops}

This method not only mitigates data scarcity but also accommodates the decentralized nature of embedded data. Furthermore, techniques like incremental on-device learning can further customize models to specific use cases. The lack of broad labeled data in many domains also motivates the use of semi-supervised techniques, complementing the transfer learning approach. By leveraging pre-existing knowledge and adapting it to specialized tasks, transfer learning within an MLOps framework enables models to achieve higher performance with fewer resources, even in data-constrained environments.

For example, a smart home assistant may pre-train an audio recognition model on public YouTube clips, which helps bootstrap with general knowledge. It then transfers learning to a small sample of home data to classify customized appliances and events, specializing in the model. The model transforms into a lightweight neural network optimized for microphone-enabled devices across the home.

So, embedded MLOps face acute challenges in constructing training datasets, designing efficient models, and distributing compute for model development compared to traditional settings. Given the embedded constraints, careful adaptation, such as transfer learning and distributed training, is required to train models.

#### Model Evaluation

In traditional centralized MLOps, models are evaluated primarily using accuracy metrics and holdout test datasets. However, embedded MLOps require a more holistic evaluation that accounts for system constraints beyond accuracy.

Models must be tested early and often on deployed edge hardware covering diverse configurations. In addition to accuracy, factors like latency, CPU usage, memory footprint, and power consumption are critical evaluation criteria. Models are selected based on tradeoffs between these metrics to meet edge device constraints.

Data drift must also be monitored - where models trained on cloud data degrade in accuracy over time on local edge data. Embedded data often has more variability than centralized training sets. Evaluating models across diverse operational edge data samples is key. But sometimes, getting the data for monitoring the drift can be challenging if these devices are in the wild and communication is a barrier.

Ongoing monitoring provides visibility into real-world performance post-deployment, revealing bottlenecks not caught during testing. For instance, a smart camera model update may be canary tested on 100 cameras first and rolled back if degraded accuracy is observed before expanding to all 5000 cameras.

#### Model Deployment

In traditional MLOps, new model versions are directly deployed onto servers via API endpoints. However, embedded devices require optimized delivery mechanisms to receive updated models. Over-the-air (OTA) updates provide a standardized approach to wirelessly distributing new software or firmware releases to embedded devices. Rather than direct API access, OTA packages allow remote deploying models and dependencies as pre-built bundles. Alternatively, [federated learning](@sec-fl) allows model updates without direct access to raw training data. This decentralized approach has the potential for continuous model improvement but needs robust MLOps platforms.

Model delivery relies on physical interfaces like USB or UART serial connections for deeply embedded devices lacking connectivity. The model packaging still follows similar principles to OTA updates, but the deployment mechanism is tailored to the capabilities of the edge hardware. Moreover, specialized OTA protocols optimized for IoT networks are often used rather than standard WiFi or Bluetooth protocols. Key factors include efficiency, reliability, security, and telemetry, such as progress tracking—solutions like [Mender. Io](https://mender.io/) provides embedded-focused OTA services handling differential updates across device fleets.

@fig-model-lifecycle presents an overview of Model Lifecycle Management in an MLOps context, illustrating the flow from development (top left) to deployment and monitoring (bottom right). The process begins with ML Development, where code and configurations are version-controlled. Data and model management are central to the process, involving datasets and feature repositories. Continuous training, model conversion, and model registry are key stages in the operationalization of training. The model deployment includes serving the model and managing serving logs. Alerting mechanisms are in place to flag issues, which feed into continuous monitoring to ensure model performance and reliability over time. This integrated approach ensures that models are developed and maintained effectively throughout their lifecycle.

![Model lifecycle management. Source: HarvardX.](images/png/mlops_flow.png){#fig-model-lifecycle}

### DevOps Integration

#### CI/CD Pipelines

In traditional MLOps, robust CI/CD infrastructure like Jenkins and Kubernetes enables pipeline automation for large-scale model deployment. However, embedded MLOps need this centralized infrastructure and more tailored CI/CD workflows for edge devices.

Building CI/CD pipelines has to account for a fragmented landscape of diverse hardware, firmware versions, and connectivity constraints. There is no standard platform to orchestrate pipelines, and tooling support is more limited.

Testing must cover this wide spectrum of target embedded devices early, which is difficult without centralized access. Companies must invest significant effort into acquiring and managing test infrastructure across the heterogeneous embedded ecosystem.

Over-the-air updates require setting up specialized servers to distribute model bundles securely to devices in the field. Rollout and rollback procedures must also be carefully tailored for particular device families.

With traditional CI/CD tools less applicable, embedded MLOps rely more on custom scripts and integration. Companies take varied approaches, from open-source frameworks to fully in-house solutions. Tight integration between developers, edge engineers, and end customers establishes trusted release processes.

Therefore, embedded MLOps can't leverage centralized cloud infrastructure for CI/CD. Companies combine custom pipelines, testing infrastructure, and OTA delivery to deploy models across fragmented and disconnected edge systems.

#### Infrastructure Management

In traditional centralized MLOps, infrastructure entails provisioning cloud servers, GPUs, and high-bandwidth networks for intensive workloads like model training and serving predictions at scale. However, embedded MLOps require more heterogeneous infrastructure spanning edge devices, gateways, and the cloud.

Edge devices like sensors capture and preprocess data locally before intermittent transmission to avoid overloading networks—gateways aggregate and process device data before sending select subsets to the cloud for training and analysis. The cloud provides centralized management and supplemental computing.

This infrastructure needs tight integration and balancing processing and communication loads. Network bandwidth is limited, requiring careful data filtering and compression. Edge computing capabilities are modest compared to the cloud, imposing optimization constraints.

Managing secure OTA updates across large device fleets presents challenges at the edge. Rollouts must be incremental and rollback-ready for quick mitigation. Given decentralized environments, updating edge infrastructure requires coordination.

For example, an industrial plant may perform basic signal processing on sensors before sending data to an on-prem gateway. The gateway handles data aggregation, infrastructure monitoring, and OTA updates. Only curated data is transmitted to the cloud for advanced analytics and model retraining.

Embedded MLOps requires holistic management of distributed infrastructure spanning constrained edge, gateways, and centralized cloud. Workloads are balanced across tiers while accounting for connectivity, computing, and security challenges.

#### Communication and Collaboration

In traditional MLOps, collaboration tends to center around data scientists, ML engineers, and DevOps teams. However, embedded MLOps require tighter cross-functional coordination between additional roles to address system constraints.

Edge engineers optimize model architectures for target hardware environments. They provide feedback to data scientists during development so models fit device capabilities early on. Similarly, product teams define operational requirements informed by end-user contexts.

With more stakeholders across the embedded ecosystem, communication channels must facilitate information sharing between centralized and remote teams. Issue tracking and project management ensure alignment.

Collaborative tools optimize models for particular devices. Data scientists can log issues replicated from field devices so models specialize in niche data. Remote device access aids debugging and data collection.

For example, data scientists may collaborate with field teams managing fleets of wind turbines to retrieve operational data samples. This data is used to specialize models detecting anomalies specific to that turbine class. Model updates are tested in simulations and reviewed by engineers before field deployment.

Embedded MLOps mandates continuous coordination between data scientists, engineers, end customers, and other stakeholders throughout the ML lifecycle. Through close collaboration, models can be tailored and optimized for targeted edge devices.

### Operational Excellence

#### Monitoring

Traditional MLOps monitoring focuses on centrally tracking model accuracy, performance metrics, and data drift. However, embedded MLOps must account for decentralized monitoring across diverse edge devices and environments.

Edge devices require optimized data collection to transmit key monitoring metrics without overloading networks. Metrics help assess model performance, data patterns, resource usage, and other behaviors on remote devices.

With limited connectivity, more analysis occurs at the edge before aggregating insights centrally. Gateways play a key role in monitoring fleet health and coordinating software updates. Confirmed indicators are eventually propagated to the cloud.

Broad device coverage is challenging but critical. Issues specific to certain device types may arise, so monitoring needs to cover the full spectrum. Canary deployments help trial monitoring processes before scaling.

Anomaly detection identifies incidents requiring rolling back models or retraining on new data. However, interpreting alerts requires understanding unique device contexts based on input from engineers and customers.

For example, an automaker may monitor autonomous vehicles for indicators of model degradation using caching, aggregation, and real-time streams. Engineers assess when identified anomalies warrant OTA updates to improve models based on factors like location and vehicle age.

Embedded MLOps monitoring provides observability into model and system performance across decentralized edge environments. Careful data collection, analysis, and collaboration deliver meaningful insights to maintain reliability.

#### Governance

In traditional MLOps, governance focuses on model explainability, fairness, and compliance for centralized systems. However, embedded MLOps must also address device-level governance challenges related to data privacy, security, and safety.

With sensors collecting personal and sensitive data, local data governance on devices is critical. Data access controls, anonymization, and encrypted caching help address privacy risks and compliance like HIPAA and GDPR. Updates must maintain security patches and settings.

Safety governance considers the physical impacts of flawed device behavior. Failures could cause unsafe conditions in vehicles, factories, and critical systems. Redundancy, fail-safes, and warning systems help mitigate risks.

Traditional governance, such as bias monitoring and model explainability, remains imperative but is harder to implement for embedded AI. Peeking into black-box models on low-power devices also poses challenges.

For example, a medical device may scrub personal data on the device before transmission. Strict data governance protocols approve model updates. Model explainability is limited, but the focus is on detecting anomalous behavior. Backup systems prevent failures.

Embedded MLOps governance must encompass privacy, security, safety, transparency, and ethics. Specialized techniques and team collaboration are needed to help establish trust and accountability within decentralized environments.

### Comparison

@tbl-mlops-comparison highlights the similarities and differences between Traditional MLOps and Embedded MLOps based on all the things we have learned thus far:

+------------------------+-----------------------------------------------------------+---------------------------------------------------------------+
| Area                   | Traditional MLOps                                         | Embedded MLOps                                                |
+:=======================+:==========================================================+:==============================================================+
| Data Management        | Large datasets, data lakes, feature stores                | On-device data capture, edge caching and processing           |
+------------------------+-----------------------------------------------------------+---------------------------------------------------------------+
| Model Development      | Leverage deep learning, complex neural nets, GPU training | Constraints on model complexity, need for optimization        |
+------------------------+-----------------------------------------------------------+---------------------------------------------------------------+
| Deployment             | Server clusters, cloud deployment, low latency at scale   | OTA deployment to devices, intermittent connectivity          |
+------------------------+-----------------------------------------------------------+---------------------------------------------------------------+
| Monitoring             | Dashboards, logs, alerts for cloud model performance      | On-device monitoring of predictions, resource usage           |
+------------------------+-----------------------------------------------------------+---------------------------------------------------------------+
| Retraining             | Retrain models on new data                                | Federated learning from devices, edge retraining              |
+------------------------+-----------------------------------------------------------+---------------------------------------------------------------+
| Infrastructure         | Dynamic cloud infrastructure                              | Heterogeneous edge/cloud infrastructure                       |
+------------------------+-----------------------------------------------------------+---------------------------------------------------------------+
| Collaboration          | Shared experiment tracking and model registry             | Collaboration for device-specific optimization                |
+------------------------+-----------------------------------------------------------+---------------------------------------------------------------+

: Comparison of Traditional MLOps and Embedded MLOps practices. {#tbl-mlops-comparison .striped .hover}

So, while Embedded MLOps shares foundational MLOps principles, it faces unique constraints in tailoring workflows and infrastructure specifically for resource-constrained edge devices.

### Embedded MLOps Services

Despite the proliferation of new MLOps tools in response to the increase in demand, the challenges described earlier have constrained the availability of such tools in embedded systems environments. More recently, new tools such as Edge Impulse [@janapa2023edge] have made the development process somewhat easier, as described below.

#### Edge Impulse

[Edge Impulse](https://edgeimpulse.com/) is an end-to-end development platform for creating and deploying machine learning models onto edge devices such as microcontrollers and small processors. It makes embedded machine learning more accessible to software developers through its easy-to-use web interface and integrated tools for data collection, model development, optimization, and deployment. Its key capabilities include the following:

* Intuitive drag-and-drop workflow for building ML models without coding required
* Tools for acquiring, labeling, visualizing, and preprocessing data from sensors
* Choice of model architectures, including neural networks and unsupervised learning
* Model optimization techniques to balance performance metrics and hardware constraints
* Seamless deployment onto edge devices through compilation, SDKs, and benchmarks
* Collaboration features for teams and integration with other platforms

Edge Impulse offers a comprehensive solution for creating embedded intelligence and advancing machine learning, particularly for developers with limited data science expertise. This platform enables the development of specialized ML models that run efficiently within small computing environments. As illustrated in @fig-edge-impulse, Edge Impulse facilitates the journey from data collection to model deployment, highlighting its user-friendly interface and tools that simplify the creation of embedded ML solutions, thus making it accessible to a broader range of developers and applications.

![Edge impulse overview. Source: [Edge Impulse](https://www.edgeimpulse.com/blog/getting-started-with-edge-impulse/)](images/png/impulse.png){#fig-edge-impulse}

##### User Interface

Edge Impulse was designed with seven key principles: accessibility, end-to-end capabilities, a data-centric approach, interactiveness, extensibility, team orientation, and community support. The intuitive user interface, shown in @fig-edge-impulse-ui, guides developers at all experience levels through uploading data, selecting a model architecture, training the model, and deploying it across relevant hardware platforms. It should be noted that, like any tool, Edge Impulse is intended to assist with, not replace, foundational considerations such as determining if ML is an appropriate solution or acquiring the requisite domain expertise for a given application.

![Screenshot of Edge Impulse user interface for building workflows from input data to output features.](images/png/edge_impulse_dashboard.png){#fig-edge-impulse-ui}

What makes Edge Impulse notable is its comprehensive yet intuitive end-to-end workflow. Developers start by uploading their data via the graphical user interface (GUI) or command line interface (CLI) tools, after which they can examine raw samples and visualize the data distribution in the training and test splits. Next, users can pick from various preprocessing "blocks" to facilitate digital signal processing (DSP). While default parameter values are provided, users can customize the parameters as needed, with considerations around memory and latency displayed. Users can easily choose their neural network architecture - without any code needed.

Thanks to the platform's visual editor, users can customize the architecture's components and specific parameters while ensuring that the model is still trainable. Users can also leverage unsupervised learning algorithms, such as K-means clustering and Gaussian mixture models (GMM).

##### Optimizations

To accommodate the resource constraints of TinyML applications, Edge Impulse provides a confusion matrix summarizing key performance metrics, including per-class accuracy and F1 scores. The platform elucidates the tradeoffs between model performance, size, and latency using simulations in [Renode](https://renode.io/) and device-specific benchmarking. For streaming data use cases, a performance calibration tool leverages a genetic algorithm to find ideal post-processing configurations balancing false acceptance and false rejection rates. Techniques like quantization, code optimization, and device-specific optimization are available to optimize models. For deployment, models can be compiled in appropriate formats for target edge devices. Native firmware SDKs also enable direct data collection on devices.

In addition to streamlining development, Edge Impulse scales the modeling process itself. A key capability is the [EON Tuner](https://docs.edgeimpulse.com/docs/edge-impulse-studio/eon-tuner), an automated machine learning (AutoML) tool that assists users in hyperparameter tuning based on system constraints. It runs a random search to generate configurations for digital signal processing and training steps quickly. The resulting models are displayed for the user to select based on relevant performance, memory, and latency metrics. For data, active learning facilitates training on a small labeled subset, followed by manually or automatically labeling new samples based on proximity to existing classes. This expands data efficiency.

##### Use Cases

Beyond the accessibility of the platform itself, the Edge Impulse team has expanded the knowledge base of the embedded ML ecosystem. The platform lends itself to academic environments, having been used in online courses and on-site workshops globally. Numerous case studies featuring industry and research use cases have been published, most notably [Oura Ring](https://ouraring.com/), which uses ML to identify sleep patterns. The team has made repositories open source on GitHub, facilitating community growth. Users can also make projects public to share techniques and download libraries to share via Apache. Organization-level access enables collaboration on workflows.

Overall, Edge Impulse is uniquely comprehensive and integrateable for developer workflows. Larger platforms like Google and Microsoft focus more on cloud versus embedded systems. TinyMLOps frameworks such as Neuton AI and Latent AI offer some functionality but lack Edge Impulse's end-to-end capabilities. TensorFlow Lite Micro is the standard inference engine due to flexibility, open source status, and TensorFlow integration, but it uses more memory and storage than Edge Impulse's EON Compiler. Other platforms need to be updated, academic-focused, or more versatile. In summary, Edge Impulse streamlines and scale embedded ML through an accessible, automated platform.

#### Limitations

While Edge Impulse provides an accessible pipeline for embedded ML, important limitations and risks remain. A key challenge is data quality and availability - the models are only as good as the data used to train them. Users must have sufficient labeled samples that capture the breadth of expected operating conditions and failure modes. Labeled anomalies and outliers are critical yet time-consuming to collect and identify. Insufficient or biased data leads to poor model performance regardless of the tool's capabilities.

Deploying low-powered devices also presents inherent challenges. Optimized models may still need to be more resource-intensive for ultra-low-power MCUs. Striking the right balance of compression versus accuracy takes some experimentation. The tool simplifies but still needs to eliminate the need for foundational ML and signal processing expertise. Embedded environments also constrain debugging and interpretability compared to the cloud.

While impressive results are achievable, users shouldn't view Edge Impulse as a "Push Button ML" solution. Careful project scoping, data collection, model evaluation, and testing are still essential. As with any development tool, reasonable expectations and diligence in application are advised. However, Edge Impulse can accelerate embedded ML prototyping and deployment for developers willing to invest the requisite data science and engineering effort.

:::{#exr-ei .callout-caution collapse="true" title="Edge Impulse"}

Ready to level up your tiny machine-learning projects? Let's combine the power of Edge Impulse with the awesome visualizations of Weights & Biases (WandB). In this Colab, you'll learn to track your model's training progress like a pro! Imagine seeing cool graphs of your model getting smarter, comparing different versions, and ensuring your AI performs its best even on tiny devices.

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/edgeimpulse/notebooks/blob/main/notebooks/python-sdk-with-wandb.ipynb#scrollTo=7583a486-afd6-42d8-934b-fdb33a6f3362)

:::

## Case Studies

### Oura Ring Case Study

The [Oura Ring](https://ouraring.com/) is a wearable that can measure activity, sleep, and recovery when placed on the user's finger. Using sensors to track physiological metrics, the device uses embedded ML to predict the stages of sleep. To establish a baseline of legitimacy in the industry, Oura conducted a correlation experiment to evaluate the device's success in predicting sleep stages against a baseline study. This resulted in a solid 62% correlation compared to the 82-83% baseline. Thus, the team set out to determine how to improve their performance even further.

The first challenge was to obtain better data in terms of both quantity and quality. They could host a larger study to get a more comprehensive data set, but the data would be so noisy and large that it would be difficult to aggregate, scrub, and analyze. This is where Edge Impulse comes in.

We hosted a massive sleep study of 100 men and women between the ages of 15 and 73 across three continents (Asia, Europe, and North America). In addition to wearing the Oura Ring, participants were responsible for undergoing the industry standard PSG testing, which provided a "label" for this data set. With 440 nights of sleep from 106 participants, the data set totaled 3,444 hours in length across Ring and PSG data. With Edge Impulse, Oura could easily upload and consolidate data from different sources into a private S3 bucket. They were also able to set up a Data Pipeline to merge data samples into individual files and preprocess the data without having to conduct manual scrubbing.

Because of the time saved on data processing thanks to Edge Impulse, the Oura team could focus on the key drivers of their prediction. They only extracted three types of sensor data: heart rate, motion, and body temperature. After partitioning the data using five-fold cross-validation and classifying sleep stages, the team achieved a correlation of 79% - just a few percentage points off the standard. They readily deployed two types of sleep detection models: one simplified using just the ring's accelerometer and one more comprehensive leveraging Autonomic Nervous System (ANS)-mediated peripheral signals and circadian features. With Edge Impulse, they plan to conduct further analyses of different activity types and leverage the platform's scalability to continue experimenting with different data sources and subsets of extracted features.

While most ML research focuses on model-dominant steps such as training and finetuning, this case study underscores the importance of a holistic approach to MLOps, where even the initial steps of data aggregation and preprocessing fundamentally impact successful outcomes.

### ClinAIOps Case Study

Let's look at MLOps in the context of medical health monitoring to better understand how MLOps "matures" in a real-world deployment. Specifically, let's consider continuous therapeutic monitoring (CTM) enabled by wearable devices and sensors. CTM captures detailed physiological data from patients, providing the opportunity for more frequent and personalized adjustments to treatments.

Wearable ML-enabled sensors enable continuous physiological and activity monitoring outside clinics, opening up possibilities for timely, data-driven therapy adjustments. For example, wearable insulin biosensors [@psoma2023wearable] and wrist-worn ECG sensors for glucose monitoring [@li2021noninvasive] can automate insulin dosing for diabetes, wrist-worn ECG and PPG sensors can adjust blood thinners based on atrial fibrillation patterns [@attia2018noninvasive; @guo2019mobile], and accelerometers tracking gait can trigger preventative care for declining mobility in the elderly [@liu2022monitoring]. The variety of signals that can now be captured passively and continuously allows therapy titration and optimization tailored to each patient's changing needs. By closing the loop between physiological sensing and therapeutic response with TinyML and on-device learning, wearables are poised to transform many areas of personalized medicine.

ML holds great promise in analyzing CTM data to provide data-driven recommendations for therapy adjustments. But simply deploying AI models in silos, without integrating them properly into clinical workflows and decision-making, can lead to poor adoption or suboptimal outcomes. In other words, thinking about MLOps alone is insufficient to make them useful in practice. This study shows that frameworks are needed to incorporate AI and CTM into real-world clinical practice seamlessly.

This case study analyzes "ClinAIOps" as a model for embedded ML operations in complex clinical environments [@chen2023framework]. We provide an overview of the framework and why it's needed, walk through an application example, and discuss key implementation challenges related to model monitoring, workflow integration, and stakeholder incentives. Analyzing real-world examples like ClinAIOps illuminates crucial principles and best practices for reliable and effective AI Ops across many domains.

Traditional MLOps frameworks are insufficient for integrating continuous therapeutic monitoring and AI in clinical settings for a few key reasons:

* MLOps focuses on the ML model lifecycle—training, deployment, monitoring. But healthcare involves coordinating multiple human stakeholders—patients and clinicians—not just models.

* MLOps automates IT system monitoring and management. However, optimizing patient health requires personalized care and human oversight, not just automation.

* CTM and healthcare delivery are complex sociotechnical systems with many moving parts. MLOps doesn't provide a framework for coordinating human and AI decision-making.

* Ethical considerations regarding healthcare AI require human judgment, oversight, and accountability. MLOps frameworks lack processes for ethical oversight.

* Patient health data is highly sensitive and regulated. MLOps alone doesn't ensure the handling of protected health information to privacy and regulatory standards.

* Clinical validation of AI-guided treatment plans is essential for provider adoption. MLOps doesn't incorporate domain-specific evaluation of model recommendations.

* Optimizing healthcare metrics like patient outcomes requires aligning stakeholder incentives and workflows, which pure tech-focused MLOps overlooks.

Thus, effectively integrating AI/ML and CTM in clinical practice requires more than just model and data pipelines; it requires coordinating complex human-AI collaborative decision-making, which ClinAIOps addresses via its multi-stakeholder feedback loops.

#### Feedback Loops

The ClinAIOps framework, shown in @fig-clinaiops, provides these mechanisms through three feedback loops. The loops are useful for coordinating the insights from continuous physiological monitoring, clinician expertise, and AI guidance via feedback loops, enabling data-driven precision medicine while maintaining human accountability. ClinAIOps provides a model for effective human-AI symbiosis in healthcare: the patient is at the center, providing health challenges and goals that inform the therapy regimen; the clinician oversees this regimen, giving inputs for adjustments based on continuous monitoring data and health reports from the patient; whereas AI developers play a crucial role by creating systems that generate alerts for therapy updates, which the clinician then vets.

These feedback loops, which we will discuss below, help maintain clinician responsibility and control over treatment plans by reviewing AI suggestions before they impact patients. They help dynamically customize AI model behavior and outputs to each patient's changing health status. They help improve model accuracy and clinical utility over time by learning from clinician and patient responses. They facilitate shared decision-making and personalized care during patient-clinician interactions. They enable rapid optimization of therapies based on frequent patient data that clinicians cannot manually analyze.

![ClinAIOps cycle. Source: @chen2023framework.](images/png/clinaiops.png){#fig-clinaiops}

##### Patient-AI Loop

The patient-AI loop enables frequent therapy optimization driven by continuous physiological monitoring. Patients are prescribed wearables like smartwatches or skin patches to collect relevant health signals passively. For example, a diabetic patient could have a continuous glucose monitor, or a heart disease patient may wear an ECG patch. An AI model analyzes the patient's longitudinal health data streams in the context of their electronic medical records - their diagnoses, lab tests, medications, and demographics. The AI model suggests adjustments to the treatment regimen tailored to that individual, like changing a medication dose or administration schedule. Minor adjustments within a pre-approved safe range can be made by the patient independently, while major changes are reviewed by the clinician first. This tight feedback between the patient's physiology and AI-guided therapy allows data-driven, timely optimizations like automated insulin dosing recommendations based on real-time glucose levels for diabetes patients.

##### Clinician-AI Loop

The clinician-AI loop allows clinical oversight over AI-generated recommendations to ensure safety and accountability. The AI model provides the clinician with treatment recommendations and easily reviewed summaries of the relevant patient data on which the suggestions are based. For instance, an AI may suggest lowering a hypertension patient's blood pressure medication dose based on continuously low readings. The clinician can accept, reject, or modify the AI's proposed prescription changes. This clinician feedback further trains and improves the model.
Additionally, the clinician sets the bounds for the types and extent of treatment changes the AI can autonomously recommend to patients. By reviewing AI suggestions, the clinician maintains ultimate treatment authority based on their clinical judgment and accountability. This loop allows them to oversee patient cases with AI assistance efficiently.

##### Patient-Clinician Loop

Instead of routine data collection, the clinician can focus on interpreting high-level data patterns and collaborating with the patient to set health goals and priorities. The AI assistance will also free up clinicians' time, allowing them to focus more deeply on listening to patients' stories and concerns. For instance, the clinician may discuss diet and exercise changes with a diabetes patient to improve their glucose control based on their continuous monitoring data. Appointment frequency can also be dynamically adjusted based on patient progress rather than following a fixed calendar. Freed from basic data gathering, the clinician can provide coaching and care customized to each patient informed by their continuous health data. The patient-clinician relationship is made more productive and personalized.

#### Hypertension Case Example

Let's consider an example. According to the Centers for Disease Control and Prevention, nearly half of adults have hypertension (48.1%, 119.9 million). Hypertension can be managed through ClinAIOps with the help of wearable sensors using the following approach:

##### Data Collection

The data collected would include continuous blood pressure monitoring using a wrist-worn device equipped with photoplethysmography (PPG) and electrocardiography (ECG) sensors to estimate blood pressure [@zhang2017highly]. The wearable would also track the patient's physical activity via embedded accelerometers. The patient would log any antihypertensive medications they take, along with the time and dose. The patient's demographic details and medical history from their electronic health record (EHR) would also be incorporated. This multimodal real-world data provides valuable context for the AI model to analyze the patient's blood pressure patterns, activity levels, medication adherence, and responses to therapy.

##### AI Model

The on-device AI model would analyze the patient's continuous blood pressure trends, circadian patterns, physical activity levels, medication adherence behaviors, and other contexts. It would use ML to predict optimal antihypertensive medication doses and timing to control the individual's blood pressure. The model would send dosage change recommendations directly to the patient for minor adjustments or to the reviewing clinician for approval for more significant modifications. By observing clinician feedback on its recommendations and evaluating the resulting blood pressure outcomes in patients, the AI model could be continually retrained to improve performance. The goal is fully personalized blood pressure management optimized for each patient's needs and responses.

##### Patient-AI Loop

In the Patient-AI loop, the hypertensive patient would receive notifications on their wearable device or tethered smartphone app recommending adjustments to their antihypertensive medications. For minor dose changes within a pre-defined safe range, the patient could independently implement the AI model's suggested adjustment to their regimen. However, the patient must obtain clinician approval before changing their dosage for more significant modifications. Providing personalized and timely medication recommendations automates an element of hypertension self-management for the patient. It can improve their adherence to the regimen as well as treatment outcomes. The patient is empowered to leverage AI insights to control their blood pressure better.

##### Clinician-AI Loop

In the Clinician-AI loop, the provider would receive summaries of the patient's continuous blood pressure trends and visualizations of their medication-taking patterns and adherence. They review the AI model's suggested antihypertensive dosage changes and decide whether to approve, reject, or modify the recommendations before they reach the patient. The clinician also specifies the boundaries for how much the AI can independently recommend changing dosages without clinician oversight. If the patient's blood pressure is trending at dangerous levels, the system alerts the clinician so they can promptly intervene and adjust medications or request an emergency room visit. This loop maintains accountability and safety while allowing the clinician to harness AI insights by keeping the clinician in charge of approving major treatment changes.

##### Patient-Clinician Loop

In the Patient-Clinician loop, shown in @fig-interactive-loop, the in-person visits would focus less on collecting data or basic medication adjustments. Instead, the clinician could interpret high-level trends and patterns in the patient's continuous monitoring data and have focused discussions about diet, exercise, stress management, and other lifestyle changes to improve their blood pressure control holistically. The frequency of appointments could be dynamically optimized based on the patient's stability rather than following a fixed calendar. Since the clinician would not need to review all the granular data, they could concentrate on delivering personalized care and recommendations during visits. With continuous monitoring and AI-assisted optimization of medications between visits, the clinician-patient relationship focuses on overall wellness goals and becomes more impactful. This proactive and tailored data-driven approach can help avoid hypertension complications like stroke, heart failure, and other threats to patient health and well-being.

![ClinAIOps interactive loop. Source: @chen2023framework.](images/png/clinaiops_loops.png){#fig-interactive-loop}

#### MLOps vs. ClinAIOps Comparison

The hypertension example illustrates well why traditional MLOps are insufficient for many real-world AI applications and why frameworks like ClinAIOps are needed instead.

With hypertension, simply developing and deploying an ML model for adjusting medications would only succeed if it considered the broader clinical context. The patient, clinician, and health system have concerns about shaping adoption. The AI model cannot optimize blood pressure outcomes alone—it requires integrating with workflows, behaviors, and incentives.

* Some key gaps the example highlights in a pure MLOps approach:
* The model itself would lack the real-world patient data at scale to recommend treatments reliably. ClinAIOps enables this by collecting feedback from clinicians and patients via continuous monitoring.
* Clinicians would only trust model recommendations with transparency, explainability, and accountability. ClinAIOps keeps the clinician in the loop to build confidence.
* Patients need personalized coaching and motivation - not just AI notifications. The ClinAIOps patient-clinician loop facilitates this.
* Sensor reliability and data accuracy would only be sufficient with clinical oversight. ClinAIOps validates recommendations.
* Liability for treatment outcomes must be clarified with just an ML model. ClinAIOps maintains human accountability.
* Health systems would need to demonstrate value to change workflows. ClinAIOps aligns stakeholders.

The hypertension case clearly shows the need to look beyond training and deploying a performant ML model to consider the entire human-AI sociotechnical system. This is the key gap ClinAIOps addresses over traditional MLOps. Traditional MLOps is overly tech-focused on automating ML model development and deployment, while ClinAIOps incorporates clinical context and human-AI coordination through multi-stakeholder feedback loops.

@tbl-clinical_ops compares them. This table highlights how, when MLOps is implemented, we need to consider more than just ML models.

+------------------------+---------------------------------------------+----------------------------------------------+
|                        | Traditional MLOps                           | ClinAIOps                                    |
+:=======================+:============================================+:=============================================+
| Focus                  | ML model development and deployment         | Coordinating human and AI decision-making    |
+------------------------+---------------------------------------------+----------------------------------------------+
| Stakeholders           | Data scientists, IT engineers               | Patients, clinicians, AI developers          |
+------------------------+---------------------------------------------+----------------------------------------------+
| Feedback loops         | Model retraining, monitoring                | Patient-AI, clinician-AI, patient-clinician  |
+------------------------+---------------------------------------------+----------------------------------------------+
| Objective              | Operationalize ML deployments               | Optimize patient health outcomes             |
+------------------------+---------------------------------------------+----------------------------------------------+
| Processes              | Automated pipelines and infrastructure      | Integrates clinical workflows and oversight  |
+------------------------+---------------------------------------------+----------------------------------------------+
| Data considerations    | Building training datasets                  | Privacy, ethics, protected health information|
+------------------------+---------------------------------------------+----------------------------------------------+
| Model validation       | Testing model performance metrics           | Clinical evaluation of recommendations       |
+------------------------+---------------------------------------------+----------------------------------------------+
| Implementation         | Focuses on technical integration            | Aligns incentives of human stakeholders      |
+------------------------+---------------------------------------------+----------------------------------------------+

: Comparison of MLOps versus AI operations for clinical use. {#tbl-clinical_ops .striped .hover}

#### Summary

In complex domains like healthcare, successfully deploying AI requires moving beyond a narrow focus on training and deploying performant ML models. As illustrated through the hypertension example, real-world integration of AI necessitates coordinating diverse stakeholders, aligning incentives, validating recommendations, and maintaining accountability. Frameworks like ClinAIOps, which facilitate collaborative human-AI decision-making through integrated feedback loops, are needed to address these multifaceted challenges. Rather than just automating tasks, AI must augment human capabilities and clinical workflows. This allows AI to positively impact patient outcomes, population health, and healthcare efficiency.

## Conclusion

Embedded ML is poised to transform many industries by enabling AI capabilities directly on edge devices like smartphones, sensors, and IoT hardware. However, developing and deploying TinyML models on resource-constrained embedded systems poses unique challenges compared to traditional cloud-based MLOps.

This chapter provided an in-depth analysis of key differences between traditional and embedded MLOps across the model lifecycle, development workflows, infrastructure management, and operational practices. We discussed how factors like intermittent connectivity, decentralized data, and limited on-device computing necessitate innovative techniques like federated learning, on-device inference, and model optimization. Architectural patterns like cross-device learning and hierarchical edge-cloud infrastructure help mitigate constraints.

Through concrete examples like Oura Ring and ClinAIOps, we demonstrated applied principles for embedded MLOps. The case studies highlighted critical considerations beyond core ML engineering, like aligning stakeholder incentives, maintaining accountability, and coordinating human-AI decision-making. This underscores the need for a holistic approach spanning both technical and human elements.

While embedded MLOps face impediments, emerging tools like Edge Impulse and lessons from pioneers help accelerate TinyML innovation. A solid understanding of foundational MLOps principles tailored to embedded environments will empower more organizations to overcome constraints and deliver distributed AI capabilities. As frameworks and best practices mature, seamlessly integrating ML into edge devices and processes will transform industries through localized intelligence.

## Resources

Here is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.

:::{.callout-note collapse="false" title="Slides"}

These slides serve as a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage both students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.

* [MLOps, DevOps, and AIOps.](https://docs.google.com/presentation/d/1vsC8WpmvVRgMTpzTltAhEGzcVohMkatMZBqm3-P8TUY/edit?usp=drive_link)

* [MLOps overview.](https://docs.google.com/presentation/d/1GVduKipd0ughTpqsHupGqAPW70h0xNOOpaIeSqLOc1M/edit?usp=drive_link)

* [Tiny MLOps.](https://docs.google.com/presentation/d/1MNjVOcx5f5Nfe3ElDqTxutezcGXm4yI8PkjWOuQYHhk/edit?usp=drive_link)

* [MLOps: a use case.](https://docs.google.com/presentation/d/1449rzplaL0lOPoKh0mrpds3KPPoOHWdR5LIZdd7aXhA/edit#slide=id.g2ddfdf6e85f_0_0)

* [MLOps: Key Activities and Lifecycle.](https://docs.google.com/presentation/d/1vGCffLgemxTwTIo7vUea5CibOV7y3vY3pkJdee-y5eA/edit#slide=id.g2de2d5f2ac0_0_0)

* [ML Lifecycle.](https://docs.google.com/presentation/d/1FW8Q1Yj5g_jbArFANfncbLQj36uV2vfV8pjoqaD6gjM/edit#slide=id.g94db9f9f78_0_2)

* [Scaling TinyML: Challenges and Opportunities.](https://docs.google.com/presentation/d/1VxwhVztoTk3eG04FD9fFNpj2lVrVjYYPJi3jBz0O_mo/edit?resourcekey=0-bV7CCIPr7SxZf2p61oB_CA#slide=id.g94db9f9f78_0_2)

* Training Operationalization:
  * [Training Ops: CI/CD trigger.](https://docs.google.com/presentation/d/1YyRY6lOzdC7NjutJSvl_VXYu29qwHKqx0y98zAUCJCU/edit?resourcekey=0-PTh1FxqkQyhOO0bKKHBldQ#slide=id.g94db9f9f78_0_2)

  * [Continuous Integration.](https://docs.google.com/presentation/d/1poGgYTH44X0dVGwG9FGIyVwot4EET_jJOt-4kgcQawo/edit?usp=drive_link)

  * [Continuous Deployment.](https://docs.google.com/presentation/d/1nxbIluROAOl5cN6Ug4Dm-mHh1Fwm5aEng_S5iLfiCqo/edit?usp=drive_link&resourcekey=0-xFOl8i7ea2vNtiilXz8CaQ)

  * [Production Deployment.](https://docs.google.com/presentation/d/1m8KkCZRnbJCCTWsmcwMt9EJhYLoaVG_Wm7zUE2bQkZI/edit?usp=drive_link)

  * [Production Deployment: Online Experimentation.](https://docs.google.com/presentation/d/1elFEK61X5Kc-5UV_4AEtRvCT7l1TqTdABmJV8uAYykY/edit?usp=drive_link)

  * [Training Ops Impact on MLOps.](https://docs.google.com/presentation/d/1-6QL2rq0ahGVz8BL1M1BT0lR-HDxsHady9lGTN93wLc/edit?usp=drive_link&resourcekey=0-sRqqoa7pX9IkDDSwe2MLyw)

* Model Deployment:
  * [Scaling ML Into Production Deployment.](https://docs.google.com/presentation/d/12sf-PvSxDIlCQCXULWy4jLY_2fIq-jpRojRsmeMGq6k/edit?resourcekey=0-knPSQ5h4ffhgeV6CXvwlSg#slide=id.gf209f12c63_0_314)

  * [Containers for Scaling ML Deployment.](https://docs.google.com/presentation/d/1YXE4cAWMwL79Vqr_8TJi-LsQD9GFdiyBqY--HcoBpKg/edit?usp=drive_link&resourcekey=0-yajtiQTx2SdJ6BCVG0Bfng)

  * [Challenges for Scaling TinyML Deployment: Part 1.](https://docs.google.com/presentation/d/1mw5FFERf5r-q8R7iyNf6kx2MMcwNOTBd5WwFOj8Zs20/edit?resourcekey=0-u80KeJio3iIWco00crGD9g#slide=id.gdc4defd718_0_0)

  * [Challenges for Scaling TinyML Deployment: Part 2.](https://docs.google.com/presentation/d/1NB63wTHoEPGSn--KqFu1vjHx3Ild9AOhpBbflJP-k7I/edit?usp=drive_link&resourcekey=0-MsEi1Lba2dpl0G-bzakHJQ)

  * [Model Deployment Impact on MLOps.](https://docs.google.com/presentation/d/1A0pfm55s03dFbYKKFRV-x7pRCm_2-VpoIM0O9kW0TAA/edit?usp=drive_link&resourcekey=0--O2AFFmVzAmz5KO0mJeVHA)

:::

:::{.callout-important collapse="false" title="Videos"}

* @vid-mlops

* @vid-datapipe

* @vid-monitoring

* @vid-deploy

:::

:::{.callout-caution collapse="false" title="Exercises"}

To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.

* @exr-ei
:::
