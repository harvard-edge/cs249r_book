<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; AI Frameworks – Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/training/training.html" rel="next">
<link href="../../../contents/core/data_engineering/data_engineering.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-fc6d358c97f25a8ea829b86655043430.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-5c6c0ad7bdfb89369003da8042cd4f02.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-a14f63a0e19d6d184e2c452ae6c32a55.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/frameworks/frameworks.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Frameworks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/copyright.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/dedication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedication</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors &amp; Thanks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ML Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ML Operations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/generative_ai/generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shared Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">6.1</span> Introduction</a></li>
  <li><a href="#framework-evolution" id="toc-framework-evolution" class="nav-link" data-scroll-target="#framework-evolution"><span class="header-section-number">6.2</span> Framework Evolution</a></li>
  <li><a href="#sec-deep_dive_into_tensorflow" id="toc-sec-deep_dive_into_tensorflow" class="nav-link" data-scroll-target="#sec-deep_dive_into_tensorflow"><span class="header-section-number">6.3</span> Deep Dive into TensorFlow</a>
  <ul>
  <li><a href="#tf-ecosystem" id="toc-tf-ecosystem" class="nav-link" data-scroll-target="#tf-ecosystem"><span class="header-section-number">6.3.1</span> TF Ecosystem</a></li>
  <li><a href="#static-computation-graph" id="toc-static-computation-graph" class="nav-link" data-scroll-target="#static-computation-graph"><span class="header-section-number">6.3.2</span> Static Computation Graph</a></li>
  <li><a href="#usability-deployment" id="toc-usability-deployment" class="nav-link" data-scroll-target="#usability-deployment"><span class="header-section-number">6.3.3</span> Usability &amp; Deployment</a></li>
  <li><a href="#architecture-design" id="toc-architecture-design" class="nav-link" data-scroll-target="#architecture-design"><span class="header-section-number">6.3.4</span> Architecture Design</a></li>
  <li><a href="#built-in-functionality-keras" id="toc-built-in-functionality-keras" class="nav-link" data-scroll-target="#built-in-functionality-keras"><span class="header-section-number">6.3.5</span> Built-in Functionality &amp; Keras</a></li>
  <li><a href="#limitations-and-challenges" id="toc-limitations-and-challenges" class="nav-link" data-scroll-target="#limitations-and-challenges"><span class="header-section-number">6.3.6</span> Limitations and Challenges</a></li>
  <li><a href="#sec-pytorch_vs_tensorflow" id="toc-sec-pytorch_vs_tensorflow" class="nav-link" data-scroll-target="#sec-pytorch_vs_tensorflow"><span class="header-section-number">6.3.7</span> PyTorch vs.&nbsp;TensorFlow</a></li>
  </ul></li>
  <li><a href="#basic-framework-components" id="toc-basic-framework-components" class="nav-link" data-scroll-target="#basic-framework-components"><span class="header-section-number">6.4</span> Basic Framework Components</a>
  <ul>
  <li><a href="#sec-tensor-data-structures" id="toc-sec-tensor-data-structures" class="nav-link" data-scroll-target="#sec-tensor-data-structures"><span class="header-section-number">6.4.1</span> Tensor data structures</a></li>
  <li><a href="#computational-graphs" id="toc-computational-graphs" class="nav-link" data-scroll-target="#computational-graphs"><span class="header-section-number">6.4.2</span> Computational graphs</a>
  <ul class="collapse">
  <li><a href="#graph-definition" id="toc-graph-definition" class="nav-link" data-scroll-target="#graph-definition">Graph Definition</a></li>
  <li><a href="#static-vs.-dynamic-graphs" id="toc-static-vs.-dynamic-graphs" class="nav-link" data-scroll-target="#static-vs.-dynamic-graphs">Static vs.&nbsp;Dynamic Graphs</a></li>
  </ul></li>
  <li><a href="#data-pipeline-tools" id="toc-data-pipeline-tools" class="nav-link" data-scroll-target="#data-pipeline-tools"><span class="header-section-number">6.4.3</span> Data Pipeline Tools</a>
  <ul class="collapse">
  <li><a href="#data-loaders" id="toc-data-loaders" class="nav-link" data-scroll-target="#data-loaders">Data Loaders</a></li>
  </ul></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation"><span class="header-section-number">6.4.4</span> Data Augmentation</a></li>
  <li><a href="#loss-functions-and-optimization-algorithms" id="toc-loss-functions-and-optimization-algorithms" class="nav-link" data-scroll-target="#loss-functions-and-optimization-algorithms"><span class="header-section-number">6.4.5</span> Loss Functions and Optimization Algorithms</a></li>
  <li><a href="#model-training-support" id="toc-model-training-support" class="nav-link" data-scroll-target="#model-training-support"><span class="header-section-number">6.4.6</span> Model Training Support</a></li>
  <li><a href="#validation-and-analysis" id="toc-validation-and-analysis" class="nav-link" data-scroll-target="#validation-and-analysis"><span class="header-section-number">6.4.7</span> Validation and Analysis</a>
  <ul class="collapse">
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization">Visualization</a></li>
  </ul></li>
  <li><a href="#differentiable-programming" id="toc-differentiable-programming" class="nav-link" data-scroll-target="#differentiable-programming"><span class="header-section-number">6.4.8</span> Differentiable programming</a></li>
  <li><a href="#hardware-acceleration" id="toc-hardware-acceleration" class="nav-link" data-scroll-target="#hardware-acceleration"><span class="header-section-number">6.4.9</span> Hardware Acceleration</a></li>
  </ul></li>
  <li><a href="#sec-ai_frameworks-advanced" id="toc-sec-ai_frameworks-advanced" class="nav-link" data-scroll-target="#sec-ai_frameworks-advanced"><span class="header-section-number">6.5</span> Advanced Features</a>
  <ul>
  <li><a href="#distributed-training" id="toc-distributed-training" class="nav-link" data-scroll-target="#distributed-training"><span class="header-section-number">6.5.1</span> Distributed training</a></li>
  <li><a href="#model-conversion" id="toc-model-conversion" class="nav-link" data-scroll-target="#model-conversion"><span class="header-section-number">6.5.2</span> Model Conversion</a></li>
  <li><a href="#automl-no-codelow-code-ml" id="toc-automl-no-codelow-code-ml" class="nav-link" data-scroll-target="#automl-no-codelow-code-ml"><span class="header-section-number">6.5.3</span> AutoML, No-Code/Low-Code ML</a></li>
  <li><a href="#advanced-learning-methods" id="toc-advanced-learning-methods" class="nav-link" data-scroll-target="#advanced-learning-methods"><span class="header-section-number">6.5.4</span> Advanced Learning Methods</a>
  <ul class="collapse">
  <li><a href="#transfer-learning" id="toc-transfer-learning" class="nav-link" data-scroll-target="#transfer-learning">Transfer Learning</a></li>
  <li><a href="#federated-learning" id="toc-federated-learning" class="nav-link" data-scroll-target="#federated-learning">Federated Learning</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#framework-specialization" id="toc-framework-specialization" class="nav-link" data-scroll-target="#framework-specialization"><span class="header-section-number">6.6</span> Framework Specialization</a>
  <ul>
  <li><a href="#cloud" id="toc-cloud" class="nav-link" data-scroll-target="#cloud"><span class="header-section-number">6.6.1</span> Cloud</a></li>
  <li><a href="#edge" id="toc-edge" class="nav-link" data-scroll-target="#edge"><span class="header-section-number">6.6.2</span> Edge</a></li>
  <li><a href="#embedded" id="toc-embedded" class="nav-link" data-scroll-target="#embedded"><span class="header-section-number">6.6.3</span> Embedded</a></li>
  </ul></li>
  <li><a href="#sec-ai_frameworks_embedded" id="toc-sec-ai_frameworks_embedded" class="nav-link" data-scroll-target="#sec-ai_frameworks_embedded"><span class="header-section-number">6.7</span> Embedded AI Frameworks</a>
  <ul>
  <li><a href="#resource-constraints" id="toc-resource-constraints" class="nav-link" data-scroll-target="#resource-constraints"><span class="header-section-number">6.7.1</span> Resource Constraints</a></li>
  <li><a href="#frameworks-libraries" id="toc-frameworks-libraries" class="nav-link" data-scroll-target="#frameworks-libraries"><span class="header-section-number">6.7.2</span> Frameworks &amp; Libraries</a></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges"><span class="header-section-number">6.7.3</span> Challenges</a>
  <ul class="collapse">
  <li><a href="#fragmented-ecosystem" id="toc-fragmented-ecosystem" class="nav-link" data-scroll-target="#fragmented-ecosystem">Fragmented Ecosystem</a></li>
  <li><a href="#disparate-hardware-needs" id="toc-disparate-hardware-needs" class="nav-link" data-scroll-target="#disparate-hardware-needs">Disparate Hardware Needs</a></li>
  <li><a href="#lack-of-portability" id="toc-lack-of-portability" class="nav-link" data-scroll-target="#lack-of-portability">Lack of Portability</a></li>
  <li><a href="#incomplete-infrastructure" id="toc-incomplete-infrastructure" class="nav-link" data-scroll-target="#incomplete-infrastructure">Incomplete Infrastructure</a></li>
  <li><a href="#no-standard-benchmark" id="toc-no-standard-benchmark" class="nav-link" data-scroll-target="#no-standard-benchmark">No Standard Benchmark</a></li>
  <li><a href="#minimal-real-world-testing" id="toc-minimal-real-world-testing" class="nav-link" data-scroll-target="#minimal-real-world-testing">Minimal Real-World Testing</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="header-section-number">6.8</span> Examples</a>
  <ul>
  <li><a href="#interpreter" id="toc-interpreter" class="nav-link" data-scroll-target="#interpreter"><span class="header-section-number">6.8.1</span> Interpreter</a></li>
  <li><a href="#compiler-based" id="toc-compiler-based" class="nav-link" data-scroll-target="#compiler-based"><span class="header-section-number">6.8.2</span> Compiler-based</a></li>
  <li><a href="#library" id="toc-library" class="nav-link" data-scroll-target="#library"><span class="header-section-number">6.8.3</span> Library</a></li>
  </ul></li>
  <li><a href="#choosing-the-right-framework" id="toc-choosing-the-right-framework" class="nav-link" data-scroll-target="#choosing-the-right-framework"><span class="header-section-number">6.9</span> Choosing the Right Framework</a>
  <ul>
  <li><a href="#model" id="toc-model" class="nav-link" data-scroll-target="#model"><span class="header-section-number">6.9.1</span> Model</a></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software"><span class="header-section-number">6.9.2</span> Software</a></li>
  <li><a href="#hardware" id="toc-hardware" class="nav-link" data-scroll-target="#hardware"><span class="header-section-number">6.9.3</span> Hardware</a></li>
  <li><a href="#other-factors" id="toc-other-factors" class="nav-link" data-scroll-target="#other-factors"><span class="header-section-number">6.9.4</span> Other Factors</a>
  <ul class="collapse">
  <li><a href="#performance" id="toc-performance" class="nav-link" data-scroll-target="#performance">Performance</a></li>
  <li><a href="#scalability" id="toc-scalability" class="nav-link" data-scroll-target="#scalability">Scalability</a></li>
  <li><a href="#integration-with-data-engineering-tools" id="toc-integration-with-data-engineering-tools" class="nav-link" data-scroll-target="#integration-with-data-engineering-tools">Integration with Data Engineering Tools</a></li>
  <li><a href="#integration-with-model-optimization-tools" id="toc-integration-with-model-optimization-tools" class="nav-link" data-scroll-target="#integration-with-model-optimization-tools">Integration with Model Optimization Tools</a></li>
  <li><a href="#ease-of-use" id="toc-ease-of-use" class="nav-link" data-scroll-target="#ease-of-use">Ease of Use</a></li>
  <li><a href="#community-support" id="toc-community-support" class="nav-link" data-scroll-target="#community-support">Community Support</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#future-trends-in-ml-frameworks" id="toc-future-trends-in-ml-frameworks" class="nav-link" data-scroll-target="#future-trends-in-ml-frameworks"><span class="header-section-number">6.10</span> Future Trends in ML Frameworks</a>
  <ul>
  <li><a href="#decomposition" id="toc-decomposition" class="nav-link" data-scroll-target="#decomposition"><span class="header-section-number">6.10.1</span> Decomposition</a></li>
  <li><a href="#high-performance-compilers-libraries" id="toc-high-performance-compilers-libraries" class="nav-link" data-scroll-target="#high-performance-compilers-libraries"><span class="header-section-number">6.10.2</span> High-Performance Compilers &amp; Libraries</a></li>
  <li><a href="#ml-for-ml-frameworks" id="toc-ml-for-ml-frameworks" class="nav-link" data-scroll-target="#ml-for-ml-frameworks"><span class="header-section-number">6.10.3</span> ML for ML Frameworks</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6.11</span> Conclusion</a></li>
  <li><a href="#sec-ai-frameworks-resource" id="toc-sec-ai-frameworks-resource" class="nav-link" data-scroll-target="#sec-ai-frameworks-resource"><span class="header-section-number">6.12</span> Resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/frameworks/frameworks.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/frameworks/frameworks.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-ai_frameworks" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Resources: <a href="#sec-ai-frameworks-resource">Slides</a>, <a href="#sec-ai-frameworks-resource">Videos</a>, <a href="#sec-ai-frameworks-resource">Exercises</a>, <a href="#sec-ai-frameworks-resource">Labs</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ml_frameworks.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E 3 Prompt: Illustration in a rectangular format, designed for a professional textbook, where the content spans the entire width. The vibrant chart represents training and inference frameworks for ML. Icons for TensorFlow, Keras, PyTorch, ONNX, and TensorRT are spread out, filling the entire horizontal space, and aligned vertically. Each icon is accompanied by brief annotations detailing their features. The lively colors like blues, greens, and oranges highlight the icons and sections against a soft gradient background. The distinction between training and inference frameworks is accentuated through color-coded sections, with clean lines and modern typography maintaining clarity and focus.</em></figcaption>
</figure>
</div>
<p>This chapter explores the landscape of AI frameworks that serve as the foundation for developing machine learning systems. AI frameworks provide the tools, libraries, and environments to design, train, and deploy machine learning models. We explore the evolutionary trajectory of these frameworks, dissect the workings of TensorFlow, and provide insights into the core components and advanced features that define these frameworks.</p>
<p>Furthermore, we investigate the specialization of frameworks tailored to specific needs, the emergence of frameworks specifically designed for embedded AI, and the criteria for selecting the most suitable framework for your project. This exploration will be rounded off by a glimpse into the future trends expected to shape the landscape of ML frameworks in the coming years.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand the evolution and capabilities of major machine learning frameworks. This includes graph execution models, programming paradigms, hardware acceleration support, and how they have expanded over time.</p></li>
<li><p>Learn frameworks’ core components and functionality, such as computational graphs, data pipelines, optimization algorithms, training loops, etc., that enable efficient model building.</p></li>
<li><p>Compare frameworks across different environments, such as cloud, edge, and TinyML. Learn how frameworks specialize based on computational constraints and hardware.</p></li>
<li><p>Dive deeper into embedded and TinyML-focused frameworks like TensorFlow Lite Micro, CMSIS-NN, TinyEngine, etc., and how they optimize for microcontrollers.</p></li>
<li><p>When choosing a framework, explore model conversion and deployment considerations, including latency, memory usage, and hardware support.</p></li>
<li><p>Evaluate key factors in selecting the right framework, like performance, hardware compatibility, community support, ease of use, etc., based on the specific project needs and constraints.</p></li>
<li><p>Understand the limitations of current frameworks and potential future trends, such as using ML to improve frameworks, decomposed ML systems, and high-performance compilers.</p></li>
</ul>
</div>
</div>
<section id="introduction" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">6.1</span> Introduction</h2>
<p>Machine learning frameworks provide the tools and infrastructure to efficiently build, train, and deploy machine learning models. In this chapter, we will explore the evolution and key capabilities of major frameworks like <a href="https://www.tensorflow.org/">TensorFlow (TF)</a>, <a href="https://pytorch.org/">PyTorch</a>, and specialized frameworks for embedded devices. We will dive into the components like computational graphs, optimization algorithms, hardware acceleration, and more that enable developers to construct performant models quickly. Understanding these frameworks is essential to leverage the power of deep learning across the spectrum from cloud to edge devices.</p>
<p>ML frameworks handle much of the complexity of model development through high-level APIs and domain-specific languages that allow practitioners to quickly construct models by combining pre-made components and abstractions. For example, frameworks like TensorFlow and PyTorch provide Python APIs to define neural network architectures using layers, optimizers, datasets, and more. This enables rapid iteration compared to coding every model detail from scratch.</p>
<p>A key capability offered by these frameworks is distributed training engines that can scale model training across clusters of GPUs and TPUs. This makes it feasible to train state-of-the-art models with billions or trillions of parameters on vast datasets. Frameworks also integrate with specialized hardware like NVIDIA GPUs to further accelerate training via optimizations like parallelization and efficient matrix operations.</p>
<p>In addition, frameworks simplify deploying finished models into production through tools like <a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a> for scalable model serving and <a href="https://www.tensorflow.org/lite">TensorFlow Lite</a> for optimization on mobile and edge devices. Other valuable capabilities include visualization, model optimization techniques like quantization and pruning, and monitoring metrics during training.</p>
<p>Leading open-source frameworks like TensorFlow, PyTorch, and <a href="https://mxnet.apache.org/versions/1.9.1/">MXNet</a> power much of AI research and development today. Commercial offerings like <a href="https://aws.amazon.com/pm/sagemaker/">Amazon SageMaker</a> and <a href="https://azure.microsoft.com/en-us/free/machine-learning/search/?ef_id=_k_CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE_k_&amp;OCID=AIDcmm5edswduu_SEM__k_CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE_k_&amp;gad=1&amp;gclid=CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE">Microsoft Azure Machine Learning</a> integrate these open source frameworks with proprietary capabilities and enterprise tools.</p>
<p>Machine learning engineers and practitioners leverage these robust frameworks to focus on high-value tasks like model architecture, feature engineering, and hyperparameter tuning instead of infrastructure. The goal is to build and deploy performant models that solve real-world problems efficiently.</p>
<p>In this chapter, we will explore today’s leading cloud frameworks and how they have adapted models and tools specifically for embedded and edge deployment. We will compare programming models, supported hardware, optimization capabilities, and more to fully understand how frameworks enable scalable machine learning from the cloud to the edge.</p>
</section>
<section id="framework-evolution" class="level2 page-columns page-full" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="framework-evolution"><span class="header-section-number">6.2</span> Framework Evolution</h2>
<p>Machine learning frameworks have evolved significantly to meet the diverse needs of machine learning practitioners and advancements in AI techniques. A few decades ago, building and training machine learning models required extensive low-level coding and infrastructure. Alongside the need for low-level coding, early neural network research was constrained by insufficient data and computing power. However, machine learning frameworks have evolved considerably over the past decade to meet the expanding needs of practitioners and rapid advances in deep learning techniques. The release of large datasets like <a href="https://www.image-net.org/">ImageNet</a> <span class="citation" data-cites="deng2009imagenet">(<a href="#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>)</span> and advancements in parallel GPU computing unlocked the potential for far deeper neural networks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. <span>“<span>ImageNet:</span> <span>A</span> Large-Scale Hierarchical Image Database.”</span> In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. IEEE. <a href="https://doi.org/10.1109/cvpr.2009.5206848">https://doi.org/10.1109/cvpr.2009.5206848</a>.
</div><div id="ref-al2016theano" class="csl-entry" role="listitem">
Team, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et al. 2016. <span>“Theano: <span>A</span> Python Framework for Fast Computation of Mathematical Expressions.”</span> <a href="https://arxiv.org/abs/1605.02688">https://arxiv.org/abs/1605.02688</a>.
</div><div id="ref-jia2014caffe" class="csl-entry" role="listitem">
Jia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. <span>“Caffe: Convolutional Architecture for Fast Feature Embedding.”</span> In <em>Proceedings of the 22nd ACM International Conference on Multimedia</em>, 675–78. ACM. <a href="https://doi.org/10.1145/2647868.2654889">https://doi.org/10.1145/2647868.2654889</a>.
</div><div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. <span>“<span>ImageNet</span> Classification with Deep Convolutional Neural Networks.”</span> In <em>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake Tahoe, Nevada, United States</em>, edited by Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Weinberger, 1106–14. <a href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a>.
</div><div id="ref-chollet2018keras" class="csl-entry" role="listitem">
Chollet, François. 2018. <span>“Introduction to Keras.”</span> <em>March 9th</em>.
</div><div id="ref-tokui2015chainer" class="csl-entry" role="listitem">
Tokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki Yamazaki Vincent. 2019. <span>“Chainer: A Deep Learning Framework for Accelerating the Research Cycle.”</span> In <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp;Amp; Data Mining</em>, 5:1–6. ACM. <a href="https://doi.org/10.1145/3292500.3330756">https://doi.org/10.1145/3292500.3330756</a>.
</div><div id="ref-seide2016cntk" class="csl-entry" role="listitem">
Seide, Frank, and Amit Agarwal. 2016. <span>“Cntk: Microsoft’s Open-Source Deep-Learning Toolkit.”</span> In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 2135–35. ACM. <a href="https://doi.org/10.1145/2939672.2945397">https://doi.org/10.1145/2939672.2945397</a>.
</div><div id="ref-paszke2019pytorch" class="csl-entry" role="listitem">
Ansel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. <span>“<span>PyTorch</span> 2: <span>Faster</span> Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.”</span> In <em>Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>, edited by Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, 8024–35. ACM. <a href="https://doi.org/10.1145/3620665.3640366">https://doi.org/10.1145/3620665.3640366</a>.
</div></div><p>The first ML frameworks, <a href="https://pypi.org/project/Theano/#:~:text=Theano">Theano</a> by <span class="citation" data-cites="al2016theano">Team et al. (<a href="#ref-al2016theano" role="doc-biblioref">2016</a>)</span> and <a href="https://caffe.berkeleyvision.org/">Caffe</a> by <span class="citation" data-cites="jia2014caffe">Jia et al. (<a href="#ref-jia2014caffe" role="doc-biblioref">2014</a>)</span>, were developed by academic institutions. Theano was created by the Montreal Institute for Learning Algorithms, while Caffe was developed by the Berkeley Vision and Learning Center. Amid growing interest in deep learning due to state-of-the-art performance of AlexNet <span class="citation" data-cites="krizhevsky2012imagenet">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky2012imagenet" role="doc-biblioref">2012</a>)</span> on the ImageNet dataset, private companies and individuals began developing ML frameworks, resulting in frameworks such as <a href="https://keras.io/">Keras</a> by <span class="citation" data-cites="chollet2018keras">Chollet (<a href="#ref-chollet2018keras" role="doc-biblioref">2018</a>)</span>, <a href="https://chainer.org/">Chainer</a> by <span class="citation" data-cites="tokui2015chainer">Tokui et al. (<a href="#ref-tokui2015chainer" role="doc-biblioref">2019</a>)</span>, TensorFlow from Google <span class="citation" data-cites="abadi2016tensorflow">(<a href="#ref-abadi2016tensorflow" role="doc-biblioref">Yu et al. 2018</a>)</span>, <a href="https://learn.microsoft.com/en-us/cognitive-toolkit/">CNTK</a> by Microsoft <span class="citation" data-cites="seide2016cntk">(<a href="#ref-seide2016cntk" role="doc-biblioref">Seide and Agarwal 2016</a>)</span>, and PyTorch by Facebook <span class="citation" data-cites="paszke2019pytorch">(<a href="#ref-paszke2019pytorch" role="doc-biblioref">Ansel et al. 2024</a>)</span>.</p>
<p>Many of these ML frameworks can be divided into high-level vs.&nbsp;low-level frameworks and static vs.&nbsp;dynamic computational graph frameworks. High-level frameworks provide a higher level of abstraction than low-level frameworks. High-level frameworks have pre-built functions and modules for common ML tasks, such as creating, training, and evaluating common ML models, preprocessing data, engineering features, and visualizing data, which low-level frameworks do not have. Thus, high-level frameworks may be easier to use but are less customizable than low-level frameworks (i.e., users of low-level frameworks can define custom layers, loss functions, optimization algorithms, etc.). Examples of high-level frameworks include TensorFlow/Keras and PyTorch. Examples of low-level ML frameworks include TensorFlow with low-level APIs, Theano, Caffe, Chainer, and CNTK.</p>
<p>Frameworks like Theano and Caffe used static computational graphs, which required defining the full model architecture upfront, thus limiting flexibility. In contract, dynamic graphs are constructed on the fly for more iterative development. Around 2016, frameworks like PyTorch and TensorFlow 2.0 began adopting dynamic graphs, providing greater flexibility for model development. We will discuss these concepts and details later in the AI Training section.</p>
<p>The development of these frameworks facilitated an explosion in model size and complexity over time—from early multilayer perceptrons and convolutional networks to modern transformers with billions or trillions of parameters. In 2016, ResNet models by <span class="citation" data-cites="he2016deep">He et al. (<a href="#ref-he2016deep" role="doc-biblioref">2016</a>)</span> achieved record ImageNet accuracy with over 150 layers and 25 million parameters. Then, in 2020, the GPT-3 language model from OpenAI <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> pushed parameters to an astonishing 175 billion using model parallelism in frameworks to train across thousands of GPUs and TPUs.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–78. IEEE. <a href="https://doi.org/10.1109/cvpr.2016.90">https://doi.org/10.1109/cvpr.2016.90</a>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual</em>, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.
</div></div><p>Each generation of frameworks unlocked new capabilities that powered advancement:</p>
<ul>
<li><p>Theano and TensorFlow (2015) introduced computational graphs and automatic differentiation to simplify model building.</p></li>
<li><p>CNTK (2016) pioneered efficient distributed training by combining model and data parallelism.</p></li>
<li><p>PyTorch (2016) provided imperative programming and dynamic graphs for flexible experimentation.</p></li>
<li><p>TensorFlow 2.0 (2019) defaulted eager execution for intuitiveness and debugging.</p></li>
<li><p>TensorFlow Graphics (2020) added 3D data structures to handle point clouds and meshes.</p></li>
</ul>
<p>In recent years, the landscape of machine learning frameworks has significantly consolidated. <a href="#fig-ml-framework" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ml-framework</span></a> illustrates this convergence, showing that TensorFlow and PyTorch have become the overwhelmingly dominant ML frameworks, collectively representing more than 95% of ML frameworks used in research and production. While both frameworks have risen to prominence, they have distinct characteristics. <a href="#fig-tensorflow-pytorch" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tensorflow-pytorch</span></a> draws a contrast between the attributes of TensorFlow and PyTorch, helping to explain their complementary dominance in the field.</p>
<div id="fig-tensorflow-pytorch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensorflow-pytorch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/tensorflowpytorch.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensorflow-pytorch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: PyTorch vs.&nbsp;TensorFlow: Features and Functions. Source: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fkruschecompany.com%2Fpytorch-vs-tensorflow%2F&amp;psig=AOvVaw1-DSFxXYprQmYH7Z4Nk6Tk&amp;ust=1722533288351000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqFwoTCPDhst7m0YcDFQAAAAAdAAAAABAg">K&amp;C</a>
</figcaption>
</figure>
</div>
<div id="fig-ml-framework" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image6.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Popularity of ML frameworks in the United States as measured by Google web searches. Source: Google.
</figcaption>
</figure>
</div>
<p>A one-size-fits-all approach does not work well across the spectrum from cloud to tiny edge devices. Different frameworks represent various philosophies around graph execution, declarative versus imperative APIs, and more. Declaratives define what the program should do, while imperatives focus on how it should be done step-by-step. For instance, TensorFlow uses graph execution and declarative-style modeling, while PyTorch adopts eager execution and imperative modeling for more Pythonic flexibility. Each approach carries tradeoffs which we will discuss in <a href="#sec-pytorch_vs_tensorflow" class="quarto-xref"><span class="quarto-unresolved-ref">sec-pytorch_vs_tensorflow</span></a>.</p>
<p>Today’s advanced frameworks enable practitioners to develop and deploy increasingly complex models - a key driver of innovation in the AI field. These frameworks continue to evolve and expand their capabilities for the next generation of machine learning. To understand how these systems continue to evolve, we will dive deeper into TensorFlow as an example of how the framework grew in complexity over time.</p>
</section>
<section id="sec-deep_dive_into_tensorflow" class="level2 page-columns page-full" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="sec-deep_dive_into_tensorflow"><span class="header-section-number">6.3</span> Deep Dive into TensorFlow</h2>
<p>TensorFlow was developed by the Google Brain team and was released as an open-source software library on November 9, 2015. It was designed for numerical computation using data flow graphs and has since become popular for a wide range of machine learning and deep learning applications.</p>
<p>TensorFlow is a training and inference framework that provides built-in functionality to handle everything from model creation and training to deployment, as shown in <a href="#fig-tensorflow-architecture" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tensorflow-architecture</span></a>. Since its initial development, the TensorFlow ecosystem has grown to include many different “varieties” of TensorFlow, each intended to allow users to support ML on different platforms. In this section, we will mainly discuss only the core package.</p>
<section id="tf-ecosystem" class="level3 page-columns page-full" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="tf-ecosystem"><span class="header-section-number">6.3.1</span> TF Ecosystem</h3>
<ol type="1">
<li><p><a href="https://www.tensorflow.org/tutorials">TensorFlow Core</a>: primary package that most developers engage with. It provides a comprehensive, flexible platform for defining, training, and deploying machine learning models. It includes <a href="https://www.tensorflow.org/guide/keras">tf.keras</a> as its high-level API.</p></li>
<li><p><a href="https://www.tensorflow.org/lite">TensorFlow Lite</a>: designed for deploying lightweight models on mobile, embedded, and edge devices. It offers tools to convert TensorFlow models to a more compact format suitable for limited-resource devices and provides optimized pre-trained models for mobile.</p></li>
<li><p><a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite Micro</a>: designed for running machine learning models on microcontrollers with minimal resources. It operates without the need for operating system support, standard C or C++ libraries, or dynamic memory allocation, using only a few kilobytes of memory.</p></li>
<li><p><a href="https://www.tensorflow.org/js">TensorFlow.js</a>: JavaScript library that allows training and deployment of machine learning models directly in the browser or on Node.js. It also provides tools for porting pre-trained TensorFlow models to the browser-friendly format.</p></li>
<li><p><a href="https://developers.googleblog.com/2019/03/introducing-coral-our-platform-for.html">TensorFlow on Edge Devices (Coral)</a>: platform of hardware components and software tools from Google that allows the execution of TensorFlow models on edge devices, leveraging Edge TPUs for acceleration.</p></li>
<li><p><a href="https://www.tensorflow.org/federated">TensorFlow Federated (TFF)</a>: framework for machine learning and other computations on decentralized data. TFF facilitates federated learning, allowing model training across many devices without centralizing the data.</p></li>
<li><p><a href="https://www.tensorflow.org/graphics">TensorFlow Graphics</a>: library for using TensorFlow to carry out graphics-related tasks, including 3D shapes and point clouds processing, using deep learning.</p></li>
<li><p><a href="https://www.tensorflow.org/hub">TensorFlow Hub</a>: repository of reusable machine learning model components to allow developers to reuse pre-trained model components, facilitating transfer learning and model composition.</p></li>
<li><p><a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a>: framework designed for serving and deploying machine learning models for inference in production environments. It provides tools for versioning and dynamically updating deployed models without service interruption.</p></li>
<li><p><a href="https://www.tensorflow.org/tfx">TensorFlow Extended (TFX)</a>: end-to-end platform designed to deploy and manage machine learning pipelines in production settings. TFX encompasses data validation, preprocessing, model training, validation, and serving components.</p></li>
</ol>
<div id="fig-tensorflow-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensorflow-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/tensorflow.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensorflow-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: Architecture overview of TensorFlow 2.0. Source: <a href="https://blog.tensorflow.org/2019/01/whats-coming-in-tensorflow-2-0.html">Tensorflow.</a>
</figcaption>
</figure>
</div>
<p>TensorFlow was developed to address the limitations of DistBelief <span class="citation" data-cites="abadi2016tensorflow">(<a href="#ref-abadi2016tensorflow" role="doc-biblioref">Yu et al. 2018</a>)</span>—the framework in use at Google from 2011 to 2015—by providing flexibility along three axes: 1) defining new layers, 2) refining training algorithms, and 3) defining new training algorithms. To understand what limitations in DistBelief led to the development of TensorFlow, we will first give a brief overview of the Parameter Server Architecture that DistBelief employed <span class="citation" data-cites="dean2012large">(<a href="#ref-dean2012large" role="doc-biblioref">Dean et al. 2012</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-abadi2016tensorflow" class="csl-entry" role="listitem">
Yu, Yuan, Martı́n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, et al. 2018. <span>“Dynamic Control Flow in Large-Scale Machine Learning.”</span> In <em>Proceedings of the Thirteenth EuroSys Conference</em>, 265–83. ACM. <a href="https://doi.org/10.1145/3190508.3190551">https://doi.org/10.1145/3190508.3190551</a>.
</div><div id="ref-dean2012large" class="csl-entry" role="listitem">
Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, et al. 2012. <span>“Large Scale Distributed Deep Networks.”</span> In <em>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake Tahoe, Nevada, United States</em>, edited by Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Weinberger, 1232–40. <a href="https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html">https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html</a>.
</div></div><p>The Parameter Server (PS) architecture is a popular design for distributing the training of machine learning models, especially deep neural networks, across multiple machines. The fundamental idea is to separate the storage and management of model parameters from the computation used to update these parameters. Typically, parameter servers handle the storage and management of model parameters, partitioning them across multiple servers. Worker processes perform the computational tasks, including data processing and computation of gradients, which are then sent back to the parameter servers for updating.</p>
<p><strong>Storage:</strong> The stateful parameter server processes handled the storage and management of model parameters. Given the large scale of models and the system’s distributed nature, these parameters were shared across multiple parameter servers. Each server maintained a portion of the model parameters, making it "stateful" as it had to maintain and manage this state across the training process.</p>
<p><strong>Computation:</strong> The worker processes, which could be run in parallel, were stateless and purely computational. They processed data and computed gradients without maintaining any state or long-term memory <span class="citation" data-cites="li2014communication">(<a href="#ref-li2014communication" role="doc-biblioref">M. Li et al. 2014</a>)</span>. Workers did not retain information between different tasks. Instead, they periodically communicated with the parameter servers to retrieve the latest parameters and send back computed gradients.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2014communication" class="csl-entry" role="listitem">
Li, Mu, David G. Andersen, Alexander J. Smola, and Kai Yu. 2014. <span>“Communication Efficient Distributed Machine Learning with the Parameter Server.”</span> In <em>Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada</em>, edited by Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, 19–27. <a href="https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html">https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html</a>.
</div></div><div id="exr-tfc" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise&nbsp;6.1: TensorFlow Core
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let’s comprehensively understand core machine learning algorithms using TensorFlow and their practical applications in data analysis and predictive modeling. We will start with linear regression to predict survival rates from the Titanic dataset. Then, using TensorFlow, we will construct classifiers to identify different species of flowers based on their attributes. Next, we will use the K-Means algorithm and its application in segmenting datasets into cohesive clusters. Finally, we will apply hidden Markov models (HMM) to foresee weather patterns.</p>
<p><a href="https://colab.research.google.com/drive/15Cyy2H7nT40sGR7TBN5wBvgTd57mVKay#scrollTo=IEeIRxlbx0wY"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
<div id="exr-tfl" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise&nbsp;6.2: TensorFlow Lite
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here, we will see how to build a miniature machine-learning model for microcontrollers. We will build a mini neural network that is streamlined to learn from data even with limited resources and optimized for deployment by shrinking our model for efficient use on microcontrollers. TensorFlow Lite, a powerful technology derived from TensorFlow, shrinks models for tiny devices and helps enable on-device features like image recognition in smart devices. It is used in edge computing to allow for faster analysis and decisions in devices processing data locally.</p>
<p><a href="https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/TFLite-Micro-Hello-World/train_TFL_Micro_hello_world_model.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
<p>DistBelief and its architecture defined above were crucial in enabling distributed deep learning at Google but also introduced limitations that motivated the development of TensorFlow:</p>
</section>
<section id="static-computation-graph" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="static-computation-graph"><span class="header-section-number">6.3.2</span> Static Computation Graph</h3>
<p>Model parameters are distributed across various parameter servers in the parameter server architecture. Since DistBelief was primarily designed for the neural network paradigm, parameters corresponded to a fixed neural network structure. If the computation graph were dynamic, the distribution and coordination of parameters would become significantly more complicated. For example, a change in the graph might require the initialization of new parameters or the removal of existing ones, complicating the management and synchronization tasks of the parameter servers. This made it harder to implement models outside the neural framework or models that required dynamic computation graphs.</p>
<p>TensorFlow was designed as a more general computation framework that expresses computation as a data flow graph. This allows for a wider variety of machine learning models and algorithms outside of neural networks and provides flexibility in refining models.</p>
</section>
<section id="usability-deployment" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="usability-deployment"><span class="header-section-number">6.3.3</span> Usability &amp; Deployment</h3>
<p>The parameter server model delineates roles (worker nodes and parameter servers) and is optimized for data center deployments, which might only be optimal for some use cases. For instance, this division introduces overheads or complexities on edge devices or in other non-data center environments.</p>
<p>TensorFlow was built to run on multiple platforms, from mobile devices and edge devices to cloud infrastructure. It also aimed to be lighter and developer-friendly and to provide ease of use between local and distributed training.</p>
</section>
<section id="architecture-design" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="architecture-design"><span class="header-section-number">6.3.4</span> Architecture Design</h3>
<p>Rather than using the parameter server architecture, TensorFlow deploys tasks across a cluster. These tasks are named processes that can communicate over a network, and each can execute TensorFlow’s core construct, the dataflow graph, and interface with various computing devices (like CPUs or GPUs). This graph is a directed representation where nodes symbolize computational operations, and edges depict the tensors (data) flowing between these operations.</p>
<p>Despite the absence of traditional parameter servers, some “PS tasks” still store and manage parameters reminiscent of parameter servers in other systems. The remaining tasks, which usually handle computation, data processing, and gradient calculations, are referred to as “worker tasks.” TensorFlow’s PS tasks can execute any computation representable by the dataflow graph, meaning they aren’t just limited to parameter storage, and the computation can be distributed. This capability makes them significantly more versatile and gives users the power to program the PS tasks using the standard TensorFlow interface, the same one they’d use to define their models. As mentioned above, dataflow graphs’ structure also makes them inherently good for parallelism, allowing for the processing of large datasets.</p>
</section>
<section id="built-in-functionality-keras" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="built-in-functionality-keras"><span class="header-section-number">6.3.5</span> Built-in Functionality &amp; Keras</h3>
<p>TensorFlow includes libraries to help users develop and deploy more use-case-specific models, and since this framework is open-source, this list continues to grow. These libraries address the entire ML development lifecycle: data preparation, model building, deployment, and responsible AI.</p>
<p>One of TensorFlow’s biggest advantages is its integration with Keras, though, as we will cover in the next section, Pytorch recently added a Keras integration. Keras is another ML framework built to be extremely user-friendly and, as a result, has a high level of abstraction. We will cover Keras in more depth later in this chapter. However, when discussing its integration with TensorFlow, it was important to note that it was originally built to be backend-agnostic. This means users could abstract away these complexities, offering a cleaner, more intuitive way to define and train models without worrying about compatibility issues with different backends. TensorFlow users had some complaints about the usability and readability of TensorFlow’s API, so as TF gained prominence, it integrated Keras as its high-level API. This integration offered major benefits to TensorFlow users since it introduced more intuitive readability and portability of models while still taking advantage of powerful backend features, Google support, and infrastructure to deploy models on various platforms.</p>
<div id="exr-k" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise&nbsp;6.3: Exploring Keras: Building, Training, and Evaluating Neural Networks
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here, we’ll learn how to use Keras, a high-level neural network API, for model development and training. We will explore the functional API for concise model building, understand loss and metric classes for model evaluation, and use built-in optimizers to update model parameters during training. Additionally, we’ll discover how to define custom layers and metrics tailored to our needs. Lastly, we’ll look into Keras’ training loops to streamline the process of training neural networks on large datasets. This knowledge will empower us to build and optimize neural network models across various machine learning and artificial intelligence applications.</p>
<p><a href="https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO#scrollTo=fxINLLGitX_n"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="limitations-and-challenges" class="level3" data-number="6.3.6">
<h3 data-number="6.3.6" class="anchored" data-anchor-id="limitations-and-challenges"><span class="header-section-number">6.3.6</span> Limitations and Challenges</h3>
<p>TensorFlow is one of the most popular deep learning frameworks, but it has faced criticisms and weaknesses, primarily related to usability and resource usage. While advantageous, the rapid pace of updates through its support from Google has sometimes led to backward compatibility issues, deprecated functions, and shifting documentation. Additionally, even with the Keras implementation, TensorFlow’s syntax and learning curve can be difficult for new users. Another major critique of TensorFlow is its high overhead and memory consumption due to the range of built-in libraries and support. While pared-down versions can address some of these concerns, they may still be limited in resource-constrained environments.</p>
</section>
<section id="sec-pytorch_vs_tensorflow" class="level3" data-number="6.3.7">
<h3 data-number="6.3.7" class="anchored" data-anchor-id="sec-pytorch_vs_tensorflow"><span class="header-section-number">6.3.7</span> PyTorch vs.&nbsp;TensorFlow</h3>
<p>PyTorch and TensorFlow have established themselves as frontrunners in the industry. Both frameworks offer robust functionalities but differ in design philosophies, ease of use, ecosystem, and deployment capabilities.</p>
<p><strong>Design Philosophy and Programming Paradigm:</strong> PyTorch uses a dynamic computational graph termed eager execution. This makes it intuitive and facilitates debugging since operations are executed immediately and can be inspected on the fly. In comparison, earlier versions of TensorFlow were centered around a static computational graph, which required the graph’s complete definition before execution. However, TensorFlow 2.0 introduced eager execution by default, making it more aligned with PyTorch. PyTorch’s dynamic nature and Python-based approach have enabled its simplicity and flexibility, particularly for rapid prototyping. TensorFlow’s static graph approach in its earlier versions had a steeper learning curve; the introduction of TensorFlow 2.0, with its Keras integration as the high-level API, has significantly simplified the development process.</p>
<p><strong>Deployment:</strong> PyTorch is heavily favored in research environments, but deploying PyTorch models in production settings has traditionally been challenging. However, deployment has become more feasible with the introduction of TorchScript, the TorchServe tool, and <a href="https://pytorch.org/mobile/home/">PyTorch Mobile</a>. TensorFlow stands out for its strong scalability and deployment capabilities, particularly on embedded and mobile platforms with TensorFlow Lite. TensorFlow Serving and TensorFlow.js further facilitate deployment in various environments, thus giving it a broader reach in the ecosystem.</p>
<p><strong>Performance:</strong> Both frameworks offer efficient hardware acceleration for their operations. However, TensorFlow has a slightly more robust optimization workflow, such as the XLA (Accelerated Linear Algebra) compiler, which can further boost performance. Its static computational graph was also advantageous for certain optimizations in the early versions.</p>
<p><strong>Ecosystem:</strong> PyTorch has a growing ecosystem with tools like TorchServe for serving models and libraries like TorchVision, TorchText, and TorchAudio for specific domains. As we mentioned earlier, TensorFlow has a broad and mature ecosystem. TensorFlow Extended (TFX) provides an end-to-end platform for deploying production machine learning pipelines. Other tools and libraries include TensorFlow Lite, TensorFlow Lite Micro, TensorFlow.js, TensorFlow Hub, and TensorFlow Serving. <a href="#tbl-pytorch_vs_tf" class="quarto-xref">Table&nbsp;<span class="quarto-unresolved-ref">tbl-pytorch_vs_tf</span></a> provides a comparative analysis:</p>
<div id="tbl-pytorch_vs_tf" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pytorch_vs_tf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.1: Comparison of PyTorch and TensorFlow.
</figcaption>
<div aria-describedby="tbl-pytorch_vs_tf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 31%">
<col style="width: 53%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Pytorch</th>
<th style="text-align: left;">TensorFlow</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Design Philosophy</td>
<td style="text-align: left;">Dynamic computational graph (eager execution)</td>
<td style="text-align: left;">Static computational graph (early versions); Eager execution in TensorFlow 2.0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Deployment</td>
<td style="text-align: left;">Traditionally challenging; Improved with TorchScript &amp; TorchServe</td>
<td style="text-align: left;">Scalable, especially on embedded platforms with TensorFlow Lite</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Performance &amp; Optimization</td>
<td style="text-align: left;">Efficient GPU acceleration</td>
<td style="text-align: left;">Robust optimization with XLA compiler</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ecosystem</td>
<td style="text-align: left;">TorchServe, TorchVision, TorchText, TorchAudio, PyTorch Mobile</td>
<td style="text-align: left;">TensorFlow Extended (TFX), TensorFlow Lite, TensorFlow Lite Micro TensorFlow.js, TensorFlow Hub, TensorFlow Serving</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Ease of Use</td>
<td style="text-align: left;">Preferred for its Pythonic approach and rapid prototyping</td>
<td style="text-align: left;">Initially steep learning curve; Simplified with Keras in TensorFlow 2.0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="basic-framework-components" class="level2 page-columns page-full" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="basic-framework-components"><span class="header-section-number">6.4</span> Basic Framework Components</h2>
<p>Having introduced the popular machine learning frameworks and provided a high-level comparison, this section will introduce you to the core functionalities that form the fabric of these frameworks. It will cover the special structure called tensors, which these frameworks use to handle complex multi-dimensional data more easily. You will also learn how these frameworks represent different types of neural network architectures and their required operations through computational graphs. Additionally, you will see how they offer tools that make the development of machine learning models more abstract and efficient, such as data loaders, implemented loss optimization algorithms, efficient differentiation techniques, and the ability to accelerate your training process on hardware accelerators.</p>
<section id="sec-tensor-data-structures" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="sec-tensor-data-structures"><span class="header-section-number">6.4.1</span> Tensor data structures</h3>
<p>As shown in the figure, vectors can be represented as a stack of numbers in a 1-dimensional array. Matrices follow the same idea, and one can think of them as many vectors stacked on each other, making them 2 dimensional. Higher dimensional tensors work the same way. A 3-dimensional tensor, as illustrated in <a href="#fig-tensor-data-structure-a" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tensor-data-structure-a</span></a>, is simply a set of matrices stacked on each other in another direction. Therefore, vectors and matrices can be considered special cases of tensors with 1D and 2D dimensions, respectively.</p>
<div id="fig-tensor-data-structure-a" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensor-data-structure-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-data-structure-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: Visualization of Tensor Data Structure.
</figcaption>
</figure>
</div>
<p>Tensors offer a flexible structure that can represent data in higher dimensions. <a href="#fig-tensor-data-structure-b" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tensor-data-structure-b</span></a> illustrates how this concept applies to image data. As shown in the figure, images are not represented by just one matrix of pixel values. Instead, they typically have three channels, where each channel is a matrix containing pixel values that represent the intensity of red, green, or blue. Together, these channels create a colored image. Without tensors, storing all this information from multiple matrices can be complex. However, as <a href="#fig-tensor-data-structure-b" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tensor-data-structure-b</span></a> illustrates, tensors make it easy to contain image data in a single 3-dimensional structure, with each number representing a certain color value at a specific location in the image.</p>
<div id="fig-tensor-data-structure-b" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensor-data-structure-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/color_channels_of_image.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-data-structure-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: Visualization of colored image structure that can be easily stored as a 3D Tensor. Credit: <a href="https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff">Niklas Lang</a>
</figcaption>
</figure>
</div>
<p>You don’t have to stop there. If we wanted to store a series of images, we could use a 4-dimensional tensor, where the new dimension represents different images. This means you are storing multiple images, each having three matrices that represent the three color channels. This gives you an idea of the usefulness of tensors when dealing with multi-dimensional data efficiently.</p>
<p>Tensors also have a unique attribute that enables frameworks to automatically compute gradients, simplifying the implementation of complex models and optimization algorithms. In machine learning, as discussed in <a href="../../../contents/core/dl_primer/dl_primer.html#sec-backward_pass">Chapter 3</a>, backpropagation requires taking the derivative of equations. One of the key features of tensors in PyTorch and TensorFlow is their ability to track computations and calculate gradients. This is crucial for backpropagation in neural networks. For example, in PyTorch, you can use the <code>requires_grad</code> attribute, which allows you to automatically compute and store gradients during the backward pass, facilitating the optimization process. Similarly, in TensorFlow, <code>tf.GradientTape</code> records operations for automatic differentiation.</p>
<p>Consider this simple mathematical equation that you want to differentiate. Mathematically, you can compute the gradient in the following way:</p>
<p>Given: <span class="math display">\[
y = x^2
\]</span></p>
<p>The derivative of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(x\)</span> is: <span class="math display">\[
\frac{dy}{dx} = 2x
\]</span></p>
<p>When <span class="math inline">\(x = 2\)</span>: <span class="math display">\[
\frac{dy}{dx} = 2*2 = 4
\]</span></p>
<p>The gradient of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(x\)</span>, at <span class="math inline">\(x = 2\)</span>, is 4.</p>
<p>A powerful feature of tensors in PyTorch and TensorFlow is their ability to easily compute derivatives (gradients). Here are the corresponding code examples in PyTorch and TensorFlow:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" aria-current="page">PyTorch</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">TensorFlow</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tensor with gradient tracking</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple function</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the gradient</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y.backward()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the gradient</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.grad)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>tensor(<span class="fl">4.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tensor with gradient tracking</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.Variable(<span class="fl">2.0</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple function</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the gradient</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> tape.gradient(y, x)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the gradient</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(grad)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>tf.Tensor(<span class="fl">4.0</span>, shape<span class="op">=</span>(), dtype<span class="op">=</span>float32)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>This automatic differentiation is a powerful feature of tensors in frameworks like PyTorch and TensorFlow, making it easier to implement and optimize complex machine learning models.</p>
</section>
<section id="computational-graphs" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="computational-graphs"><span class="header-section-number">6.4.2</span> Computational graphs</h3>
<section id="graph-definition" class="level4">
<h4 class="anchored" data-anchor-id="graph-definition">Graph Definition</h4>
<p>Computational graphs are a key component of deep learning frameworks like TensorFlow and PyTorch. They allow us to express complex neural network architectures efficiently and differently. A computational graph consists of a directed acyclic graph (DAG) where each node represents an operation or variable, and edges represent data dependencies between them.</p>
<p>It is important to differentiate computational graphs from neural network diagrams, such as those for multilayer perceptrons (MLPs), which depict nodes and layers. Neural network diagrams, as depicted in <a href="../../../contents/core/dl_primer/dl_primer.html">Chapter 3</a>, visualize the architecture and flow of data through nodes and layers, providing an intuitive understanding of the model’s structure. In contrast, computational graphs provide a low-level representation of the underlying mathematical operations and data dependencies required to implement and train these networks.</p>
<p>For example, a node might represent a matrix multiplication operation, taking two input matrices (or tensors) and producing an output matrix (or tensor). To visualize this, consider the simple example in <a href="#fig-comp-graph" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-comp-graph</span></a>. The directed acyclic graph computes <span class="math inline">\(z = x \times y\)</span>, where each variable is just numbers.</p>
<div id="fig-comp-graph" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-comp-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image1.png" style="width:50.0%" data-align="center" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-comp-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.6: Basic example of a computational graph.
</figcaption>
</figure>
</div>
<p>Frameworks like TensorFlow and PyTorch create computational graphs to implement the architectures of neural networks that we typically represent with diagrams. When you define a neural network layer in code (e.g., a dense layer in TensorFlow), the framework constructs a computational graph that includes all the necessary operations (such as matrix multiplication, addition, and activation functions) and their data dependencies. This graph enables the framework to efficiently manage the flow of data, optimize the execution of operations, and automatically compute gradients for training. Underneath the hood, the computational graphs represent abstractions for common layers like convolutional, pooling, recurrent, and dense layers, with data including activations, weights, and biases represented in tensors. This representation allows for efficient computation, leveraging the structure of the graph to parallelize operations and apply optimizations.</p>
<p>Some common layers that computational graphs might implement include convolutional layers, attention layers, recurrent layers, and dense layers. Layers serve as higher-level abstractions that define specific computations on top of the basic operations represented in the graph. For example, a Dense layer performs matrix multiplication and addition between input, weight, and bias tensors. It is important to note that a layer operates on tensors as inputs and outputs; the layer itself is not a tensor. Some key differences between layers and tensors are:</p>
<ul>
<li><p>Layers contain states like weights and biases. Tensors are stateless, just holding data.</p></li>
<li><p>Layers can modify internal state during training. Tensors are immutable/read-only.</p></li>
<li><p>Layers are higher-level abstractions. Tensors are at a lower level and directly represent data and math operations.</p></li>
<li><p>Layers define fixed computation patterns. Tensors flow between layers during execution.</p></li>
<li><p>Layers are used indirectly when building models. Tensors flow between layers during execution.</p></li>
</ul>
<p>So, while tensors are a core data structure that layers consume and produce, layers have additional functionality for defining parameterized operations and training. While a layer configures tensor operations under the hood, the layer remains distinct from the tensor objects. The layer abstraction makes building and training neural networks much more intuitive. This abstraction enables developers to build models by stacking these layers together without implementing the layer logic. For example, calling <code>tf.keras.layers.Conv2D</code> in TensorFlow creates a convolutional layer. The framework handles computing the convolutions, managing parameters, etc. This simplifies model development, allowing developers to focus on architecture rather than low-level implementations. Layer abstractions use highly optimized implementations for performance. They also enable portability, as the same architecture can run on different hardware backends like GPUs and TPUs.</p>
<p>In addition, computational graphs include activation functions like ReLU, sigmoid, and tanh that are essential to neural networks, and many frameworks provide these as standard abstractions. These functions introduce non-linearities that enable models to approximate complex functions. Frameworks provide these as simple, predefined operations that can be used when constructing models, for example, if.nn.relu in TensorFlow. This abstraction enables flexibility, as developers can easily swap activation functions for tuning performance. Predefined activations are also optimized by the framework for faster execution.</p>
<p>In recent years, models like ResNets and MobileNets have emerged as popular architectures, with current frameworks pre-packaging these as computational graphs. Rather than worrying about the fine details, developers can use them as a starting point, customizing as needed by substituting layers. This simplifies and speeds up model development, avoiding reinventing architectures from scratch. Predefined models include well-tested, optimized implementations that ensure good performance. Their modular design also enables transferring learned features to new tasks via transfer learning. These predefined architectures provide high-performance building blocks to create robust models quickly.</p>
<p>These layer abstractions, activation functions, and predefined architectures the frameworks provide constitute a computational graph. When a user defines a layer in a framework (e.g., <code>tf.keras.layers.Dense()</code>), the framework configures computational graph nodes and edges to represent that layer. The layer parameters like weights and biases become variables in the graph. The layer computations become operation nodes (such as the x and y in the figure above). When you call an activation function like <code>tf.nn.relu()</code>, the framework adds a ReLU operation node to the graph. Predefined architectures are just pre-configured subgraphs that can be inserted into your model’s graph. Thus, model definition via high-level abstractions creates a computational graph—the layers, activations, and architectures we use become graph nodes and edges.</p>
<p>We implicitly construct a computational graph when defining a neural network architecture in a framework. The framework uses this graph to determine operations to run during training and inference. Computational graphs bring several advantages over raw code, and that’s one of the core functionalities that is offered by a good ML framework:</p>
<ul>
<li><p>Explicit representation of data flow and operations</p></li>
<li><p>Ability to optimize graph before execution</p></li>
<li><p>Automatic differentiation for training</p></li>
<li><p>Language agnosticism - graph can be translated to run on GPUs, TPUs, etc.</p></li>
<li><p>Portability - graph can be serialized, saved, and restored later</p></li>
</ul>
<p>Computational graphs are the fundamental building blocks of ML frameworks. Model definition via high-level abstractions creates a computational graph—the layers, activations, and architectures we use become graph nodes and edges. The framework compilers and optimizers operate on this graph to generate executable code. The abstractions provide a developer-friendly API for building computational graphs. Under the hood, it’s still graphs down! So, while you may not directly manipulate graphs as a framework user, they enable your high-level model specifications to be efficiently executed. The abstractions simplify model-building, while computational graphs make it possible.</p>
</section>
<section id="static-vs.-dynamic-graphs" class="level4">
<h4 class="anchored" data-anchor-id="static-vs.-dynamic-graphs">Static vs.&nbsp;Dynamic Graphs</h4>
<p>Deep learning frameworks have traditionally followed one of two approaches for expressing computational graphs.</p>
<p><strong>Static graphs (declare-then-execute):</strong> With this model, the entire computational graph must be defined upfront before running it. All operations and data dependencies must be specified during the declaration phase. TensorFlow originally followed this static approach - models were defined in a separate context, and then a session was created to run them. The benefit of static graphs is they allow more aggressive optimization since the framework can see the full graph. However, it also tends to be less flexible for research and interactivity. Changes to the graph require re-declaring the full model.</p>
<p>For example:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.placeholder(tf.float32)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.matmul(x, weights) <span class="op">+</span> biases</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In this example, x is a placeholder for input data, and y is the result of a matrix multiplication operation followed by an addition. The model is defined in this declaration phase, where all operations and variables must be specified upfront.</p>
<p>Once the entire graph is defined, the framework compiles and optimizes it. This means that the computational steps are set in stone, and the framework can apply various optimizations to improve efficiency and performance. When you later execute the graph, you provide the actual input tensors, and the pre-defined operations are carried out in the optimized sequence.</p>
<p>This approach is similar to building a blueprint where every detail is planned before construction begins. While this allows for powerful optimizations, it also means that any changes to the model require redefining the entire graph from scratch.</p>
<p><strong>Dynamic graphs (define-by-run):</strong> Unlike declaring (all) first and then executing, the graph is built dynamically as execution happens. There is no separate declaration phase - operations execute immediately as defined. This style is imperative and flexible, facilitating experimentation.</p>
<p>PyTorch uses dynamic graphs, building the graph on the fly as execution happens. For example, consider the following code snippet, where the graph is built as the execution is taking place:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">4</span>,<span class="dv">784</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.matmul(x, weights) <span class="op">+</span> biases</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The above example does not have separate compile/build/run phases. Ops define and execute immediately. With dynamic graphs, the definition is intertwined with execution, providing a more intuitive, interactive workflow. However, the downside is that there is less potential for optimization since the framework only sees the graph as it is built. <a href="#fig-static-vs-dynamic" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-static-vs-dynamic</span></a> demonstrates the differences between a static and dynamic computation graph.</p>
<div id="fig-static-vs-dynamic" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-static-vs-dynamic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/staticvsdynamic.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-static-vs-dynamic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.7: Comparing static and dynamic graphs. Source: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fdev-jm.tistory.com%2F4&amp;psig=AOvVaw0r1cZbZa6iImYP-fesrN7H&amp;ust=1722533107591000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBQQjhxqFwoTCLC8nYHm0YcDFQAAAAAdAAAAABAY">Dev</a>
</figcaption>
</figure>
</div>
<p>Recently, the distinction has blurred as frameworks adopt both modes. TensorFlow 2.0 defaults to dynamic graph mode while letting users work with static graphs when needed. Dynamic declaration offers flexibility and ease of use, making frameworks more user-friendly, while static graphs provide optimization benefits at the cost of interactivity. The ideal framework balances these approaches. <a href="#tbl-exec-graph" class="quarto-xref">Table&nbsp;<span class="quarto-unresolved-ref">tbl-exec-graph</span></a> compares the pros and cons of static versus dynamic execution graphs:</p>
<div id="tbl-exec-graph" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-exec-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.2: Comparison between Static (Declare-then-execute) and Dynamic (Define-by-run) Execution Graphs, highlighting their respective pros and cons.
</figcaption>
<div aria-describedby="tbl-exec-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 35%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Execution Graph</th>
<th style="text-align: left;">Pros</th>
<th style="text-align: left;">Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Static (Declare-then-execute)</td>
<td style="text-align: left;"><ul>
<li>Enable graph optimizations by seeing full model ahead of time</li>
<li>Can export and deploy frozen graphs</li>
<li>Graph is packaged independently of code</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Less flexible for research and iteration</li>
<li>Changes require rebuilding graph</li>
<li>Execution has separate compile and run phases</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: left;">Dynamic (Define-by-run)</td>
<td style="text-align: left;"><ul>
<li>Intuitive imperative style like Python code</li>
<li>Interleave graph build with execution</li>
<li>Easy to modify graphs</li>
<li>Debugging seamlessly fits workflow</li>
</ul></td>
<td style="text-align: left;"><ul>
<li>Harder to optimize without full graph</li>
<li>Possible slowdowns from graph building during execution</li>
<li>Can require more memory</li>
</ul></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="data-pipeline-tools" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="data-pipeline-tools"><span class="header-section-number">6.4.3</span> Data Pipeline Tools</h3>
<p>Computational graphs can only be as good as the data they learn from and work on. Therefore, feeding training data efficiently is crucial for optimizing deep neural network performance, though it is often overlooked as one of the core functionalities. Many modern AI frameworks provide specialized pipelines to ingest, process, and augment datasets for model training.</p>
<section id="data-loaders" class="level4">
<h4 class="anchored" data-anchor-id="data-loaders">Data Loaders</h4>
<p>At the core of these pipelines are data loaders, which handle reading training examples from sources like files, databases, and object storage. Data loaders facilitate efficient data loading and preprocessing, crucial for deep learning models. For instance, TensorFlow’s <a href="https://www.tensorflow.org/guide/data">tf.data</a> dataloading pipeline is designed to manage this process. Depending on the application, deep learning models require diverse data formats such as CSV files or image folders. Some popular formats include:</p>
<ul>
<li><p><strong>CSV</strong>: A versatile, simple format often used for tabular data.</p></li>
<li><p><strong>TFRecord</strong>: TensorFlow’s proprietary format, optimized for performance.</p></li>
<li><p><strong>Parquet</strong>: Columnar storage, offering efficient data compression and retrieval.</p></li>
<li><p><strong>JPEG/PNG</strong>: Commonly used for image data.</p></li>
<li><p><strong>WAV/MP3</strong>: Prevalent formats for audio data.</p></li>
</ul>
<p>Data loaders batch examples to leverage vectorization support in hardware. Batching refers to grouping multiple data points for simultaneous processing, leveraging the vectorized computation capabilities of hardware like GPUs. While typical batch sizes range from 32 to 512 examples, the optimal size often depends on the data’s memory footprint and the specific hardware constraints. Advanced loaders can stream virtually unlimited datasets from disk and cloud storage. They stream large datasets from disks or networks instead of fully loading them into memory, enabling unlimited dataset sizes.</p>
<p>Data loaders can also shuffle data across epochs for randomization and preprocess features in parallel with model training to expedite the training process. Randomly shuffling the order of examples between training epochs reduces bias and improves generalization.</p>
<p>Data loaders also support caching and prefetching strategies to optimize data delivery for fast, smooth model training. Caching preprocessed batches in memory allows them to be reused efficiently during multiple training steps and eliminates redundant processing. Prefetching, conversely, involves preloading subsequent batches, ensuring that the model never idles waiting for data.</p>
</section>
</section>
<section id="data-augmentation" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="data-augmentation"><span class="header-section-number">6.4.4</span> Data Augmentation</h3>
<p>Machine learning frameworks like TensorFlow and PyTorch provide tools to simplify and streamline the process of data augmentation, enhancing the efficiency of expanding datasets synthetically. These frameworks offer integrated functionalities to apply random transformations, such as flipping, cropping, rotating, altering color, and adding noise for images. For audio data, common augmentations involve mixing clips with background noise or modulating speed, pitch, and volume.</p>
<p>By integrating augmentation tools into the data pipeline, frameworks enable these transformations to be applied on the fly during each training epoch. This approach increases the variation in the training data distribution, thereby reducing overfitting and improving model generalization. <a href="#fig-overfitting" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-overfitting</span></a> demonstrates the cases of overfitting and underfitting. The use of performant data loaders in combination with extensive augmentation capabilities allows practitioners to efficiently feed massive, varied datasets to neural networks.</p>
<div id="fig-overfitting" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-overfitting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/overfittingunderfitting.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-overfitting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.8: Overfitting versus underfitting. Source: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.aquariumlearning.com%2Fblog-posts%2Fto-make-your-model-better-first-figure-out-whats-wrong&amp;psig=AOvVaw3FodMJATpeLeeSsuQZBD51&amp;ust=1722534629114000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqGAoTCNiU49br0YcDFQAAAAAdAAAAABCoAQ">Aquarium Learning</a>
</figcaption>
</figure>
</div>
<p>These hands-off data pipelines represent a significant improvement in usability and productivity. They allow developers to focus more on model architecture and less on data wrangling when training deep learning models.</p>
</section>
<section id="loss-functions-and-optimization-algorithms" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="loss-functions-and-optimization-algorithms"><span class="header-section-number">6.4.5</span> Loss Functions and Optimization Algorithms</h3>
<p>Training a neural network is fundamentally an iterative process that seeks to minimize a loss function. The goal is to fine-tune the model weights and parameters to produce predictions close to the true target labels. Machine learning frameworks have greatly streamlined this process by offering loss functions and optimization algorithms.</p>
<p>Machine learning frameworks provide implemented loss functions that are needed for quantifying the difference between the model’s predictions and the true values. Different datasets require a different loss function to perform properly, as the loss function tells the computer the “objective” for it to aim. Commonly used loss functions include Mean Squared Error (MSE) for regression tasks, Cross-Entropy Loss for classification tasks, and Kullback-Leibler (KL) Divergence for probabilistic models. For instance, TensorFlow’s <a href="https://www.tensorflow.org/api_docs/python/tf/keras/losses">tf.keras.losses</a> holds a suite of these commonly used loss functions.</p>
<p>Optimization algorithms are used to efficiently find the set of model parameters that minimize the loss function, ensuring the model performs well on training data and generalizes to new data. Modern frameworks come equipped with efficient implementations of several optimization algorithms, many of which are variants of gradient descent with stochastic methods and adaptive learning rates. Some examples of these variants are Stochastic Gradient Descent, Adagrad, Adadelta, and Adam. The implementation of such variants are provided in <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers">tf.keras.optimizers</a>. More information with clear examples can be found in the AI Training section.</p>
</section>
<section id="model-training-support" class="level3" data-number="6.4.6">
<h3 data-number="6.4.6" class="anchored" data-anchor-id="model-training-support"><span class="header-section-number">6.4.6</span> Model Training Support</h3>
<p>A compilation step is required before training a defined neural network model. During this step, the neural network’s high-level architecture is transformed into an optimized, executable format. This process comprises several steps. The first step is to construct the computational graph, which represents all the mathematical operations and data flow within the model. We discussed this earlier.</p>
<p>During training, the focus is on executing the computational graph. Every parameter within the graph, such as weights and biases, is assigned an initial value. Depending on the chosen initialization method, this value might be random or based on a predefined logic.</p>
<p>The next critical step is memory allocation. Essential memory is reserved for the model’s operations on both CPUs and GPUs, ensuring efficient data processing. The model’s operations are then mapped to the available hardware resources, particularly GPUs or TPUs, to expedite computation. Once the compilation is finalized, the model is prepared for training.</p>
<p>The training process employs various tools to improve efficiency. Batch processing is commonly used to maximize computational throughput. Techniques like vectorization enable operations on entire data arrays rather than proceeding element-wise, which bolsters speed. Optimizations such as kernel fusion (refer to the Optimizations chapter) amalgamate multiple operations into a single action, minimizing computational overhead. Operations can also be segmented into phases, facilitating the concurrent processing of different mini-batches at various stages.</p>
<p>Frameworks consistently checkpoint the state, preserving intermediate model versions during training. This ensures that progress is recovered if an interruption occurs, and training can be recommenced from the last checkpoint. Additionally, the system vigilantly monitors the model’s performance against a validation data set. Should the model begin to overfit (if its performance on the validation set declines), training is automatically halted, conserving computational resources and time.</p>
<p>ML frameworks incorporate a blend of model compilation, enhanced batch processing methods, and utilities such as checkpointing and early stopping. These resources manage the complex aspects of performance, enabling practitioners to zero in on model development and training. As a result, developers experience both speed and ease when utilizing neural networks’ capabilities.</p>
</section>
<section id="validation-and-analysis" class="level3" data-number="6.4.7">
<h3 data-number="6.4.7" class="anchored" data-anchor-id="validation-and-analysis"><span class="header-section-number">6.4.7</span> Validation and Analysis</h3>
<p>After training deep learning models, frameworks provide utilities to evaluate performance and gain insights into the models’ workings. These tools enable disciplined experimentation and debugging.</p>
<section id="evaluation-metrics" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h4>
<p>Frameworks include implementations of common evaluation metrics for validation:</p>
<ul>
<li><p>Accuracy - Fraction of correct predictions overall. They are widely used for classification.</p></li>
<li><p>Precision - Of positive predictions, how many were positive. Useful for imbalanced datasets.</p></li>
<li><p>Recall - Of actual positives, how many did we predict correctly? Measures completeness.</p></li>
<li><p>F1-score - Harmonic mean of precision and recall. Combines both metrics.</p></li>
<li><p>AUC-ROC - Area under ROC curve. They are used for classification threshold analysis.</p></li>
<li><p>MAP - Mean Average Precision. Evaluate ranked predictions in retrieval/detection.</p></li>
<li><p>Confusion Matrix - Matrix that shows the true positives, true negatives, false positives, and false negatives. Provides a more detailed view of classification performance.</p></li>
</ul>
<p>These metrics quantify model performance on validation data for comparison.</p>
</section>
<section id="visualization" class="level4">
<h4 class="anchored" data-anchor-id="visualization">Visualization</h4>
<p>Visualization tools provide insight into models:</p>
<ul>
<li><p>Loss curves - Plot training and validation loss over time to spot Overfitting.</p></li>
<li><p>Activation grids - Illustrate features learned by convolutional filters.</p></li>
<li><p>Projection - Reduce dimensionality for intuitive visualization.</p></li>
<li><p>Precision-recall curves - Assess classification tradeoffs. <a href="#fig-precision-recall" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-precision-recall</span></a> shows an example of a precision-recall curve.</p></li>
</ul>
<div id="fig-precision-recall" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-precision-recall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/precisionrecall.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-precision-recall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.9: Reading a precision-recall curve. Source: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.techtarget.com%2Fsearchcio%2Fdefinition%2Ftransfer-learning&amp;psig=AOvVaw0Cbiewbu_6NsNVf314C9Q8&amp;ust=1722534991962000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqFwoTCPj5jITt0YcDFQAAAAAdAAAAABAE">AIM</a>
</figcaption>
</figure>
</div>
<p>Tools like <a href="https://www.tensorflow.org/tensorboard/scalars_and_keras">TensorBoard</a> for TensorFlow and <a href="https://github.com/microsoft/tensorwatch">TensorWatch</a> for PyTorch enable real-time metrics and visualization during training.</p>
</section>
</section>
<section id="differentiable-programming" class="level3" data-number="6.4.8">
<h3 data-number="6.4.8" class="anchored" data-anchor-id="differentiable-programming"><span class="header-section-number">6.4.8</span> Differentiable programming</h3>
<p>Machine learning training methods such as backpropagation rely on the change in the loss function with respect to the change in weights (which essentially is the definition of derivatives). Thus, the ability to quickly and efficiently train large machine learning models relies on the computer’s ability to take derivatives. This makes differentiable programming one of the most important elements of a machine learning framework.</p>
<p>We can use four primary methods to make computers take derivatives. First, we can manually figure out the derivatives by hand and input them into the computer. This would quickly become a nightmare with many layers of neural networks if we had to compute all the derivatives in the backpropagation steps by hand. Another method is symbolic differentiation using computer algebra systems such as Mathematica, which can introduce a layer of inefficiency, as there needs to be a level of abstraction to take derivatives. Numerical derivatives, the practice of approximating gradients using finite difference methods, suffer from many problems, including high computational costs and larger grid sizes, leading to many errors. This leads to automatic differentiation, which exploits the primitive functions that computers use to represent operations to obtain an exact derivative. With automatic differentiation, the computational complexity of computing the gradient is proportional to computing the function itself. Intricacies of automatic differentiation are not dealt with by end users now, but resources to learn more can be found widely, such as from <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf">here</a>. Today’s automatic differentiation and differentiable programming are ubiquitous and are done efficiently and automatically by modern machine learning frameworks.</p>
</section>
<section id="hardware-acceleration" class="level3 page-columns page-full" data-number="6.4.9">
<h3 data-number="6.4.9" class="anchored" data-anchor-id="hardware-acceleration"><span class="header-section-number">6.4.9</span> Hardware Acceleration</h3>
<p>The trend to continuously train and deploy larger machine-learning models has made hardware acceleration support necessary for machine-learning platforms. <a href="#fig-hardware-accelerator" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-hardware-accelerator</span></a> shows the large number of companies that are offering hardware accelerators in different domains, such as “Very Low Power” and “Embedded” machine learning. Deep layers of neural networks require many matrix multiplications, which attract hardware that can compute matrix operations quickly and in parallel. In this landscape, two hardware architectures, the <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">GPU and TPU</a>, have emerged as leading choices for training machine learning models.</p>
<p>The use of hardware accelerators began with <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a>, which paved the way for future works to use GPUs as hardware accelerators for training computer vision models. GPUs, or Graphics Processing Units, excel in handling many computations at once, making them ideal for the matrix operations central to neural network training. Their architecture, designed for rendering graphics, is perfect for the mathematical operations required in machine learning. While they are very useful for machine learning tasks and have been implemented in many hardware platforms, GPUs are still general purpose in that they can be used for other applications.</p>
<p>On the other hand, <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">Tensor Processing Units</a> (TPU) are hardware units designed specifically for neural networks. They focus on the multiply and accumulate (MAC) operation, and their hardware consists of a large hardware matrix that contains elements that efficiently compute the MAC operation. This concept, called the <a href="https://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf">systolic array architecture</a>, was pioneered by <span class="citation" data-cites="kung1979systolic">Kung and Leiserson (<a href="#ref-kung1979systolic" role="doc-biblioref">1979</a>)</span>, but has proven to be a useful structure to efficiently compute matrix products and other operations within neural networks (such as convolutions).</p>
<div class="no-row-height column-margin column-container"><div id="ref-kung1979systolic" class="csl-entry" role="listitem">
Kung, Hsiang Tsung, and Charles E Leiserson. 1979. <span>“Systolic Arrays (for <span>VLSI)</span>.”</span> In <em>Sparse Matrix Proceedings 1978</em>, 1:256–82. Society for industrial; applied mathematics Philadelphia, PA, USA.
</div></div><p>While TPUs can drastically reduce training times, they also have disadvantages. For example, many operations within the machine learning frameworks (primarily TensorFlow here since the TPU directly integrates with it) are not supported by TPUs. They cannot also support custom operations from the machine learning frameworks, and the network design must closely align with the hardware capabilities.</p>
<p>Today, NVIDIA GPUs dominate training, aided by software libraries like <a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a>, <a href="https://developer.nvidia.com/cudnn">cuDNN</a>, and <a href="https://developer.nvidia.com/tensorrt#:~:text=NVIDIA">TensorRT.</a> Frameworks also include optimizations to maximize performance on these hardware types, such as pruning unimportant connections and fusing layers. Combining these techniques with hardware acceleration provides greater efficiency. For inference, hardware is increasingly moving towards optimized ASICs and SoCs. Google’s TPUs accelerate models in data centers, while Apple, Qualcomm, the NVIDIA Jetson family, and others now produce AI-focused mobile chips.</p>
<div id="fig-hardware-accelerator" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hardware-accelerator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hardware_accelerator.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-accelerator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.10: Companies offering ML hardware accelerators. Source: <a href="https://gradientflow.com/one-simple-chart-companies-that-offer-deep-neural-network-accelerators/">Gradient Flow.</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ai_frameworks-advanced" class="level2 page-columns page-full" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="sec-ai_frameworks-advanced"><span class="header-section-number">6.5</span> Advanced Features</h2>
<p>Beyond providing the essential tools for training machine learning models, frameworks also offer advanced features. These features include distributing training across different hardware platforms, fine-tuning large pre-trained models with ease, and facilitating federated learning. Implementing these capabilities independently would be highly complex and resource-intensive, but frameworks simplify these processes, making advanced machine learning techniques more accessible.</p>
<section id="distributed-training" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="distributed-training"><span class="header-section-number">6.5.1</span> Distributed training</h3>
<p>As machine learning models have become larger over the years, it has become essential for large models to use multiple computing nodes in the training process. This process, distributed learning, has allowed for higher training capabilities but has also imposed challenges in implementation.</p>
<p>We can consider three different ways to spread the work of training machine learning models to multiple computing nodes. Input data partitioning (or data parallelism) refers to multiple processors running the same model on different input partitions. This is the easiest implementation and is available for many machine learning frameworks. The more challenging distribution of work comes with model parallelism, which refers to multiple computing nodes working on different parts of the model, and pipelined model parallelism, which refers to multiple computing nodes working on different layers of the model on the same input. The latter two mentioned here are active research areas.</p>
<p>ML frameworks that support distributed learning include TensorFlow (through its <a href="https://www.tensorflow.org/api_docs/python/tf/distribute">tf.distribute</a> module), PyTorch (through its <a href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html">torch.nn.DataParallel</a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">torch.nn.DistributedDataParallel</a> modules), and MXNet (through its <a href="https://mxnet.apache.org/versions/1.9.1/api/python/docs/api/gluon/index.html">gluon</a> API).</p>
</section>
<section id="model-conversion" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="model-conversion"><span class="header-section-number">6.5.2</span> Model Conversion</h3>
<p>Machine learning models have various methods to be represented and used within different frameworks and for different device types. For example, a model can be converted to be compatible with inference frameworks within the mobile device. The default format for TensorFlow models is checkpoint files containing weights and architectures, which are needed to retrain the models. However, models are typically converted to TensorFlow Lite format for mobile deployment. TensorFlow Lite uses a compact flat buffer representation and optimizations for fast inference on mobile hardware, discarding all the unnecessary baggage associated with training metadata, such as checkpoint file structures.</p>
<p>Model optimizations like quantization (see <a href="../../../contents/core/optimizations/optimizations.html">Optimizations</a> chapter) can further optimize models for target architectures like mobile. This reduces the precision of weights and activations to <code>uint8</code> or <code>int8</code> for a smaller footprint and faster execution with supported hardware accelerators. For post-training quantization, TensorFlow’s converter handles analysis and conversion automatically.</p>
<p>Frameworks like TensorFlow simplify deploying trained models to mobile and embedded IoT devices through easy conversion APIs for TFLite format and quantization. Ready-to-use conversion enables high-performance inference on mobile without a manual optimization burden. Besides TFLite, other common targets include TensorFlow.js for web deployment, TensorFlow Serving for cloud services, and TensorFlow Hub for transfer learning. TensorFlow’s conversion utilities handle these scenarios to streamline end-to-end workflows.</p>
<p>More information about model conversion in TensorFlow is linked <a href="https://www.tensorflow.org/lite/models/convert">here</a>.</p>
</section>
<section id="automl-no-codelow-code-ml" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="automl-no-codelow-code-ml"><span class="header-section-number">6.5.3</span> AutoML, No-Code/Low-Code ML</h3>
<p>In many cases, machine learning can have a relatively high barrier of entry compared to other fields. To successfully train and deploy models, one needs to have a critical understanding of a variety of disciplines, from data science (data processing, data cleaning), model structures (hyperparameter tuning, neural network architecture), hardware (acceleration, parallel processing), and more depending on the problem at hand. The complexity of these problems has led to the introduction of frameworks such as AutoML, which tries to make “Machine learning available for non-Machine Learning experts” and to “automate research in machine learning.” They have constructed AutoWEKA, which aids in the complex process of hyperparameter selection, and Auto-sklearn and Auto-pytorch, an extension of AutoWEKA into the popular sklearn and PyTorch Libraries.</p>
<p>While these efforts to automate parts of machine learning tasks are underway, others have focused on making machine learning models easier by deploying no-code/low-code machine learning, utilizing a drag-and-drop interface with an easy-to-navigate user interface. Companies such as Apple, Google, and Amazon have already created these easy-to-use platforms to allow users to construct machine learning models that can integrate into their ecosystem.</p>
<p>These steps to remove barriers to entry continue to democratize machine learning, make it easier for beginners to access, and simplify workflow for experts.</p>
</section>
<section id="advanced-learning-methods" class="level3 page-columns page-full" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="advanced-learning-methods"><span class="header-section-number">6.5.4</span> Advanced Learning Methods</h3>
<section id="transfer-learning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h4>
<p>Transfer learning is the practice of using knowledge gained from a pre-trained model to train and improve the performance of a model for a different task. For example, models such as MobileNet and ResNet are trained on the ImageNet dataset. To do so, one may freeze the pre-trained model, utilizing it as a feature extractor to train a much smaller model built on top of the feature extraction. One can also fine-tune the entire model to fit the new task. Machine learning frameworks make it easy to load pre-trained models, freeze specific layers, and train custom layers on top. They simplify this process by providing intuitive APIs and easy access to large repositories of <a href="https://keras.io/api/applications/">pre-trained models</a>.</p>
<p>Transfer learning, while powerful, comes with challenges. One significant issue is the modified model’s potential inability to conduct its original tasks after transfer learning. To address these challenges, researchers have proposed various solutions. For example, <span class="citation" data-cites="li2017learning">Z. Li and Hoiem (<a href="#ref-li2017learning" role="doc-biblioref">2018</a>)</span> introduced the concept of “Learning without Forgetting” in their paper <a href="https://browse.arxiv.org/pdf/1606.09282.pdf">“Learning without Forgetting”</a>, which has since been implemented in modern machine learning platforms. <a href="#fig-tl" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tl</span></a> provides a simplified illustration of the transfer learning concept:</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017learning" class="csl-entry" role="listitem">
Li, Zhizhong, and Derek Hoiem. 2018. <span>“Learning Without Forgetting.”</span> <em>IEEE Trans. Pattern Anal. Mach. Intell.</em> 40 (12): 2935–47. <a href="https://doi.org/10.1109/tpami.2017.2773081">https://doi.org/10.1109/tpami.2017.2773081</a>.
</div></div><div id="fig-tl" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/transferlearning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.11: Transfer learning. Source: <a href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fanalyticsindiamag.com%2Fdevelopers-corner%2Fcomplete-guide-to-understanding-precision-and-recall-curves%2F&amp;psig=AOvVaw3MosZItazJt2eermLTArjj&amp;ust=1722534897757000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBEQjRxqFwoTCIi389bs0YcDFQAAAAAdAAAAABAw">Tech Target</a>
</figcaption>
</figure>
</div>
<p>As shown in <a href="#fig-tl" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tl</span></a>, transfer learning involves taking a model trained on one task (the source task) and adapting it to perform a new, related task (the target task). This process allows the model to leverage knowledge gained from the source task, potentially improving performance and reducing training time on the target task. However, as mentioned earlier, care must be taken to ensure that the model doesn’t “forget” its ability to perform the original task during this process.</p>
</section>
<section id="federated-learning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="federated-learning">Federated Learning</h4>
<p>Federated learning by <span class="citation" data-cites="mcmahan2023communicationefficient">McMahan et al. (<a href="#ref-mcmahan2023communicationefficient" role="doc-biblioref">2017</a>)</span> is a form of distributed computing that involves training models on personal devices rather than centralizing the data on a single server (<a href="#fig-federated-learning" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-federated-learning</span></a>). Initially, a base global model is trained on a central server to be distributed to all devices. Using this base model, the devices individually compute the gradients and send them back to the central hub. Intuitively, this transfers model parameters instead of the data itself. Federated learning enhances privacy by keeping sensitive data on local devices and only sharing model updates with a central server. This method is particularly useful when dealing with sensitive data or when a large-scale infrastructure is impractical.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mcmahan2023communicationefficient" class="csl-entry" role="listitem">
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. 2017. <span>“Communication-Efficient Learning of Deep Networks from Decentralized Data.”</span> In <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA</em>, edited by Aarti Singh and Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.
</div></div><div id="fig-federated-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-federated-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/federated_learning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-federated-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.12: A centralized-server approach to federated learning. Source: <a href="https://blogs.nvidia.com/blog/what-is-federated-learning/">NVIDIA.</a>
</figcaption>
</figure>
</div>
<p>However, federated learning faces challenges such as ensuring data accuracy, managing non-IID (independent and identically distributed) data, dealing with unbalanced data production, and overcoming communication overhead and device heterogeneity. Privacy and security concerns, such as gradient inversion attacks, also pose significant challenges.</p>
<p>Machine learning frameworks simplify the implementation of federated learning by providing necessary tools and libraries. For example, <a href="https://www.tensorflow.org/federated">TensorFlow Federated (TFF)</a> offers an open-source framework to support federated learning. TFF allows developers to simulate and implement federated learning algorithms, offering a federated core for low-level operations and high-level APIs for common federated tasks. It seamlessly integrates with TensorFlow, enabling the use of TensorFlow models and optimizers in a federated setting. TFF supports secure aggregation techniques to improve privacy and allows for customization of federated learning algorithms. By leveraging these tools, developers can efficiently distribute training, fine-tune pre-trained models, and handle federated learning’s inherent complexities.</p>
<p>Other open source programs such as <a href="https://flower.dev/">Flower</a> have also been developed to simplify implementing federated learning with various machine learning frameworks.</p>
</section>
</section>
</section>
<section id="framework-specialization" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="framework-specialization"><span class="header-section-number">6.6</span> Framework Specialization</h2>
<p>Thus far, we have talked about ML frameworks generally. However, typically, frameworks are optimized based on the target environment’s computational capabilities and application requirements, ranging from the cloud to the edge to tiny devices. Choosing the right framework is crucial based on the target environment for deployment. This section provides an overview of the major types of AI frameworks tailored for cloud, edge, and TinyML environments to help understand the similarities and differences between these ecosystems.</p>
<section id="cloud" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="cloud"><span class="header-section-number">6.6.1</span> Cloud</h3>
<p>Cloud-based AI frameworks assume access to ample computational power, memory, and storage resources in the cloud. They generally support both training and inference. Cloud-based AI frameworks are suited for applications where data can be sent to the cloud for processing, such as cloud-based AI services, large-scale data analytics, and web applications. Popular cloud AI frameworks include the ones we mentioned earlier, such as TensorFlow, PyTorch, MXNet, Keras, etc. These frameworks utilize GPUs, TPUs, distributed training, and AutoML to deliver scalable AI. Concepts like model serving, MLOps, and AIOps relate to the operationalization of AI in the cloud. Cloud AI powers services like Google Cloud AI and enables transfer learning using pre-trained models.</p>
</section>
<section id="edge" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="edge"><span class="header-section-number">6.6.2</span> Edge</h3>
<p>Edge AI frameworks are tailored to deploy AI models on IoT devices, smartphones, and edge servers. Edge AI frameworks are optimized for devices with moderate computational resources, balancing power and performance. Edge AI frameworks are ideal for applications requiring real-time or near-real-time processing, including robotics, autonomous vehicles, and smart devices. Key edge AI frameworks include TensorFlow Lite, PyTorch Mobile, CoreML, and others. They employ optimizations like model compression, quantization, and efficient neural network architectures. Hardware support includes CPUs, GPUs, NPUs, and accelerators like the Edge TPU. Edge AI enables use cases like mobile vision, speech recognition, and real-time anomaly detection.</p>
</section>
<section id="embedded" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="embedded"><span class="header-section-number">6.6.3</span> Embedded</h3>
<p>TinyML frameworks are specialized for deploying AI models on extremely resource-constrained devices, specifically microcontrollers and sensors within the IoT ecosystem. TinyML frameworks are designed for devices with limited resources, emphasizing minimal memory and power consumption. TinyML frameworks are specialized for use cases on resource-constrained IoT devices for predictive maintenance, gesture recognition, and environmental monitoring applications. Major TinyML frameworks include TensorFlow Lite Micro, uTensor, and ARM NN. They optimize complex models to fit within kilobytes of memory through techniques like quantization-aware training and reduced precision. TinyML allows intelligent sensing across battery-powered devices, enabling collaborative learning via federated learning. The choice of framework involves balancing model performance and computational constraints of the target platform, whether cloud, edge, or TinyML. <a href="#tbl-ml_frameworks" class="quarto-xref">Table&nbsp;<span class="quarto-unresolved-ref">tbl-ml_frameworks</span></a> compares the major AI frameworks across cloud, edge, and TinyML environments:</p>
<div id="tbl-ml_frameworks" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ml_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.3: Comparison of framework types for Cloud AI, Edge AI, and TinyML.
</figcaption>
<div aria-describedby="tbl-ml_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 20%">
<col style="width: 37%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Framework Type</th>
<th style="text-align: left;">Examples</th>
<th style="text-align: left;">Key Technologies</th>
<th style="text-align: left;">Use Cases</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Cloud AI</td>
<td style="text-align: left;">TensorFlow, PyTorch, MXNet, Keras</td>
<td style="text-align: left;">GPUs, TPUs, distributed training, AutoML, MLOps</td>
<td style="text-align: left;">Cloud services, web apps, big data analytics</td>
</tr>
<tr class="even">
<td style="text-align: left;">Edge AI</td>
<td style="text-align: left;">TensorFlow Lite, PyTorch Mobile, Core ML</td>
<td style="text-align: left;">Model optimization, compression, quantization, efficient NN architectures</td>
<td style="text-align: left;">Mobile apps, autonomous systems, real-time processing</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TinyML</td>
<td style="text-align: left;">TensorFlow Lite Micro, uTensor, ARM NN</td>
<td style="text-align: left;">Quantization-aware training, reduced precision, neural architecture search</td>
<td style="text-align: left;">IoT sensors, wearables, predictive maintenance, gesture recognition</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>Key differences:</strong></p>
<ul>
<li><p>Cloud AI leverages massive computational power for complex models using GPUs/TPUs and distributed training</p></li>
<li><p>Edge AI optimizes models to run locally on resource-constrained edge devices.</p></li>
<li><p>TinyML fits models into extremely low memory and computes environments like microcontrollers</p></li>
</ul>
</section>
</section>
<section id="sec-ai_frameworks_embedded" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="sec-ai_frameworks_embedded"><span class="header-section-number">6.7</span> Embedded AI Frameworks</h2>
<section id="resource-constraints" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="resource-constraints"><span class="header-section-number">6.7.1</span> Resource Constraints</h3>
<p>Embedded systems face severe resource constraints that pose unique challenges when deploying machine learning models compared to traditional computing platforms. For example, microcontroller units (MCUs) commonly used in IoT devices often have:</p>
<ul>
<li><p><strong>RAM</strong> ranges from tens of kilobytes to a few megabytes. The popular <a href="https://www.espressif.com/en/products/socs/esp8266">ESP8266 MCU</a> has around 80KB RAM available to developers. This contrasts with 8GB or more on typical laptops and desktops today.</p></li>
<li><p><strong>Flash storage</strong> ranges from hundreds of kilobytes to a few megabytes. The Arduino Uno microcontroller provides just 32KB of code storage. Standard computers today have disk storage in the order of terabytes.</p></li>
<li><p><strong>Processing power</strong> from just a few MHz to approximately 200MHz. The ESP8266 operates at 80MHz. This is several orders of magnitude slower than multi-GHz multi-core CPUs in servers and high-end laptops.</p></li>
</ul>
<p>These tight constraints often make training machine learning models directly on microcontrollers infeasible. The limited RAM precludes handling large datasets for training. Energy usage for training would also quickly deplete battery-powered devices. Instead, models are trained on resource-rich systems and deployed on microcontrollers for optimized inference. But even inference poses challenges:</p>
<ol type="1">
<li><p><strong>Model Size:</strong> AI models are too large to fit on embedded and IoT devices. This necessitates model compression techniques, such as quantization, pruning, and knowledge distillation. Additionally, as we will see, many of the frameworks used by developers for AI development have large amounts of overhead and built-in libraries that embedded systems can’t support.</p></li>
<li><p><strong>Complexity of Tasks:</strong> With only tens of KBs to a few MBs of RAM, IoT devices and embedded systems are constrained in the complexity of tasks they can handle. Tasks that require large datasets or sophisticated algorithms—for example, LLMs—that would run smoothly on traditional computing platforms might be infeasible on embedded systems without compression or other optimization techniques due to memory limitations.</p></li>
<li><p><strong>Data Storage and Processing:</strong> Embedded systems often process data in real time and might only store small amounts locally. Conversely, traditional computing systems can hold and process large datasets in memory, enabling faster data operations analysis and real-time updates.</p></li>
<li><p><strong>Security and Privacy:</strong> Limited memory also restricts the complexity of security algorithms and protocols, data encryption, reverse engineering protections, and more that can be implemented on the device. This could make some IoT devices more vulnerable to attacks.</p></li>
</ol>
<p>Consequently, specialized software optimizations and ML frameworks tailored for microcontrollers must work within these tight resource bounds. Clever optimization techniques like quantization, pruning, and knowledge distillation compress models to fit within limited memory (see Optimizations section). Learnings from neural architecture search help guide model designs.</p>
<p>Hardware improvements like dedicated ML accelerators on microcontrollers also help alleviate constraints. For instance, <a href="https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor">Qualcomm’s Hexagon DSP</a> accelerates TensorFlow Lite models on Snapdragon mobile chips. <a href="https://cloud.google.com/edge-tpu">Google’s Edge TPU</a> packs ML performance into a tiny ASIC for edge devices. <a href="https://www.arm.com/products/silicon-ip-cpu/ethos/ethos-u55">ARM Ethos-U55</a> offers efficient inference on Cortex-M class microcontrollers. These customized ML chips unlock advanced capabilities for resource-constrained applications.</p>
<p>Due to limited processing power, it’s almost always infeasible to train AI models on IoT or embedded systems. Instead, models are trained on powerful traditional computers (often with GPUs) and then deployed on the embedded device for inference. TinyML specifically deals with this, ensuring models are lightweight enough for real-time inference on these constrained devices.</p>
</section>
<section id="frameworks-libraries" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="frameworks-libraries"><span class="header-section-number">6.7.2</span> Frameworks &amp; Libraries</h3>
<p>Embedded AI frameworks are software tools and libraries designed to enable AI and ML capabilities on embedded systems. These frameworks are essential for bringing AI to IoT devices, robotics, and other edge computing platforms, and they are designed to work where computational resources, memory, and power consumption are limited.</p>
</section>
<section id="challenges" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="challenges"><span class="header-section-number">6.7.3</span> Challenges</h3>
<p>While embedded systems present an enormous opportunity for deploying machine learning to enable intelligent capabilities at the edge, these resource-constrained environments pose significant challenges. Unlike typical cloud or desktop environments rich with computational resources, embedded devices introduce severe constraints around memory, processing power, energy efficiency, and specialized hardware. As a result, existing machine learning techniques and frameworks designed for server clusters with abundant resources do not directly translate to embedded systems. This section uncovers some of the challenges and opportunities for embedded systems and ML frameworks.</p>
<section id="fragmented-ecosystem" class="level4">
<h4 class="anchored" data-anchor-id="fragmented-ecosystem">Fragmented Ecosystem</h4>
<p>The lack of a unified ML framework led to a highly fragmented ecosystem. Engineers at companies like <a href="https://www.st.com/">STMicroelectronics</a>, <a href="https://www.nxp.com/">NXP Semiconductors</a>, and <a href="https://www.renesas.com/">Renesas</a> had to develop custom solutions tailored to their specific microcontroller and DSP architectures. These ad-hoc frameworks required extensive manual optimization for each low-level hardware platform. This made porting models extremely difficult, requiring redevelopment for new Arm, RISC-V, or proprietary architectures.</p>
</section>
<section id="disparate-hardware-needs" class="level4">
<h4 class="anchored" data-anchor-id="disparate-hardware-needs">Disparate Hardware Needs</h4>
<p>Without a shared framework, there was no standard way to assess hardware’s capabilities. Vendors like Intel, Qualcomm, and NVIDIA created integrated solutions, blending models and improving software and hardware. This made it hard to discern the sources of performance gains - whether new chip designs like Intel’s low-power x86 cores or software optimizations were responsible. A standard framework was needed so vendors could evaluate their hardware’s capabilities fairly and reproducibly.</p>
</section>
<section id="lack-of-portability" class="level4">
<h4 class="anchored" data-anchor-id="lack-of-portability">Lack of Portability</h4>
<p>With standardized tools, adapting models trained in common frameworks like TensorFlow or PyTorch to run efficiently on microcontrollers was easier. It required time-consuming manual translation of models to run on specialized DSPs from companies like CEVA or low-power Arm M-series cores. No turnkey tools were enabling portable deployment across different architectures.</p>
</section>
<section id="incomplete-infrastructure" class="level4">
<h4 class="anchored" data-anchor-id="incomplete-infrastructure">Incomplete Infrastructure</h4>
<p>The infrastructure to support key model development workflows needed to be improved. More support is needed for compression techniques to fit large models within constrained memory budgets. Tools for quantization to lower precision for faster inference were missing. Standardized APIs for integration into applications were incomplete. Essential functionality like on-device debugging, metrics, and performance profiling was absent. These gaps increased the cost and difficulty of embedded ML development.</p>
</section>
<section id="no-standard-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="no-standard-benchmark">No Standard Benchmark</h4>
<p>Without unified benchmarks, there was no standard way to assess and compare the capabilities of different hardware platforms from vendors like NVIDIA, Arm, and Ambiq Micro. Existing evaluations relied on proprietary benchmarks tailored to showcase the strengths of particular chips. This made it impossible to measure hardware improvements objectively in a fair, neutral manner. The <a href="../../../contents/core/benchmarking/benchmarking.html">Benchmarking AI</a> chapter discusses this topic in more detail.</p>
</section>
<section id="minimal-real-world-testing" class="level4">
<h4 class="anchored" data-anchor-id="minimal-real-world-testing">Minimal Real-World Testing</h4>
<p>Much of the benchmarks relied on synthetic data. Rigorously testing models on real-world embedded applications was difficult without standardized datasets and benchmarks, raising questions about how performance claims would translate to real-world usage. More extensive testing was needed to validate chips in actual use cases.</p>
<p>The lack of shared frameworks and infrastructure slowed TinyML adoption, hampering the integration of ML into embedded products. Recent standardized frameworks have begun addressing these issues through improved portability, performance profiling, and benchmarking support. However, ongoing innovation is still needed to enable seamless, cost-effective deployment of AI to edge devices.</p>
</section>
<section id="summary" class="level4">
<h4 class="anchored" data-anchor-id="summary">Summary</h4>
<p>The absence of standardized frameworks, benchmarks, and infrastructure for embedded ML has traditionally hampered adoption. However, recent progress has been made in developing shared frameworks like TensorFlow Lite Micro and benchmark suites like MLPerf Tiny that aim to accelerate the proliferation of TinyML solutions. However, overcoming the fragmentation and difficulty of embedded deployment remains an ongoing process.</p>
</section>
</section>
</section>
<section id="examples" class="level2 page-columns page-full" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="examples"><span class="header-section-number">6.8</span> Examples</h2>
<p>Machine learning deployment on microcontrollers and other embedded devices often requires specially optimized software libraries and frameworks to work within tight memory, compute, and power constraints. Several options exist for performing inference on such resource-limited hardware, each with its approach to optimizing model execution. This section will explore the key characteristics and design principles behind TFLite Micro, TinyEngine, and CMSIS-NN, providing insight into how each framework tackles the complex problem of high-accuracy yet efficient neural network execution on microcontrollers. It will also showcase different approaches for implementing efficient TinyML frameworks.</p>
<p><a href="#tbl-compare_frameworks" class="quarto-xref">Table&nbsp;<span class="quarto-unresolved-ref">tbl-compare_frameworks</span></a> summarizes the key differences and similarities between these three specialized machine-learning inference frameworks for embedded systems and microcontrollers.</p>
<div id="tbl-compare_frameworks" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-compare_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.4: Comparison of frameworks: TensorFlow Lite Micro, TinyEngine, and CMSIS-NN
</figcaption>
<div aria-describedby="tbl-compare_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 20%">
<col style="width: 30%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Framework</th>
<th style="text-align: left;">TensorFlow Lite Micro</th>
<th style="text-align: left;">TinyEngine</th>
<th style="text-align: left;">CMSIS-NN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Approach</td>
<td style="text-align: left;">Interpreter-based</td>
<td style="text-align: left;">Static compilation</td>
<td style="text-align: left;">Optimized neural network kernels</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hardware Focus</td>
<td style="text-align: left;">General embedded devices</td>
<td style="text-align: left;">Microcontrollers</td>
<td style="text-align: left;">ARM Cortex-M processors</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Arithmetic Support</td>
<td style="text-align: left;">Floating point</td>
<td style="text-align: left;">Floating point, fixed point</td>
<td style="text-align: left;">Floating point, fixed point</td>
</tr>
<tr class="even">
<td style="text-align: left;">Model Support</td>
<td style="text-align: left;">General neural network models</td>
<td style="text-align: left;">Models co-designed with TinyNAS</td>
<td style="text-align: left;">Common neural network layer types</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Code Footprint</td>
<td style="text-align: left;">Larger due to inclusion of interpreter and ops</td>
<td style="text-align: left;">Small, includes only ops needed for model</td>
<td style="text-align: left;">Lightweight by design</td>
</tr>
<tr class="even">
<td style="text-align: left;">Latency</td>
<td style="text-align: left;">Higher due to interpretation overhead</td>
<td style="text-align: left;">Very low due to compiled model</td>
<td style="text-align: left;">Low latency focus</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Memory Management</td>
<td style="text-align: left;">Dynamically managed by interpreter</td>
<td style="text-align: left;">Model-level optimization</td>
<td style="text-align: left;">Tools for efficient allocation</td>
</tr>
<tr class="even">
<td style="text-align: left;">Optimization Approach</td>
<td style="text-align: left;">Some code generation features</td>
<td style="text-align: left;">Specialized kernels, operator fusion</td>
<td style="text-align: left;">Architecture-specific assembly optimizations</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Key Benefits</td>
<td style="text-align: left;">Flexibility, portability, ease of updating models</td>
<td style="text-align: left;">Maximizes performance, optimized memory usage</td>
<td style="text-align: left;">Hardware acceleration, standardized API, portability</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>We will understand each of these in greater detail in the following sections.</p>
<section id="interpreter" class="level3 page-columns page-full" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="interpreter"><span class="header-section-number">6.8.1</span> Interpreter</h3>
<p><a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite Micro (TFLM)</a> is a machine learning inference framework designed for embedded devices with limited resources. It uses an interpreter to load and execute machine learning models, which provides flexibility and ease of updating models in the field <span class="citation" data-cites="david2021tensorflow">(<a href="#ref-david2021tensorflow" role="doc-biblioref">David et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-david2021tensorflow" class="csl-entry" role="listitem">
David, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. <span>“Tensorflow Lite Micro: <span>Embedded</span> Machine Learning for Tinyml Systems.”</span> <em>Proceedings of Machine Learning and Systems</em> 3: 800–811.
</div></div><p>Traditional interpreters often have significant branching overhead, which can reduce performance. However, machine learning model interpretation benefits from the efficiency of long-running kernels, where each kernel runtime is relatively large and helps mitigate interpreter overhead.</p>
<p>An alternative to an interpreter-based inference engine is to generate native code from a model during export. This can improve performance, but it sacrifices portability and flexibility, as the generated code needs recompilation for each target platform and must be replaced entirely to modify a model.</p>
<p>TFLM balances the simplicity of code compilation and the flexibility of an interpreter-based approach by incorporating certain code-generation features. For example, the library can be constructed solely from source files, offering much of the compilation simplicity associated with code generation while retaining the benefits of an interpreter-based model execution framework.</p>
<p>An interpreter-based approach offers several benefits over code generation for machine learning inference on embedded devices:</p>
<ul>
<li><p><strong>Flexibility:</strong> Models can be updated in the field without recompiling the entire application.</p></li>
<li><p><strong>Portability:</strong> The interpreter can be used to execute models on different target platforms without porting the code.</p></li>
<li><p><strong>Memory efficiency:</strong> The interpreter can share code across multiple models, reducing memory usage.</p></li>
<li><p><strong>Ease of development:</strong> Interpreters are easier to develop and maintain than code generators.</p></li>
</ul>
<p>TensorFlow Lite Micro is a powerful and flexible framework for machine learning inference on embedded devices. Its interpreter-based approach offers several benefits over code generation, including flexibility, portability, memory efficiency, and ease of development.</p>
</section>
<section id="compiler-based" class="level3 page-columns page-full" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="compiler-based"><span class="header-section-number">6.8.2</span> Compiler-based</h3>
<p><a href="https://github.com/mit-han-lab/tinyengine">TinyEngine</a> is an ML inference framework designed specifically for resource-constrained microcontrollers. It employs several optimizations to enable high-accuracy neural network execution within the tight constraints of memory, computing, and storage on microcontrollers <span class="citation" data-cites="lin2020mcunet">(<a href="#ref-lin2020mcunet" role="doc-biblioref">Lin et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2020mcunet" class="csl-entry" role="listitem">
Lin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. 2020. <span>“<span>MCUNet:</span> <span>Tiny</span> Deep Learning on <span>IoT</span> Devices.”</span> In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual</em>, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html</a>.
</div></div><p>While inference frameworks like TFLite Micro use interpreters to execute the neural network graph dynamically at runtime, this adds significant overhead regarding memory usage to store metadata, interpretation latency, and lack of optimizations. However, TFLite argues that the overhead is small. TinyEngine eliminates this overhead by employing a code generation approach. It analyzes the network graph during compilation and generates specialized code to execute just that model. This code is natively compiled into the application binary, avoiding runtime interpretation costs.</p>
<p>Conventional ML frameworks schedule memory per layer, trying to minimize usage for each layer separately. TinyEngine does model-level scheduling instead of analyzing memory usage across layers. It allocates a common buffer size based on the maximum memory needs of all layers. This buffer is then shared efficiently across layers to increase data reuse.</p>
<p>TinyEngine also specializes in the kernels for each layer through techniques like tiling, unrolling, and fusing operators. For example, it will generate unrolled compute kernels with the number of loops needed for a 3x3 or 5x5 convolution. These specialized kernels extract maximum performance from the microcontroller hardware. It uses optimized depthwise convolutions to minimize memory allocations by computing each channel’s output in place over the input channel data. This technique exploits the channel-separable nature of depthwise convolutions to reduce peak memory size.</p>
<p>Like TFLite Micro, the compiled TinyEngine binary only includes operations needed for a specific model rather than all possible operations. This results in a very small binary footprint, keeping code size low for memory-constrained devices.</p>
<p>One difference between TFLite Micro and TinyEngine is that the latter is co-designed with “TinyNAS,” an architecture search method for microcontroller models similar to differential NAS for microcontrollers. TinyEngine’s efficiency allows for exploring larger and more accurate models through NAS. It also provides feedback to TinyNAS on which models can fit within the hardware constraints.</p>
<p>Through various custom techniques, such as static compilation, model-based scheduling, specialized kernels, and co-design with NAS, TinyEngine enables high-accuracy deep learning inference within microcontrollers’ tight resource constraints.</p>
</section>
<section id="library" class="level3 page-columns page-full" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="library"><span class="header-section-number">6.8.3</span> Library</h3>
<p><a href="https://www.keil.com/pack/doc/CMSIS/NN/html/index.html">CMSIS-NN</a>, standing for Cortex Microcontroller Software Interface Standard for Neural Networks, is a software library devised by ARM. It offers a standardized interface for deploying neural network inference on microcontrollers and embedded systems, focusing on optimization for ARM Cortex-M processors <span class="citation" data-cites="lai2018cmsis">(<a href="#ref-lai2018cmsis" role="doc-biblioref">Lai, Suda, and Chandra 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lai2018cmsis" class="csl-entry" role="listitem">
Lai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018. <span>“Cmsis-Nn: <span>Efficient</span> Neural Network Kernels for Arm Cortex-m Cpus.”</span> <em>ArXiv Preprint</em> abs/1801.06601. <a href="https://arxiv.org/abs/1801.06601">https://arxiv.org/abs/1801.06601</a>.
</div></div><p><strong>Neural Network Kernels:</strong> CMSIS-NN has highly efficient kernels that handle fundamental neural network operations such as convolution, pooling, fully connected layers, and activation functions. It caters to a broad range of neural network models by supporting floating and fixed-point arithmetic. The latter is especially beneficial for resource-constrained devices as it curtails memory and computational requirements (Quantization).</p>
<p><strong>Hardware Acceleration:</strong> CMSIS-NN harnesses the power of Single Instruction, Multiple Data (SIMD) instructions available on many Cortex-M processors. This allows for parallel processing of multiple data elements within a single instruction, thereby boosting computational efficiency. Certain Cortex-M processors feature Digital Signal Processing (DSP) extensions that CMSIS-NN can exploit for accelerated neural network execution. The library also incorporates assembly-level optimizations tailored to specific microcontroller architectures to improve performance further.</p>
<p><strong>Standardized API:</strong> CMSIS-NN offers a consistent and abstracted API that protects developers from the complexities of low-level hardware details. This makes the integration of neural network models into applications simpler. It may also encompass tools or utilities for converting popular neural network model formats into a format that is compatible with CMSIS-NN.</p>
<p><strong>Memory Management:</strong> CMSIS-NN provides functions for efficient memory allocation and management, which is vital in embedded systems where memory resources are scarce. It ensures optimal memory usage during inference and, in some instances, allows in-place operations to decrease memory overhead.</p>
<p><strong>Portability:</strong> CMSIS-NN is designed for portability across various Cortex-M processors. This enables developers to write code that can operate on different microcontrollers without significant modifications.</p>
<p><strong>Low Latency:</strong> CMSIS-NN minimizes inference latency, making it an ideal choice for real-time applications where swift decision-making is paramount.</p>
<p><strong>Energy Efficiency:</strong> The library is designed with a focus on energy efficiency, making it suitable for battery-powered and energy-constrained devices.</p>
</section>
</section>
<section id="choosing-the-right-framework" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="choosing-the-right-framework"><span class="header-section-number">6.9</span> Choosing the Right Framework</h2>
<p>Choosing the right machine learning framework for a given application requires carefully evaluating models, hardware, and software considerations. <a href="#fig-tf-comparison" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tf-comparison</span></a> provides a comparison of different TensorFlow frameworks, which we’ll discuss in more detail:</p>
<div id="fig-tf-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="TensorFlow Framework Comparison - General">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image4.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - General" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.13: TensorFlow Framework Comparison - General. Source: TensorFlow.
</figcaption>
</figure>
</div>
<p>Analyzing these three aspects—models, hardware, and software—as depicted in <a href="#fig-tf-comparison" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tf-comparison</span></a>, ML engineers can select the optimal framework and customize it as needed for efficient and performant on-device ML applications. The goal is to balance model complexity, hardware limitations, and software integration to design a tailored ML pipeline for embedded and edge devices. As we examine the differences shown in <a href="#fig-tf-comparison" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tf-comparison</span></a>, we’ll gain insights into how to pick the right framework and understand what causes the variations between frameworks.</p>
<section id="model" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="model"><span class="header-section-number">6.9.1</span> Model</h3>
<p><a href="#fig-tf-comparison" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tf-comparison</span></a> illustrates the key differences between TensorFlow variants, particularly in terms of supported operations (ops) and features. TensorFlow supports significantly more operations than TensorFlow Lite and TensorFlow Lite Micro, as it is typically used for research or cloud deployment, which require a large number of and more flexibility with operators.</p>
<p>The figure clearly demonstrates this difference in op support across the frameworks. TensorFlow Lite supports select ops for on-device training, whereas TensorFlow Micro does not. Additionally, the figure shows that TensorFlow Lite supports dynamic shapes and quantization-aware training, features that are absent in TensorFlow Micro. In contrast, both TensorFlow Lite and TensorFlow Micro offer native quantization tooling and support. Here, quantization refers to transforming an ML program into an approximated representation with available lower precision operations, a crucial feature for embedded and edge devices with limited computational resources.</p>
</section>
<section id="software" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="software"><span class="header-section-number">6.9.2</span> Software</h3>
<p>As shown in <a href="#fig-tf-sw-comparison" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tf-sw-comparison</span></a>, TensorFlow Lite Micro does not have OS support, while TensorFlow and TensorFlow Lite do. This design choice for TensorFlow Lite Micro helps reduce memory overhead, make startup times faster, and consume less energy. Instead, TensorFlow Lite Micro can be used in conjunction with real-time operating systems (RTOS) like FreeRTOS, Zephyr, and Mbed OS.</p>
<p>The figure also highlights an important memory management feature: TensorFlow Lite and TensorFlow Lite Micro support model memory mapping, allowing models to be directly accessed from flash storage rather than loaded into RAM. In contrast, TensorFlow does not offer this capability.</p>
<div id="fig-tf-sw-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="TensorFlow Framework Comparison - Model">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-sw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image5.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - Model" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-sw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.14: TensorFlow Framework Comparison - Software. Source: TensorFlow.
</figcaption>
</figure>
</div>
<p>Another key difference is accelerator delegation. TensorFlow and TensorFlow Lite support this feature, allowing them to schedule code to different accelerators. However, TensorFlow Lite Micro does not offer accelerator delegation, as embedded systems tend to have a limited array of specialized accelerators.</p>
<p>These differences demonstrate how each TensorFlow variant is optimized for its target deployment environment, from powerful cloud servers to resource-constrained embedded devices.</p>
</section>
<section id="hardware" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="hardware"><span class="header-section-number">6.9.3</span> Hardware</h3>
<p>TensorFlow Lite and TensorFlow Lite Micro have significantly smaller base binary sizes and memory footprints than TensorFlow (see <a href="#fig-tf-hw-comparison" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-tf-hw-comparison</span></a>). For example, a typical TensorFlow Lite Micro binary is less than 200KB, whereas TensorFlow is much larger. This is due to the resource-constrained environments of embedded systems. TensorFlow supports x86, TPUs, and GPUs like NVIDIA, AMD, and Intel.</p>
<div id="fig-tf-hw-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="TensorFlow Framework Comparison - Hardware">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-hw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image3.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - Hardware" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-hw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.15: TensorFlow Framework Comparison - Hardware. Source: TensorFlow.
</figcaption>
</figure>
</div>
<p>TensorFlow Lite supports Arm Cortex-A and x86 processors commonly used on mobile phones and tablets. The latter is stripped of all the unnecessary training logic for on-device deployment. TensorFlow Lite Micro provides support for microcontroller-focused Arm Cortex M cores like M0, M3, M4, and M7, as well as DSPs like Hexagon and SHARC and MCUs like STM32, NXP Kinetis, Microchip AVR.</p>
</section>
<section id="other-factors" class="level3" data-number="6.9.4">
<h3 data-number="6.9.4" class="anchored" data-anchor-id="other-factors"><span class="header-section-number">6.9.4</span> Other Factors</h3>
<p>Selecting the appropriate AI framework is essential to ensure that embedded systems can efficiently execute AI models. Several key factors beyond models, hardware, and software should be considered when evaluating AI frameworks for embedded systems.</p>
<p>Other key factors to consider when choosing a machine learning framework are performance, scalability, ease of use, integration with data engineering tools, integration with model optimization tools, and community support. Developers can make informed decisions and maximize the potential of your machine-learning initiatives by understanding these various factors.</p>
<section id="performance" class="level4">
<h4 class="anchored" data-anchor-id="performance">Performance</h4>
<p>Performance is critical in embedded systems where computational resources are limited. Evaluate the framework’s ability to optimize model inference for embedded hardware. Model quantization and hardware acceleration support are crucial in achieving efficient inference.</p>
</section>
<section id="scalability" class="level4">
<h4 class="anchored" data-anchor-id="scalability">Scalability</h4>
<p>Scalability is essential when considering the potential growth of an embedded AI project. The framework should support the deployment of models on various embedded devices, from microcontrollers to more powerful processors. It should also seamlessly handle both small-scale and large-scale deployments.</p>
</section>
<section id="integration-with-data-engineering-tools" class="level4">
<h4 class="anchored" data-anchor-id="integration-with-data-engineering-tools">Integration with Data Engineering Tools</h4>
<p>Data engineering tools are essential for data preprocessing and pipeline management. An ideal AI framework for embedded systems should seamlessly integrate with these tools, allowing for efficient data ingestion, transformation, and model training.</p>
</section>
<section id="integration-with-model-optimization-tools" class="level4">
<h4 class="anchored" data-anchor-id="integration-with-model-optimization-tools">Integration with Model Optimization Tools</h4>
<p>Model optimization ensures that AI models are well-suited for embedded deployment. Evaluate whether the framework integrates with model optimization tools like TensorFlow Lite Converter or ONNX Runtime to facilitate model quantization and size reduction.</p>
</section>
<section id="ease-of-use" class="level4">
<h4 class="anchored" data-anchor-id="ease-of-use">Ease of Use</h4>
<p>The ease of use of an AI framework significantly impacts development efficiency. A framework with a user-friendly interface and clear documentation reduces developers’ learning curve. Consideration should be given to whether the framework supports high-level APIs, allowing developers to focus on model design rather than low-level implementation details. This factor is incredibly important for embedded systems, which have fewer features than typical developers might be accustomed to.</p>
</section>
<section id="community-support" class="level4">
<h4 class="anchored" data-anchor-id="community-support">Community Support</h4>
<p>Community support plays another essential factor. Frameworks with active and engaged communities often have well-maintained codebases, receive regular updates, and provide valuable forums for problem-solving. As a result, community support also plays into Ease of Use because it ensures that developers have access to a wealth of resources, including tutorials and example projects. Community support provides some assurance that the framework will continue to be supported for future updates. There are only a few frameworks that cater to TinyML needs. TensorFlow Lite Micro is the most popular and has the most community support.</p>
</section>
</section>
</section>
<section id="future-trends-in-ml-frameworks" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="future-trends-in-ml-frameworks"><span class="header-section-number">6.10</span> Future Trends in ML Frameworks</h2>
<section id="decomposition" class="level3" data-number="6.10.1">
<h3 data-number="6.10.1" class="anchored" data-anchor-id="decomposition"><span class="header-section-number">6.10.1</span> Decomposition</h3>
<p>Currently, the ML system stack consists of four abstractions as shown in <a href="#fig-mlsys-stack" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-mlsys-stack</span></a>, namely (1) computational graphs, (2) tensor programs, (3) libraries and runtimes, and (4) hardware primitives.</p>
<div id="fig-mlsys-stack" class="quarto-float quarto-figure quarto-figure-center anchored" data-align="center" data-caption="Four Abstractions in Current ML System Stack">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlsys-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image8.png" class="img-fluid figure-img" data-align="center" data-caption="Four Abstractions in Current ML System Stack">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlsys-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.16: Four abstractions in current ML system stacks. Source: <a href="https://tvm.apache.org/2021/12/15/tvm-unity">TVM.</a>
</figcaption>
</figure>
</div>
<p>This has led to vertical (i.e., between abstraction levels) and horizontal (i.e., library-driven vs.&nbsp;compilation-driven approaches to tensor computation) boundaries, which hinder innovation for ML. Future work in ML frameworks can look toward breaking these boundaries. In December 2021, <a href="https://tvm.apache.org/2021/12/15/tvm-unity">Apache TVM</a> Unity was proposed, which aimed to facilitate interactions between the different abstraction levels (as well as the people behind them, such as ML scientists, ML engineers, and hardware engineers) and co-optimize decisions in all four abstraction levels.</p>
</section>
<section id="high-performance-compilers-libraries" class="level3" data-number="6.10.2">
<h3 data-number="6.10.2" class="anchored" data-anchor-id="high-performance-compilers-libraries"><span class="header-section-number">6.10.2</span> High-Performance Compilers &amp; Libraries</h3>
<p>As ML frameworks further develop, high-performance compilers and libraries will continue to emerge. Some current examples include <a href="https://www.tensorflow.org/xla/architecture">TensorFlow XLA</a> and Nvidia’s <a href="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS</a>, which accelerate linear algebra operations in computational graphs, and Nvidia’s <a href="https://developer.nvidia.com/tensorrt">TensorRT</a>, which accelerates and optimizes inference.</p>
</section>
<section id="ml-for-ml-frameworks" class="level3" data-number="6.10.3">
<h3 data-number="6.10.3" class="anchored" data-anchor-id="ml-for-ml-frameworks"><span class="header-section-number">6.10.3</span> ML for ML Frameworks</h3>
<p>We can also use ML to improve ML frameworks in the future. Some current uses of ML for ML frameworks include:</p>
<ul>
<li><p>Hyperparameter optimization using techniques such as Bayesian optimization, random search, and grid search</p></li>
<li><p>Neural Architecture Search (NAS) to automatically search for optimal network architectures</p></li>
<li><p>AutoML, which as described in <a href="#sec-ai_frameworks-advanced" class="quarto-xref"><span class="quarto-unresolved-ref">sec-ai_frameworks-advanced</span></a>, automates the ML pipeline.</p></li>
</ul>
</section>
</section>
<section id="conclusion" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">6.11</span> Conclusion</h2>
<p>In summary, selecting the optimal machine learning framework requires a thorough evaluation of various options against criteria such as usability, community support, performance, hardware compatibility, and model conversion capabilities. There is no one-size-fits-all solution, as the right framework depends on specific constraints and use cases.</p>
<p>We first introduced the necessity of machine learning frameworks like TensorFlow and PyTorch. These frameworks offer features such as tensors for handling multi-dimensional data, computational graphs for defining and optimizing model operations, and a suite of tools including loss functions, optimizers, and data loaders that streamline model development.</p>
<p>Advanced features further improve these frameworks’ usability, enabling tasks like fine-tuning large pre-trained models and facilitating federated learning. These capabilities are critical for developing sophisticated machine learning models efficiently.</p>
<p>Embedded AI or TinyML frameworks, such as TensorFlow Lite Micro, provide specialized tools for deploying models on resource-constrained platforms. TensorFlow Lite Micro, for instance, offers comprehensive optimization tooling, including quantization mapping and kernel optimizations, to ensure high performance on microcontroller-based platforms like Arm Cortex-M and RISC-V processors. Frameworks specifically built for specialized hardware like CMSIS-NN on Cortex-M processors can further maximize performance but sacrifice portability. Integrated frameworks from processor vendors tailor the stack to their architectures, unlocking the full potential of their chips but locking you into their ecosystem.</p>
<p>Ultimately, choosing the right framework involves finding the best match between its capabilities and the requirements of the target platform. This requires balancing trade-offs between performance needs, hardware constraints, model complexity, and other factors. Thoroughly assessing the intended models and use cases and evaluating options against key metrics will guide developers in selecting the ideal framework for their machine learning applications.</p>
</section>
<section id="sec-ai-frameworks-resource" class="level2" data-number="6.12">
<h2 data-number="6.12" class="anchored" data-anchor-id="sec-ai-frameworks-resource"><span class="header-section-number">6.12</span> Resources</h2>
<p>Here is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>These slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1zbnsihiO68oIUE04TVJEcDQ_Kyec4mhdQkIG6xoR0DY/edit#slide=id.g1ff94734162_0_0">Frameworks overview.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1BK2M2krnI24jSWO0r8tXegl1wgflGZTJyMkjfGolURI/edit#slide=id.g202a6885eb3_0_0">Embedded systems software.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1Jr7HzdZ7YaKO6KY9HBGbOG0BrTnKhbboQtf9d6xy3Ls/edit?usp=drive_link">Inference engines: TF vs.&nbsp;TFLite.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1_DwBbas8wAVWnJ0tbOorqotf9Gns1qNc3JJ6tw8bce0/edit?usp=drive_link">TF flavors: TF vs.&nbsp;TFLite vs.&nbsp;TFLite Micro.</a></p></li>
<li><p>TFLite Micro:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1XdwcZS0pz6kyuk6Vx90kE11hwUMAtS1cMoFQHZAxS20/edit?usp=drive_link">TFLite Micro Big Picture.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/10llaugp6EroGekFzB1pAH1OJ1dpJ4d7yxKglK1BsqlI/edit?usp=drive_link&amp;resourcekey=0-C6_PHSaI6u4x0Mv2KxWKbg">TFLite Micro Interpreter.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/123kdwjRXvbukyaOBvdp0PJpIs2JSxQ7GoDjB8y0FgIE/edit?usp=drive_link">TFLite Micro Model Format.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1_sHuWa3DDTCB9mBzKA4ElPWaUFA8oOelqHCBOHmsvC4/edit?usp=drive_link">TFLite Micro Memory Allocation.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1ZwLOLvYbKodNmyuKKGb_gD83NskrvNmnFC0rvGugJlY/edit?usp=drive_link">TFLite Micro NN Operations.</a></p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Videos
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercises
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.</p>
<ul>
<li><p><a href="#exr-tfc" class="quarto-xref">Exercise&nbsp;<span class="quarto-unresolved-ref">exr-tfc</span></a></p></li>
<li><p><a href="#exr-tfl" class="quarto-xref">Exercise&nbsp;<span class="quarto-unresolved-ref">exr-tfl</span></a></p></li>
<li><p><a href="#exr-k" class="quarto-xref">Exercise&nbsp;<span class="quarto-unresolved-ref">exr-k</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Labs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>In addition to exercises, we offer a series of hands-on labs allowing students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.</p>
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/data_engineering/data_engineering.html" class="pagination-link" aria-label="Data Engineering">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/training/training.html" class="pagination-link" aria-label="AI Training">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Training</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/frameworks/frameworks.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/frameworks/frameworks.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>