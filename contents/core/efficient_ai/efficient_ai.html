<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>9&nbsp; Efficient AI ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/optimizations/optimizations.html" rel="next">
<link href="../../../contents/core/training/training.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-de84f8d6bb715db06a919283c2d1e787.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-9a023c9ade86a60361e96e3e3f11bc54.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-66905c77fcb280f4455d0d2d82ac86a4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-706ab4a40b8818d4fd97fe002c8dbb18.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script><script type="module" src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script><script type="module" src="../../../scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script><script type="module" src="../../../scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script><script type="module" src="../../../scripts/ai_menu/dist/worker-voUF5YDa.js"></script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/efficient_ai/efficient_ai.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav><div id="quarto-announcement" data-announcement-id="f69d736fda9cfc5a10a08f2d07a04bd8" class="alert alert-primary hidden">
<i class="bi bi-book quarto-announcement-icon"></i><div class="quarto-announcement-content">
üîî <b>What‚Äôs New in the Book?</b><br> üìÖ <b>[Mar 25]:</b> Major updates! Ch. 13 (ML Operations ‚öôÔ∏è), Ch. 17‚Äì19 (Sustainable AI üå±, Robust AI üõ°Ô∏è, and AI for Good üåç).<br> üìÖ <b>[Mar 03]:</b> New! Ch. 10 (AI Acceleration üöÄ) &amp; Ch. 12 (Benchmarking AI üìä).<br> üìÖ <b>[Feb 02]:</b> Updated Ch. 8 &amp; 9 (AI Training üèãÔ∏è &amp; Efficient AI ‚ö°).<br> üìÖ <b>[Jan 16]:</b> Expanded Ch. 1-7 + brand-new Ch. 4! üî¢üõ†Ô∏è<br> üóíÔ∏è <b>More Updates:</b> <a href="contents/frontmatter/changelog/changelog.html">See the Full Changelog</a>.
<div style="height: 1px; background-color: #ccc; margin: 5px 0;">

</div>
<p>üöÄ <b>Shaping the Future:</b> Every GitHub ‚≠ê helps empower learners and expand the global AI engineering community.<br> üôè <b>Support the Mission:</b> Your ‚≠ê helps us keep this resource free, open, and improving for everyone.<br> ‚úçÔ∏è <b>Keep It Growing:</b> A ‚≠ê a day keeps Vijay writing all day! ‚Üí <a href="https://github.com/harvard-edge/cs249r_book"><b>Star the book on GitHub</b></a></p>
</div>
<i class="bi bi-x-lg quarto-announcement-action"></i>
</div>
</header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Author‚Äôs Note</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/ai/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">AI Essentials</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ML Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">DNN Architectures</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">AI Engineering Principles</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ML Operations</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">AI Best Practices</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">AI Perspectives</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">LABS</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shared Labs</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendix</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/appendix/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">REFERENCES</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#purpose" id="toc-purpose" class="nav-link active" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview"><span class="header-section-number">9.1</span> Overview</a></li>
  <li>
<a href="#ai-scaling-laws" id="toc-ai-scaling-laws" class="nav-link" data-scroll-target="#ai-scaling-laws"><span class="header-section-number">9.2</span> AI Scaling Laws</a>
  <ul>
<li><a href="#fundamental-principles" id="toc-fundamental-principles" class="nav-link" data-scroll-target="#fundamental-principles"><span class="header-section-number">9.2.1</span> Fundamental Principles</a></li>
  <li><a href="#empirical-scaling-laws" id="toc-empirical-scaling-laws" class="nav-link" data-scroll-target="#empirical-scaling-laws"><span class="header-section-number">9.2.2</span> Empirical Scaling Laws</a></li>
  <li><a href="#scaling-regimes" id="toc-scaling-regimes" class="nav-link" data-scroll-target="#scaling-regimes"><span class="header-section-number">9.2.3</span> Scaling Regimes</a></li>
  <li><a href="#system-design" id="toc-system-design" class="nav-link" data-scroll-target="#system-design"><span class="header-section-number">9.2.4</span> System Design</a></li>
  <li><a href="#scaling-vs.-efficiency" id="toc-scaling-vs.-efficiency" class="nav-link" data-scroll-target="#scaling-vs.-efficiency"><span class="header-section-number">9.2.5</span> Scaling vs.&nbsp;Efficiency</a></li>
  <li><a href="#scaling-breakdown" id="toc-scaling-breakdown" class="nav-link" data-scroll-target="#scaling-breakdown"><span class="header-section-number">9.2.6</span> Scaling Breakdown</a></li>
  <li><a href="#toward-efficient-scaling" id="toc-toward-efficient-scaling" class="nav-link" data-scroll-target="#toward-efficient-scaling"><span class="header-section-number">9.2.7</span> Toward Efficient Scaling</a></li>
  </ul>
</li>
  <li>
<a href="#the-pillars-of-ai-efficiency" id="toc-the-pillars-of-ai-efficiency" class="nav-link" data-scroll-target="#the-pillars-of-ai-efficiency"><span class="header-section-number">9.3</span> The Pillars of AI Efficiency</a>
  <ul>
<li>
<a href="#algorithmic-efficiency" id="toc-algorithmic-efficiency" class="nav-link" data-scroll-target="#algorithmic-efficiency"><span class="header-section-number">9.3.1</span> Algorithmic Efficiency</a>
  <ul class="collapse">
<li><a href="#early-efficiency" id="toc-early-efficiency" class="nav-link" data-scroll-target="#early-efficiency">Early Efficiency</a></li>
  <li><a href="#deep-learning-era" id="toc-deep-learning-era" class="nav-link" data-scroll-target="#deep-learning-era">Deep Learning Era</a></li>
  <li><a href="#modern-efficiency" id="toc-modern-efficiency" class="nav-link" data-scroll-target="#modern-efficiency">Modern Efficiency</a></li>
  <li><a href="#efficiency-in-design" id="toc-efficiency-in-design" class="nav-link" data-scroll-target="#efficiency-in-design">Efficiency in Design</a></li>
  </ul>
</li>
  <li>
<a href="#compute-efficiency" id="toc-compute-efficiency" class="nav-link" data-scroll-target="#compute-efficiency"><span class="header-section-number">9.3.2</span> Compute Efficiency</a>
  <ul class="collapse">
<li><a href="#general-purpose-computing-era" id="toc-general-purpose-computing-era" class="nav-link" data-scroll-target="#general-purpose-computing-era">General-Purpose Computing Era</a></li>
  <li><a href="#accelerated-computing-era" id="toc-accelerated-computing-era" class="nav-link" data-scroll-target="#accelerated-computing-era">Accelerated Computing Era</a></li>
  <li><a href="#sustainable-computing-era" id="toc-sustainable-computing-era" class="nav-link" data-scroll-target="#sustainable-computing-era">Sustainable Computing Era</a></li>
  <li><a href="#compute-efficiencys-role" id="toc-compute-efficiencys-role" class="nav-link" data-scroll-target="#compute-efficiencys-role">Compute Efficiency‚Äôs Role</a></li>
  </ul>
</li>
  <li>
<a href="#data-efficiency" id="toc-data-efficiency" class="nav-link" data-scroll-target="#data-efficiency"><span class="header-section-number">9.3.3</span> Data Efficiency</a>
  <ul class="collapse">
<li><a href="#data-scarcity-era" id="toc-data-scarcity-era" class="nav-link" data-scroll-target="#data-scarcity-era">Data Scarcity Era</a></li>
  <li><a href="#big-data-era" id="toc-big-data-era" class="nav-link" data-scroll-target="#big-data-era">Big Data Era</a></li>
  <li><a href="#modern-data-efficiency-era" id="toc-modern-data-efficiency-era" class="nav-link" data-scroll-target="#modern-data-efficiency-era">Modern Data Efficiency Era</a></li>
  <li><a href="#data-efficiencys-role" id="toc-data-efficiencys-role" class="nav-link" data-scroll-target="#data-efficiencys-role">Data Efficiency‚Äôs Role</a></li>
  </ul>
</li>
  </ul>
</li>
  <li>
<a href="#system-efficiency" id="toc-system-efficiency" class="nav-link" data-scroll-target="#system-efficiency"><span class="header-section-number">9.4</span> System Efficiency</a>
  <ul>
<li><a href="#defining-system-efficiency" id="toc-defining-system-efficiency" class="nav-link" data-scroll-target="#defining-system-efficiency"><span class="header-section-number">9.4.1</span> Defining System Efficiency</a></li>
  <li>
<a href="#efficiency-interdependencies" id="toc-efficiency-interdependencies" class="nav-link" data-scroll-target="#efficiency-interdependencies"><span class="header-section-number">9.4.2</span> Efficiency Interdependencies</a>
  <ul class="collapse">
<li><a href="#algorithmic-efficiency-aids-compute-and-data" id="toc-algorithmic-efficiency-aids-compute-and-data" class="nav-link" data-scroll-target="#algorithmic-efficiency-aids-compute-and-data">Algorithmic Efficiency Aids Compute and Data</a></li>
  <li><a href="#compute-efficiency-supports-model-and-data" id="toc-compute-efficiency-supports-model-and-data" class="nav-link" data-scroll-target="#compute-efficiency-supports-model-and-data">Compute Efficiency Supports Model and Data</a></li>
  <li><a href="#data-efficiency-strengthens-model-and-compute" id="toc-data-efficiency-strengthens-model-and-compute" class="nav-link" data-scroll-target="#data-efficiency-strengthens-model-and-compute">Data Efficiency Strengthens Model and Compute</a></li>
  <li><a href="#efficiency-trade-offs" id="toc-efficiency-trade-offs" class="nav-link" data-scroll-target="#efficiency-trade-offs">Efficiency Trade-offs</a></li>
  <li><a href="#progression-and-takeaways" id="toc-progression-and-takeaways" class="nav-link" data-scroll-target="#progression-and-takeaways">Progression and Takeaways</a></li>
  </ul>
</li>
  <li>
<a href="#scalability-and-sustainability" id="toc-scalability-and-sustainability" class="nav-link" data-scroll-target="#scalability-and-sustainability"><span class="header-section-number">9.4.3</span> Scalability and Sustainability</a>
  <ul class="collapse">
<li><a href="#efficiency-scalability-relationship" id="toc-efficiency-scalability-relationship" class="nav-link" data-scroll-target="#efficiency-scalability-relationship">Efficiency-Scalability Relationship</a></li>
  <li><a href="#scalability-sustainability-relationship" id="toc-scalability-sustainability-relationship" class="nav-link" data-scroll-target="#scalability-sustainability-relationship">Scalability-Sustainability Relationship</a></li>
  <li><a href="#sustainability-efficiency-relationship" id="toc-sustainability-efficiency-relationship" class="nav-link" data-scroll-target="#sustainability-efficiency-relationship">Sustainability-Efficiency Relationship</a></li>
  </ul>
</li>
  </ul>
</li>
  <li>
<a href="#efficiency-trade-offs-and-challenges" id="toc-efficiency-trade-offs-and-challenges" class="nav-link" data-scroll-target="#efficiency-trade-offs-and-challenges"><span class="header-section-number">9.5</span> Efficiency Trade-offs and Challenges</a>
  <ul>
<li>
<a href="#trade-offs-source" id="toc-trade-offs-source" class="nav-link" data-scroll-target="#trade-offs-source"><span class="header-section-number">9.5.1</span> Trade-offs Source</a>
  <ul class="collapse">
<li><a href="#efficiency-and-compute-requirements" id="toc-efficiency-and-compute-requirements" class="nav-link" data-scroll-target="#efficiency-and-compute-requirements">Efficiency and Compute Requirements</a></li>
  <li><a href="#efficiency-and-real-time-needs" id="toc-efficiency-and-real-time-needs" class="nav-link" data-scroll-target="#efficiency-and-real-time-needs">Efficiency and Real-Time Needs</a></li>
  <li><a href="#efficiency-and-model-generalization" id="toc-efficiency-and-model-generalization" class="nav-link" data-scroll-target="#efficiency-and-model-generalization">Efficiency and Model Generalization</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
</li>
  <li>
<a href="#common-trade-offs" id="toc-common-trade-offs" class="nav-link" data-scroll-target="#common-trade-offs"><span class="header-section-number">9.5.2</span> Common Trade-offs</a>
  <ul class="collapse">
<li><a href="#complexity-vs.-resources" id="toc-complexity-vs.-resources" class="nav-link" data-scroll-target="#complexity-vs.-resources">Complexity vs.&nbsp;Resources</a></li>
  <li><a href="#energy-vs.-performance" id="toc-energy-vs.-performance" class="nav-link" data-scroll-target="#energy-vs.-performance">Energy vs.&nbsp;Performance</a></li>
  <li><a href="#data-size-vs.-generalization" id="toc-data-size-vs.-generalization" class="nav-link" data-scroll-target="#data-size-vs.-generalization">Data Size vs.&nbsp;Generalization</a></li>
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1">Summary</a></li>
  </ul>
</li>
  </ul>
</li>
  <li>
<a href="#managing-trade-offs" id="toc-managing-trade-offs" class="nav-link" data-scroll-target="#managing-trade-offs"><span class="header-section-number">9.6</span> Managing Trade-offs</a>
  <ul>
<li><a href="#contextual-prioritization" id="toc-contextual-prioritization" class="nav-link" data-scroll-target="#contextual-prioritization"><span class="header-section-number">9.6.1</span> Contextual Prioritization</a></li>
  <li><a href="#test-time-compute" id="toc-test-time-compute" class="nav-link" data-scroll-target="#test-time-compute"><span class="header-section-number">9.6.2</span> Test-Time Compute</a></li>
  <li><a href="#co-design" id="toc-co-design" class="nav-link" data-scroll-target="#co-design"><span class="header-section-number">9.6.3</span> Co-Design</a></li>
  <li><a href="#automation" id="toc-automation" class="nav-link" data-scroll-target="#automation"><span class="header-section-number">9.6.4</span> Automation</a></li>
  <li><a href="#summary-2" id="toc-summary-2" class="nav-link" data-scroll-target="#summary-2"><span class="header-section-number">9.6.5</span> Summary</a></li>
  </ul>
</li>
  <li>
<a href="#efficiency-first-mindset" id="toc-efficiency-first-mindset" class="nav-link" data-scroll-target="#efficiency-first-mindset"><span class="header-section-number">9.7</span> Efficiency-First Mindset</a>
  <ul>
<li><a href="#end-to-end-perspective" id="toc-end-to-end-perspective" class="nav-link" data-scroll-target="#end-to-end-perspective"><span class="header-section-number">9.7.1</span> End-to-End Perspective</a></li>
  <li>
<a href="#scenarios" id="toc-scenarios" class="nav-link" data-scroll-target="#scenarios"><span class="header-section-number">9.7.2</span> Scenarios</a>
  <ul class="collapse">
<li><a href="#prototypes-vs.-production" id="toc-prototypes-vs.-production" class="nav-link" data-scroll-target="#prototypes-vs.-production">Prototypes vs.&nbsp;Production</a></li>
  <li><a href="#cloud-apps-vs.-constrained-systems" id="toc-cloud-apps-vs.-constrained-systems" class="nav-link" data-scroll-target="#cloud-apps-vs.-constrained-systems">Cloud Apps vs.&nbsp;Constrained Systems</a></li>
  <li><a href="#frequent-retraining-vs.-stability" id="toc-frequent-retraining-vs.-stability" class="nav-link" data-scroll-target="#frequent-retraining-vs.-stability">Frequent Retraining vs.&nbsp;Stability</a></li>
  </ul>
</li>
  <li><a href="#summary-3" id="toc-summary-3" class="nav-link" data-scroll-target="#summary-3"><span class="header-section-number">9.7.3</span> Summary</a></li>
  </ul>
</li>
  <li>
<a href="#broader-challenges" id="toc-broader-challenges" class="nav-link" data-scroll-target="#broader-challenges"><span class="header-section-number">9.8</span> Broader Challenges</a>
  <ul>
<li><a href="#optimization-limits" id="toc-optimization-limits" class="nav-link" data-scroll-target="#optimization-limits"><span class="header-section-number">9.8.1</span> Optimization Limits</a></li>
  <li>
<a href="#moores-law-case-study" id="toc-moores-law-case-study" class="nav-link" data-scroll-target="#moores-law-case-study"><span class="header-section-number">9.8.2</span> Moore‚Äôs Law Case Study</a>
  <ul class="collapse">
<li><a href="#ml-optimization-parallels" id="toc-ml-optimization-parallels" class="nav-link" data-scroll-target="#ml-optimization-parallels">ML Optimization Parallels</a></li>
  </ul>
</li>
  <li>
<a href="#equity-concerns" id="toc-equity-concerns" class="nav-link" data-scroll-target="#equity-concerns"><span class="header-section-number">9.8.3</span> Equity Concerns</a>
  <ul class="collapse">
<li><a href="#uneven-access" id="toc-uneven-access" class="nav-link" data-scroll-target="#uneven-access">Uneven Access</a></li>
  <li><a href="#low-resource-challenges" id="toc-low-resource-challenges" class="nav-link" data-scroll-target="#low-resource-challenges">Low-Resource Challenges</a></li>
  <li><a href="#efficiency-for-accessibility" id="toc-efficiency-for-accessibility" class="nav-link" data-scroll-target="#efficiency-for-accessibility">Efficiency for Accessibility</a></li>
  <li><a href="#democratization-pathways" id="toc-democratization-pathways" class="nav-link" data-scroll-target="#democratization-pathways">Democratization Pathways</a></li>
  </ul>
</li>
  <li>
<a href="#balancing-innovation-and-efficiency" id="toc-balancing-innovation-and-efficiency" class="nav-link" data-scroll-target="#balancing-innovation-and-efficiency"><span class="header-section-number">9.8.4</span> Balancing Innovation and Efficiency</a>
  <ul class="collapse">
<li><a href="#stability-vs.-experimentation" id="toc-stability-vs.-experimentation" class="nav-link" data-scroll-target="#stability-vs.-experimentation">Stability vs.&nbsp;Experimentation</a></li>
  <li><a href="#resource-intensive-innovation" id="toc-resource-intensive-innovation" class="nav-link" data-scroll-target="#resource-intensive-innovation">Resource-Intensive Innovation</a></li>
  <li><a href="#efficiency-creativity-constraint" id="toc-efficiency-creativity-constraint" class="nav-link" data-scroll-target="#efficiency-creativity-constraint">Efficiency-Creativity Constraint</a></li>
  <li><a href="#striking-a-balance" id="toc-striking-a-balance" class="nav-link" data-scroll-target="#striking-a-balance">Striking a Balance</a></li>
  </ul>
</li>
  </ul>
</li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">9.9</span> Conclusion</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/widget_quiz/contents/core/efficient_ai/efficient_ai.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/widget_quiz/contents/core/efficient_ai/efficient_ai.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title">
<span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span>
</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>Resources: <a href="#sec-efficient-ai-resource">Slides</a>, <a href="#sec-efficient-ai-resource">Videos</a>, <a href="#sec-efficient-ai-resource">Exercises</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><a href="images/png/cover_efficient_ai.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="DALL¬∑E 3 Prompt: A conceptual illustration depicting efficiency in artificial intelligence using a shipyard analogy. The scene shows a bustling shipyard where containers represent bits or bytes of data. These containers are being moved around efficiently by cranes and vehicles, symbolizing the streamlined and rapid information processing in AI systems. The shipyard is meticulously organized, illustrating the concept of optimal performance within the constraints of limited resources. In the background, ships are docked, representing different platforms and scenarios where AI is applied. The atmosphere should convey advanced technology with an underlying theme of sustainability and wide applicability."><img src="images/png/cover_efficient_ai.png" class="img-fluid figure-img" alt="DALL¬∑E 3 Prompt: A conceptual illustration depicting efficiency in artificial intelligence using a shipyard analogy. The scene shows a bustling shipyard where containers represent bits or bytes of data. These containers are being moved around efficiently by cranes and vehicles, symbolizing the streamlined and rapid information processing in AI systems. The shipyard is meticulously organized, illustrating the concept of optimal performance within the constraints of limited resources. In the background, ships are docked, representing different platforms and scenarios where AI is applied. The atmosphere should convey advanced technology with an underlying theme of sustainability and wide applicability."></a></p>
<figcaption><em>DALL¬∑E 3 Prompt: A conceptual illustration depicting efficiency in artificial intelligence using a shipyard analogy. The scene shows a bustling shipyard where containers represent bits or bytes of data. These containers are being moved around efficiently by cranes and vehicles, symbolizing the streamlined and rapid information processing in AI systems. The shipyard is meticulously organized, illustrating the concept of optimal performance within the constraints of limited resources. In the background, ships are docked, representing different platforms and scenarios where AI is applied. The atmosphere should convey advanced technology with an underlying theme of sustainability and wide applicability.</em></figcaption></figure>
</div>
<section id="purpose" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>What principles guide the efficient design of machine learning systems, and why is understanding the interdependence of key resources essential?</em></p>
<p>Machine learning systems are shaped by the complex interplay among data, models, and computing resources. Decisions on efficiency in one dimension often have ripple effects in the others, presenting both opportunities for synergy and inevitable trade-offs. Understanding these individual components and their interdependencies exposes not only how systems can be optimized but also why these optimizations are crucial for achieving scalability, sustainability, and real-world applicability. The relationship between data, model, and computing efficiency forms the basis for designing machine learning systems that maximize capabilities while working within resource limitations. Each efficiency decision represents a balance between performance and practicality, underscoring the significance of a holistic approach to system design. Exploring these relationships equips us with the strategies necessary to navigate the intricacies of developing efficient, impactful AI solutions.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Define the principles of algorithmic, compute and data efficiency in AI systems.</p></li>
<li><p>Identify and analyze trade-offs between algorithmic, compute, and data efficiency in system design.</p></li>
<li><p>Apply strategies for achieving efficiency across diverse deployment contexts, such as edge, cloud, and Tiny ML applications.</p></li>
<li><p>Examine the historical evolution and emerging trends in machine learning efficiency.</p></li>
<li><p>Evaluate the broader ethical and environmental implications of efficient AI system design.</p></li>
</ul>
</div>
</div>
</section><section id="overview" class="level2" data-number="9.1"><h2 data-number="9.1" class="anchored" data-anchor-id="overview">
<span class="header-section-number">9.1</span> Overview</h2>
<p>Machine learning systems have become ubiquitous, permeating nearly every aspect of modern life. As these systems grow in complexity and scale, they must operate effectively across a wide range of deployments and scenarios. This necessitates careful consideration of factors such as processing speed, memory usage, and power consumption to ensure that models can handle large workloads, operate on energy-constrained devices, and remain cost-effective.</p>
<p>Achieving this balance involves navigating trade-offs. For instance, in autonomous vehicles, reducing a model‚Äôs size to fit the low-power constraints of an edge device in a car might slightly decrease accuracy, but it ensures real-time processing and decision-making. Conversely, a cloud-based system can afford higher model complexity for improved accuracy, though this often comes at the cost of increased latency and energy consumption. In the medical field, deploying machine learning models on portable devices for diagnostics requires efficient models that can operate with limited computational resources and power, ensuring accessibility in remote or resource-constrained areas. Conversely, hospital-based systems can leverage more powerful hardware to run complex models for detailed analysis, albeit with higher energy demands.</p>
<p>Understanding and managing these trade-offs is crucial for designing machine learning systems that meet diverse application needs within real-world constraints. The implications of these design choices extend beyond performance and cost. Efficient systems can be deployed across diverse environments, from cloud infrastructures to edge devices, enhancing accessibility and adoption. Additionally, they help reduce the environmental impact of machine learning workloads by lowering energy consumption and carbon emissions, aligning technological progress with ethical and ecological responsibilities.</p>
<p>This chapter focuses on the ‚Äòwhy‚Äô and ‚Äòhow‚Äô of efficiency in machine learning systems. By establishing the foundational principles of efficient AI and exploring strategies to achieve it, this chapter sets the stage for deeper discussions on topics such as scaling, optimization, deployment, and sustainability in later chapters.</p>
</section><section id="ai-scaling-laws" class="level2 page-columns page-full" data-number="9.2"><h2 data-number="9.2" class="anchored" data-anchor-id="ai-scaling-laws">
<span class="header-section-number">9.2</span> AI Scaling Laws</h2>
<p>The advancement of machine learning systems has been characterized by a consistent trend: the augmentation of model scale, encompassing parameters, training data, and computational resources, typically results in enhanced performance. This observation, which was discovered empirically, has driven significant progress across domains such as natural language processing, computer vision, and speech recognition, where larger models trained on extensive datasets have consistently achieved state-of-the-art results.</p>
<p>However, the pursuit of performance gains through scaling incurs substantial resource demands, raising critical inquiries regarding the efficiency and sustainability of such practices. Specifically, questions arise concerning the incremental computational resources required for marginal improvements in accuracy, the scalability of data requirements as task complexity increases, and the point at which diminishing returns render further scaling economically or practically infeasible.</p>
<p>To address these concerns, researchers have developed scaling laws‚Äîempirical relationships that quantify the correlation between model performance and training resources. These laws provide a formal framework for analyzing the trade-offs inherent in scaling and elucidate the increasing importance of efficiency as systems expand in size and complexity.</p>
<p>This section introduces the concept of scaling laws, delineates their manifestation across model, compute, and data dimensions, and examines their implications for system design. By doing so, it establishes a foundation for understanding the limitations of brute-force scaling and underscores the necessity of efficient methodologies that balance performance with practical resource constraints.</p>
<section id="fundamental-principles" class="level3 page-columns page-full" data-number="9.2.1"><h3 data-number="9.2.1" class="anchored" data-anchor-id="fundamental-principles">
<span class="header-section-number">9.2.1</span> Fundamental Principles</h3>
<p>To comprehend the intricacies of efficiency in large-scale machine learning systems, it is imperative to establish the fundamental principles that govern their performance. These principles are encapsulated in scaling laws, which describe the empirical relationships between model performance and the allocation of resources, including model size, dataset size, and computational capacity. While initially popularized within the context of large language models, the implications of scaling laws extend across diverse domains of machine learning.</p>
<p>The genesis of scaling laws in machine learning is intrinsically linked to the advent of deep learning and the proliferation of large-scale models. In the 2010s, researchers observed a consistent trend: augmenting the size of neural networks resulted in notable performance enhancements <span class="citation" data-cites="hestness2017deep">(<a href="../references.html#ref-hestness2017deep" role="doc-biblioref">Hestness et al. 2017</a>)</span>, particularly in complex tasks such as image recognition and natural language processing. This observation, termed the ‚Äòscaling hypothesis,‚Äô posited that larger models possess an increased capacity to capture intricate data patterns, thereby facilitating improved accuracy and generalization. Consequently, the field witnessed a surge in models with millions or billions of parameters, trained on extensive datasets to attain state-of-the-art results. However, this trend also precipitated concerns regarding the sustainability and efficiency of scaling, necessitating a rigorous examination of the associated trade-offs.</p>
<div class="no-row-height column-margin column-container"></div><p>Scaling laws provide a quantitative framework for analyzing these trade-offs. They elucidate how model performance, training time, and resource consumption vary with scale, enabling researchers to identify optimal strategies for developing high-performing, resource-efficient systems. These laws have become indispensable tools for guiding the design of contemporary machine learning architectures, ensuring that advancements in scale are harmonized with broader objectives of efficiency and sustainability.</p>
<p>Scaling laws reveal that model performance exhibits predictable patterns as resources are augmented. For example, power-law scaling,<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> a common phenomenon in deep learning, posits that performance improves as a power function of model size, dataset size, or computational resources. This relationship can be mathematically expressed as:</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Power-law relationship</strong>: A mathematical relationship where one quantity varies as a power of another. In ML scaling laws, performance improvements are proportional to resource increases raised to some power, showing diminishing returns as resources scale up.</p></div></div><p><span class="math display">\[
\text{Performance} \propto \text{Resource}^{-\alpha}
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> denotes a scaling exponent that varies based on the task and model architecture. This expression indicates that increasing model size, dataset size, or computational resources leads to predictable enhancements in performance, adhering to a power-law relationship.</p>
<p>Empirical studies of large language models (LLMs) further elucidate the interplay between these factors‚Äîparameters, data, and compute‚Äîunder fixed resource constraints. As illustrated in <a href="#fig-compute-optimal" class="quarto-xref">Figure&nbsp;<span>9.1</span></a>, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens) that minimizes training loss. The left panel depicts ‚ÄòIsoFLOP curves,‚Äô where each curve corresponds to a constant number of floating-point operations (FLOPs) during transformer training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models.</p>
<div id="fig-compute-optimal" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-compute-optimal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/compute_optimal.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;9.1: The left panel shows training loss as a function of model size for fixed compute budgets, revealing that there exists an optimal parameter count for each compute level. The central and right panels depict how the optimal number of model parameters and training tokens scale with available FLOPs. These empirical curves illustrate the need to balance model size and data volume when scaling under resource constraints. Source: [@hoffmann2022training]."><img src="images/png/compute_optimal.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compute-optimal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: The left panel shows training loss as a function of model size for fixed compute budgets, revealing that there exists an optimal parameter count for each compute level. The central and right panels depict how the optimal number of model parameters and training tokens scale with available FLOPs. These empirical curves illustrate the need to balance model size and data volume when scaling under resource constraints. Source: <span class="citation" data-cites="hoffmann2022training">(<a href="../references.html#ref-hoffmann2022training" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>For instance, in computer vision tasks, doubling the size of convolutional neural networks typically yields consistent accuracy gains, provided that proportional increases in training data are supplied. Similarly, language models exhibit analogous patterns, with studies of models such as GPT-3 demonstrating that performance scales predictably with both model parameters and training data volume.</p>
<p>However, scaling laws also underscore critical constraints. While larger models can achieve superior performance, the requisite resource demands increase exponentially. As illustrated in <a href="#fig-compute-trends" class="quarto-xref">Figure&nbsp;<span>9.8</span></a>, the computational demands of training state-of-the-art models are escalating at an unsustainable rate. This raises pertinent questions regarding the environmental impact and economic viability of continued scaling.</p>
<div id="fig-compute-trends" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-compute-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/compute-trends.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;9.2: Model training compute is growing at faster and faster rates, especially in the recent deep learning era. Source: [@Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022.]"><img src="images/png/compute-trends.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compute-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: Model training compute is growing at faster and faster rates, especially in the recent deep learning era. Source: <span class="citation" data-cites="Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022">(<a href="../references.html#ref-Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022" role="doc-biblioref">Sevilla et al. 2022a</a>.)</span>
</figcaption><div class="no-row-height column-margin column-container"></div></figure>
</div>
<p>Understanding these fundamental relationships is crucial for informing critical decisions pertaining to system design and resource allocation. They delineate both the potential benefits of scaling and its inherent costs, guiding the development of more efficient and sustainable machine learning systems. This understanding provides the necessary context for our subsequent examination of algorithmic, compute, and data efficiency.</p>
</section><section id="empirical-scaling-laws" class="level3 page-columns page-full" data-number="9.2.2"><h3 data-number="9.2.2" class="anchored" data-anchor-id="empirical-scaling-laws">
<span class="header-section-number">9.2.2</span> Empirical Scaling Laws</h3>
<p>Scaling laws delineate the relationship between the performance of machine learning models and the augmentation of resources, including model size, dataset size, and computational budget. These relationships are typically expressed as power-law functions,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> which demonstrate that model loss decreases predictably with increased resource allocation. Empirical investigations across diverse domains have corroborated that performance metrics‚Äîsuch as accuracy or perplexity‚Äîexhibit smooth and monotonic improvements when models are scaled along these dimensions.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Power-law behavior has been a fundamental pattern in machine learning since early neural network research. While theoretical work in the 1960s and 1980s posited capacity scaling benefits, empirical validation of these relationships across multiple orders of magnitude became feasible only with contemporary computational resources and large-scale datasets.</p></div></div><p>A key example of this behavior is the relationship between generalization error and dataset size, which exhibits three distinct regimes: a Small Data Region, a Power-law Region, and an Irreducible Error Region <span class="citation" data-cites="hestness2017deep">(<a href="../references.html#ref-hestness2017deep" role="doc-biblioref">Hestness et al. 2017</a>)</span>. As shown in <a href="#fig-data-scaling-regimes" class="quarto-xref">Figure&nbsp;<span>9.3</span></a>, small datasets lead to high generalization error constrained by poor estimates (best-guess error). As more data becomes available, models enter the power-law region, where generalization error decreases predictably as a function of dataset size. Eventually, this trend saturates, approaching the irreducible error floor, beyond which further data yields negligible improvements. This visualization demonstrates the principle of diminishing returns and highlights the operational regime in which data scaling is most effective.</p>
<div id="fig-data-scaling-regimes" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-data-scaling-regimes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/efficient_data_scaling_regimes.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;9.3: The relationship between generalization error and dataset size exhibits three distinct regimes. As dataset size increases, generalization error decreases predictably until it reaches an irreducible error floor. Source: [@hestness2017deep]."><img src="images/png/efficient_data_scaling_regimes.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-scaling-regimes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: The relationship between generalization error and dataset size exhibits three distinct regimes. As dataset size increases, generalization error decreases predictably until it reaches an irreducible error floor. Source: <span class="citation" data-cites="hestness2017deep">(<a href="../references.html#ref-hestness2017deep" role="doc-biblioref">Hestness et al. 2017</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-hestness2017deep" class="csl-entry" role="listitem">
Hestness, Joel, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. <span>‚ÄúDeep Learning Scaling Is Predictable, Empirically.‚Äù</span> <em>arXiv Preprint arXiv:1712.00409</em>, December. <a href="http://arxiv.org/abs/1712.00409v1">http://arxiv.org/abs/1712.00409v1</a>.
</div></div></figure>
</div>
<p>A general formulation of this relationship is expressed as:</p>
<p><span class="math display">\[
\mathcal{L}(N) = A N^{-\alpha} + B
\]</span></p>
<p>where <span class="math inline">\(\mathcal{L}(N)\)</span> represents the loss achieved with resource quantity <span class="math inline">\(N\)</span>, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are task-dependent constants, and <span class="math inline">\(\alpha\)</span> is the scaling exponent that characterizes the rate of performance improvement. A larger value of <span class="math inline">\(\alpha\)</span> signifies that performance improvements are more efficient with respect to scaling. This formulation also encapsulates the principle of diminishing returns: incremental gains in performance decrease as <span class="math inline">\(N\)</span> increases.</p>
<p>Empirical evidence for scaling laws is most prominently observed in large language models. In a seminal study, <span class="citation" data-cites="kaplan2020scaling">Kaplan et al. (<a href="../references.html#ref-kaplan2020scaling" role="doc-biblioref">2020</a>)</span> demonstrated that the cross-entropy loss of transformer-based language models scales predictably with three pivotal factors: the number of model parameters, the volume of the training dataset (measured in tokens), and the total computational budget (measured in floating-point operations).<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;This study significantly altered the machine learning community‚Äôs understanding of the impact of scale on model performance through comprehensive empirical validation of scaling laws. Its findings directly influenced training methodologies for large language models such as GPT-3, establishing a quantitative framework for predicting performance improvements based on compute, data, and model size.</p></div></div><p>When these factors are augmented proportionally, models exhibit consistent performance improvements without necessitating architectural modifications or task-specific tuning. This behavior underlies contemporary training strategies for large-scale language models and has significantly influenced design decisions in both research and production environments.</p>
<p>These empirical patterns are illustrated in <a href="#fig-kaplan-scaling" class="quarto-xref">Figure&nbsp;<span>9.4</span></a>, which presents test loss curves for models spanning a range of sizes, from <span class="math inline">\(10^3\)</span> to <span class="math inline">\(10^9\)</span> parameters. The figure reveals two key insights. First, larger models demonstrate superior sample efficiency, achieving target performance levels with fewer training tokens. Second, as computational resources increase, the optimal model size correspondingly grows, with loss decreasing predictably when compute is allocated efficiently. The curves also highlight a practical consideration in large-scale training: compute-optimal solutions often entail early stopping before full convergence, demonstrating the inherent trade-off between training duration and resource utilization.</p>
<div id="fig-kaplan-scaling" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-kaplan-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/kaplan_scaling_data_compute.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;9.4: Test loss curves show that: (1) On the left, larger models achieve better performance with fewer training tokens, leading to better sample efficiency. (2) On the right, with increased compute budget, optimal model size grows steadily and loss decreases consistently. Source: @kaplan2020scaling"><img src="images/png/kaplan_scaling_data_compute.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kaplan-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: Test loss curves show that: (1) On the left, larger models achieve better performance with fewer training tokens, leading to better sample efficiency. (2) On the right, with increased compute budget, optimal model size grows steadily and loss decreases consistently. Source: <span class="citation" data-cites="kaplan2020scaling">Kaplan et al. (<a href="../references.html#ref-kaplan2020scaling" role="doc-biblioref">2020</a>)</span>
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-kaplan2020scaling" class="csl-entry" role="listitem">
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. <span>‚ÄúScaling Laws for Neural Language Models.‚Äù</span> <em>arXiv Preprint arXiv:2001.08361</em>, January. <a href="http://arxiv.org/abs/2001.08361v1">http://arxiv.org/abs/2001.08361v1</a>.
</div></div></figure>
</div>
<p>More fundamentally, this work established a theoretical scaling relationship that defines the optimal allocation of compute between model size and dataset size. For a fixed compute budget <span class="math inline">\(C\)</span>, the optimal trade-off occurs when the dataset size <span class="math inline">\(D\)</span> and model size <span class="math inline">\(N\)</span> satisfy the relationship <span class="math inline">\(D \propto N^{0.74}\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> This equation defines the compute-optimal scaling frontier, where neither the model is undertrained nor the data underutilized. Deviations from this equilibrium‚Äîsuch as training a large model on insufficient data‚Äîresult in suboptimal compute utilization and degraded performance. In practical terms, this implies that scaling model size alone is insufficient; proportional increases in data and compute are required to maintain efficient training dynamics.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;The exponent <span class="math inline">\(0.74\)</span> is empirically derived for transformer-based language models under autoregressive training. It represents the balance point where scaling both data and model together yields optimal performance for a fixed compute budget, and has informed practical model development pipelines across industry.</p></div></div><p>This theoretical prediction is corroborated by empirical fits across multiple model configurations. As shown in <a href="#fig-loss-vs-n-d" class="quarto-xref">Figure&nbsp;<span>9.5</span></a>, the early-stopped test loss <span class="math inline">\(\mathcal{L}(N, D)\)</span> varies predictably with both dataset size and model size, and learning curves across configurations can be aligned through appropriate parameterization. These results further substantiate the regularity of scaling behavior and provide a practical tool for guiding model development under resource constraints.</p>
<div id="fig-loss-vs-n-d" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-loss-vs-n-d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/data_model_loss.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;9.5: Test loss surface \mathcal{L}(N, D) shows predictable variation across model size N and dataset size D."><img src="images/png/data_model_loss.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-vs-n-d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.5: Test loss surface <span class="math inline">\(\mathcal{L}(N, D)\)</span> shows predictable variation across model size <span class="math inline">\(N\)</span> and dataset size <span class="math inline">\(D\)</span>.
</figcaption></figure>
</div>
<p>Similar trends have been observed in other domains. In computer vision, model families such as ResNet and EfficientNet exhibit consistent accuracy improvements when scaled along dimensions of depth, width, and resolution, provided the scaling adheres to principled heuristics. These empirical patterns reinforce the observation that the benefits of scale are governed by underlying regularities that apply broadly across architectures and tasks.</p>
<p>As long as the scaling regime remains balanced‚Äîsuch that the model is underfitting the data and compute capacity is fully utilized‚Äîperformance continues to improve predictably. However, once these assumptions are violated, scaling may lead to overfitting, underutilized resources, or inefficiencies, as explored in subsequent sections.</p>
</section><section id="scaling-regimes" class="level3 page-columns page-full" data-number="9.2.3"><h3 data-number="9.2.3" class="anchored" data-anchor-id="scaling-regimes">
<span class="header-section-number">9.2.3</span> Scaling Regimes</h3>
<p>While the scaling laws discussed thus far have focused primarily on pre-training, recent research indicates that scaling behavior extends to other phases of model development and deployment. A more complete understanding emerges by examining three distinct scaling regimes that characterize different stages of the machine learning pipeline.</p>
<p>The first regime, pre-training scaling, encompasses the traditional domain of scaling laws‚Äîhow model performance improves with larger architectures, expanded datasets, and increased compute during initial training. This has been extensively studied in the context of foundation models<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, where clear power-law relationships emerge between resources and capabilities.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Foundation Models</strong>: Large-scale AI models pre-trained on vast amounts of data that can be adapted to a wide range of downstream tasks. Examples include GPT-3, PaLM, and BERT. These models demonstrate emergent capabilities as they scale in size and training data.</p></div></div><p>Post-training scaling is the second regime that focuses on improvements achieved after initial training through techniques such as fine-tuning, prompt engineering, and task-specific data augmentation. This regime has gained prominence with the rise of foundation models, where adaptation rather than retraining often provides the most efficient path to enhanced performance.</p>
<p>The third regime, test-time scaling, addresses how performance can be improved by allocating additional compute during inference, without modifying the model‚Äôs parameters. This includes methods such as ensemble prediction, chain-of-thought prompting, and iterative refinement, which effectively allow models to spend more time processing each input.</p>
<p>As shown in <a href="#fig-scaling-regimes" class="quarto-xref">Figure&nbsp;<span>9.6</span></a>, these regimes exhibit distinct characteristics in how they trade computational resources for improved performance. Pre-training scaling typically requires massive resources but provides broad capability improvements. Post-training scaling offers more targeted enhancements with moderate resource requirements. Test-time scaling provides flexible performance-compute trade-offs that can be adjusted per inference.</p>
<div id="fig-scaling-regimes" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-scaling-regimes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/three-scaling-laws.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;9.6: The three scaling regimes: pre-training, post-training, and test-time scaling. Each regime exhibits different compute‚Äìperformance characteristics."><img src="images/png/three-scaling-laws.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scaling-regimes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.6: The three scaling regimes: pre-training, post-training, and test-time scaling. Each regime exhibits different compute‚Äìperformance characteristics.
</figcaption></figure>
</div>
<!-- @zh: please redraw the above figure based on the general trend; it doesn't have to be exact. -->
<p>Understanding these regimes is crucial for system design, as it reveals multiple paths to improving performance beyond simply scaling up model size or training data. For resource-constrained deployments, post-training and test-time scaling may provide more practical approaches than full model retraining. Similarly, in high-stakes applications, test-time scaling offers a way to trade latency for accuracy when needed.</p>
<p>This framework provides a more nuanced view of scaling in machine learning systems. By considering all three regimes, designers can make more informed decisions about resource allocation and optimization strategies across the full model lifecycle. The interplay between these regimes also suggests opportunities for hybrid approaches that leverage the strengths of each scaling mode while managing their respective costs and limitations.</p>
</section><section id="system-design" class="level3 page-columns page-full" data-number="9.2.4"><h3 data-number="9.2.4" class="anchored" data-anchor-id="system-design">
<span class="header-section-number">9.2.4</span> System Design</h3>
<p>Scaling laws provide insights into the behavior of machine learning systems as resource allocation increases. The consistent observation of power-law trends suggests that, within a well-defined operational regime, model performance is predominantly determined by scale rather than idiosyncratic architectural innovations. This observation has significant ramifications for system design, resource planning, and the evaluation of efficiency.</p>
<p>A salient characteristic of these laws is the phenomenon of diminishing returns. While augmenting model size or training data volume yields performance improvements, the rate of these improvements diminishes with increasing scale. For instance, doubling the parameter count from 100 million to 200 million may produce substantial gains, whereas a similar doubling from 100 billion to 200 billion may yield only incremental enhancements. This behavior is mathematically captured by the scaling exponent <span class="math inline">\(\alpha\)</span>, which dictates the slope of the performance curve. Lower values of <span class="math inline">\(\alpha\)</span> indicate that more aggressive scaling is necessary to achieve comparable performance gains.</p>
<p>Practically, this implies that unmitigated scaling is ultimately unsustainable. Each successive increment in performance necessitates a disproportionately larger investment in data, compute, or model size. Consequently, scaling laws underscore the escalating tension between model performance and resource expenditure‚Äîa central theme of this discourse. They also emphasize the imperative of balanced scaling, wherein increments in one resource dimension (e.g., model parameters) must be accompanied by commensurate increments in other dimensions (e.g., dataset size and compute budget) to maintain optimal performance progression.</p>
<p>Furthermore, scaling laws can serve as a diagnostic instrument for identifying performance bottlenecks. Performance plateaus despite increased resource allocation may indicate saturation in one dimension‚Äîsuch as inadequate data relative to model size‚Äîor inefficient utilization of computational resources. This diagnostic capability renders scaling laws not only predictive but also prescriptive, enabling practitioners to ascertain the optimal allocation of resources for maximum efficacy.</p>
<p>Understanding scaling laws is not merely of theoretical interest‚Äîit has direct implications for the practical design of efficient machine learning systems. By revealing how performance responds to increases in model size, data, and compute, scaling laws provide a principled framework for making informed design decisions across the full lifecycle of system development.</p>
<p>One key application is in resource budgeting. Scaling laws allow practitioners to estimate the returns on investment for different types of resources. For example, when facing a fixed computational budget, designers can use empirical scaling curves to determine whether performance gains are better achieved by increasing model size, expanding the dataset, or improving training duration. This enables more strategic allocation of limited resources, particularly in scenarios where cost, energy, or time constraints are dominant factors.</p>
<p>In OpenAI‚Äôs development of GPT-3, the authors followed scaling laws derived from earlier experiments to determine the appropriate training dataset size and model parameter count <span class="citation" data-cites="brown2020language">(<a href="../references.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. Rather than conducting expensive architecture searches, they scaled a known transformer architecture along the compute-optimal frontier to 175 billion parameters and 300 billion tokens. This approach allowed them to predict model performance and resource requirements in advance, highlighting the practical value of scaling laws in large-scale system planning.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>‚ÄúLanguage Models Are Few-Shot Learners.‚Äù</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877‚Äì1901.
</div></div><p>Scaling laws also inform decisions about model architecture. Rather than relying on exhaustive architecture search or ad hoc heuristics, system designers can use scaling trends to identify when architectural changes are likely to yield significant improvements and when gains are better pursued through scale alone. For instance, if a given model family follows a favorable scaling curve, it may be preferable to scale that architecture rather than switching to a more complex but untested design. Conversely, if scaling saturates early, it may indicate that architectural innovations are needed to overcome current limitations.</p>
<p>Moreover, scaling laws can guide deployment strategy. In edge and embedded environments, system designers often face tight resource budgets. By understanding how performance degrades when a model is scaled down, it is possible to choose smaller configurations that deliver acceptable accuracy within the deployment constraints. This supports the use of model families with predictable scaling properties, enabling a continuum of options from high-performance cloud deployment to lightweight on-device inference.</p>
<p>Finally, scaling laws provide insight into efficiency limits. By quantifying the trade-offs between scale and performance, they highlight when brute-force scaling becomes inefficient and signal the need for alternative approaches. This includes methods such as knowledge distillation, transfer learning, sparsity, and hardware-aware model design‚Äîall of which aim to extract more value from existing resources without requiring further increases in raw scale.</p>
<p>In this way, scaling laws serve as a compass for system designers, helping them navigate the complex landscape of performance, efficiency, and practicality. They do not dictate a single path forward, but they provide the analytical foundation for choosing among competing options in a principled and data-driven manner.</p>
</section><section id="scaling-vs.-efficiency" class="level3" data-number="9.2.5"><h3 data-number="9.2.5" class="anchored" data-anchor-id="scaling-vs.-efficiency">
<span class="header-section-number">9.2.5</span> Scaling vs.&nbsp;Efficiency</h3>
<p>While scaling laws elucidate a pathway to performance enhancement through the augmentation of model size, dataset volume, and computational budget, they concurrently reveal the rapidly escalating resource demands associated with such progress. As models become increasingly large and sophisticated, the resources necessary to support their training and deployment expand disproportionately. This phenomenon introduces a fundamental tension within contemporary machine learning: the performance gains achieved through scaling are often accompanied by a significant compromise in system efficiency.</p>
<p>A primary concern is the computational expenditure. Training large-scale models necessitates substantial processing power, typically requiring distributed infrastructures comprising hundreds or thousands of accelerators. For instance, the training of state-of-the-art language models may require tens of thousands of GPU-days, consuming millions of kilowatt-hours of electricity and incurring financial costs that are prohibitive for many institutions. As previously discussed, the energy demands of training have outpaced Moore‚Äôs Law, raising critical questions regarding the long-term sustainability of continued scaling.</p>
<p>In addition to computational resources, data acquisition and curation present significant trade-offs. Large models demand not only extensive data volumes but also high-quality, diverse datasets to realize their full potential. The collection, cleansing, and labeling of such datasets are both time-consuming and costly. Furthermore, as models approach saturation of available high-quality data‚Äîparticularly in domains such as natural language‚Äîfurther performance gains through data scaling become increasingly challenging. This necessitates a focus on extracting greater value from existing data, emphasizing the importance of data efficiency as a complement to brute-force scaling.</p>
<p>The financial and environmental implications of scaling also warrant careful consideration. Training runs for large foundation models can incur millions of U.S. dollars in computational expenses alone, and the carbon footprint associated with such training has garnered increasing scrutiny. These costs limit accessibility to cutting-edge research and exacerbate disparities in access to advanced AI systems. From a system design perspective, this underscores the imperative to develop more resource-efficient scaling strategies that minimize consumption without sacrificing performance.</p>
<p>Collectively, these trade-offs highlight that while scaling laws provide a valuable framework for understanding performance growth, they do not offer an unencumbered path to improvement. Each incremental performance gain must be evaluated against the corresponding resource requirements. As machine learning systems approach the practical limits of scale, the focus must shift from mere scaling to efficient scaling. This transition necessitates a holistic approach to system design that balances performance, cost, energy, and environmental impact, ensuring that advancements in AI are not only effective but also sustainable and equitable.</p>
</section><section id="scaling-breakdown" class="level3 page-columns page-full" data-number="9.2.6"><h3 data-number="9.2.6" class="anchored" data-anchor-id="scaling-breakdown">
<span class="header-section-number">9.2.6</span> Scaling Breakdown</h3>
<p>While scaling laws exhibit remarkable consistency within specific operational regimes, they are not devoid of limitations. As machine learning systems expand, they inevitably encounter boundaries where the underlying assumptions of smooth, predictable scaling no longer hold. These breakdown points reveal critical inefficiencies and underscore the necessity for more refined system design.</p>
<p>A common failure mode is imbalanced scaling. For scaling laws to remain valid, model size, dataset size, and computational budget must be augmented in a coordinated manner. Over-investment in one dimension while maintaining others constant often results in suboptimal outcomes. For example, increasing model size without expanding the training dataset may induce overfitting, whereas increasing computational resources without model redesign may lead to inefficient resource utilization <span class="citation" data-cites="hoffmann2022training">(<a href="../references.html#ref-hoffmann2022training" role="doc-biblioref">Hoffmann et al. 2022</a>)</span>. In such scenarios, performance plateaus or even declines despite increased resource expenditure.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hoffmann2022training" class="csl-entry" role="listitem">
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. <span>‚ÄúTraining Compute-Optimal Large Language Models.‚Äù</span> <em>arXiv Preprint arXiv:2203.15556</em>, March. <a href="http://arxiv.org/abs/2203.15556v1">http://arxiv.org/abs/2203.15556v1</a>.
</div></div><p>Closely related is the issue of underutilized compute budgets. Large-scale models often require carefully tuned training schedules and learning rates to make full use of available resources. When compute is insufficiently allocated‚Äîeither through premature stopping, batch size misalignment, or ineffective parallelism‚Äîmodels may fail to reach their performance potential despite significant infrastructure investment.</p>
<p>Another prevalent mode of failure is data saturation. Scaling laws presuppose that model performance will continue to improve with access to sufficient training data. However, in numerous domains‚Äînotably language and vision‚Äîthe availability of high-quality, human-annotated data is finite. As models consume increasingly large datasets, they eventually reach a point of diminishing marginal utility, where additional data points contribute minimal new information. Beyond this threshold, larger models may exhibit memorization rather than generalization, leading to degraded performance on out-of-distribution tasks. This issue is particularly acute when scaling is pursued without commensurate enhancements in data diversity or quality.</p>
<p>Infrastructure bottlenecks also impose practical scaling constraints. As models grow in size, they demand greater memory bandwidth, interconnect capacity, and I/O throughput. These hardware limitations become increasingly challenging to overcome, even with specialized accelerators. For instance, distributing a trillion-parameter model across a cluster necessitates meticulous management of data parallelism, communication overhead, and fault tolerance. The complexity of orchestrating such large-scale systems introduces engineering challenges that can diminish the theoretical gains predicted by scaling laws.</p>
<p>Finally, semantic saturation presents a significant conceptual challenge. At extreme scales, models may approach the limits of what can be learned from their training distributions. Performance on benchmark tasks may continue to improve, but these improvements may no longer reflect meaningful gains in generalization or understanding. Instead, models may become increasingly brittle, susceptible to adversarial examples, or prone to generating plausible but inaccurate outputs‚Äîparticularly in generative tasks.</p>
<p>These breakdown points demonstrate that scaling laws, while powerful, are not absolute. They describe empirical regularities under specific conditions, which become increasingly difficult to maintain at scale. As machine learning systems continue to evolve, it is essential to discern where and why scaling ceases to be effective‚Äîand to develop strategies that enhance performance without relying solely on scale.</p>
<p>To synthesize the primary causes of scaling failure, the following diagnostic matrix (<a href="#tbl-scaling-breakdown" class="quarto-xref">Table&nbsp;<span>9.1</span></a>) outlines typical breakdown types, their underlying causes, and representative scenarios. This table serves as a reference point for anticipating inefficiencies and guiding more balanced system design.</p>
<div id="tbl-scaling-breakdown" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-scaling-breakdown-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.1: Common failure modes associated with unbalanced or excessive scaling across model, data, and compute dimensions.
</figcaption><div aria-describedby="tbl-scaling-breakdown-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 32%">
<col style="width: 32%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Dimension Scaled</th>
<th style="text-align: left;">Type of Breakdown</th>
<th style="text-align: left;">Underlying Cause</th>
<th style="text-align: left;">Example Scenario</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Model Size</td>
<td style="text-align: left;">Overfitting</td>
<td style="text-align: left;">Model capacity exceeds available data</td>
<td style="text-align: left;">Billion-parameter model on limited dataset</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data Volume</td>
<td style="text-align: left;">Diminishing Returns</td>
<td style="text-align: left;">Saturation of new or diverse information</td>
<td style="text-align: left;">Scaling web text beyond useful threshold</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Compute Budget</td>
<td style="text-align: left;">Underutilized Resources</td>
<td style="text-align: left;">Insufficient training steps or inefficient use</td>
<td style="text-align: left;">Large model with truncated training duration</td>
</tr>
<tr class="even">
<td style="text-align: left;">Imbalanced Scaling</td>
<td style="text-align: left;">Inefficiency</td>
<td style="text-align: left;">Uncoordinated increase in model/data/compute</td>
<td style="text-align: left;">Doubling model size without more data or time</td>
</tr>
<tr class="odd">
<td style="text-align: left;">All Dimensions</td>
<td style="text-align: left;">Semantic Saturation</td>
<td style="text-align: left;">Exhaustion of learnable patterns in the domain</td>
<td style="text-align: left;">No further gains despite scaling all inputs</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In this section, we have explored the fundamental principles of AI scaling laws, examining their empirical foundations, practical implications, and inherent limitations. Scaling laws provide a valuable framework for understanding how model performance scales with resources, but they also highlight the importance of efficiency and sustainability. The trade-offs and challenges associated with scaling underscore the need for a holistic approach to system design‚Äîone that balances performance with resource constraints. In the following sections, we will delve into the specific dimensions of efficiency‚Äîalgorithmic, compute, and data‚Äîexploring how these areas contribute to the development of more sustainable and effective machine learning systems.</p>
</section><section id="toward-efficient-scaling" class="level3" data-number="9.2.7"><h3 data-number="9.2.7" class="anchored" data-anchor-id="toward-efficient-scaling">
<span class="header-section-number">9.2.7</span> Toward Efficient Scaling</h3>
<p>While the empirical success of scaling laws has driven substantial progress in artificial intelligence, these observations raise foundational questions that extend beyond resource allocation. The sustainability of the current scaling trajectory, and its adequacy in capturing the principles of efficient AI system design, must be critically examined.</p>
<p>The empirical regularities observed in scaling laws prompt deeper inquiry: can observation be translated into a theoretical framework that explains the mechanisms driving these patterns? Establishing such a framework would enhance scientific understanding and inform the design of more efficient algorithms and architectures. The limitations inherent in brute-force scaling‚Äîsuch as diminishing returns and observed breakdowns‚Äîhighlight the need for architectural innovations that reshape scaling behavior and address efficiency from an algorithmic standpoint.</p>
<p>Simultaneously, the increasing demand for data emphasizes the importance of transitioning to a data-centric paradigm. As data saturation is approached, the efficiency of data acquisition, curation, and utilization becomes critical. This shift requires a deeper understanding of data dynamics and the development of strategies that maximize the utility of limited data resources. These considerations form the basis for the upcoming discussion on data efficiency.</p>
<p>The computational demands of large-scale models further underscore the necessity of compute efficiency. Sustainable scaling depends on minimizing resource consumption while maintaining or improving performance. This objective motivates the exploration of hardware-optimized architectures and training methodologies that support efficient execution, to be discussed in the context of compute efficiency.</p>
<p>Algorithmic, compute, and data efficiency are not independent; their interdependence shapes the overall performance of machine learning systems. The emergence of novel capabilities in extremely large models suggests the potential for synergistic effects across these dimensions. Achieving real-world efficiency requires a holistic approach to system design in which these elements are carefully orchestrated. This perspective introduces the forthcoming discussion on system efficiency.</p>
<p>Finally, the ethical considerations surrounding access to compute and data resources demonstrate that efficiency is not solely a technical goal. Ensuring equitable distribution of the benefits of efficient AI represents a broader societal imperative. Subsequent sections will address these challenges, including the limits of optimization, the implications of Moore‚Äôs Law, and the balance between innovation and accessibility.</p>
<p>In summary, the future of scaling lies not in unbounded expansion but in the coordinated optimization of algorithmic, compute, and data resources. The sections that follow will examine each of these dimensions and their contributions to the development of efficient and sustainable machine learning systems. Although scaling laws offer a valuable perspective, they represent only one component of a more comprehensive framework.</p>
</section></section><section id="the-pillars-of-ai-efficiency" class="level2 page-columns page-full" data-number="9.3"><h2 data-number="9.3" class="anchored" data-anchor-id="the-pillars-of-ai-efficiency">
<span class="header-section-number">9.3</span> The Pillars of AI Efficiency</h2>
<p>The trajectory of machine learning has been significantly shaped by scaling laws and the evolving concept of efficiency. While scaling laws demonstrate the potential benefits of increasing model size, dataset volume, and computational resources, they also highlight the critical need for efficient resource utilization. To systematically address these challenges, we delineate three fundamental and interconnected pillars of AI efficiency: algorithmic efficiency, compute efficiency, and data efficiency. These pillars represent critical domains that have profoundly influenced how we navigate the trade-offs revealed by scaling laws.</p>
<p>Algorithmic efficiency pertains to the design and optimization of algorithms to maximize performance within given resource constraints. As scaling laws indicate that larger models generally perform better, algorithmic efficiency becomes crucial for making these models practical and deployable. Contemporary research focuses on techniques such as model compression, architectural optimization, and algorithmic refinement, all aimed at preserving the benefits of scale while minimizing resource consumption.</p>
<p>Compute efficiency addresses the optimization of computational resources, including hardware and energy utilization. Scaling laws have shown that training compute requirements are growing at an exponential rate, making compute efficiency increasingly critical. The advent of specialized hardware accelerators, such as GPUs and TPUs, has enabled the development of large-scale models. However, the energy demands associated with training and deploying these models have raised concerns regarding sustainability. Compute efficiency, therefore, encompasses strategies for optimizing hardware utilization, reducing energy footprint, and exploring alternative computing paradigms that can support continued scaling.</p>
<p>Data efficiency focuses on maximizing the information gained from available data while minimizing the required data volume. Scaling laws demonstrate that model performance improves with larger datasets, but they also reveal diminishing returns and practical limits to data collection. This pillar becomes especially important as we approach the boundaries of available high-quality data in domains like language modeling. Methods such as data augmentation, active learning, and efficient data representation aim to achieve the benefits predicted by scaling laws with reduced data requirements.</p>
<p>These three pillars are not mutually exclusive; rather, they are deeply intertwined and often mutually reinforcing. Improvements in one pillar can lead to gains in others, and trade-offs between them are frequently necessary. As we examine the historical evolution of these dimensions, as depicted in <a href="#fig-evolution-efficiency" class="quarto-xref">Figure&nbsp;<span>9.7</span></a>, we will elucidate the dynamic interplay between algorithmic, compute, and data efficiency, providing a foundation for understanding how to achieve efficient scaling in contemporary machine learning.</p>
<div id="fig-evolution-efficiency" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-evolution-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="2abdd66045808c7d07317e46f41d4af3ac1aea8a.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;9.7: Evolution of AI Efficiency over the past few decades."><img src="efficient_ai_files/mediabag/2abdd66045808c7d07317e46f41d4af3ac1aea8a.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-evolution-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.7: Evolution of AI Efficiency over the past few decades.
</figcaption></figure>
</div>
<section id="algorithmic-efficiency" class="level3 page-columns page-full" data-number="9.3.1"><h3 data-number="9.3.1" class="anchored" data-anchor-id="algorithmic-efficiency">
<span class="header-section-number">9.3.1</span> Algorithmic Efficiency</h3>
<p>Model efficiency addresses the design and optimization of machine learning models to deliver high performance while minimizing computational and memory requirements. It is a critical component of machine learning systems, enabling models to operate effectively across a range of platforms, from cloud servers to resource-constrained edge devices. The evolution of algorithmic efficiency mirrors the broader trajectory of machine learning itself, shaped by algorithmic advances, hardware developments, and the increasing complexity of real-world applications.</p>
<section id="early-efficiency" class="level4"><h4 class="anchored" data-anchor-id="early-efficiency">Early Efficiency</h4>
<p>During the early decades of machine learning, algorithmic efficiency was closely tied to computational constraints, particularly in terms of parallelization. Early algorithms like decision trees and SVMs were primarily optimized for single-machine performance, with parallel implementations limited mainly to ensemble methods where multiple models could be trained independently on different data batches.</p>
<p>Neural networks also began to emerge during this period, but they were constrained by the limited computational capacity of the time. Unlike earlier algorithms, neural networks showed potential for model parallelism‚Äîthe ability to distribute model components across multiple processors‚Äîthough this advantage wouldn‚Äôt be fully realized until the deep learning era. This led to careful optimizations in their design, such as limiting the number of layers or neurons to keep computations manageable. Efficiency was achieved not only through model simplicity but also through innovations in optimization techniques, such as the adoption of stochastic gradient descent, which made training more practical for the hardware available.</p>
<p>The era of algorithmic efficiency laid the groundwork for machine learning by emphasizing the importance of achieving high performance under strict resource constraints. These principles remain important even in today‚Äôs datacenter-scale computing, where hardware limitations in memory bandwidth and power consumption continue to drive innovation in algorithmic efficiency. It was an era of problem-solving through mathematical rigor and computational restraint, establishing patterns that would prove valuable as models grew in scale and complexity.</p>
</section><section id="deep-learning-era" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="deep-learning-era">Deep Learning Era</h4>
<p>The introduction of deep learning in the early 2010s marked a turning point for algorithmic efficiency. Neural networks, which had previously been constrained by hardware limitations, now benefited from advancements in computational power, particularly the adoption of GPUs <span class="citation" data-cites="Krizhevsky_Sutskever_Hinton_2012">(<a href="../references.html#ref-Krizhevsky_Sutskever_Hinton_2012" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2017</a>)</span>. This capability allowed researchers to train larger, more complex models, leading to breakthroughs in tasks such as image recognition, natural language processing, and speech synthesis.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Krizhevsky_Sutskever_Hinton_2012" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>‚ÄúImageNet Classification with Deep Convolutional Neural Networks.‚Äù</span> <em>Communications of the ACM</em> 60 (6): 84‚Äì90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div><div id="fn6"><p><sup>6</sup>&nbsp;Pruning was inspired by biological development where unused connections between neurons are eliminated during brain development.</p></div><div id="ref-LeCun_Denker_Solla_1990" class="csl-entry" role="listitem">
LeCun, Yann, John S. Denker, and Sara A. Solla. 1989. <span>‚ÄúOptimal Brain Damage.‚Äù</span> In <em>Advances in Neural Information Processing Systems</em>, 2:598‚Äì605. Morgan-Kaufmann. <a href="http://papers.nips.cc/paper/250-optimal-brain-damage">http://papers.nips.cc/paper/250-optimal-brain-damage</a>.
</div><div id="ref-Jacob_et_al_2018" class="csl-entry" role="listitem">
Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. <span>‚ÄúQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.‚Äù</span> In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2704‚Äì13. IEEE. <a href="https://doi.org/10.1109/cvpr.2018.00286">https://doi.org/10.1109/cvpr.2018.00286</a>.
</div><div id="ref-Hinton_Vinyals_2015" class="csl-entry" role="listitem">
Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. <span>‚ÄúDistilling the Knowledge in a Neural Network.‚Äù</span> <em>arXiv Preprint arXiv:1503.02531</em>, March. <a href="http://arxiv.org/abs/1503.02531v1">http://arxiv.org/abs/1503.02531v1</a>.
</div></div><p>However, the growing size and complexity of these models introduced new challenges. Larger models required significant computational resources and memory, making them difficult to deploy in practical applications. To address these challenges, researchers developed techniques to reduce model size and computational requirements without sacrificing accuracy. Pruning<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, for instance, involved removing redundant or less significant connections within a neural network, reducing both the model‚Äôs parameters and its computational overhead <span class="citation" data-cites="LeCun_Denker_Solla_1990">(<a href="../references.html#ref-LeCun_Denker_Solla_1990" role="doc-biblioref">LeCun, Denker, and Solla 1989</a>)</span>. Quantization focused on lowering the precision of numerical representations, enabling models to run faster and with less memory <span class="citation" data-cites="Jacob_et_al_2018">(<a href="../references.html#ref-Jacob_et_al_2018" role="doc-biblioref">Jacob et al. 2018</a>)</span>. Knowledge distillation allowed large, resource-intensive models (referred to as ‚Äúteachers‚Äù) to transfer their knowledge to smaller, more efficient models (referred to as ‚Äústudents‚Äù), achieving comparable performance with reduced complexity <span class="citation" data-cites="Hinton_Vinyals_2015">(<a href="../references.html#ref-Hinton_Vinyals_2015" role="doc-biblioref">Hinton, Vinyals, and Dean 2015</a>)</span>.</p>
<p>At the same time, new architectures specifically designed for efficiency began to emerge. Models such as MobileNet <span class="citation" data-cites="Howard_et_al_2017">(<a href="../references.html#ref-Howard_et_al_2017" role="doc-biblioref">Howard et al. 2017</a>)</span>, EfficientNet <span class="citation" data-cites="Tan_Le_2019">(<a href="../references.html#ref-Tan_Le_2019" role="doc-biblioref">Tan and Le 2019</a>)</span>, and SqueezeNet <span class="citation" data-cites="Iandola_et_al_2016">(<a href="../references.html#ref-Iandola_et_al_2016" role="doc-biblioref">Iandola et al. 2016</a>)</span> demonstrated that compact designs could deliver high performance, enabling their deployment on devices with limited computational power, such as smartphones and IoT devices<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Howard_et_al_2017" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. <span>‚ÄúMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,‚Äù</span> April. <a href="http://arxiv.org/abs/1704.04861v1">http://arxiv.org/abs/1704.04861v1</a>.
</div><div id="ref-Tan_Le_2019" class="csl-entry" role="listitem">
Tan, Mingxing, and Quoc V. Le. 2019. <span>‚ÄúEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.‚Äù</span> In <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 6105‚Äì14.
</div><div id="ref-Iandola_et_al_2016" class="csl-entry" role="listitem">
Iandola, Forrest N., Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. 2016. <span>‚ÄúSqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and &lt;0.5MB Model Size,‚Äù</span> February. <a href="http://arxiv.org/abs/1602.07360v4">http://arxiv.org/abs/1602.07360v4</a>.
</div><div id="fn7"><p><sup>7</sup>&nbsp;<strong>MobileNet/EfficientNet/SqueezeNet</strong>: Compact neural network architectures designed for efficiency, balancing high performance with reduced computational demands. MobileNet introduced depthwise separable convolutions (2017), EfficientNet applied compound scaling (2019), and SqueezeNet focused on reducing parameters using 1x1 convolutions (2016).</p></div></div></section><section id="modern-efficiency" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="modern-efficiency">Modern Efficiency</h4>
<p>As machine learning systems continue to grow in scale and complexity, the focus on algorithmic efficiency has expanded to address sustainability and scalability. Today‚Äôs challenges require balancing performance with resource efficiency, particularly as models like GPT-4 and beyond are applied to increasingly diverse tasks and environments. One emerging approach involves sparsity, where only the most critical parameters of a model are retained, significantly reducing computational and memory demands. Hardware-aware design has also become a priority, as researchers optimize models to take full advantage of specific accelerators, such as GPUs, TPUs, and edge processors. Another important trend is parameter-efficient fine-tuning, where large pre-trained models can be adapted to new tasks by updating only a small subset of parameters. Low-Rank Adaptation (LoRA)<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> and prompt-tuning exemplify this approach, allowing systems to achieve task-specific performance while maintaining the efficiency advantages of smaller models.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Low-Rank Adaptation (LoRA)</strong>: A technique that adapts large pre-trained models to new tasks by updating only a small subset of parameters, significantly reducing computational and memory requirements.</p></div></div><p>As shown in <a href="#fig-compute-trends" class="quarto-xref">Figure&nbsp;<span>9.8</span></a>, model training compute requirements have been growing at an accelerating rate, especially in the deep learning era. This trend underscores the necessity for algorithmic innovations that enhance efficiency without compromising performance.</p>
<div id="fig-compute-trends" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-compute-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/compute-trends.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9.8: Model training compute is growing at faster and faster rates, especially in the recent deep learning era. Source: [@Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022.]"><img src="images/png/compute-trends.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compute-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.8: Model training compute is growing at faster and faster rates, especially in the recent deep learning era. Source: <span class="citation" data-cites="Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022">(<a href="../references.html#ref-Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022" role="doc-biblioref">Sevilla et al. 2022a</a>.)</span>
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-Sevilla_Heim_Ho_Besiroglu_Hobbhahn_Villalobos_2022" class="csl-entry" role="listitem">
Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. 2022a. <span>‚ÄúCompute Trends Across Three Eras of Machine Learning.‚Äù</span> In <em>2022 International Joint Conference on Neural Networks (IJCNN)</em>, 1‚Äì8. IEEE. <a href="https://doi.org/10.1109/ijcnn55064.2022.9891914">https://doi.org/10.1109/ijcnn55064.2022.9891914</a>.
</div></div></figure>
</div>
<p>These advancements reflect a broader shift in focus: from scaling models indiscriminately to creating architectures that are purpose-built for efficiency. This modern era emphasizes not only technical excellence but also the practicality and sustainability of machine learning systems.</p>
</section><section id="efficiency-in-design" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="efficiency-in-design">Efficiency in Design</h4>
<p>Model efficiency is fundamental to the design of scalable and sustainable machine learning systems. By reducing computational and memory demands, efficient models lower energy consumption and operational costs, making machine learning systems accessible to a wider range of applications and deployment environments. Moreover, algorithmic efficiency complements other dimensions of efficiency, such as compute and data efficiency, by reducing the overall burden on hardware and enabling faster training and inference cycles.</p>
<div id="fig-algo-efficiency" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-algo-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/algo_efficiency.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;9.9: Within just seven years, 44 times less compute was required to achieve AlexNet performance. Source: [@AIandeff21:online]."><img src="images/png/algo_efficiency.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-algo-efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.9: Within just seven years, 44 times less compute was required to achieve AlexNet performance. Source: <span class="citation" data-cites="AIandeff21:online">(<a href="../references.html#ref-AIandeff21:online" role="doc-biblioref">Jaech et al. 2024</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-AIandeff21:online" class="csl-entry" role="listitem">
Jaech, Aaron, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, et al. 2024. <span>‚ÄúOpenAI O1 System Card.‚Äù</span> <em>CoRR</em>. <a href="https://doi.org/10.48550/ARXIV.2412.16720">https://doi.org/10.48550/ARXIV.2412.16720</a>.
</div></div></figure>
</div>
<p>Notably, as <a href="#fig-algo-efficiency" class="quarto-xref">Figure&nbsp;<span>9.9</span></a> shows, by 9, the computational resources needed to train a neural network to achieve AlexNet-level performance on ImageNet classification had decreased by <span class="math inline">\(44\times\)</span> compared to 2012. This improvement‚Äîhalving every 16 months‚Äîoutpaced the hardware efficiency gains of Moore‚Äôs Law<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongside hardware innovations <span class="citation" data-cites="Hernandez_et_al_2020">(<a href="../references.html#ref-Hernandez_et_al_2020" role="doc-biblioref">Hernandez, Brown, et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Moore‚Äôs Law</strong>: An observation made by <a href="https://en.wikipedia.org/wiki/Gordon_Moore">Gordon Moore</a> in 1965, stating that the number of transistors on a microchip doubles approximately every two years, leading to an exponential increase in computational power and a corresponding decrease in relative cost.</p></div><div id="ref-Hernandez_et_al_2020" class="csl-entry" role="listitem">
Hernandez, Danny, Tom B. Brown, et al. 2020. <span>‚ÄúMeasuring the Algorithmic Efficiency of Neural Networks.‚Äù</span> <em>OpenAI Blog</em>. <a href="https://openai.com/research/ai-and-efficiency">https://openai.com/research/ai-and-efficiency</a>.
</div></div><p>The evolution of algorithmic efficiency, from algorithmic innovations to hardware-aware optimization, is of importance in machine learning. As the field advances, algorithmic efficiency will remain central to the design of systems that are high-performing, scalable, and sustainable.</p>
</section></section><section id="compute-efficiency" class="level3 page-columns page-full" data-number="9.3.2"><h3 data-number="9.3.2" class="anchored" data-anchor-id="compute-efficiency">
<span class="header-section-number">9.3.2</span> Compute Efficiency</h3>
<p>Compute efficiency focuses on the effective use of hardware and computational resources to train and deploy machine learning models. It encompasses strategies for reducing energy consumption, optimizing processing speed, and leveraging hardware capabilities to achieve scalable and sustainable system performance. The evolution of compute efficiency is closely tied to advancements in hardware technologies, reflecting the growing demands of machine learning applications over time.</p>
<section id="general-purpose-computing-era" class="level4"><h4 class="anchored" data-anchor-id="general-purpose-computing-era">General-Purpose Computing Era</h4>
<p>In the early days of machine learning, compute efficiency was shaped by the limitations of general-purpose CPUs. During this period, machine learning models had to operate within strict computational constraints, as specialized hardware for machine learning did not yet exist. Efficiency was achieved through algorithmic innovations, such as simplifying mathematical operations, reducing model size, and optimizing data handling to minimize computational overhead.</p>
<p>Researchers worked to maximize the capabilities of CPUs by using parallelism where possible, though options were limited. Training times for models were often measured in days or weeks, as even relatively small datasets and models pushed the boundaries of available hardware. The focus on compute efficiency during this era was less about hardware optimization and more about designing algorithms that could run effectively within these constraints.</p>
</section><section id="accelerated-computing-era" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="accelerated-computing-era">Accelerated Computing Era</h4>
<p>The introduction of deep learning in the early 2010s brought a seismic shift in the landscape of compute efficiency. Models like AlexNet and ResNet showed the potential of neural networks, but their computational demands quickly surpassed the capabilities of traditional CPUs. As shown in <a href="#fig-comp_efficiency" class="quarto-xref">Figure&nbsp;<span>9.10</span></a>, this marked the beginning of an era of exponential growth in compute usage. OpenAI‚Äôs analysis reveals that the amount of compute used in AI training has increased 300,000 times since 2012, doubling approximately every 3.4 months‚Äîa rate far exceeding Moore‚Äôs Law <span class="citation" data-cites="Amodei_et_al_2018">(<a href="../references.html#ref-Amodei_et_al_2018" role="doc-biblioref">Amodei, Hernandez, et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Amodei_et_al_2018" class="csl-entry" role="listitem">
Amodei, Dario, Danny Hernandez, et al. 2018. <span>‚ÄúAI and Compute.‚Äù</span> <em>OpenAI Blog</em>. <a href="https://openai.com/research/ai-and-compute">https://openai.com/research/ai-and-compute</a>.
</div></div><div id="fig-comp_efficiency" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-comp_efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/compute_efficiency.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;9.10: From AlexNet to AlphaGo Zero, there has been a 300,000x increase in demand for computing power over seven years. Source: [@AIandcom90:online]."><img src="images/png/compute_efficiency.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-comp_efficiency-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.10: From AlexNet to AlphaGo Zero, there has been a 300,000x increase in demand for computing power over seven years. Source: <span class="citation" data-cites="AIandcom90:online">(<a href="../references.html#ref-AIandcom90:online" role="doc-biblioref">LeCun, Bengio, and Hinton 2015</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-AIandcom90:online" class="csl-entry" role="listitem">
LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. <span>‚ÄúDeep Learning.‚Äù</span> <em>Nature</em> 521 (7553): 436‚Äì44. <a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a>.
</div></div></figure>
</div>
<p>This rapid growth was driven not only by the adoption of GPUs, which offered unparalleled parallel processing capabilities, but also by the willingness of researchers to scale up experiments by using large GPU clusters. Specialized hardware accelerators such as Google‚Äôs Tensor Processing Units (TPUs)<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> and application-specific integrated circuits (ASICs)<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> further revolutionized compute efficiency. These innovations enabled significant reductions in training times for deep learning models, transforming tasks that once took weeks into operations completed in hours or days.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<strong>Tensor Processing Units (TPUs)</strong>: Google‚Äôs custom-designed AI accelerator chips, introduced in 2016, demonstrated significant performance gains‚Äîprocessing AI workloads up to 30 times faster than contemporary GPUs and 80 times faster than CPUs.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Application-Specific Integrated Circuits (ASICs)</strong>: Custom-designed chips optimized for specific machine learning workloads, offering superior performance and energy efficiency compared to general-purpose processors. ASICs can achieve 10-100x better performance per watt than GPUs for targeted applications.</p></div></div><p>The rise of large-scale compute also highlighted the complementary relationship between algorithmic innovation and hardware efficiency. Advances such as neural architecture search and massive batch processing leveraged the increasing availability of computational power, demonstrating that more compute could directly lead to better performance in many domains.</p>
</section><section id="sustainable-computing-era" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sustainable-computing-era">Sustainable Computing Era</h4>
<p>As machine learning systems scale further, compute efficiency has become closely tied to sustainability. Training state-of-the-art models like GPT-4 requires massive computational resources, leading to increased attention on the environmental impact of large-scale computing. The projected electricity usage of data centers, shown in <a href="#fig-datacenter-energy-usage" class="quarto-xref">Figure&nbsp;<span>9.11</span></a>, highlights this concern. Between 2010 and 2030, electricity consumption is expected to rise sharply, particularly under the ‚ÄúWorst‚Äù scenario, where it could exceed 8,000 TWh by 2030<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;The ‚ÄúBest,‚Äù ‚ÄúExpected,‚Äù and ‚ÄúWorst‚Äù scenarios in the figure reflect different assumptions about how efficiently data centers can handle increasing internet traffic, with the best-case scenario assuming the fastest improvements in energy efficiency and the worst-case scenario assuming minimal gains, leading to sharply rising energy demands.</p></div></div><div class="cell page-columns page-full">
<div class="cell-output-display page-columns page-full">
<div id="fig-datacenter-energy-usage" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-datacenter-energy-usage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="efficient_ai_files/figure-html/fig-datacenter-energy-usage-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;9.11: Electricity usage (TWh) of Data Centers from 2010 to 2030. Source: @andrae2015global."><img src="efficient_ai_files/figure-html/fig-datacenter-energy-usage-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-datacenter-energy-usage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.11: Electricity usage (TWh) of Data Centers from 2010 to 2030. Source: <span class="citation" data-cites="andrae2015global">Andrae and Edler (<a href="../references.html#ref-andrae2015global" role="doc-biblioref">2015</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-andrae2015global" class="csl-entry" role="listitem">
Andrae, Anders, and Tomas Edler. 2015. <span>‚ÄúOn Global Electricity Usage of Communication Technology: Trends to 2030.‚Äù</span> <em>Challenges</em> 6 (1): 117‚Äì57. <a href="https://doi.org/10.3390/challe6010117">https://doi.org/10.3390/challe6010117</a>.
</div></div></figure>
</div>
</div>
</div>
<p>The dramatic demand for energy usage underscores the urgency for compute efficiency, as even large data centers face energy constraints due to limitations in electrical grid capacity and power availability in specific locations. To address these challenges, the focus today is on optimizing hardware utilization and minimizing energy consumption, both in cloud data centers and at the edge.</p>
<p>One key trend is the adoption of energy-aware scheduling and resource allocation techniques, which ensure that computational workloads are distributed efficiently across available hardware <span class="citation" data-cites="Patterson_et_al_2021">(<a href="../references.html#ref-Patterson_et_al_2021" role="doc-biblioref">D. Patterson et al. 2021</a>)</span>. Researchers are also developing methods to dynamically adjust precision levels during training and inference, using lower precision operations (e.g., mixed-precision training) to reduce power consumption without sacrificing accuracy.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Patterson_et_al_2021" class="csl-entry" role="listitem">
Patterson, David, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. <span>‚ÄúCarbon Emissions and Large Neural Network Training.‚Äù</span> <em>arXiv Preprint arXiv:2104.10350</em>, April. <a href="http://arxiv.org/abs/2104.10350v3">http://arxiv.org/abs/2104.10350v3</a>.
</div></div><p>Another focus is on distributed systems, where compute efficiency is achieved by splitting workloads across multiple machines. Techniques such as model parallelism and data parallelism allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to maximize throughput. These methods reduce training times while minimizing the idle time of hardware resources.</p>
<p>At the edge, compute efficiency is evolving to address the growing demand for real-time processing in energy-constrained environments. Innovations such as hardware-aware model optimization, lightweight inference engines, and adaptive computing architectures are paving the way for highly efficient edge systems. These advancements are critical for enabling applications like autonomous vehicles and smart home devices, where latency and energy efficiency are paramount.</p>
</section><section id="compute-efficiencys-role" class="level4"><h4 class="anchored" data-anchor-id="compute-efficiencys-role">Compute Efficiency‚Äôs Role</h4>
<p>Compute efficiency is a critical enabler of system-wide performance and scalability. By optimizing hardware utilization and energy consumption, it ensures that machine learning systems remain practical and cost-effective, even as models and datasets grow larger. Moreover, compute efficiency directly complements model and data efficiency. For example, compact models reduce computational requirements, while efficient data pipelines streamline hardware usage.</p>
<p>The evolution of compute efficiency highlights its essential role in addressing the growing demands of modern machine learning systems. From early reliance on CPUs to the emergence of specialized accelerators and sustainable computing practices, this dimension remains central to building scalable, accessible, and environmentally responsible machine learning systems.</p>
</section></section><section id="data-efficiency" class="level3 page-columns page-full" data-number="9.3.3"><h3 data-number="9.3.3" class="anchored" data-anchor-id="data-efficiency">
<span class="header-section-number">9.3.3</span> Data Efficiency</h3>
<p>Data efficiency focuses on optimizing the amount and quality of data required to train machine learning models effectively. As datasets have grown in scale and complexity, managing data efficiently has become an increasingly critical challenge for machine learning systems. While historically less emphasized than model or compute efficiency, data efficiency has emerged as a pivotal dimension, driven by the rising costs of data collection, storage, and processing. Its evolution reflects the changing role of data in machine learning, from a scarce resource to a massive but unwieldy asset.</p>
<section id="data-scarcity-era" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="data-scarcity-era">Data Scarcity Era</h4>
<p>In the early days of machine learning, data efficiency was not a significant focus, largely because datasets were relatively small and manageable. The challenge during this period was often acquiring enough labeled data to train models effectively. Researchers relied heavily on curated datasets, such as <a href="https://archive.ics.uci.edu/">UCI‚Äôs Machine Learning Repository</a>, which provided clean, well-structured data for experimentation. Feature selection and dimensionality reduction techniques, such as principal component analysis (PCA), were common methods for ensuring that models extracted the most valuable information from limited data.</p>
<p>During this era, data efficiency was achieved through careful preprocessing and data cleaning. Algorithms were designed to work well with relatively small datasets such as MNIST <span class="citation" data-cites="deng2012mnist">(<a href="../references.html#ref-deng2012mnist" role="doc-biblioref">Deng 2012</a>)</span>, Caltech 101 <span class="citation" data-cites="FeiFei2004LearningGV">(<a href="../references.html#ref-FeiFei2004LearningGV" role="doc-biblioref">Fei-Fei, Fergus, and Perona, n.d.</a>)</span> and CIFAR-10 <span class="citation" data-cites="Krizhevsky09learningmultiple">(<a href="../references.html#ref-Krizhevsky09learningmultiple" role="doc-biblioref">Krizhevsky 2009</a>)</span>, and computational limitations reinforced the need for data parsimony. These constraints shaped the development of techniques that maximized performance with minimal data, ensuring that every data point contributed meaningfully to the learning process.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deng2012mnist" class="csl-entry" role="listitem">
Deng, Li. 2012. <span>‚ÄúThe MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web].‚Äù</span> <em>IEEE Signal Processing Magazine</em> 29 (6): 141‚Äì42. <a href="https://doi.org/10.1109/msp.2012.2211477">https://doi.org/10.1109/msp.2012.2211477</a>.
</div><div id="ref-FeiFei2004LearningGV" class="csl-entry" role="listitem">
Fei-Fei, Li, R. Fergus, and P. Perona. n.d. <span>‚ÄúLearning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories.‚Äù</span> In <em>2004 Conference on Computer Vision and Pattern Recognition Workshop</em>. IEEE. <a href="https://doi.org/10.1109/cvpr.2004.383">https://doi.org/10.1109/cvpr.2004.383</a>.
</div><div id="ref-Krizhevsky09learningmultiple" class="csl-entry" role="listitem">
Krizhevsky, Alex. 2009. <span>‚ÄúLearning Multiple Layers of Features from Tiny Images.‚Äù</span>
</div></div></section><section id="big-data-era" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="big-data-era">Big Data Era</h4>
<p>The advent of deep learning in the 2010s transformed the role of data in machine learning. Models such as AlexNet and GPT-3 demonstrated that larger datasets often led to better performance, particularly for complex tasks like image classification and natural language processing. This marked the beginning of the ‚Äúbig data‚Äù era, where the focus shifted from making the most of limited data to scaling data collection and processing to unprecedented levels.</p>
<p>However, this reliance on large datasets introduced significant inefficiencies. Data collection became a costly and time-consuming endeavor, requiring vast amounts of labeled data for supervised learning tasks. To address these challenges, researchers developed techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specific data <span class="citation" data-cites="yosinski2014transferable">(<a href="../references.html#ref-yosinski2014transferable" role="doc-biblioref">Yosinski et al. 2014</a>)</span>. Data augmentation techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creating new variations of existing samples. Additionally, active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort while maintaining performance <span class="citation" data-cites="Settles_2009">(<a href="../references.html#ref-Settles_2009" role="doc-biblioref">Settles 2012a</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-yosinski2014transferable" class="csl-entry" role="listitem">
Yosinski, Jason, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. <span>‚ÄúHow Transferable Are Features in Deep Neural Networks?‚Äù</span> <em>Advances in Neural Information Processing Systems</em> 27.
</div><div id="ref-Settles_2009" class="csl-entry" role="listitem">
Settles, Burr. 2012a. <em>Active Learning</em>. <em>Computer Sciences Technical Report</em>. University of Wisconsin‚ÄìMadison; Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01560-1">https://doi.org/10.1007/978-3-031-01560-1</a>.
</div></div><p>Despite these advancements, the ‚Äúmore data is better‚Äù paradigm dominated this period, with less attention paid to streamlining data usage. As a result, the environmental and economic costs of managing large datasets began to emerge as significant concerns.</p>
</section><section id="modern-data-efficiency-era" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="modern-data-efficiency-era">Modern Data Efficiency Era</h4>
<p>As machine learning systems grow in scale, the inefficiencies of large datasets have become increasingly apparent. Recent work has focused on developing approaches that maximize the value of data while minimizing resource requirements. This shift reflects a growing understanding that bigger datasets do not always lead to better performance, particularly when considering the computational and environmental costs of training on massive scales.</p>
<p>Data-centric AI has emerged as a key paradigm, emphasizing the importance of data quality over quantity. This approach focuses on enhancing data preprocessing, removing redundancy, and improving labeling efficiency. Research has shown that careful curation and filtering of datasets can achieve comparable or superior model performance while using only a fraction of the original data volume. For instance, systematic analyses of web-scale datasets demonstrate that targeted filtering techniques can maintain model capabilities while significantly reducing training data requirements <span class="citation" data-cites="penedo2024fineweb">(<a href="../references.html#ref-penedo2024fineweb" role="doc-biblioref">Penedo et al. 2024</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>Several techniques have emerged to support this transition toward data efficiency. Self-supervised learning enables models to learn meaningful representations from unlabeled data, reducing the dependency on expensive human-labeled datasets. Active learning strategies selectively identify the most informative examples for labeling, while curriculum learning structures the training process to progress from simple to complex examples, improving learning efficiency. These approaches work together to minimize data requirements while maintaining model performance.</p>
<p>The importance of data efficiency is particularly evident in foundation models. As these models grow in scale and capability, they are approaching the limits of available high-quality training data, especially for language tasks (<a href="#fig-running-out-of-human-data" class="quarto-xref">Figure&nbsp;<span>9.12</span></a>). This scarcity drives innovation in data processing and curation techniques, pushing the field to develop more sophisticated approaches to data efficiency.</p>
<div id="fig-running-out-of-human-data" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-running-out-of-human-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/running_out_of_data.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;9.12: Datasets for foundation model training are quickly growing in size and capturing most of stock of human-generated text. Source: @Villalobos_Ho_Sevilla_Besiroglu_Heim_Hobbhahn_2024."><img src="images/png/running_out_of_data.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-running-out-of-human-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.12: Datasets for foundation model training are quickly growing in size and capturing most of stock of human-generated text. Source: <span class="citation" data-cites="Villalobos_Ho_Sevilla_Besiroglu_Heim_Hobbhahn_2024">Sevilla et al. (<a href="../references.html#ref-Villalobos_Ho_Sevilla_Besiroglu_Heim_Hobbhahn_2024" role="doc-biblioref">2022b</a>)</span>.
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-Villalobos_Ho_Sevilla_Besiroglu_Heim_Hobbhahn_2024" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî. 2022b. <span>‚ÄúCompute Trends Across Three Eras of Machine Learning.‚Äù</span> In <em>2022 International Joint Conference on Neural Networks (IJCNN)</em>, 1‚Äì8. IEEE. <a href="https://doi.org/10.1109/ijcnn55064.2022.9891914">https://doi.org/10.1109/ijcnn55064.2022.9891914</a>.
</div></div></figure>
</div>
<p>Evidence for the impact of data quality appears across different scales of deployment. In Tiny ML applications, datasets like Wake Vision demonstrate how model performance critically depends on careful data curation <span class="citation" data-cites="banbury2024wakevisiontailoreddataset">(<a href="../references.html#ref-banbury2024wakevisiontailoreddataset" role="doc-biblioref">Banbury et al. 2024</a>)</span>. At larger scales, research on language models trained on web-scale datasets shows that intelligent filtering and selection strategies can significantly improve performance on downstream tasks <span class="citation" data-cites="penedo2024fineweb">(<a href="../references.html#ref-penedo2024fineweb" role="doc-biblioref">Penedo et al. 2024</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-banbury2024wakevisiontailoreddataset" class="csl-entry" role="listitem">
Banbury, Colby, Emil Njor, Andrea Mattia Garavagno, Matthew Stewart, Pete Warden, Manjunath Kudlur, Nat Jeffries, Xenofon Fafoutis, and Vijay Janapa Reddi. 2024. <span>‚ÄúWake Vision: A Tailored Dataset and Benchmark Suite for TinyML Computer Vision Applications,‚Äù</span> May. <a href="http://arxiv.org/abs/2405.00892v4">http://arxiv.org/abs/2405.00892v4</a>.
</div><div id="ref-penedo2024fineweb" class="csl-entry" role="listitem">
Penedo, Guilherme, Hynek Kydlƒ±ÃÅƒçek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. <span>‚ÄúThe FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale.‚Äù</span> <em>arXiv Preprint arXiv:2406.17557</em>, June. <a href="http://arxiv.org/abs/2406.17557v2">http://arxiv.org/abs/2406.17557v2</a>.
</div></div><p>This modern era of data efficiency represents a fundamental shift in how machine learning systems approach data utilization. By focusing on quality over quantity and developing sophisticated techniques for data selection and processing, the field is moving toward more sustainable and effective approaches to model training and deployment.</p>
</section><section id="data-efficiencys-role" class="level4"><h4 class="anchored" data-anchor-id="data-efficiencys-role">Data Efficiency‚Äôs Role</h4>
<p>Data efficiency is integral to the design of scalable and sustainable machine learning systems. By reducing the dependency on large datasets, data efficiency directly impacts both model and compute efficiency. For instance, smaller, higher-quality datasets reduce training times and computational demands, while enabling models to generalize more effectively. This dimension of efficiency is particularly critical for edge applications, where bandwidth and storage limitations make it impractical to rely on large datasets.</p>
<p>As the field advances, data efficiency will play an increasingly prominent role in addressing the challenges of scalability, accessibility, and sustainability. By rethinking how data is collected, processed, and utilized, machine learning systems can achieve higher levels of efficiency across the entire pipeline.</p>
</section></section></section><section id="system-efficiency" class="level2 page-columns page-full" data-number="9.4"><h2 data-number="9.4" class="anchored" data-anchor-id="system-efficiency">
<span class="header-section-number">9.4</span> System Efficiency</h2>
<p>The efficiency of machine learning systems has become a crucial area of focus. Optimizing these systems helps us ensure that they are not only high-performing but also adaptable, cost-effective, and environmentally sustainable. Understanding the concept of ML system efficiency, its key dimensions, and the interplay between them is essential for uncovering how these principles can drive impactful, scalable, and responsible AI solutions.</p>
<section id="defining-system-efficiency" class="level3 page-columns page-full" data-number="9.4.1"><h3 data-number="9.4.1" class="anchored" data-anchor-id="defining-system-efficiency">
<span class="header-section-number">9.4.1</span> Defining System Efficiency</h3>
<p>Machine learning is a highly complex field, involving a multitude of components across a vast domain. Despite its complexity, there has not been a synthesis of what it truly means to have an efficient machine learning system. Here, we take a first step towards defining this concept.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition of Machine Learning System Efficiency">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of Machine Learning System Efficiency
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Machine Learning System Efficiency</strong> refers to the optimization of machine learning systems across three interconnected dimensions‚Äî<em>algorithmic efficiency</em>, <em>compute efficiency</em>, and <em>data efficiency</em>. Its goal is to minimize <em>computational, memory, and energy</em> demands while maintaining or improving system performance. This efficiency ensures that machine learning systems are <em>scalable, cost-effective, and sustainable</em>, which allows them to adapt to diverse deployment contexts, ranging from <em>cloud data centers</em> to <em>edge devices</em>. Achieving system efficiency, however, often requires navigating <em>trade-offs</em> between dimensions, such as balancing <em>model complexity</em> with <em>hardware constraints</em> or reducing <em>data dependency</em> without compromising <em>generalization</em>.</p>
</div>
</div>
<p>This definition highlights the holistic nature of efficiency in machine learning systems, emphasizing that the three dimensions‚Äîalgorithmic efficiency, compute efficiency, and data efficiency‚Äîare deeply interconnected. Optimizing one dimension often affects the others, either by creating synergies or necessitating trade-offs. Understanding these interdependencies is essential for designing systems that are not only performant but also scalable, adaptable, and sustainable <span class="citation" data-cites="patterson2021carbon">(<a href="../references.html#ref-patterson2021carbon" role="doc-biblioref">D. A. Patterson and Hennessy 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-patterson2021carbon" class="csl-entry" role="listitem">
Patterson, David A, and John L Hennessy. 2021. <span>‚ÄúCarbon Emissions and Large Neural Network Optimization.‚Äù</span> <em>Communications of the ACM</em> 64 (7): 54‚Äì61.
</div></div><p>To better understand this interplay, we must examine how these dimensions reinforce one another and the challenges in balancing them. While each dimension contributes uniquely, the true complexity lies in their interdependencies. Historically, optimizations were often approached in isolation. However, recent years have seen a shift towards co-design, where multiple dimensions are optimized concurrently to achieve superior overall efficiency.</p>
</section><section id="efficiency-interdependencies" class="level3" data-number="9.4.2"><h3 data-number="9.4.2" class="anchored" data-anchor-id="efficiency-interdependencies">
<span class="header-section-number">9.4.2</span> Efficiency Interdependencies</h3>
<p>The efficiency of machine learning systems is inherently a multifaceted challenge that encompasses model design, computational resources, and data utilization. These dimensions‚Äîalgorithmic efficiency, compute efficiency, and data efficiency‚Äîare deeply interdependent, forming a dynamic ecosystem where improvements in one area often ripple across the others. Understanding these interdependencies is crucial for building scalable, cost-effective, and high-performing systems that can adapt to diverse application demands.</p>
<p>This interplay is best captured through a conceptual visualization. <a href="#fig-interdependece" class="quarto-xref">Figure&nbsp;<span>9.13</span></a> illustrates how these efficiency dimensions overlap and interact with each other in a simple Venn diagram. Each circle represents one of the efficiency dimensions, and their intersections highlight the areas where they influence one another, which we will explore next.</p>
<div id="fig-interdependece" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-interdependece-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="63bdb7b8d0596ef1d0ba591a64031aa42ab119a6.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;9.13: Interdependence of the different efficiency dimensions."><img src="efficient_ai_files/mediabag/63bdb7b8d0596ef1d0ba591a64031aa42ab119a6.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interdependece-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.13: Interdependence of the different efficiency dimensions.
</figcaption></figure>
</div>
<section id="algorithmic-efficiency-aids-compute-and-data" class="level4"><h4 class="anchored" data-anchor-id="algorithmic-efficiency-aids-compute-and-data">Algorithmic Efficiency Aids Compute and Data</h4>
<p>Model efficiency is essential for efficient machine learning systems. By designing compact and streamlined models, we can significantly reduce computational demands, leading to faster and more cost-effective inference. These compact models not only consume fewer resources but are also easier to deploy across diverse environments, such as resource-constrained edge devices or energy-intensive cloud infrastructure.</p>
<p>Moreover, efficient models often require less data for training, as they avoid over-parameterization and focus on capturing essential patterns within the data. This results in shorter training times and reduced dependency on massive datasets, which can be expensive and time-consuming to curate. As a result, optimizing algorithmic efficiency creates a ripple effect, enhancing both compute and data efficiency.</p>
<section id="mobile-deployment-example" class="level5"><h5 class="anchored" data-anchor-id="mobile-deployment-example">Mobile Deployment Example</h5>
<p>Mobile devices, such as smartphones, provide an accessible introduction to the interplay of efficiency dimensions. Consider a photo-editing application that uses machine learning to apply real-time filters. Compute efficiency is achieved through hardware accelerators like mobile GPUs or Neural Processing Units (NPUs), ensuring tasks are performed quickly while minimizing battery usage.</p>
<p>This compute efficiency, in turn, is supported by algorithmic efficiency. The application relies on a lightweight neural network architecture, such as MobileNets, that reduces the computational load, allowing it to take full advantage of the mobile device‚Äôs hardware. Streamlined models also help reduce memory consumption, further enhancing computational performance and enabling real-time responsiveness.</p>
<p>Furthermore, data efficiency strengthens both compute and algorithmic efficiency by ensuring the model is trained on carefully curated and augmented datasets. These datasets allow the model to generalize effectively, reducing the need for extensive retraining and lowering the demand for computational resources during training. Additionally, by minimizing the complexity of the training data, the model can remain lightweight without sacrificing accuracy, reinforcing both model and compute efficiency.</p>
<p>Integrating these dimensions means mobile deployments achieve a seamless balance between performance, energy efficiency, and practicality. The interdependence of model, compute, and data efficiencies ensures that even resource-constrained devices can deliver advanced AI capabilities to users on the go.</p>
</section></section><section id="compute-efficiency-supports-model-and-data" class="level4"><h4 class="anchored" data-anchor-id="compute-efficiency-supports-model-and-data">Compute Efficiency Supports Model and Data</h4>
<p>Compute efficiency is a key factor in optimizing machine learning systems. By maximizing hardware utilization and employing efficient algorithms, compute efficiency speeds up both model training and inference processes, ultimately cutting down on the time and resources needed, even when working with complex or large-scale models.</p>
<p>Efficient computation enables models to handle large datasets more effectively, minimizing bottlenecks associated with memory or processing power. Techniques such as parallel processing, hardware accelerators (e.g., GPUs, TPUs), and energy-aware scheduling contribute to reducing overhead while ensuring peak performance. As a result, compute efficiency not only supports model optimization but also enhances data handling, making it feasible to train models on high-quality datasets without unnecessary computational strain.</p>
<section id="edge-deployment-example" class="level5"><h5 class="anchored" data-anchor-id="edge-deployment-example">Edge Deployment Example</h5>
<p>Edge deployments, such as those in autonomous vehicles, highlight the intricate balance required between real-time constraints and energy efficiency. Compute efficiency is central, as vehicles rely on high-performance onboard hardware to process massive streams of sensor data‚Äîsuch as from cameras, LiDAR, and radar‚Äîin real time. These computations must be performed with minimal latency to ensure safe navigation and split-second decision-making.</p>
<p>This compute efficiency is closely supported by algorithmic efficiency, as the system depends on compact, high-accuracy models designed for low latency. By employing streamlined neural network architectures or hybrid models combining deep learning and traditional algorithms, the computational demands on hardware are reduced. These optimized models not only lower the processing load but also consume less energy, reinforcing the system‚Äôs overall energy efficiency.</p>
<p>Data efficiency enhances both compute and algorithmic efficiency by reducing the dependency on vast amounts of training data. Through synthetic and augmented datasets, the model can generalize effectively across diverse scenarios‚Äîsuch as varying lighting, weather, and traffic conditions‚Äîwithout requiring extensive retraining. This targeted approach minimizes computational costs during training and allows the model to remain efficient while adapting to a wide range of real-world environments.</p>
<p>Together, the interdependence of these efficiencies ensures that autonomous vehicles can operate safely and reliably while minimizing energy consumption. This balance not only improves real-time performance but also contributes to broader goals, such as reducing fuel consumption and enhancing environmental sustainability.</p>
</section></section><section id="data-efficiency-strengthens-model-and-compute" class="level4"><h4 class="anchored" data-anchor-id="data-efficiency-strengthens-model-and-compute">Data Efficiency Strengthens Model and Compute</h4>
<p>Data efficiency is fundamental to bolstering both model and compute efficiency. By focusing on high-quality, compact datasets, the training process becomes more streamlined, requiring fewer computational resources to achieve comparable or superior model performance. This targeted approach reduces data redundancy and minimizes the overhead associated with handling excessively large datasets.</p>
<p>Furthermore, data efficiency enables more focused model design. When datasets emphasize relevant features and minimize noise, models can achieve high performance with simpler architectures. Consequently, this reduces computational requirements during both training and inference, allowing more efficient use of computing resources.</p>
<section id="cloud-deployment-example" class="level5"><h5 class="anchored" data-anchor-id="cloud-deployment-example">Cloud Deployment Example</h5>
<p>Cloud deployments exemplify how system efficiency can be achieved across interconnected dimensions. Consider a recommendation system operating in a data center, where high throughput and rapid inference are critical. Compute efficiency is achieved by leveraging parallelized processing on GPUs or TPUs, which optimize the computational workload to ensure timely and resource-efficient performance. This high-performance hardware allows the system to handle millions of simultaneous queries while keeping energy and operational costs in check.</p>
<p>This compute efficiency is bolstered by algorithmic efficiency, as the recommendation system employs streamlined architectures, such as pruned or simplified models. By reducing the computational and memory footprint, these models enable the system to scale efficiently, processing large volumes of data without overwhelming the infrastructure. The streamlined design also reduces the burden on accelerators, improving energy usage and maintaining throughput.</p>
<p>Data efficiency strengthens both compute and algorithmic efficiency by enabling the system to learn and adapt without excessive data overhead. By focusing on actively labeled datasets, the system can prioritize high-value training data, ensuring better model performance with fewer computational resources. This targeted approach reduces the size and complexity of training tasks, freeing up resources for inference and scaling while maintaining high recommendation accuracy.</p>
<p>Together, the interdependence of these efficiencies enables cloud-based systems to achieve a balance of performance, scalability, and cost-effectiveness. By optimizing model, compute, and data dimensions in harmony, cloud deployments become a cornerstone of modern AI applications, supporting millions of users with efficiency and reliability.</p>
</section></section><section id="efficiency-trade-offs" class="level4"><h4 class="anchored" data-anchor-id="efficiency-trade-offs">Efficiency Trade-offs</h4>
<p>In many machine learning applications, efficiency is not merely a goal for optimization but a prerequisite for system feasibility. Extreme resource constraints, such as limited computational power, energy availability, and storage capacity, demand careful trade-offs between algorithmic efficiency, compute efficiency, and data efficiency. These constraints are particularly relevant in scenarios where machine learning models must operate in low-power embedded devices, remote sensors, or battery-operated systems.</p>
<p>Unlike cloud-based or even edge-based deployments, where computational resources are relatively abundant, resource-constrained environments require severe optimizations to ensure that models can function within tight operational limits. Achieving efficiency in such settings often involves trade-offs: smaller models may sacrifice some predictive accuracy, lower precision computations may introduce noise, and constrained datasets may limit generalization. The key challenge is to balance these trade-offs to maintain functionality while staying within strict power and compute budgets.</p>
<section id="tiny-deployment-case-study" class="level5"><h5 class="anchored" data-anchor-id="tiny-deployment-case-study">Tiny Deployment Case Study</h5>
<p>A clear example of these trade-offs can be seen in Tiny ML, where machine learning models are deployed on ultra-low-power microcontrollers, often operating on milliwatts of power. Consider an IoT-based environmental monitoring system designed to detect temperature anomalies in remote agricultural fields. The device must process sensor data locally while operating on a small battery for months or even years without requiring recharging or maintenance.</p>
<p>In this setting, compute efficiency is critical, as the microcontroller has extremely limited processing capabilities, meaning the model must perform inference with minimal computational overhead. Algorithmic efficiency plays a central role, as the model must be compact enough to fit within the tiny memory available on the device, requiring streamlined architectures that eliminate unnecessary complexity. Data efficiency becomes essential, since collecting and storing large datasets in a remote location is impractical, requiring the model to learn effectively from small, carefully selected datasets to make reliable predictions with minimal training data.</p>
<p>Because of these constraints, Tiny ML deployments require a holistic approach to efficiency, where improvements in one area must compensate for limitations in another. A model that is computationally lightweight but requires excessive amounts of training data may not be viable. Similarly, a highly accurate model that demands too much energy will drain the battery too quickly. The success of Tiny ML hinges on balancing these interdependencies, ensuring that machine learning remains practical even in environments with severe resource constraints.</p>
</section></section><section id="progression-and-takeaways" class="level4"><h4 class="anchored" data-anchor-id="progression-and-takeaways">Progression and Takeaways</h4>
<p>Starting with Mobile ML deployments and progressing to Edge ML, Cloud ML, and Tiny ML, these examples illustrate how system efficiency adapts to diverse operational contexts. Mobile ML emphasizes battery life and hardware limitations, edge systems balance real-time demands with energy efficiency, cloud systems prioritize scalability and throughput, and Tiny ML demonstrates how AI can thrive in environments with severe resource constraints.</p>
<p>Despite these differences, the fundamental principles remain consistent: achieving system efficiency requires optimizing model, compute, and data dimensions. These dimensions are deeply interconnected, with improvements in one often reinforcing the others. For instance, lightweight models enhance computational performance and reduce data requirements, while efficient hardware accelerates model training and inference. Similarly, focused datasets streamline model training and reduce computational overhead.</p>
<p>By understanding the interplay between these dimensions, we can design machine learning systems that meet specific deployment requirements while maintaining flexibility across contexts. For instance, a model architected for edge deployment can often be adapted for cloud scaling or simplified for mobile use, provided we carefully consider the relationships between model architecture, computational resources, and data requirements.</p>
</section></section><section id="scalability-and-sustainability" class="level3" data-number="9.4.3"><h3 data-number="9.4.3" class="anchored" data-anchor-id="scalability-and-sustainability">
<span class="header-section-number">9.4.3</span> Scalability and Sustainability</h3>
<p>System efficiency serves as a fundamental driver of environmental sustainability in machine learning systems. When systems are optimized for efficiency, they can be deployed at scale while minimizing their environmental footprint. This relationship creates a positive feedback loop, as sustainable design practices naturally encourage further efficiency improvements.</p>
<p>The interconnection between efficiency, scalability, and sustainability forms a virtuous cycle, as shown in <a href="#fig-virtuous-cycle" class="quarto-xref">Figure&nbsp;<span>9.14</span></a>, that enhances the broader impact of machine learning systems. Efficient system design enables widespread deployment, which amplifies the positive environmental effects of sustainable practices. As organizations prioritize sustainability, they drive innovation in efficient system design, ensuring that advances in artificial intelligence align with global sustainability goals.</p>
<div id="fig-virtuous-cycle" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-virtuous-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="156db28d95c8492bd98355f06f933c4f2ae09992.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;9.14: The virtuous cycle of machine learning system. Efficiency drives scalability and widespread adoption, which in turn drives the need for sustainable solutions, fueling the need for further efficiency."><img src="efficient_ai_files/mediabag/156db28d95c8492bd98355f06f933c4f2ae09992.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-virtuous-cycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.14: The virtuous cycle of machine learning system. Efficiency drives scalability and widespread adoption, which in turn drives the need for sustainable solutions, fueling the need for further efficiency.
</figcaption></figure>
</div>
<section id="efficiency-scalability-relationship" class="level4"><h4 class="anchored" data-anchor-id="efficiency-scalability-relationship">Efficiency-Scalability Relationship</h4>
<p>Efficient systems are inherently scalable. Reducing resource demands through lightweight models, targeted datasets, and optimized compute utilization allows systems to deploy broadly across diverse environments. For example, a speech recognition model that is efficient enough to run on mobile devices can serve millions of users globally without relying on costly infrastructure upgrades. Similarly, Tiny ML technologies, designed to operate on low-power hardware, make it possible to deploy thousands of devices in remote areas for applications like environmental monitoring or precision agriculture.</p>
<p>Scalability becomes feasible because efficiency reduces barriers to entry. Systems that are compact and energy-efficient require less infrastructure, making them more adaptable to different deployment contexts, from cloud data centers to edge and IoT devices. This adaptability is key to ensuring that advanced AI solutions reach users worldwide, fostering inclusion and innovation.</p>
</section><section id="scalability-sustainability-relationship" class="level4"><h4 class="anchored" data-anchor-id="scalability-sustainability-relationship">Scalability-Sustainability Relationship</h4>
<p>When efficient systems scale, they amplify their contribution to sustainability. Energy-efficient designs deployed at scale reduce overall energy consumption and computational waste, mitigating the environmental impact of machine learning systems. For instance, deploying Tiny ML devices for on-device data processing avoids the energy costs of transmitting raw data to the cloud, while efficient recommendation engines in the cloud reduce the operational footprint of serving millions of users.</p>
<p>The wide-scale adoption of efficient systems not only reduces environmental costs but also fosters sustainable development in underserved regions. Efficient AI applications in healthcare, education, and agriculture can provide transformative benefits without imposing significant resource demands, aligning technological growth with ethical and environmental goals.</p>
</section><section id="sustainability-efficiency-relationship" class="level4"><h4 class="anchored" data-anchor-id="sustainability-efficiency-relationship">Sustainability-Efficiency Relationship</h4>
<p>Sustainability itself reinforces the need for efficiency, creating a feedback loop that strengthens the entire system. Practices like minimizing data redundancy, designing energy-efficient hardware, and developing low-power models all emphasize efficient resource utilization. These efforts not only reduce the environmental footprint of AI systems but also set the stage for further scalability by making systems cost-effective and accessible.</p>
</section></section></section><section id="efficiency-trade-offs-and-challenges" class="level2" data-number="9.5"><h2 data-number="9.5" class="anchored" data-anchor-id="efficiency-trade-offs-and-challenges">
<span class="header-section-number">9.5</span> Efficiency Trade-offs and Challenges</h2>
<p>Thus far, we explored how the dimensions of system efficiency‚Äîalgorithmic efficiency, compute efficiency, and data efficiency‚Äîare deeply interconnected. Ideally, these dimensions reinforce one another, creating a system that is both efficient and high-performing. Compact models reduce computational demands, efficient hardware accelerates processes, and high-quality datasets streamline training and inference. However, achieving this harmony is far from straightforward.</p>
<section id="trade-offs-source" class="level3" data-number="9.5.1"><h3 data-number="9.5.1" class="anchored" data-anchor-id="trade-offs-source">
<span class="header-section-number">9.5.1</span> Trade-offs Source</h3>
<p>In practice, balancing these dimensions often uncovers underlying tensions. Improvements in one area can impose constraints on others, highlighting the interconnected nature of machine learning systems. For instance, simplifying a model to reduce computational demands might result in reduced accuracy, while optimizing compute efficiency for real-time responsiveness can conflict with energy efficiency goals. These trade-offs are not limitations but reflections of the intricate design decisions required to build adaptable and efficient systems.</p>
<p>Understanding the root of these trade-offs is essential for navigating the challenges of system design. Each efficiency dimension influences the others, creating a dynamic interplay that shapes system performance. The following sections delve into these interdependencies, beginning with the relationship between algorithmic efficiency and compute requirements.</p>
<section id="efficiency-and-compute-requirements" class="level4"><h4 class="anchored" data-anchor-id="efficiency-and-compute-requirements">Efficiency and Compute Requirements</h4>
<p>Model efficiency focuses on designing compact and streamlined models that minimize computational and memory demands. By reducing the size or complexity of a model, it becomes easier to deploy on devices with limited resources, such as mobile phones or IoT sensors.</p>
<p>However, overly simplifying a model can reduce its accuracy, especially for complex tasks. To make up for this loss, additional computational resources may be required during training to fine-tune the model or during deployment to apply more sophisticated inference algorithms. Thus, while algorithmic efficiency can reduce computational costs, achieving this often places additional strain on compute efficiency.</p>
</section><section id="efficiency-and-real-time-needs" class="level4"><h4 class="anchored" data-anchor-id="efficiency-and-real-time-needs">Efficiency and Real-Time Needs</h4>
<p>Compute efficiency aims to minimize the resources required for tasks like training and inference, reducing energy consumption, processing time, and memory use. In many applications, particularly in cloud computing or data centers, this optimization works seamlessly with algorithmic efficiency to improve system performance.</p>
<p>However, in scenarios that require real-time responsiveness‚Äîsuch as autonomous vehicles or augmented reality‚Äîcompute efficiency is harder to maintain. Real-time systems often require high-performance hardware to process large amounts of data instantly, which can conflict with energy efficiency goals or increase system costs. Balancing compute efficiency with stringent real-time application needs becomes a key challenge in such applications.</p>
</section><section id="efficiency-and-model-generalization" class="level4"><h4 class="anchored" data-anchor-id="efficiency-and-model-generalization">Efficiency and Model Generalization</h4>
<p>Data efficiency seeks to minimize the amount of data required to train a model without sacrificing performance. By curating smaller, high-quality datasets, the training process becomes faster and less resource-intensive. Ideally, this reinforces both model and compute efficiency, as smaller datasets reduce the computational load and support more compact models.</p>
<p>However, reducing the size of a dataset can also limit its diversity, making it harder for the model to generalize to unseen scenarios. To address this, additional compute resources or model complexity may be required, creating a tension between data efficiency and the broader goals of system efficiency.</p>
</section><section id="summary" class="level4"><h4 class="anchored" data-anchor-id="summary">Summary</h4>
<p>The interdependencies between model, compute, and data efficiency are the foundation of a well-designed machine learning system. While these dimensions can reinforce one another, building a system that achieves this synergy often requires navigating difficult trade-offs. These trade-offs highlight the complexity of designing machine learning systems that balance performance, scalability, and resource constraints.</p>
</section></section><section id="common-trade-offs" class="level3" data-number="9.5.2"><h3 data-number="9.5.2" class="anchored" data-anchor-id="common-trade-offs">
<span class="header-section-number">9.5.2</span> Common Trade-offs</h3>
<p>In machine learning system design, trade-offs are an inherent reality. As we explored in the previous section, the interdependencies between algorithmic efficiency, compute efficiency, and data efficiency ideally work together to create powerful, resource-conscious systems. However, achieving this harmony is far from straightforward. In practice, improvements in one dimension often come at the expense of another. Designers must carefully weigh these trade-offs to achieve a balance that aligns with the system‚Äôs goals and deployment context.</p>
<p>This balancing act is especially challenging because trade-offs are rarely one-dimensional. Decisions made in one area often have cascading effects on the rest of the system. For instance, choosing a larger, more complex model may improve accuracy, but it also increases computational demands and the size of the training dataset required. Similarly, reducing energy consumption may limit the ability to meet real-time performance requirements, particularly in latency-sensitive applications.</p>
<p>We explore three of the most common trade-offs encountered in machine learning system design:</p>
<ol type="1">
<li><p><strong>Model complexity vs.&nbsp;compute resources</strong>,</p></li>
<li><p><strong>Energy efficiency vs.&nbsp;real-time performance</strong>, and</p></li>
<li><p><strong>Data size vs.&nbsp;model generalization</strong>.</p></li>
</ol>
<p>Each of these trade-offs illustrates the nuanced decisions that system designers must make and the challenges involved in achieving efficient, high-performing systems.</p>
<section id="complexity-vs.-resources" class="level4"><h4 class="anchored" data-anchor-id="complexity-vs.-resources">Complexity vs.&nbsp;Resources</h4>
<p>The relationship between model complexity and compute resources is one of the most fundamental trade-offs in machine learning system design. Complex models, such as deep neural networks with millions or even billions of parameters, are often capable of achieving higher accuracy by capturing intricate patterns in data. However, this complexity comes at a cost. These models require significant computational power and memory to train and deploy, often making them impractical for environments with limited resources.</p>
<p>For example, consider a recommendation system deployed in a cloud data center. A highly complex model may deliver better recommendations, but it increases the computational demands on servers, leading to higher energy consumption and operating costs. On the other hand, a simplified model may reduce these demands but might compromise the quality of recommendations, especially when handling diverse or unpredictable user behavior.</p>
<p>The trade-off becomes even more pronounced in resource-constrained environments such as mobile or edge devices. A compact, streamlined model designed for a smartphone or an autonomous vehicle may operate efficiently within the device‚Äôs hardware limits but might require more sophisticated data preprocessing or training procedures to compensate for its reduced capacity. This balancing act highlights the interconnected nature of efficiency dimensions, where gains in one area often demand sacrifices in another.</p>
</section><section id="energy-vs.-performance" class="level4"><h4 class="anchored" data-anchor-id="energy-vs.-performance">Energy vs.&nbsp;Performance</h4>
<p>Energy efficiency and real-time performance often pull machine learning systems in opposite directions, particularly in applications requiring low-latency responses. Real-time systems, such as those in autonomous vehicles or augmented reality applications, rely on high-performance hardware to process large volumes of data quickly. This ensures responsiveness and safety in scenarios where even small delays can lead to significant consequences. However, achieving such performance typically increases energy consumption, creating tension with the goal of minimizing resource use.</p>
<p>For instance, an autonomous vehicle must process sensor data from cameras, LiDAR, and radar in real time to make navigation decisions. The computational demands of these tasks often require specialized accelerators, such as GPUs, which can consume significant energy. While optimizing hardware utilization and model architecture can improve energy efficiency to some extent, the demands of real-time responsiveness make it challenging to achieve both goals simultaneously.</p>
<p>In edge deployments, where devices rely on battery power or limited energy sources, this trade-off becomes even more critical. Striking a balance between energy efficiency and real-time performance often involves prioritizing one over the other, depending on the application‚Äôs requirements. This trade-off underscores the importance of context-specific design, where the constraints and priorities of the deployment environment dictate the balance between competing objectives.</p>
</section><section id="data-size-vs.-generalization" class="level4"><h4 class="anchored" data-anchor-id="data-size-vs.-generalization">Data Size vs.&nbsp;Generalization</h4>
<p>The size and quality of the dataset used to train a machine learning model play a role in its ability to generalize to new, unseen data. Larger datasets generally provide greater diversity and coverage, enabling models to capture subtle patterns and reduce the risk of overfitting. However, the computational and memory demands of training on large datasets can be substantial, leading to trade-offs between data efficiency and computational requirements.</p>
<p>In resource-constrained environments such as Tiny ML deployments, the challenge of dataset size is particularly evident. For example, an IoT device monitoring environmental conditions might need a model that generalizes well to varying temperatures, humidity levels, or geographic regions. Collecting and processing extensive datasets to capture these variations may be impractical due to storage, computational, and energy limitations. In such cases, smaller, carefully curated datasets or synthetic data generated to mimic real-world conditions are used to reduce computational strain. However, this reduction often risks missing key edge cases, which could degrade the model‚Äôs performance in diverse environments.</p>
<p>Conversely, in cloud-based systems, where compute resources are more abundant, training on massive datasets can still pose challenges. Managing data redundancy, ensuring high-quality labeling, and handling the time and cost associated with large-scale data pipelines often require significant computational infrastructure. This trade-off highlights how the need to balance dataset size and model generalization depends heavily on the deployment context and available resources.</p>
</section><section id="summary-1" class="level4"><h4 class="anchored" data-anchor-id="summary-1">Summary</h4>
<p>The interplay between model complexity, compute resources, energy efficiency, real-time performance, and dataset size illustrates the inherent trade-offs in machine learning system design. These trade-offs are rarely one-dimensional; decisions to optimize one aspect of a system often ripple through the others, requiring careful consideration of the specific goals and constraints of the application.</p>
<p>Designers must weigh the advantages and limitations of each trade-off in the context of the deployment environment. For instance, a cloud-based system might prioritize scalability and throughput over energy efficiency, while an edge system must balance real-time performance with strict power constraints. Similarly, resource-limited Tiny ML deployments require exceptional data and algorithmic efficiency to operate within severe hardware restrictions.</p>
<p>By understanding these common trade-offs, we can begin to identify strategies for navigating them effectively. The next section will explore practical approaches to managing these tensions, focusing on techniques and design principles that enable system efficiency while addressing the complexities of real-world applications.</p>
</section></section></section><section id="managing-trade-offs" class="level2 page-columns page-full" data-number="9.6"><h2 data-number="9.6" class="anchored" data-anchor-id="managing-trade-offs">
<span class="header-section-number">9.6</span> Managing Trade-offs</h2>
<p>The trade-offs inherent in machine learning system design require thoughtful strategies to navigate effectively. While the interdependencies between algorithmic efficiency, compute efficiency, and data efficiency create opportunities for synergy, achieving this balance often involves difficult decisions. The specific goals and constraints of the deployment environment heavily influence how these trade-offs are addressed. For example, a system designed for cloud deployment may prioritize scalability and throughput, while a Tiny ML system must focus on extreme resource efficiency.</p>
<p>To manage these challenges, designers can adopt a range of strategies that address the unique requirements of different contexts. By prioritizing efficiency dimensions based on the application, collaborating across system components, and leveraging automated optimization tools, it is possible to create systems that balance performance, cost, and resource use. This section explores these approaches and provides guidance for designing systems that are both efficient and adaptable.</p>
<section id="contextual-prioritization" class="level3" data-number="9.6.1"><h3 data-number="9.6.1" class="anchored" data-anchor-id="contextual-prioritization">
<span class="header-section-number">9.6.1</span> Contextual Prioritization</h3>
<p>Efficiency goals are rarely universal. The specific demands of an application or deployment scenario heavily influence which dimension of efficiency‚Äîmodel, compute, or data‚Äîtakes precedence. Designing an efficient system requires a deep understanding of the operating environment and the constraints it imposes. Prioritizing the right dimensions based on context is the first step in effectively managing trade-offs.</p>
<p>For instance, in Mobile ML deployments, battery life is often the primary constraint. This places a premium on compute efficiency, as energy consumption must be minimized to preserve the device‚Äôs operational time. As a result, lightweight models are prioritized, even if it means sacrificing some accuracy or requiring additional data preprocessing. The focus is on balancing acceptable performance with energy-efficient operation.</p>
<p>In contrast, Cloud ML-based systems prioritize scalability and throughput. These systems must process large volumes of data and serve millions of users simultaneously. While compute resources in cloud environments are more abundant, energy efficiency and operational costs still remain important considerations. Here, algorithmic efficiency plays a critical role in ensuring that the system can scale without overwhelming the underlying infrastructure.</p>
<p>Edge ML systems present an entirely different set of priorities. Autonomous vehicles or real-time monitoring systems require low-latency processing to ensure safe and reliable operation. This makes real-time performance and compute efficiency paramount, often at the expense of energy consumption. However, the hardware constraints of edge devices mean that these systems must still carefully manage energy and computational resources to remain viable.</p>
<p>Finally, Tiny ML deployments demand extreme levels of efficiency due to the severe limitations of hardware and energy availability. For these systems, model and data efficiency are the top priorities. Models must be highly compact and capable of operating on microcontrollers with minimal memory and compute power. At the same time, the training process must rely on small, carefully curated datasets to ensure the model generalizes well without requiring extensive resources.</p>
<p>In each of these contexts, prioritizing the right dimensions of efficiency ensures that the system meets its functional and resource requirements. Recognizing the unique demands of each deployment scenario allows designers to navigate trade-offs effectively and tailor solutions to specific needs.</p>
</section><section id="test-time-compute" class="level3" data-number="9.6.2"><h3 data-number="9.6.2" class="anchored" data-anchor-id="test-time-compute">
<span class="header-section-number">9.6.2</span> Test-Time Compute</h3>
<p>We can further enhance system adaptability through dynamic resource allocation during inference, a concept often referred to as ‚ÄúTest-Time Compute.‚Äù This approach recognizes that resource needs may fluctuate even within a specific deployment context. By adjusting the computational effort expended at inference time, systems can fine-tune their performance to meet immediate demands.</p>
<p>For example, in a cloud-based video analysis system, standard video streams might be processed with a streamlined, low-compute model to maintain high throughput. However, when a critical event is detected, the system could dynamically allocate more computational resources to a more complex model, enabling higher precision analysis of the event. This flexibility allows for a trade-off between latency and accuracy on demand.</p>
<p>Similarly, in mobile applications, a voice assistant might use a lightweight model for routine commands, conserving battery life. But when faced with a complex query, the system could temporarily activate a more resource-intensive model for improved accuracy. This ability to adjust compute based on the complexity of the task or the importance of the result is a powerful tool for optimizing system performance in real-time.</p>
<p>However, implementing ‚ÄúTest-Time Compute‚Äù introduces new challenges. Dynamic resource allocation requires sophisticated monitoring and control mechanisms to ensure that the system remains stable and responsive. Additionally, there is a point of diminishing returns; increasing compute beyond a certain threshold may not yield significant performance improvements, making it crucial to strike a balance between resource usage and desired outcomes. Furthermore, the ability to dynamically increase compute can create disparities in access to high-performance AI, raising equity concerns about who benefits from advanced AI capabilities.</p>
<p>Despite these challenges, ‚ÄúTest-Time Compute‚Äù offers a valuable strategy for enhancing system adaptability and optimizing performance in dynamic environments. It complements the contextual prioritization approach by enabling systems to respond effectively to varying demands within specific deployment scenarios.</p>
</section><section id="co-design" class="level3 page-columns page-full" data-number="9.6.3"><h3 data-number="9.6.3" class="anchored" data-anchor-id="co-design">
<span class="header-section-number">9.6.3</span> Co-Design</h3>
<p>Efficient machine learning systems are rarely the product of isolated optimizations. Achieving balance across model, compute, and data efficiency requires an end-to-end perspective, where each component of the system is designed in tandem with the others. This holistic approach, often referred to as co-design, involves aligning model architectures, hardware platforms, and data pipelines to work seamlessly together.</p>
<p>One of the key benefits of co-design is its ability to mitigate trade-offs by tailoring each component to the specific requirements of the system. For instance, consider a speech recognition system deployed on a mobile device. The model must be compact enough to fit within the device‚Äôs tiny ML memory constraints while still delivering real-time performance. By designing the model architecture to leverage the capabilities of hardware accelerators, such as NPUs, it becomes possible to achieve low-latency inference without excessive energy consumption. Similarly, careful preprocessing and augmentation of the training data can ensure robust performance, even with a smaller, streamlined model.</p>
<p>Co-design becomes essential in resource-constrained environments like Edge ML and Tiny ML deployments. Models must align precisely with hardware capabilities. For example, 8-bit models<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> require hardware support for efficient integer operations, while pruned models benefit from sparse tensor operations. Similarly, edge accelerators often optimize specific operations like convolutions or matrix multiplication, influencing model architecture choices. This creates a tight coupling between hardware and model design decisions.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;<strong>8-bit models</strong>: ML models use 8-bit integer representations for weights and activations instead of the standard 32-bit floating-point format, reducing memory usage and computational requirements for faster, more energy-efficient inference on compatible hardware.</p></div></div><p>This approach extends beyond the interaction of models and hardware. Data pipelines, too, play a central role in co-design. For example, in applications requiring real-time adaptation, such as personalized recommendation systems, the data pipeline must deliver high-quality, timely information that minimizes computational overhead while maximizing model effectiveness. By integrating data management into the design process, it becomes possible to reduce redundancy, streamline training, and support efficient deployment.</p>
<p>End-to-end co-design ensures that the trade-offs inherent in machine learning systems are addressed holistically. By designing each component with the others in mind, it becomes possible to balance competing priorities and create systems that are not only efficient but also robust and adaptable.</p>
</section><section id="automation" class="level3 page-columns page-full" data-number="9.6.4"><h3 data-number="9.6.4" class="anchored" data-anchor-id="automation">
<span class="header-section-number">9.6.4</span> Automation</h3>
<p>Navigating the trade-offs between model, compute, and data efficiency is a complex task that often involves numerous iterations and expert judgment. Automation and optimization tools have emerged as powerful solutions for managing these challenges, streamlining the process of balancing efficiency dimensions while reducing the time and expertise required.</p>
<p>One widely used approach is automated machine learning (AutoML), which enables the exploration of different model architectures, hyperparameter configurations, and feature engineering techniques. By automating these aspects of the design process, AutoML can identify models that achieve an optimal balance between performance and efficiency. For instance, an AutoML pipeline might search for a lightweight model architecture that delivers high accuracy while fitting within the resource constraints of an edge device <span class="citation" data-cites="hutter2019automated">(<a href="../references.html#ref-hutter2019automated" role="doc-biblioref">Hutter, Kotthoff, and Vanschoren 2019</a>)</span>. This approach reduces the need for manual trial-and-error, making optimization faster and more accessible.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hutter2019automated" class="csl-entry" role="listitem">
Hutter, Frank, Lars Kotthoff, and Joaquin Vanschoren. 2019. <em>Automated Machine Learning: Methods, Systems, Challenges</em>. <em>Automated Machine Learning</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-05318-5">https://doi.org/10.1007/978-3-030-05318-5</a>.
</div><div id="ref-elsken2019neural" class="csl-entry" role="listitem">
Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. 2019. <span>‚ÄúNeural Architecture Search.‚Äù</span> In <em>Automated Machine Learning</em>, 20:63‚Äì77. 55. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-05318-5%5C_3">https://doi.org/10.1007/978-3-030-05318-5\_3</a>.
</div></div><p>Neural architecture search (NAS) takes automation a step further by designing model architectures tailored to specific hardware or deployment scenarios. NAS algorithms evaluate a wide range of architectural possibilities, selecting those that maximize performance while minimizing computational demands. For example, NAS can design models that leverage quantization or sparsity techniques, ensuring compatibility with energy-efficient accelerators like TPUs or microcontrollers <span class="citation" data-cites="elsken2019neural">(<a href="../references.html#ref-elsken2019neural" role="doc-biblioref">Elsken, Metzen, and Hutter 2019</a>)</span>. This automated co-design of models and hardware helps mitigate trade-offs by aligning efficiency goals across dimensions.</p>
<p>Data efficiency, too, benefits from automation. Tools that automate dataset curation, augmentation, and active learning reduce the size of training datasets without sacrificing model performance. These tools prioritize high-value data points, ensuring that models are trained on the most informative examples. This not only speeds up training but also reduces computational overhead, reinforcing both compute and algorithmic efficiency <span class="citation" data-cites="settles2009active">(<a href="../references.html#ref-settles2009active" role="doc-biblioref">Settles 2012b</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-settles2009active" class="csl-entry" role="listitem">
‚Äî‚Äî‚Äî. 2012b. <em>Active Learning</em>. <em>University of Wisconsin-Madison Department of Computer Sciences</em>. Vol. 1648. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01560-1">https://doi.org/10.1007/978-3-031-01560-1</a>.
</div></div><p>While automation tools are not a panacea, they play a critical role in addressing the complexity of trade-offs. By leveraging these tools, system designers can achieve efficient solutions more quickly and at lower cost, freeing them to focus on broader design challenges and deployment considerations.</p>
</section><section id="summary-2" class="level3" data-number="9.6.5"><h3 data-number="9.6.5" class="anchored" data-anchor-id="summary-2">
<span class="header-section-number">9.6.5</span> Summary</h3>
<p>Designing efficient machine learning systems requires a deliberate approach to managing trade-offs between model, compute, and data efficiency. These trade-offs are influenced by the context of the deployment, the constraints of the hardware, and the goals of the application. By prioritizing efficiency dimensions based on the specific needs of the system, embracing end-to-end co-design, and leveraging automation tools, it becomes possible to navigate these challenges effectively.</p>
<p>The strategies explored illustrate how thoughtful design can transform trade-offs into opportunities for synergy. For example, aligning model architectures with hardware capabilities can mitigate energy constraints, while automation tools like AutoML and NAS streamline the process of optimizing efficiency dimensions. These approaches underscore the importance of treating system efficiency as a holistic endeavor, where components are designed to complement and reinforce one another.</p>
</section></section><section id="efficiency-first-mindset" class="level2" data-number="9.7"><h2 data-number="9.7" class="anchored" data-anchor-id="efficiency-first-mindset">
<span class="header-section-number">9.7</span> Efficiency-First Mindset</h2>
<p>Designing an efficient machine learning system requires a holistic approach. While it is tempting to focus on optimizing individual components, such as the model architecture or the hardware platform, true efficiency emerges when the entire system is considered as a whole. This end-to-end perspective ensures that trade-offs are balanced across all stages of the machine learning pipeline, from data collection to deployment.</p>
<p>Efficiency is not a static goal but a dynamic process shaped by the context of the application. A system designed for a cloud data center will prioritize scalability and throughput, while an edge deployment will focus on low latency and energy conservation. These differing priorities influence decisions at every step of the design process, requiring careful alignment of the model, compute resources, and data strategy.</p>
<p>An end-to-end perspective can transform system design, enabling machine learning practitioners to build systems that effectively balance trade-offs. Through case studies and examples, we will highlight how efficient systems are designed to meet the unique challenges of their deployment environments, whether in the cloud, on mobile devices, or in resource-constrained Tiny ML applications.</p>
<section id="end-to-end-perspective" class="level3" data-number="9.7.1"><h3 data-number="9.7.1" class="anchored" data-anchor-id="end-to-end-perspective">
<span class="header-section-number">9.7.1</span> End-to-End Perspective</h3>
<p>Efficiency in machine learning systems is achieved not through isolated optimizations but by considering the entire pipeline as a unified whole. Each stage‚Äîdata collection, model training, hardware deployment, and inference‚Äîcontributes to the overall efficiency of the system. Decisions made at one stage can ripple through the rest, influencing performance, resource use, and scalability.</p>
<p>For example, data collection and preprocessing are often the starting points of the pipeline. The quality and diversity of the data directly impact model performance and efficiency. Curating smaller, high-quality datasets can reduce computational costs during training while simplifying the model‚Äôs design. However, insufficient data diversity may affect generalization, necessitating compensatory measures in model architecture or training procedures. By aligning the data strategy with the model and deployment context, designers can avoid inefficiencies downstream.</p>
<p>Model training is another critical stage. The choice of architecture, optimization techniques, and hyperparameters must consider the constraints of the deployment hardware. A model designed for high-performance cloud systems may emphasize accuracy and scalability, leveraging large datasets and compute resources. Conversely, a model intended for edge devices must balance accuracy with size and energy efficiency, often requiring compact architectures and quantization techniques tailored to specific hardware.</p>
<p>Deployment and inference demand precise hardware alignment. Each platform offers distinct capabilities. GPUs excel at parallel matrix operations, TPUs optimize specific neural network computations, and microcontrollers provide energy-efficient scalar processing. For example, a smartphone speech recognition system might leverage an NPU‚Äôs dedicated convolution units for 5-millisecond inference times at 1-watt power draw, while an autonomous vehicle‚Äôs FPGA-based accelerator processes multiple sensor streams with 50-microsecond latency. This hardware-software integration determines real-world efficiency.</p>
<p>An end-to-end perspective ensures that trade-offs are addressed holistically, rather than shifting inefficiencies from one stage of the pipeline to another. By treating the system as an integrated whole, machine learning practitioners can design solutions that are not only efficient but also robust and scalable across diverse deployment scenarios.</p>
</section><section id="scenarios" class="level3" data-number="9.7.2"><h3 data-number="9.7.2" class="anchored" data-anchor-id="scenarios">
<span class="header-section-number">9.7.2</span> Scenarios</h3>
<p>The efficiency needs of machine learning systems differ significantly depending on the lifecycle stage and deployment environment. From research prototypes to production systems, and from high-performance cloud applications to resource-constrained edge deployments, each scenario presents unique challenges and trade-offs. Understanding these differences is crucial for designing systems that meet their operational requirements effectively.</p>
<section id="prototypes-vs.-production" class="level4"><h4 class="anchored" data-anchor-id="prototypes-vs.-production">Prototypes vs.&nbsp;Production</h4>
<p>In the research phase, the primary focus is often on model performance, with efficiency taking a secondary role. Prototypes are typically trained and tested using abundant compute resources, allowing researchers to experiment with large architectures, extensive hyperparameter tuning, and diverse datasets. While this approach enables the exploration of cutting-edge techniques, the resulting systems are often too resource-intensive for real-world use.</p>
<p>In contrast, production systems must prioritize efficiency to operate within practical constraints. Deployment environments‚Äîwhether cloud data centers, mobile devices, or IoT sensors‚Äîimpose strict limitations on compute power, memory, and energy consumption. Transitioning from a research prototype to a production-ready system often involves significant optimization, such as model pruning, quantization, or retraining on targeted datasets. This shift highlights the need to balance performance and efficiency as systems move from concept to deployment.</p>
</section><section id="cloud-apps-vs.-constrained-systems" class="level4"><h4 class="anchored" data-anchor-id="cloud-apps-vs.-constrained-systems">Cloud Apps vs.&nbsp;Constrained Systems</h4>
<p>Cloud-based systems, such as those used for large-scale analytics or recommendation engines, are designed to handle massive workloads. Scalability is the primary concern, requiring models and infrastructure that can support millions of users simultaneously. While compute resources are relatively abundant in cloud environments, energy efficiency and operational costs remain critical considerations. Techniques such as model compression and hardware-specific optimizations help manage these trade-offs, ensuring the system scales efficiently.</p>
<p>In contrast, edge and mobile systems operate under far stricter constraints. Real-time performance, energy efficiency, and hardware limitations are often the dominant concerns. For example, a speech recognition application on a smartphone must balance model size and latency to provide a seamless user experience without draining the device‚Äôs battery. Similarly, an IoT sensor deployed in a remote location must operate for months on limited power, requiring an ultra-efficient model and compute pipeline. These scenarios demand solutions that prioritize efficiency over raw performance.</p>
</section><section id="frequent-retraining-vs.-stability" class="level4"><h4 class="anchored" data-anchor-id="frequent-retraining-vs.-stability">Frequent Retraining vs.&nbsp;Stability</h4>
<p>Some systems, such as recommendation engines or fraud detection platforms, require frequent retraining to remain effective in dynamic environments. These systems depend heavily on data efficiency, using actively labeled datasets and sampling strategies to minimize retraining costs. Compute efficiency also plays a role, as scalable infrastructure is needed to process new data and update models regularly.</p>
<p>Other systems, such as embedded models in medical devices or industrial equipment, require long-term stability with minimal updates. In these cases, upfront optimizations in model and data efficiency are critical to ensure the system performs reliably over time. Reducing dependency on frequent updates minimizes computational and operational overhead, making the system more sustainable in the long run.</p>
</section></section><section id="summary-3" class="level3" data-number="9.7.3"><h3 data-number="9.7.3" class="anchored" data-anchor-id="summary-3">
<span class="header-section-number">9.7.3</span> Summary</h3>
<p>Designing machine learning systems with efficiency in mind requires a holistic approach that considers the specific needs and constraints of the deployment context. From research prototypes to production systems, and across environments as varied as cloud data centers, mobile devices, and Tiny ML applications, the priorities for efficiency differ significantly. Each stage of the machine learning pipeline‚Äîdata collection, model design, training, deployment, and inference‚Äîpresents unique trade-offs that must be navigated thoughtfully.</p>
<p>The examples and scenarios in this section demonstrate the importance of aligning system design with operational requirements. Cloud systems prioritize scalability and throughput, edge systems focus on real-time performance, and Tiny ML applications emphasize extreme resource efficiency. Understanding these differences enables practitioners to tailor their approach, leveraging strategies such as end-to-end co-design and automation tools to balance competing priorities effectively.</p>
<p>Ultimately, the key to designing efficient systems lies in recognizing that efficiency is not a one-size-fits-all solution. It is a dynamic process that requires careful consideration of trade-offs, informed prioritization, and a commitment to addressing the unique challenges of each scenario. With these principles in mind, machine learning practitioners can create systems that are not only efficient but also robust, scalable, and sustainable.</p>
</section></section><section id="broader-challenges" class="level2 page-columns page-full" data-number="9.8"><h2 data-number="9.8" class="anchored" data-anchor-id="broader-challenges">
<span class="header-section-number">9.8</span> Broader Challenges</h2>
<p>While efficiency in machine learning is often framed as a technical challenge, it is also deeply tied to broader questions about the purpose and impact of AI systems. Designing efficient systems involves navigating not only practical trade-offs but also complex ethical and philosophical considerations, such as the following:</p>
<ul>
<li><p>What are the limits of optimization?</p></li>
<li><p>How do we ensure that efficiency benefits are distributed equitably?</p></li>
<li><p>Can the pursuit of efficiency stifle innovation or creativity in the field?</p></li>
</ul>
<p>We must explore these questions as engineers, inviting reflection on the broader implications of system efficiency. By examining the limits of optimization, equity concerns, and the tension between innovation and efficiency, we can have a deeper understanding of the challenges involved in balancing technical goals with ethical and societal values.</p>
<section id="optimization-limits" class="level3 page-columns page-full" data-number="9.8.1"><h3 data-number="9.8.1" class="anchored" data-anchor-id="optimization-limits">
<span class="header-section-number">9.8.1</span> Optimization Limits</h3>
<p>Optimization plays a central role in building efficient machine learning systems, but it is not an infinite process. As systems become more refined, each additional improvement often requires exponentially more effort, time, or resources, while delivering increasingly smaller benefits. This phenomenon, known as diminishing returns, is a common challenge in many engineering domains, including machine learning.</p>
<p>The No Free Lunch (NFL) theorems for optimization further illustrate the inherent limitations of optimization efforts. According to the NFL theorems, no single optimization algorithm can outperform all others across every possible problem. This implies that the effectiveness of an optimization technique is highly problem-specific, and improvements in one area may not translate to others <span class="citation" data-cites="wolpert1997no">(<a href="../references.html#ref-wolpert1997no" role="doc-biblioref">Wolpert and Macready 1997</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wolpert1997no" class="csl-entry" role="listitem">
Wolpert, D. H., and W. G. Macready. 1997. <span>‚ÄúNo Free Lunch Theorems for Optimization.‚Äù</span> <em>IEEE Transactions on Evolutionary Computation</em> 1 (1): 67‚Äì82. <a href="https://doi.org/10.1109/4235.585893">https://doi.org/10.1109/4235.585893</a>.
</div></div><p>For example, compressing a machine learning model can initially reduce memory usage and compute requirements significantly with minimal loss in accuracy. However, as compression progresses, maintaining performance becomes increasingly challenging. Achieving additional gains may necessitate sophisticated techniques, such as hardware-specific optimizations or extensive retraining, which increase both complexity and cost. These costs extend beyond financial investment in specialized hardware and training resources to include the time and expertise required to fine-tune models, iterative testing efforts, and potential trade-offs in model robustness and generalizability. As such, pursuing extreme efficiency often leads to diminishing returns, where escalating costs and complexity outweigh incremental benefits.</p>
<p>The NFL theorems highlight that no universal optimization solution exists, emphasizing the need to balance efficiency pursuits with practical considerations. Recognizing the limits of optimization is critical for designing systems that are not only efficient but also practical and sustainable. Over-optimization risks wasted resources and reduced adaptability, complicating future system updates or adjustments to changing requirements. Identifying when a system is ‚Äúgood enough‚Äù ensures resources are allocated effectively, focusing on efforts with the greatest overall impact.</p>
<p>Similarly, optimizing datasets for training efficiency may initially save resources but excessively reducing dataset size risks compromising diversity and weakening model generalization. Likewise, pushing hardware to its performance limits may improve metrics such as latency or power consumption, yet the associated reliability concerns and engineering costs can ultimately outweigh these gains.</p>
<p>In summary, understanding the limits of optimization is essential for creating systems that balance efficiency with practicality and sustainability. This perspective helps avoid over-optimization and ensures resources are invested in areas with the most meaningful returns.</p>
</section><section id="moores-law-case-study" class="level3 page-columns page-full" data-number="9.8.2"><h3 data-number="9.8.2" class="anchored" data-anchor-id="moores-law-case-study">
<span class="header-section-number">9.8.2</span> Moore‚Äôs Law Case Study</h3>
<p>One of the most insightful examples of the limits of optimization can be seen in Moore‚Äôs Law and the economic curve it depends on. While Moore‚Äôs Law is often celebrated as a predictor of exponential growth in computational power, its success relied on an intricate economic balance. The relationship between integration and cost, as illustrated in the accompanying plot, provides a compelling analogy for the diminishing returns seen in machine learning optimization.</p>
<p><a href="#fig-moores-law-plot" class="quarto-xref">Figure&nbsp;<span>9.15</span></a> shows the relative manufacturing cost per component as the number of components in an integrated circuit increases. Initially, as more components are packed onto a chip (<span class="math inline">\(x\)</span>-axis), the cost per component (<span class="math inline">\(y\)</span>-axis) decreases. This is because higher integration reduces the need for supporting infrastructure such as packaging and interconnects, creating economies of scale. For example, in the early years of integrated circuit design, moving from hundreds to thousands of components per chip drastically reduced costs and improved performance <span class="citation" data-cites="moore2021cramming">(<a href="../references.html#ref-moore2021cramming" role="doc-biblioref">Moore 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-moores-law-plot" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full"><div aria-describedby="fig-moores-law-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/moores-law.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;9.15: The economics of Moore‚Äôs law. Source: [@moore2021cramming]"><img src="images/png/moores-law.png" class="img-fluid figure-img" style="width:45.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-moores-law-plot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.15: The economics of Moore‚Äôs law. Source: <span class="citation" data-cites="moore2021cramming">(<a href="../references.html#ref-moore2021cramming" role="doc-biblioref">Moore 2021</a>)</span>
</figcaption><div class="no-row-height column-margin column-container"><div id="ref-moore2021cramming" class="csl-entry" role="listitem">
Moore, Gordon. 2021. <span>‚ÄúCramming More Components onto Integrated Circuits (1965).‚Äù</span> In <em>Ideas That Created the Future</em>, 261‚Äì66. The MIT Press. <a href="https://doi.org/10.7551/mitpress/12274.003.0027">https://doi.org/10.7551/mitpress/12274.003.0027</a>.
</div></div></figure>
</div>
<p>However, as integration continues, the curve begins to rise. This inflection point occurs because the challenges of scaling become more pronounced. Components packed closer together face reliability issues, such as increased heat dissipation and signal interference. Addressing these issues requires more sophisticated manufacturing techniques, such as advanced lithography, error correction, and improved materials. These innovations increase the complexity and cost of production, driving the curve upward. This U-shaped curve captures the fundamental trade-off in optimization: early improvements yield substantial benefits, but beyond a certain point, each additional gain comes at a greater cost.</p>
<section id="ml-optimization-parallels" class="level4"><h4 class="anchored" data-anchor-id="ml-optimization-parallels">ML Optimization Parallels</h4>
<p>The dynamics of this curve mirror the challenges faced in machine learning optimization. For instance, compressing a deep learning model to reduce its size and energy consumption follows a similar trajectory. Initial optimizations, such as pruning redundant parameters or reducing precision, often lead to significant savings with minimal impact on accuracy. However, as the model is further compressed, the losses in performance become harder to recover. Techniques such as quantization or hardware-specific tuning can restore some of this performance, but these methods add complexity and cost to the design process.</p>
<p>Similarly, in data efficiency, reducing the size of training datasets often improves computational efficiency at first, as less data requires fewer resources to process. Yet, as the dataset shrinks further, it may lose diversity, compromising the model‚Äôs ability to generalize. Addressing this often involves introducing synthetic data or sophisticated augmentation techniques, which demand additional engineering effort.</p>
<p>The Moore‚Äôs Law plot (<a href="#fig-moores-law-plot" class="quarto-xref">Figure&nbsp;<span>9.15</span></a>) serves as a visual reminder that optimization is not an infinite process. The cost-benefit balance is always context-dependent, and the point of diminishing returns varies based on the goals and constraints of the system. Machine learning practitioners, like semiconductor engineers, must identify when further optimization ceases to provide meaningful benefits. Over-optimization can lead to wasted resources, reduced adaptability, and systems that are overly specialized to their initial conditions.</p>
</section></section><section id="equity-concerns" class="level3 page-columns page-full" data-number="9.8.3"><h3 data-number="9.8.3" class="anchored" data-anchor-id="equity-concerns">
<span class="header-section-number">9.8.3</span> Equity Concerns</h3>
<p>Efficiency in machine learning has the potential to reduce costs, improve scalability, and expand accessibility. However, the resources needed to achieve efficiency‚Äîadvanced hardware, curated datasets, and state-of-the-art optimization techniques‚Äîare often concentrated in well-funded organizations or regions. This disparity creates inequities in who can leverage efficiency gains, limiting the reach of machine learning in low-resource contexts. By examining compute, data, and algorithmic efficiency inequities, we can better understand these challenges and explore pathways toward democratization.</p>
<section id="uneven-access" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="uneven-access">Uneven Access</h4>
<p>The training costs of state-of-the-art AI models have reached unprecedented levels. For example, OpenAI‚Äôs GPT-4 used an estimated USD $78 million worth of compute to train, while Google‚Äôs Gemini Ultra cost USD $191 million for compute <span class="citation" data-cites="perrault2024artificial">(<a href="../references.html#ref-perrault2024artificial" role="doc-biblioref">Maslej et al. 2024</a>)</span>. Computational efficiency depends on access to specialized hardware and infrastructure. The discrepancy in access is significant: training even a small language model (SLM) like LLlama with 7 billion parameters can require millions of dollars in computing resources, while many research institutions operate with significantly lower annual compute budgets.</p>
<div class="no-row-height column-margin column-container"><div id="ref-perrault2024artificial" class="csl-entry" role="listitem">
Maslej, Nestor, Loredana Fattorini, C. Raymond Perrault, Vanessa Parli, Anka Reuel, Erik Brynjolfsson, John Etchemendy, et al. 2024. <span>‚ÄúArtificial Intelligence Index Report 2024.‚Äù</span> <em>CoRR</em>. <a href="https://doi.org/10.48550/ARXIV.2405.19522">https://doi.org/10.48550/ARXIV.2405.19522</a>.
</div><div id="ref-oecd_ai_2021" class="csl-entry" role="listitem">
OECD.AI. 2021. <span>‚ÄúMeasuring the Geographic Distribution of AI Computing Capacity.‚Äù</span> &lt;https://oecd.ai/en/policy-circle/computing-capacity&gt;.
</div></div><p>Research conducted by <a href="https://oecd.ai/en/">OECD.AI</a> indicates that 90% of global AI computing capacity is centralized in only five countries, posing significant challenges for researchers and professionals in other regions <span class="citation" data-cites="oecd_ai_2021">(<a href="../references.html#ref-oecd_ai_2021" role="doc-biblioref">OECD.AI 2021</a>)</span>.</p>
<p>A concrete illustration of this disparity is the compute divide in academia versus industry. Academic institutions often lack the hardware needed to replicate state-of-the-art results, particularly when competing with large technology firms that have access to custom supercomputers or cloud resources. This imbalance not only stifles innovation in underfunded sectors but also makes it harder for diverse voices to contribute to advancing machine learning.</p>
<p>Energy-efficient compute technologies, such as accelerators designed for Tiny ML or Mobile ML, present a promising avenue for democratization. By enabling powerful processing on low-cost, low-power devices, these technologies allow organizations without access to high-end infrastructure to build and deploy impactful systems. For instance, energy-efficient Tiny ML models can be deployed on affordable microcontrollers, opening doors for applications in healthcare, agriculture, and education in underserved regions.</p>
</section><section id="low-resource-challenges" class="level4"><h4 class="anchored" data-anchor-id="low-resource-challenges">Low-Resource Challenges</h4>
<p>Data efficiency is essential in contexts where high-quality datasets are scarce, but the challenges of achieving it are unequally distributed. For example, natural language processing (NLP) for low-resource languages suffers from a lack of sufficient training data, leading to significant performance gaps compared to high-resource languages like English. Efforts like the Masakhane project, which builds open-source datasets for African languages, show how collaborative initiatives can address this issue. However, scaling such efforts globally requires far greater investment and coordination.</p>
<p>Even when data is available, the ability to process and curate it efficiently depends on computational and human resources. Large organizations routinely employ data engineering teams and automated pipelines for curation and augmentation, enabling them to optimize data efficiency and improve downstream performance. In contrast, smaller groups often lack access to the tools or expertise needed for such tasks, leaving them at a disadvantage in both research and practical applications.</p>
<p>Democratizing data efficiency requires more open sharing of pre-trained models and datasets. Initiatives like Hugging Face‚Äôs open access to transformers or multilingual models by organizations like Meta‚Äôs No Language Left Behind aim to make state-of-the-art NLP models available to researchers and practitioners worldwide. These efforts help reduce the barriers to entry for data-scarce regions, enabling more equitable access to AI capabilities.</p>
</section><section id="efficiency-for-accessibility" class="level4"><h4 class="anchored" data-anchor-id="efficiency-for-accessibility">Efficiency for Accessibility</h4>
<p>Model efficiency plays a crucial role in democratizing machine learning by enabling advanced capabilities on low-cost, resource-constrained devices. Compact, efficient models designed for edge devices or mobile phones have already begun to bridge the gap in accessibility. For instance, AI-powered diagnostic tools running on smartphones are transforming healthcare in remote areas, while low-power Tiny ML models enable environmental monitoring in regions without reliable electricity or internet connectivity.</p>
<p>Technologies like <a href="https://ai.google.dev/edge/litert">TensorFlow Lite</a> and <a href="https://pytorch.org/mobile/home/">PyTorch Mobile</a> allow developers to deploy lightweight models on everyday devices, expanding access to AI applications in resource-constrained settings. These tools demonstrate how algorithmic efficiency can serve as a practical pathway to equity, particularly when combined with energy-efficient compute hardware.</p>
<p>However, scaling the benefits of algorithmic efficiency requires addressing barriers to entry. Many efficient architectures, such as those designed through NAS, remain resource-intensive to develop. Open-source efforts to share pre-optimized models, like MobileNet or EfficientNet, play a critical role in democratizing access to efficient AI by allowing under-resourced organizations to deploy state-of-the-art solutions without needing to invest in expensive optimization processes.</p>
</section><section id="democratization-pathways" class="level4"><h4 class="anchored" data-anchor-id="democratization-pathways">Democratization Pathways</h4>
<p>Efforts to close the equity gap in machine learning must focus on democratizing access to tools and techniques that enhance efficiency. Open-source initiatives, such as community-driven datasets and shared model repositories, provide a foundation for equitable access to efficient systems. Affordable hardware platforms, such as Raspberry Pi devices or open-source microcontroller frameworks, further enable resource-constrained organizations to build and deploy AI solutions tailored to their needs.</p>
<p>Collaborative partnerships between well-resourced organizations and underrepresented groups also offer opportunities to share expertise, funding, and infrastructure. For example, initiatives that provide subsidized access to cloud computing platforms or pre-trained models for underserved regions can empower diverse communities to leverage efficiency for social impact.</p>
<p>Through efforts in model, computation, and data efficiency, the democratization of machine learning can become a reality. These efforts not only expand access to AI capabilities but also foster innovation and inclusivity, ensuring that the benefits of efficiency are shared across the global community.</p>
</section></section><section id="balancing-innovation-and-efficiency" class="level3" data-number="9.8.4"><h3 data-number="9.8.4" class="anchored" data-anchor-id="balancing-innovation-and-efficiency">
<span class="header-section-number">9.8.4</span> Balancing Innovation and Efficiency</h3>
<p>The pursuit of efficiency in machine learning often brings with it a tension between optimizing for what is known and exploring what is new. On one hand, efficiency drives the practical deployment of machine learning systems, enabling scalability, cost reduction, and environmental sustainability. On the other hand, focusing too heavily on efficiency can stifle innovation by discouraging experimentation with untested, resource-intensive ideas.</p>
<section id="stability-vs.-experimentation" class="level4"><h4 class="anchored" data-anchor-id="stability-vs.-experimentation">Stability vs.&nbsp;Experimentation</h4>
<p>Efficiency often favors established techniques and systems that have already been proven to work well. For instance, optimizing neural networks through pruning, quantization, or distillation typically involves refining existing architectures rather than developing entirely new ones. While these approaches provide incremental improvements, they may come at the cost of exploring novel designs or paradigms that could yield transformative breakthroughs.</p>
<p>Consider the shift from traditional machine learning methods to deep learning. Early neural network research in the 1990s and 2000s required significant computational resources and often failed to outperform simpler methods on practical tasks. Despite this, researchers continued to push the boundaries of what was possible, eventually leading to the breakthroughs in deep learning that define modern AI. If the field had focused exclusively on efficiency during that period, these innovations might never have emerged.</p>
</section><section id="resource-intensive-innovation" class="level4"><h4 class="anchored" data-anchor-id="resource-intensive-innovation">Resource-Intensive Innovation</h4>
<p>Pioneering research often requires significant resources, from massive datasets to custom hardware. For example, large language models like GPT-4 or PaLM are not inherently efficient; their training processes consume enormous amounts of compute power and energy. Yet, these models have opened up entirely new possibilities in language understanding, prompting advancements that eventually lead to more efficient systems, such as smaller fine-tuned versions for specific tasks.</p>
<p>However, this reliance on resource-intensive innovation raises questions about who gets to participate in these advancements. Well-funded organizations can afford to explore new frontiers, while smaller institutions may be constrained to incremental improvements that prioritize efficiency over novelty. Balancing the need for experimentation with the realities of resource availability is a key challenge for the field.</p>
</section><section id="efficiency-creativity-constraint" class="level4"><h4 class="anchored" data-anchor-id="efficiency-creativity-constraint">Efficiency-Creativity Constraint</h4>
<p>Efficiency-focused design often requires adhering to strict constraints, such as reducing model size, energy consumption, or latency. While these constraints can drive ingenuity, they can also limit the scope of what researchers and engineers are willing to explore. For instance, edge computing applications often demand ultra-compact models, leading to a narrow focus on compression techniques rather than entirely new approaches to machine learning on constrained devices.</p>
<p>At the same time, the drive for efficiency can have a positive impact on innovation. Constraints force researchers to think creatively, leading to the development of new methods that maximize performance within tight resource budgets. Techniques like NAS and attention mechanisms arose, in part, from the need to balance performance and efficiency, demonstrating that innovation and efficiency can coexist when approached thoughtfully.</p>
</section><section id="striking-a-balance" class="level4"><h4 class="anchored" data-anchor-id="striking-a-balance">Striking a Balance</h4>
<p>The tension between innovation and efficiency highlights the need for a balanced approach to system design and research priorities. Organizations and researchers must recognize when it is appropriate to prioritize efficiency and when to embrace the risks of experimentation. For instance, applied systems for real-world deployment may demand strict efficiency constraints, while exploratory research labs can focus on pushing boundaries without immediate concern for resource optimization.</p>
<p>Ultimately, the relationship between innovation and efficiency is not adversarial but complementary. Efficient systems create the foundation for scalable, practical applications, while resource-intensive experimentation drives the breakthroughs that redefine what is possible. Balancing these priorities ensures that machine learning continues to evolve while remaining accessible, impactful, and sustainable.</p>
</section></section></section><section id="conclusion" class="level2" data-number="9.9"><h2 data-number="9.9" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">9.9</span> Conclusion</h2>
<p>Efficiency in machine learning systems is essential not just for achieving technical goals but for addressing broader questions about scalability, sustainability, and inclusivity. This chapter has focused on the why and how of efficiency‚Äîwhy it is critical to modern machine learning and how to achieve it through a balanced focus on model, compute, and data dimensions. We began by exploring the empirical foundations of scaling laws, revealing how model performance scales with resources and highlighting the critical importance of efficient resource utilization as models grow in complexity. The trade-offs and challenges inherent in scaling, as well as the potential for scaling breakdowns, underscore the necessity of a holistic approach to system design.</p>
<p>By understanding the interdependencies and trade-offs inherent in the algorithmic, compute, and data dimensions of efficiency, we can build systems that align with their operational contexts and long-term objectives. The challenges discussed in this chapter, from the limits of optimization to equity concerns and the tension between efficiency and innovation, highlight the need for a thoughtful approach. Whether working on a high-performance cloud system or a constrained Tiny ML application, the principles of efficiency serve as a compass for navigating the complexities of system design.</p>
<p>The future of scaling laws is a critical area of exploration, particularly as we consider the practical and sustainable limits of continued scaling. Research into the theoretical foundations of scaling, architectural innovations, and the role of data quality will be essential for guiding the development of next-generation AI systems. Moreover, addressing the equity concerns associated with access to compute, data, and efficient models is crucial for ensuring that the benefits of AI are shared broadly.</p>
<p>With this foundation in place, we can now dive into the what‚Äîthe specific techniques and strategies that enable efficient machine learning systems. By grounding these practices in a clear understanding of the why and the how, we ensure that efficiency remains a guiding principle rather than a reactive afterthought, and that the insights from scaling laws are applied in a way that promotes both performance and sustainability.</p>



</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script><script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><input type="hidden" id="giscus-base-theme" value="light"><input type="hidden" id="giscus-alt-theme" value="dark"><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../../contents/core/training/training.html" class="pagination-link" aria-label="AI Training">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI Training</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/optimizations/optimizations.html" class="pagination-link" aria-label="Model Optimizations">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/widget_quiz/contents/core/efficient_ai/efficient_ai.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/widget_quiz/contents/core/efficient_ai/efficient_ai.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>


</body></html>