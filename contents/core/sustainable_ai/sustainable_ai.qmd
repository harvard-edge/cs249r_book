---
bibliography: sustainable_ai.bib
---

# Sustainable AI {#sec-sustainable_ai}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-sustainable-ai-resource), [Videos](#sec-sustainable-ai-resource), [Exercises](#sec-sustainable-ai-resource)
:::

![_DALL·E 3 Prompt: 3D illustration on a light background of a sustainable AI network interconnected with a myriad of eco-friendly energy sources. The AI actively manages and optimizes its energy from sources like solar arrays, wind turbines, and hydro dams, emphasizing power efficiency and performance. Deep neural networks spread throughout, receiving energy from these sustainable resources._](images/png/cover_sustainable_ai.png)

## Purpose {.unnumbered}

*_How do environmental considerations influence the design and implementation of machine learning systems, and what principles emerge from examining AI through an ecological perspective?_*

Machine learning systems inherently require significant computational resources, raising critical concerns about their environmental impact. Addressing these concerns requires a deep understanding of how architectural decisions affect energy consumption, resource utilization, and ecological sustainability. Designers and engineers must consider the relationships between computational demands, resource utilization, and environmental consequences across various system components. A systematic exploration of these considerations helps identify key architectural principles and design strategies that harmonize performance objectives with ecological stewardship.

::: {.callout-tip title="Learning Objectives"}

* Coming soon.

:::

## Overview

Machine learning has become an essential driver of technological progress, powering advancements across industries and scientific domains. However, as AI models grow in complexity and scale, the computational demands required to train and deploy them have increased significantly, raising critical concerns about sustainability. The environmental impact of AI extends beyond energy consumption, encompassing carbon emissions, resource extraction, and electronic waste. As a result, it is imperative to examine AI systems through the lens of sustainability and assess the trade-offs between performance and ecological responsibility.

Developing large-scale AI models, such as state-of-the-art language and vision models, requires substantial computational power. Training a single large model can consume thousands of megawatt-hours of electricity, equivalent to powering hundreds of households for a month. Much of this energy is supplied by data centers, which rely heavily on nonrenewable energy sources, contributing to global carbon emissions. Estimates indicate that AI-related emissions are comparable to those of entire industrial sectors, highlighting the urgency of transitioning to more energy-efficient models and renewable-powered infrastructure.

Beyond energy consumption, AI systems also impact the environment through hardware manufacturing and resource utilization. Training and inference workloads depend on specialized processors, such as GPUs and TPUs, which require rare earth metals whose extraction and processing generate significant pollution. Additionally, the growing demand for AI applications accelerates electronic waste production, as hardware rapidly becomes obsolete. Even small-scale AI systems, such as those deployed on edge devices, contribute to sustainability challenges, necessitating careful consideration of their lifecycle impact.

This chapter examines the sustainability challenges associated with AI systems and explores emerging solutions to mitigate their environmental footprint. It discusses strategies for improving algorithmic efficiency, optimizing training infrastructure, and designing energy-efficient hardware. Additionally, it considers the role of renewable energy sources, regulatory frameworks, and industry best practices in promoting sustainable AI development. By addressing these challenges, the field can advance toward more ecologically responsible AI systems while maintaining technological progress.

## Social and Ethical Responsibility

### Long-Term Sustainability

The long-term sustainability of AI is increasingly challenged by the exponential growth of computational demands required to train and deploy machine learning models. Over the past decade, AI systems have scaled at an unprecedented rate, with compute requirements increasing 350,000× from 2012 to 2019 [@schwartz2020green]. This trend shows no signs of slowing down, as advancements in deep learning continue to prioritize larger models with more parameters, larger training datasets, and higher computational complexity. However, sustaining this trajectory poses significant sustainability challenges, particularly as the efficiency gains from hardware improvements fail to keep pace with the rising demands of AI workloads.

Historically, computational efficiency improved with advances in semiconductor technology. Moore’s Law, which predicted that the number of transistors on a chip would double approximately every two years, led to continuous improvements in processing power and energy efficiency. However, Moore’s Law is now reaching fundamental physical limits, making further transistor scaling increasingly difficult and costly. Dennard scaling, which once ensured that smaller transistors would operate at lower power levels, has also ended, leading to stagnation in energy efficiency improvements per transistor. As a result, while AI models continue to scale in size and capability, the hardware running these models is no longer improving at the same exponential rate. This growing divergence between computational demand and hardware efficiency creates an unsustainable trajectory in which AI consumes ever-increasing amounts of energy.

This rising energy demand is particularly evident in the training of state-of-the-art models. For example, training GPT-3 was estimated to require 1,300 megawatt-hours (MWh) of electricity, roughly equivalent to the monthly energy consumption of 1,450 average U.S. households [@maslej2023artificial]. Similarly, AI-powered applications such as large-scale recommender systems and generative models require continuous inference at scale, consuming significant energy even after training is complete. As AI adoption grows across industries, from finance to healthcare to entertainment, the cumulative energy burden of AI workloads will continue to increase, raising concerns about the environmental impact of widespread deployment.

Beyond electricity consumption, the sustainability challenges of AI extend to hardware resource demands. High-performance computing (HPC) clusters and AI accelerators rely on specialized hardware, including GPUs, TPUs, and FPGAs, all of which require rare earth metals and complex manufacturing processes. The production of AI chips is energy-intensive, involving multiple fabrication steps that contribute to Scope 3 emissions, which account for the majority of the carbon footprint in semiconductor manufacturing. As model sizes continue to grow, the demand for AI hardware increases, further exacerbating the environmental impact of semiconductor production and disposal.

The long-term sustainability of AI requires a shift in how machine learning systems are designed, optimized, and deployed. As compute demands outpace efficiency improvements, addressing AI’s environmental impact will require rethinking system architecture, energy-aware computing, and lifecycle management. Without intervention, the unchecked growth of AI models will continue to place unsustainable pressures on energy grids, data centers, and natural resources, underscoring the need for a more systematic approach to sustainable AI development.

The environmental impact of AI is not just a technical issue but also an ethical and social one. As AI becomes more integrated into our lives and industries, its sustainability becomes increasingly critical.

### **Ethical Considerations**

The environmental impact of AI raises fundamental ethical questions regarding the responsibility of developers, organizations, and policymakers to mitigate its carbon footprint. As AI systems continue to scale, their energy consumption and resource demands have far-reaching implications, necessitating a proactive approach to sustainability. Developers and companies that build and deploy AI systems must consider not only performance and efficiency but also the broader environmental consequences of their design choices.

A key ethical challenge lies in balancing **technological progress with ecological responsibility**. The pursuit of increasingly large models often prioritizes accuracy and capability over energy efficiency, leading to substantial environmental costs. While optimizing for sustainability may introduce trade-offs—such as increased development time or minor reductions in accuracy—it is an ethical imperative to integrate environmental considerations into AI system design. This requires shifting industry norms toward sustainable computing practices, such as energy-aware training techniques, low-power hardware designs, and carbon-conscious deployment strategies.

Beyond sustainability, AI development also raises **broader ethical concerns** related to transparency, fairness, and accountability. Figure @fig-ethical-ai illustrates the **ethical challenges associated with AI development**, linking different types of concerns—such as inscrutable evidence, unfair outcomes, and traceability—to issues like opacity, bias, and automation bias. These concerns extend to sustainability, as the environmental trade-offs of AI development are often opaque and difficult to quantify. The lack of **traceability** in energy consumption and carbon emissions can lead to unjustified actions, where companies prioritize performance gains without fully understanding or disclosing the environmental costs.

Addressing these concerns also demands greater transparency and accountability from AI companies. Large technology firms operate extensive cloud infrastructures that power modern AI applications, yet their environmental impact is often opaque. Organizations must take active steps to measure, report, and reduce their carbon footprint across the entire AI lifecycle, from hardware manufacturing to model training and inference. Voluntary self-regulation is an important first step, but **policy interventions and industry-wide standards** may be necessary to ensure long-term sustainability. Reported metrics such as energy consumption, carbon emissions, and efficiency benchmarks could serve as mechanisms to hold organizations accountable.

Furthermore, ethical AI development must encourage open discourse on environmental trade-offs. Researchers should be empowered to **advocate for sustainability within their institutions and organizations**, ensuring that environmental concerns are factored into AI development priorities. The broader AI community has already begun addressing these issues, as exemplified by the [open letter advocating a pause on large-scale AI experiments](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), which highlights concerns about unchecked expansion. By fostering a culture of transparency and ethical responsibility, the AI industry can work toward aligning technological advancement with ecological sustainability.

AI has the potential to reshape industries and societies, but its long-term viability depends on how responsibly it is developed. Ethical AI development is not only about preventing harm to individuals and communities but also about ensuring that AI-driven innovation does not come at the cost of environmental degradation. As stewards of these powerful technologies, developers and organizations have a profound duty to integrate sustainability into AI’s future trajectory.

### **Case Study: DeepMind's AI for AI Energy Efficiency**

Google's data centers form the backbone of services such as Search, Gmail, and YouTube, handling billions of queries daily. These data centers operate at massive scales, consuming vast amounts of electricity, particularly for cooling infrastructure that ensures optimal server performance. Improving the **energy efficiency** of data centers has long been a priority, but conventional engineering approaches faced diminishing returns due to the complexity of the cooling systems and the highly dynamic nature of environmental conditions. To address these challenges, Google collaborated with DeepMind to develop a machine learning-driven optimization system that could **automate and enhance energy management at scale**.

Building on more than a decade of efforts to optimize **data center design, energy-efficient hardware, and renewable energy integration**, DeepMind’s AI approach targeted one of the most energy-intensive aspects of data centers: **cooling systems**. Traditional cooling relies on manually set heuristics that account for factors such as server heat output, external weather conditions, and architectural constraints. However, these systems exhibit **nonlinear interactions**, meaning that simple rule-based optimizations often fail to capture the full complexity of their operations. The result was **suboptimal cooling efficiency**, leading to unnecessary energy waste.

DeepMind’s team trained a neural network model using Google's **historical sensor data**, which included real-time temperature readings, power consumption levels, cooling pump activity, and other operational parameters. The model learned the intricate relationships between these factors and could dynamically predict **the most efficient cooling configurations**. Unlike traditional approaches, which relied on human engineers periodically adjusting system settings, the AI model **continuously adapted in real time** to changing environmental and workload conditions.

The results were **unprecedented efficiency gains**. When deployed in live data center environments, DeepMind’s AI-driven cooling system **reduced cooling energy consumption by 40%**, leading to an overall **15% improvement in Power Usage Effectiveness (PUE)**—a key metric for data center energy efficiency that measures the ratio of total energy consumption to the energy used purely for computing tasks [@barroso2019datacenter]. Notably, these improvements were achieved without any additional hardware modifications, demonstrating the potential of **software-driven optimizations** to significantly reduce AI’s carbon footprint.

Beyond a single data center, DeepMind’s AI model provided a **generalizable framework** that could be adapted to different facility designs and climate conditions, offering a scalable solution for optimizing power consumption across **global data center networks**. This case study exemplifies how AI can be leveraged **not just as a consumer of computational resources but as a tool for sustainability**, driving substantial efficiency improvements in the infrastructure that supports machine learning itself.

The integration of **data-driven decision-making, real-time adaptation, and scalable AI models** demonstrates the growing role of **intelligent resource management in sustainable AI system design**. This breakthrough exemplifies how **machine learning can be applied to optimize the very infrastructure that powers it**, ensuring a **more energy-efficient future for large-scale AI deployments**.

## **Understanding AI’s Carbon Footprint**

The carbon footprint of artificial intelligence is a critical aspect of its overall environmental impact. As AI adoption continues to expand, so does its **energy consumption and associated greenhouse gas emissions**. Training and deploying AI models require vast computational resources, often powered by energy-intensive data centers that contribute significantly to global carbon emissions. However, the carbon footprint of AI extends beyond electricity usage, encompassing **hardware manufacturing, data storage, and end-user interactions**—all of which contribute to emissions across an AI system’s lifecycle.

Quantifying the carbon impact of AI is complex, as it depends on multiple factors, including the **size of the model, the duration of training, the hardware used, and the energy sources powering data centers**. Large-scale AI models, such as **GPT-3**, require thousands of **megawatt-hours (MWh) of electricity**, equivalent to the energy consumption of entire communities. The energy required for **inference**—the phase in which trained models generate outputs—is also substantial, particularly for widely deployed AI services such as **real-time translation, image generation, and personalized recommendations**. Unlike traditional software, which has a relatively static energy footprint, AI models **consume energy continuously**, leading to an ongoing sustainability challenge.

Beyond direct energy use, the carbon footprint of AI must also account for **indirect emissions from hardware production and supply chains**. Manufacturing AI accelerators such as **GPUs, TPUs, and custom chips** involves energy-intensive fabrication processes that rely on rare earth metals and complex supply chains. The **full life cycle emissions of AI systems—encompassing data centers, hardware manufacturing, and global AI deployments—must be considered to develop more sustainable AI practices**.

Understanding AI’s carbon footprint requires breaking down **where emissions come from, how they are measured, and what strategies can be employed to mitigate them**. This section explores:

- **Carbon emissions and energy consumption trends in AI**—quantifying AI’s energy demand and providing real-world comparisons.
- **Scopes of carbon emissions (Scope 1, 2, and 3)**—differentiating between direct, indirect, and supply chain-related emissions.
- **The energy cost of training vs. inference**—analyzing how different phases of AI impact sustainability.

By dissecting these components, we can better assess **the true environmental impact of AI systems and identify opportunities to reduce their footprint** through more efficient design, energy-conscious deployment, and sustainable infrastructure choices.

### **Updated Section: Carbon Emissions & Energy Consumption**

### **Carbon Emissions & Energy Consumption**

Artificial intelligence systems require vast computational resources, making them one of the most energy-intensive workloads in modern computing. The energy consumed by AI systems extends beyond the training of large models to include ongoing inference workloads, data storage, and communication across distributed computing infrastructure. As AI adoption scales across industries, understanding its energy consumption patterns and carbon emissions is critical for designing more sustainable machine learning infrastructure.

Data centers play a central role in AI's energy demands, consuming vast amounts of electricity to power compute servers, storage, and cooling systems. Without access to **renewable energy**, these facilities rely heavily on **nonrenewable sources such as coal and natural gas**, contributing significantly to global carbon emissions. Current estimates suggest that **data centers produce up to 2% of total global CO₂ emissions**—a figure that is **closing in on the airline industry’s footprint** [@liu2020energy]. The energy burden of AI is expected to **grow exponentially** due to three key factors: **increasing data center capacity, rising AI training workloads, and surging inference demands** [@patterson2022carbon]. Without intervention, these trends risk making AI’s environmental footprint **unsustainably large**.

#### **AI’s Growing Energy Demands in Data Centers**

AI workloads are among the most compute-intensive operations in modern data centers. Companies such as **Meta operate hyperscale data centers** spanning multiple **football fields in size**, housing **hundreds of thousands of AI-optimized servers** [@facebookdatacenters]. The training of **large language models (LLMs) such as GPT-4** required over **25,000 Nvidia A100 GPUs running continuously for 90 to 100 days** [@semianalysisGPT4], consuming **thousands of megawatt-hours (MWh) of electricity**. These facilities rely on **high-performance AI accelerators** like **NVIDIA DGX H100 units**, each of which can **draw up to 10.2 kW at peak power** [@nvidiadgxH100]. The combined **compute, networking, and storage demands** of AI data centers result in electricity consumption levels comparable to **small cities**.

Cooling is another major factor in AI’s energy footprint. Large-scale AI training and inference workloads generate **massive amounts of heat**, necessitating advanced **cooling solutions** to prevent hardware failures. Estimates indicate that **30–40% of a data center’s total electricity usage goes into cooling alone** [@dayarathna2015data]. Companies have begun adopting alternative cooling methods to reduce this demand. For example, **Microsoft’s data center in Ireland leverages a nearby fjord, using over half a million gallons of seawater daily** to dissipate heat [@microsoftcooling]. However, as **AI models scale** in complexity, **cooling demands continue to grow**, making sustainable AI infrastructure design a pressing challenge.

#### **AI’s Carbon Footprint Compared to Other Industries**

The environmental impact of AI workloads has emerged as a significant concern, with carbon emissions approaching levels comparable to established carbon-intensive sectors. Research demonstrates that training a single large AI model generates carbon emissions equivalent to multiple passenger vehicles over their complete lifecycle [@strubell2019energy]. To contextualize AI's environmental footprint:

The training phase of large natural language processing models produces carbon dioxide emissions comparable to hundreds of transcontinental flights. When examining the broader industry impact, AI's aggregate computational carbon footprint is approaching parity with the commercial aviation sector. Furthermore, as AI applications scale to serve billions of users globally, the cumulative emissions from continuous inference operations may ultimately exceed those generated during training.

Figure @fig-carbonfootprint provides a detailed analysis of carbon emissions across various large-scale machine learning tasks, illustrating the substantial environmental impact of different AI applications and architectures.

This quantitative assessment of AI's carbon footprint underscores the pressing need to develop more sustainable approaches to machine learning development and deployment. Understanding these environmental costs is crucial for implementing effective mitigation strategies and advancing the field responsibly.


![Carbon footprint of large-scale ML tasks. Source: @wu2022sustainable.](images/png/model_carbonfootprint.png){#fig-carbonfootprint}

Moreover, AI’s impact extends beyond **energy consumption during operation**. The **full lifecycle emissions** of AI include **hardware manufacturing, supply chain emissions, and end-of-life disposal**, making AI a **significant contributor to environmental degradation**. AI models not only require **electricity to train and infer**, but they also **depend on a complex infrastructure of semiconductor fabrication, rare earth metal mining, and electronic waste disposal**. The next section breaks down AI’s carbon emissions into **Scope 1 (direct emissions), Scope 2 (indirect emissions from electricity), and Scope 3 (supply chain and lifecycle emissions)** to provide a more detailed view of its environmental impact.

### **Scopes of Carbon Emissions**

AI is expected to see an [annual growth rate of 37.3% between 2023 and 2030](https://www.forbes.com/advisor/business/ai-statistics/). Yet, applying the same growth rate to operational computing could multiply annual AI energy needs up to 1,000 times by 2030. So, while model optimization tackles one facet, responsible innovation must also consider total lifecycle costs at global deployment scales that were unfathomable just years ago but now pose infrastructure and sustainability challenges ahead.

#### Scope 1

Scope 1 emissions refer to the **direct greenhouse gas emissions produced by AI data centers and computing facilities**. This includes emissions from on-site power generation, backup diesel generators used for reliability in large cloud environments, and other direct energy-consuming operations. While AI data centers increasingly rely on grid electricity, some facilities operate their own power plants or backup systems, contributing directly to carbon emissions. These emissions are typically lower in regions where data centers use cleaner energy sources, but in locations reliant on fossil fuels, they can be substantial.

#### Scope 2

Scope 2 emissions encompass the **indirect emissions generated from electricity purchased to power AI infrastructure**. The majority of AI’s operational energy consumption falls under this category, as cloud providers and enterprise computing facilities rely on massive electrical inputs to run GPUs, TPUs, and high-density servers. The carbon intensity of Scope 2 emissions depends largely on the energy mix of the region where AI workloads are executed. In areas where coal and natural gas dominate electricity generation, AI systems have a significantly higher carbon footprint than in locations where renewable energy sources such as wind, hydro, and solar are more prevalent. This geographic variation has led some technology companies to strategically locate data centers in regions with cleaner energy sources, leveraging carbon-aware scheduling techniques to minimize emissions.

#### Scope 3

Scope 3 emissions represent **the largest and most complex category, accounting for all indirect emissions throughout the AI supply chain**. These emissions originate from the manufacturing, transportation, and disposal of AI hardware, including the production of GPUs, CPUs, TPUs, and memory modules. The fabrication of semiconductor chips used in AI accelerators is an energy-intensive process that requires rare earth metals, chemical etching, and extreme ultraviolet (EUV) lithography, all of which contribute to substantial carbon output. Industry reports suggest that the carbon emissions associated with manufacturing a single high-performance AI chip can be equivalent to **multiple years’ worth of operational energy consumption**, making hardware production a critical yet often overlooked factor in AI’s sustainability equation.

Beyond manufacturing, Scope 3 emissions include the downstream impact of AI once deployed. AI services such as search engines, social media platforms, and cloud-based recommendation systems operate at enormous scale, requiring continuous inference across millions or even billions of user interactions. The cumulative electricity demand of inference workloads can ultimately surpass the energy used for training, further amplifying AI’s carbon impact. End-user devices, including smartphones, IoT devices, and edge computing platforms, also contribute to Scope 3 emissions, as their AI-enabled functionality depends on sustained computation. Companies such as Meta and Google report that Scope 3 emissions from AI-powered services make up the largest share of their total environmental footprint, due to the sheer scale at which AI operates.

These massive facilities provide the infrastructure for training complex neural networks on vast datasets. For instance, based on [leaked information](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure), OpenAI's language model GPT-4 was trained on Azure data centers packing over 25,000 Nvidia A100 GPUs, used continuously for over 90 to 100 days.

The **GHG Protocol framework**, illustrated in **Figure @fig-ghg-protocol**, provides a structured way to visualize the sources of AI-related carbon emissions. Scope 1 emissions arise from direct company operations, such as data center power generation and company-owned infrastructure. Scope 2 covers electricity purchased from the grid, the primary source of emissions for cloud computing workloads. Scope 3 extends beyond an organization’s direct control, including emissions from hardware manufacturing, transportation, and even the end-user energy consumption of AI-powered services. Understanding this breakdown allows for **more targeted sustainability strategies**, ensuring that efforts to reduce AI’s environmental impact are not solely focused on energy efficiency but also address the broader supply chain and lifecycle emissions that contribute significantly to the industry’s carbon footprint.

![The GHG Protocol framework categorizes emissions into Scope 1, 2, and 3, helping organizations assess their direct and indirect carbon impact. Source: Circularise.](images/png/ghg_protocol.png){#fig-ghg-protocol}

### **Training vs. Inference Carbon Impact**

The energy consumption of AI systems is often associated with the training phase, where vast computational resources are used to develop large-scale machine learning models. However, while training requires substantial power, it is a **one-time cost per model version**. In contrast, inference—the process of applying a trained model to new data—occurs **continuously at scale**, often becoming the **dominant energy consumer over time**. As AI-powered services, such as real-time translation, recommender systems, and generative AI applications, expand globally, inference workloads increasingly contribute to AI’s overall carbon footprint.

#### **The Energy Demands of Training**

Training state-of-the-art AI models requires **enormous compute power and sustained high-performance workloads**. Large-scale models like **GPT-4** were trained using more than **25,000 Nvidia A100 GPUs** running continuously for **90 to 100 days** in cloud-based data centers [@semianalysisGPT4]. OpenAI’s supercomputer, built specifically for AI workloads, consists of **285,000 CPU cores, 10,000 GPUs, and 400 gigabits per second of network bandwidth per server**, highlighting the **massive scale of AI training infrastructure**. These models undergo trillions of **floating-point operations**, consuming thousands of **megawatt-hours (MWh) of electricity** in the process.

High-performance AI accelerators, such as **NVIDIA DGX H100 systems**, are specifically designed for training deep learning models. Each **DGX H100 unit can draw up to 10.2 kW at peak power** [@nvidiadgxH100], and large AI training clusters consist of thousands of interconnected nodes running around the clock. The intense computational load of training results in high **heat dissipation**, requiring extensive cooling infrastructure. At data centers where **thousands of GPUs operate simultaneously**, cooling systems alone account for **30–40% of total energy consumption** [@dayarathna2015data].

While training requires immense power, it is a **one-time process per model version**. The true sustainability challenge of AI arises when models transition into production, where inference workloads **run continuously, serving billions of users worldwide**.

#### **The Growing Energy Cost of Inference**

Inference workloads are executed every time an AI model generates a response, classifies an image, or makes a prediction. Unlike training, inference happens at **massive scale**, with billions of AI-powered interactions occurring **every second** across search engines, social media platforms, and cloud services. While each individual inference request consumes significantly less energy than training, the **cumulative impact of inference energy consumption can exceed that of training over time**.

For example, AI-driven **search engines process billions of queries per day**, recommender systems generate **personalized content for hundreds of millions of users**, and generative AI models like **ChatGPT and DALL-E require substantial compute resources per query**. The carbon footprint of inference is **particularly high for large transformer-based models**, which require **multiple forward passes and high memory bandwidth**, making inference computationally expensive.

Unlike traditional software applications, which have a relatively fixed energy footprint, AI inference workloads **scale dynamically** based on demand. Services such as **Alexa, Siri, and Google Assistant** rely on **real-time cloud-based inference**, where millions of voice queries are processed every minute. The **server farms supporting these applications** must remain operational 24/7, leading to sustained energy consumption at cloud data centers.

#### **Even Edge AI at Scale Has a Significant Impact**

Inference does not always happen in large data centers—**edge AI** is emerging as a viable alternative to reduce cloud dependency. Instead of routing every AI request to centralized cloud servers, some AI models can be deployed **directly on user devices** or **at edge computing nodes**. This approach **reduces data transmission energy costs** and **lowers the dependency on high-power cloud inference**.

However, **running inference at the edge does not eliminate energy concerns**—especially when AI is deployed at scale. **Autonomous vehicles**, for instance, require **millisecond-latency AI inference**, meaning cloud processing is impractical. Instead, vehicles are now being equipped with **onboard AI accelerators** that function as **"data centers on wheels"** [@sudhakar2023data]. These embedded computing systems process **real-time sensor data equivalent to small data centers**, consuming significant power even without relying on cloud inference.

The same issue arises with **AI-powered consumer devices** such as smartphones, wearables, and IoT sensors. While individual devices have relatively low power demands, the **cumulative energy impact** of deploying **billions of AI-powered devices worldwide** becomes substantial. The efficiency benefits of edge computing must therefore be weighed against **the sheer scale of AI deployment**, which can result in a distributed but still significant energy footprint.

#### **Sustainability Strategies for AI Inference**

Optimizing AI for sustainable inference requires both hardware and software innovations. Model quantization techniques enable inference with lower-precision arithmetic, significantly reducing power consumption while maintaining accuracy. Knowledge distillation allows smaller, energy-efficient models to capture the capabilities of larger networks. These optimization approaches, combined with specialized hardware accelerators and efficient model architectures, help minimize the environmental impact of AI inference at scale.

Software optimizations can also significantly reduce energy consumption. Frameworks such as Zeus [@jie2023zeus] and Perseus [@jaewon2023perseus] enable energy-aware AI training, balancing computational speed with power efficiency. These frameworks analyze trade-offs between computation time and energy consumption at multiple system levels, optimizing model execution to reduce power usage without degrading performance. Open-source platforms like Zeus facilitate broader adoption of these techniques, promoting community-driven advancements in energy-efficient AI.

Cloud providers are also **adopting carbon-aware scheduling**, shifting inference workloads to **regions with abundant renewable energy** to minimize carbon emissions. Additionally, **AI accelerators designed for inference**, such as **TPUs optimized for low-power computation**, help reduce the power demands of serving AI models at scale.

While AI training represents a **one-time, high-energy cost**, inference is an **ongoing, growing energy demand** that requires sustainable solutions. As AI-powered applications expand globally, **optimizing inference efficiency is essential to mitigating AI’s long-term carbon footprint**, ensuring that machine learning can scale responsibly while minimizing its environmental impact.

## **Beyond Carbon Footprint**

Efforts to reduce AI’s carbon emissions and energy consumption focus on one critical aspect of sustainability, yet the environmental impact of AI extends far beyond its operational footprint. The **manufacturing of semiconductors and AI hardware** involves significant resource extraction, hazardous chemical usage, and high water consumption—factors that receive comparatively less attention despite their long-term ecological consequences. The development of cutting-edge AI accelerators, GPUs, and other specialized hardware requires an extensive supply chain with **severe environmental implications that go beyond CO₂ emissions**.

The **construction and operation of semiconductor fabrication plants (fabs)** exemplify this broader impact. These facilities, which produce the AI chips powering modern machine learning systems, demand vast amounts of raw materials, chemical inputs, and energy. A **state-of-the-art fabrication facility producing advanced 5nm or 3nm chips may require up to four million gallons of ultrapure water daily** [@wccftech], a consumption level approaching that of a **city with a population of half a million people**. In regions already experiencing **water stress**, such as **Taiwan, Arizona, and Singapore**, the increasing demand for semiconductor manufacturing exacerbates existing shortages, leading to **depleted water tables and ecosystem disruptions**.

Additionally, the **chemical footprint of semiconductor production** is vast, involving over **250 unique hazardous substances** [@mills1997overview]. These include volatile solvents such as **sulfuric acid, nitric acid, hydrogen fluoride, and phosphine gases**, all of which require **extensive handling protocols to prevent environmental contamination**. If not properly managed, these chemicals can **leach into soil and water supplies**, threatening biodiversity and public health. The risks associated with improper waste disposal in semiconductor fabs make **chemical containment and treatment a critical sustainability challenge**.

Beyond its immediate environmental toll, semiconductor manufacturing is also **deeply reliant on scarce materials**, including **gallium, indium, arsenic, and helium**, which are essential for AI accelerators and high-speed communication chips [@chen2006gallium]. These materials face **geopolitical supply risks and depletion concerns**, making long-term sustainability increasingly uncertain. The depletion of these elements, along with the increasing volumes of **hazardous waste generated by AI hardware production**, underscores the **urgent need for sustainable AI infrastructure**.

While reducing AI’s carbon footprint is essential, ignoring the broader **ecological impacts of semiconductor manufacturing risks exacerbating environmental harm**. AI's sustainability must be evaluated **holistically**, accounting for the **full lifecycle of AI hardware, from material extraction and fabrication to disposal and electronic waste management**. The following sections explore these critical yet often overlooked aspects, including **water consumption, hazardous waste production, rare material scarcity, and biodiversity disruption**, to provide a comprehensive understanding of AI’s long-term environmental footprint.

### **Water Usage and Stress**

Semiconductor fabrication is an **exceptionally water-intensive** process, requiring vast quantities of **ultrapure water (UPW)** for cleaning, cooling, and chemical processing. The scale of water consumption in modern fabs is comparable to that of entire urban populations. For example, **TSMC’s latest fab in Arizona is projected to consume 8.9 million gallons of water per day**, accounting for nearly **3% of the city’s total water production** [@wccftech]. This demand places **significant strain on local water resources**, particularly in **water-scarce regions such as Taiwan, Arizona, and Singapore**, where semiconductor manufacturing is concentrated.

The primary use of ultrapure water in semiconductor fabrication is for **flushing contaminants** from wafers at various production stages. Water also serves as a **coolant and carrier fluid** in thermal oxidation, chemical deposition, and planarization processes. A **single 300mm silicon wafer requires over 8,300 liters of water**, with **more than two-thirds of this being ultrapure water** [@cope2009pure]. During peak summer months, the cumulative daily water consumption of major fabs **rivals that of cities with populations exceeding half a million people**.

The impact of this massive water usage extends beyond consumption. Excessive water withdrawal from local aquifers **lowers groundwater levels**, leading to issues such as **land subsidence and saltwater intrusion**. In **Hsinchu, Taiwan**, one of the world's largest semiconductor hubs, extensive water extraction by fabs has led to **falling water tables and encroaching seawater contamination**, affecting both **agriculture and drinking water supplies** [@wccftech].

Figure @fig-water_footprint contextualizes **the daily water footprint of data centers compared to other industrial uses**, illustrating the **immense water demand of high-tech infrastructure**.

![Daily Water Footprint of Datacenters in comparison with other water uses. Source: [Google's Data Center Cooling](https://blog.google/outreach-initiatives/sustainability/our-commitment-to-climate-conscious-data-center-cooling/)](images/png/water_footprint.png){#fig-water_footprint}

While some semiconductor manufacturers implement **water recycling systems**, the effectiveness of these measures varies. Intel reports that **97% of its direct water consumption** is attributed to fabrication processes [@cooper2011semiconductor], and while **water reuse is increasing**, the sheer scale of water withdrawals remains a critical sustainability challenge.

Beyond depletion, **water discharge from semiconductor fabs introduces contamination risks** if not properly managed. Wastewater from fabrication contains **metals, acids, and chemical residues** that must be **thoroughly treated before release**. Although modern fabs employ advanced purification systems, the **extraction of contaminants still generates hazardous byproducts**, which, if not carefully disposed of, **pose risks to local ecosystems**.

The growing demand for semiconductor manufacturing, driven by AI acceleration and computing infrastructure expansion, makes **water management a crucial factor in sustainable AI development**. Ensuring the long-term viability of semiconductor production requires not only **reducing direct water consumption** but also **enhancing wastewater treatment and developing alternative cooling technologies** that minimize reliance on fresh water sources.
