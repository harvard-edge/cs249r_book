---
bibliography: sustainable_ai.bib
---

# Sustainable AI {#sec-sustainable_ai}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-sustainable-ai-resource), [Videos](#sec-sustainable-ai-resource), [Exercises](#sec-sustainable-ai-resource)
:::

![_DALL·E 3 Prompt: 3D illustration on a light background of a sustainable AI network interconnected with a myriad of eco-friendly energy sources. The AI actively manages and optimizes its energy from sources like solar arrays, wind turbines, and hydro dams, emphasizing power efficiency and performance. Deep neural networks spread throughout, receiving energy from these sustainable resources._](images/png/cover_sustainable_ai.png)

## Purpose {.unnumbered}

*_How do environmental considerations influence the design and implementation of machine learning systems, and what principles emerge from examining AI through an ecological perspective?_*

Machine learning systems inherently require significant computational resources, raising critical concerns about their environmental impact. Addressing these concerns requires a deep understanding of how architectural decisions affect energy consumption, resource utilization, and ecological sustainability. Designers and engineers must consider the relationships between computational demands, resource utilization, and environmental consequences across various system components. A systematic exploration of these considerations helps identify key architectural principles and design strategies that harmonize performance objectives with ecological stewardship.

::: {.callout-tip title="Learning Objectives"}

* Coming soon.

:::

## Overview

Machine learning has become an essential driver of technological progress, powering advancements across industries and scientific domains. However, as AI models grow in complexity and scale, the computational demands required to train and deploy them have increased significantly, raising critical concerns about sustainability. The environmental impact of AI extends beyond energy consumption, encompassing carbon emissions, resource extraction, and electronic waste. As a result, it is imperative to examine AI systems through the lens of sustainability and assess the trade-offs between performance and ecological responsibility.

Developing large-scale AI models, such as state-of-the-art language and vision models, requires substantial computational power. Training a single large model can consume thousands of megawatt-hours of electricity, equivalent to powering hundreds of households for a month. Much of this energy is supplied by data centers, which rely heavily on nonrenewable energy sources, contributing to global carbon emissions. Estimates indicate that AI-related emissions are comparable to those of entire industrial sectors, highlighting the urgency of transitioning to more energy-efficient models and renewable-powered infrastructure.

Beyond energy consumption, AI systems also impact the environment through hardware manufacturing and resource utilization. Training and inference workloads depend on specialized processors, such as GPUs and TPUs, which require rare earth metals whose extraction and processing generate significant pollution. Additionally, the growing demand for AI applications accelerates electronic waste production, as hardware rapidly becomes obsolete. Even small-scale AI systems, such as those deployed on edge devices, contribute to sustainability challenges, necessitating careful consideration of their lifecycle impact.

This chapter examines the sustainability challenges associated with AI systems and explores emerging solutions to mitigate their environmental footprint. It discusses strategies for improving algorithmic efficiency, optimizing training infrastructure, and designing energy-efficient hardware. Additionally, it considers the role of renewable energy sources, regulatory frameworks, and industry best practices in promoting sustainable AI development. By addressing these challenges, the field can advance toward more ecologically responsible AI systems while maintaining technological progress.

## Social and Ethical Responsibility

### Long-Term Sustainability

The long-term sustainability of AI is increasingly challenged by the exponential growth of computational demands required to train and deploy machine learning models. Over the past decade, AI systems have scaled at an unprecedented rate, with compute requirements increasing 350,000× from 2012 to 2019 [@schwartz2020green]. This trend shows no signs of slowing down, as advancements in deep learning continue to prioritize larger models with more parameters, larger training datasets, and higher computational complexity. However, sustaining this trajectory poses significant sustainability challenges, particularly as the efficiency gains from hardware improvements fail to keep pace with the rising demands of AI workloads.

Historically, computational efficiency improved with advances in semiconductor technology. Moore’s Law, which predicted that the number of transistors on a chip would double approximately every two years, led to continuous improvements in processing power and energy efficiency. However, Moore’s Law is now reaching fundamental physical limits, making further transistor scaling increasingly difficult and costly. Dennard scaling, which once ensured that smaller transistors would operate at lower power levels, has also ended, leading to stagnation in energy efficiency improvements per transistor. As a result, while AI models continue to scale in size and capability, the hardware running these models is no longer improving at the same exponential rate. This growing divergence between computational demand and hardware efficiency creates an unsustainable trajectory in which AI consumes ever-increasing amounts of energy.

This rising energy demand is particularly evident in the training of state-of-the-art models. For example, training GPT-3 was estimated to require 1,300 megawatt-hours (MWh) of electricity, roughly equivalent to the monthly energy consumption of 1,450 average U.S. households [@maslej2023artificial]. Similarly, AI-powered applications such as large-scale recommender systems and generative models require continuous inference at scale, consuming significant energy even after training is complete. As AI adoption grows across industries, from finance to healthcare to entertainment, the cumulative energy burden of AI workloads will continue to increase, raising concerns about the environmental impact of widespread deployment.

Beyond electricity consumption, the sustainability challenges of AI extend to hardware resource demands. High-performance computing (HPC) clusters and AI accelerators rely on specialized hardware, including GPUs, TPUs, and FPGAs, all of which require rare earth metals and complex manufacturing processes. The production of AI chips is energy-intensive, involving multiple fabrication steps that contribute to Scope 3 emissions, which account for the majority of the carbon footprint in semiconductor manufacturing. As model sizes continue to grow, the demand for AI hardware increases, further exacerbating the environmental impact of semiconductor production and disposal.

The long-term sustainability of AI requires a shift in how machine learning systems are designed, optimized, and deployed. As compute demands outpace efficiency improvements, addressing AI’s environmental impact will require rethinking system architecture, energy-aware computing, and lifecycle management. Without intervention, the unchecked growth of AI models will continue to place unsustainable pressures on energy grids, data centers, and natural resources, underscoring the need for a more systematic approach to sustainable AI development.

The environmental impact of AI is not just a technical issue but also an ethical and social one. As AI becomes more integrated into our lives and industries, its sustainability becomes increasingly critical.

### **Ethical Considerations**

The environmental impact of AI raises fundamental ethical questions regarding the responsibility of developers, organizations, and policymakers to mitigate its carbon footprint. As AI systems continue to scale, their energy consumption and resource demands have far-reaching implications, necessitating a proactive approach to sustainability. Developers and companies that build and deploy AI systems must consider not only performance and efficiency but also the broader environmental consequences of their design choices.

A key ethical challenge lies in balancing **technological progress with ecological responsibility**. The pursuit of increasingly large models often prioritizes accuracy and capability over energy efficiency, leading to substantial environmental costs. While optimizing for sustainability may introduce trade-offs—such as increased development time or minor reductions in accuracy—it is an ethical imperative to integrate environmental considerations into AI system design. This requires shifting industry norms toward sustainable computing practices, such as energy-aware training techniques, low-power hardware designs, and carbon-conscious deployment strategies.

Beyond sustainability, AI development also raises **broader ethical concerns** related to transparency, fairness, and accountability. Figure @fig-ethical-ai illustrates the **ethical challenges associated with AI development**, linking different types of concerns—such as inscrutable evidence, unfair outcomes, and traceability—to issues like opacity, bias, and automation bias. These concerns extend to sustainability, as the environmental trade-offs of AI development are often opaque and difficult to quantify. The lack of **traceability** in energy consumption and carbon emissions can lead to unjustified actions, where companies prioritize performance gains without fully understanding or disclosing the environmental costs.

Addressing these concerns also demands greater transparency and accountability from AI companies. Large technology firms operate extensive cloud infrastructures that power modern AI applications, yet their environmental impact is often opaque. Organizations must take active steps to measure, report, and reduce their carbon footprint across the entire AI lifecycle, from hardware manufacturing to model training and inference. Voluntary self-regulation is an important first step, but **policy interventions and industry-wide standards** may be necessary to ensure long-term sustainability. Reported metrics such as energy consumption, carbon emissions, and efficiency benchmarks could serve as mechanisms to hold organizations accountable.

Furthermore, ethical AI development must encourage open discourse on environmental trade-offs. Researchers should be empowered to **advocate for sustainability within their institutions and organizations**, ensuring that environmental concerns are factored into AI development priorities. The broader AI community has already begun addressing these issues, as exemplified by the [open letter advocating a pause on large-scale AI experiments](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), which highlights concerns about unchecked expansion. By fostering a culture of transparency and ethical responsibility, the AI industry can work toward aligning technological advancement with ecological sustainability.

AI has the potential to reshape industries and societies, but its long-term viability depends on how responsibly it is developed. Ethical AI development is not only about preventing harm to individuals and communities but also about ensuring that AI-driven innovation does not come at the cost of environmental degradation. As stewards of these powerful technologies, developers and organizations have a profound duty to integrate sustainability into AI’s future trajectory.

### **Case Study: DeepMind's AI for AI Energy Efficiency**

Google's data centers form the backbone of services such as Search, Gmail, and YouTube, handling billions of queries daily. These data centers operate at massive scales, consuming vast amounts of electricity, particularly for cooling infrastructure that ensures optimal server performance. Improving the **energy efficiency** of data centers has long been a priority, but conventional engineering approaches faced diminishing returns due to the complexity of the cooling systems and the highly dynamic nature of environmental conditions. To address these challenges, Google collaborated with DeepMind to develop a machine learning-driven optimization system that could **automate and enhance energy management at scale**.

Building on more than a decade of efforts to optimize **data center design, energy-efficient hardware, and renewable energy integration**, DeepMind’s AI approach targeted one of the most energy-intensive aspects of data centers: **cooling systems**. Traditional cooling relies on manually set heuristics that account for factors such as server heat output, external weather conditions, and architectural constraints. However, these systems exhibit **nonlinear interactions**, meaning that simple rule-based optimizations often fail to capture the full complexity of their operations. The result was **suboptimal cooling efficiency**, leading to unnecessary energy waste.

DeepMind’s team trained a neural network model using Google's **historical sensor data**, which included real-time temperature readings, power consumption levels, cooling pump activity, and other operational parameters. The model learned the intricate relationships between these factors and could dynamically predict **the most efficient cooling configurations**. Unlike traditional approaches, which relied on human engineers periodically adjusting system settings, the AI model **continuously adapted in real time** to changing environmental and workload conditions.

The results were **unprecedented efficiency gains**. When deployed in live data center environments, DeepMind’s AI-driven cooling system **reduced cooling energy consumption by 40%**, leading to an overall **15% improvement in Power Usage Effectiveness (PUE)**—a key metric for data center energy efficiency that measures the ratio of total energy consumption to the energy used purely for computing tasks [@barroso2019datacenter]. Notably, these improvements were achieved without any additional hardware modifications, demonstrating the potential of **software-driven optimizations** to significantly reduce AI’s carbon footprint.

Beyond a single data center, DeepMind’s AI model provided a **generalizable framework** that could be adapted to different facility designs and climate conditions, offering a scalable solution for optimizing power consumption across **global data center networks**. This case study exemplifies how AI can be leveraged **not just as a consumer of computational resources but as a tool for sustainability**, driving substantial efficiency improvements in the infrastructure that supports machine learning itself.

The integration of **data-driven decision-making, real-time adaptation, and scalable AI models** demonstrates the growing role of **intelligent resource management in sustainable AI system design**. This breakthrough exemplifies how **machine learning can be applied to optimize the very infrastructure that powers it**, ensuring a **more energy-efficient future for large-scale AI deployments**.

## **Understanding AI’s Carbon Footprint**

The carbon footprint of artificial intelligence is a critical aspect of its overall environmental impact. As AI adoption continues to expand, so does its **energy consumption and associated greenhouse gas emissions**. Training and deploying AI models require vast computational resources, often powered by energy-intensive data centers that contribute significantly to global carbon emissions. However, the carbon footprint of AI extends beyond electricity usage, encompassing **hardware manufacturing, data storage, and end-user interactions**—all of which contribute to emissions across an AI system’s lifecycle.

Quantifying the carbon impact of AI is complex, as it depends on multiple factors, including the **size of the model, the duration of training, the hardware used, and the energy sources powering data centers**. Large-scale AI models, such as **GPT-3**, require thousands of **megawatt-hours (MWh) of electricity**, equivalent to the energy consumption of entire communities. The energy required for **inference**—the phase in which trained models generate outputs—is also substantial, particularly for widely deployed AI services such as **real-time translation, image generation, and personalized recommendations**. Unlike traditional software, which has a relatively static energy footprint, AI models **consume energy continuously**, leading to an ongoing sustainability challenge.

Beyond direct energy use, the carbon footprint of AI must also account for **indirect emissions from hardware production and supply chains**. Manufacturing AI accelerators such as **GPUs, TPUs, and custom chips** involves energy-intensive fabrication processes that rely on rare earth metals and complex supply chains. The **full life cycle emissions of AI systems—encompassing data centers, hardware manufacturing, and global AI deployments—must be considered to develop more sustainable AI practices**.

Understanding AI’s carbon footprint requires breaking down **where emissions come from, how they are measured, and what strategies can be employed to mitigate them**. This section explores:

- **Carbon emissions and energy consumption trends in AI**—quantifying AI’s energy demand and providing real-world comparisons.
- **Scopes of carbon emissions (Scope 1, 2, and 3)**—differentiating between direct, indirect, and supply chain-related emissions.
- **The energy cost of training vs. inference**—analyzing how different phases of AI impact sustainability.

By dissecting these components, we can better assess **the true environmental impact of AI systems and identify opportunities to reduce their footprint** through more efficient design, energy-conscious deployment, and sustainable infrastructure choices.

### **Carbon Emissions & Energy Consumption**

Artificial intelligence systems require vast computational resources, making them one of the most energy-intensive workloads in modern computing. The energy consumed by AI systems extends beyond the training of large models to include ongoing inference workloads, data storage, and communication across distributed computing infrastructure. As AI adoption scales across industries, understanding its energy consumption patterns and carbon emissions is critical for designing more sustainable machine learning infrastructure.

Training state-of-the-art machine learning models demands enormous amounts of compute power, leading to high energy consumption. The scale of this demand is evident in models like GPT-3, which required an estimated 1,300 megawatt-hours (MWh) of electricity—equivalent to the monthly energy consumption of 1,450 U.S. households. The latest iteration, **GPT-4, was trained using more than 25,000 Nvidia A100 GPUs running continuously for over 90 to 100 days**, housed in specialized **Azure data centers optimized for AI workloads** [@semianalysisGPT4]. The sheer scale of AI compute infrastructure, exemplified by such massive clusters, raises concerns about sustainability as energy demands continue to rise.

The increasing reliance on AI services has also led to rapid growth in **data center infrastructure**, where large-scale computing operations are performed. Companies such as Meta operate data centers **spanning multiple football fields in size** [@facebookdatacenters], housing hundreds of thousands of high-performance AI servers. These facilities not only support AI training workloads but also power real-time inference for consumer AI applications. Virtual assistants such as **Alexa, Siri, and Google Assistant** process billions of user queries per month, leveraging data centers for low-latency responses. The future of AI, particularly in areas such as **autonomous vehicles, precision medicine, and climate forecasting**, will depend on vast **on-demand cloud computing resources**, further increasing the energy footprint of data centers.

AI’s growing compute demands place increasing pressure on data centers, which are responsible for a significant portion of the industry’s carbon emissions. The energy required to operate these facilities can be divided into four primary components: **infrastructure, networking, storage, and compute servers**. Research from the International Energy Agency (IEA) highlights that **cooling and computing servers account for the largest share of total energy consumption in data centers** [@shehabi2016united]. AI-optimized servers, such as NVIDIA **DGX H100** machines, can consume up to **10.2 kW per unit** at peak power [@nvidiadgxH100]. At large cloud facilities, thousands of such power-hungry nodes are interconnected to train the latest AI models, creating an energy-intensive cycle that **runs continuously, 24/7, year-round**. The total power consumption of data centers housing AI compute can reach into the tens or even hundreds of megawatts, rivaling the energy demands of entire cities.

Cooling represents another major energy cost in AI infrastructure, as densely packed GPUs and networking equipment generate massive amounts of heat that must be dissipated to maintain system stability. Estimates indicate that **cooling alone accounts for 30–40% of the total electricity consumption of AI data centers** [@dayarathna2015data]. Companies have experimented with alternative cooling solutions to reduce this energy demand. For example, **Microsoft’s data center in Ireland leverages a nearby fjord, using over half a million gallons of seawater daily** to dissipate heat [@microsoftcooling]. Similar innovations in **liquid cooling and AI-driven thermal optimization** are being explored to improve efficiency, but as AI models continue to grow in complexity, cooling demands will remain a major sustainability challenge.

The environmental impact of AI extends beyond electricity consumption, as training and inference workloads are only part of the equation. The materials and hardware that power AI, including **semiconductors, networking infrastructure, and storage components**, require energy-intensive manufacturing and supply chains. The next section explores how AI’s carbon footprint can be categorized into **direct, indirect, and supply chain emissions (Scope 1, Scope 2, and Scope 3)** to provide a more comprehensive understanding of its environmental impact.

### **Scopes of Carbon Emissions**

The carbon footprint of AI extends far beyond direct electricity consumption. To fully account for AI’s environmental impact, it is essential to consider emissions generated across the entire lifecycle of AI systems. The **Greenhouse Gas (GHG) Protocol**, an internationally recognized framework for measuring emissions, classifies carbon output into three categories: **Scope 1 (direct emissions), Scope 2 (indirect emissions from purchased electricity), and Scope 3 (supply chain and end-user emissions)**. These categories provide a structured approach to understanding how AI contributes to climate change and where efforts to mitigate its impact should be focused.

Scope 1 emissions refer to the **direct greenhouse gas emissions produced by AI data centers and computing facilities**. This includes emissions from on-site power generation, backup diesel generators used for reliability in large cloud environments, and other direct energy-consuming operations. While AI data centers increasingly rely on grid electricity, some facilities operate their own power plants or backup systems, contributing directly to carbon emissions. These emissions are typically lower in regions where data centers use cleaner energy sources, but in locations reliant on fossil fuels, they can be substantial.

Scope 2 emissions encompass the **indirect emissions generated from electricity purchased to power AI infrastructure**. The majority of AI’s operational energy consumption falls under this category, as cloud providers and enterprise computing facilities rely on massive electrical inputs to run GPUs, TPUs, and high-density servers. The carbon intensity of Scope 2 emissions depends largely on the energy mix of the region where AI workloads are executed. In areas where coal and natural gas dominate electricity generation, AI systems have a significantly higher carbon footprint than in locations where renewable energy sources such as wind, hydro, and solar are more prevalent. This geographic variation has led some technology companies to strategically locate data centers in regions with cleaner energy sources, leveraging carbon-aware scheduling techniques to minimize emissions.

Scope 3 emissions represent **the largest and most complex category, accounting for all indirect emissions throughout the AI supply chain**. These emissions originate from the manufacturing, transportation, and disposal of AI hardware, including the production of GPUs, CPUs, TPUs, and memory modules. The fabrication of semiconductor chips used in AI accelerators is an energy-intensive process that requires rare earth metals, chemical etching, and extreme ultraviolet (EUV) lithography, all of which contribute to substantial carbon output. Industry reports suggest that the carbon emissions associated with manufacturing a single high-performance AI chip can be equivalent to **multiple years’ worth of operational energy consumption**, making hardware production a critical yet often overlooked factor in AI’s sustainability equation.

Beyond manufacturing, Scope 3 emissions also include **the downstream impact of AI models once deployed**. Many AI-driven services, such as large-scale recommendation engines, search algorithms, and generative AI applications, require continuous inference at scale, leading to sustained energy consumption even after training is complete. For consumer-facing AI products, the electricity used by end-user devices—smartphones, laptops, and edge computing hardware—contributes to Scope 3 emissions, extending AI’s carbon footprint beyond centralized cloud data centers. In some cases, Scope 3 emissions can account for the **majority** of an AI system’s total environmental impact, as seen in companies with widely deployed AI-powered services, such as social media platforms and video streaming applications.

A clearer picture of AI’s carbon emissions emerges when analyzing corporate sustainability reports from major technology firms. Companies such as Google, Microsoft, Meta, and semiconductor manufacturers like TSMC have begun publishing emissions breakdowns, revealing that while Scope 1 and Scope 2 emissions are relatively well-documented, Scope 3 remains **the most difficult to quantify** due to its complexity and interdependencies across global supply chains. Intel’s sustainability report, for example, highlights that while manufacturing contributes a significant portion of emissions, the total environmental impact is distributed across the full hardware lifecycle.

The **GHG Protocol framework**, illustrated in **Figure @fig-ghg-protocol**, provides a structured way to visualize the sources of AI-related carbon emissions. Scope 1 emissions arise from direct company operations, such as data center power generation and company-owned infrastructure. Scope 2 covers electricity purchased from the grid, the primary source of emissions for cloud computing workloads. Scope 3 extends beyond an organization’s direct control, including emissions from hardware manufacturing, transportation, and even the end-user energy consumption of AI-powered services. Understanding this breakdown allows for **more targeted sustainability strategies**, ensuring that efforts to reduce AI’s environmental impact are not solely focused on energy efficiency but also address the broader supply chain and lifecycle emissions that contribute significantly to the industry’s carbon footprint.

![The GHG Protocol framework categorizes emissions into Scope 1, 2, and 3, helping organizations assess their direct and indirect carbon impact. Source: Circularise.](images/png/ghg_protocol.png){#fig-ghg-protocol}

### **Training vs. Inference Carbon Impact**

The energy consumption of AI systems is often associated with the training phase, where vast computational resources are used to develop large-scale machine learning models. However, while training requires substantial power, it is a **one-time cost per model version**. In contrast, inference—the process of applying a trained model to new data—occurs **continuously at scale**, often becoming the **dominant energy consumer over time**. As AI-powered services, such as real-time translation, recommender systems, and generative AI applications, expand globally, inference workloads increasingly contribute to AI’s overall carbon footprint.

#### **The Energy Demands of Training**

Training state-of-the-art AI models requires **enormous compute power and sustained high-performance workloads**. Large-scale models like **GPT-4** were trained using more than **25,000 Nvidia A100 GPUs** running continuously for **90 to 100 days** in cloud-based data centers [@semianalysisGPT4]. OpenAI’s supercomputer, built specifically for AI workloads, consists of **285,000 CPU cores, 10,000 GPUs, and 400 gigabits per second of network bandwidth per server**, highlighting the **massive scale of AI training infrastructure**. These models undergo trillions of **floating-point operations**, consuming thousands of **megawatt-hours (MWh) of electricity** in the process.

High-performance AI accelerators, such as **NVIDIA DGX H100 systems**, are specifically designed for training deep learning models. Each **DGX H100 unit can draw up to 10.2 kW at peak power** [@nvidiadgxH100], and large AI training clusters consist of thousands of interconnected nodes running around the clock. The intense computational load of training results in high **heat dissipation**, requiring extensive cooling infrastructure. At data centers where **thousands of GPUs operate simultaneously**, cooling systems alone account for **30–40% of total energy consumption** [@dayarathna2015data].

While training requires immense power, it is a **one-time process per model version**. The true sustainability challenge of AI arises when models transition into production, where inference workloads **run continuously, serving billions of users worldwide**.

#### **The Growing Energy Cost of Inference**

Inference workloads are executed every time an AI model generates a response, classifies an image, or makes a prediction. Unlike training, inference happens at **massive scale**, with billions of AI-powered interactions occurring **every second** across search engines, social media platforms, and cloud services. While each individual inference request consumes significantly less energy than training, the **cumulative impact of inference energy consumption can exceed that of training over time**.

For example, AI-driven **search engines process billions of queries per day**, recommender systems generate **personalized content for hundreds of millions of users**, and generative AI models like **ChatGPT and DALL-E require substantial compute resources per query**. The carbon footprint of inference is **particularly high for large transformer-based models**, which require **multiple forward passes and high memory bandwidth**, making inference computationally expensive.

Unlike traditional software applications, which have a relatively fixed energy footprint, AI inference workloads **scale dynamically** based on demand. Services such as **Alexa, Siri, and Google Assistant** rely on **real-time cloud-based inference**, where millions of voice queries are processed every minute. The **server farms supporting these applications** must remain operational 24/7, leading to sustained energy consumption at cloud data centers.

#### **Even Edge AI at Scale Has a Significant Impact**

Inference does not always happen in large data centers—**edge AI** is emerging as a viable alternative to reduce cloud dependency. Instead of routing every AI request to centralized cloud servers, some AI models can be deployed **directly on user devices** or **at edge computing nodes**. This approach **reduces data transmission energy costs** and **lowers the dependency on high-power cloud inference**.

However, **running inference at the edge does not eliminate energy concerns**—especially when AI is deployed at scale. **Autonomous vehicles**, for instance, require **millisecond-latency AI inference**, meaning cloud processing is impractical. Instead, vehicles are now being equipped with **onboard AI accelerators** that function as **"data centers on wheels"** [@sudhakar2023data]. These embedded computing systems process **real-time sensor data equivalent to small data centers**, consuming significant power even without relying on cloud inference.

The same issue arises with **AI-powered consumer devices** such as smartphones, wearables, and IoT sensors. While individual devices have relatively low power demands, the **cumulative energy impact** of deploying **billions of AI-powered devices worldwide** becomes substantial. The efficiency benefits of edge computing must therefore be weighed against **the sheer scale of AI deployment**, which can result in a distributed but still significant energy footprint.

#### **Sustainability Strategies for AI Inference**

Optimizing AI for sustainable inference requires both hardware and software innovations. Model quantization techniques enable inference with lower-precision arithmetic, significantly reducing power consumption while maintaining accuracy. Knowledge distillation allows smaller, energy-efficient models to capture the capabilities of larger networks. These optimization approaches, combined with specialized hardware accelerators and efficient model architectures, help minimize the environmental impact of AI inference at scale.

Software optimizations can also significantly reduce energy consumption. Frameworks such as Zeus [@jie2023zeus] and Perseus [@jaewon2023perseus] enable energy-aware AI training, balancing computational speed with power efficiency. These frameworks analyze trade-offs between computation time and energy consumption at multiple system levels, optimizing model execution to reduce power usage without degrading performance. Open-source platforms like Zeus facilitate broader adoption of these techniques, promoting community-driven advancements in energy-efficient AI.

Cloud providers are also **adopting carbon-aware scheduling**, shifting inference workloads to **regions with abundant renewable energy** to minimize carbon emissions. Additionally, **AI accelerators designed for inference**, such as **TPUs optimized for low-power computation**, help reduce the power demands of serving AI models at scale.

While AI training represents a **one-time, high-energy cost**, inference is an **ongoing, growing energy demand** that requires sustainable solutions. As AI-powered applications expand globally, **optimizing inference efficiency is essential to mitigating AI’s long-term carbon footprint**, ensuring that machine learning can scale responsibly while minimizing its environmental impact.
