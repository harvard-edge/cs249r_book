---
bibliography: responsible_ai.bib
---

# Responsible AI

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-responsible-ai-resource), [Videos](#sec-responsible-ai-resource), [Exercises](#sec-responsible-ai-resource)
:::

![_DALL·E 3 Prompt: Illustration of responsible AI in a futuristic setting with the universe in the backdrop: A human hand or hands nurturing a seedling that grows into an AI tree, symbolizing a neural network. The tree has digital branches and leaves, resembling a neural network, to represent the interconnected nature of AI. The background depicts a future universe where humans and animals with general intelligence collaborate harmoniously. The scene captures the initial nurturing of the AI as a seedling, emphasizing the ethical development of AI technology in harmony with humanity and the universe._](images/png/cover_responsible_ai.png)

## Purpose {.unnumbered}

_How do human values translate into machine learning systems architecture, and what principles enable responsible system behavior at scale?_

Machine learning systems do not exist in isolation—they operate within social, economic, and technical environments where their outputs affect people and institutions. As these systems grow in capability and reach, questions of responsibility become central to their design. The integration of fairness, transparency, and accountability is not an afterthought but a systems-level constraint that shapes data pipelines, model architectures, and deployment strategies. Recognizing the moral dimension of engineering choices is essential for building machine learning systems that serve human needs, avoid harm, and support long-term trust in automation.

::: {.callout-tip title="Learning Objectives"}

* Understand responsible AI's core principles and motivations, including fairness, transparency, privacy, safety, and accountability.

* Learn technical methods for implementing responsible AI principles, such as detecting dataset biases, building interpretable models, adding noise for privacy, and testing model robustness.

* Recognize organizational and social challenges to achieving responsible AI, including data quality, model objectives, communication, and job impacts.

* Knowledge of ethical frameworks and considerations for AI systems, spanning AI safety, human autonomy, and economic consequences.

* Appreciate the increased complexity and costs of developing ethical, trustworthy AI systems compared to unprincipled AI.

:::

## Overview

Machine learning systems are increasingly deployed in high-stakes domains such as healthcare, criminal justice, and employment. As their influence expands, so do the risks of embedding bias, compromising privacy, and enabling unintended harms. For example, a loan approval model trained exclusively on data from high-income neighborhoods may unfairly penalize applicants from underrepresented communities, reinforcing structural inequities.

Responsible machine learning aims to mitigate such outcomes by integrating ethical principles, such as fairness, transparency, accountability, and safety, into system design and operation. Fairness seeks to prevent discriminatory outcomes; explainability allows practitioners and users to interpret model behavior; robustness helps defend against adversarial manipulation and edge-case failures; and thorough validation supports trustworthy deployment.

Implementing these principles presents deep technical and organizational challenges. Engineers must grapple with mathematically defining fairness, reconciling competing objectives such as accuracy versus interpretability, and ensuring representative and reliable data pipelines. At the same time, institutions must align policies, incentives, and governance frameworks to uphold ethical development and deployment practices.

This chapter provides the foundations for understanding and implementing responsible machine learning. By examining the technical methods, design trade-offs, and broader system implications, it equips you to critically evaluate AI systems and contribute to their development in a way that advances both capability and human values.

## Core Principles

Responsible AI refers to the development and deployment of machine learning systems that intentionally uphold ethical principles and promote socially beneficial outcomes. These principles serve not only as policy ideals but as concrete constraints on system design, implementation, and governance.

**Fairness** refers to the expectation that machine learning systems do not discriminate against individuals or groups on the basis of protected attributes such as race, gender, or socioeconomic status. Fairness encompasses both statistical definitions—such as demographic parity or equalized odds—and normative concerns about equity, justice, and structural bias.

**Explainability** concerns the ability of stakeholders to interpret how a model produces its outputs. This may involve post hoc explanations for individual decisions or global understanding of a model’s behavior. Explainability is essential for error analysis, regulatory compliance, and building user trust.

**Transparency** refers to openness about how AI systems are built, trained, validated, and deployed. It includes disclosure of data sources, design assumptions, system limitations, and performance characteristics. While explainability focuses on understanding outputs, transparency addresses the broader lifecycle of the system.

**Accountability** denotes the mechanisms by which individuals or organizations are held responsible for the outcomes of AI systems. It involves traceability, documentation, auditing, and the ability to remedy harms. Accountability ensures that AI failures are not treated as abstract malfunctions but as consequences with real-world impact.

**Value Alignment** is the principle that AI systems should pursue goals that are consistent with human intent and ethical norms. In practice, this involves both technical challenges—such as reward design and constraint specification—and broader questions about whose values are represented and enforced.

**Human Oversight** emphasizes the role of human judgment in supervising, correcting, or halting automated decisions. This includes humans-in-the-loop during operation, as well as organizational structures that ensure AI use remains accountable to societal values and real-world complexity.

Other essential principles such as **privacy** and **robustness** are discussed in dedicated chapters on [Security and Privacy](../privacy_security/privacy_security.qmd) and [Robust AI](../robust_ai/robust_ai.qmd), where their technical implementations and risks are explored in greater depth.

## Principles in Practice

The principles of responsible AI such as fairness, explainability, accountability, and others must be instantiated through concrete mechanisms within machine learning systems. Their application extends beyond abstract definitions, requiring deliberate design choices and operational strategies that respond to the constraints of real-world deployment. In practice, implementing these principles entails navigating tradeoffs between competing objectives, accommodating domain-specific requirements, and addressing the broader societal and institutional contexts in which AI systems operate. We examine how responsible AI principles are operationalized, with attention to technical methods, regulatory frameworks, and system-level consequences.

### Transparency and Explainability

Machine learning systems are frequently criticized for their opacity. In many cases, models function as "black boxes," producing outputs that are difficult for users, developers, and regulators to interpret. This lack of transparency presents a significant barrier to trust, especially in high-stakes domains where accountability and recourse are essential. For example, the [COMPAS](https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx) algorithm, used in the United States to assess the risk of criminal recidivism, was found to exhibit racial bias. Yet the proprietary nature of the model and the absence of interpretability tools made it difficult to investigate or remediate the problem.

Explainability refers to the ability to understand how a machine learning model arrives at its predictions. This can involve local explanations—providing insight into a single prediction—or global explanations that describe the overall behavior of the model. Techniques such as [LIME](https://homes.cs.washington.edu/~marcotcr/blog/lime/) (Local Interpretable Model-Agnostic Explanations [@ribeiro2016should]) construct locally linear approximations to explain individual predictions. Shapley values, derived from cooperative game theory, measure the marginal contribution of each input feature to the model’s output. For image-based models, saliency maps visually highlight regions that most influenced the classification decision, offering intuitive insights into model reasoning.

Transparency, by contrast, encompasses the broader system context. It includes disclosure of the training data sources, model architecture, evaluation protocols, known limitations, and deployment constraints. Transparency also involves clear documentation of intended use cases, access controls, and performance bounds. In systems deployed in regulated industries, such as finance, healthcare, and education, transparency enables external auditing, risk management, and post-deployment review.

Explainability and transparency are not only best practices; in many jurisdictions, they are legal obligations. The [General Data Protection Regulation (GDPR)](https://gdpr.eu/tag/gdpr/) in the European Union requires that individuals receive “meaningful information” about the logic involved in automated decisions that significantly affect them. This has made explainability an operational requirement for systems that impact credit scoring, hiring, and access to public services.

The appropriate form of explanation depends on the system’s users and domain. A clinician interpreting a diagnostic model may need different types of evidence than a data scientist debugging a production pipeline. Consequently, explainability must be aligned with system constraints such as latency, compute budget, and user interface capabilities. In edge deployments, for instance, saliency-based methods may be favored over sampling-based techniques due to their lower computational overhead.

Explainability and transparency are also critical for system debugging and governance. When models fail or behave unexpectedly, interpretability tools help isolate root causes and improve accountability. For example, during model retraining, shifts in attribution maps or concept activations can signal unintended behavior drift. Without transparent documentation of prior system states, such regressions may be difficult to detect or correct.

As with fairness, explainability is not merely a model-level concern. It is a property of the full machine learning system. The ability to interpret predictions, trace decision paths, and audit system behavior depends on choices made throughout the pipeline—from feature selection and data preprocessing to logging, monitoring, and deployment interface design. Responsible AI requires that explainability and transparency be treated as architectural priorities, integrated from the outset rather than added post hoc.

### Fairness in Machine Learning

Fairness in machine learning refers to the principle that automated systems should not disproportionately disadvantage individuals or groups based on protected attributes such as race, gender, age, or socioeconomic status. Because machine learning systems are trained on historical data, they are susceptible to reproducing patterns of systemic bias embedded in that data. Without careful design, such systems may inadvertently reinforce inequality rather than mitigate it.

A widely cited example comes from healthcare. An algorithm used to allocate care management resources in U.S. hospitals systematically underestimated the health needs of Black patients [@obermeyer2019dissecting]. The model used healthcare expenditures as a proxy for health status, but due to long-standing disparities in access and spending, Black patients were less likely to incur high costs. As a result, the model learned that they were less sick, when in fact they often had equal or greater medical need. This case illustrates how biased proxies in the training data can lead to harmful downstream decisions.

To assess and improve fairness, machine learning researchers have developed formal criteria that quantify how a model behaves across groups. Suppose a model $h(x)$ predicts a binary outcome (e.g., loan repayment), and let $S$ be a sensitive attribute such as gender, with subgroups $a$ and $b$. The following fairness metrics are widely used in practice:

#### Demographic Parity

This criterion requires the model to assign positive predictions at equal rates across groups:

$$
P(h(x) = 1 \mid S = a) = P(h(x) = 1 \mid S = b).
$$

This definition ensures that access to beneficial outcomes, such as loans or job offers, is not correlated with group membership. However, it does not account for differences in true labels across groups, and may unintentionally favor uniform outcomes over equitable ones.

#### Equalized Odds

This metric compares model performance conditional on the true label. It requires both the true positive rate and the false positive rate to be equal across groups:

$$
P(h(x) = 1 \mid S = a, Y = y) = P(h(x) = 1 \mid S = b, Y = y), \quad \text{for } y \in \{0, 1\}.
$$

Equalized odds ensures that the model behaves similarly for individuals who are actually in the same category, such as repaying or defaulting on a loan, regardless of group membership. It is often used in criminal justice and healthcare, where disparities in error rates can have particularly harmful consequences.

#### Equality of Opportunity

A relaxed version of equalized odds, this criterion focuses only on true positive rates:

$$
P(h(x) = 1 \mid S = a, Y = 1) = P(h(x) = 1 \mid S = b, Y = 1).
$$

It is most appropriate in settings where access to a beneficial outcome (e.g., job offers, treatments, housing) is the primary concern. This metric emphasizes equal treatment among those who are equally qualified or in need.

Each of these definitions formalizes a different aspect of fairness, and they are generally incompatible. For example, a model satisfying demographic parity may not meet equalized odds, and vice versa. These tensions reflect the deeper reality that fairness is not a singular objective but a set of competing goals. Deciding which metric to apply requires careful consideration of context, values, and potential downstream effects.

In practice, fairness interventions occur at multiple stages of the machine learning pipeline. Preprocessing methods aim to correct bias in the input data by reweighting, sampling, or synthesizing examples from underrepresented groups. In-processing approaches alter the learning algorithm itself, such as through fairness-aware loss functions or constrained optimization. Post-processing techniques adjust the model’s predictions after training—for instance, by applying different thresholds to different groups to equalize outcomes.

All of these approaches introduce tradeoffs. A threshold adjustment that increases access for one group may increase false positives. Rebalancing training data may reduce predictive performance on overrepresented groups. Importantly, most definitions focus on predefined demographic groups and may fail to capture intersectional identities or subtle structural harms. More recent work on *multicalibration* [@hebert2018multicalibration] aims to address this by requiring calibration across many overlapping subgroups, enabling more fine-grained fairness guarantees.

Fairness cannot be reduced to a mathematical condition alone. It requires attention to the system’s purpose, the domain’s history, and the needs and perspectives of affected stakeholders. This includes understanding how data were collected, what proxies are being used, and what consequences follow from false positives or false negatives. Responsible AI design therefore requires integrating fairness throughout the system lifecycle—not as a compliance step, but as a design constraint.

Fairness considerations must also be treated as part of system architecture. Decisions about model objectives, training regimes, data sampling, thresholding, and feedback mechanisms all influence how fairness is realized—or violated—in deployed systems. For example, choosing between a threshold-based classifier and a ranking model may lead to different resource allocations across groups. Similarly, the interfaces between data ingestion, model selection, and evaluation must be designed to surface disparities rather than obscure them. In this way, fairness emerges not from any single component, but from the interactions and tradeoffs embedded in the system as a whole.

### Privacy and Data Governance

Machine learning systems often rely on large volumes of personal data to train models and deliver personalized services. As a result, they raise critical questions about user privacy, data protection, and ethical data stewardship. Responsible AI systems must incorporate privacy as a fundamental constraint—not only to comply with regulations, but also to maintain user trust and reduce systemic risk.

One of the core challenges in privacy-preserving machine learning is the tension between **data utility** and **individual protection**. Rich, granular datasets can improve model performance but may expose sensitive information, especially when aggregated or combined with external data sources. For example, language models trained on chat logs or medical notes can memorize specific phrases or facts, which may later be revealed through model queries or adversarial prompts [@carlini2023extracting_llm].

In some domains, the use of seemingly innocuous data can lead to unexpected privacy violations. Consider wearable health devices that record heart rate, location, or sleep cycles. While no single data stream may be identifying on its own, their combination can reconstruct highly sensitive behavioral profiles. These risks are amplified in edge deployments, where data may be collected continuously, stored locally, and updated on-device—often with limited user visibility or control.

**Data governance** refers to the policies, infrastructure, and controls that determine how data is collected, used, stored, and shared. Responsible governance begins at the point of data acquisition: what data is collected, from whom, and under what consent model. Effective governance includes mechanisms for minimizing data collection, labeling data accurately, logging access and modifications, and ensuring compliance with jurisdiction-specific rules.

In high-stakes applications, poor governance can undermine entire systems. In the COMPAS case described earlier, the lack of transparency around the data used for training made it impossible to identify or audit embedded biases. Similarly, healthcare systems trained on electronic health records often inherit data collection artifacts such as missing values, label leakage, or demographic skew. Without rigorous governance, these issues become systemic vulnerabilities.

A central tool for formal privacy guarantees is **differential privacy**, which provides a mathematical framework for bounding how much information about an individual can be inferred from a model’s outputs. Algorithms such as DP-SGD (differentially private stochastic gradient descent) introduce noise during training to limit the influence of any single data point [@abadi2016deep]. While these techniques are powerful, they often reduce model accuracy or require large training datasets—tradeoffs that must be carefully evaluated in system design.

In addition to training-time privacy, **inference-time leakage** must be addressed. Membership inference attacks, for example, allow adversaries to determine whether a specific datapoint was used in model training. These attacks are particularly concerning for models deployed in personalized or high-privacy environments, such as education platforms or mental health apps. Addressing these risks requires a combination of privacy-aware modeling, restricted interfaces, and audit logging.

Modern regulations increasingly codify these concerns. The European Union’s [General Data Protection Regulation (GDPR)](https://gdpr.eu) mandates principles of data minimization, purpose limitation, and the right to be forgotten. The [California Consumer Privacy Act (CCPA)](https://oag.ca.gov/privacy/ccpa) and Japan’s [Act on the Protection of Personal Information (APPI)](https://www.dataguidance.com/notes/japan-data-protection-overview) establish similar requirements for notice, consent, and accountability. These legal frameworks place explicit design constraints on machine learning systems, from data pipelines to model retraining procedures.

Privacy is not solely a software-level concern. It intersects with architecture, deployment modality, and model lifecycle. Cloud-based systems may centralize data and rely on access controls and encryption, while edge systems prioritize local processing and differential privacy at the client level. TinyML systems—deployed on microcontrollers and wearables—pose additional constraints: limited storage, no persistent connectivity, and minimal computational budget. In these settings, privacy must be enforced through design-time choices, including data minimization, restricted retention, and provable erasure guarantees.

As with fairness and explainability, privacy is a system-wide property. Ensuring strong data protection requires coordinated decisions across system layers, including consent interface design, model training protocols, data retention policies, and update mechanisms. Responsible AI development demands that privacy not be treated as an isolated constraint, but rather as an architectural priority embedded throughout the machine learning lifecycle.

### Designing for Safety and Robustness

Safety in machine learning refers to the assurance that models behave reliably under expected conditions and fail gracefully under stress or uncertainty. Robustness, while related, specifically addresses the model’s ability to maintain consistent performance when subjected to perturbations, whether in the input distribution, environment, or system configuration. Together, these properties form a critical foundation for deploying machine learning systems in real-world, safety-critical domains.

The absence of robustness can have severe consequences. In one widely publicized case, a self-driving car operated by Uber failed to correctly detect a pedestrian crossing the street, resulting in a fatal accident [@uber2018accident]. Investigations revealed limitations in the object detection pipeline and inadequate fail-safes in the broader system. Similarly, Tesla’s Autopilot system has been involved in multiple incidents where drivers over-relied on automation that could not adequately handle edge cases [@tesla2024investigation]. These examples highlight that safety is not merely about model accuracy, but about how systems behave when uncertainty or failure modes are encountered.

One of the most widely studied threats to robustness is the adversarial example. In image classification, for instance, imperceptible changes to a pixel array can cause a model to confidently misclassify an input [@szegedy2013intriguing]. Let $x$ be a valid input and $\delta$ be a small perturbation. Then $x + \delta$ may still appear identical to a human observer but yield a radically different prediction:

$$
f(x) = \text{"panda"}, \quad f(x + \delta) = \text{"gibbon"}.
$$

Such vulnerabilities extend beyond images. Adversarial examples have been demonstrated in speech recognition, malware detection, and financial forecasting—often in ways that attackers can exploit with limited system knowledge.

From a systems perspective, robustness is not solely about defense against intentional manipulation. It also includes resistance to distribution shift, i.e., when the data encountered during deployment differ meaningfully from the data seen during training. These shifts may arise due to seasonality, changes in user behavior, sensor degradation, or transfer to new environments. For example, a traffic prediction model trained on urban road networks may fail when deployed in a rural region, even if the underlying learning algorithm remains unchanged.

Robustness failures undermine not only predictive accuracy but also downstream safety. In healthcare, a miscalibrated model may overestimate treatment efficacy for a subgroup underrepresented in training. In finance, a small misclassification may trigger large-scale automated trading decisions. In autonomous vehicles, a slight failure in object localization can result in fatal collisions. In each case, the system’s failure is not due to randomness but to a predictable fragility that was not adequately tested or constrained during development.

Addressing robustness requires architectural choices beyond model training. Techniques such as adversarial training, input sanitization, ensemble modeling, and anomaly detection all aim to make predictions more stable and resilient. But these methods introduce tradeoffs in performance, latency, or interpretability. For example, adversarial training often reduces performance on clean data and increases computational cost. More fundamentally, no robustness method can guarantee complete safety in the face of unforeseen events.

These limitations highlight the need for layered system design. In safety-critical applications, failover mechanisms, redundancy, and human-in-the-loop oversight play essential roles. For instance, aircraft autopilot systems are required to hand control back to a human pilot under certain error conditions. In machine learning systems, similar fail-safes can be implemented through confidence thresholds, abstention mechanisms, or fallback rule-based systems that activate when uncertainty is high.

Deployment context also shapes robustness requirements. In cloud-based deployments, model updates can be pushed frequently, and telemetry data can inform robustness monitoring. In contrast, edge devices and embedded systems often operate offline and must be robust under fixed conditions, limited updates, and hardware constraints. In these settings, testing must simulate environmental variability upfront, and robustness must be engineered into the deployed model through quantization-aware training, noise tolerance, or redundancy at the sensor-fusion level.

Ultimately, robustness is not simply a model attribute. It is a system-level property that affects safety, user experience, and operational risk. Ensuring robustness requires integrating defensive strategies throughout the ML lifecycle: from data collection and model training to deployment testing, user interface design, and monitoring infrastructure. Responsible ML systems treat robustness not as an optimization detail, but as a precondition for safety and trustworthiness.

### Accountability and Governance

Accountability in machine learning refers to the ability to identify, attribute, and respond to the consequences of automated decisions. In responsible AI systems, accountability is not solely about tracing failures; it is about building mechanisms that ensure actions have responsible owners, that harms can be addressed, and that ethical principles are maintained through oversight, feedback, and regulation. Without accountability, even well-intentioned systems can cause harm with no means of redress, eroding public trust and institutional legitimacy.

In traditional software systems, accountability often resides with developers or operators. But in machine learning systems, the lines of responsibility are more diffuse. Decisions are shaped by data provenance, model assumptions, feedback loops, and deployment interfaces—often spanning multiple stakeholders. For example, if an AI-driven hiring platform disproportionately excludes candidates from a protected group, accountability may lie with the data supplier, the model architect, the platform integrator, or the company deploying the system. Responsible system design requires that these pathways be explicitly mapped and governed.

For instance, Google Flu Trends—an early attempt to predict flu outbreaks based on search queries—failed to account for distribution shift and feedback loops. Without public visibility into the model’s assumptions, data curation, or update policies, it consistently overestimated flu activity and was eventually discontinued. The absence of transparent governance made failure modes difficult to detect and correct.

A growing number of legal frameworks reflect this need. The [Illinois Artificial Intelligence Video Interview Act](https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=4015&ChapterID=68) mandates that companies disclose when AI is used to evaluate video interviews, obtain explicit consent, and allow candidates to opt out. Similarly, the [EU AI Act](https://artificialintelligenceact.eu/the-act/) introduces risk-based governance requirements for AI systems, including mandatory documentation, human oversight, and post-market monitoring for high-risk applications. Facebook’s targeted advertising system has faced repeated litigation for enabling discriminatory housing and employment ads. In response, the company introduced restrictions on how advertisers can use demographic attributes and deployed fairness-aware auditing tools to enforce compliance. This illustrates how external regulation can drive internal governance reforms and technical controls.

These policies embed accountability not only in outcomes, but in processes. They require traceability—mechanisms that document data flows, model decisions, and human interventions over time. This implies that machine learning systems must be auditable by design. Logging infrastructure, versioned model registries, and governance APIs are not operational luxuries; they are structural components of responsible system engineering. Emerging tools like [model cards](https://arxiv.org/abs/1810.03993) and [datasheets for datasets](https://arxiv.org/abs/1803.09010) support this process by providing structured documentation of a model’s intended use, performance metrics, data sources, and known limitations.

Governance structures extend beyond legal compliance. Within organizations, internal governance includes ethics review boards, model risk management protocols, responsible AI committees, and cross-functional audits. These mechanisms institutionalize reflection and review, ensuring that technical teams are not isolated from the downstream impact of their systems. For example, impact assessments may evaluate a model’s fairness performance across multiple demographics before deployment and require mitigation plans for any adverse outcomes.

Effective governance also includes provisions for grievance redressal. Users must have a channel for contesting decisions, seeking explanations, and requesting updates or corrections. These capabilities are particularly important in dynamic systems that continuously learn from user interactions. A notable example occurred in 2019, when Apple Card users reported that women were receiving significantly lower credit limits than men, even when all financial indicators were comparable. The model’s internal logic was opaque, and the company initially lacked mechanisms for affected individuals to receive explanations or contest their limits. This prompted regulatory scrutiny and public outcry, highlighting the importance of traceability and redress in automated decision systems.

Accountability is also architectural. Interfaces that expose explanations, allow user corrections, or throttle high-risk decisions can help reintroduce human judgment into automated workflows. At the infrastructure level, scheduling systems can ensure that retraining processes incorporate governance feedback, while logging and analytics platforms can be configured to flag anomalous behaviors or distribution shifts that may indicate ethical drift.

In distributed deployments, such as federated learning or edge AI, accountability becomes more challenging. Device heterogeneity, intermittent connectivity, and limited observability make it difficult to enforce uniform policies or respond quickly to incidents. In these settings, governance must be embedded through protocol-level standards (e.g., secure aggregation, update constraints) and architectural commitments (e.g., local logging, trusted execution environments).

Importantly, governance is not synonymous with control. It is a framework for shared responsibility across design, deployment, and operation. Technical teams, legal advisors, domain experts, and end users all contribute to maintaining responsible system behavior over time. Designing for accountability means building infrastructure that enables this collaboration—making roles, decisions, and consequences visible, traceable, and actionable.

As machine learning systems grow more complex and autonomous, accountability must scale with them. This requires governance to be treated not as a reactive compliance function, but as a proactive layer in system architecture—tightly coupled to the interfaces, feedback loops, and institutional norms that sustain responsible AI in practice.

## Deployment Contexts

Responsible AI principles such as fairness, privacy, transparency, and robustness do not apply uniformly across all system architectures. Their implementation is shaped by the constraints and capabilities of the deployment environment. A model running in a centralized cloud environment benefits from abundant compute, high data throughput, and centralized monitoring—but may raise acute privacy and governance concerns. In contrast, models deployed on mobile devices or at the edge must operate under resource, latency, and connectivity constraints, often with limited oversight or delayed update cycles.

Understanding how deployment context affects responsible AI practices is essential for building machine learning systems that are not only performant but also ethically aligned and socially robust. The design space varies significantly across four major architectural categories:

* Cloud-based ML systems, typically hosted on centralized infrastructure, where compute and storage resources are abundant but user data is often aggregated and exposed.

* Mobile ML systems, running on smartphones and tablets, where models can be updated over time, run intermittently offline, and are tightly coupled to user interaction and behavior.

* Edge-based ML systems, deployed on local devices such as home assistants, drones, and autonomous vehicles, which require real-time responsiveness and localized processing under limited compute and memory budgets.

* TinyML systems, operating on extremely resource-constrained devices such as microcontrollers and embedded sensors, where responsible AI must be compiled into the model itself, with no opportunity for retraining or post-deployment adjustment.

Each deployment model raises distinct tradeoffs in how responsible AI principles can be supported or enforced. The following subsections compare how core values—such as explainability, fairness, privacy, safety, and accountability—manifest in these architectural regimes, and how system designers can anticipate and address these challenges during development.

### System Explainability

The practical realization of explainability depends not only on the choice of model or explanation technique, but also on the broader system architecture. Compute budget, latency sensitivity, user interaction models, and the availability of interfaces for logging or explanation delivery all influence which forms of interpretability are feasible and effective. While the previous discussion focused on the motivations for explainability and common techniques, this section examines how those techniques are constrained, adapted, or restructured in the context of real-world deployments.

A primary consideration is **computational complexity**. In high-resource environments, such as centralized cloud systems, explanation methods like SHAP and LIME can be deployed at scale to provide detailed insights into model behavior. These techniques involve sampling, retraining, or multiple forward passes through the model, which makes them impractical for low-power or latency-sensitive settings. On mobile devices, smart sensors, or embedded systems, designers often rely on simpler methods like saliency maps or input gradients, which require only a single backward pass. In extremely constrained environments such as TinyML, explainability may not be feasible at runtime at all—making development-time visualization and static inspection the only viable options.

**Latency and interaction constraints** also shape how and when explanations are delivered. Real-time systems, such as autonomous drones or industrial control loops, may have only milliseconds to make decisions. In these cases, providing a full explanation is not possible during operation. Instead, systems may log internal signals or confidence scores that can later be used for offline diagnosis. In contrast, applications like credit scoring or diagnostic tools often involve asynchronous interaction, allowing for deeper explanations to be delivered after a decision. Here, delayed interpretability is acceptable and even expected, as long as it is traceable and useful.

Another important distinction lies in the audience for the explanation. **User-facing explanations**—those shown to consumers or domain experts—must be understandable without technical knowledge. A health app on a mobile phone might explain a prediction by highlighting “unusual sleep patterns” or “elevated heart rate,” rather than by visualizing activations in a neural network. On the other hand, **internal explanations** aimed at developers, auditors, or regulators must provide a more detailed account of how the model behaves, possibly using attribution values, concept scores, or input counterfactuals. These internal tools are often integrated into development workflows, debugging infrastructure, or compliance review pipelines.

Explainability also depends on **when it occurs** in the system lifecycle. During development, explanations support model diagnostics, dataset auditing, and validation of assumptions. In this setting, interpretability tools are used to identify spurious correlations, measure representation gaps, and visualize learned concepts. After deployment, explainability becomes part of the runtime interface—supporting monitoring, accountability, and user trust. Systems may include explanation APIs, interfaces for querying past decisions, or alerts when the model encounters unfamiliar patterns. In low-resource settings where runtime explanation is infeasible, rigorous development-time validation becomes especially important, as design decisions must anticipate and address potential failure modes in advance.

Ultimately, explainability must be treated as an **architectural design constraint**, not an optional post-processing step. It interacts with model structure, interface design, user expectations, and the operational limits of the deployment environment. A system designed for explainability allocates budget not just for accuracy, but for interpretability—ensuring that the people who interact with the system can understand its behavior and take appropriate action.

In sum, explainability must be architected, not appended. Its feasibility depends on when and where inference occurs, what stakeholders require, and what the system can afford in terms of computation and communication. These tradeoffs must be made explicit during system design to ensure that interpretability goals are achievable in practice.

### Fairness Constraints

While fairness principles can be defined mathematically, their realization in practice depends on system-level constraints and deployment choices. Machine learning systems do not operate in idealized environments. Differences in data access, personalization level, inference constraints, and retraining infrastructure shape how fairness is evaluated, monitored, and enforced.

A key factor is **data availability and control**. In centralized systems—such as cloud-hosted APIs or platforms—developers often have access to large datasets with demographic annotations. This makes it feasible to compute group-level fairness metrics, apply fairness-aware training objectives, and audit models for disparate impact. In contrast, decentralized systems like mobile apps or federated learning environments often lack access to global group statistics due to privacy constraints or data fragmentation. In these settings, developers may be unable to measure or mitigate bias post-deployment, forcing fairness interventions to occur upstream during model training or dataset curation.

Fairness is also influenced by the system’s **personalization and adaptation mechanisms**. Global models that serve many users from a common interface, such as search engines or ad platforms, typically aim for parity across demographic groups. Localized models, such as those used in mobile health apps or on-device recommendation systems, may instead prioritize *individual fairness*—ensuring that similar users receive similar predictions. However, this is often harder to enforce, as it requires defining and measuring similarity in a principled way. Moreover, systems that adapt to individual behavior over time risk drifting toward patterns that reinforce existing disparities, particularly when data from underrepresented users is sparse or noisy.

**Real-time and resource-limited environments** impose additional constraints. On embedded systems or real-time control platforms, there may be no opportunity to evaluate fairness metrics or adjust thresholds on the fly. Instead, designers must rely on rigorous pre-deployment testing, static reweighting of training data, or use of inherently fair algorithms. These systems demand conservative design: fairness must be “baked in,” as post-hoc adjustment or audit is often infeasible. A voice recognition system on a low-power wearable, for example, may need to prioritize balanced performance across accents and dialects at training time, since performance cannot be dynamically recalibrated in the field.

System interface and decision policy also affect fairness. A model may have comparable accuracy across groups, but if its outputs are thresholded in a way that leads to uneven approval rates, the deployed system can still produce discriminatory effects. For instance, in a mobile loan approval app, using the same threshold across all users may result in under-approval for one group if the underlying score distribution differs. Designers must decide whether to apply a common threshold or group-specific cutoffs—and must be prepared to justify that decision in terms of impact and equity.

Finally, **feedback loops** can amplify unfairness over time. Systems that retrain on user behavior—such as content recommendation engines or automated hiring tools—may reinforce existing inequalities if corrective feedback is not incorporated. For example, if a resume screening system favors candidates from certain schools and retrains on historical selections, it may increasingly prioritize those profiles and overlook qualified applicants from underrepresented backgrounds. Addressing these dynamics requires governance mechanisms that span not only model training but also monitoring, logging, and policy evaluation.

Fairness, like explainability, is not localized to the model alone. It emerges from a sequence of design decisions across the system lifecycle: data collection, feature representation, model selection, deployment policy, user interaction, and monitoring strategy. Designing for fairness requires treating it as a first-class system property—one that constrains and informs architectural choices, even in the absence of perfect data or ideal conditions.

### Privacy Architectures

Privacy in machine learning systems is not solely a matter of protecting individual data—it is shaped by how data is collected, stored, transmitted, and incorporated into system behavior. Privacy constraints intersect with architectural design, regulatory obligations, and user interaction. These factors vary widely across deployment settings, creating distinct challenges for systems engineers tasked with minimizing risk while maintaining functionality and performance.

One key dimension is the distinction between **centralized and decentralized data handling**. In centralized systems, such as cloud-hosted services, user data is often aggregated at scale, enabling powerful modeling and analytics capabilities. However, this concentration increases exposure to data breaches and surveillance risks, making strong access control, encryption, and policy compliance essential. In decentralized systems—such as mobile apps, federated learning clients, or TinyML deployments—data remains local, reducing aggregation risk but also limiting visibility and observability. Engineers must design for privacy without assuming access to global training or audit data.

Privacy risks are especially pronounced in **personalized systems**. Applications that adapt to individual behavior—like smart keyboards, fitness monitors, or recommendation engines—must process and store sensitive behavioral signals, often over extended periods. Even when raw data is not stored explicitly, models can memorize rare or unique examples, enabling adversarial extraction of personal information through inference-time queries. Techniques such as differential privacy and gradient clipping can reduce memorization risk, but typically introduce tradeoffs in model accuracy, training time, and convergence behavior.

**Connectivity assumptions** further shape privacy design. In cloud-connected systems, data transfer enables centralized monitoring and encryption protocols but introduces latency and dependency on external infrastructure. In contrast, edge systems process data locally and often operate offline or intermittently. Privacy-preserving mechanisms in these contexts must rely on architectural constraints, such as collecting only minimal features, using quantized or obfuscated inputs, and avoiding persistent storage. On TinyML devices—such as microcontrollers in wearables or industrial sensors—privacy protection must be implemented at compile time, since post-deployment updates or patching may be infeasible.

Privacy risks also arise in **model serving and monitoring pipelines**. A model deployed with logging or active learning may inadvertently expose sensitive information if logs are retained improperly or used to fine-tune the model without auditing. Membership inference attacks can exploit subtle differences in model output to determine whether a particular record was seen during training. Defending against such attacks requires not only training-time privacy guarantees, but also careful design of API interfaces, rate limiting, and model access control.

System-level privacy also depends on **interface design and user expectations**. A model may meet formal privacy definitions but still violate user trust if its behavior is opaque or if data collection is unexpected. For example, a health-tracking app that infers stress levels from keystroke dynamics may not explicitly ask for this information, but still reveals sensitive behavioral traits. Transparent consent flows, explainable data practices, and opt-out mechanisms are essential components of a privacy-aware system design.

Architectural decisions influence every stage of the data lifecycle: what data is collected, how it is preprocessed, where it is stored, how it is used for inference, and how it is monitored. Designing for privacy requires more than encryption or access control; it involves making principled tradeoffs that reflect user needs, legal constraints, and platform limitations. In some systems, privacy is maintained through local processing and minimization. In others, it is protected through cryptographic protocols and policy infrastructure. In the most constrained environments, it must be embedded into the hardware and training pipeline, with no expectation of runtime flexibility.

Privacy is not a feature bolted onto a model after training. It is a property of the entire system—and must be treated as such in any responsible machine learning deployment.


+------------------------+--------------------------------------+------------------------------------+--------------------------------+
| Principle              | Cloud ML                             | Edge ML                            | TinyML                         |
+:=======================+:=====================================+:===================================+:===============================+
| Explainability         | Supports complex models and methods  | Needs lightweight, low-latency     | Severely limited due to        |
|                        | like SHAP and sampling approaches    | methods like saliency maps         | constrained hardware           |
+------------------------+--------------------------------------+------------------------------------+--------------------------------+
| Fairness               | Large datasets enable bias detection | Localized biases harder to detect  | Minimal data limits bias       |
|                        | and mitigation                       | but allows on-device adjustments   | analysis and mitigation        |
+------------------------+--------------------------------------+------------------------------------+--------------------------------+
| Privacy                | Centralized data at risk of breaches | Sensitive personal data on-device  | Distributed data reduces       |
|                        | but can leverage strong encryption   | requires on-device protections     | centralized risks but poses    |
|                        | and differential privacy             |                                    | challenges for anonymization   |
+------------------------+--------------------------------------+------------------------------------+--------------------------------+
| Safety                 | Vulnerable to hacking and            | Real-world interactions make       | Needs distributed safety       |
|                        | large-scale attacks                  | reliability critical               | mechanisms due to autonomy     |
+------------------------+--------------------------------------+------------------------------------+--------------------------------+
| Accountability         | Corporate policies and audits ensure | Fragmented supply chains complicate| Traceability required across   |
|                        | responsibility                       | accountability                     | long, complex hardware chains  |
+------------------------+--------------------------------------+------------------------------------+--------------------------------+
| Governance             | External oversight and regulations   | Requires self-governance by        | Relies on built-in protocols   |
|                        | like GDPR or CCPA are feasible       | developers and stakeholders        | and cryptographic assurances   |
+------------------------+--------------------------------------+------------------------------------+--------------------------------+

: Comparison of key principles in Cloud ML, Edge ML, and TinyML. {#tbl-ml-principles-comparison .striped .hover}

## Technical Aspects

### Bias Detection and Mitigation

Machine learning models, like any complex system, can sometimes exhibit biases in their predictions. These biases may manifest in underperformance for specific groups or in decisions that inadvertently restrict access to certain opportunities or resources [@buolamwini2018genderShades]. Understanding and addressing these biases is critical, especially as machine learning systems are increasingly used in sensitive domains like lending, healthcare, and criminal justice.

To evaluate and address these issues, fairness in machine learning is typically assessed by analyzing "subgroup attributes," which are characteristics unrelated to the prediction task, such as geographic location, age group, income level, race, gender, or religion. For example, in a loan default prediction model, subgroups could include race, gender, or religion. When models are trained with the sole objective of maximizing accuracy, they may overlook performance differences across these subgroups, potentially resulting in biased or inconsistent outcomes.

This concept is illustrated in @fig-fairness-example, which visualizes the performance of a machine learning model predicting loan repayment for two subgroups, Subgroup A (blue) and Subgroup B (red). Each individual in the dataset is represented by a symbol: plusses (+) indicate individuals who will repay their loans (true positives), while circles (O) indicate individuals who will default on their loans (true negatives). The model's objective is to correctly classify these individuals into repayers and defaulters.

![Illustrates the trade-off in setting classification thresholds for two subgroups (A and B) in a loan repayment model. Plusses (+) represent true positives (repayers), and circles (O) represent true negatives (defaulters). Different thresholds (75% for B and 81.25% for A) maximize subgroup accuracy but reveal fairness challenges.](images/png/fairness_cartoon.png){#fig-fairness-example}

To evaluate performance, two dotted lines are shown, representing the thresholds at which the model achieves acceptable accuracy for each subgroup. For Subgroup A, the threshold needs to be set at 81.25% accuracy (the second dotted line) to correctly classify all repayers (plusses). However, using this same threshold for Subgroup B would result in misclassifications, as some repayers in Subgroup B would incorrectly fall below this threshold and be classified as defaulters. For Subgroup B, a lower threshold of 75% accuracy (the first dotted line) is necessary to correctly classify its repayers. However, applying this lower threshold to Subgroup A would result in misclassifications for that group. This illustrates how the model performs unequally across the two subgroups, with each requiring a different threshold to maximize their true positive rates.

The disparity in required thresholds highlights the challenge of achieving fairness in model predictions. If positive classifications lead to loan approvals, individuals in Subgroup B would be disadvantaged unless the threshold is adjusted specifically for their subgroup. However, adjusting thresholds introduces trade-offs between group-level accuracy and fairness, demonstrating the inherent tension in optimizing for these objectives in machine learning systems.

Thus, the fairness literature has proposed three main _fairness metrics_ for quantifying how fair a model performs over a dataset [@hardt2016equality]. Given a model $h$ and a dataset $D$ consisting of $(x, y, s)$ samples, where $x$ is the data features, $y$ is the label, and $s$ is the subgroup attribute, and we assume there are simply two subgroups $a$ and $b$, we can define the following:

1. **Demographic Parity** asks how accurate a model is for each subgroup. In other words, $P(h(X) = Y \mid S = a) = P(h(X) = Y \mid S = b)$.

2. **Equalized Odds** asks how precise a model is on positive and negative samples for each subgroup. $P(h(X) = y \mid S = a, Y = y) = P(h(X) = y \mid S = b, Y = y)$.

3. **Equality of Opportunity** is a special case of equalized odds that only asks how precise a model is on positive samples. This is relevant in cases such as resource allocation, where we care about how positive (i.e., resource-allocated) labels are distributed across groups. For example, we care that an equal proportion of loans are given to both men and women. $P(h(X) = 1 \mid S = a, Y = 1) = P(h(X) = 1 \mid S = b, Y = 1)$.

Note: These definitions often take a narrow view when considering binary comparisons between two subgroups. Another thread of fair machine learning research focusing on _multicalibration_ and _multiaccuracy_ considers the interactions between an arbitrary number of identities, acknowledging the inherent intersectionality of individual identities in the real world [@hebert2018multicalibration].

#### Contextual Importance

Before making any technical decisions to develop an unbiased ML algorithm, we need to understand the context surrounding our model. Here are some of the key questions to think about:

* Who will this model make decisions for?
* Who is represented in the training data?
* Who is represented, and who is missing at the table of engineers, designers, and managers?
* What sort of long-lasting impacts could this model have? For example, will it impact an individual's financial security at a generational scale, such as determining college admissions or admitting a loan for a house?
* What historical and systematic biases are present in this setting, and are they present in the training data the model will generalize from?

Understanding a system's social, ethical, and historical background is critical to preventing harm and should inform decisions throughout the model development lifecycle. After understanding the context, one can make various technical decisions to remove bias. First, one must decide what fairness metric is the most appropriate criterion for optimizing. Next, there are generally three main areas where one can intervene to debias an ML system.

First, preprocessing is when one balances a dataset to ensure fair representation or even increases the weight on certain underrepresented groups to ensure the model performs well. Second, in processing attempts to modify the training process of an ML system to ensure it prioritizes fairness. This can be as simple as adding a fairness regularizer [@lowy2021fermi] to training an ensemble of models and sampling from them in a specific manner [@agarwal2018reductions].

Finally, post-processing debases a model after the fact, taking a trained model and modifying its predictions in a specific manner to ensure fairness is preserved [@alghamdi2022beyond; @hardt2016equality]. Post-processing builds on the preprocessing and in-processing steps by providing another opportunity to address bias and fairness issues in the model after it has already been trained.

The three-step process of preprocessing, in-processing, and post-processing provides a framework for intervening at different stages of model development to mitigate issues around bias and fairness. While preprocessing and in-processing focus on data and training, post-processing allows for adjustments after the model has been fully trained. Together, these three approaches give multiple opportunities to detect and remove unfair bias.

#### Considerate Deployment

The breadth of existing fairness definitions and debiasing interventions underscores the need for thoughtful assessment before deploying ML systems. As ML researchers and developers, responsible model development requires proactively educating ourselves on the real-world context, consulting domain experts and end-users, and centering harm prevention.

Rather than seeing fairness considerations as a box to check, we must deeply engage with the unique social implications and ethical tradeoffs around each model we build. Every technical choice about datasets, model architectures, evaluation metrics, and deployment constraints embeds values. By broadening our perspective beyond narrow technical metrics, carefully evaluating tradeoffs, and listening to impacted voices, we can work to ensure our systems expand opportunity rather than encode bias.

The path forward lies not in an arbitrary debiasing checklist but in a commitment to understanding and upholding our ethical responsibility at each step. This commitment starts with proactively educating ourselves and consulting others rather than just going through the motions of a fairness checklist. It requires engaging deeply with ethical tradeoffs in our technical choices, evaluating impacts on different groups, and listening to those voices most impacted.

Ultimately, responsible and ethical AI systems do not come from checkbox debiasing but from upholding our duty to assess harms, broaden perspectives, understand tradeoffs, and ensure we provide opportunity for all groups. This ethical responsibility should drive every step.

The connection between the paragraphs is that the first paragraph establishes the need for a thoughtful assessment of fairness issues rather than a checkbox approach. The second paragraph then expands on what that thoughtful assessment looks like in practice—engaging with tradeoffs, evaluating impacts on groups, and listening to impacted voices. Finally, the last paragraph refers to avoiding an "arbitrary debiasing checklist" and committing to ethical responsibility through assessment, understanding tradeoffs, and providing opportunity.

### Privacy Preservation

Recent incidents have shed light on how AI models can memorize sensitive user data in ways that violate privacy. @carlini2023extracting_llm demonstrate that language models tend to memorize training data and can even reproduce specific training examples. These risks are amplified with personalized ML systems deployed in intimate environments like homes or wearables. Consider a smart speaker that uses our conversations to improve its service quality for users who appreciate such enhancements. While potentially beneficial, this also creates privacy risks, as malicious actors could attempt to extract what the speaker "remembers." The issue extends beyond language models. @fig-diffusion-model-example showcases how diffusion models can memorize and generate individual training examples [@carlini2023extracting], further demonstrating the potential privacy risks associated with AI systems learning from user data.

![Diffusion models memorizing samples from training data. Source: @carlini2023extracting_llm.](images/png/diffusion_memorization.png){#fig-diffusion-model-example}

As AI becomes increasingly integrated into our daily lives, it is becoming more important that privacy concerns and robust safeguards to protect user information are developed with a critical eye. The challenge lies in balancing the benefits of personalized AI with the fundamental right to privacy.

Adversaries can use these memorization capabilities and train models to detect if specific training data influenced a target model. For example, membership inference attacks train a secondary model that learns to detect a change in the target model's outputs when making inferences over data it was trained on versus not trained on [@shokri2017membership].

ML devices are especially vulnerable because they are often personalized on user data and are deployed in even more intimate settings such as the home. Private machine learning techniques have evolved to establish safeguards against adversaries, as mentioned in the [Security and Privacy](../privacy_security/privacy_security.qmd) chapter to combat these privacy issues. Methods like differential privacy add mathematical noise during training to obscure individual data points' influence on the model. Popular techniques like DP-SGD [@abadi2016deep] also clip gradients to limit what the model leaks about the data. Still, users should also be able to delete the impact of their data after the fact.

### Machine Unlearning

With ML devices personalized to individual users and then deployed to remote edges without connectivity, a challenge arises---how can models responsively "forget" data points after deployment? If users request their data be removed from a personalized model, the lack of connectivity makes retraining infeasible. Thus, efficient on-device data forgetting is necessary but poses hurdles.

Initial unlearning approaches faced limitations in this context. Given the resource constraints, retrieving models from scratch on the device to forget data points proves inefficient or even impossible. Fully retraining also requires retaining all the original training data on the device, which brings its own security and privacy risks. Common machine unlearning techniques [@bourtoule2021machine] for remote embedded ML systems fail to enable responsive, secure data removal.

However, newer methods show promise in modifying models to approximately forget data without full retraining. While the accuracy loss from avoiding full rebuilds is modest, guaranteeing data privacy should still be the priority when handling sensitive user information ethically. Even slight exposure to private data can violate user trust. As ML systems become deeply personalized, efficiency and privacy must be enabled from the start---not afterthoughts.

Global privacy regulations, such as the well-established [GDPR](https://gdpr-info.eu) in the European Union, the [CCPA](https://oag.ca.gov/privacy/ccpa) in California, and newer proposals like Canada's [CPPA](https://www.didomi.io/blog/canada-data-privacy-law) and Japan's [APPI](https://www.dataguidance.com/notes/japan-data-protection-overview), emphasize the right to delete personal data. These policies, alongside high-profile AI incidents such as Stable Diffusion memorizing artist data, have highlighted the ethical imperative for models to allow users to delete their data even after training.

The right to remove data arises from privacy concerns around corporations or adversaries misusing sensitive user information. Machine unlearning refers to removing the influence of specific points from an already-trained model. Naively, this involves full retraining without the deleted data. However, connectivity constraints often make retraining infeasible for ML systems personalized and deployed to remote edges. If a smart speaker learns from private home conversations, retaining access to delete that data is important.

Although limited, methods are evolving to enable efficient approximations of retraining for unlearning. By modifying models' inference time, they can mimic "forgetting" data without full access to training data. However, most current techniques are restricted to simple models, still have resource costs, and trade some accuracy. Though methods are evolving, enabling efficient data removal and respecting user privacy remains imperative for responsible TinyML deployment.

### Adversarial Examples and Robustness

Machine learning models, especially deep neural networks, have a well-documented Achilles heel: they often break when even tiny perturbations are made to their inputs [@szegedy2013intriguing]. This surprising fragility highlights a major robustness gap threatening real-world deployment in high-stakes domains. It also opens the door for adversarial attacks designed to fool models deliberately.

Machine learning models can exhibit surprising brittleness—minor input tweaks can cause shocking malfunctions, even in state-of-the-art deep neural networks [@szegedy2013intriguing]. This unpredictability around out-of-sample data underscores gaps in model generalization and robustness. Given the growing ubiquity of ML, it also enables adversarial threats that weaponize models' blindspots.

Deep neural networks demonstrate an almost paradoxical dual nature - human-like proficiency in training distributions coupled with extreme fragility to tiny input perturbations [@szegedy2013intriguing]. This adversarial vulnerability gap highlights gaps in standard ML procedures and threats to real-world reliability. At the same time, it can be exploited: attackers can find model-breaking points humans wouldn't perceive.

@fig-adversarial-example includes an example of a small meaningless perturbation that changes a model prediction. This fragility has real-world impacts: lack of robustness undermines trust in deploying models for high-stakes applications like self-driving cars or medical diagnosis. Moreover, the vulnerability leads to security threats: attackers can deliberately craft adversarial examples that are perceptually indistinguishable from normal data but cause model failures.

![Perturbation effect on prediction. Source: [Microsoft.](https://www.microsoft.com/en-us/research/blog/adversarial-robustness-as-a-prior-for-better-transfer-learning/)](images/png/adversarial_robustness.png){#fig-adversarial-example}

For instance, past work shows successful attacks that trick models for tasks like NSFW detection [@bhagoji2018practical], ad-blocking [@tramer2019adversarial], and speech recognition [@carlini2016hidden]. While errors in these domains already pose security risks, the problem extends beyond IT security. Recently, adversarial robustness has been proposed as an additional performance metric by approximating worst-case behavior.

The surprising model fragility highlighted above casts doubt on real-world reliability and opens the door to adversarial manipulation. This growing vulnerability underscores several needs. First, moral robustness evaluations are essential for quantifying model vulnerabilities before deployment. Approximating worst-case behavior surfaces blindspots.

Second, effective defenses across domains must be developed to close these robustness gaps. With security on the line, developers cannot ignore the threat of attacks exploiting model weaknesses. Moreover, we cannot afford any fragility-induced failures for safety-critical applications like self-driving vehicles and medical diagnosis. Lives are at stake.

Finally, the research community continues mobilizing rapidly in response. Interest in adversarial machine learning has exploded as attacks reveal the need to bridge the robustness gap between synthetic and real-world data. Conferences now commonly feature defenses for securing and stabilizing models. The community recognizes that model fragility is a critical issue that must be addressed through robustness testing, defense development, and ongoing research. By surfacing blindspots and responding with principled defenses, we can work to ensure reliability and safety for machine learning systems, especially in high-stakes domains.

### Interpretable Model Construction

As models are deployed more frequently in high-stakes settings, practitioners, developers, downstream end-users, and increasing regulation have highlighted the need for explainability in machine learning. The goal of many interpretability and explainability methods is to provide practitioners with more information about the models' overall behavior or the behavior given a specific input. This allows users to decide whether or not a model's output or prediction is trustworthy.

Such analysis can help developers debug models and improve performance by pointing out biases, spurious correlations, and failure modes of models. In cases where models can surpass human performance on a task, interpretability can help users and researchers better understand relationships in their data and previously unknown patterns.

There are many classes of explainability/interpretability methods, including post hoc explainability, inherent interpretability, and mechanistic interpretability. These methods aim to make complex machine learning models more understandable and ensure users can trust model predictions, especially in critical settings. By providing transparency into model behavior, explainability techniques are an important tool for developing safe, fair, and reliable AI systems.

#### Post Hoc Explainability

Post hoc explainability methods typically explain the output behavior of a black-box model on a specific input. Popular methods include counterfactual explanations, feature attribution methods, and concept-based explanations.

**Counterfactual explanations**, also frequently called algorithmic recourse, "If X had not occurred, Y would not have occurred" [@wachter2017counterfactual]. For example, consider a person applying for a bank loan whose application is rejected by a model. They may ask their bank for recourse or how to change to be eligible for a loan. A counterfactual explanation would tell them which features they need to change and by how much such that the model's prediction changes.

**Feature attribution methods** highlight the input features that are important or necessary for a particular prediction. For a computer vision model, this would mean highlighting the individual pixels that contributed most to the predicted label of the image. Note that these methods do not explain how those pixels/features impact the prediction, only that they do. Common methods include input gradients, GradCAM [@selvaraju2017grad], SmoothGrad [@smilkov2017smoothgrad], LIME [@ribeiro2016should], and SHAP [@lundberg2017unified].

By providing examples of changes to input features that would alter a prediction (counterfactuals) or indicating the most influential features for a given prediction (attribution), these post hoc explanation techniques shed light on model behavior for individual inputs. This granular transparency helps users determine whether they can trust and act upon specific model outputs.

**Concept-based explanations** aim to explain model behavior and outputs using a pre-defined set of semantic concepts (e.g., the model recognizes scene class "bedroom" based on the presence of concepts "bed" and "pillow"). Recent work shows that users often prefer these explanations to attribution and example-based explanations because they "resemble human reasoning and explanations" [@ramaswamy2023ufo]. Popular concept-based explanation methods include TCAV [@kim2018interpretability], Network Dissection [@bau2017network], and interpretable basis decomposition [@zhou2018interpretable].

Note that these methods are extremely sensitive to the size and quality of the concept set, and there is a tradeoff between their accuracy and faithfulness and their interpretability or understandability to humans [@ramaswamy2023overlooked]. However, by mapping model predictions to human-understandable concepts, concept-based explanations can provide transparency into the reasoning behind model outputs.

#### Inherent Interpretability

Inherently interpretable models are constructed such that their explanations are part of the model architecture and are thus naturally faithful, which sometimes makes them preferable to post-hoc explanations applied to black-box models, especially in high-stakes domains where transparency is imperative [@rudin2019stop]. Often, these models are constrained so that the relationships between input features and predictions are easy for humans to follow (linear models, decision trees, decision sets, k-NN models), or they obey structural knowledge of the domain, such as monotonicity [@gupta2016monotonic], causality, or additivity [@lou2013accurate; @beck1998beyond].

However, more recent works have relaxed the restrictions on inherently interpretable models, using black-box models for feature extraction and a simpler inherently interpretable model for classification, allowing for faithful explanations that relate high-level features to prediction. For example, Concept Bottleneck Models [@koh2020concept] predict a concept set c that is passed into a linear classifier. ProtoPNets [@chen2019looks] dissect inputs into linear combinations of similarities to prototypical parts from the training set.

#### Mechanistic Interpretability

Mechanistic interpretability methods seek to reverse engineer neural networks, often analogizing them to how one might reverse engineer a compiled binary or how neuroscientists attempt to decode the function of individual neurons and circuits in brains. Most research in mechanistic interpretability views models as a computational graph [@geiger2021causal], and circuits are subgraphs with distinct functionality [@wang2022interpretability]. Current approaches to extracting circuits from neural networks and understanding their functionality rely on human manual inspection of visualizations produced by circuits [@olah2020zoom].

Alternatively, some approaches build sparse autoencoders that encourage neurons to encode disentangled interpretable features [@bricken2023towards]. This field is much newer than existing areas in explainability and interpretability, and as such, most works are generally exploratory rather than solution-oriented.

There are many problems in mechanistic interpretability, including the polysemanticity of neurons and circuits, the inconvenience and subjectivity of human labeling, and the exponential search space for identifying circuits in large models with billions or trillions of neurons.

#### Challenges and Considerations

As methods for interpreting and explaining models progress, it is important to note that humans overtrust and misuse interpretability tools [@kaur2020interpreting] and that a user's trust in a model due to an explanation can be independent of the correctness of the explanations [@lakkaraju2020fool]. As such, it is necessary that aside from assessing the faithfulness/correctness of explanations, researchers must also ensure that interpretability methods are developed and deployed with a specific user in mind and that user studies are performed to evaluate their efficacy and usefulness in practice.

Furthermore, explanations should be tailored to the user's expertise, the task they are using the explanation for and the corresponding minimal amount of information required for the explanation to be useful to prevent information overload.

While interpretability/explainability are popular areas in machine learning research, very few works study their intersection with TinyML and edge computing. Given that a significant application of TinyML is healthcare, which often requires high transparency and interpretability, existing techniques must be tested for scalability and efficiency concerning edge devices. Many methods rely on extra forward and backward passes, and some even require extensive training in proxy models, which are infeasible on resource-constrained microcontrollers.

That said, explainability methods can be highly useful in developing models for edge devices, as they can give insights into how input data and models can be compressed and how representations may change post-compression. Furthermore, many interpretable models are often smaller than their black-box counterparts, which could benefit TinyML applications.

### Model Performance Monitoring

While developers may train models that seem adversarially robust, fair, and interpretable before deployment, it is imperative that both the users and the model owners continue to monitor the model's performance and trustworthiness during the model's full lifecycle. Data is frequently changing in practice, which can often result in distribution shifts. These distribution shifts can profoundly impact the model's vanilla predictive performance and its trustworthiness (fairness, robustness, and interpretability) in real-world data.

Furthermore, definitions of fairness frequently change with time, such as what society considers a protected attribute, and the expertise of the users asking for explanations may also change.

To ensure that models keep up to date with such changes in the real world, developers must continually evaluate their models on current and representative data and standards and update models when necessary.

## Implementation Challenges

### Organizational Structures

While innovation and regulation are often seen as having competing interests, many countries have found it necessary to provide oversight as AI systems expand into more sectors. As shown in in @fig-human-centered-ai, this oversight has become crucial as these systems continue permeating various industries and impacting people's lives. Further discussion of this topic can be found in [Human-Centered AI, Chapter 22 "Government Interventions and Regulations"](https://academic-oup-com.ezp-prod1.hul.harvard.edu/book/41126/chapter/350465542).

![How various groups impact human-centered AI. Source: @schneiderman2020.](images/png/human_centered_ai.png){#fig-human-centered-ai}

Throughout this chapter, we have touched on several key policies aimed at guiding responsible AI development and deployment. Below is a summary of these policies, alongside additional noteworthy frameworks that reflect a global push for transparency in AI systems:

* The European Union's [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) mandates transparency and data protection measures for AI systems handling personal data.
* The [AI Bill of Rights](https://bidenwhitehouse.archives.gov/ostp/ai-bill-of-rights/) outlines principles for ethical AI use in the United States, emphasizing fairness, privacy, and accountability.
* The [California Consumer Privacy Act (CCPA)](https://oag.ca.gov/privacy/ccpa) protects consumer data and holds organizations accountable for data misuse.
* Canada's [Responsible Use of Artificial Intelligence](https://www.canada.ca/en/government/system/digital-government/digital-government-innovations/responsible-use-ai.html) outlines best practices for ethical AI deployment.
* Japan's [Act on the Protection of Personal Information (APPI)](https://www.dataguidance.com/notes/japan-data-protection-overview) establishes guidelines for handling personal data in AI systems.
* Canada's proposed [Consumer Privacy Protection Act (CPPA)](https://blog.didomi.io/en-us/canada-data-privacy-law) aims to strengthen privacy protections in digital ecosystems.
* The European Commission's [White Paper on Artificial Intelligence: A European Approach to Excellence and Trust](https://commission.europa.eu/publications/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en) emphasizes ethical AI development alongside innovation.
* The UK's Information Commissioner's Office and Alan Turing Institute's [Guidance on Explaining AI Decisions](https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/explaining-decisions-made-with-artificial-intelligence) provides recommendations for increasing AI transparency.

These policies highlight an ongoing global effort to balance innovation with accountability and ensure that AI systems are developed and deployed responsibly.

### Quality Data Acquisition

As discussed in the [Data Engineering](../data_engineering/data_engineering.qmd) chapter, responsible AI design must occur at all pipeline stages, including data collection. This begs the question: what does it mean for data to be high-quality and representative? Consider the following scenarios that _hinder_ the representativeness of data:

#### Subgroup Imbalance

This is likely what comes to mind when hearing "representative data." Subgroup imbalance means the dataset contains relatively more data from one subgroup than another. This imbalance can negatively affect the downstream ML model by causing it to overfit a subgroup of people while performing poorly on another.

One example consequence of subgroup imbalance is racial discrimination in facial recognition technology [@buolamwini2018genderShades]; commercial facial recognition algorithms have up to 34% worse error rates on darker-skinned females than lighter-skinned males.

Note that data imbalance goes both ways, and subgroups can also be harmful _overrepresented_ in the dataset. For example, the Allegheny Family Screening Tool (AFST) predicts the likelihood that a child will eventually be removed from a home. The AFST produces [disproportionate scores for different subgroups](https://www.aclu.org/the-devil-is-in-the-details-interrogating-values-embedded-in-the-allegheny-family-screening-tool#4-2-the-more-data-the-better), one of the reasons being that it is trained on historically biased data, sourced from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs.

#### Target Outcome Quantification

This occurs in applications where the ground-truth label cannot be measured or is difficult to represent in a single quantity. For example, an ML model in a mobile wellness application may want to predict individual stress levels. The true stress labels themselves are impossible to obtain directly and must be inferred from other biosignals, such as heart rate variability and user self-reported data. In these situations, noise is built into the data by design, making this a challenging ML task.

#### Distribution Shift

Data may no longer represent a task if a major external event causes the data source to change drastically. The most common way to think about distribution shifts is with respect to time; for example, data on consumer shopping habits collected pre-covid may no longer be present in consumer behavior today.

The transfer causes another form of distribution shift. For instance, when applying a triage system that was trained on data from one hospital to another, a distribution shift may occur if the two hospitals are very different.

#### Data Gathering

A reasonable solution for many of the above problems with non-representative or low-quality data is to collect more; we can collect more data targeting an underrepresented subgroup or from the target hospital to which our model might be transferred. However, for some reasons, gathering more data is an inappropriate or infeasible solution for the task at hand.

* _Data collection can be harmful._ This is the _paradox of exposure_, the situation in which those who stand to significantly gain from their data being collected are also those who are put at risk by the collection process (@d2023dataFeminism, Chapter 4). For example, collecting more data on non-binary individuals may be important for ensuring the fairness of the ML application, but it also puts them at risk, depending on who is collecting the data and how (whether the data is easily identifiable, contains sensitive content, etc.).

* _Data collection can be costly._ In some domains, such as healthcare, obtaining data can be costly in terms of time and money.

* _Biased data collection._ Electronic Health Records is a huge data source for ML-driven healthcare applications. Issues of subgroup representation aside, the data itself may be collected in a biased manner. For example, negative language ("nonadherent," "unwilling") is disproportionately used on black patients [@himmelstein2022examination].

We conclude with several additional strategies for maintaining data quality. First, fostering a deeper understanding of the data is crucial. This can be achieved through the implementation of standardized labels and measures of data quality, such as in the [Data Nutrition Project](https://datanutrition.org/). Collaborating with organizations responsible for collecting data helps ensure the data is interpreted correctly. Second, employing effective tools for data exploration is important. Visualization techniques and statistical analyses can reveal issues with the data. Finally, establishing a feedback loop within the ML pipeline is essential for understanding the real-world implications of the data. Metrics, such as fairness measures, allow us to define "data quality" in the context of the downstream application; improving fairness may directly improve the quality of the predictions that the end users receive.

### Accuracy-Objective Balance

Machine learning models are often evaluated on accuracy alone, but this single metric cannot fully capture model performance and tradeoffs for responsible AI systems. Other ethical dimensions, such as fairness, robustness, interpretability, and privacy, may compete with pure predictive accuracy during model development. For instance, inherently interpretable models such as small decision trees or linear classifiers with simplified features intentionally trade some accuracy for transparency in the model behavior and predictions. While these simplified models achieve lower accuracy by not capturing all the complexity in the dataset, improved interpretability builds trust by enabling direct analysis by human practitioners.

Additionally, certain techniques meant to improve adversarial robustness, such as adversarial training examples or dimensionality reduction, can degrade the accuracy of clean validation data. In sensitive applications like healthcare, focusing narrowly on state-of-the-art accuracy carries ethical risks if it allows models to rely more on spurious correlations that introduce bias or use opaque reasoning. Therefore, the appropriate performance objectives depend greatly on the sociotechnical context.

Methodologies like [Value Sensitive Design](https://vsdesign.org/) provide frameworks for formally evaluating the priorities of various stakeholders within the real-world deployment system. These explain the tensions between values like accuracy, interpretability and fairness, which can then guide responsible tradeoff decisions. For a medical diagnosis system, achieving the highest accuracy may not be the singular goal - improving transparency to build practitioner trust or reducing bias towards minority groups could justify small losses in accuracy. Analyzing the sociotechnical context is key for setting these objectives.

By taking a holistic view, we can responsibly balance accuracy with other ethical objectives for model success. Ongoing performance monitoring along multiple dimensions is crucial as the system evolves after deployment.

## AI Design Ethics

We must discuss at least some of the many ethical issues at stake in designing and applying AI systems and diverse frameworks for approaching these issues, including those from AI safety, Human-Computer Interaction (HCI), and Science, Technology, and Society (STS).

### AI Safety and Value Alignment

In 1960, Norbert Weiner wrote, "'if we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively... we had better be quite sure that the purpose put into the machine is the purpose which we desire" [@wiener1960some].

In recent years, as the capabilities of deep learning models have achieved, and sometimes even surpassed, human abilities, the issue of creating AI systems that act in accord with human intentions instead of pursuing unintended or undesirable goals has become a source of concern [@russell2021human]. Within the field of AI safety, a particular goal concerns "value alignment," or the problem of how to code the "right" purpose into machines [Human-Compatible Artificial Intelligence](https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf). Present AI research assumes we know the objectives we want to achieve and "studies the ability to achieve objectives, not the design of those objectives."

However, complex real-world deployment contexts make explicitly defining "the right purpose" for machines difficult, requiring frameworks for responsible and ethical goal-setting. Methodologies like [Value Sensitive Design](https://vsdesign.org/) provide formal mechanisms to surface tensions between stakeholder values and priorities.

By taking a holistic sociotechnical view, we can better ensure intelligent systems pursue objectives that align with broad human intentions rather than maximizing narrow metrics like accuracy alone. Achieving this in practice remains an open and critical research question as AI capabilities advance rapidly.

The absence of this alignment can lead to several AI safety issues, as have been documented in a variety of [deep learning models](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/). A common feature of systems that optimize for an objective is that variables not directly included in the objective may be set to extreme values to help optimize for that objective, leading to issues characterized as specification gaming, reward hacking, etc., in reinforcement learning (RL).

In recent years, a particularly popular implementation of RL has been models pre-trained using self-supervised learning and fine-tuned reinforcement learning from human feedback (RLHF) [@christiano2017deep]. Ngo 2022 [@ngo2022alignment] argues that by rewarding models for appearing harmless and ethical while also maximizing useful outcomes, RLHF could encourage the emergence of three problematic properties: situationally aware reward hacking, where policies exploit human fallibility to gain high reward, misaligned internally-represented goals that generalize beyond the RLHF fine-tuning distribution, and power-seeking strategies.

Similarly, @amodei2016concrete outlines six concrete problems for AI safety, including avoiding negative side effects, avoiding reward hacking, scalable oversight for aspects of the objective that are too expensive to be frequently evaluated during training, safe exploration strategies that encourage creativity while preventing harm, and robustness to distributional shift in unseen testing environments.

### Autonomous Systems and Trust

The consequences of autonomous systems that act independently of human oversight and often outside human judgment have been well documented across several industries and use cases. Most recently, the California Department of Motor Vehicles suspended Cruise's deployment and testing permits for its autonomous vehicles citing ["unreasonable risks to public safety"](https://www.cnbc.com/2023/10/24/california-dmv-suspends-cruises-self-driving-car-permits.html). One such [accident](https://www.cnbc.com/2023/10/17/cruise-under-nhtsa-probe-into-autonomous-driving-pedestrian-injuries.html) occurred when a vehicle struck a pedestrian who stepped into a crosswalk after the stoplight had turned green, and the vehicle was allowed to proceed. In 2018, a pedestrian crossing the street with her bike was killed when a self-driving Uber car, which was operating in autonomous mode, [failed to accurately classify her moving body as an object to be avoided](https://www.bbc.com/news/technology-54175359).

Autonomous systems beyond self-driving vehicles are also susceptible to such issues, with potentially graver consequences, as remotely-powered drones are already [reshaping warfare](https://www.reuters.com/technology/human-machine-teams-driven-by-ai-are-about-reshape-warfare-2023-09-08/). While such incidents bring up important ethical questions regarding [who should be held responsible](https://www.cigionline.org/articles/who-responsible-when-autonomous-systems-fail/) when these systems fail, they also highlight the technical challenges of giving full control of complex, real-world tasks to machines.

At its core, there is a tension between human and machine autonomy. Engineering and computer science disciplines have tended to focus on machine autonomy. For example, as of 2019, a search for the word "autonomy" in the Digital Library of the Association for Computing Machinery (ACM) reveals that of the top 100 most cited papers, 90% are on machine autonomy [@calvo2020supporting]. In an attempt to build systems for the benefit of humanity, these disciplines have taken, without question, increasing productivity, efficiency, and automation as primary strategies for benefiting humanity.

These goals put machine automation at the forefront, often at the expense of the human. This approach suffers from inherent challenges, as noted since the early days of AI through the Frame problem and qualification problem, which formalizes the observation that it is impossible to specify all the preconditions needed for a real-world action to succeed [@mccarthy1981epistemological].

These logical limitations have given rise to mathematical approaches such as Responsibility-sensitive safety (RSS) [@shalev2017formal], which is aimed at breaking down the end goal of an automated driving system (namely safety) into concrete and checkable conditions that can be rigorously formulated in mathematical terms. The goal of RSS is that those safety rules guarantee Automated Driving System (ADS) safety in the rigorous form of mathematical proof. However, such approaches tend towards using automation to address the problems of automation and are susceptible to many of the same issues.

Another approach to combating these issues is to focus on the human-centered design of interactive systems that incorporate human control. Value-sensitive design [@friedman1996value] described three key design factors for a user interface that impact autonomy, including system capability, complexity, misrepresentation, and fluidity. A more recent model, called METUX (A Model for Motivation, Engagement, and Thriving in the User Experience), leverages insights from Self-determination Theory (SDT) in Psychology to identify six distinct spheres of technology experience that contribute to the design systems that promote well-being and human flourishing [@peters2018designing]. SDT defines autonomy as acting by one's goals and values, which is distinct from the use of autonomy as simply a synonym for either independence or being in control [@ryan2000self].

@calvo2020supporting elaborates on METUX and its six "spheres of technology experience" in the context of AI-recommender systems. They propose these spheres---Adoption, Interface, Tasks, Behavior, Life, and Society—as a way of organizing thinking and evaluation of technology design in order to appropriately capture contradictory and downstream impacts on human autonomy when interacting with AI systems.

### AI's Economic Impact

A major concern of the current rise of AI technologies is widespread unemployment. As AI systems' capabilities expand, many fear these technologies will cause an absolute loss of jobs as they replace current workers and overtake alternative employment roles across industries. However, changing economic landscapes at the hands of automation is not new, and historically, have been found to reflect patterns of _displacement_ rather than replacement [@shneiderman2022human]---Chapter 4. In particular, automation usually lowers costs and increases quality, greatly increasing access and demand. The need to serve these growing markets pushes production, creating new jobs.

Furthermore, studies have found that attempts to achieve "lights-out" automation -- productive and flexible automation with a minimal number of human workers -- have been unsuccessful. Attempts to do so have led to what the MIT Work of the Future taskforce has termed ["zero-sum automation"](https://hbr.org/2023/03/a-smarter-strategy-for-using-robots), in which process flexibility is sacrificed for increased productivity.

In contrast, the task force proposes a "positive-sum automation" approach in which flexibility is increased by designing technology that strategically incorporates humans where they are very much needed, making it easier for line employees to train and debug robots, using a bottom-up approach to identifying what tasks should be automated; and choosing the right metrics for measuring success (see MIT's [Work of the Future](https://workofthefuture-mit-edu.ezp-prod1.hul.harvard.edu/wp-content/uploads/2021/01/2020-Final-Report4.pdf)).

However, the optimism of the high-level outlook does not preclude individual harm, especially to those whose skills and jobs will be rendered obsolete by automation. Public and legislative pressure, as well as corporate social responsibility efforts, will need to be directed at creating policies that share the benefits of automation with workers and result in higher minimum wages and benefits.

### AI Literacy and Communication

A 1993 survey of 3000 North American adults' beliefs about the "electronic thinking machine" revealed two primary perspectives of the early computer: the "beneficial tool of man" perspective and the "awesome thinking machine" perspective. The attitudes contributing to the "awesome thinking machine" view in this and other studies revealed a characterization of computers as "intelligent brains, smarter than people, unlimited, fast, mysterious, and frightening" [@martin1993myth]. These fears highlight an easily overlooked component of responsible AI, especially amidst the rush to commercialize such technologies: scientific communication that accurately communicates the capabilities _and_ limitations of these systems while providing transparency about the limitations of experts' knowledge about these systems.

As AI systems' capabilities expand beyond most people's comprehension, there is a natural tendency to assume the kinds of apocalyptic worlds painted by our media. This is partly due to the apparent difficulty of assimilating scientific information, even in technologically advanced cultures, which leads to the products of science being perceived as magic—"understandable only in terms of what it did, not how it worked" [@handlin1965science].

While tech companies should be held responsible for limiting grandiose claims and not falling into cycles of hype, research studying scientific communication, especially concerning (generative) AI, will also be useful in tracking and correcting public understanding of these technologies. An analysis of the Scopus scholarly database found that such research is scarce, with only a handful of papers mentioning both "science communication" and "artificial intelligence" [@schafer2023notorious].

Research that exposes the perspectives, frames, and images of the future promoted by academic institutions, tech companies, stakeholders, regulators, journalists, NGOs, and others will also help to identify potential gaps in AI literacy among adults [@lindgren2023handbook]. Increased focus on AI literacy from all stakeholders will be important in helping people whose skills are rendered obsolete by AI automation [@ng2021ai].

_"But even those who never acquire that understanding need assurance that there is a connection between the goals of science and their welfare, and above all, that the scientist is not a man altogether apart but one who shares some of their value."_ (Handlin, 1965)

## Conclusion

Responsible artificial intelligence is crucial as machine learning systems exert growing influence across healthcare, employment, finance, and criminal justice sectors. While AI promises immense benefits, thoughtlessly designed models risk perpetrating harm through biases, privacy violations, unintended behaviors, and other pitfalls.

Upholding principles of fairness, explainability, accountability, safety, and transparency enables the development of ethical AI aligned with human values. However, implementing these principles involves surmounting complex technical and social challenges around detecting dataset biases, choosing appropriate model tradeoffs, securing quality training data, and more. Frameworks like value-sensitive design guide balancing accuracy versus other objectives based on stakeholder needs.

Looking forward, advancing responsible AI necessitates continued research and industry commitment. More standardized benchmarks are required to compare model biases and robustness. As personalized TinyML expands, enabling efficient transparency and user control for edge devices warrants focus. Revised incentive structures and policies must encourage deliberate, ethical development before reckless deployment. Education around AI literacy and its limitations will further contribute to public understanding.

Responsible methods underscore that while machine learning offers immense potential, thoughtless application risks adverse consequences. Cross-disciplinary collaboration and human-centered design are imperative so AI can promote broad social benefit. The path ahead lies not in an arbitrary checklist but in a steadfast commitment to understand and uphold our ethical responsibility at each step. By taking conscientious action, the machine learning community can lead AI toward empowering all people equitably and safely.

## Resources

Here is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will be adding new exercises soon.

:::{.callout-note collapse="false" title="Slides"}

These slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.

* [What am I building? What is the goal?](https://docs.google.com/presentation/d/1Z9VpUKGOOfUIg6x04aXLVYl-9QoablElOlxhTLkAVno/edit?usp=drive_link&resourcekey=0-Nr9tvJ9KGgaL44O_iJpe4A)

* [Who is the audience?](https://docs.google.com/presentation/d/1IwIXrTQNf6MLlXKV-qOuafZhWS9saTxpY2uawQUHKfg/edit?usp=drive_link&resourcekey=0-Jc1kfKFb4OOhs919kyR2mA)

* [What are the consequences?](https://docs.google.com/presentation/d/1UDmrEZAJtH5LkHA_mDuFovOh6kam9FnC3uBAAah4RJo/edit?usp=drive_link&resourcekey=0-HFb4nRGGNRxJHz8wHXpgtg)

* [Responsible Data Collection.](https://docs.google.com/presentation/d/1vcmuhLVNFT2asKSCSGh_Ix9ht0mJZxMii8MufEMQhFA/edit?resourcekey=0-_pYLcW5aF3p3Bvud0PPQNg#slide=id.ga4ca29c69e_0_195)

:::

:::{.callout-important collapse="false" title="Videos"}

* @vid-fakeobama

:::

:::{.callout-caution collapse="false" title="Exercises"}

To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.

* _Coming soon._
:::
