<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning Systems - 10&nbsp; AI Acceleration</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/benchmarking/benchmarking.html" rel="next">
<link href="../../contents/optimizations/optimizations.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../scripts/ai_menu/dist/bundle.js" defer=""></script>
<script src="../../scripts/ai_menu/dist/142.bundle.js" defer=""></script>
<script src="../../scripts/ai_menu/dist/384.bundle.js" defer=""></script>
<script src="../../scripts/ai_menu/dist/761.bundle.js" defer=""></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/foreword.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/hw_acceleration/hw_acceleration.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Acceleration</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">FRONT MATTER</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedication</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MAIN</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ML Systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ML Operations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HANDS-ON LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/niclav_sys/niclav_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CV on Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/object_detection_fomo/object_detection_fomo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_nicla/kws_nicla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP - Spectral Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/motion_classify_ad/motion_classify_ad.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Communities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">10.1</span> Introduction</a></li>
  <li><a href="#background-and-basics" id="toc-background-and-basics" class="nav-link" data-scroll-target="#background-and-basics"><span class="header-section-number">10.2</span> Background and Basics</a>
  <ul>
  <li><a href="#historical-background" id="toc-historical-background" class="nav-link" data-scroll-target="#historical-background"><span class="header-section-number">10.2.1</span> Historical Background</a></li>
  <li><a href="#the-need-for-acceleration" id="toc-the-need-for-acceleration" class="nav-link" data-scroll-target="#the-need-for-acceleration"><span class="header-section-number">10.2.2</span> The Need for Acceleration</a></li>
  <li><a href="#general-principles" id="toc-general-principles" class="nav-link" data-scroll-target="#general-principles"><span class="header-section-number">10.2.3</span> General Principles</a>
  <ul class="collapse">
  <li><a href="#performance-within-power-budgets" id="toc-performance-within-power-budgets" class="nav-link" data-scroll-target="#performance-within-power-budgets">Performance Within Power Budgets</a></li>
  <li><a href="#managing-silicon-area-and-costs" id="toc-managing-silicon-area-and-costs" class="nav-link" data-scroll-target="#managing-silicon-area-and-costs">Managing Silicon Area and Costs</a></li>
  <li><a href="#workload-specific-optimizations" id="toc-workload-specific-optimizations" class="nav-link" data-scroll-target="#workload-specific-optimizations">Workload-Specific Optimizations</a></li>
  <li><a href="#sustainable-hardware-design" id="toc-sustainable-hardware-design" class="nav-link" data-scroll-target="#sustainable-hardware-design">Sustainable Hardware Design</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-aihw" id="toc-sec-aihw" class="nav-link" data-scroll-target="#sec-aihw"><span class="header-section-number">10.3</span> Accelerator Types</a>
  <ul>
  <li><a href="#application-specific-integrated-circuits-asics" id="toc-application-specific-integrated-circuits-asics" class="nav-link" data-scroll-target="#application-specific-integrated-circuits-asics"><span class="header-section-number">10.3.1</span> Application-Specific Integrated Circuits (ASICs)</a>
  <ul class="collapse">
  <li><a href="#advantages" id="toc-advantages" class="nav-link" data-scroll-target="#advantages">Advantages</a></li>
  <li><a href="#disadvantages" id="toc-disadvantages" class="nav-link" data-scroll-target="#disadvantages">Disadvantages</a></li>
  </ul></li>
  <li><a href="#field-programmable-gate-arrays-fpgas" id="toc-field-programmable-gate-arrays-fpgas" class="nav-link" data-scroll-target="#field-programmable-gate-arrays-fpgas"><span class="header-section-number">10.3.2</span> Field-Programmable Gate Arrays (FPGAs)</a>
  <ul class="collapse">
  <li><a href="#advantages-1" id="toc-advantages-1" class="nav-link" data-scroll-target="#advantages-1">Advantages</a></li>
  <li><a href="#disadvantages-1" id="toc-disadvantages-1" class="nav-link" data-scroll-target="#disadvantages-1">Disadvantages</a></li>
  </ul></li>
  <li><a href="#digital-signal-processors-dsps" id="toc-digital-signal-processors-dsps" class="nav-link" data-scroll-target="#digital-signal-processors-dsps"><span class="header-section-number">10.3.3</span> Digital Signal Processors (DSPs)</a>
  <ul class="collapse">
  <li><a href="#advantages-2" id="toc-advantages-2" class="nav-link" data-scroll-target="#advantages-2">Advantages</a></li>
  <li><a href="#disadvantages-2" id="toc-disadvantages-2" class="nav-link" data-scroll-target="#disadvantages-2">Disadvantages</a></li>
  </ul></li>
  <li><a href="#graphics-processing-units-gpus" id="toc-graphics-processing-units-gpus" class="nav-link" data-scroll-target="#graphics-processing-units-gpus"><span class="header-section-number">10.3.4</span> Graphics Processing Units (GPUs)</a>
  <ul class="collapse">
  <li><a href="#advantages-3" id="toc-advantages-3" class="nav-link" data-scroll-target="#advantages-3">Advantages</a></li>
  <li><a href="#disadvantages-3" id="toc-disadvantages-3" class="nav-link" data-scroll-target="#disadvantages-3">Disadvantages</a></li>
  <li><a href="#case-study-1" id="toc-case-study-1" class="nav-link" data-scroll-target="#case-study-1">Case Study</a></li>
  </ul></li>
  <li><a href="#central-processing-units-cpus" id="toc-central-processing-units-cpus" class="nav-link" data-scroll-target="#central-processing-units-cpus"><span class="header-section-number">10.3.5</span> Central Processing Units (CPUs)</a>
  <ul class="collapse">
  <li><a href="#advantages-4" id="toc-advantages-4" class="nav-link" data-scroll-target="#advantages-4">Advantages</a></li>
  <li><a href="#disadvantages-4" id="toc-disadvantages-4" class="nav-link" data-scroll-target="#disadvantages-4">Disadvantages</a></li>
  </ul></li>
  <li><a href="#comparison" id="toc-comparison" class="nav-link" data-scroll-target="#comparison"><span class="header-section-number">10.3.6</span> Comparison</a></li>
  </ul></li>
  <li><a href="#hardware-software-co-design" id="toc-hardware-software-co-design" class="nav-link" data-scroll-target="#hardware-software-co-design"><span class="header-section-number">10.4</span> Hardware-Software Co-Design</a>
  <ul>
  <li><a href="#the-need-for-co-design" id="toc-the-need-for-co-design" class="nav-link" data-scroll-target="#the-need-for-co-design"><span class="header-section-number">10.4.1</span> The Need for Co-Design</a>
  <ul class="collapse">
  <li><a href="#increasing-model-size-and-complexity" id="toc-increasing-model-size-and-complexity" class="nav-link" data-scroll-target="#increasing-model-size-and-complexity">Increasing Model Size and Complexity</a></li>
  <li><a href="#constraints-of-embedded-deployment" id="toc-constraints-of-embedded-deployment" class="nav-link" data-scroll-target="#constraints-of-embedded-deployment">Constraints of Embedded Deployment</a></li>
  <li><a href="#rapid-evolution-of-ai-algorithms" id="toc-rapid-evolution-of-ai-algorithms" class="nav-link" data-scroll-target="#rapid-evolution-of-ai-algorithms">Rapid Evolution of AI Algorithms</a></li>
  <li><a href="#complex-hardware-software-interactions" id="toc-complex-hardware-software-interactions" class="nav-link" data-scroll-target="#complex-hardware-software-interactions">Complex Hardware-Software Interactions</a></li>
  <li><a href="#need-for-specialization" id="toc-need-for-specialization" class="nav-link" data-scroll-target="#need-for-specialization">Need for Specialization</a></li>
  <li><a href="#demand-for-higher-efficiency" id="toc-demand-for-higher-efficiency" class="nav-link" data-scroll-target="#demand-for-higher-efficiency">Demand for Higher Efficiency</a></li>
  </ul></li>
  <li><a href="#principles-of-hardware-software-co-design" id="toc-principles-of-hardware-software-co-design" class="nav-link" data-scroll-target="#principles-of-hardware-software-co-design"><span class="header-section-number">10.4.2</span> Principles of Hardware-Software Co-Design</a>
  <ul class="collapse">
  <li><a href="#hardware-aware-software-optimization" id="toc-hardware-aware-software-optimization" class="nav-link" data-scroll-target="#hardware-aware-software-optimization">Hardware-Aware Software Optimization</a></li>
  <li><a href="#algorithm-driven-hardware-specialization" id="toc-algorithm-driven-hardware-specialization" class="nav-link" data-scroll-target="#algorithm-driven-hardware-specialization">Algorithm-Driven Hardware Specialization</a></li>
  <li><a href="#algorithm-hardware-co-exploration" id="toc-algorithm-hardware-co-exploration" class="nav-link" data-scroll-target="#algorithm-hardware-co-exploration">Algorithm-Hardware Co-exploration</a></li>
  </ul></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges"><span class="header-section-number">10.4.3</span> Challenges</a>
  <ul class="collapse">
  <li><a href="#increased-prototyping-costs" id="toc-increased-prototyping-costs" class="nav-link" data-scroll-target="#increased-prototyping-costs">Increased Prototyping Costs</a></li>
  <li><a href="#team-and-organizational-hurdles" id="toc-team-and-organizational-hurdles" class="nav-link" data-scroll-target="#team-and-organizational-hurdles">Team and Organizational Hurdles</a></li>
  <li><a href="#simulation-and-modeling-complexity" id="toc-simulation-and-modeling-complexity" class="nav-link" data-scroll-target="#simulation-and-modeling-complexity">Simulation and Modeling Complexity</a></li>
  <li><a href="#over-specialization-risks" id="toc-over-specialization-risks" class="nav-link" data-scroll-target="#over-specialization-risks">Over-Specialization Risks</a></li>
  <li><a href="#adoption-challenges" id="toc-adoption-challenges" class="nav-link" data-scroll-target="#adoption-challenges">Adoption Challenges</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#software-for-ai-hardware" id="toc-software-for-ai-hardware" class="nav-link" data-scroll-target="#software-for-ai-hardware"><span class="header-section-number">10.5</span> Software for AI Hardware</a>
  <ul>
  <li><a href="#sec-programming-models" id="toc-sec-programming-models" class="nav-link" data-scroll-target="#sec-programming-models"><span class="header-section-number">10.5.1</span> Programming Models</a></li>
  <li><a href="#libraries-and-runtimes" id="toc-libraries-and-runtimes" class="nav-link" data-scroll-target="#libraries-and-runtimes"><span class="header-section-number">10.5.2</span> Libraries and Runtimes</a></li>
  <li><a href="#optimizing-compilers" id="toc-optimizing-compilers" class="nav-link" data-scroll-target="#optimizing-compilers"><span class="header-section-number">10.5.3</span> Optimizing Compilers</a></li>
  <li><a href="#simulation-and-modeling" id="toc-simulation-and-modeling" class="nav-link" data-scroll-target="#simulation-and-modeling"><span class="header-section-number">10.5.4</span> Simulation and Modeling</a></li>
  </ul></li>
  <li><a href="#benchmarking-ai-hardware" id="toc-benchmarking-ai-hardware" class="nav-link" data-scroll-target="#benchmarking-ai-hardware"><span class="header-section-number">10.6</span> Benchmarking AI Hardware</a></li>
  <li><a href="#challenges-and-solutions" id="toc-challenges-and-solutions" class="nav-link" data-scroll-target="#challenges-and-solutions"><span class="header-section-number">10.7</span> Challenges and Solutions</a>
  <ul>
  <li><a href="#portabilitycompatibility-issues" id="toc-portabilitycompatibility-issues" class="nav-link" data-scroll-target="#portabilitycompatibility-issues"><span class="header-section-number">10.7.1</span> Portability/Compatibility Issues</a>
  <ul class="collapse">
  <li><a href="#solutions-and-strategies" id="toc-solutions-and-strategies" class="nav-link" data-scroll-target="#solutions-and-strategies">Solutions and Strategies</a></li>
  </ul></li>
  <li><a href="#power-consumption-concerns" id="toc-power-consumption-concerns" class="nav-link" data-scroll-target="#power-consumption-concerns"><span class="header-section-number">10.7.2</span> Power Consumption Concerns</a></li>
  <li><a href="#overcoming-resource-constraints" id="toc-overcoming-resource-constraints" class="nav-link" data-scroll-target="#overcoming-resource-constraints"><span class="header-section-number">10.7.3</span> Overcoming Resource Constraints</a></li>
  </ul></li>
  <li><a href="#emerging-technologies" id="toc-emerging-technologies" class="nav-link" data-scroll-target="#emerging-technologies"><span class="header-section-number">10.8</span> Emerging Technologies</a>
  <ul>
  <li><a href="#integration-methods" id="toc-integration-methods" class="nav-link" data-scroll-target="#integration-methods"><span class="header-section-number">10.8.1</span> Integration Methods</a>
  <ul class="collapse">
  <li><a href="#wafer-scale-ai" id="toc-wafer-scale-ai" class="nav-link" data-scroll-target="#wafer-scale-ai">Wafer-scale AI</a></li>
  <li><a href="#chiplets-for-ai" id="toc-chiplets-for-ai" class="nav-link" data-scroll-target="#chiplets-for-ai">Chiplets for AI</a></li>
  </ul></li>
  <li><a href="#sec-neuromorphic" id="toc-sec-neuromorphic" class="nav-link" data-scroll-target="#sec-neuromorphic"><span class="header-section-number">10.8.2</span> Neuromorphic Computing</a></li>
  <li><a href="#analog-computing" id="toc-analog-computing" class="nav-link" data-scroll-target="#analog-computing"><span class="header-section-number">10.8.3</span> Analog Computing</a></li>
  <li><a href="#flexible-electronics" id="toc-flexible-electronics" class="nav-link" data-scroll-target="#flexible-electronics"><span class="header-section-number">10.8.4</span> Flexible Electronics</a></li>
  <li><a href="#memory-technologies" id="toc-memory-technologies" class="nav-link" data-scroll-target="#memory-technologies"><span class="header-section-number">10.8.5</span> Memory Technologies</a></li>
  <li><a href="#optical-computing" id="toc-optical-computing" class="nav-link" data-scroll-target="#optical-computing"><span class="header-section-number">10.8.6</span> Optical Computing</a></li>
  <li><a href="#quantum-computing" id="toc-quantum-computing" class="nav-link" data-scroll-target="#quantum-computing"><span class="header-section-number">10.8.7</span> Quantum Computing</a></li>
  </ul></li>
  <li><a href="#future-trends" id="toc-future-trends" class="nav-link" data-scroll-target="#future-trends"><span class="header-section-number">10.9</span> Future Trends</a>
  <ul>
  <li><a href="#ml-for-hardware-design-automation" id="toc-ml-for-hardware-design-automation" class="nav-link" data-scroll-target="#ml-for-hardware-design-automation"><span class="header-section-number">10.9.1</span> ML for Hardware Design Automation</a></li>
  <li><a href="#ml-based-hardware-simulation-and-verification" id="toc-ml-based-hardware-simulation-and-verification" class="nav-link" data-scroll-target="#ml-based-hardware-simulation-and-verification"><span class="header-section-number">10.9.2</span> ML-Based Hardware Simulation and Verification</a></li>
  <li><a href="#ml-for-efficient-hardware-architectures" id="toc-ml-for-efficient-hardware-architectures" class="nav-link" data-scroll-target="#ml-for-efficient-hardware-architectures"><span class="header-section-number">10.9.3</span> ML for Efficient Hardware Architectures</a></li>
  <li><a href="#ml-to-optimize-manufacturing-and-reduce-defects" id="toc-ml-to-optimize-manufacturing-and-reduce-defects" class="nav-link" data-scroll-target="#ml-to-optimize-manufacturing-and-reduce-defects"><span class="header-section-number">10.9.4</span> ML to Optimize Manufacturing and Reduce Defects</a></li>
  <li><a href="#toward-foundation-models-for-hardware-design" id="toc-toward-foundation-models-for-hardware-design" class="nav-link" data-scroll-target="#toward-foundation-models-for-hardware-design"><span class="header-section-number">10.9.5</span> Toward Foundation Models for Hardware Design</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">10.10</span> Conclusion</a></li>
  <li><a href="#sec-ai-acceleration-resource" id="toc-sec-ai-acceleration-resource" class="nav-link" data-scroll-target="#sec-ai-acceleration-resource"><span class="header-section-number">10.11</span> Resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/hw_acceleration/hw_acceleration.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/hw_acceleration/hw_acceleration.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/foreword.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/hw_acceleration/hw_acceleration.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Acceleration</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ai_acceleration" class="quarto-section-identifier"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Resources: <a href="#sec-ai-acceleration-resource">Slides</a>, <a href="#sec-ai-acceleration-resource">Exercises</a>, <a href="#sec-ai-acceleration-resource">Labs</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ai_hardware.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E 3 Prompt: Create an intricate and colorful representation of a System on Chip (SoC) design in a rectangular format. Showcase a variety of specialized machine learning accelerators and chiplets, all integrated into the processor. Provide a detailed view inside the chip, highlighting the rapid movement of electrons. Each accelerator and chiplet should be designed to interact with neural network neurons, layers, and activations, emphasizing their processing speed. Depict the neural networks as a network of interconnected nodes, with vibrant data streams flowing between the accelerator pieces, showcasing the enhanced computation speed.</em></figcaption>
</figure>
</div>
<p>Machine learning has emerged as a transformative technology across many industries. However, deploying ML capabilities in real-world edge devices faces challenges due to limited computing resources. Specialized hardware acceleration is essential to enable high-performance machine learning under these constraints. Hardware accelerators optimize compute-intensive operations like inference using custom silicon optimized for matrix multiplications. This provides dramatic speedups over general-purpose CPUs, unlocking real-time execution of advanced models on size, weight, and power-constrained devices.</p>
<p>This chapter provides essential background on hardware acceleration techniques for embedded machine learning and their tradeoffs. The goal is to equip readers to make informed hardware selections and software optimizations to develop performant on-device ML capabilities.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand why hardware acceleration is needed for AI workloads</p></li>
<li><p>Survey key accelerator options like GPUs, TPUs, FPGAs, and ASICs and their tradeoffs</p></li>
<li><p>Learn about programming models, frameworks, and compilers for AI accelerators</p></li>
<li><p>Appreciate the importance of benchmarking and metrics for hardware evaluation</p></li>
<li><p>Recognize the role of hardware-software co-design in building efficient systems</p></li>
<li><p>Gain exposure to cutting-edge research directions like neuromorphic and quantum computing</p></li>
<li><p>Understand how ML is beginning to augment and enhance hardware design</p></li>
</ul>
</div>
</div>
<section id="introduction" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">10.1</span> Introduction</h2>
<p>Machine learning has emerged as a transformative technology across many industries, enabling systems to learn and improve from data. There is a growing demand for embedded ML solutions to deploy machine learning capabilities in real-world environments - where models are built into edge devices like smartphones, home appliances, and autonomous vehicles. However, these edge devices have limited computing resources compared to data center servers.</p>
<p>Specialized hardware acceleration enables high-performance machine learning on resource-constrained edge devices. Hardware acceleration refers to using custom silicon chips and architectures to offload compute-intensive ML operations from the main processor. In neural networks, the most intensive computations are the matrix multiplications during inference. Hardware accelerators can optimize these matrix operations, providing 10-100x speedups over general-purpose CPUs. This acceleration unlocks the ability to run advanced neural network models on devices with size, weight, and power constraints in real-time.</p>
<p>This chapter overviews hardware acceleration techniques for embedded machine learning and their design tradeoffs. Its goal is to equip readers with an essential background in embedded ML acceleration. This will enable informed hardware selection and software optimization to develop high-performance machine learning capabilities on edge devices.</p>
</section>
<section id="background-and-basics" class="level2 page-columns page-full" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="background-and-basics"><span class="header-section-number">10.2</span> Background and Basics</h2>
<section id="historical-background" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="historical-background"><span class="header-section-number">10.2.1</span> Historical Background</h3>
<p>The origins of hardware acceleration date back to the 1960s, with the advent of floating point math co-processors to offload calculations from the main CPU. One early example was the <a href="https://en.wikipedia.org/wiki/Intel_8087">Intel 8087</a> chip released in 1980 to accelerate floating point operations for the 8086 processor. This established the practice of using specialized processors to handle math-intensive workloads efficiently.</p>
<p>In the 1990s, the first <a href="https://en.wikipedia.org/wiki/History_of_the_graphics_processor">graphics processing units (GPUs)</a> emerged to process graphics pipelines for rendering and gaming rapidly. Nvidia’s <a href="https://en.wikipedia.org/wiki/GeForce_256">GeForce 256</a> in 1999 was one of the earliest programmable GPUs capable of running custom software algorithms. GPUs exemplify domain-specific fixed-function accelerators and evolve into parallel programmable accelerators.</p>
<p>In the 2000s, GPUs were applied to general-purpose computing under <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GPGPU</a>. Their high memory bandwidth and computational throughput made them well-suited for math-intensive workloads. This included breakthroughs in using GPUs to accelerate training of deep learning models such as <a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">AlexNet</a> in 2012.</p>
<p>In recent years, Google’s <a href="https://en.wikipedia.org/wiki/Tensor_processing_unit">Tensor Processing Units (TPUs)</a> represent customized ASICs specifically architected for matrix multiplication in deep learning. During inference, their optimized tensor cores achieve higher TeraOPS/watt than CPUs or GPUs. Ongoing innovation includes model compression techniques like <a href="https://arxiv.org/abs/1506.02626">pruning</a> and <a href="https://arxiv.org/abs/1609.07061">quantization</a> to fit larger neural networks on edge devices.</p>
<p>This evolution demonstrates how hardware acceleration has focused on solving compute-intensive bottlenecks, from floating point math to graphics to matrix multiplication for ML. Understanding this history provides a crucial context for specialized AI accelerators today.</p>
</section>
<section id="the-need-for-acceleration" class="level3 page-columns page-full" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="the-need-for-acceleration"><span class="header-section-number">10.2.2</span> The Need for Acceleration</h3>
<p>The evolution of hardware acceleration is closely tied to the broader history of computing. In the early decades, chip design was governed by Moore’s Law and Dennard Scaling, which observed that the number of transistors on an integrated circuit doubled yearly, and their performance (speed) increased as transistors became smaller. At the same time, power density (power per unit area) remains constant. These two laws were held through the single-core era. <a href="#fig-moore-dennard" class="quarto-xref">Figure&nbsp;<span>10.1</span></a> shows the trends of different microprocessor metrics. As the figure denotes, Dennard Scaling fails around the mid-2000s; notice how the clock speed (frequency) remains almost constant even as the number of transistors keeps increasing.</p>
<p>However, as <span class="citation" data-cites="patterson2016computer">Patterson and Hennessy (<a href="../../references.html#ref-patterson2016computer" role="doc-biblioref">2016</a>)</span> describes, technological constraints eventually forced a transition to the multicore era, with chips containing multiple processing cores to deliver performance gains. Power limitations prevented further scaling, which led to “dark silicon” (<a href="https://en.wikipedia.org/wiki/Dark_silicon">Dark Silicon</a>), where not all chip areas could be simultaneously active <span class="citation" data-cites="xiu2019time">(<a href="../../references.html#ref-xiu2019time" role="doc-biblioref">Xiu 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-patterson2016computer" class="csl-entry" role="listitem">
Patterson, David A, and John L Hennessy. 2016. <em>Computer Organization and Design <span>ARM</span> Edition: <span>The</span> Hardware Software Interface</em>. Morgan kaufmann.
</div><div id="ref-xiu2019time" class="csl-entry" role="listitem">
Xiu, Liming. 2019. <span>“Time Moore: <span>Exploiting</span> <span class="nocase">Moore’s</span> Law from the Perspective of Time.”</span> <em>IEEE Solid-State Circuits Mag.</em> 11 (1): 39–55. <a href="https://doi.org/10.1109/mssc.2018.2882285">https://doi.org/10.1109/mssc.2018.2882285</a>.
</div></div><p>The concept of dark silicon emerged as a consequence of these constraints. “Dark silicon” refers to portions of the chip that cannot be powered simultaneously due to thermal and power limitations. Essentially, as the density of transistors increased, the proportion of the chip that could be actively used without overheating or exceeding power budgets shrank.</p>
<p>This phenomenon meant that while chips had more transistors, not all could be operational simultaneously, limiting potential performance gains. This power crisis necessitated a shift to the accelerator era, with specialized hardware units tailored for specific tasks to maximize efficiency. The explosion in AI workloads further drove demand for customized accelerators. Enabling factors included new programming languages, software tools, and manufacturing advances.</p>
<div id="fig-moore-dennard" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-moore-dennard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hwai_40yearsmicrotrenddata.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-moore-dennard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: Microprocessor trends. Credit: <a href="https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/">Karl Rupp</a>.
</figcaption>
</figure>
</div>
<p>Fundamentally, hardware accelerators are evaluated on performance, power, and silicon area (PPA)—the nature of the target application—whether memory-bound or compute-bound—heavily influences the design. For example, memory-bound workloads demand high bandwidth and low latency access, while compute-bound applications require maximal computational throughput.</p>
</section>
<section id="general-principles" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="general-principles"><span class="header-section-number">10.2.3</span> General Principles</h3>
<p>The design of specialized hardware accelerators involves navigating complex tradeoffs between performance, power efficiency, silicon area, and workload-specific optimizations. This section outlines core considerations and methodologies for achieving an optimal balance based on application requirements and hardware constraints.</p>
<section id="performance-within-power-budgets" class="level4">
<h4 class="anchored" data-anchor-id="performance-within-power-budgets">Performance Within Power Budgets</h4>
<p>Performance refers to the throughput of computational work per unit of time, commonly measured in floating point operations per second (FLOPS) or frames per second (FPS). Higher performance enables completing more work, but power consumption rises with activity.</p>
<p>Hardware accelerators aim to maximize performance within set power budgets. This requires careful balancing of parallelism, the chip’s clock frequency, the operating voltage, workload optimization, and other techniques to maximize operations per watt.</p>
<ul>
<li><strong>Performance</strong> = Throughput * Efficiency</li>
<li><strong>Throughput</strong> ~= Parallelism * Clock Frequency</li>
<li><strong>Efficiency</strong> = Operations / Watt</li>
</ul>
<p>For example, GPUs achieve high throughput via massively parallel architectures. However, their efficiency is lower than that of customized application-specific integrated circuits (ASICs) like Google’s TPU, which optimize for a specific workload.</p>
</section>
<section id="managing-silicon-area-and-costs" class="level4">
<h4 class="anchored" data-anchor-id="managing-silicon-area-and-costs">Managing Silicon Area and Costs</h4>
<p>Chip area directly impacts manufacturing cost. Larger die sizes require more materials, lower yields, and higher defect rates. Mulit-die packages help scale designs but add packaging complexity. Silicon area depends on:</p>
<ul>
<li><strong>Computational resources</strong> - e.g., number of cores, memory, caches</li>
<li><strong>Manufacturing process node</strong> - smaller transistors enable higher density</li>
<li><strong>Programming model</strong> - programmed accelerators require more flexibility</li>
</ul>
<p>Accelerator design involves squeezing maximum performance within area constraints. Techniques like pruning and compression help fit larger models on the chip.</p>
</section>
<section id="workload-specific-optimizations" class="level4">
<h4 class="anchored" data-anchor-id="workload-specific-optimizations">Workload-Specific Optimizations</h4>
<p>The target workload dictates optimal accelerator architectures. Some of the key considerations include:</p>
<ul>
<li><strong>Memory vs Compute boundedness:</strong> Memory-bound workloads require more memory bandwidth, while compute-bound apps need arithmetic throughput.</li>
<li><strong>Data locality:</strong> Data movement should be minimized for efficiency. Near-compute memory helps.</li>
<li><strong>Bit-level operations:</strong> Low precision datatypes like INT8/INT4 optimize compute density.</li>
<li><strong>Data parallelism:</strong> Multiple replicated compute units allow parallel execution.</li>
<li><strong>Pipelining:</strong> Overlapped execution of operations increases throughput.</li>
</ul>
<p>Understanding workload characteristics enables customized acceleration. For example, convolutional neural networks use sliding window operations optimally mapped to spatial arrays of processing elements.</p>
<p>By navigating these architectural tradeoffs, hardware accelerators can deliver massive performance gains and enable emerging applications in AI, graphics, scientific computing, and other domains.</p>
</section>
<section id="sustainable-hardware-design" class="level4">
<h4 class="anchored" data-anchor-id="sustainable-hardware-design">Sustainable Hardware Design</h4>
<p>In recent years, AI sustainability has become a pressing concern driven by two key factors - the exploding scale of AI workloads and their associated energy consumption.</p>
<p>First, the size of AI models and datasets has rapidly grown. For example, based on OpenAI’s AI computing trends, the amount of computing used to train state-of-the-art models doubles every 3.5 months. This exponential growth requires massive computational resources in data centers.</p>
<p>Second, the energy usage of AI training and inference presents sustainability challenges. Data centers running AI applications consume substantial energy, contributing to high carbon emissions. It’s estimated that training a large AI model can have a carbon footprint of 626,000 pounds of CO2 equivalent, almost 5 times the lifetime emissions of an average car.</p>
<p>As a result, AI research and practice must prioritize energy efficiency and carbon impact alongside accuracy. There is an increasing focus on model efficiency, data center design, hardware optimization, and other solutions to improve sustainability. Striking a balance between AI progress and environmental responsibility has emerged as a key consideration and an area of active research across the field.</p>
<p>The scale of AI systems is expected to keep growing. Developing sustainable AI is crucial for managing the environmental footprint and enabling widespread beneficial deployment of this transformative technology.</p>
<p>We will learn about <a href="../sustainable_ai/sustainable_ai. cmd">Sustainable AI</a> in a later chapter, where we will discuss it in more detail.</p>
</section>
</section>
</section>
<section id="sec-aihw" class="level2 page-columns page-full" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="sec-aihw"><span class="header-section-number">10.3</span> Accelerator Types</h2>
<p>Hardware accelerators can take on many forms. They can exist as a widget (like the <a href="https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/">Neural Engine in the Apple M1 chip</a>) or as entire chips specially designed to perform certain tasks very well. This section will examine processors for machine learning workloads along the spectrum from highly specialized ASICs to more general-purpose CPUs. We first focus on custom hardware purpose-built for AI to understand the most extreme optimizations possible when design constraints are removed. This establishes a ceiling for performance and efficiency.</p>
<p>We then progressively consider more programmable and adaptable architectures, discussing GPUs and FPGAs. These make tradeoffs in customization to maintain flexibility. Finally, we cover general-purpose CPUs that sacrifice optimizations for a particular workload in exchange for versatile programmability across applications.</p>
<p>By structuring the analysis along this spectrum, we aim to illustrate the fundamental tradeoffs between utilization, efficiency, programmability, and flexibility in accelerator design. The optimal balance point depends on the constraints and requirements of the target application. This spectrum perspective provides a framework for reasoning about hardware choices for machine learning and the capabilities required at each level of specialization.</p>
<p><a href="#fig-design-tradeoffs" class="quarto-xref">Figure&nbsp;<span>10.2</span></a> illustrates the complex interplay between flexibility, performance, functional diversity, and area of architecture design. Notice how the ASIC is on the bottom-right corner, with minimal area, flexibility, and power consumption and maximal performance, due to its highly specialized application-specific nature. A key tradeoff is functinoal diversity vs performance: general purpose architechtures can serve diverse applications but their application performance is degraded as compared to more customized architectures.</p>
<p>The progression begins with the most specialized option, ASICs purpose-built for AI, to ground our understanding in the maximum possible optimizations before expanding to more generalizable architectures. This structured approach aims to elucidate the accelerator design space.</p>
<div id="fig-design-tradeoffs" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-design-tradeoffs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/tradeoffs.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-design-tradeoffs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.2: Design tradeoffs. Credit: <span class="citation" data-cites="rayis2014">El-Rayis (<a href="../../references.html#ref-rayis2014" role="doc-biblioref">2014</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-rayis2014" class="csl-entry" role="listitem">
El-Rayis, A. O. 2014. <span>“Reconfigurable Architectures for the Next Generation of Mobile Device Telecommunications Systems.”</span> <a href=": https://www.researchgate.net/publication/292608967">: https://www.researchgate.net/publication/292608967</a>.
</div></div></figure>
</div>
<section id="application-specific-integrated-circuits-asics" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="application-specific-integrated-circuits-asics"><span class="header-section-number">10.3.1</span> Application-Specific Integrated Circuits (ASICs)</h3>
<p>An Application-Specific Integrated Circuit (ASIC) is a type of <a href="https://en.wikipedia.org/wiki/Integrated_circuit">integrated circuit</a> (IC) that is custom-designed for a specific application or workload rather than for general-purpose use. Unlike CPUs and GPUs, ASICs do not support multiple applications or workloads. Rather, they are optimized to perform a single task extremely efficiently. The Google TPU is an example of an ASIC.</p>
<p>ASICs achieve this efficiency by tailoring every aspect of the chip design - the underlying logic gates, electronic components, architecture, memory, I/O, and manufacturing process - specifically for the target application. This level of customization allows removing any unnecessary logic or functionality required for general computation. The result is an IC that maximizes performance and power efficiency on the desired workload. The efficiency gains from application-specific hardware are so substantial that these software-centric firms dedicate enormous engineering resources to designing customized ASICs.</p>
<p>The rise of more complex machine learning algorithms has made the performance advantages enabled by tailored hardware acceleration a key competitive differentiator, even for companies traditionally concentrated on software engineering. ASICs have become a high-priority investment for major cloud providers aiming to offer faster AI computation.</p>
<section id="advantages" class="level4">
<h4 class="anchored" data-anchor-id="advantages">Advantages</h4>
<p>Due to their customized nature, ASICs provide significant benefits over general-purpose processors like CPUs and GPUs. The key advantages include the following.</p>
<section id="maximized-performance-and-efficiency" class="level5">
<h5 class="anchored" data-anchor-id="maximized-performance-and-efficiency">Maximized Performance and Efficiency</h5>
<p>The most fundamental advantage of ASICs is maximizing performance and power efficiency by customizing the hardware architecture specifically for the target application. Every transistor and design aspect is optimized for the desired workload - no unnecessary logic or overhead is needed to support generic computation.</p>
<p>For example, <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">Google’s Tensor Processing Units (TPUs)</a> contain architectures tailored exactly for the matrix multiplication operations used in neural networks. To design the TPU ASICs, Google’s engineering teams need to define the chip specifications clearly, write the architecture description using Hardware Description Languages like <a href="https://www.verilog.com/">Verilog</a>, synthesize the design to map it to hardware components, and carefully place-and-route transistors and wires based on the fabrication process design rules. This complex design process, known as very-large-scale integration (VLSI), allows them to build an optimized IC for machine learning workloads.</p>
<p>As a result, TPU ASICs achieve over an order of magnitude higher efficiency in operations per watt than general-purpose GPUs on ML workloads by maximizing performance and minimizing power consumption through a full-stack custom hardware design.</p>
</section>
<section id="specialized-on-chip-memory" class="level5">
<h5 class="anchored" data-anchor-id="specialized-on-chip-memory">Specialized On-Chip Memory</h5>
<p>ASICs incorporate on-chip SRAM and caches specifically optimized to feed data to the computational units. For example, Apple’s M1 system-on-a-chip contains special low-latency SRAM to accelerate the performance of its Neural Engine machine learning hardware. Large local memory with high bandwidth enables data to be kept close to the processing elements. This provides tremendous speed advantages compared to off-chip DRAM access, which can be up to 100x slower.</p>
<p>Data locality and optimizing memory hierarchy are crucial for high throughput and low power. Below is a table, “Numbers Everyone Should Know,” from <a href="https://research.google/people/jeff/">Jeff Dean</a>.</p>
<table class="table">
<thead>
<tr class="header">
<th>Operation</th>
<th>Latency</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>L1 cache reference</td>
<td>0.5 ns</td>
<td></td>
</tr>
<tr class="even">
<td>Branch mispredict</td>
<td>5 ns</td>
<td></td>
</tr>
<tr class="odd">
<td>L2 cache reference</td>
<td>7 ns</td>
<td></td>
</tr>
<tr class="even">
<td>Mutex lock/unlock</td>
<td>25 ns</td>
<td></td>
</tr>
<tr class="odd">
<td>Main memory reference</td>
<td>100 ns</td>
<td></td>
</tr>
<tr class="even">
<td>Compress 1K bytes with Zippy</td>
<td>3,000 ns</td>
<td>3 us</td>
</tr>
<tr class="odd">
<td>Send 1 KB bytes over 1 Gbps network</td>
<td>10,000 ns</td>
<td>10 us</td>
</tr>
<tr class="even">
<td>Read 4 KB randomly from SSD</td>
<td>150,000 ns</td>
<td>150 us</td>
</tr>
<tr class="odd">
<td>Read 1 MB sequentially from memory</td>
<td>250,000 ns</td>
<td>250 us</td>
</tr>
<tr class="even">
<td>Round trip within same datacenter</td>
<td>500,000 ns</td>
<td>0.5 ms</td>
</tr>
<tr class="odd">
<td>Read 1 MB sequentially from SSD</td>
<td>1,000,000 ns</td>
<td>1 ms</td>
</tr>
<tr class="even">
<td>Disk seek</td>
<td>10,000,000 ns</td>
<td>10 ms</td>
</tr>
<tr class="odd">
<td>Read 1 MB sequentially from disk</td>
<td>20,000,000 ns</td>
<td>20 ms</td>
</tr>
<tr class="even">
<td>Send packet CA-&gt;Netherlands-&gt;CA</td>
<td>150,000,000 ns</td>
<td>150 ms</td>
</tr>
</tbody>
</table>
</section>
<section id="custom-datatypes-and-operations" class="level5">
<h5 class="anchored" data-anchor-id="custom-datatypes-and-operations">Custom Datatypes and Operations</h5>
<p>Unlike general-purpose processors, ASICs can be designed to natively support custom datatypes like INT4 or bfloat16, which are widely used in ML models. For instance, Nvidia’s Ampere GPU architecture has dedicated bfloat16 Tensor Cores to accelerate AI workloads. Low-precision datatypes enable higher arithmetic density and performance. ASICs can also directly incorporate non-standard operations common in ML algorithms as primitive operations - for example, natively supporting activation functions like ReLU makes execution more efficient. Please refer to the Efficient Numeric Representations chapter for additional details.</p>
</section>
<section id="high-parallelism" class="level5">
<h5 class="anchored" data-anchor-id="high-parallelism">High Parallelism</h5>
<p>ASIC architectures can leverage higher parallelism tuned for the target workload versus general-purpose CPUs or GPUs. More computational units tailored for the application mean more operations execute simultaneously. Highly parallel ASICs achieve tremendous throughput for data parallel workloads like neural network inference.</p>
</section>
<section id="advanced-process-nodes" class="level5">
<h5 class="anchored" data-anchor-id="advanced-process-nodes">Advanced Process Nodes</h5>
<p>Cutting-edge manufacturing processes allow more transistors to be packed into smaller die areas, increasing density. ASICs designed specifically for high-volume applications can better amortize the costs of cutting-edge process nodes.</p>
</section>
</section>
<section id="disadvantages" class="level4">
<h4 class="anchored" data-anchor-id="disadvantages">Disadvantages</h4>
<section id="long-design-timelines" class="level5">
<h5 class="anchored" data-anchor-id="long-design-timelines">Long Design Timelines</h5>
<p>The engineering process of designing and validating an ASIC can take 2-3 years. Synthesizing the architecture using hardware description languages, taping out the chip layout, and fabricating the silicon on advanced process nodes involve long development cycles. For example, to tape out a 7nm chip, teams need to define specifications carefully, write the architecture in HDL, synthesize the logic gates, place components, route all interconnections, and finalize the layout to send for fabrication. This very large-scale integration (VLSI) flow means ASIC design and manufacturing can traditionally take 2-5 years.</p>
<p>There are a few key reasons why the long design timelines of ASICs, often 2-3 years, can be challenging for machine learning workloads:</p>
<ul>
<li><strong>ML algorithms evolve rapidly:</strong> New model architectures, training techniques, and network optimizations are constantly emerging. For example, Transformers became hugely popular in NLP last few years. When an ASIC finishes tapeout, the optimal architecture for a workload may have changed.</li>
<li><strong>Datasets grow quickly:</strong> ASICs designed for certain model sizes or datatypes can become undersized relative to demand. For instance, natural language models are scaling exponentially with more data and parameters. A chip designed for BERT might not accommodate GPT-3.</li>
<li><strong>ML applications change frequently:</strong> The industry focus shifts between computer vision, speech, NLP, recommender systems, etc. An ASIC optimized for image classification may have less relevance in a few years.</li>
<li><strong>Faster design cycles with GPUs/FPGAs:</strong> Programmable accelerators like GPUs can adapt much quicker by upgrading software libraries and frameworks. New algorithms can be deployed without hardware changes.</li>
<li><strong>Time-to-market needs:</strong> Getting a competitive edge in ML requires rapidly experimenting with and deploying new ideas. Waiting several years for an ASIC is different from fast iteration.</li>
</ul>
<p>The pace of innovation in ML needs to be better matched to the multi-year timescale for ASIC development. Significant engineering efforts are required to extend ASIC lifespan through modular architectures, process scaling, model compression, and other techniques. However, the rapid evolution of ML makes fixed-function hardware challenging.</p>
</section>
<section id="high-non-recurring-engineering-costs" class="level5">
<h5 class="anchored" data-anchor-id="high-non-recurring-engineering-costs">High Non-Recurring Engineering Costs</h5>
<p>The fixed costs of taking an ASIC from design to high-volume manufacturing can be very capital-intensive, often tens of millions of dollars. Photomask fabrication for taping out chips in advanced process nodes, packaging, and one-time engineering efforts is expensive. For instance, a 7nm chip tape-out alone could cost millions. The high non-recurring engineering (NRE) investment narrows ASIC viability to high-volume production use cases where the upfront cost can be amortized.</p>
</section>
<section id="complex-integration-and-programming" class="level5">
<h5 class="anchored" data-anchor-id="complex-integration-and-programming">Complex Integration and Programming</h5>
<p>ASICs require extensive software integration work, including drivers, compilers, OS support, and debugging tools. They also need expertise in electrical and thermal packaging. Additionally, efficiently programming ASIC architectures can involve challenges like workload partitioning and scheduling across many parallel units. The customized nature necessitates significant integration efforts to turn raw hardware into fully operational accelerators.</p>
<p>While ASICs provide massive efficiency gains on target applications by tailoring every aspect of the hardware design to one specific task, their fixed nature results in tradeoffs in flexibility and development costs compared to programmable accelerators, which must be weighed based on the application.</p>
</section>
</section>
</section>
<section id="field-programmable-gate-arrays-fpgas" class="level3 page-columns page-full" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="field-programmable-gate-arrays-fpgas"><span class="header-section-number">10.3.2</span> Field-Programmable Gate Arrays (FPGAs)</h3>
<p>FPGAs are programmable integrated circuits that can be reconfigured for different applications. Their customizable nature provides advantages for accelerating AI algorithms compared to fixed ASICs or inflexible GPUs. While Google, Meta, and NVIDIA are considering putting ASICs in data centers, Microsoft deployed FPGAs in its data centers <span class="citation" data-cites="putnam2014reconfigurable">(<a href="../../references.html#ref-putnam2014reconfigurable" role="doc-biblioref">Putnam et al. 2014</a>)</span> in 2011 to efficiently serve diverse data center workloads.</p>
<div class="no-row-height column-margin column-container"></div><section id="advantages-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="advantages-1">Advantages</h4>
<p>FPGAs provide several benefits over GPUs and ASICs for accelerating machine learning workloads.</p>
<section id="flexibility-through-reconfigurable-fabric" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="flexibility-through-reconfigurable-fabric">Flexibility Through Reconfigurable Fabric</h5>
<p>The key advantage of FPGAs is the ability to reconfigure the underlying fabric to implement custom architectures optimized for different models, unlike fixed-function ASICs. For example, quant trading firms use FPGAs to accelerate their algorithms because they change frequently, and the low NRE cost of FPGAs is more viable than tapping out new ASICs. <a href="#fig-different-fpgas" class="quarto-xref">Figure&nbsp;<span>10.3</span></a> contains a table comparing three different FPGAs.</p>
<div id="fig-different-fpgas" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-different-fpgas-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/fpga.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-different-fpgas-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.3: Comparison of FPGAs. Credit: <span class="citation" data-cites="gwennap_certus-nx_nodate">Gwennap (<a href="../../references.html#ref-gwennap_certus-nx_nodate" role="doc-biblioref">n.d.</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-gwennap_certus-nx_nodate" class="csl-entry" role="listitem">
Gwennap, Linley. n.d. <span>“Certus-<span>NX</span> Innovates General-Purpose <span>FPGAs</span>.”</span>
</div></div></figure>
</div>
<p>FPGAs comprise basic building blocks - configurable logic blocks, RAM blocks, and interconnects. Vendors provide a base amount of these resources, and engineers program the chips by compiling HDL code into bitstreams that rearrange the fabric into different configurations. This makes FPGAs adaptable as algorithms evolve.</p>
<p>While FPGAs may not achieve the utmost performance and efficiency of workload-specific ASICs, their programmability provides more flexibility as algorithms change. This adaptability makes FPGAs a compelling choice for accelerating evolving machine learning applications. Microsoft has deployed FPGAs in its Azure data centers for machine learning workloads to serve diverse applications instead of ASICs. The programmability enables optimization across changing ML models.</p>
</section>
<section id="customized-parallelism-and-pipelining" class="level5">
<h5 class="anchored" data-anchor-id="customized-parallelism-and-pipelining">Customized Parallelism and Pipelining</h5>
<p>FPGA architectures can leverage spatial parallelism and pipelining by tailoring the hardware design to mirror the parallelism in ML models. For example, Intel’s HARPv2 FPGA platform splits the layers of an MNIST convolutional network across separate processing elements to maximize throughput. Unique parallel patterns like tree ensemble evaluations are also possible on FPGAs. Deep pipelines with optimized buffering and dataflow can be customized to each model’s structure and datatypes. This level of tailored parallelism and pipelining is not feasible on GPUs.</p>
</section>
<section id="low-latency-on-chip-memory" class="level5">
<h5 class="anchored" data-anchor-id="low-latency-on-chip-memory">Low Latency On-Chip Memory</h5>
<p>Large amounts of high-bandwidth on-chip memory enable localized storage for weights and activations. For instance, Xilinx Versal FPGAs contain 32MB of low-latency RAM blocks and dual-channel DDR4 interfaces for external memory. Bringing memory physically closer to the compute units reduces access latency. This provides significant speed advantages over GPUs that traverse PCIe or other system buses to reach off-chip GDDR6 memory.</p>
</section>
<section id="native-support-for-low-precision" class="level5">
<h5 class="anchored" data-anchor-id="native-support-for-low-precision">Native Support for Low Precision</h5>
<p>A key advantage of FPGAs is the ability to natively implement any bit width for arithmetic units, such as INT4 or bfloat16, used in quantized ML models. For example, Intel’s Stratix 10 NX FPGAs have dedicated INT8 cores that can achieve up to 143 INT8 TOPS at ~1 TOPS/W <a href="https://www.intel.com/content/www/us/en/products/details/fpga/stratix/10/nx.html">Intel Stratix 10 NX FPGA</a>. Lower bit widths increase arithmetic density and performance. FPGAs can even support mixed precision or dynamic precision tuning at runtime.</p>
</section>
</section>
<section id="disadvantages-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="disadvantages-1">Disadvantages</h4>
<section id="lower-peak-throughput-than-asics" class="level5">
<h5 class="anchored" data-anchor-id="lower-peak-throughput-than-asics">Lower Peak Throughput than ASICs</h5>
<p>FPGAs cannot match the raw throughput numbers of ASICs customized for a specific model and precision. The overheads of the reconfigurable fabric compared to fixed function hardware result in lower peak performance. For example, the TPU v5e pods allow up to 256 chips to be connected with more than 100 petaOps of INT8 performance, while FPGAs can offer up to 143 INT8 TOPS or 286 INT4 TOPS <a href="https://www.intel.com/content/www/us/en/products/details/fpga/stratix/10/nx.html">Intel Stratix 10 NX FPGA</a>.</p>
<p>This is because FPGAs comprise basic building blocks—configurable logic blocks, RAM blocks, and interconnects. Vendors provide a set amount of these resources. To program FPGAs, engineers write HDL code and compile it into bitstreams that rearrange the fabric, which has inherent overheads versus an ASIC purpose-built for one computation.</p>
</section>
<section id="programming-complexity" class="level5">
<h5 class="anchored" data-anchor-id="programming-complexity">Programming Complexity</h5>
<p>To optimize FPGA performance, engineers must program the architectures in low-level hardware description languages like Verilog or VHDL. This requires hardware design expertise and longer development cycles than higher-level software frameworks like TensorFlow. Maximizing utilization can be challenging despite advances in high-level synthesis from C/C++.</p>
</section>
<section id="reconfiguration-overheads" class="level5">
<h5 class="anchored" data-anchor-id="reconfiguration-overheads">Reconfiguration Overheads</h5>
<p>Changing FPGA configurations requires reloading a new bitstream, which has considerable latency and storage size costs. For example, partial reconfiguration on Xilinx FPGAs can take 100s of milliseconds. This makes dynamically swapping architectures in real-time infeasible. The bitstream storage also consumes on-chip memory.</p>
</section>
<section id="diminishing-gains-on-advanced-nodes" class="level5">
<h5 class="anchored" data-anchor-id="diminishing-gains-on-advanced-nodes">Diminishing Gains on Advanced Nodes</h5>
<p>While smaller process nodes greatly benefit ASICs, they provide fewer advantages for FPGAs. At 7nm and below, effects like process variation, thermal constraints, and aging disproportionately impact FPGA performance. The overheads of the configurable fabric also diminish gains compared to fixed-function ASICs.</p>
</section>
<section id="case-study" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="case-study">Case Study</h5>
<p>FPGAs have found widespread application in various fields, including medical imaging, robotics, and finance, where they excel in handling computationally intensive machine learning tasks. In medical imaging, an illustrative example is the application of FPGAs for brain tumor segmentation, a traditionally time-consuming and error-prone process. For instance, Xiong et al.&nbsp;developed a quantized segmentation accelerator, which they retrained using the BraTS19 and BraTS20 datasets. Their work yielded remarkable results, achieving over 5x and 44x performance improvements and 11x and 82x energy efficiency gains compared to GPU and CPU implementations, respectively <span class="citation" data-cites="xiong2021mribased">(<a href="../../references.html#ref-xiong2021mribased" role="doc-biblioref">Xiong et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-xiong2021mribased" class="csl-entry" role="listitem">
Xiong, Siyu, Guoqing Wu, Xitian Fan, Xuan Feng, Zhongcheng Huang, Wei Cao, Xuegong Zhou, et al. 2021. <span>“<span>MRI</span>-Based Brain Tumor Segmentation Using <span>FPGA</span>-Accelerated Neural Network.”</span> <em>BMC Bioinf.</em> 22 (1): 421. <a href="https://doi.org/10.1186/s12859-021-04347-6">https://doi.org/10.1186/s12859-021-04347-6</a>.
</div></div></section>
</section>
</section>
<section id="digital-signal-processors-dsps" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="digital-signal-processors-dsps"><span class="header-section-number">10.3.3</span> Digital Signal Processors (DSPs)</h3>
<p>The first digital signal processor core was built in 1948 by Texas Instruments (<a href="https://audioxpress.com/article/the-evolution-of-audio-dsps">The Evolution of Audio DSPs</a>). Traditionally, DSPs would have logic to directly access digital/audio data in memory, perform an arithmetic operation (multiply-add-accumulate-MAC was one of the most common operations), and then write the result back to memory. The DSP would include specialized analog components to retrieve digital/audio data.</p>
<p>Once we entered the smartphone era, DSPs started encompassing more sophisticated tasks. They required Bluetooth, Wi-Fi, and cellular connectivity. Media also became much more complex. Today, it’s rare to have entire chips dedicated to just DSP, but a System on Chip would include DSPs and general-purpose CPUs. For example, Qualcomm’s <a href="https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor">Hexagon Digital Signal Processor</a> claims to be a “world-class processor with both CPU and DSP functionality to support deeply embedded processing needs of the mobile platform for both multimedia and modem functions.” <a href="https://blog.google/products/pixel/google-tensor-g3-pixel-8/">Google Tensors</a>, the chip in the Google Pixel phones, also includes CPUs and specialized DSP engines.</p>
<section id="advantages-2" class="level4">
<h4 class="anchored" data-anchor-id="advantages-2">Advantages</h4>
<p>DSPs architecturally provide advantages in vector math throughput, low latency memory access, power efficiency, and support for diverse datatypes - making them well-suited for embedded ML acceleration.</p>
<section id="optimized-architecture-for-vector-math" class="level5">
<h5 class="anchored" data-anchor-id="optimized-architecture-for-vector-math">Optimized Architecture for Vector Math</h5>
<p>DSPs contain specialized data paths, register files, and instructions optimized specifically for vector math operations commonly used in machine learning models. This includes dot product engines, MAC units, and SIMD capabilities tailored for vector/matrix calculations. For example, the CEVA-XM6 DSP (<a href="https://www.ceva-dsp.com/wp-content/uploads/2020/04/Ceva-SensPro-Fuses-AI-and-Vector-DSP.pdf">“Ceva SensPro Fuses AI and Vector DSP”</a>) has 512-bit vector units to accelerate convolutions. This efficiency on vector math workloads is far beyond general CPUs.</p>
</section>
<section id="low-latency-on-chip-memory-1" class="level5">
<h5 class="anchored" data-anchor-id="low-latency-on-chip-memory-1">Low Latency On-Chip Memory</h5>
<p>DSPs integrate large amounts of fast on-chip SRAM memory to hold data locally for processing. Bringing memory physically closer to the computation units reduces access latency. For example, Analog’s SHARC+ DSP contains 10MB of on-chip SRAM. This high-bandwidth local memory provides speed advantages for real-time applications.</p>
</section>
<section id="power-efficiency" class="level5">
<h5 class="anchored" data-anchor-id="power-efficiency">Power Efficiency</h5>
<p>DSPs are engineered to provide high performance per watt on digital signal workloads. Efficient data paths, parallelism, and memory architectures enable trillions of math operations per second within tight mobile power budgets. For example, <a href="https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor">Qualcomm’s Hexagon DSP</a> can deliver 4 trillion operations per second (TOPS) while consuming minimal watts.</p>
</section>
<section id="support-for-integer-and-floating-point-math" class="level5">
<h5 class="anchored" data-anchor-id="support-for-integer-and-floating-point-math">Support for Integer and Floating Point Math</h5>
<p>Unlike GPUs that excel at single or half precision, DSPs can natively support 8/16-bit integer and 32-bit floating point datatypes used across ML models. Some DSPs support dot product acceleration at INT8 precision for quantized neural networks.</p>
</section>
</section>
<section id="disadvantages-2" class="level4">
<h4 class="anchored" data-anchor-id="disadvantages-2">Disadvantages</h4>
<p>DSPs make architectural tradeoffs that limit peak throughput, precision, and model capacity compared to other AI accelerators. However, their advantages in power efficiency and integer math make them a strong edge computing option. So, while DSPs provide some benefits over CPUs, they also come with limitations for machine learning workloads:</p>
<section id="lower-peak-throughput-than-asicsgpus" class="level5">
<h5 class="anchored" data-anchor-id="lower-peak-throughput-than-asicsgpus">Lower Peak Throughput than ASICs/GPUs</h5>
<p>DSPs cannot match the raw computational throughput of GPUs or customized ASICs designed specifically for machine learning. For example, Qualcomm’s Cloud AI 100 ASIC delivers 480 TOPS on INT8, while their Hexagon DSP provides 10 TOPS. DSPs lack the massive parallelism of GPU SM units.</p>
</section>
<section id="slower-double-precision-performance" class="level5">
<h5 class="anchored" data-anchor-id="slower-double-precision-performance">Slower Double Precision Performance</h5>
<p>Most DSPs must be optimized for the higher precision floating point needed in some ML models. Their dot product engines focus on INT8/16 and FP32, which provide better power efficiency. However, 64-bit floating point throughput is much lower, which can limit usage in models requiring high precision.</p>
</section>
<section id="constrained-model-capacity" class="level5">
<h5 class="anchored" data-anchor-id="constrained-model-capacity">Constrained Model Capacity</h5>
<p>The limited on-chip memory of DSPs constrains the model sizes that can be run. Large deep learning models with hundreds of megabytes of parameters would exceed on-chip SRAM capacity. DSPs are best suited for small to mid-sized models targeted for edge devices.</p>
</section>
<section id="programming-complexity-1" class="level5">
<h5 class="anchored" data-anchor-id="programming-complexity-1">Programming Complexity</h5>
<p>Efficient programming of DSP architectures requires expertise in parallel programming and optimizing data access patterns. Their specialized microarchitectures have a steeper learning curve than high-level software frameworks, making development more complex.</p>
</section>
</section>
</section>
<section id="graphics-processing-units-gpus" class="level3 page-columns page-full" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="graphics-processing-units-gpus"><span class="header-section-number">10.3.4</span> Graphics Processing Units (GPUs)</h3>
<p>The term graphics processing unit has existed since at least the 1980s. There had always been a demand for graphics hardware in video game consoles (high demand, needed to be relatively lower cost) and scientific simulations (lower demand, but higher resolution, could be at a high price point).</p>
<p>The term was popularized, however, in 1999 when NVIDIA launched the GeForce 256, mainly targeting the PC games market sector <span class="citation" data-cites="lindholm2008nvidia">(<a href="../../references.html#ref-lindholm2008nvidia" role="doc-biblioref">Lindholm et al. 2008</a>)</span>. As PC games became more sophisticated, NVIDIA GPUs became more programmable. Soon, users realized they could take advantage of this programmability, run various non-graphics-related workloads on GPUs, and benefit from the underlying architecture. And so, in the late 2000s, GPUs became general-purpose graphics processing units or GP-GPUs.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lindholm2008nvidia" class="csl-entry" role="listitem">
Lindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008. <span>“<span>NVIDIA</span> Tesla: <span>A</span> Unified Graphics and Computing Architecture.”</span> <em>IEEE Micro</em> 28 (2): 39–55. <a href="https://doi.org/10.1109/mm.2008.31">https://doi.org/10.1109/mm.2008.31</a>.
</div></div><p><a href="https://www.intel.com/content/www/us/en/products/details/fpga/stratix/10/nx.html">Intel Arc Graphics</a> and <a href="https://www.amd.com/en/graphics/radeon-rx-graphics">AMD Radeon RX</a> have also developed their GPUs over time.</p>
<section id="advantages-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="advantages-3">Advantages</h4>
<section id="high-computational-throughput" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="high-computational-throughput">High Computational Throughput</h5>
<p>The key advantage of GPUs is their ability to perform massively parallel floating-point calculations optimized for computer graphics and linear algebra <span class="citation" data-cites="rajat2009largescale">(<a href="../../references.html#ref-rajat2009largescale" role="doc-biblioref">Raina, Madhavan, and Ng 2009</a>)</span>. Modern GPUs like Nvidia’s A100 offer up to 19.5 teraflops of FP32 performance with 6912 CUDA cores and 40GB of graphics memory tightly coupled with 1.6TB/s of graphics memory bandwidth.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rajat2009largescale" class="csl-entry" role="listitem">
Raina, Rajat, Anand Madhavan, and Andrew Y. Ng. 2009. <span>“Large-Scale Deep Unsupervised Learning Using Graphics Processors.”</span> In <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>, edited by Andrea Pohoreckyj Danyluk, Léon Bottou, and Michael L. Littman, 382:873–80. ACM International Conference Proceeding Series. ACM. <a href="https://doi.org/10.1145/1553374.1553486">https://doi.org/10.1145/1553374.1553486</a>.
</div></div><p>This raw throughput stems from the highly parallel streaming multiprocessor (SM) architecture tailored for data-parallel workloads <span class="citation" data-cites="jia2019beyond">(<a href="../../references.html#ref-jia2019beyond" role="doc-biblioref">Zhihao Jia, Zaharia, and Aiken 2019</a>)</span>. Each SM contains hundreds of scalar cores optimized for float32/64 math. With thousands of SMs on a chip, GPUs are purpose-built for matrix multiplication and vector operations used throughout neural networks.</p>
<p>For example, Nvidia’s latest <a href="https://www.nvidia.com/en-us/data-center/h100/">H100</a> GPU provides 4000 TFLOPs of FP8, 2000 TFLOPs of FP16, 1000 TFLOPs of TF32, 67 TFLOPs of FP32 and 34 TFLOPs of FP64 Compute performance, which can dramatically accelerate large batch training on models like BERT, GPT-3, and other transformer architectures. The scalable parallelism of GPUs is key to speeding up computationally intensive deep learning.</p>
</section>
<section id="mature-software-ecosystem" class="level5">
<h5 class="anchored" data-anchor-id="mature-software-ecosystem">Mature Software Ecosystem</h5>
<p>Nvidia provides extensive runtime libraries like <a href="https://developer.nvidia.com/cudnn">cuDNN</a> and <a href="https://developer.nvidia.com/cublas">cuBLAS</a> that are highly optimized for deep learning primitives. Frameworks like TensorFlow and PyTorch integrate with these libraries to enable GPU acceleration without direct programming. CUDA provides lower-level control for custom computations.</p>
<p>This ecosystem enables quick leveraging of GPUs via high-level Python without GPU programming expertise. Known workflows and abstractions provide a convenient on-ramp for scaling up deep learning experiments. The software maturity supplements the throughput advantages.</p>
</section>
<section id="broad-availability" class="level5">
<h5 class="anchored" data-anchor-id="broad-availability">Broad Availability</h5>
<p>The economies of scale of graphics processing make GPUs broadly accessible in data centers, cloud platforms like AWS and GCP, and desktop workstations. Their availability in research environments has provided a convenient ML experimentation and innovation platform. For example, nearly every state-of-the-art deep learning result has involved GPU acceleration because of this ubiquity. The broad access supplements the software maturity to make GPUs the standard ML accelerator.</p>
</section>
<section id="programmable-architecture" class="level5">
<h5 class="anchored" data-anchor-id="programmable-architecture">Programmable Architecture</h5>
<p>While not as flexible as FPGAs, GPUs provide programmability via CUDA and shader languages to customize computations. Developers can optimize data access patterns, create new ops, and tune precisions for evolving models and algorithms.</p>
</section>
</section>
<section id="disadvantages-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="disadvantages-3">Disadvantages</h4>
<p>While GPUs have become the standard accelerator for deep learning, their architecture has some key downsides.</p>
<section id="less-efficient-than-custom-asics" class="level5">
<h5 class="anchored" data-anchor-id="less-efficient-than-custom-asics">Less Efficient than Custom ASICs</h5>
<p>The statement “GPUs are less efficient than ASICs” could spark intense debate within the ML/AI field and cause this book to explode.</p>
<p>Typically, GPUs are perceived as less efficient than ASICs because the latter are custom-built for specific tasks and thus can operate more efficiently by design. With their general-purpose architecture, GPUs are inherently more versatile and programmable, catering to a broad spectrum of computational tasks beyond ML/AI.</p>
<p>However, modern GPUs have evolved to include specialized hardware support for essential AI operations, such as generalized matrix multiplication (GEMM) and other matrix operations, native support for quantization, and native support for pruning, which are critical for running ML models effectively. These enhancements have significantly improved the efficiency of GPUs for AI tasks to the point where they can rival the performance of ASICs for certain applications.</p>
<p>Consequently, contemporary GPUs are convergent, incorporating specialized ASIC-like capabilities within a flexible, general-purpose processing framework. This adaptability has blurred the lines between the two types of hardware. GPUs offer a strong balance of specialization and programmability that is well-suited to the dynamic needs of ML/AI research and development.</p>
</section>
<section id="high-memory-bandwidth-needs" class="level5">
<h5 class="anchored" data-anchor-id="high-memory-bandwidth-needs">High Memory Bandwidth Needs</h5>
<p>The massively parallel architecture requires tremendous memory bandwidth to supply thousands of cores, as shown in Figure 1. For example, the Nvidia A100 GPU requires 1.6TB/sec to fully saturate its computer. GPUs rely on wide 384-bit memory buses to high-bandwidth GDDR6 RAM, but even the fastest GDDR6 tops out at around 1 TB/sec.&nbsp;This dependence on external DRAM incurs latency and power overheads.</p>
</section>
<section id="programming-complexity-2" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="programming-complexity-2">Programming Complexity</h5>
<p>While tools like CUDA help, optimally mapping and partitioning ML workloads across the massively parallel GPU architecture remains challenging, achieving both high utilization and memory locality requires low-level tuning <span class="citation" data-cites="jia2018dissecting">(<a href="../../references.html#ref-jia2018dissecting" role="doc-biblioref">Zhe Jia et al. 2018</a>)</span>. Abstractions like TensorFlow can leave performance on the table.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jia2018dissecting" class="csl-entry" role="listitem">
Jia, Zhe, Marco Maggioni, Benjamin Staiger, and Daniele P. Scarpazza. 2018. <span>“Dissecting the <span>NVIDIA</span> <span>Volta</span> <span>GPU</span> Architecture via Microbenchmarking.”</span> <em>ArXiv Preprint</em>. <a href="https://arxiv.org/abs/1804.06826">https://arxiv.org/abs/1804.06826</a>.
</div></div></section>
<section id="limited-on-chip-memory" class="level5">
<h5 class="anchored" data-anchor-id="limited-on-chip-memory">Limited On-Chip Memory</h5>
<p>GPUs have relatively small on-chip memory caches compared to ML models’ large working set requirements during training. They rely on high bandwidth access to external DRAM, which ASICs minimize with large on-chip SRAM.</p>
</section>
<section id="fixed-architecture" class="level5">
<h5 class="anchored" data-anchor-id="fixed-architecture">Fixed Architecture</h5>
<p>Unlike FPGAs, the fundamental GPU architecture cannot be altered post-manufacture. This constraint limits adapting to novel ML workloads or layers. The CPU-GPU boundary also creates data movement overheads.</p>
</section>
</section>
<section id="case-study-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="case-study-1">Case Study</h4>
<p>The recent groundbreaking research conducted by OpenAI <span class="citation" data-cites="brown2020language">(<a href="../../references.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> with their GPT-3 model. GPT-3, a language model with 175 billion parameters, demonstrated unprecedented language understanding and generation capabilities. Its training, which would have taken months on conventional CPUs, was accomplished in a matter of days using powerful GPUs, thus pushing the boundaries of natural language processing (NLP) capabilities.</p>
<div class="no-row-height column-margin column-container"></div></section>
</section>
<section id="central-processing-units-cpus" class="level3 page-columns page-full" data-number="10.3.5">
<h3 data-number="10.3.5" class="anchored" data-anchor-id="central-processing-units-cpus"><span class="header-section-number">10.3.5</span> Central Processing Units (CPUs)</h3>
<p>The term CPUs has a long history that dates back to 1955 <span class="citation" data-cites="weik1955survey">(<a href="../../references.html#ref-weik1955survey" role="doc-biblioref">Weik 1955</a>)</span> while the first microprocessor CPU-the Intel 4004-was invented in 1971 (<a href="https://computerhistory.org/blog/who-invented-the-microprocessor/">Who Invented the Microprocessor?</a>). Compilers compile high-level programming languages like Python, Java, or C to assemble instructions (x86, ARM, RISC-V, etc.) for CPUs to process. The set of instructions a CPU understands is called the “instruction set.” It must be agreed upon by both the hardware and software running atop it (See section 5 for a more in-depth description of instruction set architectures-ISAs).</p>
<div class="no-row-height column-margin column-container"><div id="ref-weik1955survey" class="csl-entry" role="listitem">
Weik, Martin H. 1955. <em>A Survey of Domestic Electronic Digital Computing Systems</em>. Ballistic Research Laboratories.
</div></div><p>An overview of significant developments in CPUs:</p>
<ul>
<li>** Single-core Era (1950s- 2000): ** This era is known for aggressive microarchitectural improvements. Techniques like speculative execution (executing an instruction before the previous one was done), out-of-order execution (re-ordering instructions to be more effective), and wider issue widths (executing multiple instructions at once) were implemented to increase instruction throughput. The term “System on Chip” also originated in this era as different analog components (components designed with transistors) and digital components (components designed with hardware description languages that are mapped to transistors) were put on the same platform to achieve some task.</li>
<li><strong>Multicore Era (2000s):</strong> Driven by the decrease of Moore’s Law, this era is marked by scaling the number of cores within a CPU. Now, tasks can be split across many different cores, each with its own datapath and control unit. Many of the issues in this era pertained to how to share certain resources, which resources to share, and how to maintain coherency and consistency across all the cores.</li>
<li><strong>Sea of accelerators (2010s):</strong> Again, driven by the decrease of Moore’s law, this era is marked by offloading more complicated tasks to accelerators (widgets) attached to the main datapath in CPUs. It’s common to see accelerators dedicated to various AI workloads, as well as image/digital processing, and cryptography. In these designs, CPUs are often described more as judges, deciding which tasks should be processed rather than doing the processing itself. Any task could still be run on the CPU rather than the accelerators, but the CPU would generally be slower. However, the cost of designing and programming the accelerator became a non-trivial hurdle that sparked interest in design-specific libraries (DSLs).</li>
<li><strong>Presence in data centers:</strong> Although we often hear that GPUs dominate the data center marker, CPUs are still well suited for tasks that don’t inherently possess a large amount of parallelism. CPUs often handle serial and small tasks and coordinate the data center.</li>
<li><strong>On the edge:</strong> Given the tighter resource constraints on the edge, edge CPUs often only implement a subset of the techniques developed in the sing-core era because these optimizations tend to be heavy on power and area consumption. Edge CPUs still maintain a relatively simple datapath with limited memory capacities.</li>
</ul>
<p>Traditionally, CPUs have been synonymous with general-purpose computing, a term that has also changed as the “average” workload a consumer would run changes over time. For example, floating point components were once considered reserved for “scientific computing,” they were usually implemented as a co-processor (a modular component that worked with the datapath) and seldom deployed to average consumers. Compare this attitude to today, where FPUs are built into every datapath.</p>
<section id="advantages-4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="advantages-4">Advantages</h4>
<p>While raw throughput is limited, general-purpose CPUs provide practical AI acceleration benefits.</p>
<section id="general-programmability" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="general-programmability">General Programmability</h5>
<p>CPUs support diverse workloads beyond ML, providing flexible general-purpose programmability. This versatility comes from their standardized instruction sets and mature compiler ecosystems, which allow running any application, from databases and web servers to analytics pipelines <span class="citation" data-cites="hennessy2019golden">(<a href="../../references.html#ref-hennessy2019golden" role="doc-biblioref">Hennessy and Patterson 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hennessy2019golden" class="csl-entry" role="listitem">
Hennessy, John L., and David A. Patterson. 2019. <span>“A New Golden Age for Computer Architecture.”</span> <em>Commun. ACM</em> 62 (2): 48–60. <a href="https://doi.org/10.1145/3282307">https://doi.org/10.1145/3282307</a>.
</div></div><p>This avoids the need for dedicated ML accelerators and enables leveraging existing CPU-based infrastructure for basic ML deployment. For example, X86 servers from vendors like Intel and AMD can run common ML frameworks using Python and TensorFlow packages alongside other enterprise workloads.</p>
</section>
<section id="mature-software-ecosystem-1" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="mature-software-ecosystem-1">Mature Software Ecosystem</h5>
<p>For decades, highly optimized math libraries like <a href="https://www.netlib.org/blas/">BLAS</a>, <a href="https://hpc.llnl.gov/software/mathematical-software/lapack#:~:text=The%20Linear%20Algebra%20PACKage%20(LAPACK,problems%2C%20and%20singular%20value%20decomposition.)">LAPACK</a>, and <a href="https://www.fftw.org/">FFTW</a> have leveraged vectorized instructions and multithreading on CPUs <span class="citation" data-cites="dongarra2009evolution">(<a href="../../references.html#ref-dongarra2009evolution" role="doc-biblioref">Dongarra 2009</a>)</span>. Major ML frameworks like PyTorch, TensorFlow, and SciKit-Learn are designed to integrate seamlessly with these CPU math kernels.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dongarra2009evolution" class="csl-entry" role="listitem">
Dongarra, Jack J. 2009. <span>“The Evolution of High Performance Computing on System z.”</span> <em>IBM J. Res. Dev.</em> 53: 3–4.
</div></div><p>Hardware vendors like Intel and AMD also provide low-level libraries to optimize performance for deep learning primitives fully (<a href="https://www.intel.com/content/www/us/en/developer/articles/technical/ai-inference-acceleration-on-intel-cpus.html#gs.0w9qn2">AI Inference Acceleration on CPUs</a>). This robust, mature software ecosystem allows quickly deploying ML on existing CPU infrastructure.</p>
</section>
<section id="wide-availability" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="wide-availability">Wide Availability</h5>
<p>The economies of scale of CPU manufacturing, driven by demand across many markets like PCs, servers, and mobile, make them ubiquitously available. Intel CPUs, for example, have powered most servers for decades <span class="citation" data-cites="ranganathan2011from">(<a href="../../references.html#ref-ranganathan2011from" role="doc-biblioref">Ranganathan 2011</a>)</span>. This wide availability in data centers reduces hardware costs for basic ML deployment.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ranganathan2011from" class="csl-entry" role="listitem">
Ranganathan, Parthasarathy. 2011. <span>“From Microprocessors to Nanostores: <span>Rethinking</span> Data-Centric Systems.”</span> <em>Computer</em> 44 (1): 39–48. <a href="https://doi.org/10.1109/mc.2011.18">https://doi.org/10.1109/mc.2011.18</a>.
</div></div><p>Even small embedded devices typically integrate some CPU, enabling edge inference. The ubiquity reduces the need to purchase specialized ML accelerators in many situations.</p>
</section>
<section id="low-power-for-inference" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="low-power-for-inference">Low Power for Inference</h5>
<p>Optimizations like ARM Neon and Intel AVX vector extensions provide power-efficient integer and floating point throughput optimized for “bursty” workloads such as inference <span class="citation" data-cites="ignatov2018ai">(<a href="../../references.html#ref-ignatov2018ai" role="doc-biblioref">Ignatov et al. 2018</a>)</span>. While slower than GPUs, CPU inference can be deployed in power-constrained environments. For example, ARM’s Cortex-M CPUs now deliver over 1 TOPS of INT8 performance under 1W, enabling keyword spotting and vision applications on edge devices (<a href="https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/armv8_2d00_m-based-processor-software-development-hints-and-tips">ARM</a>).</p>
<div class="no-row-height column-margin column-container"></div></section>
</section>
<section id="disadvantages-4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="disadvantages-4">Disadvantages</h4>
<p>While providing some advantages, general-purpose CPUs also have limitations for AI workloads.</p>
<section id="lower-throughput-than-accelerators" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="lower-throughput-than-accelerators">Lower Throughput than Accelerators</h5>
<p>CPUs lack the specialized architectures for massively parallel processing that GPUs and other accelerators provide. Their general-purpose design reduces computational throughput for the highly parallelizable math operations common in ML models <span class="citation" data-cites="jouppi2017datacenter">(<a href="../../references.html#ref-jouppi2017datacenter" role="doc-biblioref">N. P. Jouppi et al. 2017a</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jouppi2017datacenter" class="csl-entry" role="listitem">
Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017a. <span>“In-Datacenter Performance Analysis of a Tensor Processing Unit.”</span> In <em>Proceedings of the 44th Annual International Symposium on Computer Architecture</em>, 1–12. ISCA ’17. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3079856.3080246">https://doi.org/10.1145/3079856.3080246</a>.
</div></div></section>
<section id="not-optimized-for-data-parallelism" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="not-optimized-for-data-parallelism">Not Optimized for Data Parallelism</h5>
<p>The architectures of CPUs are not specifically optimized for data parallel workloads inherent to AI <span class="citation" data-cites="sze2017efficient">(<a href="../../references.html#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>. They allocate substantial silicon area to instruction decoding, speculative execution, caching, and flow control that provides little benefit for the array operations used in neural networks (<a href="https://www.intel.com/content/www/us/en/developer/articles/technical/ai-inference-acceleration-on-intel-cpus.html#gs.0w9qn2">AI Inference Acceleration on CPUs</a>). However, modern CPUs are equipped with vector instructions like <a href="https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/what-is-intel-avx-512.html">AVX-512</a> specifically to accelerate certain key operations like matrix multiplication.</p>
<div class="no-row-height column-margin column-container"></div><p>GPU streaming multiprocessors, for example, devote most transistors to floating point units instead of complex branch prediction logic. This specialization allows much higher utilization for ML math.</p>
</section>
<section id="higher-memory-latency" class="level5">
<h5 class="anchored" data-anchor-id="higher-memory-latency">Higher Memory Latency</h5>
<p>CPUs suffer from higher latency accessing main memory relative to GPUs and other accelerators (<a href="https://www.integralmemory.com/articles/the-evolution-of-ddr-sdram/">DDR</a>). Techniques like tiling and caching can help, but the physical separation from off-chip RAM bottlenecks data-intensive ML workloads. This emphasizes the need for specialized memory architectures in ML hardware.</p>
</section>
<section id="power-inefficiency-under-heavy-workloads" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="power-inefficiency-under-heavy-workloads">Power Inefficiency Under Heavy Workloads</h5>
<p>While suitable for intermittent inference, sustaining near-peak throughput for training results in inefficient power consumption on CPUs, especially mobile CPUs <span class="citation" data-cites="ignatov2018ai">(<a href="../../references.html#ref-ignatov2018ai" role="doc-biblioref">Ignatov et al. 2018</a>)</span>. Accelerators explicitly optimize the data flow, memory, and computation for sustained ML workloads. CPUs are energy-inefficient for training large models.</p>
<div class="no-row-height column-margin column-container"></div></section>
</section>
</section>
<section id="comparison" class="level3" data-number="10.3.6">
<h3 data-number="10.3.6" class="anchored" data-anchor-id="comparison"><span class="header-section-number">10.3.6</span> Comparison</h3>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Accelerator</th>
<th>Description</th>
<th>Key Advantages</th>
<th>Key Disadvantages</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ASICs</td>
<td>Custom ICs designed for target workloads like AI inference</td>
<td>Maximizes perf/watt. <br> Optimized for tensor ops<br> Low latency on-chip memory</td>
<td>Fixed architecture lacks flexibility<br> High NRE cost<br> Long design cycles</td>
</tr>
<tr class="even">
<td>FPGAs</td>
<td>Reconfigurable fabric with programmable logic and routing</td>
<td>Flexible architecture<br> Low latency memory access</td>
<td>Lower perf/watt than ASICs<br> Complex programming</td>
</tr>
<tr class="odd">
<td>GPUs</td>
<td>Originally for graphics, now used for neural network acceleration</td>
<td>High throughput<br> Parallel scalability<br> Software ecosystem with CUDA</td>
<td>Not as power efficient as ASICs. <br> Require high memory bandwidth</td>
</tr>
<tr class="even">
<td>CPUs</td>
<td>General purpose processors</td>
<td>Programmability<br> Ubiquitous availability</td>
<td>Lower performance for AI workloads</td>
</tr>
</tbody>
</table>
<p>In general, CPUs provide a readily available baseline, GPUs deliver broadly accessible acceleration, FPGAs offer programmability, and ASICs maximize efficiency for fixed functions. The optimal choice depends on the target application’s scale, cost, flexibility, and other requirements.</p>
<p>Although first developed for data center deployment, where [cite some benefit that Google cites], Google has also put considerable effort into developing Edge TPUs. These Edge TPUs maintain the inspiration from systolic arrays but are tailored to the limited resources accessible at the edge.</p>
</section>
</section>
<section id="hardware-software-co-design" class="level2 page-columns page-full" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="hardware-software-co-design"><span class="header-section-number">10.4</span> Hardware-Software Co-Design</h2>
<p>Hardware-software co-design is based on the principle that AI systems achieve optimal performance and efficiency when the hardware and software components are designed in tight integration. This involves an iterative, collaborative design cycle where the hardware architecture and software algorithms are concurrently developed and refined with continuous feedback between teams.</p>
<p>For example, a new neural network model may be prototyped on an FPGA-based accelerator platform to obtain real performance data early in the design process. These results provide feedback to the hardware designers on potential optimizations and the software developers on refinements to the model or framework to better leverage the hardware capabilities. This level of synergy is difficult to achieve with the common practice of software being developed independently to deploy on fixed commodity hardware.</p>
<p>Co-design is critical for embedded AI systems facing significant resource constraints like low power budgets, limited memory and compute capacity, and real-time latency requirements. Tight integration between algorithm developers and hardware architects helps unlock optimizations across the stack to meet these restrictions. Enabling techniques include algorithmic improvements like neural architecture search and pruning and hardware advances like specialized dataflows and memory hierarchies.</p>
<p>By bringing hardware and software design together, rather than developing them separately, holistic optimizations can be made that maximize performance and efficiency. The next sections provide more details on specific co-design approaches.</p>
<section id="the-need-for-co-design" class="level3 page-columns page-full" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="the-need-for-co-design"><span class="header-section-number">10.4.1</span> The Need for Co-Design</h3>
<p>Several key factors make a collaborative hardware-software co-design approach essential for building efficient AI systems.</p>
<section id="increasing-model-size-and-complexity" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="increasing-model-size-and-complexity">Increasing Model Size and Complexity</h4>
<p>State-of-the-art AI models have been rapidly growing in size, enabled by advances in neural architecture design and the availability of large datasets. For example, the GPT-3 language model contains 175 billion parameters <span class="citation" data-cites="brown2020language">(<a href="../../references.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>, requiring huge computational resources for training. This explosion in model complexity necessitates co-design to develop efficient hardware and algorithms in tandem. Techniques like model compression <span class="citation" data-cites="cheng2017survey">(<a href="../../references.html#ref-cheng2017survey" role="doc-biblioref">Cheng et al. 2018</a>)</span> and quantization must be co-optimized with the hardware architecture.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual</em>, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.
</div><div id="ref-cheng2017survey" class="csl-entry" role="listitem">
Cheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2018. <span>“Model Compression and Acceleration for Deep Neural Networks: <span>The</span> Principles, Progress, and Challenges.”</span> <em>IEEE Signal Process Mag.</em> 35 (1): 126–36. <a href="https://doi.org/10.1109/msp.2017.2765695">https://doi.org/10.1109/msp.2017.2765695</a>.
</div></div></section>
<section id="constraints-of-embedded-deployment" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="constraints-of-embedded-deployment">Constraints of Embedded Deployment</h4>
<p>Deploying AI applications on edge devices like mobile phones or smart home appliances introduces significant constraints on energy, memory, and silicon area <span class="citation" data-cites="sze2017efficient">(<a href="../../references.html#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>. Enable real-time inference under these restrictions requires co-exploring hardware optimizations like specialized dataflows and compression with efficient neural network design and pruning techniques. Co-design maximizes performance within tight deployment constraints.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="rapid-evolution-of-ai-algorithms" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="rapid-evolution-of-ai-algorithms">Rapid Evolution of AI Algorithms</h4>
<p>AI is rapidly evolving, with new model architectures, training methodologies, and software frameworks constantly emerging. For example, Transformers have recently become hugely popular for NLP <span class="citation" data-cites="young2018recent">(<a href="../../references.html#ref-young2018recent" role="doc-biblioref">Young et al. 2018</a>)</span>. Keeping pace with these algorithmic innovations requires hardware-software co-design to adapt platforms and avoid accrued technical debt quickly.</p>
<div class="no-row-height column-margin column-container"><div id="ref-young2018recent" class="csl-entry" role="listitem">
Young, Tom, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018. <span>“Recent Trends in Deep Learning Based Natural Language Processing <span>[Review</span> Article].”</span> <em>IEEE Comput. Intell. Mag.</em> 13 (3): 55–75. <a href="https://doi.org/10.1109/mci.2018.2840738">https://doi.org/10.1109/mci.2018.2840738</a>.
</div></div></section>
<section id="complex-hardware-software-interactions" class="level4">
<h4 class="anchored" data-anchor-id="complex-hardware-software-interactions">Complex Hardware-Software Interactions</h4>
<p>Many subtle interactions and tradeoffs between hardware architectural choices and software optimizations significantly impact overall efficiency. For instance, techniques like tensor partitioning and batching affect parallelism and data access patterns impact memory utilization. Co-design provides a cross-layer perspective to unravel these dependencies.</p>
</section>
<section id="need-for-specialization" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="need-for-specialization">Need for Specialization</h4>
<p>AI workloads benefit from specialized operations like low-precision math and customized memory hierarchies. This motivates incorporating custom hardware tailored to neural network algorithms rather than relying solely on flexible software running on generic hardware <span class="citation" data-cites="sze2017efficient">(<a href="../../references.html#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>. However, the software stack must explicitly target custom hardware operations to realize the benefits.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="demand-for-higher-efficiency" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="demand-for-higher-efficiency">Demand for Higher Efficiency</h4>
<p>With growing model complexity, diminishing returns and overhead from optimizing only the hardware or software in isolation <span class="citation" data-cites="putnam2014reconfigurable">(<a href="../../references.html#ref-putnam2014reconfigurable" role="doc-biblioref">Putnam et al. 2014</a>)</span> arise. Inevitable tradeoffs arise that require global optimization across layers. Jointly co-designing hardware and software provides large compound efficiency gains.</p>
<div class="no-row-height column-margin column-container"><div id="ref-putnam2014reconfigurable" class="csl-entry" role="listitem">
Putnam, Andrew, Adrian M. Caulfield, Eric S. Chung, Derek Chiou, Kypros Constantinides, John Demme, Hadi Esmaeilzadeh, et al. 2014. <span>“A Reconfigurable Fabric for Accelerating Large-Scale Datacenter Services.”</span> <em>ACM SIGARCH Computer Architecture News</em> 42 (3): 13–24. <a href="https://doi.org/10.1145/2678373.2665678">https://doi.org/10.1145/2678373.2665678</a>.
</div></div></section>
</section>
<section id="principles-of-hardware-software-co-design" class="level3 page-columns page-full" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="principles-of-hardware-software-co-design"><span class="header-section-number">10.4.2</span> Principles of Hardware-Software Co-Design</h3>
<p>The underlying hardware architecture and software stack must be tightly integrated and co-optimized to build high-performance and efficient AI systems. Neither can be designed in isolation; maximizing their synergies requires a holistic approach known as hardware-software co-design.</p>
<p>The key goal is tailoring the hardware capabilities to match the algorithms and workloads run by the software. This requires a feedback loop between hardware architects and software developers to converge on optimized solutions. Several techniques enable effective co-design:</p>
<section id="hardware-aware-software-optimization" class="level4">
<h4 class="anchored" data-anchor-id="hardware-aware-software-optimization">Hardware-Aware Software Optimization</h4>
<p>The software stack can be optimized to leverage the underlying hardware capabilities better:</p>
<ul>
<li><strong>Parallelism:</strong> Parallelize matrix computations like convolution or attention layers to maximize throughput on vector engines.</li>
<li><strong>Memory Optimization:</strong> Tune data layouts to improve cache locality based on hardware profiling. This maximizes reuse and minimizes expensive DRAM access.</li>
<li><strong>Compression:</strong> Use sparsity in the models to reduce storage space and save on computation by zero-skipping operations.</li>
<li><strong>Custom Operations:</strong> Incorporate specialized operations like low-precision INT4 or bfloat16 into models to capitalize on dedicated hardware support.</li>
<li><strong>Dataflow Mapping:</strong> Explicitly map model stages to computational units to optimize data movement on hardware.</li>
</ul>
</section>
<section id="algorithm-driven-hardware-specialization" class="level4">
<h4 class="anchored" data-anchor-id="algorithm-driven-hardware-specialization">Algorithm-Driven Hardware Specialization</h4>
<p>Hardware can be tailored to suit the characteristics of ML algorithms better:</p>
<ul>
<li><strong>Custom Datatypes:</strong> Support low precision INT8/4 or bfloat16 in hardware for higher arithmetic density.</li>
<li><strong>On-Chip Memory:</strong> Increase SRAM bandwidth and lower access latency to match model memory access patterns.</li>
<li><strong>Domain-Specific Ops:</strong> Add hardware units for key ML functions like FFTs or matrix multiplication to reduce latency and energy.</li>
<li><strong>Model Profiling:</strong> Use model simulation and profiling to identify computational hotspots and optimize hardware.</li>
</ul>
<p>The key is collaborative feedback - insights from hardware profiling guide software optimizations, while algorithmic advances inform hardware specialization. This mutual enhancement provides multiplicative efficiency gains compared to isolated efforts.</p>
</section>
<section id="algorithm-hardware-co-exploration" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="algorithm-hardware-co-exploration">Algorithm-Hardware Co-exploration</h4>
<p>A powerful co-design technique involves jointly exploring innovations in neural network architectures and custom hardware design. This allows for finding ideal pairings tailored to each other’s strengths <span class="citation" data-cites="sze2017efficient">(<a href="../../references.html#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sze2017efficient" class="csl-entry" role="listitem">
Sze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017. <span>“Efficient Processing of Deep Neural Networks: <span>A</span> Tutorial and Survey.”</span> <em>Proc. IEEE</em> 105 (12): 2295–2329. <a href="https://doi.org/10.1109/jproc.2017.2761740">https://doi.org/10.1109/jproc.2017.2761740</a>.
</div><div id="ref-howard2017mobilenets" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. <span>“<span>MobileNets:</span> <span>Efficient</span> Convolutional Neural Networks for Mobile Vision Applications.”</span> <em>ArXiv Preprint</em>. <a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a>.
</div><div id="ref-jacob2018quantization" class="csl-entry" role="listitem">
Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. <span>“Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.”</span> In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2704–13. IEEE. <a href="https://doi.org/10.1109/cvpr.2018.00286">https://doi.org/10.1109/cvpr.2018.00286</a>.
</div><div id="ref-gale2019state" class="csl-entry" role="listitem">
Gale, Trevor, Erich Elsen, and Sara Hooker. 2019. <span>“The State of Sparsity in Deep Neural Networks.”</span> <em>ArXiv Preprint</em> abs/1902.09574. <a href="https://arxiv.org/abs/1902.09574">https://arxiv.org/abs/1902.09574</a>.
</div><div id="ref-asit2021accelerating" class="csl-entry" role="listitem">
Mishra, Asit K., Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021. <span>“Accelerating Sparse Deep Neural Networks.”</span> <em>CoRR</em> abs/2104.08378. <a href="https://arxiv.org/abs/2104.08378">https://arxiv.org/abs/2104.08378</a>.
</div></div><p>For instance, the shift to mobile architectures like MobileNets <span class="citation" data-cites="howard2017mobilenets">(<a href="../../references.html#ref-howard2017mobilenets" role="doc-biblioref">Howard et al. 2017</a>)</span> was guided by edge device constraints like model size and latency. The quantization <span class="citation" data-cites="jacob2018quantization">(<a href="../../references.html#ref-jacob2018quantization" role="doc-biblioref">Jacob et al. 2018</a>)</span> and pruning techniques <span class="citation" data-cites="gale2019state">(<a href="../../references.html#ref-gale2019state" role="doc-biblioref">Gale, Elsen, and Hooker 2019</a>)</span> that unlocked these efficient models became possible thanks to hardware accelerators with native low-precision integer support and pruning support <span class="citation" data-cites="asit2021accelerating">(<a href="../../references.html#ref-asit2021accelerating" role="doc-biblioref">Mishra et al. 2021</a>)</span>.</p>
<p>Attention-based models have thrived on massively parallel GPUs and ASICs, where their computation maps well spatially, as opposed to RNN architectures, which rely on sequential processing. The co-evolution of algorithms and hardware unlocked new capabilities.</p>
<p>Effective co-exploration requires close collaboration between algorithm researchers and hardware architects. Rapid prototyping on FPGAs <span class="citation" data-cites="zhang2015fpga">(<a href="../../references.html#ref-zhang2015fpga" role="doc-biblioref">C. Zhang et al. 2015</a>)</span> or specialized AI simulators allows quick evaluation of different pairings of model architectures and hardware designs pre-silicon.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2015fpga" class="csl-entry" role="listitem">
Zhang, Chen, Peng Li, Guangyu Sun, Yijin Guan, Bingjun Xiao, and Jason Optimizing Cong. 2015. <span>“<span>FPGA</span>-Based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 <span>ACM</span>.”</span> In <em>SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA</em>, 15:161–70.
</div></div><p>For example, Google’s TPU architecture evolved with optimizations to TensorFlow models to maximize performance on image classification. This tight feedback loop yielded models tailored for the TPU that would have been unlikely in isolation.</p>
<p>Studies have shown 2-5x higher performance and efficiency gains with algorithm-hardware co-exploration than isolated algorithm or hardware optimization efforts <span class="citation" data-cites="suda2016throughput">(<a href="../../references.html#ref-suda2016throughput" role="doc-biblioref">Suda et al. 2016</a>)</span>. Parallelizing the joint development also reduces time-to-deployment.</p>
<div class="no-row-height column-margin column-container"><div id="ref-suda2016throughput" class="csl-entry" role="listitem">
Suda, Naveen, Vikas Chandra, Ganesh Dasika, Abinash Mohanty, Yufei Ma, Sarma Vrudhula, Jae-sun Seo, and Yu Cao. 2016. <span>“Throughput-Optimized <span>OpenCL</span>-Based <span>FPGA</span> Accelerator for Large-Scale Convolutional Neural Networks.”</span> In <em>Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</em>, 16–25. ACM. <a href="https://doi.org/10.1145/2847263.2847276">https://doi.org/10.1145/2847263.2847276</a>.
</div></div><p>Overall, exploring the tight interdependencies between model innovation and hardware advances unlocks opportunities that must be visible when tackled sequentially. This synergistic co-design yields solutions greater than the sum of their parts.</p>
</section>
</section>
<section id="challenges" class="level3 page-columns page-full" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="challenges"><span class="header-section-number">10.4.3</span> Challenges</h3>
<p>While collaborative co-design can improve efficiency, adaptability, and time to market, it also has engineering and organizational challenges.</p>
<section id="increased-prototyping-costs" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="increased-prototyping-costs">Increased Prototyping Costs</h4>
<p>More extensive prototyping is required to evaluate different hardware-software pairings. The need for rapid, iterative prototypes on FPGAs or emulators increases validation overhead. For example, Microsoft found that more prototypes were needed to co-design an AI accelerator than sequential design <span class="citation" data-cites="fowers2018configurable">(<a href="../../references.html#ref-fowers2018configurable" role="doc-biblioref">Fowers et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-fowers2018configurable" class="csl-entry" role="listitem">
Fowers, Jeremy, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, et al. 2018. <span>“A Configurable Cloud-Scale <span>DNN</span> Processor for Real-Time <span>AI</span>.”</span> In <em>2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</em>, 1–14. IEEE; IEEE. <a href="https://doi.org/10.1109/isca.2018.00012">https://doi.org/10.1109/isca.2018.00012</a>.
</div></div></section>
<section id="team-and-organizational-hurdles" class="level4">
<h4 class="anchored" data-anchor-id="team-and-organizational-hurdles">Team and Organizational Hurdles</h4>
<p>Co-design requires close coordination between traditionally disconnected hardware and software groups. This could introduce communication issues or misaligned priorities and schedules. Navigating different engineering workflows is also challenging. Some organizational inertia to adopting integrated practices may exist.</p>
</section>
<section id="simulation-and-modeling-complexity" class="level4">
<h4 class="anchored" data-anchor-id="simulation-and-modeling-complexity">Simulation and Modeling Complexity</h4>
<p>Capturing subtle interactions between hardware and software layers for joint simulation and modeling adds significant complexity. Full cross-layer abstractions are difficult to construct quantitatively before implementation, making holistic optimizations harder to quantify ahead of time.</p>
</section>
<section id="over-specialization-risks" class="level4">
<h4 class="anchored" data-anchor-id="over-specialization-risks">Over-Specialization Risks</h4>
<p>Tight co-design bears the risk of overfitting optimizations to current algorithms, sacrificing generality. For example, hardware tuned exclusively for Transformer models could underperform on future techniques. Maintaining flexibility requires foresight.</p>
</section>
<section id="adoption-challenges" class="level4">
<h4 class="anchored" data-anchor-id="adoption-challenges">Adoption Challenges</h4>
<p>Engineers comfortable with established discrete hardware or software design practices may only accept familiar collaborative workflows. Despite the long-term benefits, projects could face friction in transitioning to co-design.</p>
</section>
</section>
</section>
<section id="software-for-ai-hardware" class="level2 page-columns page-full" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="software-for-ai-hardware"><span class="header-section-number">10.5</span> Software for AI Hardware</h2>
<p>Specialized hardware accelerators like GPUs, TPUs, and FPGAs are essential to delivering high-performance artificial intelligence applications. However, an extensive software stack is required to leverage these hardware platforms effectively, spanning the entire development and deployment lifecycle. Frameworks and libraries form the backbone of AI hardware, offering sets of robust, pre-built code, algorithms, and functions specifically optimized to perform various AI tasks on different hardware. They are designed to simplify the complexities of utilizing the hardware from scratch, which can be time-consuming and prone to error. Software plays an important role in the following:</p>
<ul>
<li>Providing programming abstractions and models like CUDA and OpenCL to map computations onto accelerators.</li>
<li>Integrating accelerators into popular deep learning frameworks like TensorFlow and PyTorch.</li>
<li>Compilers and tools to optimize across the hardware-software stack.</li>
<li>Simulation platforms to model hardware and software together.</li>
<li>Infrastructure to manage deployment on accelerators.</li>
</ul>
<p>This expansive software ecosystem is as important as the hardware in delivering performant and efficient AI applications. This section overviews the tools available at each stack layer to enable developers to build and run AI systems powered by hardware acceleration.</p>
<section id="sec-programming-models" class="level3 page-columns page-full" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="sec-programming-models"><span class="header-section-number">10.5.1</span> Programming Models</h3>
<p>Programming models provide abstractions to map computations and data onto heterogeneous hardware accelerators:</p>
<ul>
<li><strong><a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a>:</strong> Nvidia’s parallel programming model to leverage GPUs using extensions to languages like C/C++. Allows launching kernels across GPU cores <span class="citation" data-cites="luebke2008cuda">(<a href="../../references.html#ref-luebke2008cuda" role="doc-biblioref">Luebke 2008</a>)</span>.</li>
<li><strong><a href="https://www.khronos.org/opencl/">OpenCL</a>:</strong> Open standard for writing programs spanning CPUs, GPUs, FPGAs, and other accelerators. Specifies a heterogeneous computing framework <span class="citation" data-cites="munshi2009opencl">(<a href="../../references.html#ref-munshi2009opencl" role="doc-biblioref">Munshi 2009</a>)</span>.</li>
<li><strong><a href="https://www.opengl.org">OpenGL/WebGL</a>:</strong> 3D graphics programming interfaces that can map general-purpose code to GPU cores <span class="citation" data-cites="segal1999opengl">(<a href="../../references.html#ref-segal1999opengl" role="doc-biblioref">Segal and Akeley 1999</a>)</span>.</li>
<li><strong><a href="https://www.verilog.com">Verilog</a>/VHDL:</strong> Hardware description languages (HDLs) used to configure FPGAs as AI accelerators by specifying digital circuits <span class="citation" data-cites="gannot1994verilog">(<a href="../../references.html#ref-gannot1994verilog" role="doc-biblioref">Gannot and Ligthart 1994</a>)</span>.</li>
<li><strong><a href="https://tvm.apache.org">TVM</a>:</strong> A Compiler framework providing a Python frontend to optimize and map deep learning models onto diverse hardware backends <span class="citation" data-cites="chen2018tvm">(<a href="../../references.html#ref-chen2018tvm" role="doc-biblioref">Chen et al. 2018</a>)</span>.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-luebke2008cuda" class="csl-entry" role="listitem">
Luebke, David. 2008. <span>“<span>CUDA:</span> <span>Scalable</span> Parallel Programming for High-Performance Scientific Computing.”</span> In <em>2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro</em>, 836–38. IEEE. <a href="https://doi.org/10.1109/isbi.2008.4541126">https://doi.org/10.1109/isbi.2008.4541126</a>.
</div><div id="ref-munshi2009opencl" class="csl-entry" role="listitem">
Munshi, Aaftab. 2009. <span>“The <span>OpenCL</span> Specification.”</span> In <em>2009 IEEE Hot Chips 21 Symposium (HCS)</em>, 1–314. IEEE. <a href="https://doi.org/10.1109/hotchips.2009.7478342">https://doi.org/10.1109/hotchips.2009.7478342</a>.
</div><div id="ref-segal1999opengl" class="csl-entry" role="listitem">
Segal, Mark, and Kurt Akeley. 1999. <span>“The <span>OpenGL</span> Graphics System: <span>A</span> Specification (Version 1.1).”</span>
</div><div id="ref-gannot1994verilog" class="csl-entry" role="listitem">
Gannot, G., and M. Ligthart. 1994. <span>“Verilog <span>HDL</span> Based <span>FPGA</span> Design.”</span> In <em>International Verilog HDL Conference</em>, 86–92. IEEE. <a href="https://doi.org/10.1109/ivc.1994.323743">https://doi.org/10.1109/ivc.1994.323743</a>.
</div><div id="ref-chen2018tvm" class="csl-entry" role="listitem">
Chen, Tianqi, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, et al. 2018. <span>“<span>TVM:</span> <span>An</span> Automated End-to-End Optimizing Compiler for Deep Learning.”</span> In <em>13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</em>, 578–94.
</div></div><p>Key challenges include expressing parallelism, managing memory across devices, and matching algorithms to hardware capabilities. Abstractions must balance portability with allowing hardware customization. Programming models enable developers to harness accelerators without hardware expertise. These details are discussed in the <a href="../../contents/frameworks/frameworks.html">AI frameworks</a> section.</p>
<div id="exr-tvm" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise&nbsp;10.1: Software for AI Hardware - TVM
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>We’ve learned that fancy AI hardware needs special software to work magic. TVM is like a super-smart translator, turning your code into instructions that accelerators understand. In this Colab, we’ll use TVM to make a pretend accelerator called VTA do matrix multiplication super fast. Ready to see how software powers up hardware?</p>
<p><a href="https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/04a_TVM_Tutorial_VTA_Mat_Mult.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="libraries-and-runtimes" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="libraries-and-runtimes"><span class="header-section-number">10.5.2</span> Libraries and Runtimes</h3>
<p>Specialized libraries and runtimes provide software abstractions to access and maximize the utilization of AI accelerators:</p>
<ul>
<li><strong>Math Libraries:</strong> Highly optimized implementations of linear algebra primitives like GEMM, FFTs, convolutions, etc., tailored to the target hardware. <a href="https://developer.nvidia.com/cublas">Nvidia cuBLAS</a>, <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html">Intel MKL</a>, and <a href="https://www.arm.com/technologies/compute-library">Arm compute libraries</a> are examples.</li>
<li><strong>Framework Integrations:</strong> Libraries to accelerate deep learning frameworks like TensorFlow, PyTorch, and MXNet on supported hardware. For example, <a href="https://developer.nvidia.com/cudnn">cuDNN</a> accelerates CNNs on Nvidia GPUs.</li>
<li><strong>Runtimes:</strong> Software to handle accelerator execution, including scheduling, synchronization, memory management, and other tasks. <a href="https://developer.nvidia.com/tensorrt">Nvidia TensorRT</a> is an inference optimizer and runtime.</li>
<li><strong>Drivers and Firmware:</strong> Low-level software to interface with hardware, initialize devices, and handle execution. Vendors like Xilinx provide drivers for their accelerator boards.</li>
</ul>
<p>For instance, PyTorch integrators use cuDNN and cuBLAS libraries to accelerate training on Nvidia GPUs. The TensorFlow XLA runtime optimizes and compiles models for accelerators like TPUs. Drivers initialize devices and offload operations.</p>
<p>The challenges include efficiently partitioning and scheduling workloads across heterogeneous devices like multi-GPU nodes. Runtimes must also minimize the overhead of data transfers and synchronization.</p>
<p>Libraries, runtimes, and drivers provide optimized building blocks that deep learning developers can leverage to tap into accelerator performance without hardware programming expertise. Their optimization is essential for production deployments.</p>
</section>
<section id="optimizing-compilers" class="level3" data-number="10.5.3">
<h3 data-number="10.5.3" class="anchored" data-anchor-id="optimizing-compilers"><span class="header-section-number">10.5.3</span> Optimizing Compilers</h3>
<p>Optimizing compilers is key in extracting maximum performance and efficiency from hardware accelerators for AI workloads. They apply optimizations spanning algorithmic changes, graph-level transformations, and low-level code generation.</p>
<ul>
<li><strong>Algorithm Optimization:</strong> Techniques like quantization, pruning, and neural architecture search to enhance model efficiency and match hardware capabilities.</li>
<li><strong>Graph Optimizations:</strong> Graph-level optimizations like operator fusion, rewriting, and layout transformations to optimize performance on target hardware.</li>
<li><strong>Code Generation:</strong> Generating optimized low-level code for accelerators from high-level models and frameworks.</li>
</ul>
<p>For example, the TVM open compiler stack applies quantization for a BERT model targeting Arm GPUs. It fuses pointwise convolution operations and transforms the weight layout to optimize memory access. Finally, it emits optimized OpenGL code to run the GPU workload.</p>
<p>Key compiler optimizations include maximizing parallelism, improving data locality and reuse, minimizing memory footprint, and exploiting custom hardware operations. Compilers build and optimize machine learning workloads holistically across hardware components like CPUs, GPUs, and other accelerators.</p>
<p>However, efficiently mapping complex models introduces challenges like efficiently partitioning workloads across heterogeneous devices. Production-level compilers also require extensive time tuning on representative workloads. Still, optimizing compilers is indispensable in unlocking the full capabilities of AI accelerators.</p>
</section>
<section id="simulation-and-modeling" class="level3 page-columns page-full" data-number="10.5.4">
<h3 data-number="10.5.4" class="anchored" data-anchor-id="simulation-and-modeling"><span class="header-section-number">10.5.4</span> Simulation and Modeling</h3>
<p>Simulation software is important in hardware-software co-design. It enables joint modeling of proposed hardware architectures and software stacks:</p>
<ul>
<li><strong>Hardware Simulation:</strong> Platforms like <a href="https://www.gem5.org">Gem5</a> allow detailed simulation of hardware components like pipelines, caches, interconnects, and memory hierarchies. Engineers can model hardware changes without physical prototyping <span class="citation" data-cites="binkert2011gem5">(<a href="../../references.html#ref-binkert2011gem5" role="doc-biblioref">Binkert et al. 2011</a>)</span>.</li>
<li><strong>Software Simulation:</strong> Compiler stacks like <a href="https://tvm.apache.org">TVM</a> support the simulation of machine learning workloads to estimate performance on target hardware architectures. This assists with software optimizations.</li>
<li><strong>Co-simulation:</strong> Unified platforms like the SCALE-Sim <span class="citation" data-cites="samajdar2018scale">(<a href="../../references.html#ref-samajdar2018scale" role="doc-biblioref">Samajdar et al. 2018</a>)</span> integrate hardware and software simulation into a single tool. This enables what-if analysis to quantify the system-level impacts of cross-layer optimizations early in the design cycle.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-binkert2011gem5" class="csl-entry" role="listitem">
Binkert, Nathan, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, et al. 2011. <span>“The Gem5 Simulator.”</span> <em>ACM SIGARCH Computer Architecture News</em> 39 (2): 1–7. <a href="https://doi.org/10.1145/2024716.2024718">https://doi.org/10.1145/2024716.2024718</a>.
</div><div id="ref-samajdar2018scale" class="csl-entry" role="listitem">
Samajdar, Ananda, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and Tushar Krishna. 2018. <span>“Scale-Sim: <span>Systolic</span> Cnn Accelerator Simulator.”</span> <em>ArXiv Preprint</em> abs/1811.02883. <a href="https://arxiv.org/abs/1811.02883">https://arxiv.org/abs/1811.02883</a>.
</div></div><p>For example, an FPGA-based AI accelerator design could be simulated using Verilog hardware description language and synthesized into a Gem5 model. Verilog is well-suited for describing the digital logic and interconnects of the accelerator architecture. Verilog allows the designer to specify the datapaths, control logic, on-chip memories, and other components implemented in the FPGA fabric. Once the Verilog design is complete, it can be synthesized into a model that simulates the behavior of the hardware, such as using the Gem5 simulator. Gem5 is useful for this task because it allows the modeling of full systems, including processors, caches, buses, and custom accelerators. Gem5 supports interfacing Verilog models of hardware to the simulation, enabling unified system modeling.</p>
<p>The synthesized FPGA accelerator model could then have ML workloads simulated using TVM compiled onto it within the Gem5 environment for unified modeling. TVM allows optimized compilation of ML models onto heterogeneous hardware like FPGAs. Running TVM-compiled workloads on the accelerator within the Gem5 simulation provides an integrated way to validate and refine the hardware design, software stack, and system integration before physically realizing the accelerator on a real FPGA.</p>
<p>This type of co-simulation provides estimations of overall metrics like throughput, latency, and power to guide co-design before expensive physical prototyping. They also assist with partitioning optimizations between hardware and software to guide design tradeoffs.</p>
<p>However, accuracy in modeling subtle low-level interactions between components is limited. Quantified simulations are estimates but cannot wholly replace physical prototypes and testing. Still, unified simulation and modeling provide invaluable early insights into system-level optimization opportunities during the co-design process.</p>
</section>
</section>
<section id="benchmarking-ai-hardware" class="level2 page-columns page-full" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="benchmarking-ai-hardware"><span class="header-section-number">10.6</span> Benchmarking AI Hardware</h2>
<p>Benchmarking is a critical process that quantifies and compares the performance of various hardware platforms designed to speed up artificial intelligence applications. It guides purchasing decisions, development focus, and performance optimization efforts for hardware manufacturers and software developers.</p>
<p>The <a href="../benchmarking/benchmarking. cmd">benchmarking chapter</a> explores this topic in great detail, explaining why it has become an indispensable part of the AI hardware development cycle and how it impacts the broader technology landscape. Here, we will briefly review the main concepts, but we recommend that you refer to the chapter for more details.</p>
<p>Benchmarking suites such as MLPerf, Fathom, and AI Benchmark offer a set of standardized tests that can be used across different hardware platforms. These suites measure AI accelerator performance across various neural networks and machine learning tasks, from basic image classification to complex language processing. Providing a common ground for Comparison, they help ensure that performance claims are consistent and verifiable. These “tools” are applied not only to guide the development of hardware but also to ensure that the software stack leverages the full potential of the underlying architecture.</p>
<ul>
<li><strong>MLPerf:</strong> Includes a broad set of benchmarks covering both training <span class="citation" data-cites="mattson2020mlperf">(<a href="../../references.html#ref-mattson2020mlperf" role="doc-biblioref">Mattson et al. 2020</a>)</span> and inference <span class="citation" data-cites="reddi2020mlperf">(<a href="../../references.html#ref-reddi2020mlperf" role="doc-biblioref">Reddi et al. 2020</a>)</span> for a range of machine learning tasks.</li>
<li><strong>Fathom:</strong> Focuses on core operations in deep learning models, emphasizing their execution on different architectures <span class="citation" data-cites="adolf2016fathom">(<a href="../../references.html#ref-adolf2016fathom" role="doc-biblioref">Adolf et al. 2016</a>)</span>.</li>
<li><strong>AI Benchmark:</strong> Targets mobile and consumer devices, assessing AI performance in end-user applications <span class="citation" data-cites="ignatov2018ai">(<a href="../../references.html#ref-ignatov2018ai" role="doc-biblioref">Ignatov et al. 2018</a>)</span>.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-mattson2020mlperf" class="csl-entry" role="listitem">
Mattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg Diamos, David Kanter, Paulius Micikevicius, et al. 2020. <span>“<span>MLPerf:</span> <span>An</span> Industry Standard Benchmark Suite for Machine Learning Performance.”</span> <em>IEEE Micro</em> 40 (2): 8–16. <a href="https://doi.org/10.1109/mm.2020.2974843">https://doi.org/10.1109/mm.2020.2974843</a>.
</div><div id="ref-reddi2020mlperf" class="csl-entry" role="listitem">
Reddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson, Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2020. <span>“<span>MLPerf</span> Inference Benchmark.”</span> In <em>2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</em>, 446–59. IEEE; IEEE. <a href="https://doi.org/10.1109/isca45697.2020.00045">https://doi.org/10.1109/isca45697.2020.00045</a>.
</div><div id="ref-adolf2016fathom" class="csl-entry" role="listitem">
Adolf, Robert, Saketh Rama, Brandon Reagen, Gu-yeon Wei, and David Brooks. 2016. <span>“Fathom: <span>Reference</span> Workloads for Modern Deep Learning Methods.”</span> In <em>2016 IEEE International Symposium on Workload Characterization (IISWC)</em>, 1–10. IEEE; IEEE. <a href="https://doi.org/10.1109/iiswc.2016.7581275">https://doi.org/10.1109/iiswc.2016.7581275</a>.
</div><div id="ref-ignatov2018ai" class="csl-entry" role="listitem">
Ignatov, Andrey, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, and Luc Van Gool. 2018. <span>“<span>AI</span> Benchmark: <span>Running</span> Deep Neural Networks on Android Smartphones,”</span> 0–0.
</div></div><p>Benchmarks also have performance metrics that are the quantifiable measures used to evaluate the effectiveness of AI accelerators. These metrics provide a comprehensive view of an accelerator’s capabilities and are used to guide the design and selection process for AI systems. Common metrics include:</p>
<ul>
<li><strong>Throughput:</strong> Usually measured in operations per second, this metric indicates the volume of computations an accelerator can handle.</li>
<li><strong>Latency:</strong> The time delay from input to output in a system is vital for real-time processing tasks.</li>
<li><strong>Energy Efficiency:</strong> Calculated as computations per watt, representing the tradeoff between performance and power consumption.</li>
<li><strong>Cost Efficiency:</strong> This evaluates the cost of operation relative to performance, an essential metric for budget-conscious deployments.</li>
<li><strong>Accuracy:</strong> In inference tasks, the precision of computations is critical and sometimes balanced against speed.</li>
<li><strong>Scalability:</strong> The ability of the system to maintain performance gains as the computational load scales up.</li>
</ul>
<p>Benchmark results give insights beyond just numbers—they can reveal bottlenecks in the software and hardware stack. For example, benchmarks may show how increased batch size improves GPU utilization by providing more parallelism or how compiler optimizations boost TPU performance. These learnings enable continuous optimization <span class="citation" data-cites="jia2019beyond">(<a href="../../references.html#ref-jia2019beyond" role="doc-biblioref">Zhihao Jia, Zaharia, and Aiken 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jia2019beyond" class="csl-entry" role="listitem">
Jia, Zhihao, Matei Zaharia, and Alex Aiken. 2019. <span>“Beyond Data and Model Parallelism for Deep Neural Networks.”</span> In <em>Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019</em>, edited by Ameet Talwalkar, Virginia Smith, and Matei Zaharia. mlsys.org. <a href="https://proceedings.mlsys.org/book/265.pdf">https://proceedings.mlsys.org/book/265.pdf</a>.
</div><div id="ref-zhu2018benchmarking" class="csl-entry" role="listitem">
Zhu, Hongyu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand Jayarajan, Amar Phanishayee, Bianca Schroeder, and Gennady Pekhimenko. 2018. <span>“Benchmarking and Analyzing Deep Neural Network Training.”</span> In <em>2018 IEEE International Symposium on Workload Characterization (IISWC)</em>, 88–100. IEEE; IEEE. <a href="https://doi.org/10.1109/iiswc.2018.8573476">https://doi.org/10.1109/iiswc.2018.8573476</a>.
</div></div><p>Standardized benchmarking provides a quantified, comparable evaluation of AI accelerators to inform design, purchasing, and optimization. However, real-world performance validation remains essential as well <span class="citation" data-cites="zhu2018benchmarking">(<a href="../../references.html#ref-zhu2018benchmarking" role="doc-biblioref">H. Zhu et al. 2018</a>)</span>.</p>
</section>
<section id="challenges-and-solutions" class="level2 page-columns page-full" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="challenges-and-solutions"><span class="header-section-number">10.7</span> Challenges and Solutions</h2>
<p>AI accelerators offer impressive performance improvements, but significant portability and compatibility challenges often need to be improved in their integration into the broader AI landscape. The crux of the issue lies in the diversity of the AI ecosystem—a vast array of machine learning accelerators, frameworks, and programming languages exist, each with its unique features and requirements.</p>
<section id="portabilitycompatibility-issues" class="level3" data-number="10.7.1">
<h3 data-number="10.7.1" class="anchored" data-anchor-id="portabilitycompatibility-issues"><span class="header-section-number">10.7.1</span> Portability/Compatibility Issues</h3>
<p>Developers frequently encounter difficulties transferring their AI models from one hardware environment to another. For example, a machine learning model developed for a desktop environment in Python using the PyTorch framework, optimized for an Nvidia GPU, may not easily transition to a more constrained device such as the Arduino Nano 33 BLE. This complexity stems from stark differences in programming requirements - Python and PyTorch on the desktop versus a C++ environment on an Arduino, not to mention the shift from x86 architecture to ARM ISA.</p>
<p>These divergences highlight the intricacy of portability within AI systems. Moreover, the rapid advancement in AI algorithms and models means that hardware accelerators must continually adapt, creating a moving target for compatibility. The absence of universal standards and interfaces compounds the issue, making deploying AI solutions consistently across various devices and platforms challenging.</p>
<section id="solutions-and-strategies" class="level4">
<h4 class="anchored" data-anchor-id="solutions-and-strategies">Solutions and Strategies</h4>
<p>To address these hurdles, the AI industry is moving towards several solutions:</p>
<section id="standardization-initiatives" class="level5">
<h5 class="anchored" data-anchor-id="standardization-initiatives">Standardization Initiatives</h5>
<p>The <a href="https://onnx.ai/">Open Neural Network Exchange (ONNX)</a> is at the forefront of this pursuit, proposing an open and shared ecosystem that promotes model interchangeability. ONNX facilitates the use of AI models across various frameworks, allowing models trained in one environment to be efficiently deployed in another, significantly reducing the need for time-consuming rewrites or adjustments.</p>
</section>
<section id="cross-platform-frameworks" class="level5">
<h5 class="anchored" data-anchor-id="cross-platform-frameworks">Cross-Platform Frameworks</h5>
<p>Complementing the standardization efforts, cross-platform frameworks such as TensorFlow Lite and PyTorch Mobile have been developed specifically to create cohesion between diverse computational environments ranging from desktops to mobile and embedded devices. These frameworks offer streamlined, lightweight versions of their parent frameworks, ensuring compatibility and functional integrity across different hardware types without sacrificing performance. This ensures that developers can create applications with the confidence that they will work on many devices, bridging a gap that has traditionally posed a considerable challenge in AI development.</p>
</section>
<section id="hardware-agnostic-platforms" class="level5">
<h5 class="anchored" data-anchor-id="hardware-agnostic-platforms">Hardware-agnostic Platforms</h5>
<p>The rise of hardware-agnostic platforms has also played an important role in democratizing the use of AI. By creating environments where AI applications can be executed on various accelerators, these platforms remove the burden of hardware-specific coding from developers. This abstraction simplifies the development process and opens up new possibilities for innovation and application deployment, free from the constraints of hardware specifications.</p>
</section>
<section id="advanced-compilation-tools" class="level5">
<h5 class="anchored" data-anchor-id="advanced-compilation-tools">Advanced Compilation Tools</h5>
<p>In addition, the advent of advanced compilation tools like TVM, an end-to-end tensor compiler, offers an optimized path through the jungle of diverse hardware architectures. TVM equips developers with the means to fine-tune machine learning models for a broad spectrum of computational substrates, ensuring optimal performance and avoiding manual model adjustment each time there is a shift in the underlying hardware.</p>
</section>
<section id="community-and-industry-collaboration" class="level5">
<h5 class="anchored" data-anchor-id="community-and-industry-collaboration">Community and Industry Collaboration</h5>
<p>The collaboration between open-source communities and industry consortia cannot be understated. These collective bodies are instrumental in forming shared standards and best practices that all developers and manufacturers can adhere to. Such collaboration fosters a more unified and synergistic AI ecosystem, significantly diminishing the prevalence of portability issues and smoothing the path toward global AI integration and advancement. Through these combined efforts, AI is steadily moving toward a future where seamless model deployment across various platforms becomes a standard rather than an exception.</p>
<p>Solving the portability challenges is crucial for the AI field to realize the full potential of hardware accelerators in a dynamic and diverse technological landscape. It requires a concerted effort from hardware manufacturers, software developers, and standard bodies to create a more interoperable and flexible environment. With continued innovation and collaboration, the AI community can pave the way for seamless integration and deployment of AI models across many platforms.</p>
</section>
</section>
</section>
<section id="power-consumption-concerns" class="level3 page-columns page-full" data-number="10.7.2">
<h3 data-number="10.7.2" class="anchored" data-anchor-id="power-consumption-concerns"><span class="header-section-number">10.7.2</span> Power Consumption Concerns</h3>
<p>Power consumption is a crucial issue in the development and operation of data center AI accelerators, like Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) <span class="citation" data-cites="jouppi2017indatacenter">(<a href="../../references.html#ref-jouppi2017indatacenter" role="doc-biblioref">N. P. Jouppi et al. 2017b</a>)</span> <span class="citation" data-cites="norrie2021design">(<a href="../../references.html#ref-norrie2021design" role="doc-biblioref">Norrie et al. 2021</a>)</span> <span class="citation" data-cites="jouppi2023tpu">(<a href="../../references.html#ref-jouppi2023tpu" role="doc-biblioref">N. Jouppi et al. 2023</a>)</span>. These powerful components are the backbone of contemporary AI infrastructure, but their high energy demands contribute to the environmental impact of technology and drive up operational costs significantly. As data processing needs become more complex, with the popularity of AI and deep learning increasing, there’s a pressing demand for GPUs and TPUs that can deliver the necessary computational power more efficiently. The impact of such advancements is two-fold: they can lower these technologies’ environmental footprint and reduce the cost of running AI applications.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jouppi2017indatacenter" class="csl-entry" role="listitem">
———, et al. 2017b. <span>“In-Datacenter Performance Analysis of a Tensor Processing Unit.”</span> In <em>Proceedings of the 44th Annual International Symposium on Computer Architecture</em>, 1–12. ISCA ’17. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3079856.3080246">https://doi.org/10.1145/3079856.3080246</a>.
</div><div id="ref-norrie2021design" class="csl-entry" role="listitem">
Norrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li, James Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021. <span>“The Design Process for Google’s Training Chips: <span>Tpuv2</span> and <span>TPUv3</span>.”</span> <em>IEEE Micro</em> 41 (2): 56–63. <a href="https://doi.org/10.1109/mm.2021.3058217">https://doi.org/10.1109/mm.2021.3058217</a>.
</div><div id="ref-jouppi2023tpu" class="csl-entry" role="listitem">
Jouppi, Norm, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, et al. 2023. <span>“<span>TPU</span> V4: <span>An</span> Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings.”</span> In <em>Proceedings of the 50th Annual International Symposium on Computer Architecture</em>. ISCA ’23. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3579371.3589350">https://doi.org/10.1145/3579371.3589350</a>.
</div></div><p>Emerging hardware technologies are at the cusp of revolutionizing power efficiency in this sector. Photonic computing, for instance, uses light rather than electricity to carry information, offering a promise of high-speed processing with a fraction of the power usage. We delve deeper into this and other innovative technologies in the “Emerging Hardware Technologies” section, exploring their potential to address current power consumption challenges.</p>
<p>At the edge of the network, AI accelerators are engineered to process data on devices like smartphones, IoT sensors, and smart wearables. These devices often work under severe power limitations, necessitating a careful balancing act between performance and power usage. A high-performance AI model may provide quick results but at the cost of depleting battery life swiftly and increasing thermal output, which may affect the device’s functionality and durability. The stakes are higher for devices deployed in remote or hard-to-reach areas, where consistent power supply cannot be guaranteed, underscoring the need for low-power-consuming solutions.</p>
<p>Latency issues further compound the challenge of power efficiency at the edge. Edge AI applications in fields such as autonomous driving and healthcare monitoring require speed, precision, and reliability, as delays in processing can lead to serious safety risks. For these applications, developers must optimize both the AI algorithms and the hardware design to strike an optimal balance between power consumption and latency.</p>
<p>This optimization effort is not just about making incremental improvements to existing technologies; it’s about rethinking how and where we process AI tasks. By designing AI accelerators that are both power-efficient and capable of quick processing, we can ensure these devices serve their intended purposes without unnecessary energy use or compromised performance. Such developments could propel the widespread adoption of AI across various sectors, enabling smarter, safer, and more sustainable use of technology.</p>
</section>
<section id="overcoming-resource-constraints" class="level3 page-columns page-full" data-number="10.7.3">
<h3 data-number="10.7.3" class="anchored" data-anchor-id="overcoming-resource-constraints"><span class="header-section-number">10.7.3</span> Overcoming Resource Constraints</h3>
<p>Resource constraints also pose a significant challenge for Edge AI accelerators, as these specialized hardware and software solutions must deliver robust performance within the limitations of edge devices. Due to power and size limitations, edge AI accelerators often have restricted computation, memory, and storage capacity <span class="citation" data-cites="lin2022ondevice">(<a href="../../references.html#ref-lin2022ondevice" role="doc-biblioref">L. Zhu et al. 2023</a>)</span>. This scarcity of resources necessitates a careful allocation of processing capabilities to execute machine learning models efficiently.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2022ondevice" class="csl-entry" role="listitem">
Zhu, Ligeng, Lanxiang Hu, Ji Lin, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han. 2023. <span>“<span>PockEngine:</span> <span>Sparse</span> and Efficient Fine-Tuning in a Pocket.”</span> In <em>56th Annual IEEE/ACM International Symposium on Microarchitecture</em>. ACM. <a href="https://doi.org/10.1145/3613424.3614307">https://doi.org/10.1145/3613424.3614307</a>.
</div><div id="ref-lin2023awq" class="csl-entry" role="listitem">
Lin, Ji, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. <span>“<span>AWQ:</span> <span class="nocase">Activation-aware</span> Weight Quantization for <span>LLM</span> Compression and Acceleration.”</span> <em>arXiv</em>.
</div><div id="ref-Li2020Additive" class="csl-entry" role="listitem">
Li, Yuhang, Xin Dong, and Wei Wang. 2020. <span>“Additive Powers-of-Two Quantization: <span>An</span> Efficient Non-Uniform Discretization for Neural Networks.”</span> In <em>8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net. <a href="https://openreview.net/forum?id=BkgXT24tDS">https://openreview.net/forum?id=BkgXT24tDS</a>.
</div><div id="ref-wang2020apq" class="csl-entry" role="listitem">
Wang, Tianzhe, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, and Song Han. 2020. <span>“<span>APQ:</span> <span>Joint</span> Search for Network Architecture, Pruning and Quantization Policy.”</span> In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2075–84. IEEE. <a href="https://doi.org/10.1109/cvpr42600.2020.00215">https://doi.org/10.1109/cvpr42600.2020.00215</a>.
</div></div><p>Moreover, managing constrained resources demands innovative approaches, including model quantization <span class="citation" data-cites="lin2023awq">(<a href="../../references.html#ref-lin2023awq" role="doc-biblioref">Lin et al. 2023</a>)</span> <span class="citation" data-cites="Li2020Additive">(<a href="../../references.html#ref-Li2020Additive" role="doc-biblioref">Li, Dong, and Wang 2020</a>)</span>, pruning <span class="citation" data-cites="wang2020apq">(<a href="../../references.html#ref-wang2020apq" role="doc-biblioref">Wang et al. 2020</a>)</span>, and optimizing inference pipelines. Edge AI accelerators must strike a delicate balance between providing meaningful AI functionality and not exhausting available resources while maintaining low power consumption. Overcoming these resource constraints is crucial to ensure the successful deployment of AI at the edge, where many applications, from IoT to mobile devices, rely on efficiently using limited hardware resources to deliver real-time and intelligent decision-making.</p>
</section>
</section>
<section id="emerging-technologies" class="level2 page-columns page-full" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="emerging-technologies"><span class="header-section-number">10.8</span> Emerging Technologies</h2>
<p>Thus far, we have discussed AI hardware technology in the context of conventional von Neumann architecture design and CMOS-based implementation. These specialized AI chips offer benefits like higher throughput and power efficiency but rely on traditional computing principles. The relentless growth in demand for AI computing power is driving innovations in integration methods for AI hardware.</p>
<p>Two leading approaches have emerged for maximizing compute density—wafer-scale integration and chiplet-based architectures—which we will discuss in this section. Looking much further ahead, we will examine emerging technologies that diverge from conventional architectures and adopt fundamentally different approaches for AI-specialized computing.</p>
<p>Some of these unconventional paradigms include neuromorphic computing, which mimics biological neural networks; quantum computing, which leverages quantum mechanical effects; and optical computing, which utilizes photons instead of electrons. Beyond novel computing substrates, new device technologies are enabling additional gains through better memory and interconnecting.</p>
<p>Examples include memristors for in-memory computing and nanophotonics for integrated photonic communication. Together, these technologies offer the potential for orders of magnitude improvements in speed, efficiency, and scalability compared to current AI hardware. We will examine these in this section.</p>
<section id="integration-methods" class="level3 page-columns page-full" data-number="10.8.1">
<h3 data-number="10.8.1" class="anchored" data-anchor-id="integration-methods"><span class="header-section-number">10.8.1</span> Integration Methods</h3>
<p>Integration methods refer to the approaches used to combine and interconnect an AI chip or system’s various computational and memory components. By closely linking the key processing elements, integration aims to maximize performance, power efficiency, and density.</p>
<p>In the past, AI computing was primarily performed on CPUs and GPUs built using conventional integration methods. These discrete components were manufactured separately and connected together on a board. However, this loose integration creates bottlenecks, such as data transfer overheads.</p>
<p>As AI workloads have grown, there is increasing demand for tighter integration between computing, memory, and communication elements. Some key drivers of integration include:</p>
<ul>
<li><strong>Minimizing data movement:</strong> Tight integration reduces latency and power for moving data between components. This improves efficiency.</li>
<li><strong>Customization:</strong> Tailoring all system components to AI workloads allows optimizations throughout the hardware stack.</li>
<li><strong>Parallelism:</strong> Integrating many processing elements enables massively parallel computation.</li>
<li><strong>Density:</strong> Tighter integration allows more transistors and memory to be packed into a given area.</li>
<li><strong>Cost:</strong> Economies of scale from large integrated systems can reduce costs.</li>
</ul>
<p>In response, new manufacturing techniques like wafer-scale fabrication and advanced packaging now allow much higher levels of integration. The goal is to create unified, specialized AI compute complexes tailored for deep learning and other AI algorithms. Tighter integration is key to delivering the performance and efficiency needed for the next generation of AI.</p>
<section id="wafer-scale-ai" class="level4">
<h4 class="anchored" data-anchor-id="wafer-scale-ai">Wafer-scale AI</h4>
<p>Wafer-scale AI takes an extremely integrated approach, manufacturing an entire silicon wafer as one gigantic chip. This differs drastically from conventional CPUs and GPUs, which cut each wafer into many smaller individual chips. <a href="#fig-wafer-scale" class="quarto-xref">Figure&nbsp;<span>10.4</span></a> shows a comparison between Cerebras Wafer Scale Engine 2, which is the largest chip ever built, and the largest GPU. While some GPUs may contain billions of transistors, they still pale in Comparison to the scale of a wafer-size chip with over a trillion transistors.</p>
<p>The wafer-scale approach also diverges from more modular system-on-chip designs that still have discrete components communicating by bus. Instead, wafer-scale AI enables full customization and tight integration of computation, memory, and interconnects across the entire die.</p>
<div id="fig-wafer-scale" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wafer-scale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/aimage1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wafer-scale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.4: Wafer-scale vs.&nbsp;GPU. Credit: <a href="https://www.cerebras.net/product-chip/">Cerebras</a>.
</figcaption>
</figure>
</div>
<p>By designing the wafer as one integrated logic unit, data transfer between elements is minimized. This provides lower latency and power consumption than discrete system-on-chip or chiplet designs. While chiplets can offer flexibility by mixing and matching components, communication between chiplets is challenging. The monolithic nature of wafer-scale integration eliminates these inter-chip communication bottlenecks.</p>
<p>However, the ultra-large-scale also poses difficulties for manufacturability and yield with wafer-scale designs. Defects in any region of the wafer can make (certain parts of) the chip unusable. Specialized lithography techniques are required to produce such large dies. So, wafer-scale integration pursues the maximum performance gains from integration but requires overcoming substantial fabrication challenges.</p>
<p>The following video will provide additional context.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Fcob512SJz0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="chiplets-for-ai" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="chiplets-for-ai">Chiplets for AI</h4>
<p>Chiplet design refers to a semiconductor architecture in which a single integrated circuit (IC) is constructed from multiple smaller, individual components known as chiplets. Each chiplet is a self-contained functional block, typically specialized for a specific task or functionality. These chiplets are then interconnected on a larger substrate or package to create a cohesive system. <a href="#fig-chiplet" class="quarto-xref">Figure&nbsp;<span>10.5</span></a> illustrates this concept. For AI hardware, chiplets enable the mixing of different types of chips optimized for tasks like matrix multiplication, data movement, analog I/O, and specialized memories. This heterogeneous integration differs greatly from wafer-scale integration, where all logic is manufactured as one monolithic chip. Companies like Intel and AMD have adopted chiplet designs for their CPUs.</p>
<p>Chiplets are interconnected using advanced packaging techniques like high-density substrate interposers, 2.5D/3D stacking, and wafer-level packaging. This allows combining chiplets fabricated with different process nodes, specialized memories, and various optimized AI engines.</p>
<div id="fig-chiplet" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-chiplet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/aimage2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chiplet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.5: Chiplet partitioning. Credit: <span class="citation" data-cites="vivet2021intact">Vivet et al. (<a href="../../references.html#ref-vivet2021intact" role="doc-biblioref">2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-vivet2021intact" class="csl-entry" role="listitem">
Vivet, Pascal, Eric Guthmuller, Yvain Thonnart, Gael Pillonnet, Cesar Fuguet, Ivan Miro-Panades, Guillaume Moritz, et al. 2021. <span>“<span>IntAct:</span> <span>A</span> 96-Core Processor with Six Chiplets <span>3D</span>-Stacked on an Active Interposer with Distributed Interconnects and Integrated Power Management.”</span> <em>IEEE J. Solid-State Circuits</em> 56 (1): 79–97. <a href="https://doi.org/10.1109/jssc.2020.3036341">https://doi.org/10.1109/jssc.2020.3036341</a>.
</div></div></figure>
</div>
<p>Some key advantages of using chiplets for AI include:</p>
<ul>
<li><strong>Flexibility:</strong> Flexibility: Chiplets allow for the combination of different chip types, process nodes, and memories tailored for each function. This is more modular versus a fixed wafer-scale design.</li>
<li><strong>Yield:</strong> Smaller chiplets have a higher yield than a gigantic wafer-scale chip. Defects are contained in individual chiplets.</li>
<li><strong>Cost:</strong> Leverages existing manufacturing capabilities versus requiring specialized new processes. Reduces costs by reusing mature fabrication.</li>
<li><strong>Compatibility:</strong> Can integrate with more conventional system architectures like PCIe and standard DDR memory interfaces.</li>
</ul>
<p>However, chiplets also face integration and performance challenges:</p>
<ul>
<li>Lower density compared to wafer-scale, as chiplets are limited in size.</li>
<li>Added latency when communicating between chiplets versus monolithic integration. Requires optimization for low-latency interconnect.</li>
<li>Advanced packaging adds complexity versus wafer-scale integration, though this is arguable.</li>
</ul>
<p>The key objective of chiplets is finding the right balance between modular flexibility and integration density for optimal AI performance. Chiplets aim for efficient AI acceleration while working within the constraints of conventional manufacturing techniques. Chiplets take a middle path between the extremes of wafer-scale integration and fully discrete components. This provides practical benefits but may sacrifice some computational density and efficiency versus a theoretical wafer-size system.</p>
</section>
</section>
<section id="sec-neuromorphic" class="level3 page-columns page-full" data-number="10.8.2">
<h3 data-number="10.8.2" class="anchored" data-anchor-id="sec-neuromorphic"><span class="header-section-number">10.8.2</span> Neuromorphic Computing</h3>
<p>Neuromorphic computing is an emerging field aiming to emulate the efficiency and robustness of biological neural systems for machine learning applications. A key difference from classical Von Neumann architectures is the merging of memory and processing in the same circuit <span class="citation" data-cites="schuman2022opportunities markovic2020physics furber2016large">(<a href="../../references.html#ref-schuman2022opportunities" role="doc-biblioref">Schuman et al. 2022</a>; <a href="../../references.html#ref-markovic2020physics" role="doc-biblioref">Marković et al. 2020</a>; <a href="../../references.html#ref-furber2016large" role="doc-biblioref">Furber 2016</a>)</span>, as illustrated in <a href="#fig-neuromorphic" class="quarto-xref">Figure&nbsp;<span>10.6</span></a>. The structure of the brain inspires this integrated approach. A key advantage is the potential for orders of magnitude improvement in energy-efficient computation compared to conventional AI hardware. For example, estimates project 100x-1000x gains in energy efficiency versus current GPU-based systems for equivalent workloads.</p>
<div class="no-row-height column-margin column-container"><div id="ref-markovic2020physics" class="csl-entry" role="listitem">
Marković, Danijela, Alice Mizrahi, Damien Querlioz, and Julie Grollier. 2020. <span>“Physics for Neuromorphic Computing.”</span> <em>Nature Reviews Physics</em> 2 (9): 499–510. <a href="https://doi.org/10.1038/s42254-020-0208-2">https://doi.org/10.1038/s42254-020-0208-2</a>.
</div><div id="ref-furber2016large" class="csl-entry" role="listitem">
Furber, Steve. 2016. <span>“Large-Scale Neuromorphic Computing Systems.”</span> <em>J. Neural Eng.</em> 13 (5): 051001. <a href="https://doi.org/10.1088/1741-2560/13/5/051001">https://doi.org/10.1088/1741-2560/13/5/051001</a>.
</div></div><div id="fig-neuromorphic" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-neuromorphic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/aimage3.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neuromorphic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.6: Comparison of the von Neumann architecture with the neuromorphic architecture. Credit: <span class="citation" data-cites="schuman2022opportunities">Schuman et al. (<a href="../../references.html#ref-schuman2022opportunities" role="doc-biblioref">2022</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-schuman2022opportunities" class="csl-entry" role="listitem">
Schuman, Catherine D., Shruti R. Kulkarni, Maryam Parsa, J. Parker Mitchell, Prasanna Date, and Bill Kay. 2022. <span>“Opportunities for Neuromorphic Computing Algorithms and Applications.”</span> <em>Nature Computational Science</em> 2 (1): 10–19. <a href="https://doi.org/10.1038/s43588-021-00184-y">https://doi.org/10.1038/s43588-021-00184-y</a>.
</div></div></figure>
</div>
<p>Intel and IBM are leading commercial efforts in neuromorphic hardware. Intel’s Loihi and Loihi 2 chips <span class="citation" data-cites="davies2018loihi davies2021advancing">(<a href="../../references.html#ref-davies2018loihi" role="doc-biblioref">Davies et al. 2018</a>, <a href="../../references.html#ref-davies2021advancing" role="doc-biblioref">2021</a>)</span> offer programmable neuromorphic cores with on-chip learning. IBM’s Northpole <span class="citation" data-cites="modha2023neural">(<a href="../../references.html#ref-modha2023neural" role="doc-biblioref">Modha et al. 2023</a>)</span> device comprises over 100 million magnetic tunnel junction synapses and 68 billion transistors. These specialized chips deliver benefits like low power consumption for edge inference.</p>
<div class="no-row-height column-margin column-container"><div id="ref-davies2018loihi" class="csl-entry" role="listitem">
Davies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, et al. 2018. <span>“Loihi: <span>A</span> Neuromorphic Manycore Processor with on-Chip Learning.”</span> <em>IEEE Micro</em> 38 (1): 82–99. <a href="https://doi.org/10.1109/mm.2018.112130359">https://doi.org/10.1109/mm.2018.112130359</a>.
</div><div id="ref-davies2021advancing" class="csl-entry" role="listitem">
Davies, Mike, Andreas Wild, Garrick Orchard, Yulia Sandamirskaya, Gabriel A. Fonseca Guerra, Prasad Joshi, Philipp Plank, and Sumedh R. Risbud. 2021. <span>“Advancing Neuromorphic Computing with Loihi: <span>A</span> Survey of Results and Outlook.”</span> <em>Proc. IEEE</em> 109 (5): 911–34. <a href="https://doi.org/10.1109/jproc.2021.3067593">https://doi.org/10.1109/jproc.2021.3067593</a>.
</div><div id="ref-modha2023neural" class="csl-entry" role="listitem">
Modha, Dharmendra S., Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy, John V. Arthur, Andrew S. Cassidy, Pallab Datta, et al. 2023. <span>“Neural Inference at the Frontier of Energy, Space, and Time.”</span> <em>Science</em> 382 (6668): 329–35. <a href="https://doi.org/10.1126/science.adh1174">https://doi.org/10.1126/science.adh1174</a>.
</div><div id="ref-maass1997networks" class="csl-entry" role="listitem">
Maass, Wolfgang. 1997. <span>“Networks of Spiking Neurons: <span>The</span> Third Generation of Neural Network Models.”</span> <em>Neural Networks</em> 10 (9): 1659–71. <a href="https://doi.org/10.1016/s0893-6080(97)00011-7">https://doi.org/10.1016/s0893-6080(97)00011-7</a>.
</div></div><p>Spiking neural networks (SNNs) <span class="citation" data-cites="maass1997networks">(<a href="../../references.html#ref-maass1997networks" role="doc-biblioref">Maass 1997</a>)</span> are computational models for neuromorphic hardware. Unlike deep neural networks communicating via continuous values, SNNs use discrete spikes that are more akin to biological neurons. This allows efficient event-based computation rather than constant processing. Additionally, SNNs consider the temporal and spatial characteristics of input data. This better mimics biological neural networks, where the timing of neuronal spikes plays an important role. However, training SNNs remains challenging due to the added temporal complexity. <a href="#fig-spiking" class="quarto-xref">Figure&nbsp;<span>10.7</span></a> provides an overview of the spiking methodology: (a) Diagram of a neuron; (b) Measuring an action potential propagated along the axon of a neuron. Only the action potential is detectable along the axon; (c) The neuron’s spike is approximated with a binary representation; (d) Event-Driven Processing; (e) Active Pixel Sensor and Dynamic Vision Sensor.</p>
<p>You can also watch the video linked below for a more detailed explanation.</p>
<div id="fig-spiking" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-spiking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/aimage4.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-spiking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.7: Neuromoprhic spiking. Credit: <span class="citation" data-cites="eshraghian2023training">Eshraghian et al. (<a href="../../references.html#ref-eshraghian2023training" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-eshraghian2023training" class="csl-entry" role="listitem">
Eshraghian, Jason K., Max Ward, Emre O. Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. 2023. <span>“Training Spiking Neural Networks Using Lessons from Deep Learning.”</span> <em>Proc. IEEE</em> 111 (9): 1016–54. <a href="https://doi.org/10.1109/jproc.2023.3308088">https://doi.org/10.1109/jproc.2023.3308088</a>.
</div></div></figure>
</div>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/yihk_8XnCzg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Specialized nanoelectronic devices called memristors <span class="citation" data-cites="chua1971memristor">(<a href="../../references.html#ref-chua1971memristor" role="doc-biblioref">Chua 1971</a>)</span> are synaptic components in neuromorphic systems. Memristors act as nonvolatile memory with adjustable conductance, emulating the plasticity of real synapses. Memristors enable in-situ learning without separate data transfers by combining memory and processing functions. However, memristor technology has yet to reach maturity and scalability for commercial hardware.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chua1971memristor" class="csl-entry" role="listitem">
Chua, L. 1971. <span>“Memristor-the Missing Circuit Element.”</span> <em>#IEEE_J_CT#</em> 18 (5): 507–19. <a href="https://doi.org/10.1109/tct.1971.1083337">https://doi.org/10.1109/tct.1971.1083337</a>.
</div></div><p>The integration of photonics with neuromorphic computing <span class="citation" data-cites="shastri2021photonics">(<a href="../../references.html#ref-shastri2021photonics" role="doc-biblioref">Shastri et al. 2021</a>)</span> has recently emerged as an active research area. Using light for computation and communication allows high speeds and reduced energy consumption. However, fully realizing photonic neuromorphic systems requires overcoming design and integration challenges.</p>
<p>Neuromorphic computing offers promising capabilities for efficient edge inference but faces obstacles around training algorithms, nanodevice integration, and system design. Ongoing multidisciplinary research across computer science, engineering, materials science, and physics will be key to unlocking this technology’s full potential for AI use cases.</p>
</section>
<section id="analog-computing" class="level3 page-columns page-full" data-number="10.8.3">
<h3 data-number="10.8.3" class="anchored" data-anchor-id="analog-computing"><span class="header-section-number">10.8.3</span> Analog Computing</h3>
<p>Analog computing is an emerging approach that uses analog signals and components like capacitors, inductors, and amplifiers rather than digital logic for computing. It represents information as continuous electrical signals instead of discrete 0s and 1s. This allows the computation to directly reflect the analog nature of real-world data, avoiding digitization errors and overhead.</p>
<p>Analog computing has generated renewed interest in efficient AI hardware, particularly for inference directly on low-power edge devices. Analog circuits, such as multiplication and summation at the core of neural networks, can be used with very low energy consumption. This makes analog well-suited for deploying ML models on energy-constrained end nodes. Startups like Mythic are developing analog AI accelerators.</p>
<p>While analog computing was popular in early computers, the boom of digital logic led to its decline. However, analog is compelling for niche applications requiring extreme efficiency <span class="citation" data-cites="haensch2018next">(<a href="../../references.html#ref-haensch2018next" role="doc-biblioref">Haensch, Gokmen, and Puri 2019</a>)</span>. It contrasts with digital neuromorphic approaches that still use digital spikes for computation. Analog may allow lower precision computation but requires expertise in analog circuit design. Tradeoffs around precision, programming complexity, and fabrication costs remain active research areas.</p>
<div class="no-row-height column-margin column-container"><div id="ref-haensch2018next" class="csl-entry" role="listitem">
Haensch, Wilfried, Tayfun Gokmen, and Ruchir Puri. 2019. <span>“The Next Generation of Deep Learning Hardware: <span>Analog</span> Computing.”</span> <em>Proc. IEEE</em> 107 (1): 108–22. <a href="https://doi.org/10.1109/jproc.2018.2871057">https://doi.org/10.1109/jproc.2018.2871057</a>.
</div><div id="ref-hazan2021neuromorphic" class="csl-entry" role="listitem">
Hazan, Avi, and Elishai Ezra Tsur. 2021. <span>“Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation.”</span> <em>Front. Neurosci.</em> 15 (February): 627221. <a href="https://doi.org/10.3389/fnins.2021.627221">https://doi.org/10.3389/fnins.2021.627221</a>.
</div></div><p>Neuromorphic computing, which aims to emulate biological neural systems for efficient ML inference, can use analog circuits to implement the key components and behaviors of brains. For example, researchers have designed analog circuits to model neurons and synapses using capacitors, transistors, and operational amplifiers <span class="citation" data-cites="hazan2021neuromorphic">(<a href="../../references.html#ref-hazan2021neuromorphic" role="doc-biblioref">Hazan and Ezra Tsur 2021</a>)</span>. The capacitors can exhibit the spiking dynamics of biological neurons, while the amplifiers and transistors provide a weighted summation of inputs to mimic dendrites. Variable resistor technologies like memristors can realize analog synapses with spike-timing-dependent plasticity, which can strengthen or weaken connections based on spiking activity.</p>
<p>Startups like SynSense have developed analog neuromorphic chips containing these biomimetic components <span class="citation" data-cites="bains2020business">(<a href="../../references.html#ref-bains2020business" role="doc-biblioref">Bains 2020</a>)</span>. This analog approach results in low power consumption and high scalability for edge devices versus complex digital SNN implementations.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bains2020business" class="csl-entry" role="listitem">
Bains, Sunny. 2020. <span>“The Business of Building Brains.”</span> <em>Nature Electronics</em> 3 (7): 348–51. <a href="https://doi.org/10.1038/s41928-020-0449-1">https://doi.org/10.1038/s41928-020-0449-1</a>.
</div></div><p>However, training analog SNNs on chips remains an open challenge. Overall, analog realization is a promising technique for delivering the efficiency, scalability, and biological plausibility envisioned with neuromorphic computing. The physics of analog components combined with neural architecture design could improve inference efficiency over conventional digital neural networks.</p>
</section>
<section id="flexible-electronics" class="level3 page-columns page-full" data-number="10.8.4">
<h3 data-number="10.8.4" class="anchored" data-anchor-id="flexible-electronics"><span class="header-section-number">10.8.4</span> Flexible Electronics</h3>
<p>While much of the new hardware technology in the ML workspace has been focused on optimizing and making systems more efficient, there’s a parallel trajectory aiming to adapt hardware for specific applications <span class="citation" data-cites="gates2009flexible musk2019integrated tang2023flexible tang2022soft kwon2022flexible">(<a href="../../references.html#ref-gates2009flexible" role="doc-biblioref">Gates 2009</a>; <a href="../../references.html#ref-musk2019integrated" role="doc-biblioref">Musk et al. 2019</a>; <a href="../../references.html#ref-tang2023flexible" role="doc-biblioref">Tang et al. 2023</a>; <a href="../../references.html#ref-tang2022soft" role="doc-biblioref">Tang, He, and Liu 2022</a>; <a href="../../references.html#ref-kwon2022flexible" role="doc-biblioref">Kwon and Dong 2022</a>)</span>. One such avenue is the development of flexible electronics for AI use cases.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gates2009flexible" class="csl-entry" role="listitem">
Gates, Byron D. 2009. <span>“Flexible Electronics.”</span> <em>Science</em> 323 (5921): 1566–67. <a href="https://doi.org/10.1126/science.1171230">https://doi.org/10.1126/science.1171230</a>.
</div><div id="ref-tang2023flexible" class="csl-entry" role="listitem">
Tang, Xin, Hao Shen, Siyuan Zhao, Na Li, and Jia Liu. 2023. <span>“Flexible Brain<span></span>computer Interfaces.”</span> <em>Nature Electronics</em> 6 (2): 109–18. <a href="https://doi.org/10.1038/s41928-022-00913-9">https://doi.org/10.1038/s41928-022-00913-9</a>.
</div><div id="ref-tang2022soft" class="csl-entry" role="listitem">
Tang, Xin, Yichun He, and Jia Liu. 2022. <span>“Soft Bioelectronics for Cardiac Interfaces.”</span> <em>Biophysics Reviews</em> 3 (1). <a href="https://doi.org/10.1063/5.0069516">https://doi.org/10.1063/5.0069516</a>.
</div></div><p>Flexible electronics refer to electronic circuits and devices fabricated on flexible plastic or polymer substrates rather than rigid silicon. Unlike conventional rigid boards and chips, this allows the electronics to bend, twist, and conform to irregular shapes. <a href="#fig-flexible-device" class="quarto-xref">Figure&nbsp;<span>10.8</span></a> shows an example of a flexible device prototype that wirelessly measures body temperature, which can be seamlessly integrated into clothing or skin patches. The flexibility and bendability of emerging electronic materials allow them to be integrated into thin, lightweight form factors that are well-suited for embedded AI and TinyML applications.</p>
<p>Flexible AI hardware can conform to curvy surfaces and operate efficiently with microwatt power budgets. Flexibility also enables rollable or foldable form factors to minimize device footprint and weight, ideal for small, portable smart devices and wearables incorporating TinyML. Another key advantage of flexible electronics compared to conventional technologies is lower manufacturing costs and simpler fabrication processes, which could democratize access to these technologies. While silicon masks and fabrication costs typically cost millions of dollars, flexible hardware typically costs only tens of cents to manufacture <span class="citation" data-cites="huang2010pseudo biggs2021natively">(<a href="../../references.html#ref-huang2010pseudo" role="doc-biblioref">Huang et al. 2011</a>; <a href="../../references.html#ref-biggs2021natively" role="doc-biblioref">Biggs et al. 2021</a>)</span>. The potential to fabricate flexible electronics directly onto plastic films using high-throughput printing and coating processes can reduce costs and improve manufacturability at scale versus rigid AI chips <span class="citation" data-cites="musk2019integrated">(<a href="../../references.html#ref-musk2019integrated" role="doc-biblioref">Musk et al. 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-huang2010pseudo" class="csl-entry" role="listitem">
Huang, Tsung-Ching, Kenjiro Fukuda, Chun-Ming Lo, Yung-Hui Yeh, Tsuyoshi Sekitani, Takao Someya, and Kwang-Ting Cheng. 2011. <span>“Pseudo-<span>CMOS:</span> <span>A</span> Design Style for Low-Cost and Robust Flexible Electronics.”</span> <em>IEEE Trans. Electron Devices</em> 58 (1): 141–50. <a href="https://doi.org/10.1109/ted.2010.2088127">https://doi.org/10.1109/ted.2010.2088127</a>.
</div><div id="ref-biggs2021natively" class="csl-entry" role="listitem">
Biggs, John, James Myers, Jedrzej Kufel, Emre Ozer, Simon Craske, Antony Sou, Catherine Ramsdale, Ken Williamson, Richard Price, and Scott White. 2021. <span>“A Natively Flexible 32-Bit Arm Microprocessor.”</span> <em>Nature</em> 595 (7868): 532–36. <a href="https://doi.org/10.1038/s41586-021-03625-w">https://doi.org/10.1038/s41586-021-03625-w</a>.
</div></div><div id="fig-flexible-device" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-flexible-device-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/flexible-circuit.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-flexible-device-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.8: Flexible device prototype. Credit: Jabil Circuit.
</figcaption>
</figure>
</div>
<p>The field is enabled by advances in organic semiconductors and nanomaterials that can be deposited on thin, flexible films. However, fabrication remains challenging compared to mature silicon processes. Flexible circuits currently typically exhibit lower performance than rigid equivalents. Still, they promise to transform electronics into lightweight, bendable materials.</p>
<p>Flexible electronics use cases are well-suited for intimate integration with the human body. Potential medical AI applications include bio-integrated sensors, soft assistive robots, and implants that monitor or stimulate the nervous system intelligently. Specifically, flexible electrode arrays could enable higher-density, less-invasive neural interfaces compared to rigid equivalents.</p>
<p>Therefore, flexible electronics are ushering in a new era of wearables and body sensors, largely due to innovations in organic transistors. These components allow for more lightweight and bendable electronics, ideal for wearables, electronic skin, and body-conforming medical devices.</p>
<p>They are well-suited for bioelectronic devices in terms of biocompatibility, opening avenues for applications in brain and cardiac interfaces. For example, research in flexible brain-computer interfaces and soft bioelectronics for cardiac applications demonstrates the potential for wide-ranging medical applications.</p>
<p>Companies and research institutions are not only developing and investing great amounts of resources in flexible electrodes, as showcased in Neuralink’s work <span class="citation" data-cites="musk2019integrated">(<a href="../../references.html#ref-musk2019integrated" role="doc-biblioref">Musk et al. 2019</a>)</span>. Still, they are also pushing the boundaries to integrate machine learning models within the systems <span class="citation" data-cites="kwon2022flexible">(<a href="../../references.html#ref-kwon2022flexible" role="doc-biblioref">Kwon and Dong 2022</a>)</span>. These smart sensors aim for a seamless, long-lasting symbiosis with the human body.</p>
<div class="no-row-height column-margin column-container"><div id="ref-musk2019integrated" class="csl-entry" role="listitem">
Musk, Elon et al. 2019. <span>“An Integrated Brain-Machine Interface Platform with Thousands of Channels.”</span> <em>J. Med. Internet Res.</em> 21 (10): e16194. <a href="https://doi.org/10.2196/16194">https://doi.org/10.2196/16194</a>.
</div><div id="ref-kwon2022flexible" class="csl-entry" role="listitem">
Kwon, Sun Hwa, and Lin Dong. 2022. <span>“Flexible Sensors and Machine Learning for Heart Monitoring.”</span> <em>Nano Energy</em> 102 (November): 107632. <a href="https://doi.org/10.1016/j.nanoen.2022.107632">https://doi.org/10.1016/j.nanoen.2022.107632</a>.
</div><div id="ref-segura2018ethical" class="csl-entry" role="listitem">
Segura Anaya, L. H., Abeer Alsadoon, N. Costadopoulos, and P. W. C. Prasad. 2017. <span>“Ethical Implications of User Perceptions of Wearable Devices.”</span> <em>Sci. Eng. Ethics</em> 24 (1): 1–28. <a href="https://doi.org/10.1007/s11948-017-9872-8">https://doi.org/10.1007/s11948-017-9872-8</a>.
</div><div id="ref-goodyear2017social" class="csl-entry" role="listitem">
Goodyear, Victoria A. 2017. <span>“Social Media, Apps and Wearable Technologies: <span>Navigating</span> Ethical Dilemmas and Procedures.”</span> <em>Qualitative Research in Sport, Exercise and Health</em> 9 (3): 285–302. <a href="https://doi.org/10.1080/2159676x.2017.1303790">https://doi.org/10.1080/2159676x.2017.1303790</a>.
</div><div id="ref-farah2005neuroethics" class="csl-entry" role="listitem">
Farah, Martha J. 2005. <span>“Neuroethics: <span>The</span> Practical and the Philosophical.”</span> <em>Trends Cogn. Sci.</em> 9 (1): 34–40. <a href="https://doi.org/10.1016/j.tics.2004.12.001">https://doi.org/10.1016/j.tics.2004.12.001</a>.
</div><div id="ref-roskies2002neuroethics" class="csl-entry" role="listitem">
Roskies, Adina. 2002. <span>“Neuroethics for the New Millenium.”</span> <em>Neuron</em> 35 (1): 21–23. <a href="https://doi.org/10.1016/s0896-6273(02)00763-8">https://doi.org/10.1016/s0896-6273(02)00763-8</a>.
</div></div><p>Ethically, incorporating smart, machine-learning-driven sensors within the body raises important questions. Issues surrounding data privacy, informed consent, and the long-term societal implications of such technologies are the focus of ongoing work in neuroethics and bioethics <span class="citation" data-cites="segura2018ethical goodyear2017social farah2005neuroethics roskies2002neuroethics">(<a href="../../references.html#ref-segura2018ethical" role="doc-biblioref">Segura Anaya et al. 2017</a>; <a href="../../references.html#ref-goodyear2017social" role="doc-biblioref">Goodyear 2017</a>; <a href="../../references.html#ref-farah2005neuroethics" role="doc-biblioref">Farah 2005</a>; <a href="../../references.html#ref-roskies2002neuroethics" role="doc-biblioref">Roskies 2002</a>)</span>. The field is progressing at a pace that necessitates parallel advancements in ethical frameworks to guide the responsible development and deployment of these technologies. While there are limitations and ethical hurdles to overcome, the prospects for flexible electronics are expansive and hold immense promise for future research and applications.</p>
</section>
<section id="memory-technologies" class="level3 page-columns page-full" data-number="10.8.5">
<h3 data-number="10.8.5" class="anchored" data-anchor-id="memory-technologies"><span class="header-section-number">10.8.5</span> Memory Technologies</h3>
<p>Memory technologies are critical to AI hardware, but conventional DDR DRAM and SRAM create bottlenecks. AI workloads require high bandwidth (&gt;1 TB/s). Extreme scientific applications of AI require extremely low latency (&lt;50 ns) to feed data to compute units <span class="citation" data-cites="duarte2022fastml">(<a href="../../references.html#ref-duarte2022fastml" role="doc-biblioref">Duarte et al. 2022</a>)</span>, high density (&gt;128Gb) to store large model parameters and data sets, and excellent energy efficiency (&lt;100 fJ/b) for embedded use <span class="citation" data-cites="verma2019memory">(<a href="../../references.html#ref-verma2019memory" role="doc-biblioref">Verma et al. 2019</a>)</span>. New memories are needed to meet these demands. Emerging options include several new technologies:</p>
<div class="no-row-height column-margin column-container"><div id="ref-duarte2022fastml" class="csl-entry" role="listitem">
Duarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. 2022. <span>“<span>FastML</span> Science Benchmarks: <span>Accelerating</span> Real-Time Scientific Edge Machine Learning.”</span> <em>ArXiv Preprint</em> abs/2207.07958. <a href="https://arxiv.org/abs/2207.07958">https://arxiv.org/abs/2207.07958</a>.
</div><div id="ref-verma2019memory" class="csl-entry" role="listitem">
Verma, Naveen, Hongyang Jia, Hossein Valavi, Yinqi Tang, Murat Ozatay, Lung-Yen Chen, Bonan Zhang, and Peter Deaville. 2019. <span>“In-Memory Computing: <span>Advances</span> and Prospects.”</span> <em>IEEE Solid-State Circuits Mag.</em> 11 (3): 43–55. <a href="https://doi.org/10.1109/mssc.2019.2922889">https://doi.org/10.1109/mssc.2019.2922889</a>.
</div></div><ul>
<li>Resistive RAM (ReRAM) can improve density with simple, passive arrays. However, challenges around variability remain <span class="citation" data-cites="chi2016prime">(<a href="../../references.html#ref-chi2016prime" role="doc-biblioref">Chi et al. 2016</a>)</span>.</li>
<li>Phase change memory (PCM) exploits the unique properties of chalcogenide glass. Crystalline and amorphous phases have different resistances. Intel’s Optane DCPMM provides fast (100ns), high endurance PCM. However, challenges include limited write cycles and high reset current <span class="citation" data-cites="burr2016recent">(<a href="../../references.html#ref-burr2016recent" role="doc-biblioref">Burr et al. 2016</a>)</span>.</li>
<li>3D stacking can also boost memory density and bandwidth by vertically integrating memory layers with TSV interconnects <span class="citation" data-cites="loh20083dstacked">(<a href="../../references.html#ref-loh20083dstacked" role="doc-biblioref">Loh 2008</a>)</span>. For example, HBM provides 1024-bit wide interfaces.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-burr2016recent" class="csl-entry" role="listitem">
Burr, Geoffrey W., Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng, Jau-Yi Wu, Sangbum Kim, Norma E. Sosa, et al. 2016. <span>“Recent Progress in Phase-<span>Change<span class="math inline">\(&lt;\)</span>?Pub</span> _Newline <span>?<span class="math inline">\(&gt;\)</span>Memory</span> Technology.”</span> <em>IEEE Journal on Emerging and Selected Topics in Circuits and Systems</em> 6 (2): 146–62. <a href="https://doi.org/10.1109/jetcas.2016.2547718">https://doi.org/10.1109/jetcas.2016.2547718</a>.
</div><div id="ref-loh20083dstacked" class="csl-entry" role="listitem">
Loh, Gabriel H. 2008. <span>“<span>3D</span>-Stacked Memory Architectures for Multi-Core Processors.”</span> <em>ACM SIGARCH Computer Architecture News</em> 36 (3): 453–64. <a href="https://doi.org/10.1145/1394608.1382159">https://doi.org/10.1145/1394608.1382159</a>.
</div></div><p>New memory technologies, with their innovative cell architectures and materials, are critical to unlocking the next level of AI hardware performance and efficiency. Realizing their benefits in commercial systems remains an ongoing challenge.</p>
<p>In-memory computing is gaining traction as a promising avenue for optimizing machine learning and high-performance computing workloads. At its core, the technology co-locates data storage and computation to improve energy efficiency and reduce latency <span class="citation" data-cites="verma2019memory mittal2021survey">Wong et al. (<a href="../../references.html#ref-wong2012metal" role="doc-biblioref">2012</a>)</span>. Two key technologies under this umbrella are Resistive RAM (ReRAM) and Processing-In-Memory (PIM).</p>
<div class="no-row-height column-margin column-container"><div id="ref-wong2012metal" class="csl-entry" role="listitem">
Wong, H.-S. Philip, Heng-Yuan Lee, Shimeng Yu, Yu-Sheng Chen, Yi Wu, Pang-Shiu Chen, Byoungil Lee, Frederick T. Chen, and Ming-Jinn Tsai. 2012. <span>“<span>Metal<span></span>Oxide</span> <span>RRAM</span>.”</span> <em>Proc. IEEE</em> 100 (6): 1951–70. <a href="https://doi.org/10.1109/jproc.2012.2190369">https://doi.org/10.1109/jproc.2012.2190369</a>.
</div><div id="ref-chi2016prime" class="csl-entry" role="listitem">
Chi, Ping, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, and Yuan Xie. 2016. <span>“Prime: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory.”</span> <em>ACM SIGARCH Computer Architecture News</em> 44 (3): 27–39. <a href="https://doi.org/10.1145/3007787.3001140">https://doi.org/10.1145/3007787.3001140</a>.
</div></div><p>ReRAM <span class="citation" data-cites="wong2012metal">(<a href="../../references.html#ref-wong2012metal" role="doc-biblioref">Wong et al. 2012</a>)</span> and PIM <span class="citation" data-cites="chi2016prime">(<a href="../../references.html#ref-chi2016prime" role="doc-biblioref">Chi et al. 2016</a>)</span> are the backbones for in-memory computing, storing and computing data in the same location. ReRAM focuses on issues of uniformity, endurance, retention, multi-bit operation, and scalability. On the other hand, PIM involves CPU units integrated directly into memory arrays, specialized for tasks like matrix multiplication, which are central in AI computations.</p>
<p>These technologies find applications in AI workloads and high-performance computing, where the synergy of storage and computation can lead to significant performance gains. The architecture is particularly useful for compute-intensive tasks common in machine learning models.</p>
<p>While in-memory computing technologies like ReRAM and PIM offer exciting prospects for efficiency and performance, they come with their own challenges, such as data uniformity and scalability issues in ReRAM <span class="citation" data-cites="imani2016resistive">(<a href="../../references.html#ref-imani2016resistive" role="doc-biblioref">Imani, Rahimi, and S. Rosing 2016</a>)</span>. Nonetheless, the field is ripe for innovation, and addressing these limitations can open new frontiers in AI and high-performance computing.</p>
<div class="no-row-height column-margin column-container"><div id="ref-imani2016resistive" class="csl-entry" role="listitem">
Imani, Mohsen, Abbas Rahimi, and Tajana S. Rosing. 2016. <span>“Resistive Configurable Associative Memory for Approximate Computing.”</span> In <em>Proceedings of the 2016 Design, Automation &amp;Amp; Test in Europe Conference &amp;Amp; Exhibition (DATE)</em>, 1327–32. IEEE; Research Publishing Services. <a href="https://doi.org/10.3850/9783981537079_0454">https://doi.org/10.3850/9783981537079_0454</a>.
</div></div></section>
<section id="optical-computing" class="level3 page-columns page-full" data-number="10.8.6">
<h3 data-number="10.8.6" class="anchored" data-anchor-id="optical-computing"><span class="header-section-number">10.8.6</span> Optical Computing</h3>
<p>In AI acceleration, a burgeoning area of interest lies in novel technologies that deviate from traditional paradigms. Some emerging technologies mentioned above, such as flexible electronics, in-memory computing, or even neuromorphic computing, are close to becoming a reality, given their ground-breaking innovations and applications. One of the promising and leading next-gen frontiers is optical computing technologies <span class="citation" data-cites="miller2000optical">H. Zhou et al. (<a href="../../references.html#ref-zhou2022photonic" role="doc-biblioref">2022</a>)</span>. Companies like <a href="https://lightmatter.co/">[LightMatter]</a> are pioneering the use of light photonics for calculations, thereby utilizing photons instead of electrons for data transmission and computation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhou2022photonic" class="csl-entry" role="listitem">
Zhou, Hailong, Jianji Dong, Junwei Cheng, Wenchan Dong, Chaoran Huang, Yichen Shen, Qiming Zhang, et al. 2022. <span>“Photonic Matrix Multiplication Lights up Photonic Accelerator and Beyond.”</span> <em>Light: Science &amp;Amp; Applications</em> 11 (1): 30. <a href="https://doi.org/10.1038/s41377-022-00717-8">https://doi.org/10.1038/s41377-022-00717-8</a>.
</div><div id="ref-shastri2021photonics" class="csl-entry" role="listitem">
Shastri, Bhavin J., Alexander N. Tait, T. Ferreira de Lima, Wolfram H. P. Pernice, Harish Bhaskaran, C. D. Wright, and Paul R. Prucnal. 2021. <span>“Photonics for Artificial Intelligence and Neuromorphic Computing.”</span> <em>Nat. Photonics</em> 15 (2): 102–14. <a href="https://doi.org/10.1038/s41566-020-00754-y">https://doi.org/10.1038/s41566-020-00754-y</a>.
</div></div><p>Optical computing utilizes photons and photonic devices rather than traditional electronic circuits for computing and data processing. It takes inspiration from fiber optic communication links that rely on light for fast, efficient data transfer <span class="citation" data-cites="shastri2021photonics">(<a href="../../references.html#ref-shastri2021photonics" role="doc-biblioref">Shastri et al. 2021</a>)</span>. Light can propagate with much less loss than semiconductors’ electrons, enabling inherent speed and efficiency benefits.</p>
<p>Some specific advantages of optical computing include:</p>
<ul>
<li><strong>High throughput:</strong> Photons can transmit with bandwidths &gt;100 Tb/s using wavelength division multiplexing.</li>
<li><strong>Low latency:</strong> Photons interact on femtosecond timescales, millions faster than silicon transistors.</li>
<li><strong>Parallelism:</strong> Multiple data signals can propagate simultaneously through the same optical medium.</li>
<li><strong>Low power:</strong> Photonic circuits utilizing waveguides and resonators can achieve complex logic and memory with only microwatts of power.</li>
</ul>
<p>However, optical computing currently faces significant challenges:</p>
<ul>
<li>Lack of optical memory equivalent to electronic RAM</li>
<li>Requires conversion between optical and electrical domains.</li>
<li>Limited set of available optical components compared to rich electronics ecosystem.</li>
<li>Immature integration methods to combine photonics with traditional CMOS chips.</li>
<li>Complex programming models required to handle parallelism.</li>
</ul>
<p>As a result, optical computing is still in the very early research stage despite its promising potential. However, technical breakthroughs could enable it to complement electronics and unlock performance gains for AI workloads. Companies like Lightmatter are pioneering early optical AI accelerators. In the long term, if key challenges are overcome, it could represent a revolutionary computing substrate.</p>
</section>
<section id="quantum-computing" class="level3" data-number="10.8.7">
<h3 data-number="10.8.7" class="anchored" data-anchor-id="quantum-computing"><span class="header-section-number">10.8.7</span> Quantum Computing</h3>
<p>Quantum computers leverage unique phenomena of quantum physics, like superposition and entanglement, to represent and process information in ways not possible classically. Instead of binary bits, the fundamental unit is the quantum bit or qubit. Unlike classical bits, which are limited to 0 or 1, qubits can exist simultaneously in a superposition of both states due to quantum effects.</p>
<p>Multiple qubits can also be entangled, leading to exponential information density but introducing probabilistic results. Superposition enables parallel computation on all possible states, while entanglement allows nonlocal correlations between qubits.</p>
<p>Quantum algorithms carefully manipulate these inherently quantum mechanical effects to solve problems like optimization or search more efficiently than their classical counterparts in theory.</p>
<ul>
<li>Faster training of deep neural networks by exploiting quantum parallelism for linear algebra operations.</li>
<li>Efficient quantum ML algorithms make use of the unique capabilities of qubits.</li>
<li>Quantum neural networks with inherent quantum effects baked into the model architecture.</li>
<li>Quantum optimizers leveraging quantum annealing or adiabatic algorithms for combinatorial optimization problems.</li>
</ul>
<p>However, quantum states are fragile and prone to errors that require error-correcting protocols. The non-intuitive nature of quantum programming also introduces challenges not present in classical computing.</p>
<ul>
<li>Noisy and fragile quantum bits are difficult to scale up. The largest quantum computer today has less than 100 qubits.</li>
<li>Restricted set of available quantum gates and circuits relative to classical programming.</li>
<li>Lack of datasets and benchmarks to evaluate quantum ML in practical domains.</li>
</ul>
<p>While meaningful quantum advantage for ML remains far off, active research at companies like <a href="https://www.dwavesys.com/company/about-d-wave/">D-Wave</a>, <a href="https://www.rigetti.com/">Rigetti</a>, and <a href="https://ionq.com/">IonQ</a> is advancing quantum computer engineering and quantum algorithms. Major technology companies like Google, <a href="https://www.ibm.com/quantum?utm_content=SRCWW&amp;p1=Search&amp;p4C700050385964705&amp;p5=e&amp;gclid=Cj0KCQjw-pyqBhDmARIsAKd9XIPD9U1Sjez_S0z5jeDDE4nRyd6X_gtVDUKJ-HIolx2vOc599KgW8gAaAv8gEALw_wcB&amp;gclsrc=aw.ds">IBM</a>, and Microsoft are actively exploring quantum computing. Google recently announced a 72-qubit quantum processor called <a href="https://blog.research.google/2018/03/a-preview-of-bristlecone-googles-new.html">Bristlecone</a> and plans to build a 49-qubit commercial quantum system. Microsoft also has an active research program in topological quantum computing and collaborates with quantum startup <a href="https://ionq.com/">IonQ</a></p>
<p>Quantum techniques may first make inroads into optimization before more generalized ML adoption. Realizing quantum ML’s full potential awaits major milestones in quantum hardware development and ecosystem maturity.</p>
</section>
</section>
<section id="future-trends" class="level2 page-columns page-full" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="future-trends"><span class="header-section-number">10.9</span> Future Trends</h2>
<p>In this chapter, the primary focus has been on designing specialized hardware optimized for machine learning workloads and algorithms. This discussion encompassed the tailored architectures of GPUs and TPUs for neural network training and inference. However, an emerging research direction is leveraging machine learning to facilitate the hardware design process itself.</p>
<p>The hardware design process involves many complex stages, including specification, high-level modeling, simulation, synthesis, verification, prototyping, and fabrication. Much of this process traditionally requires extensive human expertise, effort, and time. However, recent advances in machine learning are enabling parts of the hardware design workflow to be automated and enhanced using ML techniques.</p>
<p>Some examples of how ML is transforming hardware design include:</p>
<ul>
<li><strong>Automated circuit synthesis using reinforcement learning:</strong> Rather than hand-crafting transistor-level designs, ML agents such as reinforcement learning can learn to connect logic gates and generate circuit layouts automatically. This can accelerate the time-consuming synthesis process.</li>
<li><strong>ML-based hardware simulation and emulation:</strong> Deep neural network models can be trained to predict how a hardware design will perform under different conditions. For instance, deep learning models can be trained to predict cycle counts for given workloads. This allows faster and more accurate simulation than traditional RTL simulations.</li>
<li><strong>Automated chip floorplanning using ML algorithms:</strong> Chip floorplanning involves optimally placing different components on a die. Evolutionary algorithms like genetic algorithms and other ML algorithms like reinforcement learning are used to explore floorplan options. This can significantly improve manual floorplanning placements in terms of faster turnaround time and quality of placements.</li>
<li><strong>ML-driven architecture optimization:</strong> Novel hardware architectures, like those for efficient ML accelerators, can be automatically generated and optimized by searching the architectural design space. Machine learning algorithms can effectively search large architectural design spaces.</li>
</ul>
<p>Applying ML to hardware design automation holds enormous promise to make the process faster, cheaper, and more efficient. It opens up design possibilities that would require more than manual design. The use of ML in hardware design is an area of active research and early deployment, and we will study the techniques involved and their transformative potential.</p>
<section id="ml-for-hardware-design-automation" class="level3 page-columns page-full" data-number="10.9.1">
<h3 data-number="10.9.1" class="anchored" data-anchor-id="ml-for-hardware-design-automation"><span class="header-section-number">10.9.1</span> ML for Hardware Design Automation</h3>
<p>A major opportunity for machine learning in hardware design is automating parts of the complex and tedious design workflow. Hardware design automation (HDA) broadly refers to using ML techniques like reinforcement learning, genetic algorithms, and neural networks to automate tasks like synthesis, verification, floorplanning, and more. Here are a few examples of where ML for HDA shows real promise:</p>
<ul>
<li><strong>Automated circuit synthesis:</strong> Circuit synthesis involves converting a high-level description of desired logic into an optimized gate-level netlist implementation. This complex process has many design considerations and tradeoffs. ML agents can be trained through reinforcement learning (<span class="citation" data-cites="yu2023rl">Qian et al. (<a href="../../references.html#ref-yu2023rl" role="doc-biblioref">2024</a>)</span>,<span class="citation" data-cites="zhou2023area">G. Zhou and Anderson (<a href="../../references.html#ref-zhou2023area" role="doc-biblioref">2023</a>)</span>) to explore the design space and automatically output optimized syntheses. Startups like <a href="https://www.symbioticeda.com/">Symbiotic EDA</a> are bringing this technology to market.</li>
<li><strong>Automated chip floorplanning:</strong> Floorplanning refers to strategically placing different components on a chip die area. Search algorithms like genetic algorithms (<span class="citation" data-cites="valenzuela2000genetic">Valenzuela and Wang (<a href="../../references.html#ref-valenzuela2000genetic" role="doc-biblioref">2000</a>)</span>) and reinforcement learning (<span class="citation" data-cites="mirhoseini2021graph">Mirhoseini et al. (<a href="../../references.html#ref-mirhoseini2021graph" role="doc-biblioref">2021</a>)</span>, <span class="citation" data-cites="agnesina2023autodmp">Agnesina et al. (<a href="../../references.html#ref-agnesina2023autodmp" role="doc-biblioref">2023</a>)</span>) can be used to automate floorplan optimization to minimize wire length, power consumption, and other objectives. These automated ML-assisted floor planners are extremely valuable as chip complexity increases.</li>
<li><strong>ML hardware simulators:</strong> Training deep neural network models to predict how hardware designs will perform as simulators can accelerate the simulation process by over 100x compared to traditional architectural and RTL simulations.</li>
<li><strong>Automated code translation:</strong> Converting hardware description languages like Verilog to optimized RTL implementations is critical but time-consuming. ML models can be trained to act as translator agents and automate this process.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-yu2023rl" class="csl-entry" role="listitem">
Qian, Yu, Xuegong Zhou, Hao Zhou, and Lingli Wang. 2024. <span>“An Efficient Reinforcement Learning Based Framework for Exploring Logic Synthesis.”</span> <em>ACM Trans. Des. Autom. Electron. Syst.</em> 29 (2): 1–33. <a href="https://doi.org/10.1145/3632174">https://doi.org/10.1145/3632174</a>.
</div><div id="ref-zhou2023area" class="csl-entry" role="listitem">
Zhou, Guanglei, and Jason H. Anderson. 2023. <span>“Area-Driven <span>FPGA</span> Logic Synthesis Using Reinforcement Learning.”</span> In <em>Proceedings of the 28th Asia and South Pacific Design Automation Conference</em>, 159–65. ACM. <a href="https://doi.org/10.1145/3566097.3567894">https://doi.org/10.1145/3566097.3567894</a>.
</div><div id="ref-valenzuela2000genetic" class="csl-entry" role="listitem">
Valenzuela, Christine L, and Pearl Y Wang. 2000. <span>“A Genetic Algorithm for <span>VLSI</span> Floorplanning.”</span> In <em>Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 18<span></span>20, 2000 Proceedings 6</em>, 671–80. Springer.
</div><div id="ref-mirhoseini2021graph" class="csl-entry" role="listitem">
Mirhoseini, Azalia, Anna Goldie, Mustafa Yazgan, Joe Wenjie Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, et al. 2021. <span>“A Graph Placement Methodology for Fast Chip Design.”</span> <em>Nature</em> 594 (7862): 207–12. <a href="https://doi.org/10.1038/s41586-021-03544-w">https://doi.org/10.1038/s41586-021-03544-w</a>.
</div><div id="ref-agnesina2023autodmp" class="csl-entry" role="listitem">
Agnesina, Anthony, Puranjay Rajvanshi, Tian Yang, Geraldo Pradipta, Austin Jiao, Ben Keller, Brucek Khailany, and Haoxing Ren. 2023. <span>“<span>AutoDMP</span>: Automated DREAMPlace-Based Macro Placement.”</span> In <em>Proceedings of the 2023 International Symposium on Physical Design</em>, 149–57. ACM. <a href="https://doi.org/10.1145/3569052.3578923">https://doi.org/10.1145/3569052.3578923</a>.
</div></div><p>The benefits of HDA using ML are reduced design time, superior optimizations, and exploration of design spaces too complex for manual approaches. This can accelerate hardware development and lead to better designs.</p>
<p>Challenges include limits of ML generalization, the black-box nature of some techniques, and accuracy tradeoffs. However, research is rapidly advancing to address these issues and make HDA ML solutions robust and reliable for production use. HDA provides a major avenue for ML to transform hardware design.</p>
</section>
<section id="ml-based-hardware-simulation-and-verification" class="level3" data-number="10.9.2">
<h3 data-number="10.9.2" class="anchored" data-anchor-id="ml-based-hardware-simulation-and-verification"><span class="header-section-number">10.9.2</span> ML-Based Hardware Simulation and Verification</h3>
<p>Simulating and verifying hardware designs is critical before manufacturing to ensure the design behaves as intended. Traditional approaches like register-transfer level (RTL) simulation are complex and time-consuming. ML introduces new opportunities to enhance hardware simulation and verification. Some examples include:</p>
<ul>
<li><strong>Surrogate modeling for simulation:</strong> Highly accurate surrogate models of a design can be built using neural networks. These models predict outputs from inputs much faster than RTL simulation, enabling fast design space exploration. Companies like Ansys use this technique.</li>
<li><strong>ML simulators:</strong> Large neural network models can be trained on RTL simulations to learn to mimic the functionality of a hardware design. Once trained, the NN model can be a highly efficient simulator for regression testing and other tasks. <a href="https://www.graphcore.ai/posts/ai-for-simulation-how-graphcore-is-helping-transform-traditional-hpc">Graphcore</a> has demonstrated over 100x speedup with this approach.</li>
<li><strong>Formal verification using ML:</strong> Formal verification mathematically proves properties about a design. ML techniques can help generate verification properties and learn to solve the complex formal proofs needed, automating parts of this challenging process. Startups like Cortical.io are bringing formal ML verification solutions to the market.</li>
<li><strong>Bug detection:</strong> ML models can be trained to process hardware designs and identify potential issues. This assists human designers in inspecting complex designs and finding bugs. Facebook has shown bug detection models for their server hardware.</li>
</ul>
<p>The key benefits of applying ML to simulation and verification are faster design validation turnaround times, more rigorous testing, and reduced human effort. Challenges include verifying ML model correctness and handling corner cases. ML promises to accelerate testing workflows significantly.</p>
</section>
<section id="ml-for-efficient-hardware-architectures" class="level3 page-columns page-full" data-number="10.9.3">
<h3 data-number="10.9.3" class="anchored" data-anchor-id="ml-for-efficient-hardware-architectures"><span class="header-section-number">10.9.3</span> ML for Efficient Hardware Architectures</h3>
<p>A key goal is designing hardware architectures optimized for performance, power, and efficiency. ML introduces new techniques to automate and enhance architecture design space exploration for general-purpose and specialized hardware like ML accelerators. Some promising examples include:</p>
<ul>
<li><strong>Architecture search for hardware:</strong> Search techniques like evolutionary algorithms (<span class="citation" data-cites="kao2020gamma">Kao and Krishna (<a href="../../references.html#ref-kao2020gamma" role="doc-biblioref">2020</a>)</span>), Bayesian optimization (<span class="citation" data-cites="reagen2017case">Reagen et al. (<a href="../../references.html#ref-reagen2017case" role="doc-biblioref">2017</a>)</span>, <span class="citation" data-cites="bhardwaj2020comprehensive">Bhardwaj et al. (<a href="../../references.html#ref-bhardwaj2020comprehensive" role="doc-biblioref">2020</a>)</span>), reinforcement learning (<span class="citation" data-cites="kao2020confuciux">Kao, Jeong, and Krishna (<a href="../../references.html#ref-kao2020confuciux" role="doc-biblioref">2020</a>)</span>, <span class="citation" data-cites="krishnan2022multiagent">Krishnan et al. (<a href="../../references.html#ref-krishnan2022multiagent" role="doc-biblioref">2022</a>)</span>) can automatically generate novel hardware architectures by mutating and mixing design attributes like cache size, number of parallel units, memory bandwidth, and so on. This allows for efficient navigation of large design spaces.</li>
<li><strong>Predictive modeling for optimization:</strong> - ML models can be trained to predict hardware performance, power, and efficiency metrics for a given architecture. These become “surrogate models” (<span class="citation" data-cites="krishnan2023archgym">Krishnan et al. (<a href="../../references.html#ref-krishnan2023archgym" role="doc-biblioref">2023</a>)</span>) for fast optimization and space exploration by substituting lengthy simulations.</li>
<li><strong>Specialized accelerator optimization:</strong> - For specialized chips like tensor processing units for AI, automated architecture search techniques based on ML algorithms (<span class="citation" data-cites="zhang2022fullstack">D. Zhang et al. (<a href="../../references.html#ref-zhang2022fullstack" role="doc-biblioref">2022</a>)</span>) show promise for finding fast, efficient designs.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-kao2020gamma" class="csl-entry" role="listitem">
Kao, Sheng-Chun, and Tushar Krishna. 2020. <span>“Gamma: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm.”</span> In <em>Proceedings of the 39th International Conference on Computer-Aided Design</em>, 1–9. ACM. <a href="https://doi.org/10.1145/3400302.3415639">https://doi.org/10.1145/3400302.3415639</a>.
</div><div id="ref-reagen2017case" class="csl-entry" role="listitem">
Reagen, Brandon, Jose Miguel Hernandez-Lobato, Robert Adolf, Michael Gelbart, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. <span>“A Case for Efficient Accelerator Design Space Exploration via <span>Bayesian</span> Optimization.”</span> In <em>2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)</em>, 1–6. IEEE; IEEE. <a href="https://doi.org/10.1109/islped.2017.8009208">https://doi.org/10.1109/islped.2017.8009208</a>.
</div><div id="ref-bhardwaj2020comprehensive" class="csl-entry" role="listitem">
Bhardwaj, Kshitij, Marton Havasi, Yuan Yao, David M. Brooks, José Miguel Hernández-Lobato, and Gu-Yeon Wei. 2020. <span>“A Comprehensive Methodology to Determine Optimal Coherence Interfaces for Many-Accelerator <span>SoCs</span>.”</span> In <em>Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design</em>, 145–50. ACM. <a href="https://doi.org/10.1145/3370748.3406564">https://doi.org/10.1145/3370748.3406564</a>.
</div><div id="ref-kao2020confuciux" class="csl-entry" role="listitem">
Kao, Sheng-Chun, Geonhwa Jeong, and Tushar Krishna. 2020. <span>“<span>ConfuciuX:</span> <span>Autonomous</span> Hardware Resource Assignment for <span>DNN</span> Accelerators Using Reinforcement Learning.”</span> In <em>2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</em>, 622–36. IEEE; IEEE. <a href="https://doi.org/10.1109/micro50266.2020.00058">https://doi.org/10.1109/micro50266.2020.00058</a>.
</div><div id="ref-krishnan2022multiagent" class="csl-entry" role="listitem">
Krishnan, Srivatsan, Natasha Jaques, Shayegan Omidshafiei, Dan Zhang, Izzeddin Gur, Vijay Janapa Reddi, and Aleksandra Faust. 2022. <span>“Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration.”</span> <a href="https://arxiv.org/abs/2211.16385">https://arxiv.org/abs/2211.16385</a>.
</div><div id="ref-zhang2022fullstack" class="csl-entry" role="listitem">
Zhang, Dan, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, and Azalia Mirhoseini. 2022. <span>“A Full-Stack Search Technique for Domain Optimized Deep Learning Accelerators.”</span> In <em>Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em>, 27–42. ASPLOS ’22. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3503222.3507767">https://doi.org/10.1145/3503222.3507767</a>.
</div></div><p>The benefits of using ML include superior design space exploration, automated optimization, and reduced manual effort. Challenges include long training times for some techniques and local optima limitations. However, ML for hardware architecture holds great potential for unlocking performance and efficiency gains.</p>
</section>
<section id="ml-to-optimize-manufacturing-and-reduce-defects" class="level3" data-number="10.9.4">
<h3 data-number="10.9.4" class="anchored" data-anchor-id="ml-to-optimize-manufacturing-and-reduce-defects"><span class="header-section-number">10.9.4</span> ML to Optimize Manufacturing and Reduce Defects</h3>
<p>Once a hardware design is complete, it moves to manufacturing. However, variability and defects during manufacturing can impact yields and quality. ML techniques are now being applied to improve fabrication processes and reduce defects. Some examples include:</p>
<ul>
<li><strong>Predictive maintenance:</strong> ML models can analyze equipment sensor data over time and identify signals that predict maintenance needs before failure. This enables proactive upkeep, which can be very handy in the costly fabrication process.</li>
<li><strong>Process optimization:</strong> Supervised learning models can be trained on process data to identify factors that lead to low yields. The models can then optimize parameters to improve yields, throughput, or consistency.</li>
<li><strong>Yield prediction:</strong> By analyzing test data from fabricated designs using techniques like regression trees, ML models can predict yields early in production, allowing process adjustments.</li>
<li><strong>Defect detection:</strong> Computer vision ML techniques can be applied to images of designs to identify defects invisible to the human eye. This enables precision quality control and root cause analysis.</li>
<li><strong>Proactive failure analysis:</strong> - ML models can help predict, diagnose, and prevent issues that lead to downstream defects and failures by analyzing structured and unstructured process data.</li>
</ul>
<p>Applying ML to manufacturing enables process optimization, real-time quality control, predictive maintenance, and higher yields. Challenges include managing complex manufacturing data and variations. But ML is poised to transform semiconductor manufacturing.</p>
</section>
<section id="toward-foundation-models-for-hardware-design" class="level3 page-columns page-full" data-number="10.9.5">
<h3 data-number="10.9.5" class="anchored" data-anchor-id="toward-foundation-models-for-hardware-design"><span class="header-section-number">10.9.5</span> Toward Foundation Models for Hardware Design</h3>
<p>As we have seen, machine learning is opening up new possibilities across the hardware design workflow, from specification to manufacturing. However, current ML techniques are still narrow in scope and require extensive domain-specific engineering. The long-term vision is the development of general artificial intelligence systems that can be applied with versatility across hardware design tasks.</p>
<p>To fully realize this vision, investment, and research are needed to develop foundation models for hardware design. These are unified, general-purpose ML models and architectures that can learn complex hardware design skills with the right training data and objectives.</p>
<p>Realizing foundation models for end-to-end hardware design will require the following:</p>
<ul>
<li>Accumulate large, high-quality, labeled datasets across hardware design stages to train foundation models.</li>
<li>Advances in multi-modal, multi-task ML techniques to handle the diversity of hardware design data and tasks.</li>
<li>Interfaces and abstraction layers to connect foundation models to existing design flows and tools.</li>
<li>Development of simulation environments and benchmarks to train and test foundation models on hardware design capabilities.</li>
<li>Methods to explain and interpret ML models’ design decisions and optimizations for trust and verification.</li>
<li>Compilation techniques to optimize foundation models for efficient deployment across hardware platforms.</li>
</ul>
<p>While significant research remains, foundation models represent the most transformative long-term goal for imbuing AI into the hardware design process. Democratizing hardware design via versatile, automated ML systems promises to unlock a new era of optimized, efficient, and innovative chip design. The journey ahead is filled with open challenges and opportunities.</p>
<p>If you are interested in ML-aided computer architecture design <span class="citation" data-cites="krishnan2023archgym">(<a href="../../references.html#ref-krishnan2023archgym" role="doc-biblioref">Krishnan et al. 2023</a>)</span>, we encourage you to read <a href="https://www.sigarch.org/architecture-2-0-why-computer-architects-need-a-data-centric-ai-gymnasium/">Architecture 2.0</a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-krishnan2023archgym" class="csl-entry" role="listitem">
Krishnan, Srivatsan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour, Ikechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, et al. 2023. <span>“<span>ArchGym:</span> <span>An</span> Open-Source Gymnasium for Machine Learning Assisted Architecture Design.”</span> In <em>Proceedings of the 50th Annual International Symposium on Computer Architecture</em>, 1–16. ACM. <a href="https://doi.org/10.1145/3579371.3589049">https://doi.org/10.1145/3579371.3589049</a>.
</div></div><p>Alternatively, you can watch the below video.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/F5Eieaz7u1I" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
</section>
<section id="conclusion" class="level2" data-number="10.10">
<h2 data-number="10.10" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">10.10</span> Conclusion</h2>
<p>Specialized hardware acceleration has become indispensable for enabling performant and efficient artificial intelligence applications as models and datasets explode in complexity. This chapter examined the limitations of general-purpose processors like CPUs for AI workloads. Their lack of parallelism and computational throughput cannot train or run state-of-the-art deep neural networks quickly. These motivations have driven innovations in customized accelerators.</p>
<p>We surveyed GPUs, TPUs, FPGAs, and ASICs specifically designed for the math-intensive operations inherent to neural networks. By covering this spectrum of options, we aimed to provide a framework for reasoning through accelerator selection based on constraints around flexibility, performance, power, cost, and other factors.</p>
<p>We also explored the role of software in actively enabling and optimizing AI acceleration. This spans programming abstractions, frameworks, compilers, and simulators. We discussed hardware-software co-design as a proactive methodology for building more holistic AI systems by closely integrating algorithm innovation and hardware advances.</p>
<p>But there is so much more to come! Exciting frontiers like analog computing, optical neural networks, and quantum machine learning represent active research directions that could unlock orders of magnitude improvements in efficiency, speed, and scale compared to present paradigms.</p>
<p>Ultimately, specialized hardware acceleration remains indispensable for unlocking the performance and efficiency necessary to fulfill the promise of artificial intelligence from cloud to edge. We hope this chapter provides useful background and insights into the rapid innovation occurring in this domain.</p>
</section>
<section id="sec-ai-acceleration-resource" class="level2" data-number="10.11">
<h2 data-number="10.11" class="anchored" data-anchor-id="sec-ai-acceleration-resource"><span class="header-section-number">10.11</span> Resources</h2>
<p>Here is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercises
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><a href="#exr-tvm" class="quarto-xref">Exercise&nbsp;<span>10.1</span></a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Labs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/optimizations/optimizations.html" class="pagination-link  aria-label=" &lt;span="" optimizations&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/benchmarking/benchmarking.html" class="pagination-link" aria-label="<span class='chapter-number'>11</span>&nbsp; <span class='chapter-title'>Benchmarking AI</span>">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University) with special thanks to the community for their contributions and support.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/hw_acceleration/hw_acceleration.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/hw_acceleration/hw_acceleration.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>