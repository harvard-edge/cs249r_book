%comment{This file was created with betterbib v5.0.11.}


@inproceedings{Li2020Additive,
 author = {Li, Yuhang and Dong, Xin and Wang, Wei},
 title = {Additive Powers-of-Two Quantization: {An} Efficient Non-uniform Discretization for Neural Networks},
 year = {2020},
 booktitle = {8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 url = {https://openreview.net/forum?id=BkgXT24tDS},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/LiDW20.bib},
 timestamp = {Tue, 18 Aug 2020 01:00:00 +0200},
}

@inproceedings{adolf2016fathom,
 author = {Adolf, Robert and Rama, Saketh and Reagen, Brandon and Wei, Gu-yeon and Brooks, David},
 title = {Fathom: {Reference} workloads for modern deep learning methods},
 year = {2016},
 booktitle = {2016 IEEE International Symposium on Workload Characterization (IISWC)},
 publisher = {IEEE},
 pages = {1--10},
 doi = {10.1109/iiswc.2016.7581275},
 url = {https://doi.org/10.1109/iiswc.2016.7581275},
 organization = {IEEE},
 source = {Crossref},
 month = sep,
}

@inproceedings{agnesina2023autodmp,
 author = {Agnesina, Anthony and Rajvanshi, Puranjay and Yang, Tian and Pradipta, Geraldo and Jiao, Austin and Keller, Ben and Khailany, Brucek and Ren, Haoxing},
 title = {{AutoDMP}},
 year = {2023},
 booktitle = {Proceedings of the 2023 International Symposium on Physical Design},
 pages = {149--157},
 doi = {10.1145/3569052.3578923},
 source = {Crossref},
 url = {https://doi.org/10.1145/3569052.3578923},
 publisher = {ACM},
 subtitle = {Automated DREAMPlace-based Macro Placement},
 month = mar,
}

@article{asit2021accelerating,
 author = {Mishra, Asit K. and Latorre, Jorge Albericio and Pool, Jeff and Stosic, Darko and Stosic, Dusan and Venkatesh, Ganesh and Yu, Chong and Micikevicius, Paulius},
 title = {Accelerating Sparse Deep Neural Networks},
 year = {2021},
 journal = {CoRR},
 volume = {abs/2104.08378},
 url = {https://arxiv.org/abs/2104.08378},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2104-08378.bib},
 eprint = {2104.08378},
 eprinttype = {arXiv},
 timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
}

@article{bains2020business,
 author = {Bains, Sunny},
 title = {The business of building brains},
 year = {2020},
 journal = {Nature Electronics},
 publisher = {Springer Science and Business Media LLC},
 volume = {3},
 number = {7},
 pages = {348--351},
 doi = {10.1038/s41928-020-0449-1},
 issn = {2520-1131},
 url = {https://doi.org/10.1038/s41928-020-0449-1},
 source = {Crossref},
 month = jul,
}

@inproceedings{bhardwaj2020comprehensive,
 author = {Bhardwaj, Kshitij and Havasi, Marton and Yao, Yuan and Brooks, David M. and Hern\'andez-Lobato, Jos\'e Miguel and Wei, Gu-Yeon},
 title = {A comprehensive methodology to determine optimal coherence interfaces for many-accelerator {SoCs}},
 year = {2020},
 booktitle = {Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design},
 pages = {145--150},
 doi = {10.1145/3370748.3406564},
 source = {Crossref},
 url = {https://doi.org/10.1145/3370748.3406564},
 publisher = {ACM},
 month = aug,
}

@article{biggs2021natively,
 author = {Biggs, John and Myers, James and Kufel, Jedrzej and Ozer, Emre and Craske, Simon and Sou, Antony and Ramsdale, Catherine and Williamson, Ken and Price, Richard and White, Scott},
 title = {A natively flexible 32-bit Arm microprocessor},
 year = {2021},
 journal = {Nature},
 publisher = {Springer Science and Business Media LLC},
 volume = {595},
 number = {7868},
 pages = {532--536},
 doi = {10.1038/s41586-021-03625-w},
 issn = {0028-0836, 1476-4687},
 url = {https://doi.org/10.1038/s41586-021-03625-w},
 source = {Crossref},
 month = jul,
}

@article{binkert2011gem5,
 author = {Binkert, Nathan and Beckmann, Bradford and Black, Gabriel and Reinhardt, Steven K. and Saidi, Ali and Basu, Arkaprava and Hestness, Joel and Hower, Derek R. and Krishna, Tushar and Sardashti, Somayeh and Sen, Rathijit and Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and Hill, Mark D. and Wood, David A.},
 title = {The gem5 simulator},
 year = {2011},
 journal = {ACM SIGARCH Computer Architecture News},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {39},
 number = {2},
 pages = {1--7},
 doi = {10.1145/2024716.2024718},
 issn = {0163-5964},
 url = {https://doi.org/10.1145/2024716.2024718},
 source = {Crossref},
 month = may,
}

@inproceedings{brown2020language,
 author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
 title = {Language Models are Few-Shot Learners},
 year = {2020},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
 url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
}

@article{burr2016recent,
 author = {Burr, Geoffrey W. and BrightSky, Matthew J. and Sebastian, Abu and Cheng, Huai-Yu and Wu, Jau-Yi and Kim, Sangbum and Sosa, Norma E. and Papandreou, Nikolaos and Lung, Hsiang-Lan and Pozidis, Haralampos and Eleftheriou, Evangelos and Lam, Chung H.},
 title = {Recent Progress in Phase-{Change\ensuremath{<}?Pub} \_newline {?\ensuremath{>}Memory} Technology},
 year = {2016},
 journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {6},
 number = {2},
 pages = {146--162},
 doi = {10.1109/jetcas.2016.2547718},
 issn = {2156-3357, 2156-3365},
 url = {https://doi.org/10.1109/jetcas.2016.2547718},
 source = {Crossref},
 month = jun,
}

@inproceedings{chen2018tvm,
 author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
 title = {{TVM:} {An} automated End-to-End optimizing compiler for deep learning},
 year = {2018},
 booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
 pages = {578--594},
}

@article{cheng2017survey,
 author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
 title = {Model Compression and Acceleration for Deep Neural Networks: {The} Principles, Progress, and Challenges},
 year = {2018},
 journal = {IEEE Signal Process Mag.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {35},
 number = {1},
 pages = {126--136},
 doi = {10.1109/msp.2017.2765695},
 issn = {1053-5888, 1558-0792},
 url = {https://doi.org/10.1109/msp.2017.2765695},
 source = {Crossref},
 month = jan,
}

@article{chi2016prime,
 author = {Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
 title = {Prime},
 year = {2016},
 journal = {ACM SIGARCH Computer Architecture News},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {44},
 number = {3},
 pages = {27--39},
 doi = {10.1145/3007787.3001140},
 issn = {0163-5964},
 url = {https://doi.org/10.1145/3007787.3001140},
 source = {Crossref},
 subtitle = {a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory},
 month = jun,
}

@article{chua1971memristor,
 author = {Chua, L.},
 title = {Memristor-The missing circuit element},
 year = {1971},
 journal = {\#IEEE\_J\_CT\#},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {18},
 number = {5},
 pages = {507--519},
 doi = {10.1109/tct.1971.1083337},
 issn = {0018-9324},
 url = {https://doi.org/10.1109/tct.1971.1083337},
 source = {Crossref},
}

@article{davies2018loihi,
 author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
 title = {Loihi: {A} Neuromorphic Manycore Processor with On-Chip Learning},
 year = {2018},
 journal = {IEEE Micro},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {38},
 number = {1},
 pages = {82--99},
 doi = {10.1109/mm.2018.112130359},
 issn = {0272-1732, 1937-4143},
 url = {https://doi.org/10.1109/mm.2018.112130359},
 source = {Crossref},
 month = jan,
}

@article{davies2021advancing,
 author = {Davies, Mike and Wild, Andreas and Orchard, Garrick and Sandamirskaya, Yulia and Guerra, Gabriel A. Fonseca and Joshi, Prasad and Plank, Philipp and Risbud, Sumedh R.},
 title = {Advancing Neuromorphic Computing With Loihi: {A} Survey of Results and Outlook},
 year = {2021},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {109},
 number = {5},
 pages = {911--934},
 doi = {10.1109/jproc.2021.3067593},
 issn = {0018-9219, 1558-2256},
 url = {https://doi.org/10.1109/jproc.2021.3067593},
 source = {Crossref},
 month = may,
}

@article{dongarra2009evolution,
 author = {Dongarra, Jack J},
 title = {The evolution of high performance computing on system z},
 year = {2009},
 journal = {IBM J. Res. Dev.},
 volume = {53},
 pages = {3--4},
}

@article{duarte2022fastml,
 author = {Duarte, Javier and Tran, Nhan and Hawks, Ben and Herwig, Christian and Muhizi, Jules and Prakash, Shvetank and Reddi, Vijay Janapa},
 title = {{FastML} Science Benchmarks: {Accelerating} Real-Time Scientific Edge Machine Learning},
 year = {2022},
 journal = {ArXiv preprint},
 volume = {abs/2207.07958},
 url = {https://arxiv.org/abs/2207.07958},
}

@article{eshraghian2023training,
 author = {Eshraghian, Jason K. and Ward, Max and Neftci, Emre O. and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
 title = {Training Spiking Neural Networks Using Lessons From Deep Learning},
 year = {2023},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {111},
 number = {9},
 pages = {1016--1054},
 doi = {10.1109/jproc.2023.3308088},
 issn = {0018-9219, 1558-2256},
 url = {https://doi.org/10.1109/jproc.2023.3308088},
 bdsk-url-1 = {https://doi.org/10.1109/JPROC.2023.3308088},
 source = {Crossref},
 month = sep,
}

@article{farah2005neuroethics,
 author = {Farah, Martha J.},
 title = {Neuroethics: {The} practical and the philosophical},
 year = {2005},
 journal = {Trends Cogn. Sci.},
 publisher = {Elsevier BV},
 volume = {9},
 number = {1},
 pages = {34--40},
 doi = {10.1016/j.tics.2004.12.001},
 issn = {1364-6613},
 url = {https://doi.org/10.1016/j.tics.2004.12.001},
 source = {Crossref},
 month = jan,
}

@inproceedings{fowers2018configurable,
 author = {Fowers, Jeremy and Ovtcharov, Kalin and Papamichael, Michael and Massengill, Todd and Liu, Ming and Lo, Daniel and Alkalay, Shlomi and Haselman, Michael and Adams, Logan and Ghandi, Mahdi and Heil, Stephen and Patel, Prerak and Sapek, Adam and Weisz, Gabriel and Woods, Lisa and Lanka, Sitaram and Reinhardt, Steven K. and Caulfield, Adrian M. and Chung, Eric S. and Burger, Doug},
 title = {A Configurable Cloud-Scale {DNN} Processor for Real-Time {AI}},
 year = {2018},
 booktitle = {2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
 publisher = {IEEE},
 pages = {1--14},
 doi = {10.1109/isca.2018.00012},
 url = {https://doi.org/10.1109/isca.2018.00012},
 organization = {IEEE},
 source = {Crossref},
 month = jun,
}

@article{furber2016large,
 author = {Furber, Steve},
 title = {Large-scale neuromorphic computing systems},
 year = {2016},
 journal = {J. Neural Eng.},
 publisher = {IOP Publishing},
 volume = {13},
 number = {5},
 pages = {051001},
 doi = {10.1088/1741-2560/13/5/051001},
 issn = {1741-2560, 1741-2552},
 url = {https://doi.org/10.1088/1741-2560/13/5/051001},
 source = {Crossref},
 month = aug,
}

@article{gale2019state,
 author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
 title = {The state of sparsity in deep neural networks},
 year = {2019},
 journal = {ArXiv preprint},
 volume = {abs/1902.09574},
 url = {https://arxiv.org/abs/1902.09574},
}

@inproceedings{gannot1994verilog,
 author = {Gannot, G. and Ligthart, M.},
 title = {Verilog {HDL} based {FPGA} design},
 year = {1994},
 booktitle = {International Verilog HDL Conference},
 publisher = {IEEE},
 volume = {},
 number = {},
 pages = {86--92},
 doi = {10.1109/ivc.1994.323743},
 url = {https://doi.org/10.1109/ivc.1994.323743},
 bdsk-url-1 = {https://doi.org/10.1109/IVC.1994.323743},
 source = {Crossref},
}

@article{gates2009flexible,
 author = {Gates, Byron D.},
 title = {Flexible Electronics},
 year = {2009},
 journal = {Science},
 publisher = {American Association for the Advancement of Science (AAAS)},
 volume = {323},
 number = {5921},
 pages = {1566--1567},
 doi = {10.1126/science.1171230},
 issn = {0036-8075, 1095-9203},
 url = {https://doi.org/10.1126/science.1171230},
 source = {Crossref},
 month = mar,
}

@article{goodyear2017social,
 author = {Goodyear, Victoria A.},
 title = {Social media, apps and wearable technologies: {Navigating} ethical dilemmas and procedures},
 year = {2017},
 journal = {Qualitative Research in Sport, Exercise and Health},
 publisher = {Informa UK Limited},
 volume = {9},
 number = {3},
 pages = {285--302},
 doi = {10.1080/2159676x.2017.1303790},
 issn = {2159-676X, 2159-6778},
 url = {https://doi.org/10.1080/2159676x.2017.1303790},
 source = {Crossref},
 month = mar,
}

@article{gwennap_certus-nx_nodate,
 author = {Gwennap, Linley},
 title = {Certus-{NX} Innovates General-Purpose {FPGAs}},
 language = {en},
}

@article{gwennapcertusnx,
 author = {Gwennap, Linley},
 title = {Certus-{NX} Innovates General-Purpose {FPGAs}},
 language = {en},
}

@article{haensch2018next,
 author = {Haensch, Wilfried and Gokmen, Tayfun and Puri, Ruchir},
 title = {The Next Generation of Deep Learning Hardware: {Analog} Computing},
 year = {2019},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {107},
 number = {1},
 pages = {108--122},
 doi = {10.1109/jproc.2018.2871057},
 issn = {0018-9219, 1558-2256},
 url = {https://doi.org/10.1109/jproc.2018.2871057},
 source = {Crossref},
 month = jan,
}

@article{hazan2021neuromorphic,
 author = {Hazan, Avi and Ezra Tsur, Elishai},
 title = {Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation},
 year = {2021},
 journal = {Front. Neurosci.},
 publisher = {Frontiers Media SA},
 volume = {15},
 pages = {627221},
 doi = {10.3389/fnins.2021.627221},
 issn = {1662-453X},
 url = {https://doi.org/10.3389/fnins.2021.627221},
 source = {Crossref},
 month = feb,
}

@article{hennessy2019golden,
 author = {Hennessy, John L. and Patterson, David A.},
 title = {A new golden age for computer architecture},
 year = {2019},
 journal = {Commun. ACM},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {62},
 number = {2},
 pages = {48--60},
 doi = {10.1145/3282307},
 issn = {0001-0782, 1557-7317},
 url = {https://doi.org/10.1145/3282307},
 copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
 abstract = {Innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development will lead the way.},
 language = {en},
 source = {Crossref},
 month = jan,
}

@misc{howard2017mobilenets,
 author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
 title = {{MobileNets:} {Efficient} Convolutional Neural Networks for Mobile Vision Applications},
 year = {2017},
 journal = {ArXiv preprint},
 volume = {abs/1704.04861},
 url = {https://arxiv.org/abs/1704.04861},
}

@article{huang2010pseudo,
 author = {Huang, Tsung-Ching and Fukuda, Kenjiro and Lo, Chun-Ming and Yeh, Yung-Hui and Sekitani, Tsuyoshi and Someya, Takao and Cheng, Kwang-Ting},
 title = {Pseudo-{CMOS:} {A} Design Style for Low-Cost and Robust Flexible Electronics},
 year = {2011},
 journal = {IEEE Trans. Electron Devices},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {58},
 number = {1},
 pages = {141--150},
 doi = {10.1109/ted.2010.2088127},
 issn = {0018-9383, 1557-9646},
 url = {https://doi.org/10.1109/ted.2010.2088127},
 source = {Crossref},
 month = jan,
}

@article{huang2022flexible,
 author = {Huang, Shihua and Waeijen, Luc and Corporaal, Henk},
 title = {How Flexible is Your Computing System?},
 year = {2022},
 journal = {ACM Trans. Embedded Comput. Syst.},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {21},
 number = {4},
 pages = {1--41},
 doi = {10.1145/3524861},
 source = {Crossref},
 url = {https://doi.org/10.1145/3524861},
 issn = {1539-9087, 1558-3465},
 month = jul,
}

@inproceedings{ignatov2018ai,
 author = {Ignatov, Andrey and Timofte, Radu and Kulik, Andrei and Yang, Seungsoo and Wang, Ke and Baum, Felix and Wu, Max and Xu, Lirong and Van Gool, Luc},
 title = {{AI} Benchmark: {All} About Deep Learning on Smartphones in 2019},
 year = {2019},
 booktitle = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
 publisher = {IEEE},
 pages = {0--0},
 doi = {10.1109/iccvw.2019.00447},
 url = {https://doi.org/10.1109/iccvw.2019.00447},
 source = {Crossref},
 month = oct,
}

@inproceedings{imani2016resistive,
 author = {Imani, Mohsen and Rahimi, Abbas and S. Rosing, Tajana},
 title = {Resistive Configurable Associative Memory for Approximate Computing},
 year = {2016},
 booktitle = {Proceedings of the 2016 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition (DATE)},
 publisher = {Research Publishing Services},
 pages = {1327--1332},
 doi = {10.3850/9783981537079_0454},
 url = {https://doi.org/10.3850/9783981537079_0454},
 organization = {IEEE},
 source = {Crossref},
}

@inproceedings{jacob2018quantization,
 author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
 title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
 year = {2018},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 publisher = {IEEE},
 pages = {2704--2713},
 doi = {10.1109/cvpr.2018.00286},
 url = {https://doi.org/10.1109/cvpr.2018.00286},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/JacobKCZTHAK18.bib},
 timestamp = {Wed, 06 Feb 2019 00:00:00 +0100},
 source = {Crossref},
 month = jun,
}

@misc{jia2018dissecting,
 author = {Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P.},
 title = {Dissecting the {NVIDIA} {Volta} {GPU} Architecture via Microbenchmarking},
 year = {2018},
 journal = {ArXiv preprint},
 volume = {abs/1804.06826},
 url = {https://arxiv.org/abs/1804.06826},
}

@inproceedings{jia2019beyond,
 author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
 editor = {Talwalkar, Ameet and Smith, Virginia and Zaharia, Matei},
 title = {Beyond Data and Model Parallelism for Deep Neural Networks},
 year = {2019},
 booktitle = {Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019},
 publisher = {mlsys.org},
 url = {https://proceedings.mlsys.org/book/265.pdf},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/mlsys/JiaZA19.bib},
 timestamp = {Thu, 18 Jun 2020 01:00:00 +0200},
}

@inproceedings{jouppi2017datacenter,
 author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
 title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
 year = {2017},
 booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
 location = {Toronto, ON, Canada},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {ISCA '17},
 pages = {1--12},
 doi = {10.1145/3079856.3080246},
 isbn = {9781450348928},
 url = {https://doi.org/10.1145/3079856.3080246},
 abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC{\textemdash}called a Tensor Processing Unit (TPU) {\textemdash} deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X {\textendash} 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X {\textendash} 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
 bdsk-url-1 = {https://doi.org/10.1145/3079856.3080246},
 keywords = {accelerator, neural network, MLP, TPU, CNN, deep learning, domain-specific architecture, GPU, TensorFlow, DNN, RNN, LSTM},
 numpages = {12},
 source = {Crossref},
 month = jun,
}

@inproceedings{jouppi2017indatacenter,
 author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
 title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
 year = {2017},
 booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
 location = {Toronto, ON, Canada},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {ISCA '17},
 pages = {1--12},
 doi = {10.1145/3079856.3080246},
 isbn = {9781450348928},
 url = {https://doi.org/10.1145/3079856.3080246},
 abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC{\textemdash}called a Tensor Processing Unit (TPU) {\textemdash} deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X {\textendash} 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X {\textendash} 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
 bdsk-url-1 = {https://doi.org/10.1145/3079856.3080246},
 keywords = {accelerator, neural network, MLP, TPU, CNN, deep learning, domain-specific architecture, GPU, TensorFlow, DNN, RNN, LSTM},
 numpages = {12},
 source = {Crossref},
 month = jun,
}

@inproceedings{jouppi2023tpu,
 author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
 title = {{TPU} v4: {An} Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
 year = {2023},
 booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
 location = {Orlando, FL, USA},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {ISCA '23},
 doi = {10.1145/3579371.3589350},
 isbn = {9798400700958},
 url = {https://doi.org/10.1145/3579371.3589350},
 abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are lt;5\% of system cost and lt;3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x{\textendash}7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60\% of peak FLOPS/second. For similar sized systems, it is ~4.3x{\textendash}4.5x faster than the Graphcore IPU Bow and is 1.2x{\textendash}1.7x faster and uses 1.3x{\textendash}1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2{\textendash}6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
 articleno = {82},
 bdsk-url-1 = {https://doi.org/10.1145/3579371.3589350},
 keywords = {warehouse scale computer, embeddings, supercomputer, domain specific architecture, reconfigurable, TPU, large language model, power usage effectiveness, CO2 equivalent emissions, energy, optical interconnect, IPU, machine learning, GPU, carbon emissions},
 numpages = {14},
 source = {Crossref},
 month = jun,
}

@inproceedings{kao2020confuciux,
 author = {Kao, Sheng-Chun and Jeong, Geonhwa and Krishna, Tushar},
 title = {{ConfuciuX:} {Autonomous} Hardware Resource Assignment for {DNN} Accelerators using Reinforcement Learning},
 year = {2020},
 booktitle = {2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
 pages = {622--636},
 organization = {IEEE},
 doi = {10.1109/micro50266.2020.00058},
 source = {Crossref},
 url = {https://doi.org/10.1109/micro50266.2020.00058},
 publisher = {IEEE},
 month = oct,
}

@inproceedings{kao2020gamma,
 author = {Kao, Sheng-Chun and Krishna, Tushar},
 title = {Gamma},
 year = {2020},
 booktitle = {Proceedings of the 39th International Conference on Computer-Aided Design},
 pages = {1--9},
 doi = {10.1145/3400302.3415639},
 source = {Crossref},
 url = {https://doi.org/10.1145/3400302.3415639},
 publisher = {ACM},
 subtitle = {automating the HW mapping of DNN models on accelerators via genetic algorithm},
 month = nov,
}

@misc{krishnan2022multiagent,
 author = {Krishnan, Srivatsan and Jaques, Natasha and Omidshafiei, Shayegan and Zhang, Dan and Gur, Izzeddin and Reddi, Vijay Janapa and Faust, Aleksandra},
 title = {Multi-Agent Reinforcement Learning for Microprocessor Design Space Exploration},
 year = {2022},
 archiveprefix = {arXiv},
 eprint = {2211.16385},
 primaryclass = {cs.AR},
}

@inproceedings{krishnan2023archgym,
 author = {Krishnan, Srivatsan and Yazdanbakhsh, Amir and Prakash, Shvetank and Jabbour, Jason and Uchendu, Ikechukwu and Ghosh, Susobhan and Boroujerdian, Behzad and Richins, Daniel and Tripathy, Devashree and Faust, Aleksandra and Janapa Reddi, Vijay},
 title = {{ArchGym:} {An} Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
 year = {2023},
 booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
 publisher = {ACM},
 pages = {1--16},
 doi = {10.1145/3579371.3589049},
 url = {https://doi.org/10.1145/3579371.3589049},
 source = {Crossref},
 month = jun,
}

@article{kwon2022flexible,
 author = {Kwon, Sun Hwa and Dong, Lin},
 title = {Flexible sensors and machine learning for heart monitoring},
 year = {2022},
 journal = {Nano Energy},
 publisher = {Elsevier BV},
 volume = {102},
 pages = {107632},
 doi = {10.1016/j.nanoen.2022.107632},
 issn = {2211-2855},
 url = {https://doi.org/10.1016/j.nanoen.2022.107632},
 source = {Crossref},
 month = nov,
}

@inproceedings{lin2022ondevice,
 author = {Zhu, Ligeng and Hu, Lanxiang and Lin, Ji and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
 title = {{PockEngine:} {Sparse} and Efficient Fine-tuning in a Pocket},
 year = {2023},
 booktitle = {56th Annual IEEE/ACM International Symposium on Microarchitecture},
 publisher = {ACM},
 doi = {10.1145/3613424.3614307},
 url = {https://doi.org/10.1145/3613424.3614307},
 source = {Crossref},
 month = oct,
}

@article{lin2023awq,
 author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
 title = {{AWQ:} {Activation-aware} Weight Quantization for {LLM} Compression and Acceleration},
 year = {2023},
 journal = {arXiv},
}

@article{lindholm2008nvidia,
 author = {Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
 title = {{NVIDIA} Tesla: {A} Unified Graphics and Computing Architecture},
 shorttitle = {NVIDIA Tesla},
 year = {2008},
 journal = {IEEE Micro},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {28},
 number = {2},
 pages = {39--55},
 doi = {10.1109/mm.2008.31},
 issn = {0272-1732},
 url = {https://doi.org/10.1109/mm.2008.31},
 urldate = {2023-11-07},
 note = {Conference Name: IEEE Micro},
 abstract = {To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs.},
 bdsk-url-1 = {https://ieeexplore.ieee.org/document/4523358},
 bdsk-url-2 = {https://doi.org/10.1109/MM.2008.31},
 source = {Crossref},
 month = mar,
}

@article{loh20083dstacked,
 author = {Loh, Gabriel H.},
 title = {{3D}-Stacked Memory Architectures for Multi-core Processors},
 year = {2008},
 journal = {ACM SIGARCH Computer Architecture News},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {36},
 number = {3},
 pages = {453--464},
 doi = {10.1145/1394608.1382159},
 issn = {0163-5964},
 url = {https://doi.org/10.1145/1394608.1382159},
 source = {Crossref},
 month = jun,
}

@inproceedings{luebke2008cuda,
 author = {Luebke, David},
 title = {{CUDA:} {Scalable} parallel programming for high-performance scientific computing},
 year = {2008},
 booktitle = {2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
 publisher = {IEEE},
 volume = {},
 number = {},
 pages = {836--838},
 doi = {10.1109/isbi.2008.4541126},
 url = {https://doi.org/10.1109/isbi.2008.4541126},
 bdsk-url-1 = {https://doi.org/10.1109/ISBI.2008.4541126},
 source = {Crossref},
 month = may,
}

@article{maass1997networks,
 author = {Maass, Wolfgang},
 title = {Networks of spiking neurons: {The} third generation of neural network models},
 year = {1997},
 journal = {Neural Networks},
 publisher = {Elsevier BV},
 volume = {10},
 number = {9},
 pages = {1659--1671},
 doi = {10.1016/s0893-6080(97)00011-7},
 issn = {0893-6080},
 url = {https://doi.org/10.1016/s0893-6080(97)00011-7},
 source = {Crossref},
 month = dec,
}

@article{markovic2020physics,
 author = {Markovi\'c, Danijela and Mizrahi, Alice and Querlioz, Damien and Grollier, Julie},
 title = {Physics for neuromorphic computing},
 year = {2020},
 journal = {Nature Reviews Physics},
 publisher = {Springer Science and Business Media LLC},
 volume = {2},
 number = {9},
 pages = {499--510},
 doi = {10.1038/s42254-020-0208-2},
 issn = {2522-5820},
 url = {https://doi.org/10.1038/s42254-020-0208-2},
 source = {Crossref},
 month = jul,
}

@article{mattson2020mlperf,
 author = {Mattson, Peter and Reddi, Vijay Janapa and Cheng, Christine and Coleman, Cody and Diamos, Greg and Kanter, David and Micikevicius, Paulius and Patterson, David and Schmuelling, Guenther and Tang, Hanlin and Wei, Gu-Yeon and Wu, Carole-Jean},
 title = {{MLPerf:} {An} Industry Standard Benchmark Suite for Machine Learning Performance},
 year = {2020},
 journal = {IEEE Micro},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {40},
 number = {2},
 pages = {8--16},
 doi = {10.1109/mm.2020.2974843},
 issn = {0272-1732, 1937-4143},
 url = {https://doi.org/10.1109/mm.2020.2974843},
 source = {Crossref},
 month = mar,
}

@article{miller2000optical,
 author = {Miller, D.A.B.},
 title = {Optical interconnects to silicon},
 year = {2000},
 journal = {\#IEEE\_J\_JSTQE\#},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {6},
 number = {6},
 pages = {1312--1317},
 doi = {10.1109/2944.902184},
 issn = {1077-260X, 1558-4542},
 url = {https://doi.org/10.1109/2944.902184},
 source = {Crossref},
 month = nov,
}

@article{mirhoseini2021graph,
 author = {Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Nazi, Azade and Pak, Jiwoo and Tong, Andy and Srinivasa, Kavya and Hang, William and Tuncer, Emre and Le, Quoc V. and Laudon, James and Ho, Richard and Carpenter, Roger and Dean, Jeff},
 title = {A graph placement methodology for fast chip design},
 year = {2021},
 journal = {Nature},
 publisher = {Springer Science and Business Media LLC},
 volume = {594},
 number = {7862},
 pages = {207--212},
 doi = {10.1038/s41586-021-03544-w},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41586-021-03544-w},
 issn = {0028-0836, 1476-4687},
 month = jun,
}

@article{mittal2021survey,
 author = {Mittal, Sparsh and Verma, Gaurav and Kaushik, Brajesh and Khanday, Farooq A.},
 title = {A survey of {SRAM}-based in-memory computing techniques and applications},
 year = {2021},
 journal = {J. Syst. Architect.},
 publisher = {Elsevier BV},
 volume = {119},
 pages = {102276},
 doi = {10.1016/j.sysarc.2021.102276},
 issn = {1383-7621},
 url = {https://doi.org/10.1016/j.sysarc.2021.102276},
 source = {Crossref},
 month = oct,
}

@article{modha2023neural,
 author = {Modha, Dharmendra S. and Akopyan, Filipp and Andreopoulos, Alexander and Appuswamy, Rathinakumar and Arthur, John V. and Cassidy, Andrew S. and Datta, Pallab and DeBole, Michael V. and Esser, Steven K. and Otero, Carlos Ortega and Sawada, Jun and Taba, Brian and Amir, Arnon and Bablani, Deepika and Carlson, Peter J. and Flickner, Myron D. and Gandhasri, Rajamohan and Garreau, Guillaume J. and Ito, Megumi and Klamo, Jennifer L. and Kusnitz, Jeffrey A. and McClatchey, Nathaniel J. and McKinstry, Jeffrey L. and Nakamura, Yutaka and Nayak, Tapan K. and Risk, William P. and Schleupen, Kai and Shaw, Ben and Sivagnaname, Jay and Smith, Daniel F. and Terrizzano, Ignacio and Ueda, Takanori},
 title = {Neural inference at the frontier of energy, space, and time},
 year = {2023},
 journal = {Science},
 publisher = {American Association for the Advancement of Science (AAAS)},
 volume = {382},
 number = {6668},
 pages = {329--335},
 doi = {10.1126/science.adh1174},
 issn = {0036-8075, 1095-9203},
 url = {https://doi.org/10.1126/science.adh1174},
 source = {Crossref},
 month = oct,
}

@inproceedings{munshi2009opencl,
 author = {Munshi, Aaftab},
 title = {The {OpenCL} specification},
 year = {2009},
 booktitle = {2009 IEEE Hot Chips 21 Symposium (HCS)},
 publisher = {IEEE},
 volume = {},
 number = {},
 pages = {1--314},
 doi = {10.1109/hotchips.2009.7478342},
 url = {https://doi.org/10.1109/hotchips.2009.7478342},
 bdsk-url-1 = {https://doi.org/10.1109/HOTCHIPS.2009.7478342},
 source = {Crossref},
 month = aug,
}

@article{musk2019integrated,
 author = {Musk, Elon and others},
 title = {An Integrated Brain-Machine Interface Platform With Thousands of Channels},
 year = {2019},
 journal = {J. Med. Internet Res.},
 publisher = {JMIR Publications Inc.},
 volume = {21},
 number = {10},
 pages = {e16194},
 doi = {10.2196/16194},
 issn = {1438-8871},
 url = {https://doi.org/10.2196/16194},
 source = {Crossref},
 month = oct,
}

@article{norrie2021design,
 author = {Norrie, Thomas and Patil, Nishant and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Laudon, James and Young, Cliff and Jouppi, Norman and Patterson, David},
 title = {The Design Process for Google's Training Chips: {Tpuv2} and {TPUv3}},
 year = {2021},
 journal = {IEEE Micro},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {41},
 number = {2},
 pages = {56--63},
 doi = {10.1109/mm.2021.3058217},
 issn = {0272-1732, 1937-4143},
 url = {https://doi.org/10.1109/mm.2021.3058217},
 bdsk-url-1 = {https://doi.org/10.1109/MM.2021.3058217},
 source = {Crossref},
 month = mar,
}

@book{patterson2016computer,
 author = {Patterson, David A and Hennessy, John L},
 title = {Computer organization and design {ARM} edition: {The} hardware software interface},
 year = {2016},
 publisher = {Morgan kaufmann},
}

@article{putnam2014reconfigurable,
 author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
 title = {A reconfigurable fabric for accelerating large-scale datacenter services},
 year = {2014},
 journal = {ACM SIGARCH Computer Architecture News},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {42},
 number = {3},
 pages = {13--24},
 doi = {10.1145/2678373.2665678},
 issn = {0163-5964},
 url = {https://doi.org/10.1145/2678373.2665678},
 urldate = {2023-11-07},
 abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we have designed and built a composable, reconfigurablefabric to accelerate portions of large-scale software services. Each instantiation of the fabric consists of a 6x8 2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One FPGA is placed into each server, accessible through PCIe, and wired directly to other FPGAs with pairs of 10 Gb SAS cables In this paper, we describe a medium-scale deployment of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance, power, and resilience of the system when ranking candidate documents. Under high load, the largescale reconfigurable fabric improves the ranking throughput of each server by a factor of 95\% for a fixed latency distribution{\textemdash} or, while maintaining equivalent throughput, reduces the tail latency by 29\%},
 bdsk-url-1 = {https://dl.acm.org/doi/10.1145/2678373.2665678},
 bdsk-url-2 = {https://doi.org/10.1145/2678373.2665678},
 language = {en},
 source = {Crossref},
 month = jun,
}

@inproceedings{rajat2009largescale,
 author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
 editor = {Danyluk, Andrea Pohoreckyj and Bottou, L\'eon and Littman, Michael L.},
 title = {Large-scale deep unsupervised learning using graphics processors},
 year = {2009},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 publisher = {ACM},
 series = {ACM International Conference Proceeding Series},
 volume = {382},
 pages = {873--880},
 doi = {10.1145/1553374.1553486},
 url = {https://doi.org/10.1145/1553374.1553486},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/RainaMN09.bib},
 timestamp = {Wed, 14 Nov 2018 00:00:00 +0100},
 source = {Crossref},
 month = jun,
}

@article{ranganathan2011from,
 author = {Ranganathan, Parthasarathy},
 title = {From Microprocessors to Nanostores: {Rethinking} Data-Centric Systems},
 year = {2011},
 journal = {Computer},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {44},
 number = {1},
 pages = {39--48},
 doi = {10.1109/mc.2011.18},
 issn = {0018-9162},
 url = {https://doi.org/10.1109/mc.2011.18},
 source = {Crossref},
 month = jan,
}

@inproceedings{reagen2017case,
 author = {Reagen, Brandon and Hernandez-Lobato, Jose Miguel and Adolf, Robert and Gelbart, Michael and Whatmough, Paul and Wei, Gu-Yeon and Brooks, David},
 title = {A case for efficient accelerator design space exploration via {Bayesian} optimization},
 year = {2017},
 booktitle = {2017 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)},
 pages = {1--6},
 organization = {IEEE},
 doi = {10.1109/islped.2017.8009208},
 source = {Crossref},
 url = {https://doi.org/10.1109/islped.2017.8009208},
 publisher = {IEEE},
 month = jul,
}

@inproceedings{reddi2020mlperf,
 author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
 title = {{MLPerf} Inference Benchmark},
 year = {2020},
 booktitle = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
 publisher = {IEEE},
 pages = {446--459},
 doi = {10.1109/isca45697.2020.00045},
 url = {https://doi.org/10.1109/isca45697.2020.00045},
 organization = {IEEE},
 source = {Crossref},
 month = may,
}

@article{roskies2002neuroethics,
 author = {Roskies, Adina},
 title = {Neuroethics for the New Millenium},
 year = {2002},
 journal = {Neuron},
 publisher = {Elsevier BV},
 volume = {35},
 number = {1},
 pages = {21--23},
 doi = {10.1016/s0896-6273(02)00763-8},
 issn = {0896-6273},
 url = {https://doi.org/10.1016/s0896-6273(02)00763-8},
 source = {Crossref},
 month = jul,
}

@article{samajdar2018scale,
 author = {Samajdar, Ananda and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
 title = {Scale-sim: {Systolic} cnn accelerator simulator},
 year = {2018},
 journal = {ArXiv preprint},
 volume = {abs/1811.02883},
 url = {https://arxiv.org/abs/1811.02883},
}

@article{schuman2022opportunities,
 author = {Schuman, Catherine D. and Kulkarni, Shruti R. and Parsa, Maryam and Mitchell, J. Parker and Date, Prasanna and Kay, Bill},
 title = {Opportunities for neuromorphic computing algorithms and applications},
 year = {2022},
 journal = {Nature Computational Science},
 publisher = {Springer Science and Business Media LLC},
 volume = {2},
 number = {1},
 pages = {10--19},
 doi = {10.1038/s43588-021-00184-y},
 issn = {2662-8457},
 url = {https://doi.org/10.1038/s43588-021-00184-y},
 source = {Crossref},
 month = jan,
}

@misc{segal1999opengl,
 author = {Segal, Mark and Akeley, Kurt},
 title = {The {OpenGL} graphics system: {A} specification (version 1.1)},
 year = {1999},
}

@article{segura2018ethical,
 author = {Segura Anaya, L. H. and Alsadoon, Abeer and Costadopoulos, N. and Prasad, P. W. C.},
 title = {Ethical Implications of User Perceptions of Wearable Devices},
 year = {2017},
 journal = {Sci. Eng. Ethics},
 publisher = {Springer Science and Business Media LLC},
 volume = {24},
 number = {1},
 pages = {1--28},
 doi = {10.1007/s11948-017-9872-8},
 issn = {1353-3452, 1471-5546},
 url = {https://doi.org/10.1007/s11948-017-9872-8},
 source = {Crossref},
 month = feb,
}

@article{shastri2021photonics,
 author = {Shastri, Bhavin J. and Tait, Alexander N. and Ferreira de Lima, T. and Pernice, Wolfram H. P. and Bhaskaran, Harish and Wright, C. D. and Prucnal, Paul R.},
 title = {Photonics for artificial intelligence and neuromorphic computing},
 year = {2021},
 journal = {Nat. Photonics},
 publisher = {Springer Science and Business Media LLC},
 volume = {15},
 number = {2},
 pages = {102--114},
 doi = {10.1038/s41566-020-00754-y},
 issn = {1749-4885, 1749-4893},
 url = {https://doi.org/10.1038/s41566-020-00754-y},
 source = {Crossref},
 month = jan,
}

@inproceedings{suda2016throughput,
 author = {Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
 title = {Throughput-Optimized {OpenCL}-based {FPGA} Accelerator for Large-Scale Convolutional Neural Networks},
 year = {2016},
 booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
 publisher = {ACM},
 pages = {16--25},
 doi = {10.1145/2847263.2847276},
 url = {https://doi.org/10.1145/2847263.2847276},
 source = {Crossref},
 month = feb,
}

@article{sze2017efficient,
 author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
 title = {Efficient Processing of Deep Neural Networks: {A} Tutorial and Survey},
 year = {2017},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {105},
 number = {12},
 pages = {2295--2329},
 doi = {10.1109/jproc.2017.2761740},
 issn = {0018-9219, 1558-2256},
 url = {https://doi.org/10.1109/jproc.2017.2761740},
 copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
 source = {Crossref},
 abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
 archiveprefix = {arXiv},
 eprint = {1703.09039},
 primaryclass = {cs.CV},
 month = dec,
}

@article{tang2022soft,
 author = {Tang, Xin and He, Yichun and Liu, Jia},
 title = {Soft bioelectronics for cardiac interfaces},
 year = {2022},
 journal = {Biophysics Reviews},
 publisher = {AIP Publishing},
 volume = {3},
 number = {1},
 doi = {10.1063/5.0069516},
 issn = {2688-4089},
 url = {https://doi.org/10.1063/5.0069516},
 source = {Crossref},
 month = jan,
}

@article{tang2023flexible,
 author = {Tang, Xin and Shen, Hao and Zhao, Siyuan and Li, Na and Liu, Jia},
 title = {Flexible brain{\textendash}computer interfaces},
 year = {2023},
 journal = {Nature Electronics},
 publisher = {Springer Science and Business Media LLC},
 volume = {6},
 number = {2},
 pages = {109--118},
 doi = {10.1038/s41928-022-00913-9},
 issn = {2520-1131},
 url = {https://doi.org/10.1038/s41928-022-00913-9},
 source = {Crossref},
 month = feb,
}

@inproceedings{valenzuela2000genetic,
 author = {Valenzuela, Christine L and Wang, Pearl Y},
 title = {A genetic algorithm for {VLSI} floorplanning},
 year = {2000},
 booktitle = {Parallel Problem Solving from Nature PPSN VI: 6th International Conference Paris, France, September 18{\textendash}20, 2000 Proceedings 6},
 pages = {671--680},
 organization = {Springer},
}

@article{verma2019memory,
 author = {Verma, Naveen and Jia, Hongyang and Valavi, Hossein and Tang, Yinqi and Ozatay, Murat and Chen, Lung-Yen and Zhang, Bonan and Deaville, Peter},
 title = {In-Memory Computing: {Advances} and Prospects},
 year = {2019},
 journal = {IEEE Solid-State Circuits Mag.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {11},
 number = {3},
 pages = {43--55},
 doi = {10.1109/mssc.2019.2922889},
 issn = {1943-0582, 1943-0590},
 url = {https://doi.org/10.1109/mssc.2019.2922889},
 source = {Crossref},
}

@article{vivet2021intact,
 author = {Vivet, Pascal and Guthmuller, Eric and Thonnart, Yvain and Pillonnet, Gael and Fuguet, Cesar and Miro-Panades, Ivan and Moritz, Guillaume and Durupt, Jean and Bernard, Christian and Varreau, Didier and Pontes, Julian and Thuries, Sebastien and Coriat, David and Harrand, Michel and Dutoit, Denis and Lattard, Didier and Arnaud, Lucile and Charbonnier, Jean and Coudrain, Perceval and Garnier, Arnaud and Berger, Frederic and Gueugnot, Alain and Greiner, Alain and Meunier, Quentin L. and Farcy, Alexis and Arriordaz, Alexandre and Cheramy, Severine and Clermidy, Fabien},
 title = {{IntAct:} {A} 96-Core Processor With Six Chiplets {3D}-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management},
 year = {2021},
 journal = {IEEE J. Solid-State Circuits},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {56},
 number = {1},
 pages = {79--97},
 doi = {10.1109/jssc.2020.3036341},
 issn = {0018-9200, 1558-173X},
 url = {https://doi.org/10.1109/jssc.2020.3036341},
 bdsk-url-1 = {https://doi.org/10.1109/JSSC.2020.3036341},
 source = {Crossref},
 month = jan,
}

@inproceedings{wang2020apq,
 author = {Wang, Tianzhe and Wang, Kuan and Cai, Han and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Lin, Yujun and Han, Song},
 title = {{APQ:} {Joint} Search for Network Architecture, Pruning and Quantization Policy},
 year = {2020},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 publisher = {IEEE},
 pages = {2075--2084},
 doi = {10.1109/cvpr42600.2020.00215},
 url = {https://doi.org/10.1109/cvpr42600.2020.00215},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/cvpr/WangWCLL0LH20.bib},
 timestamp = {Tue, 22 Dec 2020 00:00:00 +0100},
 source = {Crossref},
 month = jun,
}

@book{weik1955survey,
 author = {Weik, Martin H.},
 title = {A Survey of Domestic Electronic Digital Computing Systems},
 year = {1955},
 publisher = {Ballistic Research Laboratories},
 language = {en},
}

@article{wong2012metal,
 author = {Wong, H.-S. Philip and Lee, Heng-Yuan and Yu, Shimeng and Chen, Yu-Sheng and Wu, Yi and Chen, Pang-Shiu and Lee, Byoungil and Chen, Frederick T. and Tsai, Ming-Jinn},
 title = {{Metal{\textendash}Oxide} {RRAM}},
 year = {2012},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {100},
 number = {6},
 pages = {1951--1970},
 doi = {10.1109/jproc.2012.2190369},
 issn = {0018-9219, 1558-2256},
 url = {https://doi.org/10.1109/jproc.2012.2190369},
 source = {Crossref},
 month = jun,
}

@article{xiong2021mribased,
 author = {Xiong, Siyu and Wu, Guoqing and Fan, Xitian and Feng, Xuan and Huang, Zhongcheng and Cao, Wei and Zhou, Xuegong and Ding, Shijin and Yu, Jinhua and Wang, Lingli and Shi, Zhifeng},
 title = {{MRI}-based brain tumor segmentation using {FPGA}-accelerated neural network},
 year = {2021},
 journal = {BMC Bioinf.},
 publisher = {Springer Science and Business Media LLC},
 volume = {22},
 number = {1},
 pages = {421},
 doi = {10.1186/s12859-021-04347-6},
 issn = {1471-2105},
 url = {https://doi.org/10.1186/s12859-021-04347-6},
 urldate = {2023-11-07},
 abstract = {Brain tumor segmentation is a challenging problem in medical image processing and analysis. It is a very time-consuming and error-prone task. In order to reduce the burden on physicians and improve the segmentation accuracy, the computer-aided detection (CAD) systems need to be developed. Due to the powerful feature learning ability of the deep learning technology, many deep learning-based methods have been applied to the brain tumor segmentation CAD systems and achieved satisfactory accuracy. However, deep learning neural networks have high computational complexity, and the brain tumor segmentation process consumes significant time. Therefore, in order to achieve the high segmentation accuracy of brain tumors and obtain the segmentation results efficiently, it is very demanding to speed up the segmentation process of brain tumors.},
 bdsk-url-1 = {https://doi.org/10.1186/s12859-021-04347-6},
 keywords = {Brain tumor segmatation, FPGA acceleration, Neural network},
 source = {Crossref},
 month = sep,
}

@article{xiu2019time,
 author = {Xiu, Liming},
 title = {Time Moore: {Exploiting} {Moore's} Law From The Perspective of Time},
 year = {2019},
 journal = {IEEE Solid-State Circuits Mag.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {11},
 number = {1},
 pages = {39--55},
 doi = {10.1109/mssc.2018.2882285},
 issn = {1943-0582, 1943-0590},
 url = {https://doi.org/10.1109/mssc.2018.2882285},
 source = {Crossref},
}

@article{young2018recent,
 author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
 title = {Recent Trends in Deep Learning Based Natural Language Processing {[Review} Article]},
 year = {2018},
 journal = {IEEE Comput. Intell. Mag.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {13},
 number = {3},
 pages = {55--75},
 doi = {10.1109/mci.2018.2840738},
 issn = {1556-603X, 1556-6048},
 url = {https://doi.org/10.1109/mci.2018.2840738},
 source = {Crossref},
 month = aug,
}

@article{yu2023rl,
 author = {Qian, Yu and Zhou, Xuegong and Zhou, Hao and Wang, Lingli},
 title = {An Efficient Reinforcement Learning Based Framework for Exploring Logic Synthesis},
 year = {2024},
 month = jan,
 journal = {ACM Trans. Des. Autom. Electron. Syst.},
 publisher = {Association for Computing Machinery (ACM)},
 address = {New York, NY, USA},
 doi = {10.1145/3632174},
 issn = {1084-4309, 1557-7309},
 url = {https://doi.org/10.1145/3632174},
 note = {Just Accepted},
 abstract = {Logic synthesis is a crucial step in electronic design automation tools. The rapid developments of reinforcement learning (RL) have enabled the automated exploration of logic synthesis. Existing RL based methods may lead to data inefficiency, and the exploration approaches for FPGA and ASIC technology mapping in recent works lack the flexibility of the learning process. This work proposes ESE, a reinforcement learning based framework to efficiently learn the logic synthesis process. The framework supports the modeling of logic optimization and technology mapping for FPGA and ASIC. The optimization for the execution time of the synthesis script is also considered. For the modeling of FPGA mapping, the logic optimization and technology mapping are combined to be learned in a flexible way. For the modeling of ASIC mapping, the standard cell based optimization and LUT optimization operations are incorporated into the ASIC synthesis flow. To improve the utilization of samples, the Proximal Policy Optimization model is adopted. Furthermore, the framework is enhanced by supporting MIG based synthesis exploration. Experiments show that for FPGA technology mapping on the VTR benchmark, the average LUT-Level-Product and script runtime are improved by more than 18.3\% and 12.4\% respectively than previous works. For ASIC mapping on the EPFL benchmark, the average Area-Delay-Product is improved by 14.5\%.},
 keywords = {technology mapping, Majority-Inverter Graph, And-Inverter Graph, Reinforcement learning, logic optimization},
 number = {2},
 source = {Crossref},
 volume = {29},
 pages = {1--33},
}

@inproceedings{zhang2015fpga,
 author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason Optimizing},
 title = {{FPGA}-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 {ACM}},
 year = {2015},
 booktitle = {SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA},
 volume = {15},
 pages = {161--170},
}

@inproceedings{zhang2022fullstack,
 author = {Zhang, Dan and Huda, Safeen and Songhori, Ebrahim and Prabhu, Kartik and Le, Quoc and Goldie, Anna and Mirhoseini, Azalia},
 title = {A full-stack search technique for domain optimized deep learning accelerators},
 year = {2022},
 booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
 location = {Lausanne, Switzerland},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {ASPLOS '22},
 pages = {27--42},
 doi = {10.1145/3503222.3507767},
 isbn = {9781450392051},
 url = {https://doi.org/10.1145/3503222.3507767},
 abstract = {The rapidly-changing deep learning landscape presents a unique opportunity for building inference accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. In this paper, we analyze bottlenecks in state-of-the-art vision and natural language processing (NLP) models, including EfficientNet and BERT, and use FAST to design accelerators capable of addressing these bottlenecks. FAST-generated accelerators optimized for single workloads improve Perf/TDP by 3.7{\texttimes} on average across all benchmarks compared to TPU-v3. A FAST-generated accelerator optimized for serving a suite of workloads improves Perf/TDP by 2.4{\texttimes} on average compared to TPU-v3. Our return on investment analysis shows that FAST-generated accelerators can potentially be practical for moderate-sized datacenter deployments.},
 keywords = {design space exploration, hardware-software codesign, tensor processing unit, machine learning, operation fusion},
 numpages = {16},
 source = {Crossref},
 month = feb,
}

@inproceedings{zhangfast,
 author = {Zhang, Dan and Huda, Safeen and Songhori, Ebrahim and Prabhu, Kartik and Le, Quoc and Goldie, Anna and Mirhoseini, Azalia},
 title = {A full-stack search technique for domain optimized deep learning accelerators},
 year = {2022},
 booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
 location = {Lausanne, Switzerland},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {ASPLOS '22},
 pages = {27--42},
 doi = {10.1145/3503222.3507767},
 isbn = {9781450392051},
 url = {https://doi.org/10.1145/3503222.3507767},
 abstract = {The rapidly-changing deep learning landscape presents a unique opportunity for building inference accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. In this paper, we analyze bottlenecks in state-of-the-art vision and natural language processing (NLP) models, including EfficientNet and BERT, and use FAST to design accelerators capable of addressing these bottlenecks. FAST-generated accelerators optimized for single workloads improve Perf/TDP by 3.7{\texttimes} on average across all benchmarks compared to TPU-v3. A FAST-generated accelerator optimized for serving a suite of workloads improves Perf/TDP by 2.4{\texttimes} on average compared to TPU-v3. Our return on investment analysis shows that FAST-generated accelerators can potentially be practical for moderate-sized datacenter deployments.},
 numpages = {16},
 keywords = {design space exploration, hardware-software codesign, tensor processing unit, machine learning, operation fusion},
 source = {Crossref},
 month = feb,
}

@article{zhou2022photonic,
 author = {Zhou, Hailong and Dong, Jianji and Cheng, Junwei and Dong, Wenchan and Huang, Chaoran and Shen, Yichen and Zhang, Qiming and Gu, Min and Qian, Chao and Chen, Hongsheng and Ruan, Zhichao and Zhang, Xinliang},
 title = {Photonic matrix multiplication lights up photonic accelerator and beyond},
 year = {2022},
 journal = {Light: Science \&amp; Applications},
 publisher = {Springer Science and Business Media LLC},
 volume = {11},
 number = {1},
 pages = {30},
 doi = {10.1038/s41377-022-00717-8},
 issn = {2047-7538},
 url = {https://doi.org/10.1038/s41377-022-00717-8},
 source = {Crossref},
 month = feb,
}

@inproceedings{zhou2023area,
 author = {Zhou, Guanglei and Anderson, Jason H.},
 title = {Area-Driven {FPGA} Logic Synthesis Using Reinforcement Learning},
 year = {2023},
 booktitle = {Proceedings of the 28th Asia and South Pacific Design Automation Conference},
 pages = {159--165},
 doi = {10.1145/3566097.3567894},
 source = {Crossref},
 url = {https://doi.org/10.1145/3566097.3567894},
 publisher = {ACM},
 month = jan,
}

@inproceedings{zhu2018benchmarking,
 author = {Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
 title = {Benchmarking and Analyzing Deep Neural Network Training},
 year = {2018},
 booktitle = {2018 IEEE International Symposium on Workload Characterization (IISWC)},
 publisher = {IEEE},
 pages = {88--100},
 doi = {10.1109/iiswc.2018.8573476},
 url = {https://doi.org/10.1109/iiswc.2018.8573476},
 organization = {IEEE},
 source = {Crossref},
 month = sep,
}
