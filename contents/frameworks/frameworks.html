<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning Systems - 6&nbsp; AI Frameworks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/training/training.html" rel="next">
<link href="../../contents/data_engineering/data_engineering.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../scripts/ai_menu/dist/bundle.js" defer=""></script>
<script src="../../scripts/ai_menu/dist/142.bundle.js" defer=""></script>
<script src="../../scripts/ai_menu/dist/384.bundle.js" defer=""></script>
<script src="../../scripts/ai_menu/dist/761.bundle.js" defer=""></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/foreword.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/frameworks/frameworks.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Frameworks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">FRONT MATTER</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedication</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MAIN</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ML Systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ML Operations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/niclav_sys/niclav_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CV on Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/object_detection_fomo/object_detection_fomo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_nicla/kws_nicla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP - Spectral Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/motion_classify_ad/motion_classify_ad.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Communities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">6.1</span> Introduction</a></li>
  <li><a href="#framework-evolution" id="toc-framework-evolution" class="nav-link" data-scroll-target="#framework-evolution"><span class="header-section-number">6.2</span> Framework Evolution</a></li>
  <li><a href="#deepdive-into-tensorflow" id="toc-deepdive-into-tensorflow" class="nav-link" data-scroll-target="#deepdive-into-tensorflow"><span class="header-section-number">6.3</span> DeepDive into TensorFlow</a>
  <ul>
  <li><a href="#tf-ecosystem" id="toc-tf-ecosystem" class="nav-link" data-scroll-target="#tf-ecosystem"><span class="header-section-number">6.3.1</span> TF Ecosystem</a></li>
  <li><a href="#static-computation-graph" id="toc-static-computation-graph" class="nav-link" data-scroll-target="#static-computation-graph"><span class="header-section-number">6.3.2</span> Static Computation Graph</a></li>
  <li><a href="#usability-deployment" id="toc-usability-deployment" class="nav-link" data-scroll-target="#usability-deployment"><span class="header-section-number">6.3.3</span> Usability &amp; Deployment</a></li>
  <li><a href="#architecture-design" id="toc-architecture-design" class="nav-link" data-scroll-target="#architecture-design"><span class="header-section-number">6.3.4</span> Architecture Design</a></li>
  <li><a href="#built-in-functionality-keras" id="toc-built-in-functionality-keras" class="nav-link" data-scroll-target="#built-in-functionality-keras"><span class="header-section-number">6.3.5</span> Built-in Functionality &amp; Keras</a></li>
  <li><a href="#limitations-and-challenges" id="toc-limitations-and-challenges" class="nav-link" data-scroll-target="#limitations-and-challenges"><span class="header-section-number">6.3.6</span> Limitations and Challenges</a></li>
  <li><a href="#pytorch-vs.-tensorflow" id="toc-pytorch-vs.-tensorflow" class="nav-link" data-scroll-target="#pytorch-vs.-tensorflow"><span class="header-section-number">6.3.7</span> PyTorch vs.&nbsp;TensorFlow</a></li>
  </ul></li>
  <li><a href="#basic-framework-components" id="toc-basic-framework-components" class="nav-link" data-scroll-target="#basic-framework-components"><span class="header-section-number">6.4</span> Basic Framework Components</a>
  <ul>
  <li><a href="#tensor-data-structures" id="toc-tensor-data-structures" class="nav-link" data-scroll-target="#tensor-data-structures"><span class="header-section-number">6.4.1</span> Tensor data structures</a></li>
  <li><a href="#computational-graphs" id="toc-computational-graphs" class="nav-link" data-scroll-target="#computational-graphs"><span class="header-section-number">6.4.2</span> Computational graphs</a>
  <ul class="collapse">
  <li><a href="#graph-definition" id="toc-graph-definition" class="nav-link" data-scroll-target="#graph-definition">Graph Definition</a></li>
  <li><a href="#static-vs.-dynamic-graphs" id="toc-static-vs.-dynamic-graphs" class="nav-link" data-scroll-target="#static-vs.-dynamic-graphs">Static vs.&nbsp;Dynamic Graphs</a></li>
  </ul></li>
  <li><a href="#data-pipeline-tools" id="toc-data-pipeline-tools" class="nav-link" data-scroll-target="#data-pipeline-tools"><span class="header-section-number">6.4.3</span> Data Pipeline Tools</a>
  <ul class="collapse">
  <li><a href="#data-loaders" id="toc-data-loaders" class="nav-link" data-scroll-target="#data-loaders">Data Loaders</a></li>
  </ul></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation"><span class="header-section-number">6.4.4</span> Data Augmentation</a></li>
  <li><a href="#optimization-algorithms" id="toc-optimization-algorithms" class="nav-link" data-scroll-target="#optimization-algorithms"><span class="header-section-number">6.4.5</span> Optimization Algorithms</a></li>
  <li><a href="#model-training-support" id="toc-model-training-support" class="nav-link" data-scroll-target="#model-training-support"><span class="header-section-number">6.4.6</span> Model Training Support</a></li>
  <li><a href="#validation-and-analysis" id="toc-validation-and-analysis" class="nav-link" data-scroll-target="#validation-and-analysis"><span class="header-section-number">6.4.7</span> Validation and Analysis</a>
  <ul class="collapse">
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization">Visualization</a></li>
  </ul></li>
  <li><a href="#differentiable-programming" id="toc-differentiable-programming" class="nav-link" data-scroll-target="#differentiable-programming"><span class="header-section-number">6.4.8</span> Differentiable programming</a></li>
  <li><a href="#hardware-acceleration" id="toc-hardware-acceleration" class="nav-link" data-scroll-target="#hardware-acceleration"><span class="header-section-number">6.4.9</span> Hardware Acceleration</a></li>
  </ul></li>
  <li><a href="#sec-ai_frameworks-advanced" id="toc-sec-ai_frameworks-advanced" class="nav-link" data-scroll-target="#sec-ai_frameworks-advanced"><span class="header-section-number">6.5</span> Advanced Features</a>
  <ul>
  <li><a href="#distributed-training" id="toc-distributed-training" class="nav-link" data-scroll-target="#distributed-training"><span class="header-section-number">6.5.1</span> Distributed training</a></li>
  <li><a href="#model-conversion" id="toc-model-conversion" class="nav-link" data-scroll-target="#model-conversion"><span class="header-section-number">6.5.2</span> Model Conversion</a></li>
  <li><a href="#automl-no-codelow-code-ml" id="toc-automl-no-codelow-code-ml" class="nav-link" data-scroll-target="#automl-no-codelow-code-ml"><span class="header-section-number">6.5.3</span> AutoML, No-Code/Low-Code ML</a></li>
  <li><a href="#advanced-learning-methods" id="toc-advanced-learning-methods" class="nav-link" data-scroll-target="#advanced-learning-methods"><span class="header-section-number">6.5.4</span> Advanced Learning Methods</a>
  <ul class="collapse">
  <li><a href="#transfer-learning" id="toc-transfer-learning" class="nav-link" data-scroll-target="#transfer-learning">Transfer Learning</a></li>
  <li><a href="#federated-learning" id="toc-federated-learning" class="nav-link" data-scroll-target="#federated-learning">Federated Learning</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#framework-specialization" id="toc-framework-specialization" class="nav-link" data-scroll-target="#framework-specialization"><span class="header-section-number">6.6</span> Framework Specialization</a>
  <ul>
  <li><a href="#cloud" id="toc-cloud" class="nav-link" data-scroll-target="#cloud"><span class="header-section-number">6.6.1</span> Cloud</a></li>
  <li><a href="#edge" id="toc-edge" class="nav-link" data-scroll-target="#edge"><span class="header-section-number">6.6.2</span> Edge</a></li>
  <li><a href="#embedded" id="toc-embedded" class="nav-link" data-scroll-target="#embedded"><span class="header-section-number">6.6.3</span> Embedded</a></li>
  </ul></li>
  <li><a href="#sec-ai_frameworks_embedded" id="toc-sec-ai_frameworks_embedded" class="nav-link" data-scroll-target="#sec-ai_frameworks_embedded"><span class="header-section-number">6.7</span> Embedded AI Frameworks</a>
  <ul>
  <li><a href="#resource-constraints" id="toc-resource-constraints" class="nav-link" data-scroll-target="#resource-constraints"><span class="header-section-number">6.7.1</span> Resource Constraints</a></li>
  <li><a href="#frameworks-libraries" id="toc-frameworks-libraries" class="nav-link" data-scroll-target="#frameworks-libraries"><span class="header-section-number">6.7.2</span> Frameworks &amp; Libraries</a></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges"><span class="header-section-number">6.7.3</span> Challenges</a>
  <ul class="collapse">
  <li><a href="#fragmented-ecosystem" id="toc-fragmented-ecosystem" class="nav-link" data-scroll-target="#fragmented-ecosystem">Fragmented Ecosystem</a></li>
  <li><a href="#disparate-hardware-needs" id="toc-disparate-hardware-needs" class="nav-link" data-scroll-target="#disparate-hardware-needs">Disparate Hardware Needs</a></li>
  <li><a href="#lack-of-portability" id="toc-lack-of-portability" class="nav-link" data-scroll-target="#lack-of-portability">Lack of Portability</a></li>
  <li><a href="#incomplete-infrastructure" id="toc-incomplete-infrastructure" class="nav-link" data-scroll-target="#incomplete-infrastructure">Incomplete Infrastructure</a></li>
  <li><a href="#no-standard-benchmark" id="toc-no-standard-benchmark" class="nav-link" data-scroll-target="#no-standard-benchmark">No Standard Benchmark</a></li>
  <li><a href="#minimal-real-world-testing" id="toc-minimal-real-world-testing" class="nav-link" data-scroll-target="#minimal-real-world-testing">Minimal Real-World Testing</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="header-section-number">6.8</span> Examples</a>
  <ul>
  <li><a href="#interpreter" id="toc-interpreter" class="nav-link" data-scroll-target="#interpreter"><span class="header-section-number">6.8.1</span> Interpreter</a></li>
  <li><a href="#compiler-based" id="toc-compiler-based" class="nav-link" data-scroll-target="#compiler-based"><span class="header-section-number">6.8.2</span> Compiler-based</a></li>
  <li><a href="#library" id="toc-library" class="nav-link" data-scroll-target="#library"><span class="header-section-number">6.8.3</span> Library</a></li>
  </ul></li>
  <li><a href="#choosing-the-right-framework" id="toc-choosing-the-right-framework" class="nav-link" data-scroll-target="#choosing-the-right-framework"><span class="header-section-number">6.9</span> Choosing the Right Framework</a>
  <ul>
  <li><a href="#model" id="toc-model" class="nav-link" data-scroll-target="#model"><span class="header-section-number">6.9.1</span> Model</a></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software"><span class="header-section-number">6.9.2</span> Software</a></li>
  <li><a href="#hardware" id="toc-hardware" class="nav-link" data-scroll-target="#hardware"><span class="header-section-number">6.9.3</span> Hardware</a></li>
  <li><a href="#other-factors" id="toc-other-factors" class="nav-link" data-scroll-target="#other-factors"><span class="header-section-number">6.9.4</span> Other Factors</a>
  <ul class="collapse">
  <li><a href="#performance" id="toc-performance" class="nav-link" data-scroll-target="#performance">Performance</a></li>
  <li><a href="#scalability" id="toc-scalability" class="nav-link" data-scroll-target="#scalability">Scalability</a></li>
  <li><a href="#integration-with-data-engineering-tools" id="toc-integration-with-data-engineering-tools" class="nav-link" data-scroll-target="#integration-with-data-engineering-tools">Integration with Data Engineering Tools</a></li>
  <li><a href="#integration-with-model-optimization-tools" id="toc-integration-with-model-optimization-tools" class="nav-link" data-scroll-target="#integration-with-model-optimization-tools">Integration with Model Optimization Tools</a></li>
  <li><a href="#ease-of-use" id="toc-ease-of-use" class="nav-link" data-scroll-target="#ease-of-use">Ease of Use</a></li>
  <li><a href="#community-support" id="toc-community-support" class="nav-link" data-scroll-target="#community-support">Community Support</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#future-trends-in-ml-frameworks" id="toc-future-trends-in-ml-frameworks" class="nav-link" data-scroll-target="#future-trends-in-ml-frameworks"><span class="header-section-number">6.10</span> Future Trends in ML Frameworks</a>
  <ul>
  <li><a href="#decomposition" id="toc-decomposition" class="nav-link" data-scroll-target="#decomposition"><span class="header-section-number">6.10.1</span> Decomposition</a></li>
  <li><a href="#high-performance-compilers-libraries" id="toc-high-performance-compilers-libraries" class="nav-link" data-scroll-target="#high-performance-compilers-libraries"><span class="header-section-number">6.10.2</span> High-Performance Compilers &amp; Libraries</a></li>
  <li><a href="#ml-for-ml-frameworks" id="toc-ml-for-ml-frameworks" class="nav-link" data-scroll-target="#ml-for-ml-frameworks"><span class="header-section-number">6.10.3</span> ML for ML Frameworks</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6.11</span> Conclusion</a></li>
  <li><a href="#sec-ai-frameworks-resource" id="toc-sec-ai-frameworks-resource" class="nav-link" data-scroll-target="#sec-ai-frameworks-resource">Resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/frameworks/frameworks.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/frameworks/frameworks.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/foreword.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/frameworks/frameworks.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Frameworks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ai_frameworks" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Resources: <a href="#sec-ai-frameworks-resource">Slides</a>, <a href="#sec-ai-frameworks-resource">Labs</a>, <a href="#sec-ai-frameworks-resource">Exercises</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_ml_frameworks.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E 3 Prompt: Illustration in a rectangular format, designed for a professional textbook, where the content spans the entire width. The vibrant chart represents training and inference frameworks for ML. Icons for TensorFlow, Keras, PyTorch, ONNX, and TensorRT are spread out, filling the entire horizontal space, and aligned vertically. Each icon is accompanied by brief annotations detailing their features. The lively colors like blues, greens, and oranges highlight the icons and sections against a soft gradient background. The distinction between training and inference frameworks is accentuated through color-coded sections, with clean lines and modern typography maintaining clarity and focus.</em></figcaption>
</figure>
</div>
<p>This chapter explores the landscape of AI frameworks that serve as the foundation for developing machine learning systems. AI frameworks provide the tools, libraries, and environments to design, train, and deploy machine learning models. We delve into the evolutionary trajectory of these frameworks, dissect the workings of TensorFlow, and provide insights into the core components and advanced features that define these frameworks.</p>
<p>Furthermore, we investigate the specialization of frameworks tailored to specific needs, the emergence of frameworks specifically designed for embedded AI, and the criteria for selecting the most suitable framework for your project. This exploration will be rounded off by a glimpse into the future trends expected to shape the landscape of ML frameworks in the coming years.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand the evolution and capabilities of major machine learning frameworks. This includes graph execution models, programming paradigms, hardware acceleration support, and how they have expanded over time.</p></li>
<li><p>Learn frameworks’ core components and functionality, such as computational graphs, data pipelines, optimization algorithms, training loops, etc., that enable efficient model building.</p></li>
<li><p>Compare frameworks across different environments, such as cloud, edge, and TinyML. Learn how frameworks specialize based on computational constraints and hardware.</p></li>
<li><p>Dive deeper into embedded and TinyML-focused frameworks like TensorFlow Lite Micro, CMSIS-NN, TinyEngine, etc., and how they optimize for microcontrollers.</p></li>
<li><p>When choosing a framework, explore model conversion and deployment considerations, including latency, memory usage, and hardware support.</p></li>
<li><p>Evaluate key factors in selecting the right framework, like performance, hardware compatibility, community support, ease of use, etc., based on the specific project needs and constraints.</p></li>
<li><p>Understand the limitations of current frameworks and potential future trends, such as using ML to improve frameworks, decomposed ML systems, and high-performance compilers.</p></li>
</ul>
</div>
</div>
<section id="introduction" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">6.1</span> Introduction</h2>
<p>Machine learning frameworks provide the tools and infrastructure to efficiently build, train, and deploy machine learning models. In this chapter, we will explore the evolution and key capabilities of major frameworks like <a href="https://www.tensorflow.org/">TensorFlow (TF)</a>, <a href="https://pytorch.org/">PyTorch</a>, and specialized frameworks for embedded devices. We will dive into the components like computational graphs, optimization algorithms, hardware acceleration, and more that enable developers to construct performant models quickly. Understanding these frameworks is essential to leverage the power of deep learning across the spectrum from cloud to edge devices.</p>
<p>ML frameworks handle much of the complexity of model development through high-level APIs and domain-specific languages that allow practitioners to quickly construct models by combining pre-made components and abstractions. For example, frameworks like TensorFlow and PyTorch provide Python APIs to define neural network architectures using layers, optimizers, datasets, and more. This enables rapid iteration compared to coding every model detail from scratch.</p>
<p>A key capability framework offered is distributed training engines that can scale model training across clusters of GPUs and TPUs. This makes it feasible to train state-of-the-art models with billions or trillions of parameters on vast datasets. Frameworks also integrate with specialized hardware like NVIDIA GPUs to further accelerate training via optimizations like parallelization and efficient matrix operations.</p>
<p>In addition, frameworks simplify deploying finished models into production through tools like <a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a> for scalable model serving and <a href="https://www.tensorflow.org/lite">TensorFlow Lite</a> for optimization on mobile and edge devices. Other valuable capabilities include visualization, model optimization techniques like quantization and pruning, and monitoring metrics during training.</p>
<p>They were leading open-source frameworks like TensorFlow, PyTorch, and <a href="https://mxnet.apache.org/versions/1.9.1/">MXNet</a>, which power much of AI research and development today. Commercial offerings like <a href="https://aws.amazon.com/pm/sagemaker/">Amazon SageMaker</a> and <a href="https://azure.microsoft.com/en-us/free/machine-learning/search/?ef_id=_k_CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE_k_&amp;OCID=AIDcmm5edswduu_SEM__k_CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE_k_&amp;gad=1&amp;gclid=CjwKCAjws9ipBhB1EiwAccEi1JVOThls797Sj3Li96_GYjoJQDx_EWaXNsDaEWeFbIaRkESUCkq64xoCSmwQAvD_BwE">Microsoft Azure Machine Learning</a> integrate these open source frameworks with proprietary capabilities and enterprise tools.</p>
<p>Machine learning engineers and practitioners leverage these robust frameworks to focus on high-value tasks like model architecture, feature engineering, and hyperparameter tuning instead of infrastructure. The goal is to build and deploy performant models that solve real-world problems efficiently.</p>
<p>This chapter, we will explore today’s leading cloud frameworks and how they have adapted models and tools specifically for embedded and edge deployment. We will compare programming models, supported hardware, optimization capabilities, and more to fully understand how frameworks enable scalable machine learning from the cloud to the edge.</p>
</section>
<section id="framework-evolution" class="level2 page-columns page-full" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="framework-evolution"><span class="header-section-number">6.2</span> Framework Evolution</h2>
<p>Machine learning frameworks have evolved significantly to meet the diverse needs of machine learning practitioners and advancements in AI techniques. A few decades ago, building and training machine learning models required extensive low-level coding and infrastructure. Machine learning frameworks have evolved considerably over the past decade to meet the expanding needs of practitioners and rapid advances in deep learning techniques. Insufficient data and computing power constrained early neural network research. Building and training machine learning models required extensive low-level coding and infrastructure. However, the release of large datasets like <a href="https://www.image-net.org/">ImageNet</a> <span class="citation" data-cites="deng2009imagenet">(<a href="../../references.html#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>)</span> and advancements in parallel GPU computing unlocked the potential for far deeper neural networks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. <span>“<span>ImageNet:</span> <span>A</span> Large-Scale Hierarchical Image Database.”</span> In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. IEEE. <a href="https://doi.org/10.1109/cvpr.2009.5206848">https://doi.org/10.1109/cvpr.2009.5206848</a>.
</div><div id="ref-al2016theano" class="csl-entry" role="listitem">
Team, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et al. 2016. <span>“Theano: <span>A</span> Python Framework for Fast Computation of Mathematical Expressions.”</span> <a href="https://arxiv.org/abs/1605.02688">https://arxiv.org/abs/1605.02688</a>.
</div><div id="ref-jia2014caffe" class="csl-entry" role="listitem">
Jia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. <span>“Caffe: Convolutional Architecture for Fast Feature Embedding.”</span> In <em>Proceedings of the 22nd ACM International Conference on Multimedia</em>, 675–78. ACM. <a href="https://doi.org/10.1145/2647868.2654889">https://doi.org/10.1145/2647868.2654889</a>.
</div><div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. <span>“<span>ImageNet</span> Classification with Deep Convolutional Neural Networks.”</span> In <em>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake Tahoe, Nevada, United States</em>, edited by Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Weinberger, 1106–14. <a href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a>.
</div><div id="ref-chollet2018keras" class="csl-entry" role="listitem">
Chollet, François. 2018. <span>“Introduction to Keras.”</span> <em>March 9th</em>.
</div><div id="ref-tokui2015chainer" class="csl-entry" role="listitem">
Tokui, Seiya, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki, Kota Uenishi, Brian Vogel, and Hiroyuki Yamazaki Vincent. 2019. <span>“Chainer: A Deep Learning Framework for Accelerating the Research Cycle.”</span> In <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp;Amp; Data Mining</em>, 5:1–6. ACM. <a href="https://doi.org/10.1145/3292500.3330756">https://doi.org/10.1145/3292500.3330756</a>.
</div><div id="ref-seide2016cntk" class="csl-entry" role="listitem">
Seide, Frank, and Amit Agarwal. 2016. <span>“Cntk: Microsoft’s Open-Source Deep-Learning Toolkit.”</span> In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 2135–35. ACM. <a href="https://doi.org/10.1145/2939672.2945397">https://doi.org/10.1145/2939672.2945397</a>.
</div><div id="ref-paszke2019pytorch" class="csl-entry" role="listitem">
Ansel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. <span>“<span>PyTorch</span> 2: <span>Faster</span> Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.”</span> In <em>Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>, edited by Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, 8024–35. ACM. <a href="https://doi.org/10.1145/3620665.3640366">https://doi.org/10.1145/3620665.3640366</a>.
</div></div><p>The first ML frameworks, <a href="https://pypi.org/project/Theano/#:~:text=Theano%20is%20a%20Python%20library,%20a%20similar%20interface%20to%20NumPy's.">Theano</a> by <span class="citation" data-cites="al2016theano">Team et al. (<a href="../../references.html#ref-al2016theano" role="doc-biblioref">2016</a>)</span> and <a href="https://caffe.berkeleyvision.org/">Caffe</a> by <span class="citation" data-cites="jia2014caffe">Jia et al. (<a href="../../references.html#ref-jia2014caffe" role="doc-biblioref">2014</a>)</span>, were developed by academic institutions (Montreal Institute for Learning Algorithms, Berkeley Vision and Learning Center). Amid growing interest in deep learning due to state-of-the-art performance of AlexNet <span class="citation" data-cites="krizhevsky2012imagenet">Krizhevsky, Sutskever, and Hinton (<a href="../../references.html#ref-krizhevsky2012imagenet" role="doc-biblioref">2012</a>)</span> on the ImageNet dataset, private companies and individuals began developing ML frameworks, resulting in frameworks such as <a href="https://keras.io/">Keras</a> by <span class="citation" data-cites="chollet2018keras">Chollet (<a href="../../references.html#ref-chollet2018keras" role="doc-biblioref">2018</a>)</span>, <a href="https://chainer.org/">Chainer</a> by <span class="citation" data-cites="tokui2015chainer">Tokui et al. (<a href="../../references.html#ref-tokui2015chainer" role="doc-biblioref">2019</a>)</span>, TensorFlow from Google <span class="citation" data-cites="abadi2016tensorflow">(<a href="../../references.html#ref-abadi2016tensorflow" role="doc-biblioref">Yu et al. 2018</a>)</span>, <a href="https://learn.microsoft.com/en-us/cognitive-toolkit/">CNTK</a> by Microsoft <span class="citation" data-cites="seide2016cntk">(<a href="../../references.html#ref-seide2016cntk" role="doc-biblioref">Seide and Agarwal 2016</a>)</span>, and PyTorch by Facebook <span class="citation" data-cites="paszke2019pytorch">(<a href="../../references.html#ref-paszke2019pytorch" role="doc-biblioref">Ansel et al. 2024</a>)</span>.</p>
<p>Many of these ML frameworks can be divided into high-level vs.&nbsp;low-level frameworks and static vs.&nbsp;dynamic computational graph frameworks. High-level frameworks provide a higher level of abstraction than low-level frameworks. High-level frameworks have pre-built functions and modules for common ML tasks, such as creating, training, and evaluating common ML models, preprocessing data, engineering features, and visualizing data, which low-level frameworks do not have. Thus, high-level frameworks may be easier to use but are less customizable than low-level frameworks (i.e., users of low-level frameworks can define custom layers, loss functions, optimization algorithms, etc.). Examples of high-level frameworks include TensorFlow/Keras and PyTorch. Examples of low-level ML frameworks include TensorFlow with low-level APIs, Theano, Caffe, Chainer, and CNTK.</p>
<p>Frameworks like Theano and Caffe used static computational graphs, which required rigidly defining the full model architecture upfront. Static graphs require upfront declaration and limit flexibility. Dynamic graphs are constructed on the fly for more iterative development. However, around 2016, frameworks began adopting dynamic graphs like PyTorch and TensorFlow 2.0, which can construct graphs on the fly. This provides greater flexibility for model development. We will discuss these concepts and details later in the AI Training section.</p>
<p>The development of these frameworks facilitated an explosion in model size and complexity over time—from early multilayer perceptrons and convolutional networks to modern transformers with billions or trillions of parameters. In 2016, ResNet models by <span class="citation" data-cites="he2016deep">He et al. (<a href="../../references.html#ref-he2016deep" role="doc-biblioref">2016</a>)</span> achieved record ImageNet accuracy with over 150 layers and 25 million parameters. Then, in 2020, the GPT-3 language model from OpenAI <span class="citation" data-cites="brown2020language">(<a href="../../references.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> pushed parameters to an astonishing 175 billion using model parallelism in frameworks to train across thousands of GPUs and TPUs.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–78. IEEE. <a href="https://doi.org/10.1109/cvpr.2016.90">https://doi.org/10.1109/cvpr.2016.90</a>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual</em>, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.
</div></div><p>Each generation of frameworks unlocked new capabilities that powered advancement:</p>
<ul>
<li><p>Theano and TensorFlow (2015) introduced computational graphs and automatic differentiation to simplify model building.</p></li>
<li><p>CNTK (2016) pioneered efficient distributed training by combining model and data parallelism.</p></li>
<li><p>PyTorch (2016) provided imperative programming and dynamic graphs for flexible experimentation.</p></li>
<li><p>TensorFlow 2.0 (2019) defaulted eager execution for intuitiveness and debugging.</p></li>
<li><p>TensorFlow Graphics (2020) added 3D data structures to handle point clouds and meshes.</p></li>
</ul>
<p>In recent years, the frameworks have converged. <a href="#fig-ml-framework" class="quarto-xref">Figure&nbsp;<span>6.1</span></a> shows that TensorFlow and PyTorch have become the overwhelmingly dominant ML frameworks, representing more than 95% of ML frameworks used in research and production. Keras was integrated into TensorFlow in 2019; Preferred Networks transitioned Chainer to PyTorch in 2019; and Microsoft stopped actively developing CNTK in 2022 to support PyTorch on Windows.</p>
<div id="fig-ml-framework" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image6.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Popularity of ML frameworks in the United States as measured by Google web searches. Credit: Google.
</figcaption>
</figure>
</div>
<p>However, a one-size-fits-all approach only works well across the spectrum from cloud to tiny edge devices. Different frameworks represent various philosophies around graph execution, declarative versus imperative APIs, and more. Declaratives define what the program should do, while imperatives focus on how it should be done step-by-step. For instance, TensorFlow uses graph execution and declarative-style modeling, while PyTorch adopts eager execution and imperative modeling for more Pythonic flexibility. Each approach carries tradeoffs, which we will discuss later in the Basic Components section.</p>
<p>Today’s advanced frameworks enable practitioners to develop and deploy increasingly complex models - a key driver of innovation in the AI field. However, they continue to evolve and expand their capabilities for the next generation of machine learning. To understand how these systems continue to evolve, we will dive deeper into TensorFlow as an example of how the framework grew in complexity over time.</p>
</section>
<section id="deepdive-into-tensorflow" class="level2 page-columns page-full" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="deepdive-into-tensorflow"><span class="header-section-number">6.3</span> DeepDive into TensorFlow</h2>
<p>TensorFlow was developed by the Google Brain team and was released as an open-source software library on November 9, 2015. It was designed for numerical computation using data flow graphs and has since become popular for a wide range of machine learning and deep learning applications.</p>
<p>TensorFlow is a training and inference framework that provides built-in functionality to handle everything from model creation and training to deployment, as shown in <a href="#fig-tensorflow-architecture" class="quarto-xref">Figure&nbsp;<span>6.2</span></a>. Since its initial development, the TensorFlow ecosystem has grown to include many different “varieties” of TensorFlow, each intended to allow users to support ML on different platforms. In this section, we will mainly discuss only the core package.</p>
<section id="tf-ecosystem" class="level3 page-columns page-full" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="tf-ecosystem"><span class="header-section-number">6.3.1</span> TF Ecosystem</h3>
<ol type="1">
<li><p><a href="https://www.tensorflow.org/tutorials">TensorFlow Core</a>: primary package that most developers engage with. It provides a comprehensive, flexible platform for defining, training, and deploying machine learning models. It includes tf—keras as its high-level API.</p></li>
<li><p><a href="#tensorflow-lite">TensorFlow Lite</a> (https://www.tensorflow.org/lite): designed for deploying lightweight models on mobile, embedded, and edge devices. It offers tools to convert TensorFlow models to a more compact format suitable for limited-resource devices and provides optimized pre-trained models for mobile.</p></li>
<li><p><a href="https://www.tensorflow.org/js">TensorFlow.js</a>: JavaScript library that allows training and deployment of machine learning models directly in the browser or on Node.js. It also provides tools for porting pre-trained TensorFlow models to the browser-friendly format.</p></li>
<li><p><a href="https://developers.googleblog.com/2019/03/introducing-coral-our-platform-for.html">TensorFlow on Edge Devices (Coral)</a>: platform of hardware components and software tools from Google that allows the execution of TensorFlow models on edge devices, leveraging Edge TPUs for acceleration.</p></li>
<li><p><a href="https://www.tensorflow.org/federated">TensorFlow Federated (TFF)</a>: framework for machine learning and other computations on decentralized data. TFF facilitates federated learning, allowing model training across many devices without centralizing the data.</p></li>
<li><p><a href="https://www.tensorflow.org/graphics">TensorFlow Graphics</a>: library for using TensorFlow to carry out graphics-related tasks, including 3D shapes and point clouds processing, using deep learning.</p></li>
<li><p><a href="https://www.tensorflow.org/hub">TensorFlow Hub</a>: repository of reusable machine learning model components to allow developers to reuse pre-trained model components, facilitating transfer learning and model composition</p></li>
<li><p><a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a>: framework designed for serving and deploying machine learning models for inference in production environments. It provides tools for versioning and dynamically updating deployed models without service interruption.</p></li>
<li><p><a href="https://www.tensorflow.org/tfx">TensorFlow Extended (TFX)</a>: end-to-end platform designed to deploy and manage machine learning pipelines in production settings. TFX encompasses data validation, preprocessing, model training, validation, and serving components.</p></li>
</ol>
<div id="fig-tensorflow-architecture" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensorflow-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/tensorflow.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensorflow-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Architecture overview of TensorFlow 2.0. Credit: <a href="https://blog.tensorflow.org/2019/01/whats-coming-in-tensorflow-2-0.html">Tensorflow.</a>
</figcaption>
</figure>
</div>
<p>TensorFlow was developed to address the limitations of DistBelief <span class="citation" data-cites="abadi2016tensorflow">(<a href="../../references.html#ref-abadi2016tensorflow" role="doc-biblioref">Yu et al. 2018</a>)</span>—the framework in use at Google from 2011 to 2015—by providing flexibility along three axes: 1) defining new layers, 2) refining training algorithms, and 3) defining new training algorithms. To understand what limitations in DistBelief led to the development of TensorFlow, we will first give a brief overview of the Parameter Server Architecture that DistBelief employed <span class="citation" data-cites="dean2012large">(<a href="../../references.html#ref-dean2012large" role="doc-biblioref">Dean et al. 2012</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-abadi2016tensorflow" class="csl-entry" role="listitem">
Yu, Yuan, Martı́n Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, et al. 2018. <span>“Dynamic Control Flow in Large-Scale Machine Learning.”</span> In <em>Proceedings of the Thirteenth EuroSys Conference</em>, 265–83. ACM. <a href="https://doi.org/10.1145/3190508.3190551">https://doi.org/10.1145/3190508.3190551</a>.
</div><div id="ref-dean2012large" class="csl-entry" role="listitem">
Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, et al. 2012. <span>“Large Scale Distributed Deep Networks.”</span> In <em>Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a Meeting Held December 3-6, 2012, Lake Tahoe, Nevada, United States</em>, edited by Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q. Weinberger, 1232–40. <a href="https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html">https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html</a>.
</div></div><p>The Parameter Server (PS) architecture is a popular design for distributing the training of machine learning models, especially deep neural networks, across multiple machines. The fundamental idea is to separate the storage and management of model parameters from the computation used to update these parameters:</p>
<p><strong>Storage:</strong> The stateful parameter server processes handled the storage and management of model parameters. Given the large scale of models and the system’s distributed nature, these parameters were sharded across multiple parameter servers. Each server maintained a portion of the model parameters, making it "stateful" as it had to maintain and manage this state across the training process.</p>
<p><strong>Computation:</strong> The worker processes, which could be run in parallel, were stateless and purely computational. They processed data and computed gradients without maintaining any state or long-term memory <span class="citation" data-cites="li2014communication">(<a href="../../references.html#ref-li2014communication" role="doc-biblioref">M. Li et al. 2014</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2014communication" class="csl-entry" role="listitem">
Li, Mu, David G. Andersen, Alexander J. Smola, and Kai Yu. 2014. <span>“Communication Efficient Distributed Machine Learning with the Parameter Server.”</span> In <em>Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada</em>, edited by Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, 19–27. <a href="https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html">https://proceedings.neurips.cc/paper/2014/hash/1ff1de774005f8da13f42943881c655f-Abstract.html</a>.
</div></div><div id="exr-tfc" class="callout callout-style-simple callout-exercise no-icon theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.1 (TensorFlow Core)</strong></span> &nbsp;</p>
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Let’s comprehensively understand core machine learning algorithms using TensorFlow and their practical applications in data analysis and predictive modeling. We will start with linear regression to predict survival rates from the Titanic dataset. Then, using TensorFlow, we will construct classifiers to identify different species of flowers based on their attributes. Next, we will use the K-Means algorithm and its application in segmenting datasets into cohesive clusters. Finally, we will apply hidden Markov models (HMM) to foresee weather patterns.</p>
<p><a href="https://colab.research.google.com/drive/15Cyy2H7nT40sGR7TBN5wBvgTd57mVKay#scrollTo=IEeIRxlbx0wY"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
<div id="exr-tfl" class="callout callout-style-simple callout-exercise no-icon theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.2 (TensorFlow Lite)</strong></span> &nbsp;</p>
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Here, we will see how to build a miniature machine-learning model for microcontrollers. We will build a mini neural network that is streamlined to learn from data even with limited resources and optimized for deployment by shrinking our model for efficient use on microcontrollers. TensorFlow Lite, a powerful technology derived from TensorFlow, shrinks models for tiny devices and helps enable on-device features like image recognition in smart devices. It is used in edge computing to allow for faster analysis and decisions in devices processing data locally.</p>
<p><a href="https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/2_Applications_Deploy/Class_16/TFLite-Micro-Hello-World/train_TFL_Micro_hello_world_model.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
<p>DistBelief and its architecture defined above were crucial in enabling distributed deep learning at Google but also introduced limitations that motivated the development of TensorFlow:</p>
</section>
<section id="static-computation-graph" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="static-computation-graph"><span class="header-section-number">6.3.2</span> Static Computation Graph</h3>
<p>Model parameters are distributed across various parameter servers in the parameter server architecture. Since DistBelief was primarily designed for the neural network paradigm, parameters corresponded to a fixed neural network structure. If the computation graph were dynamic, the distribution and coordination of parameters would become significantly more complicated. For example, a change in the graph might require the initialization of new parameters or the removal of existing ones, complicating the management and synchronization tasks of the parameter servers. This made it harder to implement models outside the neural framework or models that required dynamic computation graphs.</p>
<p>TensorFlow was designed as a more general computation framework that expresses computation as a data flow graph. This allows for a wider variety of machine learning models and algorithms outside of neural networks and provides flexibility in refining models.</p>
</section>
<section id="usability-deployment" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="usability-deployment"><span class="header-section-number">6.3.3</span> Usability &amp; Deployment</h3>
<p>The parameter server model delineates roles (worker nodes and parameter servers) and is optimized for data center deployments, which might only be optimal for some use cases. For instance, this division introduces overheads or complexities on edge devices or in other non-data center environments.</p>
<p>TensorFlow was built to run on multiple platforms, from mobile devices and edge devices to cloud infrastructure. It also aimed to be lighter and developer-friendly and to provide ease of use between local and distributed training.</p>
</section>
<section id="architecture-design" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="architecture-design"><span class="header-section-number">6.3.4</span> Architecture Design</h3>
<p>Rather than using the parameter server architecture, TensorFlow deploys tasks across a cluster. These tasks are named processes that can communicate over a network, and each can execute TensorFlow’s core construct, the dataflow graph, and interface with various computing devices (like CPUs or GPUs). This graph is a directed representation where nodes symbolize computational operations, and edges depict the tensors (data) flowing between these operations.</p>
<p>Despite the absence of traditional parameter servers, some “PS tasks” still store and manage parameters reminiscent of parameter servers in other systems. The remaining tasks, which usually handle computation, data processing, and gradient calculations, are referred to as "worker tasks." TensorFlow’s PS tasks can execute any computation representable by the dataflow graph, meaning they aren’t just limited to parameter storage, and the computation can be distributed. This capability makes them significantly more versatile and gives users the power to program the PS tasks using the standard TensorFlow interface, the same one they’d use to define their models. As mentioned above, dataflow graphs’ structure also makes them inherently good for parallelism, allowing for the processing of large datasets.</p>
</section>
<section id="built-in-functionality-keras" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="built-in-functionality-keras"><span class="header-section-number">6.3.5</span> Built-in Functionality &amp; Keras</h3>
<p>TensorFlow includes libraries to help users develop and deploy more use-case-specific models, and since this framework is open-source, this list continues to grow. These libraries address the entire ML development lifecycle: data preparation, model building, deployment, and responsible AI.</p>
<p>One of TensorFlow’s biggest advantages is its integration with Keras, though, as we will cover in the next section, Pytorch recently added a Keras integration. Keras is another ML framework built to be extremely user-friendly and, as a result, has a high level of abstraction. We will cover Keras in more depth later in this chapter. However, when discussing its integration with TensorFlow, it was important to note that it was originally built to be backend-agnostic. This means users could abstract away these complexities, offering a cleaner, more intuitive way to define and train models without worrying about compatibility issues with different backends. TensorFlow users had some complaints about the usability and readability of TensorFlow’s API, so as TF gained prominence, it integrated Keras as its high-level API. This integration offered major benefits to TensorFlow users since it introduced more intuitive readability and portability of models while still taking advantage of powerful backend features, Google support, and infrastructure to deploy models on various platforms.</p>
<div id="exr-k" class="callout callout-style-simple callout-exercise no-icon theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.3 (Exploring Keras: Building, Training, and Evaluating Neural Networks)</strong></span> &nbsp;</p>
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Here, we’ll learn how to use Keras, a high-level neural network API, for model development and training. We will explore the functional API for concise model building, understand loss and metric classes for model evaluation, and use built-in optimizers to update model parameters during training. Additionally, we’ll discover how to define custom layers and metrics tailored to our needs. Lastly, we’ll delve into Keras’ training loops to streamline the process of training neural networks on large datasets. This knowledge will empower us to build and optimize neural network models across various machine learning and artificial intelligence applications.</p>
<p><a href="https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO#scrollTo=fxINLLGitX_n"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
<section id="limitations-and-challenges" class="level3" data-number="6.3.6">
<h3 data-number="6.3.6" class="anchored" data-anchor-id="limitations-and-challenges"><span class="header-section-number">6.3.6</span> Limitations and Challenges</h3>
<p>TensorFlow is one of the most popular deep learning frameworks but has criticisms and weaknesses, mostly focusing on usability and resource usage. While advantageous, the rapid pace of updates through its support from Google has sometimes led to backward compatibility issues, deprecated functions, and shifting documentation. Additionally, even with the Keras implementation, TensorFlow’s syntax and learning curve can be difficult for new users. One major critique of TensorFlow is its high overhead and memory consumption due to the range of built-in libraries and support. Some of these concerns can be addressed using pared-down versions, but they can still be limited in resource-constrained environments.</p>
</section>
<section id="pytorch-vs.-tensorflow" class="level3" data-number="6.3.7">
<h3 data-number="6.3.7" class="anchored" data-anchor-id="pytorch-vs.-tensorflow"><span class="header-section-number">6.3.7</span> PyTorch vs.&nbsp;TensorFlow</h3>
<p>PyTorch and TensorFlow have established themselves as frontrunners in the industry. Both frameworks offer robust functionalities but differ in design philosophies, ease of use, ecosystem, and deployment capabilities.</p>
<p><strong>Design Philosophy and Programming Paradigm:</strong> PyTorch uses a dynamic computational graph termed eager execution. This makes it intuitive and facilitates debugging since operations are executed immediately and can be inspected on the fly. In comparison, earlier versions of TensorFlow were centered around a static computational graph, which required the graph’s complete definition before execution. However, TensorFlow 2.0 introduced eager execution by default, making it more aligned with PyTorch. PyTorch’s dynamic nature and Python-based approach have enabled its simplicity and flexibility, particularly for rapid prototyping. TensorFlow’s static graph approach in its earlier versions had a steeper learning curve; the introduction of TensorFlow 2.0, with its Keras integration as the high-level API, has significantly simplified the development process.</p>
<p><strong>Deployment:</strong> PyTorch is heavily favored in research environments; deploying PyTorch models in production settings was traditionally challenging. However, deployment has become more feasible with the introduction of TorchScript and the TorchServe tool. One of TensorFlow’s strengths lies in its scalability and deployment capabilities, especially on embedded and mobile platforms with TensorFlow Lite. TensorFlow Serving and TensorFlow.js further facilitate deployment in various environments, thus giving it a broader reach in the ecosystem.</p>
<p><strong>Performance:</strong> Both frameworks offer efficient hardware acceleration for their operations. However, TensorFlow has a slightly more robust optimization workflow, such as the XLA (Accelerated Linear Algebra) compiler, which can further boost performance. Its static computational graph was also advantageous for certain optimizations in the early versions.</p>
<p><strong>Ecosystem:</strong> PyTorch has a growing ecosystem with tools like TorchServe for serving models and libraries like TorchVision, TorchText, and TorchAudio for specific domains. As we mentioned earlier, TensorFlow has a broad and mature ecosystem. TensorFlow Extended (TFX) provides an end-to-end platform for deploying production machine learning pipelines. Other tools and libraries include TensorFlow Lite, TensorFlow.js, TensorFlow Hub, and TensorFlow Serving.</p>
<p><a href="#tbl-pytorch_vs_tf" class="quarto-xref">Table&nbsp;<span>6.1</span></a> provides a comparative analysis:</p>
<div id="tbl-pytorch_vs_tf" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pytorch_vs_tf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.1: Comparison of PyTorch and TensorFlow.
</figcaption>
<div aria-describedby="tbl-pytorch_vs_tf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 41%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Feature/Aspect</th>
<th>PyTorch</th>
<th>TensorFlow</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Design Philosophy</td>
<td>Dynamic computational graph (eager execution)</td>
<td>Static computational graph (early versions); Eager execution in TensorFlow 2.0</td>
</tr>
<tr class="even">
<td>Deployment</td>
<td>Traditionally challenging; Improved with TorchScript &amp; TorchServe</td>
<td>Scalable, especially on embedded platforms with TensorFlow Lite</td>
</tr>
<tr class="odd">
<td>Performance &amp; Optimization</td>
<td>Efficient GPU acceleration</td>
<td>Robust optimization with XLA compiler</td>
</tr>
<tr class="even">
<td>Ecosystem</td>
<td>TorchServe, TorchVision, TorchText, TorchAudio</td>
<td>TensorFlow Extended (TFX), TensorFlow Lite, TensorFlow.js, TensorFlow Hub, TensorFlow Serving</td>
</tr>
<tr class="odd">
<td>Ease of Use</td>
<td>Preferred for its Pythonic approach and rapid prototyping</td>
<td>Initially steep learning curve; Simplified with Keras in TensorFlow 2.0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="basic-framework-components" class="level2 page-columns page-full" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="basic-framework-components"><span class="header-section-number">6.4</span> Basic Framework Components</h2>
<section id="tensor-data-structures" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="tensor-data-structures"><span class="header-section-number">6.4.1</span> Tensor data structures</h3>
<p>To understand tensors, let us start from the familiar concepts in linear algebra. As demonstrated in <a href="#fig-tensor-data-structure" class="quarto-xref">Figure&nbsp;<span>6.3</span></a>, vectors can be represented as a stack of numbers in a 1-dimensional array. Matrices follow the same idea, and one can think of them as many vectors stacked on each other, making them 2 dimensional. Higher dimensional tensors work the same way. A 3-dimensional tensor is simply a set of matrices stacked on each other in another direction. Therefore, vectors and matrices can be considered special cases of tensors with 1D and 2D dimensions, respectively.</p>
<div id="fig-tensor-data-structure" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-data-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: Visualization of Tensor Data Structure.
</figcaption>
</figure>
</div>
<p>Defining formally, in machine learning, tensors are a multi-dimensional array of numbers. The number of dimensions defines the rank of the tensor. As a generalization of linear algebra, the study of tensors is called multilinear algebra. There are noticeable similarities between matrices and higher-ranked tensors. First, extending the definitions given in linear algebra to tensors, such as with eigenvalues, eigenvectors, and rank (in the linear algebra sense), is possible. Furthermore, with the way we have defined tensors, it is possible to turn higher dimensional tensors into matrices. This is critical in practice, as the multiplication of abstract representations of higher dimensional tensors is often completed by first converting them into matrices for multiplication.</p>
<p>Tensors offer a flexible data structure that can represent data in higher dimensions. For example, to represent color image data, for each pixel value (in 2 dimensions), one needs the color values for red, green, and blue. With tensors, it is easy to contain image data in a single 3-dimensional tensor, with each number within it representing a certain color value in a certain location of the image. Extending even further, if we wanted to store a series of images, we could extend the dimensions such that the new dimension (to create a 4-dimensional tensor) represents our different images. This is exactly what the famous <a href="https://www.tensorflow.org/datasets/catalog/mnist">MNIST</a> dataset does, loading a single 4-dimensional tensor when one calls to load the dataset, allowing a compact representation of all the data in one place.</p>
</section>
<section id="computational-graphs" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="computational-graphs"><span class="header-section-number">6.4.2</span> Computational graphs</h3>
<section id="graph-definition" class="level4">
<h4 class="anchored" data-anchor-id="graph-definition">Graph Definition</h4>
<p>Computational graphs are a key component of deep learning frameworks like TensorFlow and PyTorch. They allow us to express complex neural network architectures efficiently and differentiatedly. A computational graph consists of a directed acyclic graph (DAG) where each node represents an operation or variable, and edges represent data dependencies between them.</p>
<p>For example, a node might represent a matrix multiplication operation, taking two input matrices (or tensors) and producing an output matrix (or tensor). To visualize this, consider the simple example in <a href="../training/training.html#fig-computational-graph" class="quarto-xref">Figure&nbsp;<span>7.3</span></a>. The directed acyclic graph above computes <span class="math inline">\(z = x \times y\)</span>, where each variable is just numbers.</p>
<div id="fig-computational-graph" class="quarto-figure quarto-figure-center quarto-float anchored" data-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-computational-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image1.png" style="width:50.0%" data-align="center" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-computational-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: Basic example of a computational graph.
</figcaption>
</figure>
</div>
<p>Underneath the hood, the computational graphs represent abstractions for common layers like convolutional, pooling, recurrent, and dense layers, with data including activations, weights, and biases represented in tensors. Convolutional layers form the backbone of CNN models for computer vision. They detect spatial patterns in input data through learned filters. Recurrent layers like LSTMs and GRUs enable sequential data processing for tasks like language translation. Attention layers are used in transformers to draw global context from the entire input.</p>
<p>Layers are higher-level abstractions that define computations on top of those tensors. For example, a Dense layer performs a matrix multiplication and addition between input/weight/bias tensors. Note that a layer operates on tensors as inputs and outputs; the layer is not a tensor. Some key differences:</p>
<ul>
<li><p>Layers contain states like weights and biases. Tensors are stateless, just holding data.</p></li>
<li><p>Layers can modify internal state during training. Tensors are immutable/read-only.</p></li>
<li><p>Layers are higher-level abstractions. Tensors are at a lower level and directly represent data and math operations.</p></li>
<li><p>Layers define fixed computation patterns. Tensors flow between layers during execution.</p></li>
<li><p>Layers are used indirectly when building models. Tensors flow between layers during execution.</p></li>
</ul>
<p>So, while tensors are a core data structure that layers consume and produce, layers have additional functionality for defining parameterized operations and training. While a layer configures tensor operations under the hood, the layer remains distinct from the tensor objects. The layer abstraction makes building and training neural networks much more intuitive. This abstraction enables developers to build models by stacking these layers together without implementing the layer logic. For example, calling <code>tf.keras.layers.Conv2D</code> in TensorFlow creates a convolutional layer. The framework handles computing the convolutions, managing parameters, etc. This simplifies model development, allowing developers to focus on architecture rather than low-level implementations. Layer abstractions utilize highly optimized implementations for performance. They also enable portability, as the same architecture can run on different hardware backends like GPUs and TPUs.</p>
<p>In addition, computational graphs include activation functions like ReLU, sigmoid, and tanh that are essential to neural networks, and many frameworks provide these as standard abstractions. These functions introduce non-linearities that enable models to approximate complex functions. Frameworks provide these as simple, predefined operations that can be used when constructing models, for example, if.nn.relu in TensorFlow. This abstraction enables flexibility, as developers can easily swap activation functions for tuning performance. Predefined activations are also optimized by the framework for faster execution.</p>
<p>In recent years, models like ResNets and MobileNets have emerged as popular architectures, with current frameworks pre-packaging these as computational graphs. Rather than worrying about the fine details, developers can utilize them as a starting point, customizing as needed by substituting layers. This simplifies and speeds up model development, avoiding reinventing architectures from scratch. Predefined models include well-tested, optimized implementations that ensure good performance. Their modular design also enables transferring learned features to new tasks via transfer learning. These predefined architectures provide high-performance building blocks to create robust models quickly.</p>
<p>These layer abstractions, activation functions, and predefined architectures the frameworks provide constitute a computational graph. When a user defines a layer in a framework (e.g., tf.keras.layers.Dense()), the framework configures computational graph nodes and edges to represent that layer. The layer parameters like weights and biases become variables in the graph. The layer computations become operation nodes (such as the x and y in the figure above). When you call an activation function like tf.nn.relu(), the framework adds a ReLU operation node to the graph. Predefined architectures are just pre-configured subgraphs that can be inserted into your model’s graph. Thus, model definition via high-level abstractions creates a computational graph—the layers, activations, and architectures we use become graph nodes and edges.</p>
<p>We implicitly construct a computational graph when defining a neural network architecture in a framework. The framework uses this graph to determine operations to run during training and inference. Computational graphs bring several advantages over raw code, and that’s one of the core functionalities that is offered by a good ML framework:</p>
<ul>
<li><p>Explicit representation of data flow and operations</p></li>
<li><p>Ability to optimize graph before execution</p></li>
<li><p>Automatic differentiation for training</p></li>
<li><p>Language agnosticism - graph can be translated to run on GPUs, TPUs, etc.</p></li>
<li><p>Portability - graph can be serialized, saved, and restored later</p></li>
</ul>
<p>Computational graphs are the fundamental building blocks of ML frameworks. Model definition via high-level abstractions creates a computational graph—the layers, activations, and architectures we use become graph nodes and edges. The framework compilers and optimizers operate on this graph to generate executable code. The abstractions provide a developer-friendly API for building computational graphs. Under the hood, it’s still graphs down! So, while you may not directly manipulate graphs as a framework user, they enable your high-level model specifications to be efficiently executed. The abstractions simplify model-building, while computational graphs make it possible.</p>
</section>
<section id="static-vs.-dynamic-graphs" class="level4">
<h4 class="anchored" data-anchor-id="static-vs.-dynamic-graphs">Static vs.&nbsp;Dynamic Graphs</h4>
<p>Deep learning frameworks have traditionally followed one of two approaches for expressing computational graphs.</p>
<p><strong>Static graphs (declare-then-execute):</strong> With this model, the entire computational graph must be defined upfront before running it. All operations and data dependencies must be specified during the declaration phase. TensorFlow originally followed this static approach - models were defined in a separate context, and then a session was created to run them. The benefit of static graphs is they allow more aggressive optimization since the framework can see the full graph. However, it also tends to be less flexible for research and interactivity. Changes to the graph require re-declaring the full model.</p>
<p>For example:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.placeholder(tf.float32)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.matmul(x, weights) <span class="op">+</span> biases</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model is defined separately from execution, like building a blueprint. For TensorFlow 1. x, this is done using tf.Graph(). All ops and variables must be declared upfront. Subsequently, the graph is compiled and optimized before running. Execution is done later by feeding in tensor values.</p>
<p><strong>Dynamic graphs (define-by-run):</strong> Unlike declaring (all) first and then executing, the graph is built dynamically as execution happens. There is no separate declaration phase - operations execute immediately as defined. This style is imperative and flexible, facilitating experimentation.</p>
<p>PyTorch uses dynamic graphs, building the graph on the fly as execution happens. For example, consider the following code snippet, where the graph is built as the execution is taking place:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">4</span>,<span class="dv">784</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.matmul(x, weights) <span class="op">+</span> biases</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The above example does not have separate compile/build/run phases. Ops define and execute immediately. With dynamic graphs, the definition is intertwined with execution, providing a more intuitive, interactive workflow. However, the downside is that there is less potential for optimization since the framework only sees the graph as it is built.</p>
<p>Recently, however, the distinction has blurred as frameworks adopt both modes. TensorFlow 2.0 defaults to dynamic graph mode while letting users work with static graphs when needed. Dynamic declaration makes frameworks easier to use, while static models provide optimization benefits. The ideal framework offers both options.</p>
<p>Static graph declaration provides optimization opportunities but less interactivity. While dynamic execution offers flexibility and ease of use, it may have performance overhead. Here is a table comparing the pros and cons of static vs dynamic execution graphs:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Execution Graph</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Static (Declare-then-execute)</td>
<td>Enable graph optimizations by seeing full model ahead of time<br>Can export and deploy frozen graphs<br>Graph is packaged independently of code</td>
<td>Less flexible for research and iteration<br>Changes require rebuilding graph<br>Execution has separate compile and run phases</td>
</tr>
<tr class="even">
<td>Dynamic (Define-by-run)</td>
<td>Intuitive imperative style like Python code<br>Interleave graph build with execution<br>Easy to modify graphs<br>Debugging seamlessly fits workflow</td>
<td>Harder to optimize without full graph<br>Possible slowdowns from graph building during execution<br>Can require more memory</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="data-pipeline-tools" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="data-pipeline-tools"><span class="header-section-number">6.4.3</span> Data Pipeline Tools</h3>
<p>Computational graphs can only be as good as the data they learn from and work on. Therefore, feeding training data efficiently is crucial for optimizing deep neural network performance, though it is often overlooked as one of the core functionalities. Many modern AI frameworks provide specialized pipelines to ingest, process, and augment datasets for model training.</p>
<section id="data-loaders" class="level4">
<h4 class="anchored" data-anchor-id="data-loaders">Data Loaders</h4>
<p>These pipelines’ cores are data loaders, which handle reading examples from storage formats like CSV files or image folders. Reading training examples from sources like files, databases, object storage, etc., is the job of the data loaders. Deep learning models require diverse data formats depending on the application. Among the popular formats is CSV, a versatile, simple format often used for tabular data. TFRecord: TensorFlow’s proprietary format, optimized for performance. Parquet: Columnar storage, offering efficient data compression and retrieval. JPEG/PNG: Commonly used for image data. WAV/MP3: Prevalent formats for audio data. For instance, <code>tf.data</code> is TensorFlows’s dataloading pipeline: <a href="https://www.tensorflow.org/guide/data" class="uri">https://www.tensorflow.org/guide/data</a>.</p>
<p>Data loaders batch examples to leverage vectorization support in hardware. Batching refers to grouping multiple data points for simultaneous processing, leveraging the vectorized computation capabilities of hardware like GPUs. While typical batch sizes range from 32 to 512 examples, the optimal size often depends on the data’s memory footprint and the specific hardware constraints. Advanced loaders can stream virtually unlimited datasets from disk and cloud storage. They stream large datasets from disks or networks instead of fully loading them into memory, enabling unlimited dataset sizes.</p>
<p>Data loaders can also shuffle data across epochs for randomization and preprocess features in parallel with model training to expedite the training process. Randomly shuffling the order of examples between training epochs reduces bias and improves generalization.</p>
<p>Data loaders also support caching and prefetching strategies to optimize data delivery for fast, smooth model training. Caching preprocessed batches in memory allows them to be reused efficiently during multiple training steps and eliminates redundant processing. Prefetching, conversely, involves preloading subsequent batches, ensuring that the model never idles waiting for data.</p>
</section>
</section>
<section id="data-augmentation" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="data-augmentation"><span class="header-section-number">6.4.4</span> Data Augmentation</h3>
<p>Besides loading, data augmentation expands datasets synthetically. Augmentations apply random transformations for images like flipping, cropping, rotating, altering color, adding noise, etc. For audio, common augmentations involve mixing clips with background noise or modulating speed/pitch/volume.</p>
<p>Augmentations increase variation in the training data. Frameworks like TensorFlow and PyTorch simplify applying random augmentations each epoch by integrating them into the data pipeline. By programmatically increasing variation in the training data distribution, augmentations reduce Overfitting and improve model generalization.</p>
<p>Many frameworks simplify integrating augmentations into the data pipeline, applying them on the fly each epoch. Together, performant data loaders and extensive augmentations enable practitioners to feed massive, varied datasets to neural networks efficiently. Hands-off data pipelines represent a significant improvement in usability and productivity. They allow developers to focus more on model architecture and less on data wrangling when training deep learning models.</p>
</section>
<section id="optimization-algorithms" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="optimization-algorithms"><span class="header-section-number">6.4.5</span> Optimization Algorithms</h3>
<p>Training a neural network is fundamentally an iterative process that seeks to minimize a loss function. The goal is to fine-tune the model weights and parameters to produce predictions close to the true target labels. Machine learning frameworks have greatly streamlined this process by offering extensive support in three critical areas: loss functions, optimization algorithms, and regularization techniques.</p>
<p>Loss Functions are useful to quantify the difference between the model’s predictions and the true values. Different datasets require a different loss function to perform properly, as the loss function tells the computer the “objective” for it to aim. Commonly used loss functions are Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.</p>
<p>To demonstrate some of the loss functions, imagine you have a set of inputs and the corresponding outputs, <span class="math inline">\(Y_n\)</span>, that denote the output of <span class="math inline">\(n\)</span>’th value. The inputs are fed into the model, and the model outputs a prediction, which we can call <span class="math inline">\(\hat{Y_n}\)</span>. With the predicted value and the real value, we can, for example, use the MSE to calculate the loss function:</p>
<p><span class="math display">\[MSE = \frac{1}{N}\sum_{n=1}^{N}(Y_n - \hat{Y_n})^2\]</span></p>
<p>If the problem is a classification problem, we do not want to use the MSE since the distance between the predicted value and the real value does not have significant meaning. For example, if one wants to recognize handwritten models, while 9 is further away from 2, it does not mean that the model is wrong in making the prediction. Therefore, we use the cross-entropy loss function, which is defined as:</p>
<p><span class="math display">\[Cross-Entropy = -\sum_{n=1}^{N}Y_n\log(\hat{Y_n})\]</span></p>
<p>Once a loss like the above is computed, we need methods to adjust the model’s parameters to reduce this loss or error during the training process. To do so, current frameworks use a gradient-based approach, which computes how much changes tuning the weights in a certain way changes the value of the loss function. Knowing this gradient, the model moves in the direction that reduces the gradient. Many challenges are associated with this, primarily stemming from the fact that the optimization problem could not be more, making it very easy to solve. More details about this will come in the AI Training section. Modern frameworks come equipped with efficient implementations of several optimization algorithms, many of which are variants of gradient descent algorithms with stochastic methods and adaptive learning rates. More information with clear examples can be found in the AI Training section.</p>
<p>Lastly, overly complex models tend to overfit, meaning they perform well on the training data but must generalize to new, unseen data (see Overfitting). To counteract this, regularization methods are employed to penalize model complexity and encourage it to learn simpler patterns. Dropout randomly sets a fraction of input units to 0 at each update during training, which helps prevent Overfitting.</p>
<p>However, there are cases where the problem is more complex than the model can represent, which may result in underfitting. Therefore, choosing the right model architecture is also a critical step in the training process. Further heuristics and techniques are discussed in the AI Training section.</p>
<p>Frameworks also efficiently implement gradient descent, Adagrad, Adadelta, and Adam. Adding regularization, such as dropout and L1/L2 penalties, prevents Overfitting during training. Batch normalization accelerates training by normalizing inputs to layers.</p>
</section>
<section id="model-training-support" class="level3" data-number="6.4.6">
<h3 data-number="6.4.6" class="anchored" data-anchor-id="model-training-support"><span class="header-section-number">6.4.6</span> Model Training Support</h3>
<p>A compilation step is required before training a defined neural network model. During this step, the neural network’s high-level architecture is transformed into an optimized, executable format. This process comprises several steps. The first step is to construct the computational graph, which represents all the mathematical operations and data flow within the model. We discussed this earlier.</p>
<p>During training, the focus is on executing the computational graph. Every parameter within the graph, such as weights and biases, is assigned an initial value. Depending on the chosen initialization method, this value might be random or based on a predefined logic.</p>
<p>The next critical step is memory allocation. Essential memory is reserved for the model’s operations on both CPUs and GPUs, ensuring efficient data processing. The model’s operations are then mapped to the available hardware resources, particularly GPUs or TPUs, to expedite computation. Once the compilation is finalized, the model is prepared for training.</p>
<p>The training process employs various tools to enhance efficiency. Batch processing is commonly used to maximize computational throughput. Techniques like vectorization enable operations on entire data arrays rather than proceeding element-wise, which bolsters speed. Optimizations such as kernel fusion (refer to the Optimizations chapter) amalgamate multiple operations into a single action, minimizing computational overhead. Operations can also be segmented into phases, facilitating the concurrent processing of different mini-batches at various stages.</p>
<p>Frameworks consistently checkpoint the state, preserving intermediate model versions during training. This ensures that progress is recovered if an interruption occurs, and training can be recommenced from the last checkpoint. Additionally, the system vigilantly monitors the model’s performance against a validation data set. Should the model begin to overfit (if its performance on the validation set declines), training is automatically halted, conserving computational resources and time.</p>
<p>ML frameworks incorporate a blend of model compilation, enhanced batch processing methods, and utilities such as checkpointing and early stopping. These resources manage the complex aspects of performance, enabling practitioners to zero in on model development and training. As a result, developers experience both speed and ease when utilizing neural networks’ capabilities.</p>
</section>
<section id="validation-and-analysis" class="level3" data-number="6.4.7">
<h3 data-number="6.4.7" class="anchored" data-anchor-id="validation-and-analysis"><span class="header-section-number">6.4.7</span> Validation and Analysis</h3>
<p>After training deep learning models, frameworks provide utilities to evaluate performance and gain insights into the models’ workings. These tools enable disciplined experimentation and debugging.</p>
<section id="evaluation-metrics" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h4>
<p>Frameworks include implementations of common evaluation metrics for validation:</p>
<ul>
<li><p>Accuracy - Fraction of correct predictions overall. They are widely used for classification.</p></li>
<li><p>Precision - Of positive predictions, how many were positive. Useful for imbalanced datasets.</p></li>
<li><p>Recall - Of actual positives, how many did we predict correctly? Measures completeness.</p></li>
<li><p>F1-score - Harmonic mean of precision and recall. Combines both metrics.</p></li>
<li><p>AUC-ROC - Area under ROC curve. They are used for classification threshold analysis.</p></li>
<li><p>MAP - Mean Average Precision. Evaluate ranked predictions in retrieval/detection.</p></li>
<li><p>Confusion Matrix - Matrix that shows the true positives, true negatives, false positives, and false negatives. Provides a more detailed view of classification performance.</p></li>
</ul>
<p>These metrics quantify model performance on validation data for comparison.</p>
</section>
<section id="visualization" class="level4">
<h4 class="anchored" data-anchor-id="visualization">Visualization</h4>
<p>Visualization tools provide insight into models:</p>
<ul>
<li><p>Loss curves - Plot training and validation loss over time to spot Overfitting.</p></li>
<li><p>Activation grids - Illustrate features learned by convolutional filters.</p></li>
<li><p>Projection - Reduce dimensionality for intuitive visualization.</p></li>
<li><p>Precision-recall curves - Assess classification tradeoffs.</p></li>
</ul>
<p>Tools like <a href="https://www.tensorflow.org/tensorboard/scalars_and_keras">TensorBoard</a> for TensorFlow and <a href="https://github.com/microsoft/tensorwatch">TensorWatch</a> for PyTorch enable real-time metrics and visualization during training.</p>
</section>
</section>
<section id="differentiable-programming" class="level3" data-number="6.4.8">
<h3 data-number="6.4.8" class="anchored" data-anchor-id="differentiable-programming"><span class="header-section-number">6.4.8</span> Differentiable programming</h3>
<p>Machine learning training methods such as backpropagation rely on the change in the loss function with respect to the change in weights (which essentially is the definition of derivatives). Thus, the ability to quickly and efficiently train large machine learning models relies on the computer’s ability to take derivatives. This makes differentiable programming one of the most important elements of a machine learning framework.</p>
<p>We can use four primary methods to make computers take derivatives. First, we can manually figure out the derivatives by hand and input them into the computer. This would quickly become a nightmare with many layers of neural networks if we had to compute all the derivatives in the backpropagation steps by hand. Another method is symbolic differentiation using computer algebra systems such as Mathematica, which can introduce a layer of inefficiency, as there needs to be a level of abstraction to take derivatives. Numerical derivatives, the practice of approximating gradients using finite difference methods, suffer from many problems, including high computational costs and larger grid sizes, leading to many errors. This leads to automatic differentiation, which exploits the primitive functions that computers use to represent operations to obtain an exact derivative. With automatic differentiation, the computational complexity of computing the gradient is proportional to computing the function itself. Intricacies of automatic differentiation are not dealt with by end users now, but resources to learn more can be found widely, such as from <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf">here</a>. Today’s automatic differentiation and differentiable programming are ubiquitous and are done efficiently and automatically by modern machine learning frameworks.</p>
</section>
<section id="hardware-acceleration" class="level3 page-columns page-full" data-number="6.4.9">
<h3 data-number="6.4.9" class="anchored" data-anchor-id="hardware-acceleration"><span class="header-section-number">6.4.9</span> Hardware Acceleration</h3>
<p>The trend to continuously train and deploy larger machine-learning models has made hardware acceleration support necessary for machine-learning platforms. <a href="#fig-hardware-accelerator" class="quarto-xref">Figure&nbsp;<span>6.5</span></a> shows the large number of companies that are offering hardware accelerators in different domains, such as “Very Low Power” and “Embedded” machine learning. Deep layers of neural networks require many matrix multiplications, which attract hardware that can compute matrix operations quickly and in parallel. In this landscape, two hardware architectures, the <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">GPU and TPU</a>, have emerged as leading choices for training machine learning models.</p>
<p>The use of hardware accelerators began with <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a>, which paved the way for future works to utilize GPUs as hardware accelerators for training computer vision models. GPUs, or Graphics Processing Units, excel in handling many computations at once, making them ideal for the matrix operations central to neural network training. Their architecture, designed for rendering graphics, is perfect for the mathematical operations required in machine learning. While they are very useful for machine learning tasks and have been implemented in many hardware platforms, GPUs are still general purpose in that they can be used for other applications.</p>
<p>On the other hand, <a href="https://cloud.google.com/tpu/docs/intro-to-tpu">Tensor Processing Units</a> (TPU) are hardware units designed specifically for neural networks. They focus on the multiply and accumulate (MAC) operation, and their hardware consists of a large hardware matrix that contains elements that efficiently compute the MAC operation. This concept, called the <a href="https://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf">systolic array architecture</a>, was pioneered by <span class="citation" data-cites="kung1979systolic">Kung and Leiserson (<a href="../../references.html#ref-kung1979systolic" role="doc-biblioref">1979</a>)</span>, but has proven to be a useful structure to efficiently compute matrix products and other operations within neural networks (such as convolutions).</p>
<div class="no-row-height column-margin column-container"><div id="ref-kung1979systolic" class="csl-entry" role="listitem">
Kung, Hsiang Tsung, and Charles E Leiserson. 1979. <span>“Systolic Arrays (for <span>VLSI)</span>.”</span> In <em>Sparse Matrix Proceedings 1978</em>, 1:256–82. Society for industrial; applied mathematics Philadelphia, PA, USA.
</div></div><p>While TPUs can drastically reduce training times, they also have disadvantages. For example, many operations within the machine learning frameworks (primarily TensorFlow here since the TPU directly integrates with it) are not supported by TPUs. They cannot also support custom operations from the machine learning frameworks, and the network design must closely align with the hardware capabilities.</p>
<p>Today, NVIDIA GPUs dominate training, aided by software libraries like <a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a>, <a href="https://developer.nvidia.com/cudnn">cuDNN</a>, and <a href="https://developer.nvidia.com/tensorrt#:~:text=NVIDIA%20TensorRT-LLM%20is%20an,knowledge%20of%20C++%20or%20CUDA.">TensorRT.</a> Frameworks also include optimizations to maximize performance on these hardware types, like pruning unimportant connections and fusing layers. Combining these techniques with hardware acceleration provides greater efficiency. For inference, hardware is increasingly moving towards optimized ASICs and SoCs. Google’s TPUs accelerate models in data centers. Apple, Qualcomm, and others now produce AI-focused mobile chips. The NVIDIA Jetson family targets autonomous robots.</p>
<div id="fig-hardware-accelerator" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hardware-accelerator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hardware_accelerator.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-accelerator-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.5: Companies offering ML hardware accelerators. Credit: <a href="https://gradientflow.com/one-simple-chart-companies-that-offer-deep-neural-network-accelerators/">Gradient Flow.</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ai_frameworks-advanced" class="level2 page-columns page-full" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="sec-ai_frameworks-advanced"><span class="header-section-number">6.5</span> Advanced Features</h2>
<section id="distributed-training" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="distributed-training"><span class="header-section-number">6.5.1</span> Distributed training</h3>
<p>As machine learning models have become larger over the years, it has become essential for large models to utilize multiple computing nodes in the training process. This process, distributed learning, has allowed for higher training capabilities but has also imposed challenges in implementation.</p>
<p>We can consider three different ways to spread the work of training machine learning models to multiple computing nodes. Input data partitioning refers to multiple processors running the same model on different input partitions. This is the easiest implementation and is available for many machine learning frameworks. The more challenging distribution of work comes with model parallelism, which refers to multiple computing nodes working on different parts of the model, and pipelined model parallelism, which refers to multiple computing nodes working on different layers of the model on the same input. The latter two mentioned here are active research areas.</p>
<p>ML frameworks that support distributed learning include TensorFlow (through its <a href="https://www.tensorflow.org/api_docs/python/tf/distribute">tf.distribute</a> module), PyTorch (through its <a href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html">torch.nn.DataParallel</a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">torch.nn.DistributedDataParallel</a> modules), and MXNet (through its <a href="https://mxnet.apache.org/versions/1.9.1/api/python/docs/api/gluon/index.html">gluon</a> API).</p>
</section>
<section id="model-conversion" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="model-conversion"><span class="header-section-number">6.5.2</span> Model Conversion</h3>
<p>Machine learning models have various methods to be represented and used within different frameworks and for different device types. For example, a model can be converted to be compatible with inference frameworks within the mobile device. The default format for TensorFlow models is checkpoint files containing weights and architectures, which are needed to retrain the models. However, models are typically converted to TensorFlow Lite format for mobile deployment. TensorFlow Lite uses a compact flat buffer representation and optimizations for fast inference on mobile hardware, discarding all the unnecessary baggage associated with training metadata, such as checkpoint file structures.</p>
<p>The default format for TensorFlow models is checkpoint files containing weights and architectures. For mobile deployment, models are typically converted to TensorFlow Lite format. TensorFlow Lite uses a compact flat buffer representation and optimizations for fast inference on mobile hardware.</p>
<p>Model optimizations like quantization (see <a href="../../contents/optimizations/optimizations.html">Optimizations</a> chapter) can further optimize models for target architectures like mobile. This reduces the precision of weights and activations to <code>uint8</code> or <code>int8</code> for a smaller footprint and faster execution with supported hardware accelerators. For post-training quantization, TensorFlow’s converter handles analysis and conversion automatically.</p>
<p>Frameworks like TensorFlow simplify deploying trained models to mobile and embedded IoT devices through easy conversion APIs for TFLite format and quantization. Ready-to-use conversion enables high-performance inference on mobile without a manual optimization burden. Besides TFLite, other common targets include TensorFlow.js for web deployment, TensorFlow Serving for cloud services, and TensorFlow Hub for transfer learning. TensorFlow’s conversion utilities handle these scenarios to streamline end-to-end workflows.</p>
<p>More information about model conversion in TensorFlow is linked <a href="https://www.tensorflow.org/lite/models/convert">here</a>.</p>
</section>
<section id="automl-no-codelow-code-ml" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="automl-no-codelow-code-ml"><span class="header-section-number">6.5.3</span> AutoML, No-Code/Low-Code ML</h3>
<p>In many cases, machine learning can have a relatively high barrier of entry compared to other fields. To successfully train and deploy models, one needs to have a critical understanding of a variety of disciplines, from data science (data processing, data cleaning), model structures (hyperparameter tuning, neural network architecture), hardware (acceleration, parallel processing), and more depending on the problem at hand. The complexity of these problems has led to the introduction of frameworks such as AutoML, which aims to make “Machine learning available for non-Machine Learning exports” and to “automate research in machine learning.” They have constructed AutoWEKA, which aids in the complex process of hyperparameter selection, and Auto-sklearn and Auto-pytorch, an extension of AutoWEKA into the popular sklearn and PyTorch Libraries.</p>
<p>While these efforts to automate parts of machine learning tasks are underway, others have focused on making machine learning models easier by deploying no-code/low-code machine learning, utilizing a drag-and-drop interface with an easy-to-navigate user interface. Companies such as Apple, Google, and Amazon have already created these easy-to-use platforms to allow users to construct machine learning models that can integrate into their ecosystem.</p>
<p>These steps to remove barriers to entry continue to democratize machine learning, make it easier for beginners to access, and simplify workflow for experts.</p>
</section>
<section id="advanced-learning-methods" class="level3 page-columns page-full" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="advanced-learning-methods"><span class="header-section-number">6.5.4</span> Advanced Learning Methods</h3>
<section id="transfer-learning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h4>
<p>Transfer learning is the practice of using knowledge gained from a pre-trained model to train and improve the performance of a model for a different task. For example, datasets trained on ImageNet datasets such as MobileNet and ResNet can help classify other image datasets. To do so, one may freeze the pre-trained model, utilizing it as a feature extractor to train a much smaller model built on top of the feature extraction. One can also fine-tune the entire model to fit the new task.</p>
<p>Transfer learning has challenges, such as the modified model’s inability to conduct its original tasks after transfer learning. Papers such as <a href="https://browse.arxiv.org/pdf/1606.09282.pdf">“Learning without Forgetting”</a> by <span class="citation" data-cites="li2017learning">Z. Li and Hoiem (<a href="../../references.html#ref-li2017learning" role="doc-biblioref">2018</a>)</span> aims to address these challenges and have been implemented in modern machine learning platforms.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017learning" class="csl-entry" role="listitem">
Li, Zhizhong, and Derek Hoiem. 2018. <span>“Learning Without Forgetting.”</span> <em>IEEE Trans. Pattern Anal. Mach. Intell.</em> 40 (12): 2935–47. <a href="https://doi.org/10.1109/tpami.2017.2773081">https://doi.org/10.1109/tpami.2017.2773081</a>.
</div></div></section>
<section id="federated-learning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="federated-learning">Federated Learning</h4>
<p>Consider the problem of labeling items in a photo from personal devices and moving the image data from the devices to a central server, where a single model will train Using the image data provided by the devices. However, this presents many potential challenges. First, with many devices, one needs a massive network infrastructure to move and store data from these devices to a central location. With the number of devices present today, this is often not feasible and very costly. Furthermore, privacy challenges like those of Photos central servers are associated with moving personal data.</p>
<p>Federated learning by <span class="citation" data-cites="mcmahan2023communicationefficient">McMahan et al. (<a href="../../references.html#ref-mcmahan2023communicationefficient" role="doc-biblioref">2017</a>)</span> is a form of distributed computing that resolves these issues by distributing the models into personal devices for them to be trained on devices (<a href="#fig-federated-learning" class="quarto-xref">Figure&nbsp;<span>6.6</span></a>). Initially, a base global model is trained on a central server to be distributed to all devices. Using this base model, the devices individually compute the gradients and send them back to the central hub. Intuitively, this transfers model parameters instead of the data itself. This innovative approach allows the model to be trained with many different datasets (in our example, the set of images on personal devices) without transferring a large amount of potentially sensitive data. However, federated learning also comes with a series of challenges.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mcmahan2023communicationefficient" class="csl-entry" role="listitem">
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. 2017. <span>“Communication-Efficient Learning of Deep Networks from Decentralized Data.”</span> In <em>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA</em>, edited by Aarti Singh and Xiaojin (Jerry) Zhu, 54:1273–82. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v54/mcmahan17a.html">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.
</div></div><p>Data collected from devices may come with something other than suitable labels in many real-world situations. Users compound this issue; the primary data source can often be unreliable. This unreliability means that even when data is labeled, its accuracy or relevance is not guaranteed. Furthermore, each user’s data is unique, resulting in a significant variance in the data generated by different users. This non-IID nature of data, coupled with the unbalanced data production where some users generate more data than others, can adversely impact the performance of the global model. Researchers have worked to compensate for this by adding a proximal term to balance the local and global model and adding a frozen <a href="https://arxiv.org/abs/2207.09413">global hypersphere classifier</a>.</p>
<p>Additional challenges are associated with federated learning. The number of mobile device owners can far exceed the average number of training samples on each device, leading to substantial communication overhead. This issue is particularly pronounced in the context of mobile networks, which are often used for such communication and can be unstable. This instability can result in delayed or failed transmission of model updates, thereby affecting the overall training process.</p>
<p>The heterogeneity of device resources is another hurdle. Devices participating in Federated Learning can have varying computational powers and memory capacities. This diversity makes it challenging to design efficient algorithms across all devices. Privacy and security issues are not a guarantee for federated learning. Techniques such as inversion gradient attacks can extract information about the training data from the model parameters. Despite these challenges, the many potential benefits continue to make it a popular research area. Open source programs such as <a href="https://flower.dev/">Flower</a> have been developed to simplify implementing federated learning with various machine learning frameworks.</p>
<p><a href="#fig-federated-learning" class="quarto-xref">Figure&nbsp;<span>6.6</span></a> illustrates an example of federated learning. Consider a model used for medical predictions by diffrent hospitals. Given that medical data is extremely sensitive and must be kept private, it can’t be transferred to a centralized server for training. Instead, each hospital would firen-tune/train the base model using its own private data, while only communicating non-sensitive information with the Federated Server, such as the learned parameters.</p>
<div id="fig-federated-learning" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-federated-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/federated_learning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-federated-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.6: A centralized-server approach to federated learning. Credit: <a href="https://blogs.nvidia.com/blog/what-is-federated-learning/">NVIDIA.</a>
</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="framework-specialization" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="framework-specialization"><span class="header-section-number">6.6</span> Framework Specialization</h2>
<p>Thus far, we have talked about ML frameworks generally. However, typically, frameworks are optimized based on the target environment’s computational capabilities and application requirements, ranging from the cloud to the edge to tiny devices. Choosing the right framework is crucial based on the target environment for deployment. This section provides an overview of the major types of AI frameworks tailored for cloud, edge, and TinyML environments to help understand the similarities and differences between these ecosystems.</p>
<section id="cloud" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="cloud"><span class="header-section-number">6.6.1</span> Cloud</h3>
<p>Cloud-based AI frameworks assume access to ample computational power, memory, and storage resources in the cloud. They generally support both training and inference. Cloud-based AI frameworks are suited for applications where data can be sent to the cloud for processing, such as cloud-based AI services, large-scale data analytics, and web applications. Popular cloud AI frameworks include the ones we mentioned earlier, such as TensorFlow, PyTorch, MXNet, Keras, etc. These frameworks utilize GPUs, TPUs, distributed training, and AutoML to deliver scalable AI. Concepts like model serving, MLOps, and AIOps relate to the operationalization of AI in the cloud. Cloud AI powers services like Google Cloud AI and enables transfer learning using pre-trained models.</p>
</section>
<section id="edge" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="edge"><span class="header-section-number">6.6.2</span> Edge</h3>
<p>Edge AI frameworks are tailored to deploy AI models on IoT devices, smartphones, and edge servers. Edge AI frameworks are optimized for devices with moderate computational resources, balancing power and performance. Edge AI frameworks are ideal for applications requiring real-time or near-real-time processing, including robotics, autonomous vehicles, and smart devices. Key edge AI frameworks include TensorFlow Lite, PyTorch Mobile, CoreML, and others. They employ optimizations like model compression, quantization, and efficient neural network architectures. Hardware support includes CPUs, GPUs, NPUs, and accelerators like the Edge TPU. Edge AI enables use cases like mobile vision, speech recognition, and real-time anomaly detection.</p>
</section>
<section id="embedded" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="embedded"><span class="header-section-number">6.6.3</span> Embedded</h3>
<p>TinyML frameworks are specialized for deploying AI models on extremely resource-constrained devices, specifically microcontrollers and sensors within the IoT ecosystem. TinyML frameworks are designed for devices with limited resources, emphasizing minimal memory and power consumption. TinyML frameworks are specialized for use cases on resource-constrained IoT devices for predictive maintenance, gesture recognition, and environmental monitoring applications. Major TinyML frameworks include TensorFlow Lite Micro, uTensor, and ARM NN. They optimize complex models to fit within kilobytes of memory through techniques like quantization-aware training and reduced precision. TinyML allows intelligent sensing across battery-powered devices, enabling collaborative learning via federated learning. The choice of framework involves balancing model performance and computational constraints of the target platform, whether cloud, edge, or TinyML. <a href="#tbl-ml_frameworks" class="quarto-xref">Table&nbsp;<span>6.2</span></a> compares the major AI frameworks across cloud, edge, and TinyML environments:</p>
<div id="tbl-ml_frameworks" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ml_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.2: Comparison of framework types for Cloud AI, Edge AI, and TinyML.
</figcaption>
<div aria-describedby="tbl-ml_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 8%">
<col style="width: 19%">
<col style="width: 41%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Framework Type</th>
<th>Examples</th>
<th>Key Technologies</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cloud AI</td>
<td>TensorFlow, PyTorch, MXNet, Keras</td>
<td>GPUs, TPUs, distributed training, AutoML, MLOps</td>
<td>Cloud services, web apps, big data analytics</td>
</tr>
<tr class="even">
<td>Edge AI</td>
<td>TensorFlow Lite, PyTorch Mobile, Core ML</td>
<td>Model optimization, compression, quantization, efficient NN architectures</td>
<td>Mobile apps, robots, autonomous systems, real-time processing</td>
</tr>
<tr class="odd">
<td>TinyML</td>
<td>TensorFlow Lite Micro, uTensor, ARM NN</td>
<td>Quantization-aware training, reduced precision, neural architecture search</td>
<td>IoT sensors, wearables, predictive maintenance, gesture recognition</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>Key differences:</strong></p>
<ul>
<li><p>Cloud AI leverages massive computational power for complex models using GPUs/TPUs and distributed training</p></li>
<li><p>Edge AI optimizes models to run locally on resource-constrained edge devices.</p></li>
<li><p>TinyML fits models into extremely low memory and computes environments like microcontrollers</p></li>
</ul>
</section>
</section>
<section id="sec-ai_frameworks_embedded" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="sec-ai_frameworks_embedded"><span class="header-section-number">6.7</span> Embedded AI Frameworks</h2>
<section id="resource-constraints" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="resource-constraints"><span class="header-section-number">6.7.1</span> Resource Constraints</h3>
<p>Embedded systems face severe resource constraints that pose unique challenges when deploying machine learning models compared to traditional computing platforms. For example, microcontroller units (MCUs) commonly used in IoT devices often have:</p>
<ul>
<li><p><strong>RAM</strong> ranges from tens of kilobytes to a few megabytes. The popular <a href="https://www.espressif.com/en/products/socs/esp8266">ESP8266 MCU</a> has around 80KB RAM available to developers. This contrasts with 8GB or more on typical laptops and desktops today.</p></li>
<li><p><strong>Flash storage</strong> ranges from hundreds of kilobytes to a few megabytes. The Arduino Uno microcontroller provides just 32KB of code storage. Standard computers today have disk storage in the order of terabytes.</p></li>
<li><p><strong>Processing power</strong> from just a few MHz to approximately 200MHz. The ESP8266 operates at 80MHz. This is several orders of magnitude slower than multi-GHz multi-core CPUs in servers and high-end laptops.</p></li>
</ul>
<p>These tight constraints often make training machine learning models directly on microcontrollers infeasible. The limited RAM precludes handling large datasets for training. Energy usage for training would also quickly deplete battery-powered devices. Instead, models are trained on resource-rich systems and deployed on microcontrollers for optimized inference. But even inference poses challenges:</p>
<ol type="1">
<li><p><strong>Model Size:</strong> AI models are too large to fit on embedded and IoT devices. This necessitates model compression techniques, such as quantization, pruning, and knowledge distillation. Additionally, as we will see, many of the frameworks used by developers for AI development have large amounts of overhead and built-in libraries that embedded systems can’t support.</p></li>
<li><p><strong>Complexity of Tasks:</strong> With only tens of KBs to a few MBs of RAM, IoT devices and embedded systems are constrained in the complexity of tasks they can handle. Tasks that require large datasets or sophisticated algorithms—for example, LLMs—that would run smoothly on traditional computing platforms might be infeasible on embedded systems without compression or other optimization techniques due to memory limitations.</p></li>
<li><p><strong>Data Storage and Processing:</strong> Embedded systems often process data in real time and might only store small amounts locally. Conversely, traditional computing systems can hold and process large datasets in memory, enabling faster data operations analysis and real-time updates.</p></li>
<li><p><strong>Security and Privacy:</strong> Limited memory also restricts the complexity of security algorithms and protocols, data encryption, reverse engineering protections, and more that can be implemented on the device. This could make some IoT devices more vulnerable to attacks.</p></li>
</ol>
<p>Consequently, specialized software optimizations and ML frameworks tailored for microcontrollers must work within these tight resource bounds. Clever optimization techniques like quantization, pruning, and knowledge distillation compress models to fit within limited memory (see Optimizations section). Learnings from neural architecture search help guide model designs.</p>
<p>Hardware improvements like dedicated ML accelerators on microcontrollers also help alleviate constraints. For instance, <a href="https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor">Qualcomm’s Hexagon DSP</a> accelerates TensorFlow Lite models on Snapdragon mobile chips. <a href="https://cloud.google.com/edge-tpu">Google’s Edge TPU</a> packs ML performance into a tiny ASIC for edge devices. <a href="https://www.arm.com/products/silicon-ip-cpu/ethos/ethos-u55">ARM Ethos-U55</a> offers efficient inference on Cortex-M class microcontrollers. These customized ML chips unlock advanced capabilities for resource-constrained applications.</p>
<p>Due to limited processing power, it’s almost always infeasible to train AI models on IoT or embedded systems. Instead, models are trained on powerful traditional computers (often with GPUs) and then deployed on the embedded device for inference. TinyML specifically deals with this, ensuring models are lightweight enough for real-time inference on these constrained devices.</p>
</section>
<section id="frameworks-libraries" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="frameworks-libraries"><span class="header-section-number">6.7.2</span> Frameworks &amp; Libraries</h3>
<p>Embedded AI frameworks are software tools and libraries designed to enable AI and ML capabilities on embedded systems. These frameworks are essential for bringing AI to IoT devices, robotics, and other edge computing platforms, and they are designed to work where computational resources, memory, and power consumption are limited.</p>
</section>
<section id="challenges" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="challenges"><span class="header-section-number">6.7.3</span> Challenges</h3>
<p>While embedded systems present an enormous opportunity for deploying machine learning to enable intelligent capabilities at the edge, these resource-constrained environments pose significant challenges. Unlike typical cloud or desktop environments rich with computational resources, embedded devices introduce severe constraints around memory, processing power, energy efficiency, and specialized hardware. As a result, existing machine learning techniques and frameworks designed for server clusters with abundant resources do not directly translate to embedded systems. This section uncovers some of the challenges and opportunities for embedded systems and ML frameworks.</p>
<section id="fragmented-ecosystem" class="level4">
<h4 class="anchored" data-anchor-id="fragmented-ecosystem">Fragmented Ecosystem</h4>
<p>The lack of a unified ML framework led to a highly fragmented ecosystem. Engineers at companies like <a href="https://www.st.com/">STMicroelectronics</a>, <a href="https://www.nxp.com/">NXP Semiconductors</a>, and <a href="https://www.renesas.com/">Renesas</a> had to develop custom solutions tailored to their specific microcontroller and DSP architectures. These ad-hoc frameworks required extensive manual optimization for each low-level hardware platform. This made porting models extremely difficult, requiring redevelopment for new Arm, RISC-V, or proprietary architectures.</p>
</section>
<section id="disparate-hardware-needs" class="level4">
<h4 class="anchored" data-anchor-id="disparate-hardware-needs">Disparate Hardware Needs</h4>
<p>Without a shared framework, there was no standard way to assess hardware’s capabilities. Vendors like Intel, Qualcomm, and NVIDIA created integrated solutions, blending models and improving software and hardware. This made it hard to discern the sources of performance gains - whether new chip designs like Intel’s low-power x86 cores or software optimizations were responsible. A standard framework was needed so vendors could evaluate their hardware’s capabilities fairly and reproducibly.</p>
</section>
<section id="lack-of-portability" class="level4">
<h4 class="anchored" data-anchor-id="lack-of-portability">Lack of Portability</h4>
<p>With standardized tools, adapting models trained in common frameworks like TensorFlow or PyTorch to run efficiently on microcontrollers was easier. It required time-consuming manual translation of models to run on specialized DSPs from companies like CEVA or low-power Arm M-series cores. No turnkey tools were enabling portable deployment across different architectures.</p>
</section>
<section id="incomplete-infrastructure" class="level4">
<h4 class="anchored" data-anchor-id="incomplete-infrastructure">Incomplete Infrastructure</h4>
<p>The infrastructure to support key model development workflows needed to be improved. More support is needed for compression techniques to fit large models within constrained memory budgets. Tools for quantization to lower precision for faster inference were missing. Standardized APIs for integration into applications were incomplete. Essential functionality like on-device debugging, metrics, and performance profiling was absent. These gaps increased the cost and difficulty of embedded ML development.</p>
</section>
<section id="no-standard-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="no-standard-benchmark">No Standard Benchmark</h4>
<p>Without unified benchmarks, there was no standard way to assess and compare the capabilities of different hardware platforms from vendors like NVIDIA, Arm, and Ambiq Micro. Existing evaluations relied on proprietary benchmarks tailored to showcase the strengths of particular chips. This made it impossible to measure hardware improvements objectively in a fair, neutral manner. The <a href="../benchmarking/benchmarking. cmd">Benchmarking AI</a> chapter discusses this topic in more detail.</p>
</section>
<section id="minimal-real-world-testing" class="level4">
<h4 class="anchored" data-anchor-id="minimal-real-world-testing">Minimal Real-World Testing</h4>
<p>Much of the benchmarks relied on synthetic data. Rigorously testing models on real-world embedded applications was difficult without standardized datasets and benchmarks, raising questions about how performance claims would translate to real-world usage. More extensive testing was needed to validate chips in actual use cases.</p>
<p>The lack of shared frameworks and infrastructure slowed TinyML adoption, hampering the integration of ML into embedded products. Recent standardized frameworks have begun addressing these issues through improved portability, performance profiling, and benchmarking support. However, ongoing innovation is still needed to enable seamless, cost-effective deployment of AI to edge devices.</p>
</section>
<section id="summary" class="level4">
<h4 class="anchored" data-anchor-id="summary">Summary</h4>
<p>The absence of standardized frameworks, benchmarks, and infrastructure for embedded ML has traditionally hampered adoption. However, recent progress has been made in developing shared frameworks like TensorFlow Lite Micro and benchmark suites like MLPerf Tiny that aim to accelerate the proliferation of TinyML solutions. However, overcoming the fragmentation and difficulty of embedded deployment remains an ongoing process.</p>
</section>
</section>
</section>
<section id="examples" class="level2 page-columns page-full" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="examples"><span class="header-section-number">6.8</span> Examples</h2>
<p>Machine learning deployment on microcontrollers and other embedded devices often requires specially optimized software libraries and frameworks to work within tight memory, compute, and power constraints. Several options exist for performing inference on such resource-limited hardware, each with its approach to optimizing model execution. This section will explore the key characteristics and design principles behind TFLite Micro, TinyEngine, and CMSIS-NN, providing insight into how each framework tackles the complex problem of high-accuracy yet efficient neural network execution on microcontrollers. It will also showcase different approaches for implementing efficient TinyML frameworks.</p>
<p><a href="#tbl-compare_frameworks" class="quarto-xref">Table&nbsp;<span>6.3</span></a> summarizes the key differences and similarities between these three specialized machine-learning inference frameworks for embedded systems and microcontrollers.</p>
<div id="tbl-compare_frameworks" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-compare_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.3: Comparison of frameworks: TensorFlow Lite Micro, TinyEngine, and CMSIS-NN
</figcaption>
<div aria-describedby="tbl-compare_frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 17%">
<col style="width: 22%">
<col style="width: 29%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Framework</th>
<th style="text-align: center;">TensorFlow Lite Micro</th>
<th style="text-align: center;">TinyEngine</th>
<th style="text-align: center;">CMSIS-NN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Approach</strong></td>
<td style="text-align: center;">Interpreter-based</td>
<td style="text-align: center;">Static compilation</td>
<td style="text-align: center;">Optimized neural network kernels</td>
</tr>
<tr class="even">
<td><strong>Hardware Focus</strong></td>
<td style="text-align: center;">General embedded devices</td>
<td style="text-align: center;">Microcontrollers</td>
<td style="text-align: center;">ARM Cortex-M processors</td>
</tr>
<tr class="odd">
<td><strong>Arithmetic Support</strong></td>
<td style="text-align: center;">Floating point</td>
<td style="text-align: center;">Floating point, fixed point</td>
<td style="text-align: center;">Floating point, fixed point</td>
</tr>
<tr class="even">
<td><strong>Model Support</strong></td>
<td style="text-align: center;">General neural network models</td>
<td style="text-align: center;">Models co-designed with TinyNAS</td>
<td style="text-align: center;">Common neural network layer types</td>
</tr>
<tr class="odd">
<td><strong>Code Footprint</strong></td>
<td style="text-align: center;">Larger due to inclusion of interpreter and ops</td>
<td style="text-align: center;">Small, includes only ops needed for model</td>
<td style="text-align: center;">Lightweight by design</td>
</tr>
<tr class="even">
<td><strong>Latency</strong></td>
<td style="text-align: center;">Higher due to interpretation overhead</td>
<td style="text-align: center;">Very low due to compiled model</td>
<td style="text-align: center;">Low latency focus</td>
</tr>
<tr class="odd">
<td><strong>Memory Management</strong></td>
<td style="text-align: center;">Dynamically managed by interpreter</td>
<td style="text-align: center;">Model-level optimization</td>
<td style="text-align: center;">Tools for efficient allocation</td>
</tr>
<tr class="even">
<td><strong>Optimization Approach</strong></td>
<td style="text-align: center;">Some code generation features</td>
<td style="text-align: center;">Specialized kernels, operator fusion</td>
<td style="text-align: center;">Architecture-specific assembly optimizations</td>
</tr>
<tr class="odd">
<td><strong>Key Benefits</strong></td>
<td style="text-align: center;">Flexibility, portability, ease of updating models</td>
<td style="text-align: center;">Maximizes performance, optimized memory usage</td>
<td style="text-align: center;">Hardware acceleration, standardized API, portability</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>We will understand each of these in greater detail in the following sections.</p>
<section id="interpreter" class="level3 page-columns page-full" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="interpreter"><span class="header-section-number">6.8.1</span> Interpreter</h3>
<p><a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite Micro (TFLM)</a> is a machine learning inference framework designed for embedded devices with limited resources. It uses an interpreter to load and execute machine learning models, which provides flexibility and ease of updating models in the field <span class="citation" data-cites="david2021tensorflow">(<a href="../../references.html#ref-david2021tensorflow" role="doc-biblioref">David et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-david2021tensorflow" class="csl-entry" role="listitem">
David, Robert, Jared Duke, Advait Jain, Vijay Janapa Reddi, Nat Jeffries, Jian Li, Nick Kreeger, et al. 2021. <span>“Tensorflow Lite Micro: <span>Embedded</span> Machine Learning for Tinyml Systems.”</span> <em>Proceedings of Machine Learning and Systems</em> 3: 800–811.
</div></div><p>Traditional interpreters often have significant branching overhead, which can reduce performance. However, machine learning model interpretation benefits from the efficiency of long-running kernels, where each kernel runtime is relatively large and helps mitigate interpreter overhead.</p>
<p>An alternative to an interpreter-based inference engine is to generate native code from a model during export. This can improve performance, but it sacrifices portability and flexibility, as the generated code needs recompilation for each target platform and must be replaced entirely to modify a model.</p>
<p>TFLM balances the simplicity of code compilation and the flexibility of an interpreter-based approach by incorporating certain code-generation features. For example, the library can be constructed solely from source files, offering much of the compilation simplicity associated with code generation while retaining the benefits of an interpreter-based model execution framework.</p>
<p>An interpreter-based approach offers several benefits over code generation for machine learning inference on embedded devices:</p>
<ul>
<li><p><strong>Flexibility:</strong> Models can be updated in the field without recompiling the entire application.</p></li>
<li><p><strong>Portability:</strong> The interpreter can be used to execute models on different target platforms without porting the code.</p></li>
<li><p><strong>Memory efficiency:</strong> The interpreter can share code across multiple models, reducing memory usage.</p></li>
<li><p><strong>Ease of development:</strong> Interpreters are easier to develop and maintain than code generators.</p></li>
</ul>
<p>TensorFlow Lite Micro is a powerful and flexible framework for machine learning inference on embedded devices. Its interpreter-based approach offers several benefits over code generation, including flexibility, portability, memory efficiency, and ease of development.</p>
</section>
<section id="compiler-based" class="level3 page-columns page-full" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="compiler-based"><span class="header-section-number">6.8.2</span> Compiler-based</h3>
<p><a href="https://github.com/mit-han-lab/tinyengine">TinyEngine</a> is an ML inference framework designed specifically for resource-constrained microcontrollers. It employs several optimizations to enable high-accuracy neural network execution within the tight constraints of memory, computing, and storage on microcontrollers <span class="citation" data-cites="lin2020mcunet">(<a href="../../references.html#ref-lin2020mcunet" role="doc-biblioref">Lin et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2020mcunet" class="csl-entry" role="listitem">
Lin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. 2020. <span>“<span>MCUNet:</span> <span>Tiny</span> Deep Learning on <span>IoT</span> Devices.”</span> In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual</em>, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html</a>.
</div></div><p>While inference frameworks like TFLite Micro use interpreters to execute the neural network graph dynamically at runtime, this adds significant overhead regarding memory usage to store metadata, interpretation latency, and lack of optimizations. However, TFLite argues that the overhead is small. TinyEngine eliminates this overhead by employing a code generation approach. It analyzes the network graph during compilation and generates specialized code to execute just that model. This code is natively compiled into the application binary, avoiding runtime interpretation costs.</p>
<p>Conventional ML frameworks schedule memory per layer, trying to minimize usage for each layer separately. TinyEngine does model-level scheduling instead of analyzing memory usage across layers. It allocates a common buffer size based on the maximum memory needs of all layers. This buffer is then shared efficiently across layers to increase data reuse.</p>
<p>TinyEngine also specializes in the kernels for each layer through techniques like tiling, unrolling, and fusing operators. For example, it will generate unrolled compute kernels with the number of loops needed for a 3x3 or 5x5 convolution. These specialized kernels extract maximum performance from the microcontroller hardware. It uses optimized depthwise convolutions to minimize memory allocations by computing each channel’s output in place over the input channel data. This technique exploits the channel-separable nature of depthwise convolutions to reduce peak memory size.</p>
<p>Like TFLite Micro, the compiled TinyEngine binary only includes ops needed for a specific model rather than all possible operations. This results in a very small binary footprint, keeping code size low for memory-constrained devices.</p>
<p>One difference between TFLite Micro and TinyEngine is that the latter is co-designed with “TinyNAS,” an architecture search method for microcontroller models similar to differential NAS for microcontrollers. TinyEngine’s efficiency allows for exploring larger and more accurate models through NAS. It also provides feedback to TinyNAS on which models can fit within the hardware constraints.</p>
<p>Through various custom techniques, such as static compilation, model-based scheduling, specialized kernels, and co-design with NAS, TinyEngine enables high-accuracy deep learning inference within microcontrollers’ tight resource constraints.</p>
</section>
<section id="library" class="level3 page-columns page-full" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="library"><span class="header-section-number">6.8.3</span> Library</h3>
<p><a href="https://www.keil.com/pack/doc/CMSIS/NN/html/index.html">CMSIS-NN</a>, standing for Cortex Microcontroller Software Interface Standard for Neural Networks, is a software library devised by ARM. It offers a standardized interface for deploying neural network inference on microcontrollers and embedded systems, focusing on optimization for ARM Cortex-M processors <span class="citation" data-cites="lai2018cmsis">(<a href="../../references.html#ref-lai2018cmsis" role="doc-biblioref">Lai, Suda, and Chandra 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lai2018cmsis" class="csl-entry" role="listitem">
Lai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018. <span>“Cmsis-Nn: <span>Efficient</span> Neural Network Kernels for Arm Cortex-m Cpus.”</span> <em>ArXiv Preprint</em> abs/1801.06601. <a href="https://arxiv.org/abs/1801.06601">https://arxiv.org/abs/1801.06601</a>.
</div></div><p><strong>Neural Network Kernels:</strong> CMSIS-NN has highly efficient kernels that handle fundamental neural network operations such as convolution, pooling, fully connected layers, and activation functions. It caters to a broad range of neural network models by supporting floating and fixed-point arithmetic. The latter is especially beneficial for resource-constrained devices as it curtails memory and computational requirements (Quantization).</p>
<p><strong>Hardware Acceleration:</strong> CMSIS-NN harnesses the power of Single Instruction, Multiple Data (SIMD) instructions available on many Cortex-M processors. This allows for parallel processing of multiple data elements within a single instruction, thereby boosting computational efficiency. Certain Cortex-M processors feature Digital Signal Processing (DSP) extensions that CMSIS-NN can exploit for accelerated neural network execution. The library also incorporates assembly-level optimizations tailored to specific microcontroller architectures to enhance performance further.</p>
<p><strong>Standardized API:</strong> CMSIS-NN offers a consistent and abstracted API that protects developers from the complexities of low-level hardware details. This makes the integration of neural network models into applications simpler. It may also encompass tools or utilities for converting popular neural network model formats into a format that is compatible with CMSIS-NN.</p>
<p><strong>Memory Management:</strong> CMSIS-NN provides functions for efficient memory allocation and management, which is vital in embedded systems where memory resources are scarce. It ensures optimal memory usage during inference and, in some instances, allows in-place operations to decrease memory overhead.</p>
<p><strong>Portability:</strong> CMSIS-NN is designed for portability across various Cortex-M processors. This enables developers to write code that can operate on different microcontrollers without significant modifications.</p>
<p><strong>Low Latency:</strong> CMSIS-NN minimizes inference latency, making it an ideal choice for real-time applications where swift decision-making is paramount.</p>
<p><strong>Energy Efficiency:</strong> The library is designed with a focus on energy efficiency, making it suitable for battery-powered and energy-constrained devices.</p>
</section>
</section>
<section id="choosing-the-right-framework" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="choosing-the-right-framework"><span class="header-section-number">6.9</span> Choosing the Right Framework</h2>
<p>Choosing the right machine learning framework for a given application requires carefully evaluating models, hardware, and software considerations. By analyzing these three aspects—models, hardware, and software—ML engineers can select the optimal framework and customize it as needed for efficient and performant on-device ML applications. The goal is to balance model complexity, hardware limitations, and software integration to design a tailored ML pipeline for embedded and edge devices.</p>
<div id="fig-tf-comparison" class="quarto-figure quarto-figure-center quarto-float anchored" data-caption="TensorFlow Framework Comparison - General" data-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image4.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - General" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.7: TensorFlow Framework Comparison - General. Credit: TensorFlow.
</figcaption>
</figure>
</div>
<section id="model" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="model"><span class="header-section-number">6.9.1</span> Model</h3>
<p>TensorFlow supports significantly more ops than TensorFlow Lite and TensorFlow Lite Micro as it is typically used for research or cloud deployment, which require a large number of and more flexibility with operators (see <a href="#fig-tf-comparison" class="quarto-xref">Figure&nbsp;<span>6.7</span></a>). TensorFlow Lite supports select ops for on-device training, whereas TensorFlow Micro does not. TensorFlow Lite also supports dynamic shapes and quantization-aware training, but TensorFlow Micro does not. In contrast, TensorFlow Lite and TensorFlow Micro offer native quantization tooling and support, where quantization refers to transforming an ML program into an approximated representation with available lower precision operations.</p>
</section>
<section id="software" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="software"><span class="header-section-number">6.9.2</span> Software</h3>
<div id="fig-tf-sw-comparison" class="quarto-figure quarto-figure-center quarto-float anchored" data-caption="TensorFlow Framework Comparison - Model" data-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-sw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image5.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - Model" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-sw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.8: TensorFlow Framework Comparison - Software. Credit: TensorFlow.
</figcaption>
</figure>
</div>
<p>TensorFlow Lite Micro does not have OS support, while TensorFlow and TensorFlow Lite do, to reduce memory overhead, make startup times faster, and consume less energy (see <a href="#fig-tf-sw-comparison" class="quarto-xref">Figure&nbsp;<span>6.8</span></a>). TensorFlow Lite Micro can be used in conjunction with real-time operating systems (RTOS) like FreeRTOS, Zephyr, and Mbed OS. TensorFlow Lite and TensorFlow Lite Micro support model memory mapping, allowing models to be directly accessed from flash storage rather than loaded into RAM, whereas TensorFlow does not. TensorFlow and TensorFlow Lite support accelerator delegation to schedule code to different accelerators, whereas TensorFlow Lite Micro does not, as embedded systems tend to have a limited array of specialized accelerators.</p>
</section>
<section id="hardware" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="hardware"><span class="header-section-number">6.9.3</span> Hardware</h3>
<div id="fig-tf-hw-comparison" class="quarto-figure quarto-figure-center quarto-float anchored" data-caption="TensorFlow Framework Comparison - Hardware" data-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tf-hw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image3.png" style="width:100.0%" data-align="center" data-caption="TensorFlow Framework Comparison - Hardware" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tf-hw-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.9: TensorFlow Framework Comparison - Hardware. Credit: TensorFlow.
</figcaption>
</figure>
</div>
<p>TensorFlow Lite and TensorFlow Lite Micro have significantly smaller base binary sizes and memory footprints than TensorFlow (see <a href="#fig-tf-hw-comparison" class="quarto-xref">Figure&nbsp;<span>6.9</span></a>). For example, a typical TensorFlow Lite Micro binary is less than 200KB, whereas TensorFlow is much larger. This is due to the resource-constrained environments of embedded systems. TensorFlow supports x86, TPUs, and GPUs like NVIDIA, AMD, and Intel. TensorFlow Lite supports Arm Cortex-A and x86 processors commonly used on mobile phones and tablets. The latter is stripped of all the unnecessary training logic for on-device deployment. TensorFlow Lite Micro provides support for microcontroller-focused Arm Cortex M cores like M0, M3, M4, and M7, as well as DSPs like Hexagon and SHARC and MCUs like STM32, NXP Kinetis, Microchip AVR.</p>
<p>Selecting the appropriate AI framework is essential to ensure that embedded systems can efficiently execute AI models. Key factors to consider when choosing a machine learning framework are ease of use, community support, performance, scalability, integration with data engineering tools, and integration with model optimization tools. By understanding these factors, you can make informed decisions and maximize the potential of your machine-learning initiatives.</p>
</section>
<section id="other-factors" class="level3" data-number="6.9.4">
<h3 data-number="6.9.4" class="anchored" data-anchor-id="other-factors"><span class="header-section-number">6.9.4</span> Other Factors</h3>
<p>Several other key factors beyond models, hardware, and software should be considered when evaluating AI frameworks for embedded systems.</p>
<section id="performance" class="level4">
<h4 class="anchored" data-anchor-id="performance">Performance</h4>
<p>Performance is critical in embedded systems where computational resources are limited. Evaluate the framework’s ability to optimize model inference for embedded hardware. Model quantization and hardware acceleration support are crucial in achieving efficient inference.</p>
</section>
<section id="scalability" class="level4">
<h4 class="anchored" data-anchor-id="scalability">Scalability</h4>
<p>Scalability is essential when considering the potential growth of an embedded AI project. The framework should support the deployment of models on various embedded devices, from microcontrollers to more powerful processors. It should also seamlessly handle both small-scale and large-scale deployments.</p>
</section>
<section id="integration-with-data-engineering-tools" class="level4">
<h4 class="anchored" data-anchor-id="integration-with-data-engineering-tools">Integration with Data Engineering Tools</h4>
<p>Data engineering tools are essential for data preprocessing and pipeline management. An ideal AI framework for embedded systems should seamlessly integrate with these tools, allowing for efficient data ingestion, transformation, and model training.</p>
</section>
<section id="integration-with-model-optimization-tools" class="level4">
<h4 class="anchored" data-anchor-id="integration-with-model-optimization-tools">Integration with Model Optimization Tools</h4>
<p>Model optimization ensures that AI models are well-suited for embedded deployment. Evaluate whether the framework integrates with model optimization tools like TensorFlow Lite Converter or ONNX Runtime to facilitate model quantization and size reduction.</p>
</section>
<section id="ease-of-use" class="level4">
<h4 class="anchored" data-anchor-id="ease-of-use">Ease of Use</h4>
<p>The ease of use of an AI framework significantly impacts development efficiency. A framework with a user-friendly interface and clear documentation reduces developers’ learning curve. Consideration should be given to whether the framework supports high-level APIs, allowing developers to focus on model design rather than low-level implementation details. This factor is incredibly important for embedded systems, which have fewer features than typical developers might be accustomed to.</p>
</section>
<section id="community-support" class="level4">
<h4 class="anchored" data-anchor-id="community-support">Community Support</h4>
<p>Community support plays another essential factor. Frameworks with active and engaged communities often have well-maintained codebases, receive regular updates, and provide valuable forums for problem-solving. As a result, community support also plays into Ease of Use because it ensures that developers have access to a wealth of resources, including tutorials and example projects. Community support provides some assurance that the framework will continue to be supported for future updates. There are only a few frameworks that cater to TinyML needs. TensorFlow Lite Micro is the most popular and has the most community support.</p>
</section>
</section>
</section>
<section id="future-trends-in-ml-frameworks" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="future-trends-in-ml-frameworks"><span class="header-section-number">6.10</span> Future Trends in ML Frameworks</h2>
<section id="decomposition" class="level3" data-number="6.10.1">
<h3 data-number="6.10.1" class="anchored" data-anchor-id="decomposition"><span class="header-section-number">6.10.1</span> Decomposition</h3>
<p>Currently, the ML system stack consists of four abstractions as shown in <a href="#fig-mlsys-stack" class="quarto-xref">Figure&nbsp;<span>6.10</span></a>, namely (1) computational graphs, (2) tensor programs, (3) libraries and runtimes, and (4) hardware primitives.</p>
<div id="fig-mlsys-stack" class="quarto-figure quarto-figure-center quarto-float anchored" data-caption="Four Abstractions in Current ML System Stack" data-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlsys-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image8.png" class="img-fluid figure-img" data-align="center" data-caption="Four Abstractions in Current ML System Stack">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlsys-stack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.10: Four abstractions in current ML system stacks. Credit: <a href="https://tvm.apache.org/2021/12/15/tvm-unity">TVM.</a>
</figcaption>
</figure>
</div>
<p>This has led to vertical (i.e., between abstraction levels) and horizontal (i.e., library-driven vs.&nbsp;compilation-driven approaches to tensor computation) boundaries, which hinder innovation for ML. Future work in ML frameworks can look toward breaking these boundaries. In December 2021, <a href="https://tvm.apache.org/2021/12/15/tvm-unity">Apache TVM</a> Unity was proposed, which aimed to facilitate interactions between the different abstraction levels (as well as the people behind them, such as ML scientists, ML engineers, and hardware engineers) and co-optimize decisions in all four abstraction levels.</p>
</section>
<section id="high-performance-compilers-libraries" class="level3" data-number="6.10.2">
<h3 data-number="6.10.2" class="anchored" data-anchor-id="high-performance-compilers-libraries"><span class="header-section-number">6.10.2</span> High-Performance Compilers &amp; Libraries</h3>
<p>As ML frameworks further develop, high-performance compilers and libraries will continue to emerge. Some current examples include <a href="https://www.tensorflow.org/xla/architecture">TensorFlow XLA</a> and Nvidia’s <a href="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS</a>, which accelerate linear algebra operations in computational graphs, and Nvidia’s <a href="https://developer.nvidia.com/tensorrt">TensorRT</a>, which accelerates and optimizes inference.</p>
</section>
<section id="ml-for-ml-frameworks" class="level3" data-number="6.10.3">
<h3 data-number="6.10.3" class="anchored" data-anchor-id="ml-for-ml-frameworks"><span class="header-section-number">6.10.3</span> ML for ML Frameworks</h3>
<p>We can also use ML to improve ML frameworks in the future. Some current uses of ML for ML frameworks include:</p>
<ul>
<li><p>hyperparameter optimization using techniques such as Bayesian optimization, random search, and grid search</p></li>
<li><p>neural architecture search (NAS) to automatically search for optimal network architectures</p></li>
<li><p>AutoML, which as described in <a href="#sec-ai_frameworks-advanced" class="quarto-xref"><span>Section 6.5</span></a>, automates the ML pipeline.</p></li>
</ul>
</section>
</section>
<section id="conclusion" class="level2" data-number="6.11">
<h2 data-number="6.11" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">6.11</span> Conclusion</h2>
<p>In summary, selecting the optimal framework requires thoroughly evaluating options against criteria like usability, community support, performance, hardware compatibility, and model conversion abilities. There is no universal best solution, as the right framework depends on the specific constraints and use case.</p>
<p>TensorFlow Lite Micro currently provides a strong starting point for extremely resource-constrained microcontroller-based platforms. Its comprehensive optimization tooling, such as quantization mapping and kernel optimizations, enables high performance on devices like Arm Cortex-M and RISC-V processors. The active developer community ensures accessible technical support. Seamless integration with TensorFlow for training and converting models makes the workflow cohesive.</p>
<p>For platforms with more capable CPUs like Cortex-A, TensorFlow Lite for Microcontrollers expands possibilities. It provides greater flexibility for custom and advanced models beyond the core operators in TFLite Micro. However, this comes at the cost of a larger memory footprint. These frameworks are ideal for automotive systems, drones, and more powerful edge devices that can benefit from greater model sophistication.</p>
<p>Frameworks specifically built for specialized hardware like CMSIS-NN on Cortex-M processors can further maximize performance but sacrifice portability. Integrated frameworks from processor vendors tailor the stack to their architectures, unlocking the full potential of their chips but locking you into their ecosystem.</p>
<p>Ultimately, choosing the right framework involves finding the best match between its capabilities and the requirements of the target platform. This requires balancing tradeoffs between performance needs, hardware constraints, model complexity, and other factors. Thoroughly assessing intended models and use cases and evaluating options against key metrics will guide developers in picking the ideal framework for their embedded ML application.</p>
</section>
<section id="sec-ai-frameworks-resource" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="sec-ai-frameworks-resource">Resources</h2>
<p>Here is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.</p>
<div class="callout callout-style-simple callout-slide no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>These slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to enhance their understanding and facilitate effective knowledge transfer.</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1zbnsihiO68oIUE04TVJEcDQ_Kyec4mhdQkIG6xoR0DY/edit#slide=id.g1ff94734162_0_0">Frameworks overview.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1BK2M2krnI24jSWO0r8tXegl1wgflGZTJyMkjfGolURI/edit#slide=id.g202a6885eb3_0_0">Embedded systems software.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1Jr7HzdZ7YaKO6KY9HBGbOG0BrTnKhbboQtf9d6xy3Ls/edit?usp=drive_link">Inference engines: TF vs.&nbsp;TFLite.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1_DwBbas8wAVWnJ0tbOorqotf9Gns1qNc3JJ6tw8bce0/edit?usp=drive_link">TF flavors: TF vs.&nbsp;TFLite vs.&nbsp;TFLite Micro.</a></p></li>
<li><p>TFLite Micro:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1XdwcZS0pz6kyuk6Vx90kE11hwUMAtS1cMoFQHZAxS20/edit?usp=drive_link">TFLite Micro Big Picture.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/10llaugp6EroGekFzB1pAH1OJ1dpJ4d7yxKglK1BsqlI/edit?usp=drive_link&amp;resourcekey=0-C6_PHSaI6u4x0Mv2KxWKbg">TFLite Micro Interpreter.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/123kdwjRXvbukyaOBvdp0PJpIs2JSxQ7GoDjB8y0FgIE/edit?usp=drive_link">TFLite Micro Model Format.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1_sHuWa3DDTCB9mBzKA4ElPWaUFA8oOelqHCBOHmsvC4/edit?usp=drive_link">TFLite Micro Memory Allocation.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1ZwLOLvYbKodNmyuKKGb_gD83NskrvNmnFC0rvGugJlY/edit?usp=drive_link">TFLite Micro NN Operations.</a></p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-exercise no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercises
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.</p>
<ul>
<li><p><a href="#exr-tfc" class="quarto-xref">Exercise&nbsp;<span>6.1</span></a></p></li>
<li><p><a href="#exr-tfl" class="quarto-xref">Exercise&nbsp;<span>6.2</span></a></p></li>
<li><p><a href="#exr-k" class="quarto-xref">Exercise&nbsp;<span>6.3</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-lab no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Labs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>In addition to exercises, we offer a series of hands-on labs allowing students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.</p>
<p><em>Coming soon.</em></p>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/data_engineering/data_engineering.html" class="pagination-link  aria-label=" &lt;span="" engineering&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/training/training.html" class="pagination-link" aria-label="<span class='chapter-number'>7</span>&nbsp; <span class='chapter-title'>AI Training</span>">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Training</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/frameworks/frameworks.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/frameworks/frameworks.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>