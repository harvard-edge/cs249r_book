<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning Systems - 16&nbsp; Responsible AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/sustainable_ai/sustainable_ai.html" rel="next">
<link href="../../contents/privacy_security/privacy_security.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>


</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="../../Machine-Learning-Systems.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="../../Machine-Learning-Systems.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/introduction.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/responsible_ai/responsible_ai.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">FRONT MATTER</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedication</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">MAIN</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/embedded_sys/embedded_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Embedded Systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/embedded_ml/embedded_ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Embedded AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Embedded AIOps</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">EXERCISES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/niclav_sys/niclav_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CV on Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/object_detection_fomo/object_detection_fomo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_nicla/kws_nicla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP - Spectral Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/motion_classify_ad/motion_classify_ad.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Communities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">16.1</span> Introduction</a></li>
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition"><span class="header-section-number">16.2</span> Definition</a></li>
  <li><a href="#principles-and-concepts" id="toc-principles-and-concepts" class="nav-link" data-scroll-target="#principles-and-concepts"><span class="header-section-number">16.3</span> Principles and Concepts</a>
  <ul>
  <li><a href="#transparency-and-explainability" id="toc-transparency-and-explainability" class="nav-link" data-scroll-target="#transparency-and-explainability"><span class="header-section-number">16.3.1</span> Transparency and Explainability</a></li>
  <li><a href="#fairness-bias-and-discrimination" id="toc-fairness-bias-and-discrimination" class="nav-link" data-scroll-target="#fairness-bias-and-discrimination"><span class="header-section-number">16.3.2</span> Fairness, Bias, and Discrimination</a></li>
  <li><a href="#privacy-and-data-governance" id="toc-privacy-and-data-governance" class="nav-link" data-scroll-target="#privacy-and-data-governance"><span class="header-section-number">16.3.3</span> Privacy and Data Governance</a></li>
  <li><a href="#safety-and-robustness" id="toc-safety-and-robustness" class="nav-link" data-scroll-target="#safety-and-robustness"><span class="header-section-number">16.3.4</span> Safety and Robustness</a></li>
  <li><a href="#accountability-and-governance" id="toc-accountability-and-governance" class="nav-link" data-scroll-target="#accountability-and-governance"><span class="header-section-number">16.3.5</span> Accountability and Governance</a></li>
  </ul></li>
  <li><a href="#cloud-edge-tiny-ml" id="toc-cloud-edge-tiny-ml" class="nav-link" data-scroll-target="#cloud-edge-tiny-ml"><span class="header-section-number">16.4</span> Cloud, Edge &amp; Tiny ML</a>
  <ul>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">16.4.1</span> Summary</a></li>
  <li><a href="#explainability" id="toc-explainability" class="nav-link" data-scroll-target="#explainability"><span class="header-section-number">16.4.2</span> Explainability</a></li>
  <li><a href="#fairness" id="toc-fairness" class="nav-link" data-scroll-target="#fairness"><span class="header-section-number">16.4.3</span> Fairness</a></li>
  <li><a href="#safety" id="toc-safety" class="nav-link" data-scroll-target="#safety"><span class="header-section-number">16.4.4</span> Safety</a></li>
  <li><a href="#accountability" id="toc-accountability" class="nav-link" data-scroll-target="#accountability"><span class="header-section-number">16.4.5</span> Accountability</a></li>
  <li><a href="#governance" id="toc-governance" class="nav-link" data-scroll-target="#governance"><span class="header-section-number">16.4.6</span> Governance</a></li>
  <li><a href="#privacy" id="toc-privacy" class="nav-link" data-scroll-target="#privacy"><span class="header-section-number">16.4.7</span> Privacy</a></li>
  </ul></li>
  <li><a href="#technical-aspects" id="toc-technical-aspects" class="nav-link" data-scroll-target="#technical-aspects"><span class="header-section-number">16.5</span> Technical Aspects</a>
  <ul>
  <li><a href="#detecting-and-mitigating-bias" id="toc-detecting-and-mitigating-bias" class="nav-link" data-scroll-target="#detecting-and-mitigating-bias"><span class="header-section-number">16.5.1</span> Detecting and Mitigating Bias</a>
  <ul class="collapse">
  <li><a href="#context-matters" id="toc-context-matters" class="nav-link" data-scroll-target="#context-matters">Context Matters</a></li>
  <li><a href="#thoughtful-deployment" id="toc-thoughtful-deployment" class="nav-link" data-scroll-target="#thoughtful-deployment">Thoughtful Deployment</a></li>
  </ul></li>
  <li><a href="#preserving-privacy" id="toc-preserving-privacy" class="nav-link" data-scroll-target="#preserving-privacy"><span class="header-section-number">16.5.2</span> Preserving Privacy</a></li>
  <li><a href="#machine-unlearning" id="toc-machine-unlearning" class="nav-link" data-scroll-target="#machine-unlearning"><span class="header-section-number">16.5.3</span> Machine Unlearning</a></li>
  <li><a href="#adversarial-examples-and-robustness" id="toc-adversarial-examples-and-robustness" class="nav-link" data-scroll-target="#adversarial-examples-and-robustness"><span class="header-section-number">16.5.4</span> Adversarial Examples and Robustness</a></li>
  <li><a href="#building-interpretable-models" id="toc-building-interpretable-models" class="nav-link" data-scroll-target="#building-interpretable-models"><span class="header-section-number">16.5.5</span> Building Interpretable Models</a>
  <ul class="collapse">
  <li><a href="#post-hoc-explainability" id="toc-post-hoc-explainability" class="nav-link" data-scroll-target="#post-hoc-explainability">Post Hoc Explainability</a></li>
  <li><a href="#inherent-interpretability" id="toc-inherent-interpretability" class="nav-link" data-scroll-target="#inherent-interpretability">Inherent Interpretability</a></li>
  <li><a href="#mechanistic-interpretability" id="toc-mechanistic-interpretability" class="nav-link" data-scroll-target="#mechanistic-interpretability">Mechanistic Interpretability</a></li>
  <li><a href="#challenges-and-considerations" id="toc-challenges-and-considerations" class="nav-link" data-scroll-target="#challenges-and-considerations">Challenges and Considerations</a></li>
  </ul></li>
  <li><a href="#monitoring-model-performance" id="toc-monitoring-model-performance" class="nav-link" data-scroll-target="#monitoring-model-performance"><span class="header-section-number">16.5.6</span> Monitoring Model Performance</a></li>
  </ul></li>
  <li><a href="#implementation-challenges" id="toc-implementation-challenges" class="nav-link" data-scroll-target="#implementation-challenges"><span class="header-section-number">16.6</span> Implementation Challenges</a>
  <ul>
  <li><a href="#organizational-and-cultural-structures" id="toc-organizational-and-cultural-structures" class="nav-link" data-scroll-target="#organizational-and-cultural-structures"><span class="header-section-number">16.6.1</span> Organizational and Cultural Structures</a></li>
  <li><a href="#obtaining-quality-and-representative-data" id="toc-obtaining-quality-and-representative-data" class="nav-link" data-scroll-target="#obtaining-quality-and-representative-data"><span class="header-section-number">16.6.2</span> Obtaining Quality and Representative Data</a>
  <ul class="collapse">
  <li><a href="#subgroup-imbalance" id="toc-subgroup-imbalance" class="nav-link" data-scroll-target="#subgroup-imbalance">Subgroup Imbalance</a></li>
  <li><a href="#quantifying-target-outcomes" id="toc-quantifying-target-outcomes" class="nav-link" data-scroll-target="#quantifying-target-outcomes">Quantifying Target Outcomes</a></li>
  <li><a href="#distribution-shift" id="toc-distribution-shift" class="nav-link" data-scroll-target="#distribution-shift">Distribution Shift</a></li>
  <li><a href="#gathering-data" id="toc-gathering-data" class="nav-link" data-scroll-target="#gathering-data">Gathering Data</a></li>
  </ul></li>
  <li><a href="#balancing-accuracy-and-other-objectives" id="toc-balancing-accuracy-and-other-objectives" class="nav-link" data-scroll-target="#balancing-accuracy-and-other-objectives"><span class="header-section-number">16.6.3</span> Balancing Accuracy and Other Objectives</a></li>
  </ul></li>
  <li><a href="#ethical-considerations-in-ai-design" id="toc-ethical-considerations-in-ai-design" class="nav-link" data-scroll-target="#ethical-considerations-in-ai-design"><span class="header-section-number">16.7</span> Ethical Considerations in AI Design</a>
  <ul>
  <li><a href="#ai-safety-and-value-alignment" id="toc-ai-safety-and-value-alignment" class="nav-link" data-scroll-target="#ai-safety-and-value-alignment"><span class="header-section-number">16.7.1</span> AI Safety and Value Alignment</a></li>
  <li><a href="#autonomous-systems-and-control-and-trust" id="toc-autonomous-systems-and-control-and-trust" class="nav-link" data-scroll-target="#autonomous-systems-and-control-and-trust"><span class="header-section-number">16.7.2</span> Autonomous Systems and Control [and Trust]</a></li>
  <li><a href="#economic-impacts-on-jobs-skills-wages" id="toc-economic-impacts-on-jobs-skills-wages" class="nav-link" data-scroll-target="#economic-impacts-on-jobs-skills-wages"><span class="header-section-number">16.7.3</span> Economic Impacts on Jobs, Skills, Wages</a></li>
  <li><a href="#scientific-communication-and-ai-literacy" id="toc-scientific-communication-and-ai-literacy" class="nav-link" data-scroll-target="#scientific-communication-and-ai-literacy"><span class="header-section-number">16.7.4</span> Scientific Communication and AI Literacy</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">16.8</span> Conclusion</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/responsible_ai/responsible_ai.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/responsible_ai/responsible_ai.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/introduction.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/responsible_ai/responsible_ai.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/cover_responsible_ai.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E 3 Prompt: Illustration of responsible AI in a futuristic setting with the universe in the backdrop: A human hand or hands nurturing a seedling that grows into an AI tree, symbolizing a neural network. The tree has digital branches and leaves, resembling a neural network, to represent the interconnected nature of AI. The background depicts a future universe where humans and animals with general intelligence collaborate harmoniously. The scene captures the initial nurturing of the AI as a seedling, emphasizing the ethical development of AI technology in harmony with humanity and the universe.</em></figcaption>
</figure>
</div>
<p>As machine learning models grow across various domains, these algorithms have the potential to perpetuate historical biases, breach privacy, or enable unethical automated decisions if developed without thoughtful consideration of their societal impacts. Even systems created with good intentions can ultimately discriminate against certain demographic groups, enable surveillance, or lack transparency into their behaviors and decision-making processes. As such, machine learning engineers and companies have an ethical responsibility to proactively ensure principles of fairness, accountability, safety, and transparency are reflected in their models to prevent harm and build public trust.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand the core principles and motivations behind responsible AI, including fairness, transparency, privacy, safety, and accountability.</p></li>
<li><p>Learn technical methods for putting responsible AI principles into practice, like detecting dataset biases, building interpretable models, adding noise for privacy, and testing model robustness.</p></li>
<li><p>Recognize organizational and social challenges to achieving responsible AI, including issues around data quality, model objectives, communication, and job impacts.</p></li>
<li><p>Gain knowledge of ethical frameworks and considerations for AI systems, spanning AI safety, human autonomy, and economic consequences.</p></li>
<li><p>Appreciate the increased complexity and costs associated with developing ethical, trustworthy AI systems compared to unprincipled AI.</p></li>
</ul>
</div>
</div>
<section id="introduction" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">16.1</span> Introduction</h2>
<p>Machine learning models are increasingly used to automate decisions in high-stakes social domains like healthcare, criminal justice, and employment. However, without deliberate care, these algorithms can perpetuate biases, breach privacy, or cause other harm. For instance, a loan approval model solely trained on data from high-income neighborhoods could disadvantage applicants from lower-income areas. This motivates the need for responsible machine learning - creating fair, accountable, transparent, and ethical models.</p>
<p>Several core principles underlie responsible ML. Fairness ensures models do not discriminate based on gender, race, age, and other attributes. Explainability enables humans to interpret model behaviors and improve transparency. Robustness and safety techniques prevent vulnerabilities like adversarial examples. Rigorous testing and validation help reduce unintended model weaknesses or side effects.</p>
<p>Implementing responsible ML presents both technical and ethical challenges. Developers must grapple with defining fairness mathematically, balancing competing objectives like accuracy vs interpretability, and securing quality training data. Organizations must also align incentives, policies, and culture to uphold ethical AI.</p>
<p>This chapter will equip you to critically evaluate AI systems and contribute to developing beneficial and ethical machine learning applications by covering the foundations, methods, and real-world implications of responsible ML. The responsible ML principles discussed are crucial knowledge as algorithms mediate more aspects of human society.</p>
</section>
<section id="definition" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="definition"><span class="header-section-number">16.2</span> Definition</h2>
<p>Responsible AI is about developing AI that positively impacts society under human ethics and values. There is no universally agreed-upon definition of “responsible AI,” but here is a summary of how it is commonly described. Responsible AI refers to designing, developing, and deploying artificial intelligence systems in an ethical, socially beneficial way. The core goal is to create trustworthy, unbiased, fair, transparent, accountable, and safe AI. While there is no canonical definition, responsible AI is generally considered to encompass principles such as:</p>
<ul>
<li><p><strong>Fairness:</strong> Avoiding biases, discrimination, and potential harm to certain groups or populations</p></li>
<li><p><strong>Explainability:</strong> Enabling humans to understand and interpret how AI models make decisions</p></li>
<li><p><strong>Transparency:</strong> Openly communicating how AI systems operate, are built, and are evaluated</p></li>
<li><p><strong>Accountability:</strong> Having processes to determine responsibility and liability for AI failures or negative impacts</p></li>
<li><p><strong>Robustness:</strong> Ensuring AI systems are secure, reliable and behave as intended</p></li>
<li><p><strong>Privacy:</strong> Protecting sensitive user data and adhering to privacy laws and ethics</p></li>
</ul>
<p>Putting these principles into practice involves technical techniques, corporate policies, governance frameworks, and moral philosophy. There are also ongoing debates around defining ambiguous concepts like fairness and determining how to balance competing objectives.</p>
</section>
<section id="principles-and-concepts" class="level2 page-columns page-full" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="principles-and-concepts"><span class="header-section-number">16.3</span> Principles and Concepts</h2>
<section id="transparency-and-explainability" class="level3" data-number="16.3.1">
<h3 data-number="16.3.1" class="anchored" data-anchor-id="transparency-and-explainability"><span class="header-section-number">16.3.1</span> Transparency and Explainability</h3>
<p>Machine learning models are often criticized as mysterious “black boxes” - opaque systems where it’s unclear how they arrived at particular predictions or decisions. For example, an AI system called <a href="https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx">COMPAS</a> used to assess criminal recidivism risk in the U.S. was found to be racially biased against black defendants. Still, the opacity of the algorithm made it difficult to understand and fix the problem. This lack of transparency can obscure biases, errors, and deficiencies.</p>
<p>Explaining model behaviors helps engender trust from the public and domain experts and enables identifying issues to address. Interpretability techniques like <a href="https://homes.cs.washington.edu/~marcotcr/blog/lime/">LIME</a>, Shapley values, and saliency maps empower humans to understand and validate model logic. Laws like the EU’s GDPR also mandate transparency, which requires explainability for certain automated decisions. Overall, transparency and explainability are critical pillars of responsible AI.</p>
</section>
<section id="fairness-bias-and-discrimination" class="level3 page-columns page-full" data-number="16.3.2">
<h3 data-number="16.3.2" class="anchored" data-anchor-id="fairness-bias-and-discrimination"><span class="header-section-number">16.3.2</span> Fairness, Bias, and Discrimination</h3>
<p>ML models trained on historically biased data often perpetuate and amplify those prejudices. Healthcare algorithms have been shown to disadvantage black patients by underestimating their needs <span class="citation" data-cites="obermeyer2019dissecting">(<a href="../../references.html#ref-obermeyer2019dissecting" role="doc-biblioref">Obermeyer et al. 2019</a>)</span>. Facial recognition needs to be more accurate for women and people of color. Such algorithmic discrimination can negatively impact people’s lives in profound ways.</p>
<div class="no-row-height column-margin column-container"><div id="ref-obermeyer2019dissecting" class="csl-entry" role="listitem">
Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. <span>“Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.”</span> <em>Science</em> 366 (6464): 447–53. <a href="https://doi.org/10.1126/science.aax2342">https://doi.org/10.1126/science.aax2342</a>.
</div></div><p>Different philosophical perspectives also exist on fairness - for example, is it fairer to treat all individuals equally or try to achieve equal outcomes for groups? Ensuring fairness requires proactively detecting and mitigating biases in data and models. However, achieving perfect fairness is tremendously difficult due to contrasting mathematical definitions and ethical perspectives. Still, promoting algorithmic fairness and non-discrimination is a key responsibility in AI development.</p>
</section>
<section id="privacy-and-data-governance" class="level3" data-number="16.3.3">
<h3 data-number="16.3.3" class="anchored" data-anchor-id="privacy-and-data-governance"><span class="header-section-number">16.3.3</span> Privacy and Data Governance</h3>
<p>Maintaining individuals’ privacy is an ethical obligation and legal requirement for organizations deploying AI systems. Regulations like the EU’s GDPR mandate data privacy protections and rights like the ability to access and delete one’s data.</p>
<p>However, maximizing the utility and accuracy of data for training models can conflict with preserving privacy - modeling disease progression could benefit from access to patients’ full genomes but sharing such data widely violates privacy.</p>
<p>Responsible data governance involves carefully anonymizing data, controlling access with encryption, getting informed consent from data subjects, and collecting the minimum data needed. Honoring privacy is challenging but critical as AI capabilities and adoption expand.</p>
</section>
<section id="safety-and-robustness" class="level3" data-number="16.3.4">
<h3 data-number="16.3.4" class="anchored" data-anchor-id="safety-and-robustness"><span class="header-section-number">16.3.4</span> Safety and Robustness</h3>
<p>Putting AI systems into real-world operation requires ensuring they are safe, reliable, and robust, especially for human interaction scenarios. Self-driving cars from <a href="https://www.nytimes.com/2018/03/19/technology/uber-driverless-fatality.html">Uber</a> and <a href="https://www.washingtonpost.com/technology/2022/06/15/tesla-autopilot-crashes/">Tesla</a> have been involved in deadly crashes due to unsafe behaviors.</p>
<p>Adversarial attacks that subtly alter input data can also fool ML models and cause dangerous failures if systems are not resistant. Deepfakes represent another emerging threat area.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AmUC4m6w1wo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Promoting safety requires extensive testing, risk analysis, human oversight, and designing systems that combine multiple weak models to avoid single points of failure. Rigorous safety mechanisms are essential for the responsible deployment of capable AI.</p>
</section>
<section id="accountability-and-governance" class="level3" data-number="16.3.5">
<h3 data-number="16.3.5" class="anchored" data-anchor-id="accountability-and-governance"><span class="header-section-number">16.3.5</span> Accountability and Governance</h3>
<p>When AI systems eventually fail or produce harmful outcomes, there must be mechanisms to address resultant issues, compensate affected parties, and assign responsibility. Both corporate accountability policies and government regulations are indispensable for responsible AI governance. For instance, <a href="https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID@15&amp;ChapterIDh">Illinois’ Artificial Intelligence Video Interview Act</a> requires companies to disclose and obtain consent for AI video analysis, promoting accountability.</p>
<p>Without clear accountability, even harms caused unintentionally could go unresolved, furthering public outrage and distrust. Oversight boards, impact assessments, grievance redress processes, and independent audits promote responsible development and deployment.</p>
</section>
</section>
<section id="cloud-edge-tiny-ml" class="level2 page-columns page-full" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="cloud-edge-tiny-ml"><span class="header-section-number">16.4</span> Cloud, Edge &amp; Tiny ML</h2>
<p>While these principles broadly apply across AI systems, certain responsible AI considerations are unique or pronounced when dealing with machine learning on embedded devices versus traditional server-based modeling. Therefore, we present a high-level taxonomy comparing responsible AI considerations across cloud, edge, and TinyML systems.</p>
<section id="summary" class="level3" data-number="16.4.1">
<h3 data-number="16.4.1" class="anchored" data-anchor-id="summary"><span class="header-section-number">16.4.1</span> Summary</h3>
<p>The table below summarizes how responsible AI principles manifest differently across cloud, edge, and TinyML architectures and how core considerations tie into their unique capabilities and limitations. Each environment’s constraints and tradeoffs shape how we approach transparency, accountability, governance, and other pillars of responsible AI.</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Principle</th>
<th>Cloud ML</th>
<th>Edge ML</th>
<th>TinyML</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Explainability</td>
<td>Complex models supported</td>
<td>Lightweight required</td>
<td>Severe limits</td>
</tr>
<tr class="even">
<td>Fairness</td>
<td>Broad data available</td>
<td>On-device biases</td>
<td>Limited data labels</td>
</tr>
<tr class="odd">
<td>Privacy</td>
<td>Cloud data vulnerabilities</td>
<td>More sensitive data</td>
<td>Data dispersed</td>
</tr>
<tr class="even">
<td>Safety</td>
<td>Hacking threats</td>
<td>Real-world interaction</td>
<td>Autonomous devices</td>
</tr>
<tr class="odd">
<td>Accountability</td>
<td>Corporate policies</td>
<td>Supply chain issues</td>
<td>Component tracing</td>
</tr>
<tr class="even">
<td>Governance</td>
<td>External oversight feasible</td>
<td>Self-governance needed</td>
<td>Protocol constraints</td>
</tr>
</tbody>
</table>
</section>
<section id="explainability" class="level3 page-columns page-full" data-number="16.4.2">
<h3 data-number="16.4.2" class="anchored" data-anchor-id="explainability"><span class="header-section-number">16.4.2</span> Explainability</h3>
<p>For cloud-based machine learning, explainability techniques can leverage significant compute resources, enabling complex methods like SHAP values or sampling-based approaches to interpret model behaviors. For example, <a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/05/InterpretML-Whitepaper.pdf">Microsoft’s InterpretML</a> toolkit provides explainability techniques tailored for cloud environments.</p>
<p>However, edge ML operates on resource-constrained devices, requiring more lightweight explainability methods that can run locally without excessive latency. Techniques like LIME <span class="citation" data-cites="ribeiro2016should">(<a href="../../references.html#ref-ribeiro2016should" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span> approximate model explanations using linear models or decision trees to avoid expensive computations, which makes them ideal for resource-constrained devices. But LIME requires training hundreds to even thousands of models to generate good explanations, which is often infeasible given edge computing constraints. In contrast, saliency-based methods are often much faster in practice, only requiring a single forward pass through the network to estimate feature importance. This greater efficiency makes such methods better suited to edge devices with limited compute resources where low-latency explanations are critical.</p>
<div class="no-row-height column-margin column-container"></div><p>Embedded systems poses the most significant challenges for explainability, given tiny hardware capabilities. More compact models and limited data make inherent model transparency easier. Explaining decisions may not be feasible on high-size- and power-optimized microcontrollers. <a href="https://www.darpa.mil/program/transparent-computing">DARPA’s Transparent Computing</a> program aims to develop extremely low overhead explainability, especially for TinyML devices like sensors and wearables.</p>
</section>
<section id="fairness" class="level3" data-number="16.4.3">
<h3 data-number="16.4.3" class="anchored" data-anchor-id="fairness"><span class="header-section-number">16.4.3</span> Fairness</h3>
<p>For cloud machine learning, vast datasets and computing power enable detecting biases across large heterogeneous populations and mitigating them through techniques like re-weighting data samples. However, biases may emerge from the broad behavioral data used to train cloud models. Amazon’s Fairness Flow framework helps assess cloud ML fairness.</p>
<p>Edge ML relies on limited on-device data, making analyzing biases across diverse groups harder. But edge devices interact closely with individuals, providing an opportunity to adapt locally for fairness. <a href="https://blog.research.google/2017/04/federated-learning-collaborative.html">Google’s Federated Learning</a> distributes model training across devices to incorporate individual differences.</p>
<p>TinyML poses unique challenges for fairness with highly dispersed specialized hardware and minimal training data. Bias testing is difficult across diverse devices. Collecting representative data from many devices to mitigate bias has scale and privacy hurdles. <a href="https://www.darpa.mil/news-events/2022-06-03">DARPA’s Assured Neuro Symbolic Learning and Reasoning (ANSR)</a> efforts are geared toward developing fairness techniques given extreme hardware constraints.</p>
</section>
<section id="safety" class="level3" data-number="16.4.4">
<h3 data-number="16.4.4" class="anchored" data-anchor-id="safety"><span class="header-section-number">16.4.4</span> Safety</h3>
<p>For cloud ML, key safety risks include model hacking, data poisoning, and malware disrupting cloud services. Robustness techniques like adversarial training, anomaly detection, and diversified models aim to harden cloud ML against attacks. Redundancy and redundancy can help prevent single points of failure.</p>
<p>Edge ML and TinyML interact with the physical world, so reliability and safety validation are critical. Rigorous testing platforms like <a href="https://www.foretellix.com/">Foretellix</a> synthetically generate edge scenarios to validate safety. TinyML safety is magnified by autonomous devices with limited supervision. TinyML safety often relies on collective coordination - swarms of drones maintain safety through redundancy. Physical control barriers also constrain unsafe TinyML device behaviors.</p>
<p>In summary, safety is crucial but manifests differently in each domain. Cloud ML guards against hacking, edge ML interacts physically so reliability is key, and TinyML leverages distributed coordination for safety. Understanding the nuances guides appropriate safety techniques.</p>
</section>
<section id="accountability" class="level3" data-number="16.4.5">
<h3 data-number="16.4.5" class="anchored" data-anchor-id="accountability"><span class="header-section-number">16.4.5</span> Accountability</h3>
<p>Cloud ML’s accountability centers on corporate practices like responsible AI committees, ethical charters, and processes to address harmful incidents. Third-party audits and external government oversight promote cloud ML accountability.</p>
<p>Edge ML accountability is more complex with distributed devices and supply chain fragmentation. Companies are accountable for devices, but components come from various vendors. Industry standards help coordinate edge ML accountability across stakeholders.</p>
<p>With TinyML, accountability mechanisms must be traced across long, complex supply chains of integrated circuits, sensors, and other hardware. TinyML certification schemes help track component provenance. Trade associations should ideally promote shared accountability for ethical TinyML.</p>
</section>
<section id="governance" class="level3" data-number="16.4.6">
<h3 data-number="16.4.6" class="anchored" data-anchor-id="governance"><span class="header-section-number">16.4.6</span> Governance</h3>
<p>For cloud ML, organizations institute internal governance like ethics boards, audits, and model risk management. But external governance also oversees cloud ML, like regulations on bias and transparency such as the <a href="https://www.whitehouse.gov/ostp/ai-bill-of-rights/">AI Bill of Rights</a>, <a href="https://gdpr-info.eu/">General Data Protection Regulation (GDPR)</a>, and <a href="https://oag.ca.gov/privacy/ccpa">California Consumer Protection Act (CCPA)</a>. Third-party auditing supports cloud ML governance.</p>
<p>Edge ML is more decentralized, requiring responsible self-governance by developers and companies deploying models locally. Industry associations coordinate governance across edge ML vendors. Open software helps align incentives for ethical edge ML.</p>
<p>With TinyML, extreme decentralization and complexity make external governance infeasible. TinyML relies on protocols and standards for self-governance baked into model design and hardware. Cryptography enables the provable trustworthiness of TinyML devices.</p>
</section>
<section id="privacy" class="level3" data-number="16.4.7">
<h3 data-number="16.4.7" class="anchored" data-anchor-id="privacy"><span class="header-section-number">16.4.7</span> Privacy</h3>
<p>For cloud ML, vast amounts of user data are concentrated in the cloud, creating risks of exposure through breaches. Differential privacy techniques add noise to cloud data to preserve privacy. Strict access controls and encryption protect cloud data at rest and in transit.</p>
<p>Edge ML moves data processing onto user devices, reducing aggregated data collection but increasing potential sensitivity as personal data resides on the device. Apple uses on-device ML and differential privacy to train models while minimizing data sharing. Data anonymization and secure enclaves protect on-device data.</p>
<p>TinyML distributes data across many resource-constrained devices, making centralized breaches unlikely and challenging for scale anonymization. Data minimization and using edge devices as intermediaries help TinyML privacy.</p>
<p>So, while cloud ML must protect expansive centralized data, edge ML secures sensitive on-device data, and TinyML aims for minimal distributed data sharing due to constraints. While privacy is vital throughout, techniques must match the environment. Understanding nuances allows for selecting appropriate privacy preservation approaches.</p>
</section>
</section>
<section id="technical-aspects" class="level2 page-columns page-full" data-number="16.5">
<h2 data-number="16.5" class="anchored" data-anchor-id="technical-aspects"><span class="header-section-number">16.5</span> Technical Aspects</h2>
<section id="detecting-and-mitigating-bias" class="level3 page-columns page-full" data-number="16.5.1">
<h3 data-number="16.5.1" class="anchored" data-anchor-id="detecting-and-mitigating-bias"><span class="header-section-number">16.5.1</span> Detecting and Mitigating Bias</h3>
<p>There has been a large body of work demonstrating that machine learning models can exhibit bias, from underperforming for people of a certain identity to making decisions that limit groups’ access to important resources <span class="citation" data-cites="buolamwini2018genderShades">(<a href="../../references.html#ref-buolamwini2018genderShades" role="doc-biblioref">Buolamwini and Gebru 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>Ensuring fair and equitable treatment for all groups affected by machine learning systems is crucial as these models increasingly impact people’s lives in areas like lending, healthcare, and criminal justice. We typically evaluate model fairness by considering “subgroup attributes” - attributes unrelated to the prediction task that capture identities like race, gender, or religion. For example, in a loan default prediction model, subgroups could include race, gender, or religion. When models are trained naively to maximize accuracy, they often ignore subgroup performance. However, this can negatively impact marginalized communities.</p>
<p>To illustrate, imagine a model predicting loan repayment where the plusses (+’s) represent repayment and the circles (O’s) represent default, as shown in <a href="#fig-fairness-example" class="quarto-xref">Figure&nbsp;<span>16.1</span></a>. The optimal accuracy would be correctly classifying all of Group A while misclassifying some of Group B’s creditworthy applicants as defaults. If positive classifications allow access loans, Group A would receive many more loans—which would naturally result in a biased outcome.</p>
<div id="fig-fairness-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fairness-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/fairness_cartoon.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fairness-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.1: Fairness and accuracy.
</figcaption>
</figure>
</div>
<p>Alternatively, correcting the biases against Group B would likely increase “false positives” and reduce accuracy for Group A. Or, we could train separate models focused on maximizing true positives for each group. But this would require explicitly using sensitive attributes like race in the decision process.</p>
<p>As we see, there are inherent tensions around priorities like accuracy versus subgroup fairness, and whether to explicitly account for protected classes. Reasonable people can disagree on the appropriate tradeoffs. And constraints around costs and implementation options further complicate matters. Overall, ensuring the fair and ethical use of machine learning involves navigating these complex challenges.</p>
<p>Thus, fairness literature has proposed three main <em>fairness metrics</em> for quantifying how fair a model performs over a dataset <span class="citation" data-cites="hardt2016equality">(<a href="../../references.html#ref-hardt2016equality" role="doc-biblioref">Hardt, Price, and Srebro 2016</a>)</span>. Given a model h, a dataset D consisting of (x,y,s) samples, where x is the data features, y is the label, and s is the subgroup attribute, where we assume there are simply two subgroups a and b, we can define the following.</p>
<div class="no-row-height column-margin column-container"></div><ol type="1">
<li><p><strong>Demographic Parity</strong> asks how accurate a model is for each subgroup. In other words, P(h(X) = Y S = a) = P(h(X) = Y S = b)</p></li>
<li><p><strong>Equalized Odds</strong> asks how precise a model is on positive and negative samples for each subgroup. P(h(X) = y S = a, Y = y) = P(h(X) = y S = b, Y = y)</p></li>
<li><p><strong>Equality of Opportunity</strong> is a special case of equalized odds that asks how precise a model is on positive samples only. This is relevant in cases such as resource allocation where we care about how positive (ie resource allocated) labels are distributed across groups. For example, we care that an equal proportion of loans are given to both men and women. P(h(X) = 1 S = a, Y = 1) = P(h(X) = 1 S = b, Y = 1)</p></li>
</ol>
<p>Note: these definitions often take a narrow view of considering binary comparisons between two subgroups. Another thread of fair machine learning research focusing on <em>multicalibration</em> and <em>multiaccuracy</em> considers the interactions between an arbitrary number of identities, acknowledging the inherent intersectionality of individual identities in the real world <span class="citation" data-cites="hebert2018multicalibration">(<a href="../../references.html#ref-hebert2018multicalibration" role="doc-biblioref">Hébert-Johnson et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hebert2018multicalibration" class="csl-entry" role="listitem">
Hébert-Johnson, Úrsula, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. 2018. <span>“Multicalibration: Calibration for the (Computationally-Identifiable) Masses.”</span> In <em>Proceedings of the 35th International Conference on Machine Learning, <span>ICML</span> 2018, Stockholmsm<span>ä</span>ssan, Stockholm, Sweden, July 10-15, 2018</em>, edited by Jennifer G. Dy and Andreas Krause, 80:1944–53. Proceedings of Machine Learning Research. <span>PMLR</span>. <a href="http://proceedings.mlr.press/v80/hebert-johnson18a.html">http://proceedings.mlr.press/v80/hebert-johnson18a.html</a>.
</div></div><section id="context-matters" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="context-matters">Context Matters</h4>
<p>Before making any technical decisions in developing an unbiased ML algorithm we need to understand the context surrounding our model. Here are some of the key questions to think about:</p>
<ul>
<li>Who will this model make decisions for?</li>
<li>Who is represented in the training data?</li>
<li>Who is represented and who is missing at the table of engineers, designers, and managers?<br>
</li>
<li>What sort of long-lasting impacts could this model have? For example, will it impact the financial security of an individual at a generational scale such as determining college admissions or admitting a loan for a house?<br>
</li>
<li>What historical and systematic biases are present in this setting, and are they present in the training data the model will generalize from?</li>
</ul>
<p>Understanding the social, ethical and historical background of a system is critical to prevent harm and should inform decisions throughout the model development lifecycle. After understanding the context, there are a wide array of technical decisions one can make to remove bias. First, one must decide what fairness metric is the most appropriate criterion to optimize for. Next, there are generally three main areas where one can intervene to debias an ML system.</p>
<p>First, preprocessing is when one balances a dataset to ensure fair representation, or even increases the weight on certain underrepresented groups to ensure the model performs well on them. Second, in processing attempts to modify the training process of an ML system to ensure it prioritizes fairness. This can be as simple as adding a fairness regularizer <span class="citation" data-cites="lowy2021fermi">(<a href="../../references.html#ref-lowy2021fermi" role="doc-biblioref">Lowy et al. 2021</a>)</span>, to training an ensemble of models and sampling from them in a specific manner <span class="citation" data-cites="agarwal2018reductions">(<a href="../../references.html#ref-agarwal2018reductions" role="doc-biblioref">Agarwal et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lowy2021fermi" class="csl-entry" role="listitem">
Lowy, Andrew, Rakesh Pavan, Sina Baharlouei, Meisam Razaviyayn, and Ahmad Beirami. 2021. <span>“Fermi: <span>Fair</span> Empirical Risk Minimization via Exponential Rényi Mutual Information.”</span>
</div><div id="ref-agarwal2018reductions" class="csl-entry" role="listitem">
Agarwal, Alekh, Alina Beygelzimer, Miroslav Dudı́k, John Langford, and Hanna M. Wallach. 2018. <span>“A Reductions Approach to Fair Classification.”</span> In <em>Proceedings of the 35th International Conference on Machine Learning, <span>ICML</span> 2018, Stockholmsm<span>ä</span>ssan, Stockholm, Sweden, July 10-15, 2018</em>, edited by Jennifer G. Dy and Andreas Krause, 80:60–69. Proceedings of Machine Learning Research. <span>PMLR</span>. <a href="http://proceedings.mlr.press/v80/agarwal18a.html">http://proceedings.mlr.press/v80/agarwal18a.html</a>.
</div><div id="ref-alghamdi2022beyond" class="csl-entry" role="listitem">
Alghamdi, Wael, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak, Shahab Asoodeh, and Flavio Calmon. 2022. <span>“Beyond Adult and <span>COMPAS:</span> <span>Fair</span> Multi-Class Prediction via Information Projection.”</span> <em>Adv. Neur. In.</em> 35: 38747–60.
</div><div id="ref-hardt2016equality" class="csl-entry" role="listitem">
Hardt, Moritz, Eric Price, and Nati Srebro. 2016. <span>“Equality of Opportunity in Supervised Learning.”</span> In <em>Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain</em>, edited by Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, 3315–23. <a href="https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html">https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html</a>.
</div></div><p>Finally, post processing debiases a model after the fact, taking a trained model and modifying its predictions in a specific manner to ensure fairness is preserved <span class="citation" data-cites="alghamdi2022beyond hardt2016equality">(<a href="../../references.html#ref-alghamdi2022beyond" role="doc-biblioref">Alghamdi et al. 2022</a>; <a href="../../references.html#ref-hardt2016equality" role="doc-biblioref">Hardt, Price, and Srebro 2016</a>)</span>. Post processing builds on the preprocessing and in processing steps by providing another opportunity to address bias and fairness issues in the model after it has already been trained.</p>
<p>The three step process of preprocessing, in processing, and post processing provides a framework for intervening at different stages of model development to mitigate issues around bias and fairness. While preprocessing and in processing focus on data and training, post processing allows for adjustments after the model has been fully trained. Together, these three approaches give multiple opportunities to detect and remove unfair bias.</p>
</section>
<section id="thoughtful-deployment" class="level4">
<h4 class="anchored" data-anchor-id="thoughtful-deployment">Thoughtful Deployment</h4>
<p>The breadth of existing fairness definitions and debiasing interventions underscores the need for thoughtful assessment before deploying ML systems. As ML researchers and developers, responsible model development requires proactively educating ourselves on the real-world context, consulting domain experts and end-users, and centering harm prevention.</p>
<p>Rather than seeing fairness considerations as a box to check, we must deeply engage with the unique social implications and ethical trade offs around each model we build. Every technical choice about datasets, model architectures, evaluation metrics and deployment constraints embeds values. By broadening our perspective beyond narrow technical metrics, carefully evaluating tradeoffs, and listening to impacted voices, we can work to ensure our systems expand opportunity rather than encode bias.</p>
<p>The path forward lies not in an arbitrary debiasing checklist but in a commitment to understanding and upholding our ethical responsibility at each step. This commitment starts with proactively educating ourselves and consulting others, rather than just going through the motions of a fairness checklist. It requires engaging deeply with ethical tradeoffs in our technical choices, evaluating impacts on different groups, and listening to those voices most impacted.</p>
<p>Ultimately, responsible and ethical AI systems come not from checkbox debiasing, but from upholding our duty to assess harms, broaden perspectives, understand tradeoffs and ensure we provide opportunity for all groups. This ethical responsibility should drive every step.</p>
<p>The connection between the paragraphs is that the first paragraph sets up the need for thoughtful assessment of fairness issues rather than a checkbox approach. The second paragraph then expands on what that thoughtful assessment looks like in practice - engaging with tradeoffs, evaluating impacts on groups, listening to impacted voices. Finally, the last paragraph circles back to the idea of avoiding an “arbitrary debiasing checklist” and instead committing to ethical responsibility through assessment, understanding tradeoffs, and providing opportunity.</p>
</section>
</section>
<section id="preserving-privacy" class="level3 page-columns page-full" data-number="16.5.2">
<h3 data-number="16.5.2" class="anchored" data-anchor-id="preserving-privacy"><span class="header-section-number">16.5.2</span> Preserving Privacy</h3>
<p>Recent incidents have demonstrated how AI models can memorize sensitive user data in ways that violate privacy. For example, as shown in Figure XXX below, Stable Diffusion’s art generations were found to mimic identifiable artists’ styles and replicate existing photos, concerning many <span class="citation" data-cites="carlini2023extracting">(<a href="../../references.html#ref-carlini2023extracting" role="doc-biblioref">Ippolito et al. 2023</a>)</span>. These risks are amplified with personalized ML systems deployed in intimate environments like homes or wearables.</p>
<div class="no-row-height column-margin column-container"></div><p>Imagine if a smart speaker uses our conversations to improve the quality of service to end users who genuinely want it. Still, others could violate privacy by trying to extract what the speaker “remembers.” <a href="#fig-diffusion-model-example" class="quarto-xref">Figure&nbsp;<span>16.2</span></a> below shows an example of how diffusion models can memorize and generate individual training examples <span class="citation" data-cites="carlini2023extracting">(<a href="../../references.html#ref-carlini2023extracting" role="doc-biblioref">Ippolito et al. 2023</a>)</span>.</p>
<div id="fig-diffusion-model-example" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-diffusion-model-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/diffusion_memorization.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diffusion-model-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.2: Diffusion models memorizing samples from training data. Credit: <span class="citation" data-cites="carlini2023extracting">Ippolito et al. (<a href="../../references.html#ref-carlini2023extracting" role="doc-biblioref">2023</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-carlini2023extracting" class="csl-entry" role="listitem">
Ippolito, Daphne, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher Choquette Choo, and Nicholas Carlini. 2023. <span>“Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy.”</span> In <em>Proceedings of the 16th International Natural Language Generation Conference</em>, 5253–70. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2023.inlg-main.3">https://doi.org/10.18653/v1/2023.inlg-main.3</a>.
</div></div></figure>
</div>
<p>Adversaries can take advantage of these memorization capabilities and train models to detect if specific training data influenced a target model. For example, membership inference attacks train a secondary model which learns to detect a change in the target model’s outputs when making inference over data it was trained on versus not trained on <span class="citation" data-cites="shokri2017membership">(<a href="../../references.html#ref-shokri2017membership" role="doc-biblioref">Shokri et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shokri2017membership" class="csl-entry" role="listitem">
Shokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. <span>“Membership Inference Attacks Against Machine Learning Models.”</span> In <em>2017 IEEE Symposium on Security and Privacy (SP)</em>, 3–18. IEEE; IEEE. <a href="https://doi.org/10.1109/sp.2017.41">https://doi.org/10.1109/sp.2017.41</a>.
</div><div id="ref-abadi2016deep" class="csl-entry" role="listitem">
Abadi, Martin, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. <span>“Deep Learning with Differential Privacy.”</span> In <em>Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>, 308–18. CCS ’16. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/2976749.2978318">https://doi.org/10.1145/2976749.2978318</a>.
</div></div><p>ML devices are especially vulnerable because they are often personalized on user data and are deployed in even more intimate settings such as the home. To combat these privacy issues, private machine learning techniques have evolved to establish safeguards against adversaries, as mentioned in the <a href="../../contents/privacy_security/privacy_security.html">Security and Privacy</a> chapter. Methods like differential privacy add mathematical noise during training to obscure individual data points’ influence on the model. Popular techniques like DP-SGD <span class="citation" data-cites="abadi2016deep">(<a href="../../references.html#ref-abadi2016deep" role="doc-biblioref">Abadi et al. 2016</a>)</span> also clip gradients to limit what the model leaks about the data. Still, some argue users should also be able to delete the impact of their data after the fact.</p>
</section>
<section id="machine-unlearning" class="level3 page-columns page-full" data-number="16.5.3">
<h3 data-number="16.5.3" class="anchored" data-anchor-id="machine-unlearning"><span class="header-section-number">16.5.3</span> Machine Unlearning</h3>
<p>With ML devices personalized to individual users and then deployed to remote edges without connectivity, a challenge arises—how can models responsively “forget” data points after deployment? If a user requests their personal data be removed from a personalized model, the lack of connectivity makes retraining infeasible. Thus, efficient on-device data forgetting is necessary but poses hurdles.</p>
<p>Initial unlearning approaches faced limitations in this context. Retraining models from scratch on the device to forget data points proves inefficient or even impossible, given the resource constraints. Fully retraining also requires retaining all the original training data on the device, which brings its own security and privacy risks. Common machine unlearning techniques <span class="citation" data-cites="bourtoule2021machine">(<a href="../../references.html#ref-bourtoule2021machine" role="doc-biblioref">Bourtoule et al. 2021</a>)</span> for remote embedded ML systems fail to enable responsive, secure data removal.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bourtoule2021machine" class="csl-entry" role="listitem">
Bourtoule, Lucas, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. <span>“Machine Unlearning.”</span> In <em>2021 IEEE Symposium on Security and Privacy (SP)</em>, 141–59. IEEE; IEEE. <a href="https://doi.org/10.1109/sp40001.2021.00019">https://doi.org/10.1109/sp40001.2021.00019</a>.
</div></div><p>However, newer methods show promise in modifying models to approximately forget data [?] without full retraining. While the accuracy loss from avoiding full rebuilds is modest, guaranteeing data privacy should still be the priority when handling sensitive user information ethically. Even slight exposure to private data can violate user trust. As ML systems become deeply personalized, efficiency and privacy must be enabled from the start—not afterthoughts.</p>
<p>Recent policy discussions which include the <a href="https://gdpr-info.eu">European Union’s General Data</a>, <a href="https://gdpr-info.eu">Protection Regulation (GDPR)</a>, the <a href="https://oag.ca.gov/privacy/ccpa">California Consumer Privacy Act (CCPA)</a>, the <a href="https://www.dataguidance.com/notes/japan-data-protection-overview">Act on the Protection of Personal Information (APPI)</a>, and Canada’s proposed <a href="https://blog.didomi.io/en-us/canada-data-privacy-law">Consumer Privacy Protection Act (CPPA)</a>, require the deletion of private information. These policies coupled with AI incidents like Stable Diffusion memorizing artist data have underscored the ethical need for users to delete their data from models after training.</p>
<p>The right to remove data arises from privacy concerns around corporations or adversaries misusing sensitive user information. Machine unlearning refers to removing the influence of specific points from an already-trained model. Naively this involves full retraining without the deleted data. However, for ML systems personalized and deployed to remote edges, connectivity constraints often make retraining infeasible. If a smart speaker learns from private home conversations, retaining access to delete that data is important.</p>
<p>Although limited, methods are evolving to enable efficient approximations to retraining for unlearning. By modifying models inference-time, they can mimic “forgetting” data without full access to training data. However, most current techniques are restricted to simple models, still have resource costs, and trading some accuracy. Though methods are evolving, enabling efficient data removal and respecting user privacy remains an imperative for responsible TinyML deployment.</p>
</section>
<section id="adversarial-examples-and-robustness" class="level3 page-columns page-full" data-number="16.5.4">
<h3 data-number="16.5.4" class="anchored" data-anchor-id="adversarial-examples-and-robustness"><span class="header-section-number">16.5.4</span> Adversarial Examples and Robustness</h3>
<p>Machine learning models, especially deep neural networks, have a well-documented Achilles heel: they often break when even tiny perturbations are made to their inputs <span class="citation" data-cites="szegedy2013intriguing">(<a href="../../references.html#ref-szegedy2013intriguing" role="doc-biblioref">Szegedy et al. 2014</a>)</span>. This surprising fragility highlights a major robustness gap that threatens real-world deployment in high-stakes domains. It also opens the door for adversarial attacks designed to deliberately fool models.</p>
<div class="no-row-height column-margin column-container"></div><p>Machine learning models can exhibit a surprising brittleness - minor input tweaks can cause shocking malfunctions, even in state-of-the-art deep neural networks <span class="citation" data-cites="szegedy2013intriguing">(<a href="../../references.html#ref-szegedy2013intriguing" role="doc-biblioref">Szegedy et al. 2014</a>)</span>. This unpredictability around out-of-sample data underscores gaps in model generalization and robustness. Given the growing ubiquity of ML, it also enables adversarial threats that weaponize models’ blindspots.</p>
<p>Deep neural networks demonstrate an almost paradoxical dual nature - human-like proficiency in training distributions coupled with extreme fragility to tiny input perturbations <span class="citation" data-cites="szegedy2013intriguing">(<a href="../../references.html#ref-szegedy2013intriguing" role="doc-biblioref">Szegedy et al. 2014</a>)</span>. This adversarial vulnerability gap highlights gaps in standard ML procedures and threats to real-world reliability. At the same time, it can be exploited: attackers can find model-breaking points humans wouldn’t perceive.</p>
<div class="no-row-height column-margin column-container"><div id="ref-szegedy2013intriguing" class="csl-entry" role="listitem">
Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. <span>“Intriguing Properties of Neural Networks.”</span> In <em>2nd International Conference on Learning Representations, <span>ICLR</span> 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings</em>, edited by Yoshua Bengio and Yann LeCun. <a href="http://arxiv.org/abs/1312.6199">http://arxiv.org/abs/1312.6199</a>.
</div></div><p><a href="#fig-adversarial-example" class="quarto-xref">Figure&nbsp;<span>16.3</span></a> includes an example of a small meaningless perturbation that changes a model prediction. This fragility has real-world impacts: lack of robustness undermines trust in deploying models for high-stakes applications like self-driving cars or medical diagnosis. Moreover, the vulnerability leads to security threats: attackers can deliberately craft adversarial examples that are perceptually indistinguishable from normal data but cause model failures.</p>
<div id="fig-adversarial-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/adversarial_robustness.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.3: Perturbation effect on prediction. Credit: <a href="https://www.microsoft.com/en-us/research/blog/adversarial-robustness-as-a-prior-for-better-transfer-learning/">Microsoft.</a>
</figcaption>
</figure>
</div>
<p>For instance, past work shows successful attacks that trick models for tasks like NSFW detection <span class="citation" data-cites="bhagoji2018practical">(<a href="../../references.html#ref-bhagoji2018practical" role="doc-biblioref">Bhagoji et al. 2018</a>)</span>, ad-blocking <span class="citation" data-cites="tramer2019adversarial">(<a href="../../references.html#ref-tramer2019adversarial" role="doc-biblioref">Tramèr et al. 2019</a>)</span>, and speech recognition <span class="citation" data-cites="carlini2016hidden">(<a href="../../references.html#ref-carlini2016hidden" role="doc-biblioref">Carlini et al. 2016</a>)</span>. While errors in these domains already pose security risks, the problem extends beyond IT security: recently adversarial robustness has been proposed as an additional performance metric by approximating worst-case behavior.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bhagoji2018practical" class="csl-entry" role="listitem">
Bhagoji, Arjun Nitin, Warren He, Bo Li, and Dawn Song. 2018. <span>“Practical Black-Box Attacks on Deep Neural Networks Using Efficient Query Mechanisms.”</span> In <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>, 154–69.
</div><div id="ref-tramer2019adversarial" class="csl-entry" role="listitem">
Tramèr, Florian, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, and Dan Boneh. 2019. <span>“<span>AdVersarial</span>: Perceptual Ad Blocking Meets Adversarial Machine Learning.”</span> In <em>Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</em>, 2005–21. ACM. <a href="https://doi.org/10.1145/3319535.3354222">https://doi.org/10.1145/3319535.3354222</a>.
</div><div id="ref-carlini2016hidden" class="csl-entry" role="listitem">
Carlini, Nicholas, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. <span>“Hidden Voice Commands.”</span> In <em>25th USENIX Security Symposium (USENIX Security 16)</em>, 513–30.
</div></div><p>The surprising model fragility highlighted above casts doubt on real-world reliability and opens the door to adversarial manipulation. This growing vulnerability underscores several needs. First, principled robustness evaluations are essential for quantifying model vulnerabilities before deployment. Approximating worst-case behavior surfaces blindspots.</p>
<p>Second, effective defenses across domains must be developed to close these robustness gaps. With security on the line, developers cannot ignore the threat of attacks exploiting model weaknesses. Moreover, for safety-critical applications like self-driving vehicles and medical diagnosis, we cannot afford any fragility-induced failures. Lives are at stake.</p>
<p>Finally, the research community continues mobilizing rapidly in response. Interest in adversarial machine learning has exploded as attacks reveal the need to bridge the robustness gap between synthetic and real-world data. Conferences now commonly feature defenses for securing and stabilizing models. The community recognizes that model fragility is a critical issue that must be addressed through robustness testing, defense development, and ongoing research. By surfacing blindspots and responding with principled defenses, we can work to ensure reliability and safety for machine learning systems, especially in high-stakes domains.</p>
</section>
<section id="building-interpretable-models" class="level3 page-columns page-full" data-number="16.5.5">
<h3 data-number="16.5.5" class="anchored" data-anchor-id="building-interpretable-models"><span class="header-section-number">16.5.5</span> Building Interpretable Models</h3>
<p>As models are deployed more frequently in high-stakes settings, practitioners, developers, and downstream end-users, as well as increasing regulation, have highlighted the need for explainability in machine learning. The goal of many interpretability and explainability methods is to provide practitioners with more information about either the overall behavior of models or the behavior given a specific input. This allows users to decide whether or not the output or prediction of a model is trustworthy.</p>
<p>Such analysis can help developers debug models and improve performance by pointing out biases, spurious correlations, and failure modes of models. In cases where models are able to surpass human performance on a task, interpretability can help users and researchers better understand relationships in their data and patterns that may previously have been unknown.</p>
<p>There are many classes of methods in explainability/interpretability, including: post hoc explainability, inherent interpretability, and mechanistic interpretability. These methods aim to make complex machine learning models more understandable and ensure users can trust model predictions, especially in critical settings. By providing transparency into model behavior, explainability techniques are an important tool for developing safe, fair, and reliable AI systems.</p>
<section id="post-hoc-explainability" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="post-hoc-explainability">Post Hoc Explainability</h4>
<p>Post hoc explainability methods typically explain the output behavior of a black-box model on a specific input. Popular methods include counterfactual explanations, feature attribution methods, and concept-based explanations.</p>
<p><strong>Counterfactual explanations</strong>, also frequently referred to as algorithmic recourse, take the form of “If X had not occurred, Y would not have occurred” <span class="citation" data-cites="wachter2017counterfactual">(<a href="../../references.html#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span>. For example, consider a person applying for a bank loan whose application is rejected by a model. They may ask their bank for recourse, or how they need to change to be eligible for a loan. A counterfactual explanation would tell them which features they need to change and by how much such that the model’s prediction changes.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wachter2017counterfactual" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. <span>“Counterfactual Explanations Without Opening the Black Box: <span>Automated</span> Decisions and the <span>GDPR</span>.”</span> <em>SSRN Electronic Journal</em> 31: 841. <a href="https://doi.org/10.2139/ssrn.3063289">https://doi.org/10.2139/ssrn.3063289</a>.
</div><div id="ref-selvaraju2017grad" class="csl-entry" role="listitem">
Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. <span>“Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.”</span> In <em><span>IEEE</span> International Conference on Computer Vision, <span>ICCV</span> 2017, Venice, Italy, October 22-29, 2017</em>, 618–26. <span>IEEE</span> Computer Society. <a href="https://doi.org/10.1109/ICCV.2017.74">https://doi.org/10.1109/ICCV.2017.74</a>.
</div><div id="ref-smilkov2017smoothgrad" class="csl-entry" role="listitem">
Smilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. 2017. <span>“Smoothgrad: <span>Removing</span> Noise by Adding Noise.”</span> <em>ArXiv Preprint</em> abs/1706.03825. <a href="https://arxiv.org/abs/1706.03825">https://arxiv.org/abs/1706.03825</a>.
</div><div id="ref-ribeiro2016should" class="csl-entry" role="listitem">
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. <span>“<span>”</span> Why Should i Trust You?<span>”</span> Explaining the Predictions of Any Classifier.”</span> In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 1135–44.
</div><div id="ref-lundberg2017unified" class="csl-entry" role="listitem">
Lundberg, Scott M., and Su-In Lee. 2017. <span>“A Unified Approach to Interpreting Model Predictions.”</span> In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, <span>USA</span></em>, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 4765–74. <a href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</a>.
</div></div><p><strong>Feature attribution methods</strong> aim to highlight the input features important or necessary for a particular prediction. For a computer vision model, this would mean highlighting the individual pixels that contributed most to the predicted label of the image. Note that these methods do not explain how those pixels/features impact the prediction, only that they do. Common methods include input gradients, GradCAM <span class="citation" data-cites="selvaraju2017grad">(<a href="../../references.html#ref-selvaraju2017grad" role="doc-biblioref">Selvaraju et al. 2017</a>)</span>, SmoothGrad <span class="citation" data-cites="smilkov2017smoothgrad">(<a href="../../references.html#ref-smilkov2017smoothgrad" role="doc-biblioref">Smilkov et al. 2017</a>)</span>, LIME <span class="citation" data-cites="ribeiro2016should">(<a href="../../references.html#ref-ribeiro2016should" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>)</span>, and SHAP <span class="citation" data-cites="lundberg2017unified">(<a href="../../references.html#ref-lundberg2017unified" role="doc-biblioref">Lundberg and Lee 2017</a>)</span>.</p>
<p>By providing examples of changes to input features that would alter a prediction (counterfactuals) or indicating the most influential features for a given prediction (attribution), these post hoc explanation techniques shed light on model behavior for individual inputs. This granular transparency helps users determine whether they can trust and act upon specific model outputs.</p>
<p><strong>Concept based explanations</strong> aim to explain model behavior and outputs using a pre-defined set of semantic concepts (e.g.&nbsp;the model recognizes scene class “bedroom” based on the presence of concepts “bed” and “pillow”). Recent work shows that users often prefer these explanations to attribution and example based explanations because they “resemble human reasoning and explanations” <span class="citation" data-cites="ramaswamy2023ufo">(<a href="../../references.html#ref-ramaswamy2023ufo" role="doc-biblioref">Vikram V. Ramaswamy et al. 2023b</a>)</span>. Popular concept based explanation methods include TCAV <span class="citation" data-cites="kim2018interpretability">(<a href="../../references.html#ref-kim2018interpretability" role="doc-biblioref">Kim et al. 2018</a>)</span>, Network Dissection <span class="citation" data-cites="bau2017network">(<a href="../../references.html#ref-bau2017network" role="doc-biblioref">Bau et al. 2017</a>)</span>, and interpretable basis decomposition <span class="citation" data-cites="zhou2018interpretable">(<a href="../../references.html#ref-zhou2018interpretable" role="doc-biblioref">Zhou et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ramaswamy2023ufo" class="csl-entry" role="listitem">
Ramaswamy, Vikram V, Sunnie SY Kim, Ruth Fong, and Olga Russakovsky. 2023b. <span>“<span>UFO:</span> <span>A</span> Unified Method for Controlling Understandability and Faithfulness Objectives in Concept-Based Explanations for <span>CNNs</span>.”</span> <em>ArXiv Preprint</em> abs/2303.15632. <a href="https://arxiv.org/abs/2303.15632">https://arxiv.org/abs/2303.15632</a>.
</div><div id="ref-kim2018interpretability" class="csl-entry" role="listitem">
Kim, Been, Martin Wattenberg, Justin Gilmer, Carrie J. Cai, James Wexler, Fernanda B. Viégas, and Rory Sayres. 2018. <span>“Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors <span>(TCAV)</span>.”</span> In <em>Proceedings of the 35th International Conference on Machine Learning, <span>ICML</span> 2018, Stockholmsm<span>ä</span>ssan, Stockholm, Sweden, July 10-15, 2018</em>, edited by Jennifer G. Dy and Andreas Krause, 80:2673–82. Proceedings of Machine Learning Research. <span>PMLR</span>. <a href="http://proceedings.mlr.press/v80/kim18d.html">http://proceedings.mlr.press/v80/kim18d.html</a>.
</div><div id="ref-bau2017network" class="csl-entry" role="listitem">
Bau, David, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017. <span>“Network Dissection: Quantifying Interpretability of Deep Visual Representations.”</span> In <em>2017 <span>IEEE</span> Conference on Computer Vision and Pattern Recognition, <span>CVPR</span> 2017, Honolulu, HI, USA, July 21-26, 2017</em>, 3319–27. <span>IEEE</span> Computer Society. <a href="https://doi.org/10.1109/CVPR.2017.354">https://doi.org/10.1109/CVPR.2017.354</a>.
</div><div id="ref-zhou2018interpretable" class="csl-entry" role="listitem">
Zhou, Bolei, Yiyou Sun, David Bau, and Antonio Torralba. 2018. <span>“Interpretable Basis Decomposition for Visual Explanation.”</span> In <em>Proceedings of the European Conference on Computer Vision (ECCV)</em>, 119–34.
</div><div id="ref-ramaswamy2023overlooked" class="csl-entry" role="listitem">
Ramaswamy, Vikram V., Sunnie S. Y. Kim, Ruth Fong, and Olga Russakovsky. 2023a. <span>“Overlooked Factors in Concept-Based Explanations: <span>Dataset</span> Choice, Concept Learnability, and Human Capability.”</span> In <em>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10932–41. IEEE. <a href="https://doi.org/10.1109/cvpr52729.2023.01052">https://doi.org/10.1109/cvpr52729.2023.01052</a>.
</div></div><p>Note that these methods are extremely sensitive to the size and quality of the concept set, and that there exists a tradeoff between the accuracy and faithfulness of these methods and their interpretability or understandability to humans <span class="citation" data-cites="ramaswamy2023overlooked">(<a href="../../references.html#ref-ramaswamy2023overlooked" role="doc-biblioref">Vikram V. Ramaswamy et al. 2023a</a>)</span>. By mapping model predictions to human-understandable concepts, however, concept-based explanations can provide transparency into the reasoning behind model outputs.</p>
</section>
<section id="inherent-interpretability" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="inherent-interpretability">Inherent Interpretability</h4>
<p>Inherently interpretable models are constructed such that their explanations are part of the model architecture and are thus naturally faithful, which sometimes makes them preferable to post-hoc explanations applied to black-box models, especially in high-stakes domains where transparency is imperative <span class="citation" data-cites="rudin2019stop">(<a href="../../references.html#ref-rudin2019stop" role="doc-biblioref">Rudin 2019</a>)</span>. Often, these models are constrained so that the relationships between input features and predictions are easy for humans to follow (linear models, decision trees, decision sets, k-NN models), or they obey structural knowledge of the domain, such as monotonicity <span class="citation" data-cites="gupta2016monotonic">(<a href="../../references.html#ref-gupta2016monotonic" role="doc-biblioref">Gupta et al. 2016</a>)</span>, causality, or additivity <span class="citation" data-cites="lou2013accurate beck1998beyond">(<a href="../../references.html#ref-lou2013accurate" role="doc-biblioref">Lou et al. 2013</a>; <a href="../../references.html#ref-beck1998beyond" role="doc-biblioref">Beck and Jackman 1998</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rudin2019stop" class="csl-entry" role="listitem">
Rudin, Cynthia. 2019. <span>“Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.”</span> <em>Nature Machine Intelligence</em> 1 (5): 206–15. <a href="https://doi.org/10.1038/s42256-019-0048-x">https://doi.org/10.1038/s42256-019-0048-x</a>.
</div><div id="ref-gupta2016monotonic" class="csl-entry" role="listitem">
Gupta, Maya, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, and Alexander Van Esbroeck. 2016. <span>“Monotonic Calibrated Interpolated Look-up Tables.”</span> <em>The Journal of Machine Learning Research</em> 17 (1): 3790–3836.
</div><div id="ref-lou2013accurate" class="csl-entry" role="listitem">
Lou, Yin, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013. <span>“Accurate Intelligible Models with Pairwise Interactions.”</span> In <em>The 19th <span>ACM</span> <span>SIGKDD</span> International Conference on Knowledge Discovery and Data Mining, <span>KDD</span> 2013, Chicago, IL, USA, August 11-14, 2013</em>, edited by Inderjit S. Dhillon, Yehuda Koren, Rayid Ghani, Ted E. Senator, Paul Bradley, Rajesh Parekh, Jingrui He, Robert L. Grossman, and Ramasamy Uthurusamy, 623–31. <span>ACM</span>. <a href="https://doi.org/10.1145/2487575.2487579">https://doi.org/10.1145/2487575.2487579</a>.
</div><div id="ref-beck1998beyond" class="csl-entry" role="listitem">
Beck, Nathaniel, and Simon Jackman. 1998. <span>“Beyond Linearity by Default: <span>Generalized</span> Additive Models.”</span> <em>Am. J. Polit. Sci.</em> 42 (2): 596. <a href="https://doi.org/10.2307/2991772">https://doi.org/10.2307/2991772</a>.
</div><div id="ref-koh2020concept" class="csl-entry" role="listitem">
Koh, Pang Wei, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. 2020. <span>“Concept Bottleneck Models.”</span> In <em>Proceedings of the 37th International Conference on Machine Learning, <span>ICML</span> 2020, 13-18 July 2020, Virtual Event</em>, 119:5338–48. Proceedings of Machine Learning Research. <span>PMLR</span>. <a href="http://proceedings.mlr.press/v119/koh20a.html">http://proceedings.mlr.press/v119/koh20a.html</a>.
</div><div id="ref-chen2019looks" class="csl-entry" role="listitem">
Chen, Chaofan, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan Su. 2019. <span>“This Looks Like That: Deep Learning for Interpretable Image Recognition.”</span> In <em>Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada</em>, edited by Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, 8928–39. <a href="https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html">https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html</a>.
</div></div><p>However, more recent works have relaxed the restrictions on inherently interpretable models, using black-box models for feature extraction and using a simpler inherently interpretable model for classification, allowing for faithful explanations that relate high-level features to prediction. For example, Concept Bottleneck Models <span class="citation" data-cites="koh2020concept">(<a href="../../references.html#ref-koh2020concept" role="doc-biblioref">Koh et al. 2020</a>)</span> predict a concept set c that is passed into a linear classifier, and ProtoPNets <span class="citation" data-cites="chen2019looks">(<a href="../../references.html#ref-chen2019looks" role="doc-biblioref">Chen et al. 2019</a>)</span> dissect inputs into linear combinations of similarities to prototypical parts from the training set.</p>
</section>
<section id="mechanistic-interpretability" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="mechanistic-interpretability">Mechanistic Interpretability</h4>
<p>Mechanistic interpretability methods seek to reverse engineer neural networks, often analogized to how one might reverse engineer a compiled binary or how neuroscientists attempt to decode the function of individual neurons and circuits in brains. Most research in mechanistic interpretability views models as a computational graph <span class="citation" data-cites="geiger2021causal">(<a href="../../references.html#ref-geiger2021causal" role="doc-biblioref">Geiger et al. 2021</a>)</span> and circuits are subgraphs with distinct functionality <span class="citation" data-cites="wang2022interpretability">(<a href="../../references.html#ref-wang2022interpretability" role="doc-biblioref">Wang and Zhan 2019</a>)</span>. Current approaches to extracting circuits from neural networks and understanding their functionality rely on human manual inspection of visualizations produced by circuits <span class="citation" data-cites="olah2020zoom">(<a href="../../references.html#ref-olah2020zoom" role="doc-biblioref">Olah et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-geiger2021causal" class="csl-entry" role="listitem">
Geiger, Atticus, Hanson Lu, Thomas Icard, and Christopher Potts. 2021. <span>“Causal Abstractions of Neural Networks.”</span> In <em>Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, Virtual</em>, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, 9574–86. <a href="https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html</a>.
</div><div id="ref-wang2022interpretability" class="csl-entry" role="listitem">
Wang, LingFeng, and YaQing Zhan. 2019. <span>“A Conceptual Peer Review Model for <span class="nocase">arXiv</span> and Other Preprint Databases.”</span> <em>Learn. Publ.</em> 32 (3): 213–19. <a href="https://doi.org/10.1002/leap.1229">https://doi.org/10.1002/leap.1229</a>.
</div><div id="ref-olah2020zoom" class="csl-entry" role="listitem">
Olah, Chris, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020. <span>“Zoom in: <span>An</span> Introduction to Circuits.”</span> <em>Distill</em> 5 (3): e00024–001. <a href="https://doi.org/10.23915/distill.00024.001">https://doi.org/10.23915/distill.00024.001</a>.
</div><div id="ref-bricken2023towards" class="csl-entry" role="listitem">
Davarzani, Samaneh, David Saucier, Purva Talegaonkar, Erin Parker, Alana Turner, Carver Middleton, Will Carroll, et al. 2023. <span>“Closing the Wearable Gap: <span class="nocase">Foot<span></span>ankle</span> Kinematic Modeling via Deep Learning Models Based on a Smart Sock Wearable.”</span> <em>Wearable Technologies</em> 4. <a href="https://doi.org/10.1017/wtc.2023.3">https://doi.org/10.1017/wtc.2023.3</a>.
</div></div><p>Alternatively, some approaches build sparse autoencoders that encourage neurons to encode disentangled interpretable features <span class="citation" data-cites="bricken2023towards">(<a href="../../references.html#ref-bricken2023towards" role="doc-biblioref">Davarzani et al. 2023</a>)</span>. This field is much newer than existing areas in explainability and interpretability, and as such most works are generally exploratory rather than solution oriented.</p>
<p>There are many open problems in mechanistic interpretability, including the polysemanticity of neurons and circuits, the inconvenience and subjectivity of human labeling, and the exponential search space for identifying circuits in large models with billions or trillions of neurons.</p>
</section>
<section id="challenges-and-considerations" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="challenges-and-considerations">Challenges and Considerations</h4>
<p>As methods for interpreting and explaining models progress, it is important to note that humans overtrust and misuse interpretability tools <span class="citation" data-cites="kaur2020interpreting">(<a href="../../references.html#ref-kaur2020interpreting" role="doc-biblioref">Kaur et al. 2020</a>)</span> and that a user’s trust in a model due to an explanation can be independent of the correctness of the explanations <span class="citation" data-cites="lakkaraju2020fool">(<a href="../../references.html#ref-lakkaraju2020fool" role="doc-biblioref">Lakkaraju and Bastani 2020</a>)</span>. As such, it is necessary that aside from assessing the faithfulness/correctness of explanations, researchers must also ensure that interpretability methods are developed and deployed with a specific user in mind, and that user studies are performed to evaluate their efficacy and usefulness in practice.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kaur2020interpreting" class="csl-entry" role="listitem">
Kaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna M. Wallach, and Jennifer Wortman Vaughan. 2020. <span>“Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning.”</span> In <em><span>CHI</span> ’20: <span>CHI</span> Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020</em>, edited by Regina Bernhaupt, Florian ’Floyd’Mueller, David Verweij, Josh Andres, Joanna McGrenere, Andy Cockburn, Ignacio Avellino, et al., 1–14. <span>ACM</span>. <a href="https://doi.org/10.1145/3313831.3376219">https://doi.org/10.1145/3313831.3376219</a>.
</div><div id="ref-lakkaraju2020fool" class="csl-entry" role="listitem">
Lakkaraju, Himabindu, and Osbert Bastani. 2020. <span>“<span>”How</span> Do i Fool You?”: Manipulating User Trust via Misleading Black Box Explanations.”</span> In <em>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em>, 79–85. ACM. <a href="https://doi.org/10.1145/3375627.3375833">https://doi.org/10.1145/3375627.3375833</a>.
</div></div><p>Furthermore, explanations should be tailored with the expertise of the user in mind, as well as the task they are using the explanation for, and the corresponding minimal amount of information required for the explanation to be useful to prevent information overload.</p>
<p>While interpretability/explainability are popular areas in machine learning research, very few works study their intersection with TinyML and edge computing. Given that a significant application of TinyML is healthcare, which often requires high transparency and interpretability, it is important that existing techniques are tested for scalability and efficiency with respect to edge devices. Many methods rely on extra forward and backward passes, and some even require extensive training of proxy models, all of which would likely be infeasible on microcontrollers that are resource constrained.</p>
<p>That being said, explainability methods can be highly useful in the <em>development</em> of models for edge devices, as they can give insights into how input data and models can be compressed and how representations may change post compression. Furthermore, many interpretable models are often smaller than their black-box counterparts, which could have additional benefits in TinyML applications.</p>
</section>
</section>
<section id="monitoring-model-performance" class="level3" data-number="16.5.6">
<h3 data-number="16.5.6" class="anchored" data-anchor-id="monitoring-model-performance"><span class="header-section-number">16.5.6</span> Monitoring Model Performance</h3>
<p>While developers may train models such that they seem adversarially robust, fair, and interpretable before deployment, it is imperative that both the users and the owners of the model continue to monitor the model’s performance and trustworthiness during the model’s full lifecycle. In practice, data is frequently changing, which can often result in distribution shifts. These distribution shifts can have profound impacts on both the vanilla predictive performance of the model as well as its trustworthiness (fairness, robustness, and interpretability) on real world data.</p>
<p>Furthermore, definitions of fairness also frequently change with time, such as what society considers a protected attribute, and the expertise of the users asking for explanations may change as well.</p>
<p>To ensure that models keep up to date with such changes in the real world, developers must continually evaluate their model on current and representative data and standards, and update models when necessary.</p>
</section>
</section>
<section id="implementation-challenges" class="level2 page-columns page-full" data-number="16.6">
<h2 data-number="16.6" class="anchored" data-anchor-id="implementation-challenges"><span class="header-section-number">16.6</span> Implementation Challenges</h2>
<section id="organizational-and-cultural-structures" class="level3 page-columns page-full" data-number="16.6.1">
<h3 data-number="16.6.1" class="anchored" data-anchor-id="organizational-and-cultural-structures"><span class="header-section-number">16.6.1</span> Organizational and Cultural Structures</h3>
<p>While innovation and regulation are often seen as having competing interests, many countries have found it necessary to provide oversight as AI systems expand into more sectors. As illustrated in <a href="#fig-human-centered-ai" class="quarto-xref">Figure&nbsp;<span>16.4</span></a>, this oversight has become crucial as these systems continue permeating various industries and impacting people’s lives (see <a href="https://academic-oup-com.ezp-prod1.hul.harvard.edu/book/41126/chapter/350465542">Human-Centered AI, Chapter 8 “Government Interventions and Regulations”</a>.</p>
<div id="fig-human-centered-ai" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-human-centered-ai-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/human_centered_ai.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-human-centered-ai-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.4: How various groups impact human-centered AI. Credit: <span class="citation" data-cites="schneiderman2020">Shneiderman (<a href="../../references.html#ref-schneiderman2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-schneiderman2020" class="csl-entry" role="listitem">
Shneiderman, Ben. 2020. <span>“Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-Centered AI Systems.”</span> <em>ACM Transactions on Interactive Intelligent Systems</em> 10 (December): 1–31. <a href="https://doi.org/10.1145/3419764">https://doi.org/10.1145/3419764</a>.
</div></div></figure>
</div>
<p>Among these are:</p>
<ul>
<li><p>Canada’s <a href="https://www.canada.ca/en/government/system/digital-government/digital-government-innovations/responsible-use-ai.html">Responsible Use of Artificial Intelligence</a></p></li>
<li><p>The European Union’s <a href="https://gdpr-info.eu/">General Data Protection Regulation (GDPR)</a></p></li>
<li><p>The European Commission’s <a href="https://commission.europa.eu/publications/white-paper-artificial-intelligence-european-approach-excellence-and-trust_en">White Paper on Artificial Intelligence: a European approach to excellence and trust</a></p></li>
<li><p>The UK’s Information Commissioner’s Office and Alan Turing Institute’s <a href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/explaining-decisions-made-with-artificial-intelligence">Consultation on Explaining AI Decisions Guidance</a> co-badged guidance by the individuals affected by them.</p></li>
</ul>
</section>
<section id="obtaining-quality-and-representative-data" class="level3 page-columns page-full" data-number="16.6.2">
<h3 data-number="16.6.2" class="anchored" data-anchor-id="obtaining-quality-and-representative-data"><span class="header-section-number">16.6.2</span> Obtaining Quality and Representative Data</h3>
<p>Responsible AI design must occur at all stages of the pipeline, including data collection such as those things discussed in the <a href="../../contents/data_engineering/data_engineering.html">Data Engineering</a> chapter. This begs the question; what does it mean for data to be high-quality and representative? Consider the following scenarios that <em>hinder</em> the representativeness of data:</p>
<section id="subgroup-imbalance" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="subgroup-imbalance">Subgroup Imbalance</h4>
<p>This is likely what comes to mind when hearing the phrase “representative data.” Subgroup imbalance means that the dataset contains relatively more data from one subgroup than another. This imbalance can negatively affect the downstream ML model, by causing it to overfit to a subgroup of people while having poor performance on another.</p>
<p>One example consequence of subgroup imbalance is racial discrimination in facial recognition technology <span class="citation" data-cites="buolamwini2018genderShades">(<a href="../../references.html#ref-buolamwini2018genderShades" role="doc-biblioref">Buolamwini and Gebru 2018</a>)</span>; commercial facial recognition algorithms have up to 34% worse error rates on darker-skinned females than lighter-skinned males.</p>
<div class="no-row-height column-margin column-container"><div id="ref-buolamwini2018genderShades" class="csl-entry" role="listitem">
Buolamwini, Joy, and Timnit Gebru. 2018. <span>“Gender Shades: <span>Intersectional</span> Accuracy Disparities in Commercial Gender Classification.”</span> In <em>Conference on Fairness, Accountability and Transparency</em>, 77–91. PMLR.
</div></div><p>Note that data imbalance goes both ways, and subgroups can also be harmfully <em>overrepresented</em> in the dataset. For example, the Allegheny Family Screening Tool (AFST) is used to predict the likelihood that a child will eventually be removed from a home. The AFST produces <a href="https://www.aclu.org/the-devil-is-in-the-details-interrogating-values-embedded-in-the-allegheny-family-screening-tool#4-2-the-more-data-the-better">disproportionate scores for different subgroups</a>, one of the reasons being that it is trained on historically biased data, sourced from juvenile and adult criminal legal systems, public welfare agencies, and behavioral health agencies and programs.</p>
</section>
<section id="quantifying-target-outcomes" class="level4">
<h4 class="anchored" data-anchor-id="quantifying-target-outcomes">Quantifying Target Outcomes</h4>
<p>This occurs in applications where the ground-truth label <em>cannot be measured</em> or is <em>difficult to represent</em> in a single quantity. For example, an ML model in a mobile wellness application may want to predict individual stress levels. The true stress labels themselves are impossible to obtain directly, and must be inferred from other biosignals, such as heart rate variability and user’s self-reported data. In these situations, noise is built into the data by design, making this a challenging ML task.</p>
</section>
<section id="distribution-shift" class="level4">
<h4 class="anchored" data-anchor-id="distribution-shift">Distribution Shift</h4>
<p>Data may no longer be representative of a task if a major external event causes the source of the data to change drastically. The most common way to think about distribution shift is with respect to time; for example, data on consumer shopping habits that was collected pre-covid may no longer be representative of consumer behavior today.</p>
<p>Another form of distribution shift is that caused by transfer. For instance, in applying a triage system that was trained on data from one hospital to another, distribution shift may occur if the two hospitals are very different.#</p>
</section>
<section id="gathering-data" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="gathering-data">Gathering Data</h4>
<p>A reasonable solution for many of the above problems with non-representative or low-quality data is to collect more; we can collect more data targeting an underrepresented subgroup or collect more data from the target hospital to which our model might be transferred. However, there are also reasons that gathering more data is an inappropriate or infeasible solution for the task at hand.</p>
<ul>
<li><p><em>Data collection can be harmful.</em> This is the <em>paradox of exposure</em>, the situation in which those that stand to significantly gain from their data being collected are also those that are put at risk by the collection process (<span class="citation" data-cites="d2023dataFeminism">D’ignazio and Klein (<a href="../../references.html#ref-d2023dataFeminism" role="doc-biblioref">2023</a>)</span>, Chapter 4). For example, collecting more data on non-binary individuals may be important for ensuring fairness of the ML application, but also put them at risk, depending on who is collecting the data and how (whether the data is easily identifiable, contains sensitive content, etc).</p></li>
<li><p><em>Data collection can be costly.</em> In some domains, such as in healthcare, obtaining data can be costly in terms of time and money.</p></li>
<li><p><em>Biased data collection.</em> For example, Electronic Health Records are a huge data-source for ML driven healthcare applications. Issues of subgroup representation aside, the data itself may be collected in a biased manner. For example, negative language (“nonadherent”, “unwilling”) is disproportionately used on black patients <span class="citation" data-cites="himmelstein2022examination">(<a href="../../references.html#ref-himmelstein2022examination" role="doc-biblioref">Himmelstein, Bates, and Zhou 2022</a>)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-d2023dataFeminism" class="csl-entry" role="listitem">
D’ignazio, Catherine, and Lauren F Klein. 2023. <em>Data Feminism</em>. MIT press.
</div><div id="ref-himmelstein2022examination" class="csl-entry" role="listitem">
Himmelstein, Gracie, David Bates, and Li Zhou. 2022. <span>“Examination of Stigmatizing Language in the Electronic Health Record.”</span> <em>JAMA Network Open</em> 5 (1): e2144967. <a href="https://doi.org/10.1001/jamanetworkopen.2021.44967">https://doi.org/10.1001/jamanetworkopen.2021.44967</a>.
</div></div><p>We conclude with several additional strategies for maintaining data quality: improving understanding of the data, data exploration, and intr. First, fostering a deeper understanding of the data is crucial. This can be achieved through the implementation of standardized labels and measures of data quality, such as in the <a href="https://datanutrition.org/">Data Nutrition Project</a>.</p>
<p>Directly collaborating with organizations responsible for the data collection can help ensure that the data is interpreted correctly. Second, employing effective tools for data exploration is important. Visualization techniques and statistical analyses can reveal issues with the data. Finally, establishing a feedback loop within the ML pipeline is essential for understanding the real world implications of the data. Metrics, such as fairness measures, allow us to define “data quality” in the context of the downstream application; improving fairness may directly improve the quality of the predictions that the end users receive.</p>
</section>
</section>
<section id="balancing-accuracy-and-other-objectives" class="level3" data-number="16.6.3">
<h3 data-number="16.6.3" class="anchored" data-anchor-id="balancing-accuracy-and-other-objectives"><span class="header-section-number">16.6.3</span> Balancing Accuracy and Other Objectives</h3>
<p>Machine learning models are often evaluated on accuracy alone, but this single metric cannot fully capture model performance and tradeoffs for responsible AI systems. Other ethical dimensions like fairness, robustness, interpretability and privacy may compete with pure predictive accuracy during model development. For instance, inherently interpretable models such as small decision trees or linear classifiers with simplified features intentionally trade some accuracy for transparency into the model behavior and predictions. While these simplified models achieve lower accuracy by not capturing all complexity in the dataset, improved interpretability builds trust by enabling direct analysis by human practitioners.</p>
<p>Additionally, certain techniques meant to improve adversarial robustness like adversarial training examples or dimensionality reduction can degrade accuracy on clean validation data. In sensitive applications like healthcare, focusing narrowly on state-of-the-art accuracy carries ethical risks if it allows models to rely more on spurious correlations that introduce bias or use opaque reasoning. Therefore, the appropriate performance objectives depend greatly on the sociotechnical context.</p>
<p>Methodologies like <a href="https://vsdesign.org/">Value Sensitive Design</a> provide frameworks for formally evaluating the priorities of various stakeholders within the real-world deployment system. These elucidate tensions between values like accuracy, interpretability and fairness which can then guide responsible tradeoff decisions. For a medical diagnosis system, achieving the highest accuracy may not be the singular goal - improving transparency to build practitioner trust or reducing bias towards minority groups could justify small losses in accuracy. Analyzing the sociotechnical context is key for setting these objectives.</p>
<p>By taking a holistic view, we can responsibly balance accuracy with other ethical objectives for model success. Ongoing monitoring of performance along multiple dimensions is crucial as the system evolves after deployment.</p>
</section>
</section>
<section id="ethical-considerations-in-ai-design" class="level2 page-columns page-full" data-number="16.7">
<h2 data-number="16.7" class="anchored" data-anchor-id="ethical-considerations-in-ai-design"><span class="header-section-number">16.7</span> Ethical Considerations in AI Design</h2>
<p>We must discuss at least some of the many ethical issues at stake in the design and application of AI systems and diverse frameworks for approaching these issues, including those from AI safety, Human-Computer Interaction (HCI), and Science, Technology, and Society (STS).</p>
<section id="ai-safety-and-value-alignment" class="level3 page-columns page-full" data-number="16.7.1">
<h3 data-number="16.7.1" class="anchored" data-anchor-id="ai-safety-and-value-alignment"><span class="header-section-number">16.7.1</span> AI Safety and Value Alignment</h3>
<p>In 1960, Norbert Weiner wrote, “’if we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively… we had better be quite sure that the purpose put into the machine is the purpose which we really desire” <span class="citation" data-cites="wiener1960some">(<a href="../../references.html#ref-wiener1960some" role="doc-biblioref">Wiener 1960</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wiener1960some" class="csl-entry" role="listitem">
Wiener, Norbert. 1960. <span>“Some Moral and Technical Consequences of Automation: As Machines Learn They May Develop Unforeseen Strategies at Rates That Baffle Their Programmers.”</span> <em>Science</em> 131 (3410): 1355–58. <a href="https://doi.org/10.1126/science.131.3410.1355">https://doi.org/10.1126/science.131.3410.1355</a>.
</div><div id="ref-russell2021human" class="csl-entry" role="listitem">
Russell, Stuart. 2021. <span>“Human-Compatible Artificial Intelligence.”</span> <em>Human-Like Machine Intelligence</em>, 3–23.
</div></div><p>In recent years, as the capabilities of deep learning models have achieved, and sometimes even surpassed human abilities, the issue of how to create AI systems that act in accord with human intentions instead of pursuing unintended or undesirable goals, has become a source of concern <span class="citation" data-cites="russell2021human">(<a href="../../references.html#ref-russell2021human" role="doc-biblioref">Russell 2021</a>)</span>. Within the field of AI safety, a particular goal concerns “value alignment,” or the problem of how to code the “right” purpose into machines <a href="https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf">Human-Compatible Artificial Intelligence</a>. Present AI research assumes we know the objectives we want to achieve and “studies the ability to achieve objectives, not the design of those objectives.”</p>
<p>However, complex real-world deployment contexts make explicitly defining “the right purpose” for machines difficult, requiring frameworks for responsible and ethical goal-setting. Methodologies like <a href="https://vsdesign.org/">Value Sensitive Design</a> provide formal mechanisms to surface tensions between stakeholder values and priorities.</p>
<p>By taking a holistic sociotechnical view, we can better ensure intelligent systems pursue objectives that align with broad human intentions rather than maximizing narrow metrics like accuracy alone. Achieving this in practice remains an open and critical research question as AI capabilities continue advancing rapidly.</p>
<p>The absence of this alignment can lead to a number of AI safety issues, as have been documented in a variety of <a href="https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/">deep learning models</a>. A common feature of systems that optimize for an objective, is that variables not directly included in the said objective may be set to extreme values to help optimize for that objective, leading to issues that have been characterized as specification gaming, reward hacking, etc. in reinforcement learning (RL).</p>
<p>In recent years, a particularly popular implementation of RL has been models pre-trained using self-supervised learning and fine-tuned using reinforcement learning from human feedback (RLHF) <span class="citation" data-cites="christiano2017deep">(<a href="../../references.html#ref-christiano2017deep" role="doc-biblioref">Christiano et al. 2017</a>)</span>. Ngo 2022 <span class="citation" data-cites="ngo2022alignment">(<a href="../../references.html#ref-ngo2022alignment" role="doc-biblioref">Ngo, Chan, and Mindermann 2022</a>)</span> argue that by rewarding models for appearing harmless and ethical, while also maximizing useful outcomes, RLHF could encourage the emergence of three problematic properties: situationally-aware reward hacking where policies exploit human fallibility to gain high reward, misaligned internally-represented goals that generalize beyond the RLHF fine-tuning distribution, and power-seeking strategies.</p>
<div class="no-row-height column-margin column-container"><div id="ref-christiano2017deep" class="csl-entry" role="listitem">
Christiano, Paul F., Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. <span>“Deep Reinforcement Learning from Human Preferences.”</span> In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, <span>USA</span></em>, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 4299–4307. <a href="https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</a>.
</div><div id="ref-ngo2022alignment" class="csl-entry" role="listitem">
Ngo, Richard, Lawrence Chan, and Sören Mindermann. 2022. <span>“The Alignment Problem from a Deep Learning Perspective.”</span> <em>ArXiv Preprint</em> abs/2209.00626. <a href="https://arxiv.org/abs/2209.00626">https://arxiv.org/abs/2209.00626</a>.
</div><div id="ref-amodei2016concrete" class="csl-entry" role="listitem">
Van Noorden, Richard. 2016. <span>“<span>ArXiv</span> Preprint Server Plans Multimillion-Dollar Overhaul.”</span> <em>Nature</em> 534 (7609): 602–2. <a href="https://doi.org/10.1038/534602a">https://doi.org/10.1038/534602a</a>.
</div></div><p>Similarly, <span class="citation" data-cites="amodei2016concrete">Van Noorden (<a href="../../references.html#ref-amodei2016concrete" role="doc-biblioref">2016</a>)</span> outline six concrete problems for AI safety, including avoiding negative side effects, avoiding reward hacking, scalable oversight for aspects of the objective that are too expensive to be frequently evaluated during training, safe exploration strategies that encourage creativity but while preventing harms, and robustness to distributional shift in unseen testing environments.</p>
</section>
<section id="autonomous-systems-and-control-and-trust" class="level3 page-columns page-full" data-number="16.7.2">
<h3 data-number="16.7.2" class="anchored" data-anchor-id="autonomous-systems-and-control-and-trust"><span class="header-section-number">16.7.2</span> Autonomous Systems and Control [and Trust]</h3>
<p>The consequences of autonomous systems that act independently of human oversight, and often outside of human judgment, have been well documented across a number of different industries and use cases. Most recently, the The California Department of Motor Vehicles suspended Cruise’s deployment and testing permits for its autonomous vehicles citing <a href="https://www.cnbc.com/2023/10/24/california-dmv-suspends-cruises-self-driving-car-permits.html">“unreasonable risks to public safety”</a>. One such <a href="https://www.google.com/url?q=https://www.cnbc.com/2023/10/17/cruise-under-nhtsa-probe-into-autonomous-driving-pedestrian-injuries.html&amp;sa=D&amp;source=docs&amp;ust01805725902865&amp;usg=AOvVaw2-S0v9p1_riCDeH36fEjKa">accident</a> occurred when a vehicle struck a pedestrian who stepped into a crosswalk after the stoplight had turned green, and the vehicle was allowed to proceed. In 2018, a pedestrian crossing the street with her bike was killed when a self-driving Uber car, which was operating in autonomous mode, <a href="https://www.bbc.com/news/technology-54175359">failed to accurately classify her moving body as an object to be avoided</a>.</p>
<p>Autonomous systems beyond self-driving vehicles are also susceptible to such issues, with potentially graver consequences, as remotely-powered drones are already <a href="https://www.reuters.com/technology/human-machine-teams-driven-by-ai-are-about-reshape-warfare-2023-09-08/">reshaping warfare</a>. While such incidents bring up important ethical questions regarding <a href="https://www.cigionline.org/articles/who-responsible-when-autonomous-systems-fail/">who should be held responsible</a> when these systems fail, they also highlight the technical challenges of giving full control of complex, real-world tasks to machines.</p>
<p>At its core, there is a tension between human and machine autonomy. Engineering and computer science disciplines have tended to focus on machine autonomy. For example, as of 2019, a search for the word “autonomy” in the Digital Library of the Association for Computing Machinery (ACM) reveals that of the top 100 most cited papers, 90% are on machine autonomy <span class="citation" data-cites="calvo2020supporting">(<a href="../../references.html#ref-calvo2020supporting" role="doc-biblioref">Calvo et al. 2020</a>)</span>. In an attempt to build systems for the benefit of humanity, these disciplines have taken without question increasing productivity, efficiency, and automation as primary strategies for benefiting humanity.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mccarthy1981epistemological" class="csl-entry" role="listitem">
McCarthy, John. 1981. <span>“Epistemological Problems of Artificial Intelligence.”</span> In <em>Readings in Artificial Intelligence</em>, 459–65. Elsevier. <a href="https://doi.org/10.1016/b978-0-934613-03-3.50035-0">https://doi.org/10.1016/b978-0-934613-03-3.50035-0</a>.
</div></div><p>These goals put machine automation at the forefront, often at the expense of the human. This approach suffers from inherent challenges, as noted since the early days of AI through the Frame problem and qualification problem, which formalizes the observation that is impossible to specify all the preconditions needed for a real-world action to succeed <span class="citation" data-cites="mccarthy1981epistemological">(<a href="../../references.html#ref-mccarthy1981epistemological" role="doc-biblioref">McCarthy 1981</a>)</span>.</p>
<p>These logical limitations have given rise to mathematical approaches such as Responsibility-sensitive safety (RSS) <span class="citation" data-cites="shalev2017formal">(<a href="../../references.html#ref-shalev2017formal" role="doc-biblioref">Shalev-Shwartz, Shammah, and Shashua 2017</a>)</span>, which is aimed at breaking down the end goal of an automated driving system (namely safety) into concrete and checkable conditions that can be rigorously formulated in mathematical terms. The goal of RSS is that those safety rules guarantee ADS safety in the rigorous form of mathematical proofs. However, such approaches tend towards using automation to the problems of automation and are susceptible to many of the same issues.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shalev2017formal" class="csl-entry" role="listitem">
Shalev-Shwartz, Shai, Shaked Shammah, and Amnon Shashua. 2017. <span>“On a Formal Model of Safe and Scalable Self-Driving Cars.”</span> <em>ArXiv Preprint</em> abs/1708.06374. <a href="https://arxiv.org/abs/1708.06374">https://arxiv.org/abs/1708.06374</a>.
</div><div id="ref-friedman1996value" class="csl-entry" role="listitem">
Friedman, Batya. 1996. <span>“Value-Sensitive Design.”</span> <em>Interactions</em> 3 (6): 16–23. <a href="https://doi.org/10.1145/242485.242493">https://doi.org/10.1145/242485.242493</a>.
</div><div id="ref-peters2018designing" class="csl-entry" role="listitem">
Peters, Dorian, Rafael A. Calvo, and Richard M. Ryan. 2018. <span>“Designing for Motivation, Engagement and Wellbeing in Digital Experience.”</span> <em>Front. Psychol.</em> 9: 797. <a href="https://doi.org/10.3389/fpsyg.2018.00797">https://doi.org/10.3389/fpsyg.2018.00797</a>.
</div><div id="ref-ryan2000self" class="csl-entry" role="listitem">
Ryan, Richard M., and Edward L. Deci. 2000. <span>“Self-Determination Theory and the Facilitation of Intrinsic Motivation, Social Development, and Well-Being.”</span> <em>Am. Psychol.</em> 55 (1): 68–78. <a href="https://doi.org/10.1037/0003-066x.55.1.68">https://doi.org/10.1037/0003-066x.55.1.68</a>.
</div></div><p>Another approach to combating these issues is to turn the focus towards the human-centered design of interactive systems that incorporate human control. Value-sensitive design <span class="citation" data-cites="friedman1996value">(<a href="../../references.html#ref-friedman1996value" role="doc-biblioref">Friedman 1996</a>)</span> described three key design factors for a user interface that impact autonomy, including system capability, system complexity, misrepresentation, and fluidity. A more recent model, called METUX (A Model for Motivation, Engagement, and Thriving in the User Experience) leverages insights from Self-determination Theory (SDT) in Psychology to identifies six distinct spheres of technology experience that contribute to the design systems that promote wellbeing and human flourishing <span class="citation" data-cites="peters2018designing">(<a href="../../references.html#ref-peters2018designing" role="doc-biblioref">Peters, Calvo, and Ryan 2018</a>)</span>. SDT defines autonomy as acting in accordance with one’s goals and values, which is distinct from the use of autonomy as simply a synonym for either independence or being in control <span class="citation" data-cites="ryan2000self">(<a href="../../references.html#ref-ryan2000self" role="doc-biblioref">Ryan and Deci 2000</a>)</span>.</p>
<p>Calvo 2020 elaborates on METUX and its six “spheres of technology experience” in the context of AI-recommender systems <span class="citation" data-cites="calvo2020supporting">(<a href="../../references.html#ref-calvo2020supporting" role="doc-biblioref">Calvo et al. 2020</a>)</span>. They propose these spheres – Adoption, Interface, Tasks, Behavior, Life, and Society – as a way of organizing thinking and evaluation of technology design in order to appropriately capture contradictory and downstream impacts on human autonomy when interacting with AI systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-calvo2020supporting" class="csl-entry" role="listitem">
Calvo, Rafael A, Dorian Peters, Karina Vold, and Richard M Ryan. 2020. <span>“Supporting Human Autonomy in <span>AI</span> Systems: <span>A</span> Framework for Ethical Enquiry.”</span> <em>Ethics of Digital Well-Being: A Multidisciplinary Approach</em>, 31–54.
</div></div></section>
<section id="economic-impacts-on-jobs-skills-wages" class="level3 page-columns page-full" data-number="16.7.3">
<h3 data-number="16.7.3" class="anchored" data-anchor-id="economic-impacts-on-jobs-skills-wages"><span class="header-section-number">16.7.3</span> Economic Impacts on Jobs, Skills, Wages</h3>
<p>A major concern of the current rise of AI technologies is widespread unemployment. As AI systems’ capabilities expand, many fear that these technologies will cause an absolute loss of jobs as they replace current workers and overtake alternative employment roles across industries. However, changing economic landscapes at the hands of automation are not new, and historically, have been found to reflect patterns of <em>displacement</em> rather than replacement <span class="citation" data-cites="shneiderman2022human">(<a href="../../references.html#ref-shneiderman2022human" role="doc-biblioref">Shneiderman 2022</a>)</span>—Chapter 4. In particular, automation usually lowers costs and increases quality, which greatly increases access and demand. The need to serve these growing markets pushes production, which in turn creates new jobs.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shneiderman2022human" class="csl-entry" role="listitem">
———. 2022. <em>Human-Centered <span>AI</span></em>. Oxford University Press.
</div></div><p>Furthermore, studies have found that attempts to achieve “lights-out” automation – productive and flexible automation with a minimal number of human workers – have been unsuccessful. Attempts to do so have led to what the MIT Work of the Future taskforce has termed <a href="https://hbr.org/2023/03/a-smarter-strategy-for-using-robots">“zero-sum automation”</a>, in which process flexibility is sacrificed for increased productivity.</p>
<p>In contrast, the taskforce propose a “positive-sum automation” approach in which flexibility is increased by designing technology that strategically incorporates humans where they are very much needed: making it easier for line employees to train and debug robots; using a bottom-up approach to identifying what tasks should be automated; and choosing the right metrics for measuring success (see MIT’s <a href="https://workofthefuture-mit-edu.ezp-prod1.hul.harvard.edu/wp-content/uploads/2021/01/2020-Final-Report4.pdf">Work of the Future</a>).</p>
<p>However, the optimism of the high-level outlook does not preclude individual harms, especially to those whose skills and jobs will be rendered obsolete by automation. Public and legislative pressure as well as corporate social responsibility efforts will need to be directed to create policies that share the benefits of automation with workers and result in higher minimum wages and benefits.</p>
</section>
<section id="scientific-communication-and-ai-literacy" class="level3 page-columns page-full" data-number="16.7.4">
<h3 data-number="16.7.4" class="anchored" data-anchor-id="scientific-communication-and-ai-literacy"><span class="header-section-number">16.7.4</span> Scientific Communication and AI Literacy</h3>
<p>A 1993 survey of 3000 North American adults’ beliefs about the “electronic thinking machine” revealed two primary perspectives of the early computer: the “beneficial tool of man” perspective and the “awesome thinking machine” perspective. The attitudes contributing to the “awesome thinking machine” view in this and other studies, revealed a characterization of computers as “intelligent brains, smarter than people, unlimited, fast, mysterious, and frightening” <span class="citation" data-cites="martin1993myth">(<a href="../../references.html#ref-martin1993myth" role="doc-biblioref">Martin 1993</a>)</span>. These fears highlight an easily overlooked component of responsible AI, especially amidst the rush to commercialize such technologies: scientific communication that accurately communicates the capabilities <em>and</em> limitations of these systems, while providing transparency about the limitations of experts’ knowledge about these systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-martin1993myth" class="csl-entry" role="listitem">
Martin, C. Dianne. 1993. <span>“The Myth of the Awesome Thinking Machine.”</span> <em>Commun. ACM</em> 36 (4): 120–33. <a href="https://doi.org/10.1145/255950.153587">https://doi.org/10.1145/255950.153587</a>.
</div><div id="ref-handlin1965science" class="csl-entry" role="listitem">
Handlin, Oscar. 1965. <span>“Science and Technology in Popular Culture.”</span> <em>Daedalus-Us.</em>, 156–70.
</div></div><p>As AI systems capabilities continue to expand beyond most people’s comprehension, there is a natural tendency to assume the kinds of apocalyptic worlds painted by our media. This is in part due to the apparent difficulty of assimilating scientific information, even in technologically advanced cultures, which leads to the products of science being perceived as magic - “understandable only in terms of what it did, not how it worked” <span class="citation" data-cites="handlin1965science">(<a href="../../references.html#ref-handlin1965science" role="doc-biblioref">Handlin 1965</a>)</span>.</p>
<p>While tech companies should be held responsible for limiting grandiose claims and not falling into cycles of hype, research studying scientific communication, especially with respect to (generative) AI, will also be useful in tracking and correcting public understanding of these technologies. An analysis of the Scopus scholarly database found that such research is scarce, with only a handful of papers mentioning both “science communication” and “artificial intelligence” <span class="citation" data-cites="schafer2023notorious">(<a href="../../references.html#ref-schafer2023notorious" role="doc-biblioref">Schäfer 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-schafer2023notorious" class="csl-entry" role="listitem">
Schäfer, Mike S. 2023. <span>“The Notorious <span>GPT:</span> <span>Science</span> Communication in the Age of Artificial Intelligence.”</span> <em>Journal of Science Communication</em> 22 (02): Y02. <a href="https://doi.org/10.22323/2.22020402">https://doi.org/10.22323/2.22020402</a>.
</div><div id="ref-lindgren2023handbook" class="csl-entry" role="listitem">
Lindgren, Simon. 2023. <em>Handbook of Critical Studies of Artificial Intelligence</em>. Edward Elgar Publishing.
</div><div id="ref-ng2021ai" class="csl-entry" role="listitem">
Ng, Davy Tsz Kit, Jac Ka Lok Leung, Kai Wah Samuel Chu, and Maggie Shen Qiao. 2021. <span>“<span>AI</span> Literacy: <span>Definition,</span> Teaching, Evaluation and Ethical Issues.”</span> <em>Proceedings of the Association for Information Science and Technology</em> 58 (1): 504–9.
</div></div><p>Research that exposes the perspectives, frames, and images of the future that are promoted by academic institutions, tech companies, stakeholders, regulators, journalists, NGOs and others will also help to identify potential gaps in AI literacy among adults <span class="citation" data-cites="lindgren2023handbook">(<a href="../../references.html#ref-lindgren2023handbook" role="doc-biblioref">Lindgren 2023</a>)</span>. Increased focus on AI literacy from all stakeholders will be an important tool in helping people whose skills are rendered obsolete by AI automation <span class="citation" data-cites="ng2021ai">(<a href="../../references.html#ref-ng2021ai" role="doc-biblioref">Ng et al. 2021</a>)</span>.</p>
<p><em>“But even those who never acquire that understanding need assurance that there is a connection between the goals of science and their own welfare, and above all, that the scientist is not a man altogether apart but one who shares some of their own value.”</em> (Handlin, 1965)</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="16.8">
<h2 data-number="16.8" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">16.8</span> Conclusion</h2>
<p>Responsible artificial intelligence is crucial as machine learning systems exert growing influence across sectors like healthcare, employment, finance, and criminal justice. While AI promises immense benefits, thoughtlessly designed models risk perpetrating harm through biases, privacy violations, unintended behaviors, and other pitfalls.</p>
<p>Upholding principles of fairness, explainability, accountability, safety, and transparency enables developing ethical AI aligned with human values. However, putting these principles into practice involves surmounting complex technical and social challenges around detecting dataset biases, choosing appropriate model tradeoffs, securing quality training data, and more. Frameworks like value-sensitive design provide guidance on balancing accuracy versus other objectives based on stakeholder needs.</p>
<p>Looking forward, advancing responsible AI necessitates continued research and industry commitment. More standardized benchmarks are required for comparing model biases and robustness. Enabling efficient transparency and user control for edge devices warrants focus as personalized TinyML expands. Revised incentive structures and policies must encourage deliberate, ethical development before reckless deployment. Education around AI literacy and limitations will further responsible public understanding.</p>
<p>Responsible methods underscore that while machine learning offers immense potential, thoughtless application risks adverse consequences. Cross-disciplinary collaboration and human-centered design is imperative so AI can promote broad social benefit. The path ahead lies not in an arbitrary checklist but a steadfast commitment at each step to understand and uphold our ethical responsibility. By taking conscientious action, the machine learning community can lead AI toward empowering all people equitably and safely.</p>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/privacy_security/privacy_security.html" class="pagination-link  aria-label=" &lt;span="" &amp;="" privacy&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/sustainable_ai/sustainable_ai.html" class="pagination-link" aria-label="<span class='chapter-number'>17</span>&nbsp; <span class='chapter-title'>Sustainable AI</span>">
        <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Edited by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/responsible_ai/responsible_ai.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/responsible_ai/responsible_ai.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>