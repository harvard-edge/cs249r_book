<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="keywords" content="open-source, embedded systems, machine learning, tinyML">

<title>Embedded AI: Principles, Algorithms, and Applications - 3&nbsp; Deep Learning Primer</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./embedded_ml.html" rel="next">
<link href="./embedded_sys.html" rel="prev">
<link href="./cover.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./dl_primer.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Embedded AI: Principles, Algorithms, and Applications</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/harvard-edge/cs249r_book" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Embedded-AI--Principles,-Algorithms,-and-Applications.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Embedded-AI--Principles,-Algorithms,-and-Applications.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./front.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">FRONT MATTER</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dedication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedication</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./copyright.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./embedded_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Embedded Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dl_primer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./embedded_ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Embedded AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ondevice_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">On-Device AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Embedded AIOps</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Privacy and Security</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">AI Sustainability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./zoo_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./zoo_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learning_resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./community.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Communities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./case_studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">3.1</span> Overview</a>
  <ul class="collapse">
  <li><a href="#definition-and-importance" id="toc-definition-and-importance" class="nav-link" data-scroll-target="#definition-and-importance"><span class="header-section-number">3.1.1</span> Definition and Importance</a></li>
  <li><a href="#brief-history-of-deep-learning" id="toc-brief-history-of-deep-learning" class="nav-link" data-scroll-target="#brief-history-of-deep-learning"><span class="header-section-number">3.1.2</span> Brief History of Deep Learning</a></li>
  <li><a href="#applications-of-deep-learning" id="toc-applications-of-deep-learning" class="nav-link" data-scroll-target="#applications-of-deep-learning"><span class="header-section-number">3.1.3</span> Applications of Deep Learning</a></li>
  <li><a href="#relevance-to-embedded-ai" id="toc-relevance-to-embedded-ai" class="nav-link" data-scroll-target="#relevance-to-embedded-ai"><span class="header-section-number">3.1.4</span> Relevance to Embedded AI</a></li>
  </ul></li>
  <li><a href="#neural-networks" id="toc-neural-networks" class="nav-link" data-scroll-target="#neural-networks"><span class="header-section-number">3.2</span> Neural Networks</a>
  <ul class="collapse">
  <li><a href="#perceptrons" id="toc-perceptrons" class="nav-link" data-scroll-target="#perceptrons"><span class="header-section-number">3.2.1</span> Perceptrons</a></li>
  <li><a href="#multi-layer-perceptrons" id="toc-multi-layer-perceptrons" class="nav-link" data-scroll-target="#multi-layer-perceptrons"><span class="header-section-number">3.2.2</span> Multi-layer Perceptrons</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">3.2.3</span> Activation Functions</a></li>
  <li><a href="#computational-graphs" id="toc-computational-graphs" class="nav-link" data-scroll-target="#computational-graphs"><span class="header-section-number">3.2.4</span> Computational Graphs</a></li>
  <li><a href="#model-architectures" id="toc-model-architectures" class="nav-link" data-scroll-target="#model-architectures"><span class="header-section-number">3.2.5</span> Model Architectures</a></li>
  <li><a href="#traditional-ml-vs-deep-learning" id="toc-traditional-ml-vs-deep-learning" class="nav-link" data-scroll-target="#traditional-ml-vs-deep-learning"><span class="header-section-number">3.2.6</span> Traditional ML vs Deep Learning</a></li>
  <li><a href="#choosing-traditional-ml-vs.-dl" id="toc-choosing-traditional-ml-vs.-dl" class="nav-link" data-scroll-target="#choosing-traditional-ml-vs.-dl"><span class="header-section-number">3.2.7</span> Choosing Traditional ML vs.&nbsp;DL</a></li>
  <li><a href="#making-an-informed-choice" id="toc-making-an-informed-choice" class="nav-link" data-scroll-target="#making-an-informed-choice"><span class="header-section-number">3.2.8</span> Making an Informed Choice</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">3.3</span> Conclusion</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/harvard-edge/cs249r_book/edit/main/dl_primer.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/harvard-edge/cs249r_book/blob/main/dl_primer.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="overview" class="level2 page-columns page-full" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">3.1</span> Overview</h2>
<section id="definition-and-importance" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="definition-and-importance"><span class="header-section-number">3.1.1</span> Definition and Importance</h3>
<p>Deep learning, a specialized area within machine learning and artificial intelligence (AI), utilizes algorithms modeled after the structure and function of the human brain, known as artificial neural networks. This field is a foundational element in AI, driving progress in diverse sectors such as computer vision, natural language processing, and self-driving vehicles. Its significance in embedded AI systems is highlighted by its capability to handle intricate calculations and predictions, optimizing the limited resources in embedded settings.</p>
<p><img src="https://1394217531-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LvBP1svpACTB1R1x_U4%2F-LvCh0IFvnfX-S1za_GI%2F-LvD0gbfAKEIMXcVxdqQ%2Fimage.png?alt=media&amp;token=d6ca58f0-ebe3-4188-a90a-dc68256e1b0a" class="img-fluid"></p>
</section>
<section id="brief-history-of-deep-learning" class="level3 page-columns page-full" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="brief-history-of-deep-learning"><span class="header-section-number">3.1.2</span> Brief History of Deep Learning</h3>
<p>The idea of deep learning has origins in early artificial neural networks. It has experienced several cycles of interest, starting with the introduction of the Perceptron in the 1950s <span class="citation" data-cites="rosenblatt1957perceptron">(<a href="references.html#ref-rosenblatt1957perceptron" role="doc-biblioref">Rosenblatt 1957</a>)</span>, followed by the invention of backpropagation algorithms in the 1980s <span class="citation" data-cites="rumelhart1986learning">(<a href="references.html#ref-rumelhart1986learning" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rosenblatt1957perceptron" class="csl-entry" role="listitem">
Rosenblatt, Frank. 1957. <em>The Perceptron, a Perceiving and Recognizing Automaton Project Para</em>. Cornell Aeronautical Laboratory.
</div><div id="ref-rumelhart1986learning" class="csl-entry" role="listitem">
Rumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36.
</div><div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“Imagenet Classification with Deep Convolutional Neural Networks.”</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div></div><p>The term “deep learning” became prominent in the 2000s, characterized by advances in computational power and data accessibility. Important milestones include the successful training of deep networks like AlexNet <span class="citation" data-cites="krizhevsky2012imagenet">(<a href="references.html#ref-krizhevsky2012imagenet" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2012</a>)</span> by <a href="https://amturing.acm.org/award_winners/hinton_4791679.cfm">Geoffrey Hinton</a>, a leading figure in AI, and the renewed focus on neural networks as effective tools for data analysis and modeling.</p>
<p>In recent times, deep learning has seen exponential growth, transforming various industries. Computational growth followed an 18-month doubling pattern from 1952 to 2010, which then accelerated to a 6-month cycle from 2010 to 2022, as shown in <a href="#fig-trends">Figure&nbsp;<span>3.1</span></a>. Concurrently, we saw the emergence of large-scale models between 2015 and 2022, appearing 2 to 3 orders of magnitude faster and following a 10-month doubling cycle.</p>
<div id="fig-trends" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://epochai.org/assets/images/posts/2022/compute-trends.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3.1: Growth of deep learning models.</figcaption>
</figure>
</div>
<p>Multiple factors have contributed to this surge, including advancements in computational power, the abundance of big data, and improvements in algorithmic designs. First, the growth of computational capabilities, especially the arrival of Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) <span class="citation" data-cites="jouppi2017datacenter">(<a href="references.html#ref-jouppi2017datacenter" role="doc-biblioref">Jouppi et al. 2017</a>)</span>, has significantly sped up the training and inference times of deep learning models. These hardware improvements have enabled the construction and training of more complex, deeper networks than what was possible in earlier years.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jouppi2017datacenter" class="csl-entry" role="listitem">
Jouppi, Norman P, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. <span>“In-Datacenter Performance Analysis of a Tensor Processing Unit.”</span> In <em>Proceedings of the 44th Annual International Symposium on Computer Architecture</em>, 1–12.
</div></div><p>Second, the digital revolution has yielded a wealth of big data, offering rich material for deep learning models to learn from and excel in tasks such as image and speech recognition, language translation, and game playing. The presence of large, labeled datasets has been key in refining and successfully deploying deep learning applications in real-world settings.</p>
<p>Additionally, collaborations and open-source efforts have nurtured a dynamic community of researchers and practitioners, accelerating advancements in deep learning techniques. Innovations like deep reinforcement learning, transfer learning, and generative adversarial networks have broadened the scope of what is achievable with deep learning, opening new possibilities in various sectors including healthcare, finance, transportation, and entertainment.</p>
<p>Organizations around the world recognize the transformative potential of deep learning and are investing heavily in research and development to leverage its capabilities in providing innovative solutions, optimizing operations, and creating new business opportunities. As deep learning continues its upward trajectory, it is set to redefine how we interact with technology, enhancing convenience, safety, and connectivity in our lives.</p>
</section>
<section id="applications-of-deep-learning" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="applications-of-deep-learning"><span class="header-section-number">3.1.3</span> Applications of Deep Learning</h3>
<p>Deep learning finds extensive use across numerous industries today. In finance, it is employed for stock market prediction, risk assessment, and fraud detection. In marketing, it is used for customer segmentation, personalization, and content optimization. In healthcare, machine learning aids in diagnosis, treatment planning, and patient monitoring. The transformative impact on society is evident.</p>
<p>For instance, deep learning algorithms can predict stock market trends, guiding investment strategies and enhancing financial decisions. Similarly, in healthcare, deep learning can make medical predictions that improve patient diagnosis and save lives. The benefits are clear: machine learning not only predicts with greater accuracy than humans but also does so much more quickly.</p>
<p>In manufacturing, deep learning has had a significant impact. By continuously learning from vast amounts of data collected during the manufacturing process, companies can boost productivity while minimizing waste through improved efficiency. This financial benefit for companies translates to better quality products at lower prices for customers. Machine learning enables manufacturers to continually refine their processes, producing higher quality goods more efficiently than ever before.</p>
<p>Deep learning also enhances everyday products like Netflix recommendations and Google Translate text translations. Moreover, it helps companies like Amazon and Uber reduce customer service costs by swiftly identifying dissatisfied customers.</p>
</section>
<section id="relevance-to-embedded-ai" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="relevance-to-embedded-ai"><span class="header-section-number">3.1.4</span> Relevance to Embedded AI</h3>
<p>Embedded AI, the integration of AI algorithms directly into hardware devices, naturally gains from the capabilities of deep learning. The combination of deep learning algorithms and embedded systems has laid the groundwork for intelligent, autonomous devices capable of advanced on-device data processing and analysis. Deep learning aids in extracting complex patterns and information from input data, serving as an essential tool in the development of smart embedded systems, from household appliances to industrial machinery. This collaboration aims to usher in a new era of intelligent, interconnected devices that can learn and adapt to user behavior and environmental conditions, optimizing performance and offering unprecedented levels of convenience and efficiency.</p>
</section>
</section>
<section id="neural-networks" class="level2 page-columns page-full" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="neural-networks"><span class="header-section-number">3.2</span> Neural Networks</h2>
<p>Deep learning draws inspiration from the neural networks of the human brain to create patterns used in decision-making. This section delves into the foundational concepts that make up deep learning, providing insights into the more complex topics discussed later in this primer.</p>
<p>Neural networks serve as the foundation of deep learning, inspired by the biological neural networks in the human brain to process and analyze data hierarchically. Below, we examine the primary components and structures commonly found in neural networks.</p>
<section id="perceptrons" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="perceptrons"><span class="header-section-number">3.2.1</span> Perceptrons</h3>
<p>The perceptron is the basic unit or node that serves as the foundation for more complex structures. A perceptron takes various inputs, applies weights and a bias to these inputs, and then uses an activation function to produce an output.</p>
<div id="fig-perceptron" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Rosenblattperceptron.png/500px-Rosenblattperceptron.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3.2: Perceptron</figcaption>
</figure>
</div>
<p>Conceived in the 1950s, perceptrons paved the way for the development of more intricate neural networks and have been a fundamental building block in the field of deep learning.</p>
</section>
<section id="multi-layer-perceptrons" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="multi-layer-perceptrons"><span class="header-section-number">3.2.2</span> Multi-layer Perceptrons</h3>
<p>Multi-layer perceptrons (MLPs) are an evolution of the single-layer perceptron model, featuring multiple layers of nodes connected in a feedforward manner. These layers include an input layer for data reception, several hidden layers for data processing, and an output layer for final result generation. MLPs are skilled at identifying non-linear relationships and use a backpropagation technique for training, where weights are optimized through a gradient descent algorithm.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.nomidl.com/wp-content/uploads/2022/04/image-7.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Multilayer Perceptron</figcaption>
</figure>
</div>
</section>
<section id="activation-functions" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">3.2.3</span> Activation Functions</h3>
<p>Activation functions are crucial components in neural networks, providing the mathematical equations that determine a network’s output. These functions introduce non-linearity into the network, enabling the learning of complex patterns. Popular activation functions include the sigmoid, tanh, and ReLU (Rectified Linear Unit) functions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://1394217531-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LvBP1svpACTB1R1x_U4%2F-LvNWUoWieQqaGmU_gl9%2F-LvO3qs2RImYjpBE8vln%2Factivation-functions3.jpg?alt=media&amp;token=f96a3007-5888-43c3-a256-2dafadd5df7c" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">Activation Function</figcaption>
</figure>
</div>
</section>
<section id="computational-graphs" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="computational-graphs"><span class="header-section-number">3.2.4</span> Computational Graphs</h3>
<p>Deep learning uses computational graphs to represent the various operations and their interactions within a neural network. This subsection explores the key phases of computational graph processing.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/tensorflow/docs/blob/master/site/en/guide/images/intro_to_graphs/two-layer-network.png?raw=1" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="figure-caption">TensorFlow Computational Graph</figcaption>
</figure>
</div>
<section id="forward-pass" class="level4" data-number="3.2.4.1">
<h4 data-number="3.2.4.1" class="anchored" data-anchor-id="forward-pass"><span class="header-section-number">3.2.4.1</span> Forward Pass</h4>
<p>The forward pass is the initial phase where data moves through the network from the input to the output layer. During this phase, each layer performs specific computations on the input data, using weights and biases before passing the resulting values to subsequent layers. The final output of this phase is used to compute the loss, indicating the difference between the predicted output and actual target values.</p>
</section>
<section id="backward-pass-backpropagation" class="level4" data-number="3.2.4.2">
<h4 data-number="3.2.4.2" class="anchored" data-anchor-id="backward-pass-backpropagation"><span class="header-section-number">3.2.4.2</span> Backward Pass (Backpropagation)</h4>
<p>Backpropagation is a key algorithm in training deep neural networks. This phase involves calculating the gradient of the loss function concerning each weight by using the chain rule, effectively moving backward through the network. The gradients calculated in this step guide the adjustment of weights with the objective of minimizing the loss function, thereby enhancing the network’s performance with each iteration of training.</p>
<p>Grasping these foundational concepts paves the way to understanding more intricate deep learning architectures and techniques, fostering the development of more sophisticated and efficacious applications, especially within the realm of embedded AI systems.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/aircAruvnKk?si=qfkBf8MJjC2WSyw3" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
</section>
<section id="model-architectures" class="level3 page-columns page-full" data-number="3.2.5">
<h3 data-number="3.2.5" class="anchored" data-anchor-id="model-architectures"><span class="header-section-number">3.2.5</span> Model Architectures</h3>
<p>Deep learning architectures refer to the various structured approaches that dictate how neurons and layers are organized and interact in neural networks. These architectures have evolved to tackle different problems and data types effectively. This section offers an overview of some well-known deep learning architectures and their characteristics.</p>
<section id="multi-layer-perceptrons-mlps" class="level4" data-number="3.2.5.1">
<h4 data-number="3.2.5.1" class="anchored" data-anchor-id="multi-layer-perceptrons-mlps"><span class="header-section-number">3.2.5.1</span> Multi-Layer Perceptrons (MLPs)</h4>
<p>MLPs are basic deep learning architectures, comprising three or more layers: an input layer, one or more hidden layers, and an output layer. These layers are fully connected, meaning each neuron in a layer is linked to every neuron in the preceding and following layers. MLPs can model intricate functions and are used in a broad array of tasks, such as regression, classification, and pattern recognition. Their capacity to learn non-linear relationships through backpropagation makes them a versatile instrument in the deep learning toolkit.</p>
<p>In embedded AI systems, MLPs can function as compact models for simpler tasks like sensor data analysis or basic pattern recognition, where computational resources are limited. Their ability to learn non-linear relationships with relatively less complexity makes them a suitable choice for embedded systems.</p>
</section>
<section id="convolutional-neural-networks-cnns" class="level4" data-number="3.2.5.2">
<h4 data-number="3.2.5.2" class="anchored" data-anchor-id="convolutional-neural-networks-cnns"><span class="header-section-number">3.2.5.2</span> Convolutional Neural Networks (CNNs)</h4>
<p>CNNs are mainly used in image and video recognition tasks. This architecture employs convolutional layers that apply a series of filters to the input data to identify features like edges, corners, and textures. A typical CNN also includes pooling layers to reduce the spatial dimensions of the data, and fully connected layers for classification. CNNs have proven highly effective in tasks such as image recognition, object detection, and computer vision applications.</p>
<p>In embedded AI, CNNs are crucial for image and video recognition tasks, where real-time processing is often needed. They can be optimized for embedded systems by using techniques like quantization and pruning to minimize memory usage and computational demands, enabling efficient object detection and facial recognition functionalities in devices with limited computational resources.</p>
</section>
<section id="recurrent-neural-networks-rnns" class="level4" data-number="3.2.5.3">
<h4 data-number="3.2.5.3" class="anchored" data-anchor-id="recurrent-neural-networks-rnns"><span class="header-section-number">3.2.5.3</span> Recurrent Neural Networks (RNNs)</h4>
<p>RNNs are suitable for sequential data analysis, like time series forecasting and natural language processing. In this architecture, connections between nodes form a directed graph along a temporal sequence, allowing information to be carried across sequences through hidden state vectors. Variants of RNNs include Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), designed to capture longer dependencies in sequence data.</p>
<p>In embedded systems, these networks can be used in voice recognition systems, predictive maintenance, or in IoT devices where sequential data patterns are common. Optimizations specific to embedded platforms can assist in managing their typically high computational and memory requirements.</p>
</section>
<section id="generative-adversarial-networks-gans" class="level4 page-columns page-full" data-number="3.2.5.4">
<h4 data-number="3.2.5.4" class="anchored" data-anchor-id="generative-adversarial-networks-gans"><span class="header-section-number">3.2.5.4</span> Generative Adversarial Networks (GANs)</h4>
<p>GANs consist of two networks, a generator and a discriminator, trained simultaneously through adversarial training <span class="citation" data-cites="goodfellow2020generative">(<a href="references.html#ref-goodfellow2020generative" role="doc-biblioref">Goodfellow et al. 2020</a>)</span>. The generator produces data that tries to mimic the real data distribution, while the discriminator aims to distinguish between real and generated data. GANs are widely used in image generation, style transfer, and data augmentation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-goodfellow2020generative" class="csl-entry" role="listitem">
Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. <span>“Generative Adversarial Networks.”</span> <em>Communications of the ACM</em> 63 (11): 139–44.
</div></div><p>In embedded settings, GANs could be used for on-device data augmentation to enhance the training of models directly on the embedded device, enabling continual learning and adaptation to new data without the need for cloud computing resources.</p>
</section>
<section id="autoencoders" class="level4 page-columns page-full" data-number="3.2.5.5">
<h4 data-number="3.2.5.5" class="anchored" data-anchor-id="autoencoders"><span class="header-section-number">3.2.5.5</span> Autoencoders</h4>
<p>Autoencoders are neural networks used for data compression and noise reduction <span class="citation" data-cites="bank2023autoencoders">(<a href="references.html#ref-bank2023autoencoders" role="doc-biblioref">Bank, Koenigstein, and Giryes 2023</a>)</span>. They are structured to encode input data into a lower-dimensional representation and then decode it back to its original form. Variants like Variational Autoencoders (VAEs) introduce probabilistic layers that allow for generative properties, finding applications in image generation and anomaly detection.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bank2023autoencoders" class="csl-entry" role="listitem">
Bank, Dor, Noam Koenigstein, and Raja Giryes. 2023. <span>“Autoencoders.”</span> <em>Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook</em>, 353–74.
</div></div><p>Using autoencoders can help in efficient data transmission and storage, improving the overall performance of embedded systems with limited computational and memory resources.</p>
</section>
<section id="transformer-networks" class="level4 page-columns page-full" data-number="3.2.5.6">
<h4 data-number="3.2.5.6" class="anchored" data-anchor-id="transformer-networks"><span class="header-section-number">3.2.5.6</span> Transformer Networks</h4>
<p>Transformer networks have emerged as a powerful architecture, especially in natural language processing <span class="citation" data-cites="vaswani2017attention">(<a href="references.html#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span>. These networks use self-attention mechanisms to weigh the influence of different input words on each output word, enabling parallel computation and capturing intricate patterns in data. Transformer networks have led to state-of-the-art results in tasks like language translation, summarization, and text generation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div></div><p>These networks can be optimized to perform language-related tasks directly on-device. For example, transformers can be used in embedded systems for real-time translation services or voice-assisted interfaces, where latency and computational efficiency are crucial. Techniques such as model distillation can be employed to deploy these networks on embedded devices with limited resources.</p>
<p>Each of these architectures serves specific purposes and excels in different domains, offering a rich toolkit for addressing diverse problems in the realm of embedded AI systems. Understanding the nuances of these architectures is crucial in designing effective and efficient deep learning models for various applications.</p>
</section>
</section>
<section id="traditional-ml-vs-deep-learning" class="level3" data-number="3.2.6">
<h3 data-number="3.2.6" class="anchored" data-anchor-id="traditional-ml-vs-deep-learning"><span class="header-section-number">3.2.6</span> Traditional ML vs Deep Learning</h3>
<p>To succinctly highlight the differences, a comparative table illustrates the contrasting characteristics between traditional ML and deep learning:</p>
<table class="table">
<colgroup>
<col style="width: 20%">
<col style="width: 38%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Traditional ML</th>
<th>Deep Learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Data Requirements</td>
<td>Low to Moderate (efficient with smaller datasets)</td>
<td>High (requires large datasets for nuanced learning)</td>
</tr>
<tr class="even">
<td>Model Complexity</td>
<td>Moderate (suitable for well-defined problems)</td>
<td>High (detects intricate patterns, suited for complex tasks)</td>
</tr>
<tr class="odd">
<td>Computational Resources</td>
<td>Low to Moderate (cost-effective, less resource-intensive)</td>
<td>High (demands substantial computational power and resources)</td>
</tr>
<tr class="even">
<td>Deployment Speed</td>
<td>Fast (quicker training and deployment cycles)</td>
<td>Slow (prolonged training times, especially with larger datasets)</td>
</tr>
<tr class="odd">
<td>Interpretability</td>
<td>High (clear insights into decision pathways)</td>
<td>Low (complex layered structures, “black box” nature)</td>
</tr>
<tr class="even">
<td>Maintenance</td>
<td>Easier (simple to update and maintain)</td>
<td>Complex (requires more efforts in maintenance and updates)</td>
</tr>
</tbody>
</table>
</section>
<section id="choosing-traditional-ml-vs.-dl" class="level3" data-number="3.2.7">
<h3 data-number="3.2.7" class="anchored" data-anchor-id="choosing-traditional-ml-vs.-dl"><span class="header-section-number">3.2.7</span> Choosing Traditional ML vs.&nbsp;DL</h3>
<section id="data-availability-and-volume" class="level4" data-number="3.2.7.1">
<h4 data-number="3.2.7.1" class="anchored" data-anchor-id="data-availability-and-volume"><span class="header-section-number">3.2.7.1</span> Data Availability and Volume</h4>
<ul>
<li><p><strong>Amount of Data</strong>: Traditional machine learning algorithms, such as decision trees or Naive Bayes, are often more suitable when data availability is limited, offering robust predictions even with smaller datasets. This is particularly true in cases like medical diagnostics for disease prediction and customer segmentation in marketing.</p></li>
<li><p><strong>Data Diversity and Quality</strong>: Traditional machine learning algorithms are flexible in handling various data types and often require less preprocessing compared to deep learning models. They may also be more robust in situations with noisy data.</p></li>
</ul>
</section>
<section id="complexity-of-the-problem" class="level4" data-number="3.2.7.2">
<h4 data-number="3.2.7.2" class="anchored" data-anchor-id="complexity-of-the-problem"><span class="header-section-number">3.2.7.2</span> Complexity of the Problem</h4>
<ul>
<li><p><strong>Problem Granularity</strong>: Problems that are simple to moderately complex, which may involve linear or polynomial relationships between variables, often find a better fit with traditional machine learning methods.</p></li>
<li><p><strong>Hierarchical Feature Representation</strong>: Deep learning models are excellent in tasks that require hierarchical feature representation, such as image and speech recognition. However, not all problems require this level of complexity, and traditional machine learning algorithms may sometimes offer simpler and equally effective solutions.</p></li>
</ul>
</section>
<section id="hardware-and-computational-resources" class="level4" data-number="3.2.7.3">
<h4 data-number="3.2.7.3" class="anchored" data-anchor-id="hardware-and-computational-resources"><span class="header-section-number">3.2.7.3</span> Hardware and Computational Resources</h4>
<ul>
<li><p><strong>Resource Constraints</strong>: The availability of computational resources often influences the choice between traditional ML and deep learning. The former is generally less resource-intensive and thus preferable in environments with hardware limitations or budget constraints.</p></li>
<li><p><strong>Scalability and Speed</strong>: Traditional machine learning algorithms, like support vector machines (SVM), often allow for faster training times and easier scalability, particularly beneficial in projects with tight timelines and growing data volumes.</p></li>
</ul>
</section>
<section id="regulatory-compliance" class="level4" data-number="3.2.7.4">
<h4 data-number="3.2.7.4" class="anchored" data-anchor-id="regulatory-compliance"><span class="header-section-number">3.2.7.4</span> Regulatory Compliance</h4>
<p>Regulatory compliance is crucial in various industries, requiring adherence to guidelines and best practices such as the GDPR in the EU. Traditional ML models, due to their inherent interpretability, often align better with these regulations, especially in sectors like finance and healthcare.</p>
</section>
<section id="interpretability" class="level4" data-number="3.2.7.5">
<h4 data-number="3.2.7.5" class="anchored" data-anchor-id="interpretability"><span class="header-section-number">3.2.7.5</span> Interpretability</h4>
<p>Understanding the decision-making process is easier with traditional machine learning techniques compared to deep learning models, which function as “black boxes,” making it challenging to trace decision pathways.</p>
</section>
</section>
<section id="making-an-informed-choice" class="level3" data-number="3.2.8">
<h3 data-number="3.2.8" class="anchored" data-anchor-id="making-an-informed-choice"><span class="header-section-number">3.2.8</span> Making an Informed Choice</h3>
<p>Given the constraints of embedded AI systems, understanding the differences between traditional ML techniques and deep learning becomes essential. Both avenues offer unique advantages, and their distinct characteristics often dictate the choice of one over the other in different scenarios.</p>
<p>Despite this, deep learning has been steadily outperforming traditional machine learning methods in several key areas due to a combination of abundant data, computational advancements, and proven effectiveness in complex tasks.</p>
<p>Here are some specific reasons why we focus on deep learning in this text:</p>
<ol type="1">
<li><p><strong>Superior Performance in Complex Tasks</strong>: Deep learning models, particularly deep neural networks, excel in tasks where the relationships between data points are incredibly intricate. Tasks like image and speech recognition, language translation, and playing complex games like Go and Chess have seen significant advancements primarily through deep learning algorithms.</p></li>
<li><p><strong>Efficient Handling of Unstructured Data</strong>: Unlike traditional machine learning methods, deep learning can process unstructured data more effectively. This is crucial in today’s data landscape, where a large majority of data is unstructured, such as text, images, and videos.</p></li>
<li><p><strong>Leveraging Big Data</strong>: With the availability of big data, deep learning models have the capacity to continually learn and improve. These models excel at utilizing large datasets to enhance their predictive accuracy, a limitation in traditional machine learning approaches.</p></li>
<li><p><strong>Hardware Advancements and Parallel Computing</strong>: The advent of powerful GPUs and the availability of cloud computing platforms have enabled the rapid training of deep learning models. These advancements have addressed one of the significant challenges of deep learning—the need for substantial computational resources.</p></li>
<li><p><strong>Dynamic Adaptability and Continuous Learning</strong>: Deep learning models can adapt to new information or data dynamically. They can be trained to generalize their learning to new, unseen data, which is crucial in rapidly evolving fields like autonomous driving or real-time language translation.</p></li>
</ol>
<p>While deep learning has gained significant traction, it’s essential to understand that traditional machine learning is far from obsolete. As we delve deeper into the intricacies of deep learning, we will also highlight situations where traditional machine learning methods may be more appropriate due to their simplicity, efficiency, and interpretability. By focusing on deep learning in this text, we aim to equip readers with the knowledge and tools needed to tackle modern, complex problems across various domains, while also providing insights into the comparative advantages and appropriate application scenarios for both deep learning and traditional machine learning techniques.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">3.3</span> Conclusion</h2>
<p>Deep learning has risen as a potent set of techniques for addressing intricate pattern recognition and prediction challenges. Starting with an overview, we outlined the fundamental concepts and principles governing deep learning, laying the groundwork for more advanced studies.</p>
<p>Central to deep learning, we explored the basic ideas of neural networks, the powerful computational models inspired by the human brain’s interconnected neuron structure. This exploration allowed us to appreciate the capabilities and potential of neural networks in creating sophisticated algorithms capable of learning and adapting from data.</p>
<p>Understanding the role of libraries and frameworks was a key part of our discussion, offering insights into the tools that can facilitate the development and deployment of deep learning models. These resources not only ease the implementation of neural networks but also open avenues for innovation and optimization.</p>
<p>Next, we tackled the challenges one might face when embedding deep learning algorithms within embedded systems, providing a critical perspective on the complexities and considerations that come with bringing AI to edge devices.</p>
<p>Furthermore, we delved into an examination of the limitations of deep learning. Through a series of discussions, we unraveled the challenges faced in deep learning applications and outlined scenarios where traditional machine learning might outperform deep learning. These sections are crucial for fostering a balanced view of the capabilities and limitations of deep learning.</p>
<p>In this primer, we have equipped you with the knowledge to make informed choices between deploying traditional machine learning or deep learning techniques, depending on the unique demands and constraints of a specific problem.</p>
<p>As we conclude this chapter, we hope you are now well-equipped with the basic “language” of deep learning, prepared to delve deeper into the subsequent chapters with a solid understanding and critical perspective. The journey ahead is filled with exciting opportunities and challenges in embedding AI within systems.</p>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./embedded_sys.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Embedded Systems</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./embedded_ml.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Embedded AI</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Embedded AI edited by Prof.&nbsp;Vijay Janapa Reddi (Harvard University) and Prof.&nbsp;Song Han (MIT).</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>