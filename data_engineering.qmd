# Data Engineering

::: {.callout-note collapse="true"}
## Learning Objectives

* coming soon.

:::

## Introduction

Explanation: This section establishes the groundwork, defining data engineering and explaining its importance and role in Embedded AI. A well-rounded introduction will help in establishing the foundation for the readers.

- Definition and Importance of Data Engineering in AI
- Role of Data Engineering in Embedded AI
- Synergy with Machine Learning and Deep Learning
- Example: keyword spotting

## Problem

Explanation: This section is a crucial starting point in any data engineering project, as it lays the groundwork for the project's trajectory and ultimate success. Here's a brief explanation of why each subsection within the "Problem Definition" is important:

- Identifying the Problem
- Setting Clear Objectives
- Benchmarks for Success
- Stakeholder Engagement and Understanding
- Understanding the Constraints and Limitations of Embedded Systems
  
## Data Sourcing
The quality and type of data gathered can significantly affect the performance of both the trained model and its downstream applications. This is particularly true in embedded systems where computational resources may be limited, necessitating models that are both accurate and efficient. Generally, data can be sourced from various places depending on the project's objectives. Some common data sources include pre-existing datasets, web scraping, sensors, APIs, and crowdsourcing.

Pre-existing datasets are often the starting point for many machine learning projects. These datasets, already collected by someone else, are publicly available. Websites like [Kaggle](https://www.kaggle.com/) or the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) offer such datasets for a variety of tasks, ranging from natural language processing to computer vision. These datasets are generally clean, well-documented, and ready for immediate use.

Crowdsourcing is another effective method, particularly for tasks requiring human judgment. Services like [Amazon's Mechanical Turk](https://www.mturk.com/) facilitate the distribution of micro-tasks to a large number of people who can assist in labeling or annotating data. This method is frequently employed for natural language tasks such as sentiment analysis, or for image recognition tasks that may require human interpretation for contextual understanding. A notable example of crowdsourcing is the [ImageNet project](http://www.image-net.org/). ImageNet sourced candidate images from the internet and then used crowdsourcing to annotate these images, categorizing them into thousands of object classes. This approach took advantage of massive parallelism, making it possible to annotate a large dataset in a time-efficient mannerâ€”a task that would have been nearly impossible for a small group to accomplish. Another example is [Mozilla's Common Voice project](https://commonvoice.mozilla.org/en), which has successfully gathered a publicly available dataset of diverse voice recordings. Volunteers from around the world contribute by recording phrases in their native languages and accents. They also review and validate the recordings submitted by other users. Through this method, Common Voice has amassed a large, high-quality, and ethically sourced dataset that is freely accessible for research and development in the field of speech recognition.

Data can also be directly sourced from the field through sensors or devices, especially in the context of embedded systems or IoT devices. For instance, a weather station might collect real-time data on temperature, humidity, and wind speed. Such data is often raw and must be pre-processed before it can be utilized in a machine learning model.

APIs offer another channel for data collection. Various services like [Twitter](https://developer.twitter.com/en/docs/twitter-api), [Google](https://developers.google.com/products), or financial platforms provide APIs through which data can be collected programmatically. This allows for real-time data sourcing and can be customized to collect only the information that is relevant to the project.


## Data Processing

Explanation: Data processing is a pivotal step in transforming raw data into a usable format. This section provides a deep dive into the necessary processes, which include cleaning, integration, and establishing data pipelines, all crucial for streamlining operations in embedded AI systems.

- Data Cleaning and Transformation
- Data Pipelines
- Example: Multilingual Spoken Words Corpus
- Data Validation
- Handling Missing Values
- Outlier Detection
- Data Provenance


## Feature Engineering

Explanation: Feature engineering involves selecting and transforming variables to improve the performance of AI models. It's vital in embedded AI systems where computational resources are limited, and optimized feature sets can significantly improve performance.

- Importance of Feature Engineering 
- Techniques of Feature Selection 
- Feature Transformation for Embedded Systems
- Embeddings
- Real-time Feature Engineering in Embedded Systems

## Data Labeling

Explanation: Labeling is an essential part of preparing data for supervised learning. This section focuses on various strategies and tools available for data labeling, a vital process in the data preparation phase.

- Manual Data Labeling 
- Ethical Considerations (e.g. OpenAI issues)
- Automated Data Labeling 
- Labeling Tools 

## Data Version Control

Explanation: Version control is critical for managing changes and tracking versions of datasets during the development of AI models, facilitating reproducibility and collaboration.

- Version Control Systems 
- Metadata 

## Optimizing Data for Embedded AI

Explanation: This section concentrates on optimization techniques specifically suited for embedded systems, focusing on strategies to reduce data volume and enhance storage and retrieval efficiency, crucial for resource-constrained embedded environments.

- Low-Resource Data Challenges
- Data Reduction Techniques 
- Optimizing Data Storage and Retrieval 

## Challenges in Data Engineering

Explanation: Understanding potential challenges can help in devising strategies to mitigate them. This section discusses common challenges encountered in data engineering, particularly focusing on embedded systems.

- Scalability 
- Data Security and Privacy 
- Data Bias and Representativity 

## Promoting Transparency

Explanation: We explain that as we increasingly use these systems built on the foundation of data, we need to have more transparency in the ecosystem.

- Definition and Importance of Transparency in Data Engineering
- Transparency in Data Collection and Sourcing
- Transparency in Data Processing and Analysis
- Transparency in Model Building and Deployment
- Transparency in Data Sharing and Usage
- Tools and Techniques for Ensuring Transparency

## Licensing

Explanation: This section emphasizes why one must understand data licensing issues before they start using the data to train the models.

- Metadata
- Data Nutrition Project
- Understanding Licensing 

## Conclusion

Explanation: Close up the chapter with a summary of the key topics that we have covered in this section.

- The Future of Data Engineering in Embedded AI 
- Key Takeaways