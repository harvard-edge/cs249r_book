# Data Engineering

## Introduction

Explanation: This section establishes the groundwork, defining data engineering and explaining its importance and role in Embedded AI. A well-rounded introduction will help in establishing the foundation for the readers.

- Definition and Importance of Data Engineering in AI
- Role of Data Engineering in Embedded AI
- Synergy with Machine Learning and Deep Learning

## Problem

Explanation: This section is a crucial starting point in any data engineering project, as it lays the groundwork for the project's trajectory and ultimate success. Here's a brief explanation of why each subsection within the "Problem Definition" is important:

- Identifying the Problem
- Setting Clear Objectives
- Benchmarks for Success
- Stakeholder Engagement and Understanding
- Understanding the Constraints and Limitations of Embedded Systems
  
## Data Sourcing

Explanation: This section delves into the first step in data engineering - gathering data. Understanding various data types and sources is vital for developing robust AI systems, especially in the context of embedded systems where resources might be limited.

- Data Sources: crowdsourcing, pre-existing datasets etc.
- Data Types: Structured, Semi-Structured, and Unstructured
- Real-time Data Processing in Embedded Systems

## Data Storage

Explanation: Data must be stored and managed efficiently to facilitate easy access and processing. This section will provide insights into different data storage options and their respective advantages and challenges in embedded systems.

- Data Warehousing 
- Data Lakes 
- Metadata Management 
- Data Governance

## Data Processing

Explanation: Data processing is a pivotal step in transforming raw data into a usable format. This section provides a deep dive into the necessary processes, which include cleaning, integration, and establishing data pipelines, all crucial for streamlining operations in embedded AI systems.

- Data Cleaning and Transformation 
- Data Pipelines 
- Batch vs. Stream Processing 

## Data Quality

Explanation: Ensuring data quality is critical to developing reliable AI models. This section outlines various strategies to assure and evaluate data quality.

- Data Validation 
- Handling Missing Values 
- Outlier Detection 
- Data Provenance

## Feature Engineering

Explanation: Feature engineering involves selecting and transforming variables to improve the performance of AI models. It's vital in embedded AI systems where computational resources are limited, and optimized feature sets can significantly improve performance.

- Importance of Feature Engineering 
- Techniques of Feature Selection 
- Feature Transformation for Embedded Systems
- Embeddings
- Real-time Feature Engineering in Embedded Systems


## Data Labeling
Data labeling is an important step in creating high-quality training datasets for machine learning models. Labels provide the ground truth information that allows models to learn relationships between inputs and desired outputs. This section covers key considerations around selecting label types, formats, and content to capture the necessary information for given tasks. It discusses common annotation approaches, from manual labeling to crowdsourcing to AI-assisted methods, and best practices for ensuring label quality through training, guidelines, and quality checks. Ethical treatment of human annotators is also something we emphasize. The integration of AI to accelerate and augment human annotation is also explored. Understanding labeling needs, challenges, and strategies is essential for constructing reliable, useful datasets that can train performant, trustworthy machine learning systems.

**Label Types**
Labels capture information about key tasks or concepts. Common label types include binary classification, bounding boxes, segmentation masks, transcripts, captions, etc. The choice of label format depends on the use case and resource constraints, as more detailed labels require greater effort to collect (Johnson et al., 2016).  
Unless focused on self-supervised learning, a dataset will likely provide labels addressing one or more tasks of interest. Dataset creators must consider what information labels should capture and how they can practically obtain the necessary labels, given their unique resource constraints. Creators must first decide what type(s) of content labels should capture. For example, a creator interested in car detection would want to label cars in their dataset. Still, they might also consider whether to simultaneously collect labels for other tasks that the dataset could potentially be used for in the future, such as pedestrian detection. 
Additionally, annotators can potentially provide metadata that provides insight into how the dataset represents different characteristics of interest (see: Challenges in Data Engineering). The Common Voice dataset, for example, includes various types of metadata that provide information about the speakers, recordings, and dataset quality for each language represented. They include demographic splits showing the number of recordings by speaker age range and gender. This allows us to see the breakdown of who contributed recordings for each language. They also include statistics like average recording duration and total hours of validated recordings. These give insights into the nature and size of the datasets for each language. Additionally, quality control metrics like the percentage of recordings that have been validated are useful to know how complete and clean the datasets are. The metadata also includes normalized demographic splits scaled to 100% for comparison across languages. This highlights representation differences between higher and lower resource languages.
Next, creators must determine the format of those labels. For example, a creator interested in car detection might choose between binary classification labels that say whether a car is present, bounding boxes that show the general locations of any cars, or pixel-wise segmentation labels that show the exact location of each car. Their choice of label format may depend both on their use case and their resource constraints, as finer-grained labels are typically more expensive and time-consuming to acquire.

**Annotation Methods**
Common annotation approaches include manual labeling, crowdsourcing, and semi-automated techniques. Manual labeling by experts yields high quality but lacks scalability. Crowdsourcing enables distributed annotation by non-experts, often through dedicated platforms (Sheng et al., 2008). Weakly supervised and programmatic methods can reduce manual effort by heuristically or automatically generating labels (Ratner et al., 2018).
After deciding on their labels' desired content and format, creators begin the annotation process. To collect large numbers of labels from human annotators, creators frequently rely on dedicated annotation platforms, which can connect them to teams of human annotators. When using these platforms, creators may have little insight to annotatorsâ€™ backgrounds and levels of experience with topics of interest. However, some platforms offer access to annotators with specific expertise (e.g. doctors).

**Ensuring Label Quality**
There is no guarantee that the data labels are actually correct. It is possible that despite the best instructions being given to labelers, they still mislabel some images. Strategies like quality checks, training annotators, and collecting multiple labels per datapoint can help ensure label quality. For ambiguous tasks, multiple annotators can help identify controversial datapoints and quantify disagreement levels (Olejnik et al., 2018).

When working with human annotators, it is important to offer fair compensation and otherwise prioritize ethical treatment, as annotators can be exploited or otherwise harmed during the labeling process (https://time.com/6247678/openai-chatgpt-kenya-workers/). For example, if a dataset is likely to contain disturbing content, annotators may benefit from having the option to view images in grayscale (https://blog.google/documents/83/information\_quality\_content\_moderation_white_paper.pdf/).

**AI-Assisted Annotation**
ML has an insatiable demand for data. Therefore, no amount of data is sufficient data. This raises the question of how we can get more labeled data. Rather than always generating and curating data manually, we can rely on existing AI models to help label datasets more quickly and cheaply, though often with lower quality than human annotation. This can be done in various ways, such as the following. 

- **Pre-annotation:** AI models can generate preliminary labels for a dataset using methods such as self-supervised learning, which humans can then review and correct. This can save a significant amount of time, especially for large datasets.
- **Active learning:** AI models can identify the most informative data points in a dataset, which can then be prioritized for human annotation. This can help improve the labeled dataset's quality while reducing the overall annotation time. 
- **Quality control:** AI models can be used to identify and flag potential errors in human annotations. This can help to ensure the accuracy and consistency of the labeled dataset.

Here are some examples of how AI-assisted annotation has been proposed to be useful:
- **Medical imaging:** AI-assisted annotation is being used to label medical images, such as MRI scans and X-rays. Carefully annotating medical datasets is extremely challenging, especially at scale, since domain experts are both scarce and it becomes a costly effort. This can help to train AI models to diagnose diseases and other medical conditions more accurately and efficiently.  
- **Self-driving cars:** AI-assisted annotation is being used to label images and videos from self-driving cars. This can help to train AI models to identify objects on the road, such as other vehicles, pedestrians, and traffic signs. 
- **Social media:** AI-assisted annotation is being used to label social media posts, such as images and videos. This can help to train AI models to identify and classify different types of content, such as news, advertising, and personal posts.

## Data Version Control

Explanation: Version control is critical for managing changes and tracking versions of datasets during the development of AI models, facilitating reproducibility and collaboration.

- Version Control Systems 
- Metadata 

## Optimizing Data for Embedded AI

Explanation: This section concentrates on optimization techniques specifically suited for embedded systems, focusing on strategies to reduce data volume and enhance storage and retrieval efficiency, crucial for resource-constrained embedded environments.

- Low-Resource Data Challenges
- Data Reduction Techniques 
- Optimizing Data Storage and Retrieval 

## Challenges in Data Engineering

Explanation: Understanding potential challenges can help in devising strategies to mitigate them. This section discusses common challenges encountered in data engineering, particularly focusing on embedded systems.

- Scalability 
- Data Security and Privacy 
- Data Bias and Representativity 

## Promoting Transparency

Explanation: We explain that as we increasingly use these systems built on the foundation of data, we need to have more transparency in the ecosystem.

- Definition and Importance of Transparency in Data Engineering
- Transparency in Data Collection and Sourcing
- Transparency in Data Processing and Analysis
- Transparency in Model Building and Deployment
- Transparency in Data Sharing and Usage
- Tools and Techniques for Ensuring Transparency

## Licensing

Explanation: This section emphasizes why one must understand data licensing issues before they start using the data to train the models.

- Metadata
- Data Nutrition Project
- Understanding Licensing 

## Conclusion

Explanation: Close up the chapter with a summary of the key topics that we have covered in this section.

- The Future of Data Engineering in Embedded AI 
- Key Takeaways
