# Data Engineering

## Introduction

Explanation: This section establishes the groundwork, defining data engineering and explaining its importance and role in Embedded AI. A well-rounded introduction will help in establishing the foundation for the readers.

- Definition and Importance of Data Engineering in AI
- Role of Data Engineering in Embedded AI
- Synergy with Machine Learning and Deep Learning
- Example: keyword spotting

## Problem

Explanation: This section is a crucial starting point in any data engineering project, as it lays the groundwork for the project's trajectory and ultimate success. Here's a brief explanation of why each subsection within the "Problem Definition" is important:

- Identifying the Problem
- Setting Clear Objectives
- Benchmarks for Success
- Stakeholder Engagement and Understanding
- Understanding the Constraints and Limitations of Embedded Systems
  
## Data Sourcing
The quality and type of data gathered can significantly affect the performance of both the trained model and its downstream applications. This is particularly true in embedded systems where computational resources may be limited, necessitating models that are both accurate and efficient. Generally, data can be sourced from various places depending on the project's objectives. Some common data sources include pre-existing datasets, web scraping, sensors, APIs, and crowdsourcing.

Pre-existing datasets are often the starting point for many machine learning projects. These datasets, already collected by someone else, are publicly available. Websites like [Kaggle](https://www.kaggle.com/) or the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) offer such datasets for a variety of tasks, ranging from natural language processing to computer vision. These datasets are generally clean, well-documented, and ready for immediate use.

Crowdsourcing is another effective method, particularly for tasks requiring human judgment. Services like [Amazon's Mechanical Turk](https://www.mturk.com/) facilitate the distribution of micro-tasks to a large number of people who can assist in labeling or annotating data. This method is frequently employed for natural language tasks such as sentiment analysis, or for image recognition tasks that may require human interpretation for contextual understanding. A notable example of crowdsourcing is the [ImageNet project](http://www.image-net.org/). ImageNet sourced candidate images from the internet and then used crowdsourcing to annotate these images, categorizing them into thousands of object classes. This approach took advantage of massive parallelism, making it possible to annotate a large dataset in a time-efficient mannerâ€”a task that would have been nearly impossible for a small group to accomplish. Another example is [Mozilla's Common Voice project](https://commonvoice.mozilla.org/en), which has successfully gathered a publicly available dataset of diverse voice recordings. Volunteers from around the world contribute by recording phrases in their native languages and accents. They also review and validate the recordings submitted by other users. Through this method, Common Voice has amassed a large, high-quality, and ethically sourced dataset that is freely accessible for research and development in the field of speech recognition.

Data can also be directly sourced from the field through sensors or devices, especially in the context of embedded systems or IoT devices. For instance, a weather station might collect real-time data on temperature, humidity, and wind speed. Such data is often raw and must be pre-processed before it can be utilized in a machine learning model.

APIs offer another channel for data collection. Various services like [Twitter](https://developer.twitter.com/en/docs/twitter-api), [Google](https://developers.google.com/products), or financial platforms provide APIs through which data can be collected programmatically. This allows for real-time data sourcing and can be customized to collect only the information that is relevant to the project.


## Data Processing
Once data is collected, it must be processed to transform it into a usable format. For instance, the Multilingual Spoken Words Corpus (MSWC) used a forced alignment method that extracts individual word recordings to train keyword spotting models, from the Common Voice project which features crowdsourced sentence-level recordings.

The MSWC serves as an example of data pipelines - systematic and automated workflows that handle the transformation, storage, and processing of data for machine learning tasks. By streamlining the entire data flow, from raw data to usable datasets, data pipelines enhance productivity and facilitate the rapid development of machine learning models.

Generally, since data often comes from diverse sources and can be unstructured or semi-structured, it's essential to process and standardize it to ensure it adheres to a uniform format. Such transformations may include normalizing numerical variables, encoding categorical variables, and even using techniques like dimensionality reduction.

The process of data cleaning and transformation involves refining data to remove inconsistencies, duplications, and inaccuracies. For instance, in the MSWC data, the underlying crowd-sourced recordings are conducted in uncontrolled environments using computers and mobile devices. These recordings frequently feature background noises such as static, wind, mouse clicks, and other voices. Depending on the requirements of the models being trained, unwanted noise or malfunctions can either be removed or intentionally retained.

Data Validation ensures that the dataset adheres to certain standards, like temperature values not falling below absolute zero. Handling Missing Values involves filling gaps in the dataset through techniques such as mean imputation or interpolation to ensure models train effectively. Outlier Detection focuses on spotting and managing anomalous data points that could adversely affect model performance. Data Provenance keeps track of the origins of each data point for improved transparency and analysis.

Maintaining the integrity of the data infrastructure is a continuous endeavor. This encompasses data storage, security, error handling, and stringent version control. Periodic updates are crucial, especially in dynamic realms like keyword spotting, to adjust to evolving linguistic trends and device integrations.

As the data sources and types proliferate, the demand for more data pipelines intensifies, emphasizing the need for robust support and maintenance. Hence, it's imperative to employ consistent design patterns and automation, which aid in seamless debugging and upkeep should challenges emerge.



## Feature Engineering

Explanation: Feature engineering involves selecting and transforming variables to improve the performance of AI models. It's vital in embedded AI systems where computational resources are limited, and optimized feature sets can significantly improve performance.

- Importance of Feature Engineering 
- Techniques of Feature Selection 
- Feature Transformation for Embedded Systems
- Embeddings
- Real-time Feature Engineering in Embedded Systems

## Data Labeling

Explanation: Labeling is an essential part of preparing data for supervised learning. This section focuses on various strategies and tools available for data labeling, a vital process in the data preparation phase.

- Manual Data Labeling 
- Ethical Considerations (e.g. OpenAI issues)
- Automated Data Labeling 
- Labeling Tools 

## Data Version Control

Explanation: Version control is critical for managing changes and tracking versions of datasets during the development of AI models, facilitating reproducibility and collaboration.

- Version Control Systems 
- Metadata 

## Optimizing Data for Embedded AI

Explanation: This section concentrates on optimization techniques specifically suited for embedded systems, focusing on strategies to reduce data volume and enhance storage and retrieval efficiency, crucial for resource-constrained embedded environments.

- Low-Resource Data Challenges
- Data Reduction Techniques 
- Optimizing Data Storage and Retrieval 

## Challenges in Data Engineering

Explanation: Understanding potential challenges can help in devising strategies to mitigate them. This section discusses common challenges encountered in data engineering, particularly focusing on embedded systems.

- Scalability 
- Data Security and Privacy 
- Data Bias and Representativity 

## Promoting Transparency

Explanation: We explain that as we increasingly use these systems built on the foundation of data, we need to have more transparency in the ecosystem.

- Definition and Importance of Transparency in Data Engineering
- Transparency in Data Collection and Sourcing
- Transparency in Data Processing and Analysis
- Transparency in Model Building and Deployment
- Transparency in Data Sharing and Usage
- Tools and Techniques for Ensuring Transparency

## Licensing

Explanation: This section emphasizes why one must understand data licensing issues before they start using the data to train the models.

- Metadata
- Data Nutrition Project
- Understanding Licensing 

## Conclusion

Explanation: Close up the chapter with a summary of the key topics that we have covered in this section.

- The Future of Data Engineering in Embedded AI 
- Key Takeaways