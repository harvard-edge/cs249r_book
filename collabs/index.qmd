---
title: "Co-Labs"
subtitle: "Interactive ML Systems Simulations"
page-layout: full
toc: false
---

::: {.hero-banner}

# Co-Labs {.hero-title}

### Understanding the Interplay Between Algorithms and Systems {.hero-subtitle}

::: {.coming-soon-badge}
Coming 2026
:::

:::

## What Are Co-Labs?

Co-Labs are hands-on Google Colab simulations that bridge the gap between **reading about ML systems** (the textbook) and **building them from scratch** (TinyTorch).

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│                 │     │                 │     │                 │
│    Textbook     │────▶│     Co-Labs     │────▶│    TinyTorch    │
│                 │     │                 │     │                 │
│  Concepts &     │     │  Experiment &   │     │  Build from     │
│  Theory         │     │  Explore        │     │  Scratch        │
│                 │     │                 │     │                 │
└─────────────────┘     └─────────────────┘     └─────────────────┘
      READ                    EXPLORE                  BUILD
```

## The Learning Journey

| Phase | Resource | What You Do |
|-------|----------|-------------|
| **Understand** | [Textbook](../book/) | Learn concepts, theory, and system design principles |
| **Experiment** | Co-Labs | Explore tradeoffs, tweak parameters, see decisions ripple through systems |
| **Build** | [TinyTorch](../tinytorch/) | Implement everything from scratch, own every line of code |

## Why Co-Labs?

ML systems are where algorithms meet hardware. A model that works perfectly in theory can fail in practice due to memory limits, latency constraints, or numerical precision. Co-Labs help you develop intuition for these algorithm-system interactions.

- **See the tradeoffs** — How does batch size affect memory? How does quantization affect accuracy?
- **Explore interactively** — Adjust parameters and watch how changes ripple through the system
- **Build intuition** — Understand *why* systems behave the way they do
- **Zero setup** — Run directly in your browser via Google Colab

## Planned Topics

::: {.grid}

::: {.g-col-6}
### Memory & Compute
- Batch size vs memory footprint
- Gradient checkpointing tradeoffs
- Mixed precision training
:::

::: {.g-col-6}
### Model Efficiency
- Quantization effects (FP32 → INT8 → INT4)
- Pruning strategies comparison
- Knowledge distillation
:::

::: {.g-col-6}
### Visualization
- Attention head patterns
- Loss landscape navigation
- Gradient flow analysis
:::

::: {.g-col-6}
### Deployment
- Latency vs throughput
- Batching strategies
- Hardware utilization
:::

:::

## Stay Updated

Co-Labs are under active development. To be notified when they launch:

::: {.cta-buttons}
[Subscribe for Updates](https://buttondown.email/mlsysbook){.btn .btn-primary}
[Star on GitHub](https://github.com/harvard-edge/cs249r_book){.btn .btn-secondary}
[Join Discussions](https://github.com/harvard-edge/cs249r_book/discussions){.btn .btn-secondary}
:::

---

::: {.text-center}
**Read. Explore. Build.**
:::
