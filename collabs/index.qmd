---
title: "Co-Labs"
subtitle: "Interactive ML Systems Simulations"
page-layout: article
toc: false
---

::: {.callout-note appearance="simple"}
## Coming 2026
This project is under active development. [Share your ideas](https://github.com/harvard-edge/cs249r_book/discussions) or [subscribe for updates](#subscribe).
:::

## What I'm Building

There are many excellent Colab notebooks out there for ML. So why create more?

The challenge is that most notebooks exist in isolation. They demonstrate a technique, but don't connect to a broader learning journey. You finish one and wonder: *What should I explore next? How does this fit into the bigger picture?*

Co-Labs are different. Each notebook is **systematically aligned with the textbook chapters**, designed to let you experiment with the exact concepts you just read about. When Chapter 10 explains quantization tradeoffs, there's a Co-Lab where you can actually *see* what happens when you quantize a model from FP32 to INT8 to INT4. When Chapter 7 discusses memory hierarchies, there's a Co-Lab where you can measure cache effects yourself.

The goal isn't to replace the great resources that already exist. It's to create a curated, progressive learning path that reinforces the textbook material through hands-on exploration.

*— Vijay*

## The Learning Journey

Co-Labs bridge the gap between **reading about ML systems** (the textbook) and **building them from scratch** (TinyTorch).

| Phase | Resource | What You Do |
|-------|----------|-------------|
| **Understand** | [Textbook](../book/) | Learn concepts, theory, and system design principles |
| **Experiment** | Co-Labs | Explore tradeoffs, tweak parameters, see decisions ripple through systems |
| **Build** | [TinyTorch](../tinytorch/) | Implement everything from scratch, own every line of code |

## Example Topics

Each Co-Lab will map directly to textbook chapters:

- **Memory & Compute** — Batch size vs memory footprint, gradient checkpointing, mixed precision
- **Model Efficiency** — Quantization effects (FP32 → INT8 → INT4), pruning, distillation
- **Visualization** — Attention patterns, loss landscapes, gradient flow
- **Deployment** — Latency vs throughput, batching strategies, hardware utilization

## Get Involved

I'm still figuring out what makes the most sense. If you have suggestions or ideas:

- [GitHub Discussions](https://github.com/harvard-edge/cs249r_book/discussions) — Share ideas and feedback
- [GitHub Issues](https://github.com/harvard-edge/cs249r_book/issues) — Report problems or request features
- [Subscribe](#subscribe) — Get notified when Co-Labs launch
