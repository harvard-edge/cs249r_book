# Prerequisites & Self-Assessment

This page helps you assess whether you're ready to start TinyTorch and points you to resources if you need to brush up on fundamentals.


## Core Requirements

You need two things to start building:

**Python Programming.** You should be comfortable writing functions and classes, and have basic familiarity with NumPy arrays. No ML framework experience is required—you'll build your own! If you can write a Python class with `__init__` and methods, you're ready.

**Basic Linear Algebra.** You should understand matrix multiplication conceptually and know what a gradient (derivative) represents at a high level. If you know what multiplying two matrices means, you're ready.

That's it. You're ready to start building.


## What We Teach As You Build

You don't need deep expertise in these upfront—we teach them through implementation:

**Calculus derivatives.** Module 05 (Autograd) teaches automatic differentiation by having you build it. However, you should have seen derivatives conceptually (high school or intro calculus level). If "chain rule" is unfamiliar, watch [3Blue1Brown's calculus series](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) first—it's 3 hours that will save you 10 hours of confusion.

**Deep learning theory.** You'll learn by building networks, not by watching lectures. We introduce advanced NumPy operations as needed in each module.

TinyTorch's philosophy: understand backpropagation by building it, not just by reading about it.


## Which Learning Path Fits You?

**Path A: Foundation-First Builder** (Recommended for most). If you're a strong Python programmer curious about ML systems who wants to understand how frameworks work, start with Module 01 (Tensor) and work through sequentially. Best for CS students, software engineers transitioning to ML, and anyone wanting deep systems understanding.

**Path B: Focused Systems Engineer.** If you're a professional ML engineer who needs specific optimization skills and production deployment knowledge, review the Foundation Tier (01-07) then focus on the Optimization Tier (14-19). Best for working engineers debugging production systems and performance optimization specialists.

**Path C: Academic Researcher.** If you have ML theory background but need implementation skills to prototype novel architectures, start with Module 01 and accelerate through familiar concepts. Best for PhD students, research engineers, and anyone implementing custom operations.


## Complementary Learning Resources

**Mathematical Foundations.** The [Deep Learning Book](https://www.deeplearningbook.org/) by Goodfellow, Bengio, and Courville provides comprehensive theoretical foundations and mathematical background for concepts you'll implement. Use it alongside TinyTorch for deeper understanding.

**Visual Intuition.** [3Blue1Brown's Neural Networks series](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) offers visual explanations of backpropagation, gradient descent, and neural networks—a perfect complement to hands-on implementation. The [Linear Algebra series](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) provides geometric intuition for vectors, matrices, and transformations.

**Python & NumPy.** If NumPy is unfamiliar, review the [NumPy Quickstart Tutorial](https://numpy.org/doc/stable/user/quickstart.html) before Module 01.


## Ready to Begin?

If you can write a Python class with methods, explain what matrix multiplication does, and debug code using print statements, you're ready to start building.

Not quite there? Work through the resources above, then return when ready. TinyTorch will still be here, and you'll get more value once foundations are solid.


## Next Steps

**Ready to build?** See the [Quick Start Guide](getting-started.md) to install TinyTorch and complete your first module.

**Want the full picture?** See the [Learning Journey](chapters/learning-journey.md) to understand the complete pedagogical arc from tensors to transformers.

**Have questions?** Check the [FAQ](faq.md) or join the [Community](community.md).
