# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                        ğŸš¨ CRITICAL WARNING ğŸš¨                                â•‘
# â•‘                     AUTOGENERATED! DO NOT EDIT!                              â•‘
# â•‘                                                                               â•‘
# â•‘  This file is AUTOMATICALLY GENERATED from source modules.                   â•‘
# â•‘  ANY CHANGES MADE HERE WILL BE LOST when modules are re-exported!            â•‘
# â•‘                                                                               â•‘
# â•‘  âœ… TO EDIT: src/12_attention/12_attention.py                       â•‘
# â•‘  âœ… TO EXPORT: Run 'tito module complete <module_name>'                      â•‘
# â•‘                                                                               â•‘
# â•‘  ğŸ›¡ï¸ STUDENT PROTECTION: This file contains optimized implementations.        â•‘
# â•‘     Editing it directly may break module functionality and training.         â•‘
# â•‘                                                                               â•‘
# â•‘  ğŸ“ LEARNING TIP: Work in src/ (developers) or modules/ (learners)           â•‘
# â•‘     The tinytorch/ directory is generated code - edit source files instead!  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# %% auto 0
__all__ = ['MASK_VALUE', 'scaled_dot_product_attention', 'MultiHeadAttention']

# %% ../../modules/12_attention/attention.ipynb 0
#| default_exp core.attention
#| export

# %% ../../modules/12_attention/attention.ipynb 2
import numpy as np
import math
import time
from typing import Optional, Tuple, List

# Import dependencies from previous modules - following TinyTorch dependency chain
from .tensor import Tensor
from .layers import Linear
from .activations import Softmax

# Constants for attention computation
MASK_VALUE = -1e9  # Large negative value used for attention masking (becomes ~0 after softmax)

# %% ../../modules/12_attention/attention.ipynb 6
def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
    """
    Compute scaled dot-product attention.

    This is the fundamental attention operation that powers all transformer models.
    We'll implement it with explicit loops first to show the O(nÂ²) complexity.

    TODO: Implement scaled dot-product attention step by step

    APPROACH:
    1. Extract dimensions and validate inputs
    2. Compute attention scores with explicit nested loops (show O(nÂ²) complexity)
    3. Scale by 1/âˆšd_k for numerical stability
    4. Apply causal mask if provided (set masked positions to -inf)
    5. Apply softmax to get attention weights
    6. Apply values with attention weights (another O(nÂ²) operation)
    7. Return output and attention weights

    Args:
        Q: Query tensor of shape (batch_size, seq_len, d_model)
        K: Key tensor of shape (batch_size, seq_len, d_model)
        V: Value tensor of shape (batch_size, seq_len, d_model)
        mask: Optional causal mask, True=allow, False=mask (batch_size, seq_len, seq_len)

    Returns:
        output: Attended values (batch_size, seq_len, d_model)
        attention_weights: Attention matrix (batch_size, seq_len, seq_len)

    EXAMPLE:
    >>> Q = Tensor(np.random.randn(2, 4, 64))  # batch=2, seq=4, dim=64
    >>> K = Tensor(np.random.randn(2, 4, 64))
    >>> V = Tensor(np.random.randn(2, 4, 64))
    >>> output, weights = scaled_dot_product_attention(Q, K, V)
    >>> print(output.shape)  # (2, 4, 64)
    >>> print(weights.shape)  # (2, 4, 4)
    >>> print(weights.data[0].sum(axis=1))  # Each row sums to ~1.0

    HINTS:
    - Use explicit nested loops to compute Q[i] @ K[j] for educational purposes
    - Scale factor is 1/âˆšd_k where d_k is the last dimension of Q
    - Masked positions should be set to -1e9 before softmax
    - Remember that softmax normalizes along the last dimension
    """
    ### BEGIN SOLUTION
    # Step 1: Extract dimensions and validate
    # Note: Q, K, V can be 3D (batch, seq, dim) or 4D (batch, heads, seq, dim)
    # We use shape[-1] for d_model to handle both cases
    d_model = Q.shape[-1]

    # Step 2: Compute attention scores using matrix multiplication
    # Q: (..., seq_len, d_model)
    # K: (..., seq_len, d_model) -> K.T: (..., d_model, seq_len)
    # scores = Q @ K.T -> (..., seq_len, seq_len)

    # Transpose K for matrix multiplication
    # For 3D/4D tensors, transpose swaps the last two dimensions
    K_t = K.transpose(-2, -1)

    scores = Q.matmul(K_t)

    # Step 3: Scale by 1/âˆšd_k for numerical stability
    scale_factor = 1.0 / math.sqrt(d_model)
    scores = scores * scale_factor

    # Step 4: Apply causal mask if provided
    if mask is not None:
        # Mask values of 0 indicate positions to mask out (set to -inf)
        # We use (1 - mask) * MASK_VALUE to add large negative values to masked positions
        # mask is expected to be 0 for masked, 1 for unmasked

        # Ensure mask is broadcastable
        mask_data = mask.data
        adder_mask = (1.0 - mask_data) * MASK_VALUE
        adder_mask_tensor = Tensor(adder_mask)
        scores = scores + adder_mask_tensor

    # Step 5: Apply softmax to get attention weights
    softmax = Softmax()
    attention_weights = softmax(scores, dim=-1)

    # Step 6: Apply values with attention weights
    # weights: (..., seq_len, seq_len)
    # V: (..., seq_len, d_model)
    # output = weights @ V -> (..., seq_len, d_model)
    output = attention_weights.matmul(V)

    # ------------------------------------------------------------------
    # PEDAGOGICAL NOTE: Explicit Loop Implementation
    # ------------------------------------------------------------------
    # The following commented-out code shows how attention works conceptually
    # using explicit loops. While easier to understand, this approach is
    # NOT used here because:
    # 1. It is extremely slow (Python loops vs optimized C/BLAS)
    # 2. It breaks the autograd graph unless we manually implement the backward pass
    #
    # Conceptually, this is what the vectorized code above is doing:
    #
    # batch_size, n_heads, seq_len, d_k = Q.shape
    # scores = Tensor(np.zeros((batch_size, n_heads, seq_len, seq_len)), requires_grad=True)
    #
    # for b in range(batch_size):
    #     for h in range(n_heads):
    #         for i in range(seq_len):
    #             for j in range(seq_len):
    #                 # Dot product of query i and key j
    #                 dot_product = 0.0
    #                 for k in range(d_k):
    #                     dot_product += Q.data[b, h, i, k] * K.data[b, h, j, k]
    #
    #                 # Scale and store
    #                 scores.data[b, h, i, j] = dot_product / math.sqrt(d_k)
    #
    # # ... apply mask ...
    # # ... apply softmax ...
    #
    # output = Tensor(np.zeros((batch_size, n_heads, seq_len, d_k)), requires_grad=True)
    # for b in range(batch_size):
    #     for h in range(n_heads):
    #         for i in range(seq_len):
    #             for k in range(d_k):
    #                 # Weighted sum of values
    #                 weighted_sum = 0.0
    #                 for j in range(seq_len):
    #                     weighted_sum += attention_weights.data[b, h, i, j] * V.data[b, h, j, k]
    #                 output.data[b, h, i, k] = weighted_sum
    # ------------------------------------------------------------------

    return output, attention_weights
    ### END SOLUTION

# %% ../../modules/12_attention/attention.ipynb 10
class MultiHeadAttention:
    """
    Multi-head attention mechanism.

    Runs multiple attention heads in parallel, each learning different relationships.
    This is the core component of transformer architectures.
    """

    def __init__(self, embed_dim: int, num_heads: int):
        """
        Initialize multi-head attention.

        TODO: Set up linear projections and validate configuration

        APPROACH:
        1. Validate that embed_dim is divisible by num_heads
        2. Calculate head_dim (embed_dim // num_heads)
        3. Create linear layers for Q, K, V projections
        4. Create output projection layer
        5. Store configuration parameters

        Args:
            embed_dim: Embedding dimension (d_model)
            num_heads: Number of parallel attention heads

        EXAMPLE:
        >>> mha = MultiHeadAttention(embed_dim=512, num_heads=8)
        >>> mha.head_dim  # 64 (512 / 8)
        >>> len(mha.parameters())  # 4 linear layers * 2 params each = 8 tensors

        HINTS:
        - head_dim = embed_dim // num_heads must be integer
        - Need 4 Linear layers: q_proj, k_proj, v_proj, out_proj
        - Each projection maps embed_dim â†’ embed_dim
        """
        ### BEGIN SOLUTION
        if embed_dim % num_heads != 0:
            raise ValueError(
                f"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\n"
                f"  Issue: Multi-head attention splits embed_dim into num_heads heads.\n"
                f"  Fix: Choose embed_dim and num_heads such that embed_dim % num_heads == 0.\n"
                f"  Example: embed_dim=512, num_heads=8 works (512/8=64 per head)."
            )

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Linear projections for queries, keys, values
        self.q_proj = Linear(embed_dim, embed_dim)
        self.k_proj = Linear(embed_dim, embed_dim)
        self.v_proj = Linear(embed_dim, embed_dim)

        # Output projection to mix information across heads
        self.out_proj = Linear(embed_dim, embed_dim)
        ### END SOLUTION

    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:
        """
        Forward pass through multi-head attention.

        TODO: Implement the complete multi-head attention forward pass

        APPROACH:
        1. Extract input dimensions (batch_size, seq_len, embed_dim)
        2. Project input to Q, K, V using linear layers
        3. Reshape projections to separate heads: (batch, seq, heads, head_dim)
        4. Transpose to (batch, heads, seq, head_dim) for parallel processing
        5. Apply scaled dot-product attention to each head
        6. Transpose back and reshape to merge heads
        7. Apply output projection

        Args:
            x: Input tensor (batch_size, seq_len, embed_dim)
            mask: Optional attention mask (batch_size, seq_len, seq_len)

        Returns:
            output: Attended representation (batch_size, seq_len, embed_dim)

        EXAMPLE:
        >>> mha = MultiHeadAttention(embed_dim=64, num_heads=8)
        >>> x = Tensor(np.random.randn(2, 10, 64))  # batch=2, seq=10, dim=64
        >>> output = mha.forward(x)
        >>> print(output.shape)  # (2, 10, 64) - same as input

        HINTS:
        - Reshape: (batch, seq, embed_dim) â†’ (batch, seq, heads, head_dim)
        - Transpose: (batch, seq, heads, head_dim) â†’ (batch, heads, seq, head_dim)
        - After attention: reverse the process to merge heads
        - Use scaled_dot_product_attention for each head
        """
        ### BEGIN SOLUTION
        # Step 1: Extract dimensions
        batch_size, seq_len, embed_dim = x.shape
        if embed_dim != self.embed_dim:
            raise ValueError(
                f"Input dimension mismatch in MultiHeadAttention.forward().\n"
                f"  Expected: embed_dim={self.embed_dim} (set during initialization)\n"
                f"  Got: embed_dim={embed_dim} from input shape {x.shape}\n"
                f"  Fix: Ensure input tensor's last dimension matches the embed_dim used when creating MultiHeadAttention."
            )

        # Step 2: Project to Q, K, V
        Q = self.q_proj.forward(x)  # (batch, seq, embed_dim)
        K = self.k_proj.forward(x)
        V = self.v_proj.forward(x)

        # Step 3: Reshape to separate heads
        # From (batch, seq, embed_dim) to (batch, seq, num_heads, head_dim)
        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        K = K.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        V = V.reshape(batch_size, seq_len, self.num_heads, self.head_dim)

        # Step 4: Transpose to (batch, num_heads, seq, head_dim) for parallel processing
        Q = Q.transpose(1, 2)
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)

        # Step 5: Apply attention
        # We can apply attention to all heads at once because scaled_dot_product_attention
        # supports broadcasting or 4D tensors if implemented correctly.

        # Reshape mask if necessary to broadcast over heads
        mask_reshaped = mask
        if mask is not None and len(mask.shape) == 3:
             # Add head dimension: (batch, seq, seq) -> (batch, 1, seq, seq)
             # This allows the mask to broadcast across all attention heads
             batch_size_mask, seq_len_mask, _ = mask.shape
             mask_data = mask.data.reshape(batch_size_mask, 1, seq_len_mask, seq_len_mask)
             mask_reshaped = Tensor(mask_data)

        attended, _ = scaled_dot_product_attention(Q, K, V, mask=mask_reshaped)

        # Step 6: Concatenate heads back together
        # Transpose back: (batch, num_heads, seq, head_dim) â†’ (batch, seq, num_heads, head_dim)
        attended = attended.transpose(1, 2)

        # Reshape: (batch, seq, num_heads, head_dim) â†’ (batch, seq, embed_dim)
        concat_output = attended.reshape(batch_size, seq_len, self.embed_dim)

        # Step 7: Apply output projection
        output = self.out_proj.forward(concat_output)

        return output
        ### END SOLUTION

    def __call__(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:
        """Make MultiHeadAttention callable like attention(x)."""
        return self.forward(x, mask)

    def parameters(self) -> List[Tensor]:
        """
        Return all trainable parameters.

        TODO: Collect parameters from all linear layers

        APPROACH:
        1. Get parameters from q_proj, k_proj, v_proj, out_proj
        2. Combine into single list

        Returns:
            List of all parameter tensors

        EXAMPLE:
        >>> mha = MultiHeadAttention(embed_dim=64, num_heads=8)
        >>> params = mha.parameters()
        >>> print(len(params))  # 8 (4 layers Ã— 2 params each: weight + bias)
        >>> print(params[0].shape)  # (64, 64) - q_proj weight
        >>> print(params[1].shape)  # (64,) - q_proj bias

        HINTS:
        - Each Linear layer has .parameters() method that returns [weight, bias]
        - Use extend() to add all parameters from each layer to the list
        - Total should be 8 tensors: 4 layers Ã— 2 parameters each
        """
        ### BEGIN SOLUTION
        params = []
        params.extend(self.q_proj.parameters())
        params.extend(self.k_proj.parameters())
        params.extend(self.v_proj.parameters())
        params.extend(self.out_proj.parameters())
        return params
        ### END SOLUTION
