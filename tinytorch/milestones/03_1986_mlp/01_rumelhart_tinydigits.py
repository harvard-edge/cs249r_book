#!/usr/bin/env python3
"""
MLP on Digits (1986) - Rumelhart, Hinton, Williams
==================================================

ğŸ“š HISTORICAL CONTEXT:
In 1986, Rumelhart, Hinton, and Williams published "Learning representations by
back-propagating errors" in Nature. This paper proved that multi-layer networks
could learn useful internal representations - ending the AI Winter that began
after the 1969 XOR crisis.

ğŸ¯ MILESTONE 3: MULTI-LAYER PERCEPTRON ON TINYDIGITS
The 1986 backpropagation paper proved multi-layer networks could solve
real-world problems. Let's recreate that breakthrough using YOUR TinyğŸ”¥Torch!

âœ… REQUIRED MODULES (Run after Module 08):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Module 01 (Tensor)        : YOUR data structure with autodiff
  Module 02 (Activations)   : YOUR ReLU activation
  Module 03 (Layers)        : YOUR Linear layer
  Module 04 (Losses)        : YOUR CrossEntropyLoss
  Module 05 (DataLoader)    : YOUR data batching system
  Module 06 (Autograd)      : YOUR automatic differentiation
  Module 07 (Optimizers)    : YOUR SGD optimizer
  Module 08 (Training)      : YOUR training loops
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ—ï¸ ARCHITECTURE (The Classic MLP):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Input Image â”‚    â”‚   Flatten   â”‚    â”‚   Linear    â”‚    â”‚    ReLU     â”‚    â”‚   Linear    â”‚
    â”‚    8Ã—8      â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚
    â”‚   Pixels    â”‚    â”‚     01      â”‚    â”‚     03      â”‚    â”‚     02      â”‚    â”‚     03      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        8Ã—8=64             64 dims           64â†’32           Non-linear          32â†’10
        pixels                             Hidden Layer      activation       10 Classes

    Sample 8Ã—8 Digit Images:              What the Hidden Layer Learns:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”               â€¢ Edge detectors (horizontal, vertical)
    â”‚â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â”‚  â”‚â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â”‚               â€¢ Curve patterns (loops in 0, 6, 8, 9)
    â”‚â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â”‚  â”‚â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â”‚               â€¢ Stroke endings (1, 7 vs 8, 0)
    â”‚â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â”‚  â”‚â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â”‚               â€¢ Intersection points (4, 8)
    â”‚â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â”‚  â”‚â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â”‚
    â”‚â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â”‚  â”‚â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â”‚               32 hidden units = 32 feature detectors
    â”‚â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â”‚               that YOUR network learns automatically!
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       "1"         "7"

ğŸ” MLP LIMITATION (Why CNNs will be better):
    MLP treats each pixel INDEPENDENTLY - no spatial awareness!

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ MLP sees: [pâ‚, pâ‚‚, pâ‚ƒ, ..., pâ‚†â‚„]  (just 64 numbers)                â”‚
    â”‚                                                                    â”‚
    â”‚ MLP doesn't know that pâ‚ and pâ‚‚ are NEIGHBORS!                     â”‚
    â”‚ Shifting an image 1 pixel changes ALL feature values!              â”‚
    â”‚                                                                    â”‚
    â”‚ This is why Milestone 04 (CNNs) will show ~10% improvement:        â”‚
    â”‚ CNNs exploit SPATIAL STRUCTURE that MLPs ignore.                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š DATASET: TinyDigits (8Ã—8 Handwritten Digits)
  - 150 training + 47 test samples (curated for fast learning)
  - 8Ã—8 grayscale images (64 features)
  - 10 classes (digits 0-9)
  - Ships with TinyTorch (~310 KB, no download!)

ğŸ“Š EXPECTED RESULTS:
- Training time: ~1-2 minutes
- Accuracy: 75-85% (decent for 8Ã—8 images without spatial features!)
- Parameters: 2,378 (64Ã—32 + 32 + 32Ã—10 + 10)
- ğŸ“Œ BASELINE: CNN (Milestone 04) will show improvement by using spatial structure!
"""

import sys
import os
import numpy as np
import pickle
from pathlib import Path

# Add project root to path
sys.path.insert(0, os.getcwd())

# Import TinyTorch components YOU BUILT!
from tinytorch import Tensor, Linear, ReLU, CrossEntropyLoss, SGD
from tinytorch.core.dataloader import TensorDataset, DataLoader

# Rich for beautiful output
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.live import Live
from rich.text import Text
from rich import box

console = Console()

# =============================================================================
# ğŸ¯ YOUR TINYTORCH MODULES IN ACTION
# =============================================================================
#
# This milestone showcases YOUR complete ML system on a classification task:
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ What You Built      â”‚ How It's Used Here             â”‚ Systems Impact              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Module 01: Tensor   â”‚ Stores 8Ã—8 images as 64-dim    â”‚ Automatic gradient tracking â”‚
# â”‚                     â”‚ vectors + all gradients        â”‚ through entire network      â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 02: ReLU     â”‚ Non-linearity after hidden     â”‚ Enables learning non-linear â”‚
# â”‚                     â”‚ layer (64â†’32)                  â”‚ digit patterns              â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 03: Linear   â”‚ Two layers: feature extraction â”‚ 2,378 parameters total      â”‚
# â”‚                     â”‚ (64â†’32) + classification (32â†’10)â”‚ learned by YOUR autograd    â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 04: Loss     â”‚ CrossEntropyLoss for           â”‚ Multi-class loss guides     â”‚
# â”‚                     â”‚ 10-way classification          â”‚ learning of all 10 digits   â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 05: DataLoaderâ”‚ Batches training images       â”‚ Memory efficient iteration  â”‚
# â”‚                     â”‚ (no need to load all at once)  â”‚ over dataset                â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 06: Autograd â”‚ .backward() computes gradients â”‚ Chain rule through 2 layers â”‚
# â”‚                     â”‚ for 2,378 parameters           â”‚ automatically               â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 07: SGD      â”‚ Updates all weights each step  â”‚ Stochastic gradient descent â”‚
# â”‚                     â”‚                                â”‚ with learning rate          â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# =============================================================================
# ğŸ†• WHAT'S NEW SINCE MILESTONE 02 (XOR)
# =============================================================================
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ XOR (Milestone 02)   â”‚ This Milestone          â”‚ Why It Matters             â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 2 inputs             â”‚ 64 inputs (8Ã—8 image)   â”‚ Images are high-dim!       â”‚
# â”‚ 4 data points        â”‚ 150+ data points        â”‚ Need more data to learn    â”‚
# â”‚ Binary classificationâ”‚ 10-way classification   â”‚ Multi-class is harder      â”‚
# â”‚ 13 parameters        â”‚ 2,378 parameters        â”‚ More capacity = more power â”‚
# â”‚ No batching          â”‚ DataLoader batching     â”‚ Memory efficiency matters  â”‚
# â”‚ BinaryCrossEntropy   â”‚ CrossEntropyLoss        â”‚ Different loss for multi   â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# =============================================================================


# ============================================================================
# ğŸ“ STUDENT CODE: Multi-Layer Perceptron
# ============================================================================

class DigitMLP:
    """
    Multi-Layer Perceptron for digit classification.

    Architecture:
      Input (64) â†’ Linear(64â†’32) â†’ ReLU â†’ Linear(32â†’10) â†’ Output
    """

    def __init__(self, input_size=64, hidden_size=32, num_classes=10):
        console.print("ğŸ§  Building Multi-Layer Perceptron...")

        # Hidden layer
        self.fc1 = Linear(input_size, hidden_size)
        self.relu = ReLU()

        # Output layer
        self.fc2 = Linear(hidden_size, num_classes)

        console.print(f"  âœ“ Hidden layer: {input_size} â†’ {hidden_size} (with ReLU)")
        console.print(f"  âœ“ Output layer: {hidden_size} â†’ {num_classes}")

        total_params = (input_size * hidden_size + hidden_size) + \
                      (hidden_size * num_classes + num_classes)
        console.print(f"  âœ“ Total parameters: {total_params:,}\n")

    def __call__(self, x):
        """Make the model callable."""
        return self.forward(x)

    def forward(self, x):
        """Forward pass through the network."""
        # Flatten if needed (8Ã—8 â†’ 64)
        if len(x.data.shape) > 2:
            batch_size = x.data.shape[0]
            x = Tensor(x.data.reshape(batch_size, -1))

        # Hidden layer
        x = self.fc1(x)
        x = self.relu(x)

        # Output layer
        x = self.fc2(x)
        return x

    def parameters(self):
        """Get all trainable parameters."""
        return [self.fc1.weight, self.fc1.bias,
                self.fc2.weight, self.fc2.bias]


def load_digit_dataset():
    """Load the TinyDigits dataset (8Ã—8 curated digits)."""
    console.print(Panel.fit(
        "[bold]Loading TinyDigits Dataset[/bold]\n"
        "Curated 8Ã—8 handwritten digits optimized for fast learning",
        title="ğŸ“Š Dataset",
        border_style="cyan"
    ))

    # Load from TinyDigits dataset (shipped with TinyTorch)
    project_root = Path(__file__).parent.parent.parent
    train_path = project_root / "datasets" / "tinydigits" / "train.pkl"
    test_path = project_root / "datasets" / "tinydigits" / "test.pkl"

    if not train_path.exists() or not test_path.exists():
        console.print(f"[red]âœ— TinyDigits dataset not found![/red]")
        console.print(f"[yellow]Expected location: {train_path.parent}[/yellow]")
        console.print("[yellow]Run: python3 datasets/tinydigits/create_tinydigits.py[/yellow]")
        sys.exit(1)

    # Load training data
    with open(train_path, 'rb') as f:
        train_data = pickle.load(f)
    train_images_np = train_data['images']  # (150, 8, 8)
    train_labels_np = train_data['labels']  # (150,)

    # Load test data
    with open(test_path, 'rb') as f:
        test_data = pickle.load(f)
    test_images_np = test_data['images']  # (47, 8, 8)
    test_labels_np = test_data['labels']  # (47,)

    console.print(f"âœ“ TinyDigits loaded ({train_images_np.shape[0] + test_images_np.shape[0]} total samples)")
    console.print(f"âœ“ Image shape: {train_images_np[0].shape}")
    console.print(f"âœ“ Classes: {np.unique(train_labels_np)}")

    # Convert to Tensors
    train_images = Tensor(train_images_np.astype(np.float32))
    train_labels = Tensor(train_labels_np.astype(np.int64))
    test_images = Tensor(test_images_np.astype(np.float32))
    test_labels = Tensor(test_labels_np.astype(np.int64))

    console.print(f"\nğŸ“Š Split:")
    console.print(f"  Training: {len(train_images.data)} samples")
    console.print(f"  Testing:  {len(test_images.data)} samples\n")

    return train_images, train_labels, test_images, test_labels


def evaluate_accuracy(model, images, labels):
    """Compute classification accuracy."""
    # Forward pass
    logits = model(images)

    # Get predictions (argmax)
    predictions = np.argmax(logits.data, axis=1)

    # Compare with labels
    correct = (predictions == labels.data).sum()
    total = len(labels.data)
    accuracy = 100.0 * correct / total

    return accuracy, predictions


def compare_batch_sizes(train_images, train_labels, test_images, test_labels):
    """
    Compare different batch sizes to show DataLoader's impact on training.

    This demonstrates a key systems trade-off in ML:
    - Larger batches: Faster throughput (fewer Python loops)
    - Smaller batches: More gradient updates per epoch
    """
    import time

    console.print(Panel.fit(
        "[bold cyan]ğŸ”¬ Systems Experiment: Batch Size Impact[/bold cyan]\n\n"
        "[dim]Let's explore how batch size affects training speed and learning.\n"
        "This shows YOUR DataLoader in action![/dim]",
        title="âš™ï¸ DataLoader Capabilities",
        border_style="yellow"
    ))

    batch_sizes = [16, 64, 256]
    epochs = 5  # Quick experiment
    results = []

    for batch_size in batch_sizes:
        console.print(f"\n[bold]Testing batch_size={batch_size}[/bold]")

        # Create DataLoader with this batch size
        train_dataset = TensorDataset(train_images, train_labels)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

        console.print(f"  Batches per epoch: {len(train_loader)}")

        # Create fresh model
        model = DigitMLP(input_size=64, hidden_size=32, num_classes=10)
        optimizer = SGD(model.parameters(), lr=0.01)
        loss_fn = CrossEntropyLoss()

        # Time the training
        start_time = time.time()

        for epoch in range(epochs):
            for batch_images, batch_labels in train_loader:
                logits = model(batch_images)
                loss = loss_fn(logits, batch_labels)
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

        elapsed = time.time() - start_time

        # Evaluate
        final_acc, _ = evaluate_accuracy(model, test_images, test_labels)

        # Calculate throughput
        total_samples = len(train_dataset) * epochs
        samples_per_sec = total_samples / elapsed
        updates = len(train_loader) * epochs

        results.append({
            'batch_size': batch_size,
            'time': elapsed,
            'accuracy': final_acc,
            'updates': updates,
            'throughput': samples_per_sec
        })

        console.print(f"  Time: {elapsed*1000:.0f}ms, Accuracy: {final_acc:.1f}%")

    # Show comparison table
    console.print("\n")
    table = Table(title="ğŸ“Š Batch Size Comparison", box=box.ROUNDED)
    table.add_column("Batch Size", style="cyan", justify="center")
    table.add_column("Training Time", style="green")
    table.add_column("Gradient Updates", style="yellow", justify="center")
    table.add_column("Accuracy", style="magenta")
    table.add_column("Throughput", style="blue")

    for r in results:
        table.add_row(
            str(r['batch_size']),
            f"{r['time']*1000:.0f}ms",
            str(r['updates']),
            f"{r['accuracy']:.1f}%",
            f"{r['throughput']:.0f} samples/s"
        )

    console.print(table)

    # Key insights
    console.print("\n")
    console.print(Panel.fit(
        "[bold]ğŸ’¡ Key Systems Insights:[/bold]\n\n"
        "[green]âœ“ Larger batches process data faster[/green] (fewer Python loops)\n"
        "[green]âœ“ Smaller batches give more gradient updates[/green] (more optimization steps)\n"
        "[green]âœ“ Throughput vs update frequency trade-off[/green]\n\n"
        "[bold]What This Shows:[/bold]\n"
        f"  â€¢ Batch 16:  Slowest but {results[0]['updates']} updates\n"
        f"  â€¢ Batch 64:  Balanced - {results[1]['updates']} updates\n"
        f"  â€¢ Batch 256: Fastest but only {results[2]['updates']} updates\n\n"
        "[bold]ğŸš€ Production Tip:[/bold] In real systems, batch size is limited by:\n"
        "  â€¢ GPU memory (larger batches need more VRAM)\n"
        "  â€¢ Gradient noise (tiny batches â†’ unstable training)\n"
        "  â€¢ Sweet spot: Usually 32-128 for most tasks\n\n"
        "[dim]YOUR DataLoader makes experimenting with this trivial -\n"
        "just change one number and the whole pipeline adapts![/dim]",
        title="âš™ï¸ DataLoader Impact",
        border_style="cyan"
    ))


def train_mlp():
    """Train MLP on digit recognition task."""

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 1: THE CHALLENGE ğŸ¯
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print(Panel.fit(
        "[bold cyan]ğŸ¯ 1986 - Deep Learning on Real Data[/bold cyan]\n\n"
        "[dim]Can multi-layer networks learn from real handwritten digits?[/dim]\n"
        "[dim]Rumelhart, Hinton & Williams prove backprop works on real tasks![/dim]",
        title="ğŸ”¥ 1986 Backpropagation Revolution",
        border_style="cyan",
        box=box.DOUBLE
    ))

    console.print("\n[bold]ğŸ“Š The Data:[/bold]")
    train_images, train_labels, test_images, test_labels = load_digit_dataset()
    console.print("  â€¢ Dataset: 8Ã—8 handwritten digits (UCI repository)")
    console.print(f"  â€¢ Training samples: {len(train_images.data)}")
    console.print(f"  â€¢ Test samples: {len(test_images.data)}")
    console.print("  â€¢ Classes: 10 digits (0-9)")
    console.print("  â€¢ Challenge: Recognize handwritten digits from pixels!")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 2: THE SETUP ğŸ—ï¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("[bold]ğŸ—ï¸ The Architecture:[/bold]")
    console.print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Input Image â”‚    â”‚ Flatten â”‚    â”‚ Linear  â”‚    â”‚ Linear  â”‚
    â”‚    8Ã—8      â”‚â”€â”€â”€â–¶â”‚   64    â”‚â”€â”€â”€â–¶â”‚ 64â†’32   â”‚â”€â”€â”€â–¶â”‚ 32â†’10   â”‚
    â”‚   Pixels    â”‚    â”‚         â”‚    â”‚  +ReLU  â”‚    â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      Hidden Layer   10 Classes
    """)

    console.print("[bold]ğŸ”§ Components:[/bold]")
    model = DigitMLP(input_size=64, hidden_size=32, num_classes=10)
    console.print("  â€¢ Hidden layer: 64 â†’ 32 (learns digit features)")
    console.print("  â€¢ ReLU activation: Non-linear transformations")
    console.print("  â€¢ Output layer: 32 â†’ 10 (one per digit class)")
    console.print(f"  â€¢ Total parameters: ~{64*32 + 32 + 32*10 + 10:,}")

    console.print("\n[bold]âš™ï¸ Hyperparameters:[/bold]")
    console.print("  â€¢ Batch size: 32 (using YOUR DataLoader!)")
    console.print("  â€¢ Learning rate: 0.01")
    console.print("  â€¢ Epochs: 20")
    console.print("  â€¢ Loss: CrossEntropyLoss (for multi-class)")
    console.print("  â€¢ Optimizer: SGD with backprop")

    # Create DataLoader
    train_dataset = TensorDataset(train_images, train_labels)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    console.print(f"  â€¢ Batches per epoch: {len(train_loader)}")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 3: THE EXPERIMENT ğŸ”¬
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    loss_fn = CrossEntropyLoss()
    optimizer = SGD(model.parameters(), lr=0.01)

    initial_acc, _ = evaluate_accuracy(model, test_images, test_labels)

    console.print("[bold]ğŸ“Œ Before Training:[/bold]")
    console.print(f"  Initial accuracy: {initial_acc:.1f}% (random ~10%)")
    console.print("  Model has random weights - knows nothing about digits yet!")

    console.print("\n[bold]ğŸ”¥ Training in Progress...[/bold]")
    console.print("[dim](Watch backpropagation optimize through hidden layers!)[/dim]\n")

    epochs = 20
    initial_loss = None
    history = {
        "train_loss": [],
        "train_accuracy": [],
        "test_accuracy": []
    }

    # Use Live display with spinner for real-time feedback
    with Live(console=console, refresh_per_second=10) as live:
        for epoch in range(epochs):
            epoch_loss = 0.0
            batch_count = 0

            for batch_images, batch_labels in train_loader:
                # Forward pass
                logits = model(batch_images)
                loss = loss_fn(logits, batch_labels)

                # Backward pass
                loss.backward()

                # Update weights
                optimizer.step()
                optimizer.zero_grad()

                epoch_loss += loss.data
                batch_count += 1

                # Update spinner with current batch progress
                spinner_text = Text()
                spinner_text.append("â ‹ ", style="cyan")
                spinner_text.append(f"Epoch {epoch+1:2d}/{epochs}  Batch {batch_count}/{len(train_loader)}")
                live.update(spinner_text)

            avg_loss = epoch_loss / batch_count

            # Evaluate on both train and test to detect overfitting
            train_acc, _ = evaluate_accuracy(model, train_images, train_labels)
            test_acc, _ = evaluate_accuracy(model, test_images, test_labels)

            history["train_loss"].append(avg_loss)
            history["train_accuracy"].append(train_acc)
            history["test_accuracy"].append(test_acc)

            if initial_loss is None:
                initial_loss = avg_loss

            # Print progress every 5 epochs
            if (epoch + 1) % 5 == 0:
                gap = train_acc - test_acc
                gap_indicator = "âš ï¸" if gap > 10 else "âœ“"
                live.console.print(
                    f"Epoch {epoch+1:2d}/{epochs}  "
                    f"Loss: {avg_loss:.4f}  "
                    f"Train: {train_acc:.1f}%  "
                    f"Test: {test_acc:.1f}%  "
                    f"{gap_indicator} Gap: {gap:.1f}%"
                )

    console.print("\n[green]âœ… Training Complete![/green]")

    final_train_acc = history["train_accuracy"][-1]
    final_test_acc = history["test_accuracy"][-1]
    overfitting_gap = final_train_acc - final_test_acc

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 4: THE DIAGNOSIS ğŸ“Š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("[bold]ğŸ“Š The Results:[/bold]\n")

    table = Table(title="Training Outcome", box=box.ROUNDED)
    table.add_column("Metric", style="cyan", width=20)
    table.add_column("Value", style="green", width=20)
    table.add_column("Status", style="magenta", width=20)

    table.add_row(
        "Train Accuracy",
        f"{final_train_acc:.1f}%",
        f"â†‘ +{final_train_acc - initial_acc:.1f}%"
    )
    table.add_row(
        "Test Accuracy",
        f"{final_test_acc:.1f}%",
        f"â†‘ +{final_test_acc - initial_acc:.1f}%"
    )
    table.add_row(
        "Overfitting Gap",
        f"{overfitting_gap:.1f}%",
        "âœ“ Healthy" if overfitting_gap < 10 else "âš ï¸ Overfitting"
    )

    console.print(table)

    # Also get predictions for later use
    _, predictions = evaluate_accuracy(model, test_images, test_labels)

    console.print("\n[bold]ğŸ” Sample Predictions:[/bold]")
    console.print("[dim](First 10 test images)[/dim]\n")

    n_samples = 10
    for i in range(n_samples):
        true_label = test_labels.data[i]
        pred_label = predictions[i]
        status = "âœ“" if pred_label == true_label else "âœ—"
        color = "green" if pred_label == true_label else "red"
        console.print(f"  {status} True: {true_label}, Predicted: {pred_label}", style=color)

    console.print("\n[bold]ğŸ’¡ Key Insights:[/bold]")
    console.print("  â€¢ MLP learned to recognize handwritten digits from pixels")
    console.print("  â€¢ Hidden layer discovered useful digit features")
    console.print("  â€¢ DataLoader enabled efficient batch processing")
    console.print("  â€¢ Backprop through hidden layers works on image data!")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 5: THE REFLECTION ğŸŒŸ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("")
    console.print(Panel.fit(
        "[bold green]ğŸ‰ Success! Your MLP Learned to Recognize Digits![/bold green]\n\n"

        f"Test accuracy: [bold]{final_test_acc:.1f}%[/bold] (Gap: {overfitting_gap:.1f}%)\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸ’¡ What YOU Just Accomplished:[/bold]\n"
        "  âœ“ Built multi-layer network with YOUR components\n"
        "  âœ“ Trained on TinyDigits (synthetic handwritten digits)\n"
        "  âœ“ Used YOUR DataLoader for efficient batching\n"
        f"  âœ“ Model generalizes well (gap: {overfitting_gap:.1f}%)\n"
        "  âœ“ Backprop through hidden layers works on image data!\n"
        f"  âœ“ Achieved {final_test_acc:.1f}% test accuracy!\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸ“ Why This Matters:[/bold]\n"
        "  This proved backprop works on practical tasks, not just XOR!\n"
        "  1986 paper by Rumelhart, Hinton & Williams launched\n"
        "  modern deep learning revolution.\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸ“Œ The Key Insight:[/bold]\n"
        "  MLPs flatten images â†’ lose spatial structure.\n"
        "  Each pixel treated independently with no neighborhood info.\n"
        "  \n"
        "  [yellow]Limitation:[/yellow] 8Ã—8 images work, but larger images?\n"
        "  We need better architectures for spatial data...\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸš€ What's Next:[/bold]\n"
        "[dim]Milestone 04 (CNN) will show how preserving spatial structure\n"
        "dramatically improves performance on images![/dim]",

        title="ğŸŒŸ 1986 MLP Breakthrough Complete",
        border_style="green",
        box=box.DOUBLE
    ))

    # Optional: Batch size experiment
    console.print("\n")
    run_experiment = input("\nğŸ”¬ Run batch size experiment? (y/n): ").lower().strip() == 'y'

    if run_experiment:
        compare_batch_sizes(train_images, train_labels, test_images, test_labels)


if __name__ == "__main__":
    train_mlp()
