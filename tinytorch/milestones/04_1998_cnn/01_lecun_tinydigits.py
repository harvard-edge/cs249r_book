#!/usr/bin/env python3
"""
The CNN Revolution (1998) - LeNet Part 1: TinyDigits
====================================================

ğŸ“š HISTORICAL CONTEXT:
After backpropagation proved MLPs could learn (1986), researchers still struggled
with image recognition. MLPs treated pixels independently, requiring millions of
parameters and ignoring spatial structure.

Then in 1998, Yann LeCun's LeNet-5 revolutionized computer vision with
Convolutional Neural Networks (CNNs). By using:
- Shared weights (convolution) â†’ 100Ã— fewer parameters
- Local connectivity â†’ preserves spatial structure
- Pooling â†’ translation invariance

LeNet achieved 99%+ accuracy on handwritten digits, launching the deep learning
revolution that led to modern computer vision.

ğŸ¯ MILESTONE 4 PART 1: PROVE CNNs > MLPs (Offline)
Using YOUR TinyğŸ”¥Torch spatial modules, you'll build a CNN that OUTPERFORMS the
MLP from Milestone 03 on the SAME dataset. This proves spatial operations matter!

âœ… REQUIRED MODULES (Run after Module 09):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Module 01 (Tensor)        : YOUR data structure with autodiff
  Module 02 (Activations)   : YOUR ReLU activation
  Module 03 (Layers)        : YOUR Linear layer for classification
  Module 04 (Losses)        : YOUR CrossEntropyLoss
  Module 05 (DataLoader)    : YOUR data batching system
  Module 06 (Autograd)      : YOUR automatic differentiation
  Module 07 (Optimizers)    : YOUR SGD optimizer
  Module 08 (Training)      : YOUR training loops
  Module 09 (Convolutions)  : YOUR Conv2d + MaxPool2d  <-- NEW!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ—ï¸ ARCHITECTURE (Simple LeNet-style CNN):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Input Image â”‚    â”‚   Conv2D    â”‚    â”‚    ReLU     â”‚    â”‚  MaxPool2D  â”‚    â”‚   Flatten   â”‚    â”‚   Linear    â”‚
    â”‚   8Ã—8Ã—1     â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚â”€â”€â”€â–¶â”‚             â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚
    â”‚  Grayscale  â”‚    â”‚     09      â”‚    â”‚     02      â”‚    â”‚     09      â”‚    â”‚   72 dims   â”‚    â”‚   72â†’10     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         1â†’8 channels      Non-linear         2Ã—2 pooling        Spatialâ†’Dense     10 Classes
                         3Ã—3 kernel        activation         Reduces size

    Feature Map Dimensions:
    Input: (batch, 1, 8, 8)
    After Conv: (batch, 8, 6, 6)   â† 3Ã—3 kernel reduces 8â†’6
    After Pool: (batch, 8, 3, 3)   â† 2Ã—2 pooling halves dimensions
    Flatten: (batch, 72)           â† 8 Ã— 3 Ã— 3 = 72 features
    Output: (batch, 10)            â† 10 class probabilities

ğŸ” WHY CNNs > MLPs - The Key Insight:

    MLP (Milestone 03):                  CNN (This Milestone):

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚8Ã—8 img â”‚ â†’ Flatten â†’ 64 numbers    â”‚8Ã—8 img â”‚ â†’ Conv2d â†’ 8 feature maps
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                                    â†“
    Each pixel treated                   LOCAL patterns detected:
    INDEPENDENTLY                        â€¢ Horizontal edges
                                         â€¢ Vertical edges
    Shifting image 1 pixel               â€¢ Corners
    = COMPLETELY different               â€¢ Curves
    input to network!
                                         Shifting image 1 pixel
    âŒ No spatial awareness              = SAME features detected!

                                         âœ… Translation invariance!

ğŸ“Š EXPECTED RESULTS (Comparison with Milestone 03):

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Architecture     â”‚ Parameters     â”‚ Expected Acc.  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ MLP (M03)        â”‚ 2,378          â”‚ 75-85%         â”‚
    â”‚ CNN (This)       â”‚ ~800           â”‚ 85-95%         â”‚  â† BETTER with FEWER params!
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    The CNN should achieve ~10% HIGHER accuracy with 3Ã— FEWER parameters!
    This is the power of exploiting spatial structure.

ğŸ“Œ PART 2: After proving CNNs work, Part 2 (02_lecun_cifar10.py) scales to
   real 32Ã—32 color images using YOUR DataLoader!
"""

import sys
import os
import time
import pickle
import numpy as np
from pathlib import Path
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.live import Live
from rich.text import Text
from rich import box

# Add paths for local development
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

# Import TinyTorch components
from tinytorch import Tensor, SGD, CrossEntropyLoss
from tinytorch.core.spatial import Conv2d, MaxPool2d
from tinytorch.core.layers import Linear, ReLU
from tinytorch.core.dataloader import DataLoader, TensorDataset

console = Console()

# Note: Autograd is automatically enabled when tinytorch is imported

# =============================================================================
# ğŸ¯ YOUR TINYTORCH MODULES IN ACTION
# =============================================================================
#
# This milestone showcases YOUR NEW spatial modules for the first time:
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ What You Built      â”‚ How It's Used Here             â”‚ Systems Impact              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Module 01: Tensor   â”‚ 4D tensors for images          â”‚ (batch, channels, H, W)     â”‚
# â”‚                     â”‚ (batch, 1, 8, 8) grayscale     â”‚ format for spatial ops      â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 02: ReLU     â”‚ Non-linearity after convolutionâ”‚ Same as MLP, but on         â”‚
# â”‚                     â”‚ on 3D feature maps             â”‚ spatial feature maps!       â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 03: Linear   â”‚ Classification head only       â”‚ 72â†’10 (much smaller than    â”‚
# â”‚                     â”‚ (after spatial features)       â”‚ MLP's 64â†’32â†’10)             â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 09: Conv2d   â”‚ 3Ã—3 kernel detects local       â”‚ WEIGHT SHARING: same 3Ã—3    â”‚
# â”‚ â˜… NEW MODULE â˜…      â”‚ patterns (edges, curves)       â”‚ kernel used everywhere!     â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 09: MaxPool2dâ”‚ 2Ã—2 pooling reduces spatial    â”‚ TRANSLATION INVARIANCE:     â”‚
# â”‚ â˜… NEW MODULE â˜…      â”‚ dimensions while keeping       â”‚ small shifts don't matter   â”‚
# â”‚                     â”‚ strongest activations          â”‚                             â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# =============================================================================
# ğŸ†• WHAT'S NEW SINCE MILESTONE 03 (MLP)
# =============================================================================
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ MLP (Milestone 03)   â”‚ CNN (This Milestone)    â”‚ Why It's Better            â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Flatten first        â”‚ Conv2d first            â”‚ Preserves spatial structureâ”‚
# â”‚ 2,378 parameters     â”‚ ~800 parameters         â”‚ Weight sharing = efficient â”‚
# â”‚ No spatial awareness â”‚ Local connectivity      â”‚ Neighbors processed togetherâ”‚
# â”‚ Global patterns only â”‚ Hierarchical features   â”‚ Edges â†’ Shapes â†’ Objects   â”‚
# â”‚ 75-85% accuracy      â”‚ 85-95% accuracy         â”‚ ~10% improvement!          â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# =============================================================================


# ============================================================================
# ğŸ“Š DATA LOADING
# ============================================================================

def load_digits_dataset():
    """
    Load the TinyDigits dataset (8Ã—8 curated digits).

    Returns 150 training + 47 test grayscale images of handwritten digits (0-9).
    Each image is 8Ã—8 pixels, perfect for quick CNN demonstrations.
    Ships with TinyTorch - no downloads needed!
    """
    # Load from TinyDigits dataset (shipped with TinyTorch)
    project_root = Path(__file__).parent.parent.parent
    train_path = project_root / "datasets" / "tinydigits" / "train.pkl"
    test_path = project_root / "datasets" / "tinydigits" / "test.pkl"

    if not train_path.exists() or not test_path.exists():
        console.print(f"[red]âœ— TinyDigits dataset not found![/red]")
        console.print(f"[yellow]Expected location: {train_path.parent}[/yellow]")
        console.print("[yellow]Run: python3 datasets/tinydigits/create_tinydigits.py[/yellow]")
        sys.exit(1)

    # Load training data
    with open(train_path, 'rb') as f:
        train_data = pickle.load(f)
    train_images = train_data['images']  # (150, 8, 8)
    train_labels = train_data['labels']  # (150,)

    # Load test data
    with open(test_path, 'rb') as f:
        test_data = pickle.load(f)
    test_images = test_data['images']  # (47, 8, 8)
    test_labels = test_data['labels']  # (47,)

    # CNN expects (batch, channels, height, width)
    # Add channel dimension: (N, 8, 8) â†’ (N, 1, 8, 8)
    train_images = train_images[:, np.newaxis, :, :]  # (150, 1, 8, 8)
    test_images = test_images[:, np.newaxis, :, :]    # (47, 1, 8, 8)

    return (
        Tensor(train_images.astype(np.float32)),
        Tensor(train_labels.astype(np.int64)),
        Tensor(test_images.astype(np.float32)),
        Tensor(test_labels.astype(np.int64))
    )


# ============================================================================
# ğŸ—ï¸ NETWORK ARCHITECTURE
# ============================================================================

class SimpleCNN:
    """
    Simple Convolutional Neural Network for digit classification.

    Architecture inspired by LeNet-5 (1998):
    - Conv2d: Detects local patterns (edges, curves)
    - ReLU: Nonlinearity
    - MaxPool: Spatial down-sampling + translation invariance
    - Linear: Final classification

    Input: (batch, 1, 8, 8)
    Conv1: 1 â†’ 8 channels, 3Ã—3 kernel â†’ (batch, 8, 6, 6)
    Pool1: 2Ã—2 max pooling â†’ (batch, 8, 3, 3)
    Flatten: â†’ (batch, 72)
    Linear: 72 â†’ 10 classes
    """

    def __init__(self):
        # Convolutional layers
        self.conv1 = Conv2d(in_channels=1, out_channels=8, kernel_size=3)
        self.relu1 = ReLU()
        self.pool1 = MaxPool2d(kernel_size=2, stride=2)

        # After conv(3Ã—3) and pool(2Ã—2): 8Ã—8 â†’ 6Ã—6 â†’ 3Ã—3
        # Flattened size: 8 channels Ã— 3 Ã— 3 = 72
        self.fc = Linear(in_features=72, out_features=10)

        # Set requires_grad for all parameters
        self.conv1.weight.requires_grad = True
        self.conv1.bias.requires_grad = True
        self.fc.weight.requires_grad = True
        self.fc.bias.requires_grad = True

        self.params = [self.conv1.weight, self.conv1.bias, self.fc.weight, self.fc.bias]

    def __call__(self, x):
        """Make the model callable."""
        return self.forward(x)

    def forward(self, x):
        # Conv + ReLU + Pool
        out = self.conv1.forward(x)
        out = self.relu1.forward(out)
        out = self.pool1.forward(out)

        # Flatten: (batch, 8, 3, 3) â†’ (batch, 72)
        batch_size = out.shape[0]
        out = Tensor(out.data.reshape(batch_size, -1))

        # Final classification
        out = self.fc.forward(out)
        return out

    def parameters(self):
        return self.params


# ============================================================================
# ğŸ¯ TRAINING & EVALUATION
# ============================================================================

def train_epoch(model, dataloader, criterion, optimizer):
    """Train for one epoch."""
    total_loss = 0.0
    n_batches = 0

    for batch_images, batch_labels in dataloader:
        # Forward pass
        logits = model(batch_images)
        loss = criterion.forward(logits, batch_labels)

        # Backward pass
        loss.backward()

        # Update weights
        optimizer.step()
        optimizer.zero_grad()

        total_loss += loss.data.item()
        n_batches += 1

    return total_loss / n_batches


def evaluate_accuracy(model, images, labels):
    """Evaluate model accuracy on a dataset."""
    logits = model(images)
    predictions = np.argmax(logits.data, axis=1)
    accuracy = 100.0 * np.mean(predictions == labels.data)
    avg_loss = np.mean((predictions - labels.data) ** 2)
    return accuracy, avg_loss


# ============================================================================
# ğŸ¬ MAIN MILESTONE DEMONSTRATION
# ============================================================================

def train_cnn():
    """Main training loop following 5-Act structure."""

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 1: THE CHALLENGE ğŸ¯
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print(Panel.fit(
        "[bold cyan]1998: The Computer Vision Challenge[/bold cyan]\n\n"
        "[yellow]The Problem:[/yellow]\n"
        "MLPs flatten images â†’ lose spatial structure\n"
        "Each pixel treated independently\n"
        "Millions of parameters needed for larger images\n\n"
        "[green]The Innovation:[/green]\n"
        "Convolutional Neural Networks (CNNs)\n"
        "  â€¢ Shared weights across space (convolution)\n"
        "  â€¢ Local connectivity (receptive fields)\n"
        "  â€¢ Pooling for translation invariance\n\n"
        "[bold]Can spatial operations outperform dense layers?[/bold]",
        title="ğŸ¯ ACT 1: THE CHALLENGE",
        border_style="cyan",
        box=box.DOUBLE
    ))

    # Load data
    console.print("\n[bold]ğŸ“Š Loading Handwritten Digits Dataset...[/bold]")
    train_images, train_labels, test_images, test_labels = load_digits_dataset()

    console.print(f"  Training samples: [cyan]{len(train_images.data)}[/cyan]")
    console.print(f"  Test samples: [cyan]{len(test_images.data)}[/cyan]")
    console.print(f"  Image shape: [cyan]{train_images.data[0].shape}[/cyan] (1 channel, 8Ã—8 pixels)")
    console.print(f"  Classes: [cyan]10[/cyan] (digits 0-9)")

    # Show training data structure
    console.print(f"\n  [dim]Sample digit values (first image, top-left 3Ã—3):[/dim]")
    sample = train_images.data[0, 0, :3, :3]
    for row in sample:
        console.print(f"    {' '.join(f'{val:.2f}' for val in row)}")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 2: THE SETUP ğŸ—ï¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("[bold]ğŸ—ï¸  The Architecture:[/bold]")
    console.print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Input   â”‚    â”‚  Conv2d  â”‚    â”‚ ReLU â”‚    â”‚MaxPool2dâ”‚    â”‚ Flatten â”‚    â”‚ Linear â”‚
    â”‚ 1Ã—8Ã—8    â”‚â”€â”€â”€â–¶â”‚  1â†’8     â”‚â”€â”€â”€â–¶â”‚      â”‚â”€â”€â”€â–¶â”‚  2Ã—2    â”‚â”€â”€â”€â–¶â”‚ 8Ã—3Ã—3   â”‚â”€â”€â”€â–¶â”‚ 72â†’10  â”‚
    â”‚          â”‚    â”‚  3Ã—3     â”‚    â”‚      â”‚    â”‚         â”‚    â”‚  =72    â”‚    â”‚        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†‘ Detects                   â†‘ Spatial
                    local patterns              downsampling
    """)

    console.print("[bold]ğŸ”§ Components:[/bold]")
    console.print("  â€¢ Conv layer: Detects local patterns (edges, curves)")
    console.print("  â€¢ ReLU: Non-linear activation")
    console.print("  â€¢ MaxPool: Spatial downsampling + translation invariance")
    console.print("  â€¢ Linear: Final classification (72 â†’ 10 classes)")
    console.print("  â€¢ [bold cyan]Key insight: Shared weights â†’ 100Ã— fewer params![/bold cyan]")

    # Create model
    console.print("\nğŸ§  Building Convolutional Neural Network...")
    model = SimpleCNN()

    # Count parameters
    total_params = sum(np.prod(p.shape) for p in model.parameters())
    conv_params = np.prod(model.conv1.weight.shape) + np.prod(model.conv1.bias.shape)
    fc_params = np.prod(model.fc.weight.shape) + np.prod(model.fc.bias.shape)

    console.print(f"  âœ“ Conv layer: [cyan]{conv_params}[/cyan] parameters")
    console.print(f"  âœ“ FC layer: [cyan]{fc_params}[/cyan] parameters")
    console.print(f"  âœ“ Total: [bold cyan]{total_params}[/bold cyan] parameters")

    # Hyperparameters
    console.print("\n[bold]âš™ï¸  Training Configuration:[/bold]")
    epochs = 50
    batch_size = 32
    learning_rate = 0.01

    config_table = Table(show_header=False, box=None)
    config_table.add_row("Epochs:", f"[cyan]{epochs}[/cyan]")
    config_table.add_row("Batch size:", f"[cyan]{batch_size}[/cyan]")
    config_table.add_row("Learning rate:", f"[cyan]{learning_rate}[/cyan]")
    config_table.add_row("Optimizer:", "[cyan]SGD[/cyan]")
    config_table.add_row("Loss:", "[cyan]CrossEntropyLoss[/cyan]")
    console.print(config_table)

    # Create optimizer and loss
    optimizer = SGD(model.parameters(), lr=learning_rate)
    criterion = CrossEntropyLoss()

    # Create dataloader
    train_dataset = TensorDataset(train_images, train_labels)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 3: THE EXPERIMENT ğŸ”¬
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("[bold]ğŸ”¬ Training CNN on Handwritten Digits...[/bold]\n")

    # Before training
    initial_acc, initial_loss = evaluate_accuracy(model, test_images, test_labels)
    console.print(f"[yellow]Before training:[/yellow] Accuracy = {initial_acc:.1f}%\n")

    # Training loop
    history = {
        "train_loss": [],
        "test_accuracy": [],
        "train_accuracy": []  # Track training accuracy to detect overfitting
    }
    start_time = time.time()

    # Use Live display with spinner for real-time feedback
    with Live(console=console, refresh_per_second=10) as live:
        for epoch in range(epochs):
            # Update spinner before training
            spinner_text = Text()
            spinner_text.append("â ‹ ", style="cyan")
            spinner_text.append(f"Epoch {epoch+1:3d}/{epochs}  Training...")
            live.update(spinner_text)

            # Train
            train_loss = train_epoch(model, train_loader, criterion, optimizer)

            # Evaluate on both train and test
            train_acc, _ = evaluate_accuracy(model, train_images, train_labels)
            test_acc, _ = evaluate_accuracy(model, test_images, test_labels)

            history["train_loss"].append(train_loss)
            history["train_accuracy"].append(train_acc)
            history["test_accuracy"].append(test_acc)

            if (epoch + 1) % 5 == 0:  # Print every 5 epochs
                gap = train_acc - test_acc
                gap_indicator = "âš ï¸" if gap > 10 else "âœ“"
                live.console.print(
                    f"Epoch {epoch+1:3d}/{epochs}  "
                    f"Loss: {train_loss:.4f}  "
                    f"Train: {train_acc:.1f}%  "
                    f"Test: {test_acc:.1f}%  "
                    f"{gap_indicator} Gap: {gap:.1f}%"
                )

    training_time = time.time() - start_time

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 4: THE DIAGNOSIS ğŸ“Š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("[bold]ğŸ“Š The Results:[/bold]\n")

    final_train_acc = history["train_accuracy"][-1]
    final_test_acc = history["test_accuracy"][-1]
    final_loss = history["train_loss"][-1]
    overfitting_gap = final_train_acc - final_test_acc

    table = Table(title="Training Outcome", box=box.ROUNDED)
    table.add_column("Metric", style="cyan", width=20)
    table.add_column("Value", style="green", width=20)
    table.add_column("Status", style="magenta", width=20)

    table.add_row(
        "Train Accuracy",
        f"{final_train_acc:.1f}%",
        f"â†‘ +{final_train_acc - initial_acc:.1f}%"
    )
    table.add_row(
        "Test Accuracy",
        f"{final_test_acc:.1f}%",
        f"â†‘ +{final_test_acc - initial_acc:.1f}%"
    )
    table.add_row(
        "Overfitting Gap",
        f"{overfitting_gap:.1f}%",
        "âœ“ Healthy" if overfitting_gap < 10 else "âš ï¸ Overfitting"
    )
    table.add_row(
        "Training Time",
        f"{training_time*1000:.0f}ms",
        "â€”"
    )

    console.print(table)

    # Sample predictions
    console.print("\n[bold]ğŸ” Sample Predictions:[/bold]")
    sample_images = Tensor(test_images.data[:10])  # First 10 test samples
    logits = model(sample_images)
    predictions = np.argmax(logits.data, axis=1)

    samples_table = Table(show_header=True, box=box.SIMPLE)
    samples_table.add_column("True", style="cyan", justify="center")
    samples_table.add_column("Pred", style="green", justify="center")
    samples_table.add_column("Result", justify="center")

    for i in range(10):
        true_label = int(test_labels.data[i])
        pred_label = int(predictions[i])
        result = "âœ“" if true_label == pred_label else "âœ—"
        style = "green" if true_label == pred_label else "red"
        samples_table.add_row(str(true_label), str(pred_label), f"[{style}]{result}[/{style}]")

    console.print(samples_table)

    # Key insights
    console.print("\n[bold]ğŸ’¡ Key Insights:[/bold]")
    console.print(f"  â€¢ CNNs preserve spatial structure")
    console.print(f"  â€¢ Conv layers detect local patterns (edges â†’ digits)")
    console.print(f"  â€¢ Pooling provides translation invariance")
    console.print(f"  â€¢ {total_params} params vs ~5,000 for MLP with similar accuracy!")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 5: THE REFLECTION ğŸŒŸ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("")
    console.print(Panel.fit(
        "[bold green]ğŸ‰ Success! Your CNN Learned to Recognize Digits![/bold green]\n\n"

        f"Test accuracy: [bold]{final_test_acc:.1f}%[/bold] (Gap: {overfitting_gap:.1f}%)\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸ’¡ What YOU Just Accomplished:[/bold]\n"
        "  âœ“ Built a Convolutional Neural Network from scratch\n"
        "  âœ“ Used Conv2d for spatial feature extraction\n"
        "  âœ“ Applied MaxPooling for translation invariance\n"
        f"  âœ“ Achieved {final_test_acc:.1f}% test accuracy!\n"
        f"  âœ“ Model generalizes well (gap: {overfitting_gap:.1f}%)\n"
        "  âœ“ Used 100Ã— fewer parameters than MLP!\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸ“ Why This Matters:[/bold]\n"
        "  LeNet-5 (1998) proved CNNs work for real-world vision.\n"
        "  This breakthrough led to:\n"
        "  â€¢ AlexNet (2012) - ImageNet revolution\n"
        "  â€¢ VGG, ResNet, modern computer vision\n"
        "  â€¢ Self-driving cars, medical imaging, face recognition\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸ“Œ The Key Breakthrough:[/bold]\n"
        "  [yellow]Spatial structure matters![/yellow]\n"
        "  MLPs: Every pixel connects to everything â†’ explosion\n"
        "  CNNs: Local connectivity + shared weights â†’ efficiency\n"
        "  \n"
        "  This is why CNNs dominate computer vision today!\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸš€ What's Next:[/bold]\n"
        "[dim]You've now built the complete ML training pipeline:\n"
        "  Tensors â†’ Layers â†’ Optimizers â†’ DataLoaders â†’ CNNs\n"
        "  \n"
        "  Next modules will add modern techniques:\n"
        "  â€¢ Normalization, Dropout, Advanced architectures\n"
        "  â€¢ Attention mechanisms, Transformers\n"
        "  â€¢ Production systems, Optimization, Deployment![/dim]",

        title="ğŸŒŸ 1998 CNN Revolution Complete",
        border_style="green",
        box=box.DOUBLE
    ))


if __name__ == "__main__":
    train_cnn()
