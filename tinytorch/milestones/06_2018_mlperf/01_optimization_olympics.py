#!/usr/bin/env python3
"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           üèÜ MILESTONE 06: The Optimization Olympics (MLPerf 2018)           ‚ïë
‚ïë                  Optimize YOUR Network from Earlier Milestones               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Historical Context:
In 2018, MLPerf was launched to standardize ML benchmarking. The key insight:
It's not just about accuracy - production ML needs efficiency too.

üéØ WHAT MAKES THIS SPECIAL:
This milestone uses YOUR implementations from EVERY previous module:
  ‚Ä¢ YOUR Tensor (Module 01)
  ‚Ä¢ YOUR Layers (Module 03)  
  ‚Ä¢ YOUR Training (Module 07)
  ‚Ä¢ YOUR Profiler (Module 14) 
  ‚Ä¢ YOUR Quantization (Module 15)
  ‚Ä¢ YOUR Compression (Module 16)
  ‚Ä¢ YOUR Benchmarking (Module 19)

Everything builds on everything!

üèóÔ∏è THE OPTIMIZATION PIPELINE (Using YOUR APIs):
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                      YOUR TRAINED MLP (from Milestone 03)               ‚îÇ
    ‚îÇ                        Accurate but needs optimization                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ           STEP 1: PROFILE (using YOUR Profiler class)                   ‚îÇ
    ‚îÇ                  Count parameters, measure latency                       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ        STEP 2: QUANTIZE (using YOUR QuantizationComplete class)         ‚îÇ
    ‚îÇ                   FP32 ‚Üí INT8 (4√ó compression)                           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ        STEP 3: PRUNE (using YOUR CompressionComplete class)             ‚îÇ
    ‚îÇ                Remove small weights (2-4√ó compression)                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ      STEP 4: BENCHMARK (using YOUR TinyMLPerf class)                    ‚îÇ
    ‚îÇ              Compare before vs after with scientific rigor               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚úÖ REQUIRED MODULES (Run after Module 19):
  Module 01-03: Tensor, Activations, Layers - YOUR base model
  Module 14: Profiling - YOUR Profiler class
  Module 15: Quantization - YOUR QuantizationComplete class
  Module 16: Compression - YOUR CompressionComplete class
  Module 19: Benchmarking - YOUR TinyMLPerf class
"""

import sys
import os
import time
import copy
import numpy as np
from pathlib import Path

# Add project root
sys.path.insert(0, os.getcwd())

from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich import box

console = Console()


def main():
    # ========================================================================
    # WELCOME BANNER
    # ========================================================================
    
    console.print(Panel(
        "[bold magenta]‚ïî‚ïê‚ïê‚ïê Milestone 06: MLPerf ‚ïê‚ïê‚ïê‚ïê‚ïó[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] [bold]üèÜ THE OPTIMIZATION         [/bold][bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] [bold]OLYMPICS                    [/bold][bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta]                             [bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] MLPerf 2018: Where accuracy [bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] meets efficiency            [bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta]                             [bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] [cyan]Using YOUR implementations[/cyan] [bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] [cyan]from every module![/cyan]        [bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù[/bold magenta]",
        border_style="bright_magenta"
    ))
    
    # ========================================================================
    # IMPORT YOUR IMPLEMENTATIONS
    # ========================================================================
    
    console.print("\n[bold cyan]üì¶ Loading YOUR TinyTorch implementations...[/bold cyan]\n")
    
    try:
        # Core building blocks (Modules 01-03)
        from tinytorch.core.tensor import Tensor
        from tinytorch.core.layers import Linear
        from tinytorch.core.activations import ReLU
        console.print("  [green]‚úì[/green] Tensor, Linear, ReLU (YOUR implementations)")
        
        # YOUR Profiler (Module 14)
        from tinytorch.profiling.profiler import Profiler
        console.print("  [green]‚úì[/green] Profiler (YOUR Module 14 implementation)")
        
        # YOUR Quantization (Module 15)
        from tinytorch.perf.quantization import Quantizer
        console.print("  [green]‚úì[/green] Quantizer (YOUR Module 15 implementation)")
        
        # YOUR Compression (Module 16) 
        from tinytorch.perf.compression import Compressor
        console.print("  [green]‚úì[/green] Compressor (YOUR Module 16 implementation)")
        
        # YOUR KV Cache (Module 17)
        from tinytorch.perf.memoization import KVCache
        console.print("  [green]‚úì[/green] KVCache (YOUR Module 17 implementation)")
        
        # YOUR Acceleration (Module 18)
        from tinytorch.perf.acceleration import vectorized_matmul, fused_gelu
        console.print("  [green]‚úì[/green] vectorized_matmul, fused_gelu (YOUR Module 18 implementation)")
        
        # YOUR Benchmarking (Module 19)
        from tinytorch.benchmarking.benchmark import Benchmark, TinyMLPerf
        console.print("  [green]‚úì[/green] Benchmark, TinyMLPerf (YOUR Module 19 implementation)")
        
    except ImportError as e:
        console.print(Panel(
            f"[red]Import Error: {e}[/red]\n\n"
            f"[yellow]This milestone requires optimization modules.[/yellow]\n"
            f"[dim]Make sure you've completed and exported modules 01-03, 14-16[/dim]",
            title="Missing Modules",
            border_style="red"
        ))
        return 1
    
    console.print("\n[green]‚úÖ All YOUR implementations loaded successfully![/green]\n")
    
    # ========================================================================
    # IMPORT NETWORKS FROM PREVIOUS MILESTONES
    # ========================================================================
    
    console.print(Panel(
        "[bold cyan]üß† Loading Networks from Previous Milestones[/bold cyan]\n"
        "Using the same architectures you built earlier!",
        border_style="cyan"
    ))
    
    # Import networks (same architectures from earlier milestones, pre-built for optimization)
    try:
        # Import from local networks.py (same folder)
        sys.path.insert(0, str(Path(__file__).parent))
        from networks import DigitMLP, SimpleCNN, MinimalTransformer, Perceptron
        
        console.print("  [green]‚úì[/green] Perceptron (Milestone 01)")
        console.print("  [green]‚úì[/green] DigitMLP (Milestone 03)")
        console.print("  [green]‚úì[/green] SimpleCNN (Milestone 04)")
        console.print("  [green]‚úì[/green] MinimalTransformer (Milestone 05)")
    except ImportError as e:
        console.print(f"[yellow]‚ö†Ô∏è Could not import milestone networks: {e}[/yellow]")
        console.print("[dim]Falling back to inline MLP definition[/dim]")
        
        # Fallback: define inline
        class DigitMLP:
            def __init__(self, input_size=64, hidden_size=32, num_classes=10):
                self.fc1 = Linear(input_size, hidden_size)
                self.relu = ReLU()
                self.fc2 = Linear(hidden_size, num_classes)
                self.layers = [self.fc1, self.fc2]
                self.name = "DigitMLP"
                
            def forward(self, x):
                if len(x.shape) > 2:
                    x = x.reshape(x.shape[0], -1)
                x = self.fc1(x)
                x = self.relu(x)
                x = self.fc2(x)
                return x
            
            def __call__(self, x):
                return self.forward(x)
            
            def parameters(self):
                params = []
                for layer in self.layers:
                    params.extend(layer.parameters())
                return params
    
    # Use the MLP from Milestone 03
    model = DigitMLP()
    console.print(f"\n  [bold green]Using: {model.name}[/bold green] (same as Milestone 03)")
    
    # Load TinyDigits for testing
    console.print("\n[bold cyan]üìä Loading TinyDigits dataset...[/bold cyan]")
    
    try:
        from tinytorch.datasets import TinyDigits
        dataset = TinyDigits()
        X_train, y_train = dataset.get_train_data()
        X_test, y_test = dataset.get_test_data()
        
        # Convert to Tensors and flatten
        X_train = Tensor(X_train.reshape(X_train.shape[0], -1).astype(np.float32))
        X_test = Tensor(X_test.reshape(X_test.shape[0], -1).astype(np.float32))
        
        console.print(f"  [green]‚úì[/green] Training: {len(y_train)} samples")
        console.print(f"  [green]‚úì[/green] Test: {len(y_test)} samples")
    except Exception as e:
        # Fallback: create synthetic data
        console.print(f"  [yellow]‚ö†Ô∏è TinyDigits not available, using synthetic data[/yellow]")
        X_train = Tensor(np.random.randn(1000, 64).astype(np.float32))
        y_train = np.random.randint(0, 10, 1000)
        X_test = Tensor(np.random.randn(200, 64).astype(np.float32))
        y_test = np.random.randint(0, 10, 200)
    
    # Quick training to establish baseline accuracy
    console.print("\n[bold cyan]üèãÔ∏è Quick training (10 epochs)...[/bold cyan]")
    
    from tinytorch.core.optimizers import SGD
    from tinytorch.core.losses import CrossEntropyLoss
    
    optimizer = SGD(model.parameters(), lr=0.01)
    loss_fn = CrossEntropyLoss()
    
    with Progress(SpinnerColumn(), TextColumn("{task.description}"), transient=True) as progress:
        task = progress.add_task("Training...", total=10)
        
        for epoch in range(10):
            # Mini-batch training
            batch_size = 32
            for i in range(0, min(500, len(y_train)), batch_size):
                batch_x = Tensor(X_train.data[i:i+batch_size])
                batch_y = y_train[i:i+batch_size]
                
                # Forward
                output = model(batch_x)
                loss = loss_fn(output, Tensor(batch_y))
                
                # Backward
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            
            progress.advance(task)
    
    console.print("  [green]‚úì[/green] Training complete\n")
    
    # ========================================================================
    # STEP 1: PROFILE WITH YOUR PROFILER
    # ========================================================================
    
    console.print(Panel(
        "[bold blue]üìä STEP 1: Profile with YOUR Profiler[/bold blue]\n"
        "Using the Profiler class you built in Module 14",
        border_style="blue"
    ))
    
    profiler = Profiler()
    
    # Count parameters
    param_count = profiler.count_parameters(model)
    
    # Estimate model size
    param_bytes = param_count * 4  # FP32 = 4 bytes
    
    # Count FLOPs (computational cost)
    input_shape = (1, 64)
    flops = profiler.count_flops(model, input_shape)
    
    # Measure inference latency with proper statistics
    sample_input = Tensor(np.random.randn(1, 64).astype(np.float32))
    latency_ms = profiler.measure_latency(model, sample_input, warmup=3, iterations=10)
    
    # Calculate throughput
    throughput = 1000 / latency_ms if latency_ms > 0 else 0
    
    # Calculate baseline accuracy
    outputs = model(X_test)
    predictions = np.argmax(outputs.data, axis=1)
    baseline_acc = np.mean(predictions == y_test) * 100
    
    # Show baseline metrics - comprehensive profile
    table = Table(title="üìä Baseline Profile (YOUR Profiler - Module 14)", box=box.DOUBLE)
    table.add_column("Metric", style="cyan", width=18)
    table.add_column("Value", style="yellow", justify="right")
    table.add_column("Notes", style="dim")
    
    table.add_row("Parameters", f"{param_count:,}", "Total trainable weights")
    table.add_row("Size", f"{param_bytes:,} bytes", "FP32 precision")
    table.add_row("FLOPs", f"{flops:,}", "Operations per inference")
    table.add_row("", "", "")
    table.add_row("Accuracy", f"{baseline_acc:.1f}%", "Test set performance")
    table.add_row("Latency", f"{latency_ms:.3f} ms", "Per-sample inference")
    table.add_row("Throughput", f"{throughput:.0f} samples/sec", "Inference speed")
    
    console.print(table)
    console.print()
    
    # ========================================================================
    # STEP 2: QUANTIZE WITH YOUR QUANTIZATION
    # ========================================================================
    
    console.print(Panel(
        "[bold yellow]üóúÔ∏è STEP 2: Quantize with YOUR QuantizationComplete[/bold yellow]\n"
        "Using the quantization you built in Module 15\n"
        "FP32 ‚Üí INT8 = 4√ó smaller",
        border_style="yellow"
    ))
    
    # Use YOUR Quantizer class
    quant_result = Quantizer.quantize_model(model)
    
    quant_size = int(param_bytes / quant_result['compression_ratio'])
    
    # Show quantization results
    table = Table(title="üóúÔ∏è After Quantization (YOUR Implementation)", box=box.ROUNDED)
    table.add_column("Metric", style="cyan")
    table.add_column("Before", style="yellow")
    table.add_column("After", style="green")
    table.add_column("Change", style="bold")
    
    table.add_row(
        "Size",
        f"{param_bytes:,} B",
        f"{quant_size:,} B",
        f"[green]{quant_result['compression_ratio']:.1f}√ó smaller[/green]"
    )
    table.add_row(
        "Precision",
        "FP32 (32-bit)",
        "INT8 (8-bit)",
        "[green]4√ó memory reduction[/green]"
    )
    
    console.print(table)
    console.print()
    
    # ========================================================================
    # STEP 3: PRUNE WITH YOUR COMPRESSION
    # ========================================================================
    
    console.print(Panel(
        "[bold magenta]‚úÇÔ∏è STEP 3: Prune with YOUR CompressionComplete[/bold magenta]\n"
        "Using the compression you built in Module 16\n"
        "Remove 50% of smallest weights",
        border_style="magenta"
    ))
    
    # Create a copy for pruning
    model_copy = DigitMLP()
    for i, layer in enumerate(model.layers):
        for j, param in enumerate(layer.parameters()):
            model_copy.layers[i].parameters()[j].data = param.data.copy()
    
    # Use YOUR Compressor class
    sparsity_before = Compressor.measure_sparsity(model_copy)
    Compressor.magnitude_prune(model_copy, sparsity=0.5)
    sparsity_after = Compressor.measure_sparsity(model_copy)
    
    # Calculate pruned accuracy
    outputs_pruned = model_copy(X_test)
    predictions_pruned = np.argmax(outputs_pruned.data, axis=1)
    pruned_acc = np.mean(predictions_pruned == y_test) * 100
    
    # Show pruning results
    table = Table(title="‚úÇÔ∏è After Pruning (YOUR Implementation)", box=box.ROUNDED)
    table.add_column("Metric", style="cyan")
    table.add_column("Before", style="yellow")
    table.add_column("After", style="green")
    table.add_column("Change", style="bold")
    
    table.add_row(
        "Sparsity",
        f"{sparsity_before:.1%}",
        f"{sparsity_after:.1%}",
        f"[green]{sparsity_after:.0%} weights zeroed[/green]"
    )
    prune_acc_delta = pruned_acc - baseline_acc
    table.add_row(
        "Accuracy",
        f"{baseline_acc:.1f}%",
        f"{pruned_acc:.1f}%",
        f"[{'green' if prune_acc_delta >= 0 else 'red'}]{prune_acc_delta:+.1f}%[/]"
    )
    
    console.print(table)
    console.print()
    
    # ========================================================================
    # STEP 4: KV CACHE (Using YOUR Module 17 - Transformers Only)
    # ========================================================================
    
    console.print(Panel(
        "[bold cyan]‚ö° STEP 4: KV Cache with YOUR Module 17[/bold cyan]\n"
        "Using KVCache for transformer generation speedup\n"
        "Caches K,V to avoid recomputation during autoregressive generation",
        border_style="cyan"
    ))
    
    # Demo KV Cache with transformer architecture
    try:
        transformer = MinimalTransformer(vocab_size=27, embed_dim=32, num_heads=2, seq_len=8)
        
        # Create KV Cache for the transformer
        kv_cache = KVCache(
            batch_size=1,
            max_seq_len=8,
            num_layers=1,  # MinimalTransformer has 1 attention layer
            num_heads=2,
            head_dim=16  # embed_dim / num_heads = 32/2 = 16
        )
        
        # Show cache statistics
        cache_memory = kv_cache.batch_size * kv_cache.max_seq_len * kv_cache.num_layers * kv_cache.num_heads * kv_cache.head_dim * 2 * 4  # K+V, float32
        
        table = Table(title="‚ö° KV Cache Stats (YOUR Module 17)", box=box.ROUNDED)
        table.add_column("Property", style="cyan")
        table.add_column("Value", style="yellow")
        table.add_column("Notes", style="dim")
        
        table.add_row("Max Sequence", f"{kv_cache.max_seq_len}", "Tokens cacheable")
        table.add_row("Num Layers", f"{kv_cache.num_layers}", "Transformer layers")
        table.add_row("Num Heads", f"{kv_cache.num_heads}", "Attention heads")
        table.add_row("Cache Memory", f"{cache_memory:,} bytes", "Pre-allocated K+V")
        table.add_row("", "", "")
        table.add_row("Speedup", "~N√ó (N=seq_len)", "Avoids recomputation")
        
        console.print(table)
        console.print("  [green]‚úì[/green] KV Cache ready for generation!")
        
    except Exception as e:
        console.print(f"  [yellow]‚ö†Ô∏è KV Cache demo skipped: {e}[/yellow]")
    
    console.print()
    
    # ========================================================================
    # STEP 5: ACCELERATION (Using YOUR Module 18)
    # ========================================================================
    
    console.print(Panel(
        "[bold magenta]üöÄ STEP 5: Acceleration with YOUR Module 18[/bold magenta]\n"
        "Using vectorized operations for compute speedup\n"
        "BLAS-optimized matmul and fused operations",
        border_style="magenta"
    ))
    
    # Demo vectorized matmul speedup
    import time
    
    # Create test matrices
    A = Tensor(np.random.randn(64, 128).astype(np.float32))
    B = Tensor(np.random.randn(128, 64).astype(np.float32))
    
    # Time standard operation
    start = time.time()
    for _ in range(100):
        C_standard = Tensor(np.dot(A.data, B.data))
    standard_time = (time.time() - start) * 1000  # ms
    
    # Time vectorized operation
    start = time.time()
    for _ in range(100):
        C_vectorized = vectorized_matmul(A, B)
    vectorized_time = (time.time() - start) * 1000  # ms
    
    table = Table(title="üöÄ Acceleration Results (YOUR Module 18)", box=box.ROUNDED)
    table.add_column("Operation", style="cyan")
    table.add_column("Time (100 runs)", style="yellow")
    table.add_column("Notes", style="dim")
    
    table.add_row("Standard np.dot", f"{standard_time:.2f} ms", "Baseline")
    table.add_row("vectorized_matmul", f"{vectorized_time:.2f} ms", "YOUR implementation")
    table.add_row("Matrix Shape", f"{A.shape} @ {B.shape}", f"‚Üí {C_vectorized.shape}")
    
    console.print(table)
    console.print("  [green]‚úì[/green] Vectorized operations ready!")
    console.print()
    
    # ========================================================================
    # STEP 6: BENCHMARK (Using YOUR Benchmark & Profiler - Modules 14 & 19)
    # ========================================================================
    
    console.print(Panel(
        "[bold green]üèÅ STEP 6: Benchmark with YOUR Modules 14 & 19[/bold green]\n"
        "Using Benchmark class for standardized measurements\n"
        "Reproducible, statistically rigorous",
        border_style="green"
    ))
    
    console.print("  Running standardized benchmark with YOUR implementations...")
    
    # Use YOUR Benchmark class from Module 19
    # Benchmark needs models and datasets
    test_dataset = [(X_test, y_test)]
    benchmark = Benchmark(models=[model], datasets=test_dataset)
    
    # Run latency benchmark using YOUR implementation
    latency_results = benchmark.run_latency_benchmark(input_shape=(1, 64))
    bench_result = list(latency_results.values())[0]  # Get first model's result
    
    # Extract metrics from YOUR BenchmarkResult
    mean_latency = bench_result.mean
    std_latency = bench_result.std
    min_latency = bench_result.min_val
    max_latency = bench_result.max_val
    # Calculate P95 from values
    sorted_vals = sorted(bench_result.values)
    p95_idx = int(len(sorted_vals) * 0.95)
    p95_latency = sorted_vals[min(p95_idx, len(sorted_vals) - 1)]
    
    # Calculate derived metrics
    throughput = 1000 / mean_latency if mean_latency > 0 else 0
    
    # Show benchmark results
    table = Table(title="üèÅ Benchmark Results (YOUR Modules 14 & 19)", box=box.DOUBLE)
    table.add_column("Metric", style="cyan", width=18)
    table.add_column("Value", style="yellow", justify="right")
    table.add_column("Target", style="dim")
    
    table.add_row(
        "Latency (mean)",
        f"{mean_latency:.3f} ms",
        "< 100ms"
    )
    table.add_row(
        "Latency (std)",
        f"¬± {std_latency:.3f} ms",
        "Low = stable"
    )
    table.add_row(
        "Latency (min/max)",
        f"{min_latency:.3f} / {max_latency:.3f} ms",
        "Tight range"
    )
    table.add_row(
        "P95 Latency",
        f"{p95_latency:.3f} ms",
        "< 2√ó mean"
    )
    table.add_row("", "", "")
    table.add_row(
        "Throughput",
        f"{throughput:.0f} samples/sec",
        "Higher = better"
    )
    table.add_row(
        "Accuracy",
        f"{baseline_acc:.1f}%",
        "> 80%"
    )
    
    console.print(table)
    console.print()
    
    # ========================================================================
    # FINAL SUMMARY
    # ========================================================================
    
    console.print("=" * 70)
    console.print(Panel("[bold]üèÜ OPTIMIZATION OLYMPICS RESULTS[/bold]", border_style="gold1"))
    console.print()
    
    # Final comparison with clear accuracy delta
    table = Table(title="üéñÔ∏è Your Optimization Journey", box=box.DOUBLE)
    table.add_column("Stage", style="cyan", width=25)
    table.add_column("Size", style="yellow", justify="right")
    table.add_column("Baseline Acc", style="dim", justify="right")
    table.add_column("New Acc", style="green", justify="right")
    table.add_column("Œî Accuracy", style="bold", justify="right")
    table.add_column("YOUR Module", style="magenta")
    
    # Quantization typically preserves accuracy
    quant_acc = baseline_acc  # Quantization preserves accuracy
    quant_delta = quant_acc - baseline_acc
    prune_delta = pruned_acc - baseline_acc
    
    table.add_row(
        "üìä Baseline",
        f"{param_bytes:,} B",
        f"{baseline_acc:.1f}%",
        f"{baseline_acc:.1f}%",
        "‚Äî",
        "Profiler (14)"
    )
    table.add_row(
        "üóúÔ∏è + Quantization",
        f"{quant_size:,} B",
        f"{baseline_acc:.1f}%",
        f"{quant_acc:.1f}%",
        f"[green]{quant_delta:+.1f}%[/green]" if quant_delta >= 0 else f"[red]{quant_delta:+.1f}%[/red]",
        "Quantization (15)"
    )
    table.add_row(
        "‚úÇÔ∏è + Pruning",
        f"~{param_bytes//2:,} B**",
        f"{baseline_acc:.1f}%",
        f"{pruned_acc:.1f}%",
        f"[green]{prune_delta:+.1f}%[/green]" if prune_delta >= 0 else f"[red]{prune_delta:+.1f}%[/red]",
        "Compression (16)"
    )
    
    console.print(table)
    console.print("[dim]** With sparse storage[/dim]")
    console.print()
    
    # Key insights
    console.print(Panel(
        "[bold green]üéì KEY INSIGHTS[/bold green]\n\n"
        f"‚úÖ [cyan]YOUR Profiler (Module 14):[/cyan]\n"
        f"   ‚Ä¢ Measured {param_count:,} parameters, {flops:,} FLOPs\n"
        f"   ‚Ä¢ Found baseline latency: {latency_ms:.3f}ms\n\n"
        f"‚úÖ [cyan]YOUR Quantization (Module 15):[/cyan]\n"
        f"   ‚Ä¢ Achieved {quant_result['compression_ratio']:.1f}√ó compression\n"
        f"   ‚Ä¢ FP32 ‚Üí INT8 reduces memory 4√ó\n\n"
        f"‚úÖ [cyan]YOUR Compression (Module 16):[/cyan]\n"
        f"   ‚Ä¢ Pruned to {sparsity_after:.0%} sparsity\n"
        f"   ‚Ä¢ {abs(baseline_acc - pruned_acc):.1f}% accuracy impact\n\n"
        f"‚úÖ [cyan]YOUR KV Cache (Module 17):[/cyan]\n"
        f"   ‚Ä¢ Pre-allocated cache for transformer generation\n"
        f"   ‚Ä¢ ~N√ó speedup for sequence length N\n\n"
        f"‚úÖ [cyan]YOUR Acceleration (Module 18):[/cyan]\n"
        f"   ‚Ä¢ BLAS-optimized matrix operations\n"
        f"   ‚Ä¢ Vectorized compute kernels\n\n"
        f"üí° [yellow]Challenge: Combine All Techniques![/yellow]\n"
        f"   ‚Ä¢ Quantize + Prune + KV Cache = production-ready\n"
        f"   ‚Ä¢ This is real ML systems engineering!",
        border_style="cyan",
        box=box.ROUNDED
    ))
    
    # Success message
    console.print(Panel(
        "[bold green]üèÜ MILESTONE COMPLETE![/bold green]\n\n"
        "[green]You used YOUR implementations from:[/green]\n"
        "  ‚Ä¢ Module 01-03: Tensor, Linear, ReLU\n"
        "  ‚Ä¢ Module 14: Profiler\n"
        "  ‚Ä¢ Module 15: Quantizer\n"
        "  ‚Ä¢ Module 16: Compressor\n"
        "  ‚Ä¢ Module 17: KVCache\n"
        "  ‚Ä¢ Module 18: vectorized_matmul\n"
        "  ‚Ä¢ Module 19: Benchmark\n\n"
        "[bold]Everything you built... now works together![/bold]\n\n"
        "[cyan]What you learned:[/cyan]\n"
        "  ‚úÖ Profile models systematically\n"
        "  ‚úÖ Quantize for memory efficiency\n"
        "  ‚úÖ Prune for sparse models\n"
        "  ‚úÖ Cache K,V for fast generation\n"
        "  ‚úÖ Accelerate with vectorized ops\n"
        "  ‚úÖ Benchmark with scientific rigor\n\n"
        "[bold]You've learned ML Systems Engineering![/bold]",
        title="üéØ Milestone 06 Complete",
        border_style="bright_green",
        box=box.DOUBLE,
        padding=(1, 2)
    ))
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
