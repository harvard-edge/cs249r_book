#!/usr/bin/env python3
"""
XOR Solved! Multi-Layer Networks (1986)
========================================

ğŸ“š HISTORICAL CONTEXT:
After the 1969 XOR crisis killed neural networks, research funding dried up for over
a decade. Then in 1986, Rumelhart, Hinton, and Williams published the backpropagation
algorithm for training multi-layer networks - and XOR became trivial!

ğŸ¯ MILESTONE 2 PART 2: THE SOLUTION (After Modules 01-08)
Watch a multi-layer network SOLVE the "impossible" XOR problem that stumped AI for
17 years. The secret? Hidden layers + backpropagation (which YOU just built!).

âœ… REQUIRED MODULES (Run after Module 08):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Module 01 (Tensor)        : YOUR data structure with autodiff
  Module 02 (Activations)   : YOUR ReLU and Sigmoid (non-linearity!)
  Module 03 (Layers)        : YOUR Linear layers (multiple layers!)
  Module 04 (Losses)        : YOUR loss function
  Module 06 (Autograd)      : YOUR backpropagation through hidden layers
  Module 07 (Optimizers)    : YOUR SGD optimizer
  Module 08 (Training)      : YOUR training loop
  (Module 05 DataLoader skipped: XOR has only 4 points, no batching needed)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ—ï¸ ARCHITECTURE (The Multi-Layer Solution):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Input       â”‚    â”‚   Linear    â”‚    â”‚    ReLU     â”‚    â”‚   Linear    â”‚    â”‚  Sigmoid    â”‚
    â”‚ Features    â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚â”€â”€â”€â–¶â”‚ YOUR Module â”‚
    â”‚ (x1, x2)    â”‚    â”‚     03      â”‚    â”‚     02      â”‚    â”‚     03      â”‚    â”‚     02      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         2 inputs           2â†’4               Non-             4â†’1             Output
                        Hidden Layer!      linearity!      Combines to       probability
                                                           class prob

    THE KEY: Hidden layer creates NEW features that make XOR linearly separable!

ğŸ” HOW IT WORKS - Feature Learning:

    Original XOR Space:              Hidden Layer Feature Space:
    (NOT linearly separable)         (NOW linearly separable!)

    1 â”‚ â—‹ (0,1)    â— (1,1)           The 4 hidden units learn:
      â”‚   [1]       [0]               â€¢ hâ‚: detects "xâ‚ AND NOT xâ‚‚"
      â”‚                               â€¢ hâ‚‚: detects "xâ‚‚ AND NOT xâ‚"
    0 â”‚ â— (0,0)    â—‹ (1,0)           â€¢ hâ‚ƒ: detects "xâ‚ AND xâ‚‚"
      â”‚   [0]       [1]               â€¢ hâ‚„: detects "NOT xâ‚ AND NOT xâ‚‚"
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        0          1                 In this new 4D space, a single
                                     linear boundary WORKS!
    No line works here!

    Mathematical Transformation:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Original: (xâ‚, xâ‚‚)  â†’  Hidden: (hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„)  â†’  Output: y        â”‚
    â”‚                             â†‘                                        â”‚
    â”‚                     ReLU(Wâ‚Â·x + bâ‚)                                  â”‚
    â”‚                                                                      â”‚
    â”‚ The hidden layer TRANSFORMS the input space into one where           â”‚
    â”‚ XOR becomes a simple linear classification problem!                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š EXPECTED RESULTS:
- Training time: ~30 seconds
- Accuracy: 95-100% (problem solved!)
- Loss decreases smoothly
- Perfect XOR predictions
- âœ… YOUR backpropagation trains the hidden layer!

ğŸ”¥ THE BREAKTHROUGH (1986):
This is the architecture that ended the AI Winter! Rumelhart, Hinton, and Williams
proved that YOUR autograd can train hidden layers to learn useful features.
"""

import sys
import os
import numpy as np
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.live import Live
from rich.text import Text
from rich import box

# Add project root to path
sys.path.insert(0, os.getcwd())

# Seed will be set before training to guarantee 100% convergence

# Import TinyTorch components YOU BUILT!
from tinytorch import Tensor, Linear, ReLU, Sigmoid, BinaryCrossEntropyLoss, SGD

console = Console()

# =============================================================================
# ğŸ¯ YOUR TINYTORCH MODULES IN ACTION
# =============================================================================
#
# This milestone showcases the modules YOU built. Here's what powers this solution:
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ What You Built      â”‚ How It's Used Here             â”‚ Systems Impact              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Module 01: Tensor   â”‚ All data + gradients flow      â”‚ Automatic gradient tracking â”‚
# â”‚                     â”‚ through YOUR Tensor            â”‚ enables backpropagation     â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 02: ReLU     â”‚ Non-linearity in hidden layer  â”‚ Creates NON-LINEAR features â”‚
# â”‚            Sigmoid  â”‚ Output probability             â”‚ that make XOR separable!    â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 03: Linear   â”‚ TWO layers now!                â”‚ First layer: feature space  â”‚
# â”‚                     â”‚ (2â†’4) and (4â†’1)                â”‚ Second: classification      â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 04: Loss     â”‚ BinaryCrossEntropy measures    â”‚ Guides learning toward      â”‚
# â”‚                     â”‚ how wrong predictions are      â”‚ correct XOR outputs         â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 06: Autograd â”‚ .backward() computes gradients â”‚ Gradients flow through      â”‚
# â”‚                     â”‚ for BOTH layers automatically  â”‚ hidden layer to inputs!     â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 06: SGD      â”‚ Updates 13 parameters          â”‚ Adjusts weights to minimize â”‚
# â”‚                     â”‚ (2Ã—4 + 4 + 4Ã—1 + 1)            â”‚ loss function               â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# =============================================================================
# ğŸ†• WHAT'S NEW SINCE PART 1 (XOR Crisis)
# =============================================================================
#
# Part 1 FAILED because:          Part 2 SUCCEEDS because:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ Single Linear layer          â”‚ + Hidden Linear layer (2â†’4)                  â”‚
# â”‚ Only Sigmoid activation      â”‚ + ReLU activation (non-linearity!)           â”‚
# â”‚ No training (random weights) â”‚ + YOUR Autograd trains the hidden layer      â”‚
# â”‚ No optimizer                 â”‚ + YOUR SGD updates 13 parameters             â”‚
# â”‚ Max 75% accuracy             â”‚ + 95-100% accuracy (problem SOLVED!)         â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# =============================================================================


# ============================================================================
# ğŸ² DATA GENERATION
# ============================================================================

def generate_xor_data(n_samples=100):
    """Generate XOR dataset with slight noise."""
    # Generate each XOR case with repetition
    samples_per_case = n_samples // 4

    # Case 1: (0,0) â†’ 0
    x1 = np.random.randn(samples_per_case, 2) * 0.1 + np.array([0.0, 0.0])
    y1 = np.zeros((samples_per_case, 1))

    # Case 2: (0,1) â†’ 1
    x2 = np.random.randn(samples_per_case, 2) * 0.1 + np.array([0.0, 1.0])
    y2 = np.ones((samples_per_case, 1))

    # Case 3: (1,0) â†’ 1
    x3 = np.random.randn(samples_per_case, 2) * 0.1 + np.array([1.0, 0.0])
    y3 = np.ones((samples_per_case, 1))

    # Case 4: (1,1) â†’ 0
    x4 = np.random.randn(samples_per_case, 2) * 0.1 + np.array([1.0, 1.0])
    y4 = np.zeros((samples_per_case, 1))

    # Combine and shuffle
    X = np.vstack([x1, x2, x3, x4])
    y = np.vstack([y1, y2, y3, y4])

    indices = np.random.permutation(n_samples)
    X = X[indices]
    y = y[indices]

    return Tensor(X), Tensor(y)


# ============================================================================
# ğŸ—ï¸ MULTI-LAYER NETWORK (The Solution!)
# ============================================================================

class XORNetwork:
    """
    Multi-layer network that SOLVES XOR!

    The hidden layer creates new features that make XOR linearly separable.
    This is the architecture that ended the AI Winter.
    """

    def __init__(self, hidden_size=4):
        # Hidden layer - THE KEY INNOVATION!
        self.hidden = Linear(2, hidden_size)
        self.relu = ReLU()  # Non-linearity is essential!

        # Output layer
        self.output = Linear(hidden_size, 1)
        self.sigmoid = Sigmoid()

    def __call__(self, x):
        """
        Forward pass through hidden layer.

        Input â†’ Hidden Layer â†’ ReLU â†’ Output Layer â†’ Sigmoid
        """
        # Hidden layer transforms input space
        h = self.hidden(x)
        h_activated = self.relu(h)

        # Output layer in new feature space
        logits = self.output(h_activated)
        output = self.sigmoid(logits)

        return output

    def parameters(self):
        """Return all trainable parameters."""
        return self.hidden.parameters() + self.output.parameters()


# ============================================================================
# ğŸ”¥ TRAINING FUNCTION (That Will SUCCEED on XOR!)
# ============================================================================

def train_network(model, X, y, epochs=500, lr=0.5):
    """
    Train multi-layer network on XOR.

    This WILL succeed - hidden layers solve the problem!
    """
    loss_fn = BinaryCrossEntropyLoss()
    optimizer = SGD(model.parameters(), lr=lr)

    console.print("\n[bold cyan]ğŸ”¥ Training Multi-Layer Network...[/bold cyan]")
    console.print("[dim](This will work - hidden layers solve XOR!)[/dim]\n")

    history = {"loss": [], "accuracy": []}

    # Use Live display with spinner for real-time feedback
    with Live(console=console, refresh_per_second=10) as live:
        for epoch in range(epochs):
            # Forward pass
            predictions = model(X)
            loss = loss_fn(predictions, y)

            # Backward pass (through hidden layers!)
            loss.backward()

            # Update weights
            optimizer.step()
            optimizer.zero_grad()

            # Calculate accuracy
            pred_classes = (predictions.data > 0.5).astype(int)
            accuracy = (pred_classes == y.data).mean()

            history["loss"].append(loss.data.item())
            history["accuracy"].append(accuracy)

            # Update spinner with current progress
            spinner_text = Text()
            spinner_text.append("â ‹ ", style="cyan")
            spinner_text.append(f"Epoch {epoch+1:3d}/{epochs}  Loss: {loss.data:.4f}  Accuracy: {accuracy:.1%}")
            live.update(spinner_text)

            # Print progress every 100 epochs
            if (epoch + 1) % 100 == 0:
                live.console.print(f"Epoch {epoch+1:3d}/{epochs}  Loss: {loss.data:.4f}  Accuracy: {accuracy:.1%}")

    console.print("\n[green]âœ… Training Complete - XOR Solved![/green]")

    return history


# ============================================================================
# ğŸ“Š EVALUATION & CELEBRATION
# ============================================================================

def evaluate_and_celebrate(model, X, y, history):
    """Evaluate the successful model and celebrate the victory!"""

    predictions = model(X)
    pred_classes = (predictions.data > 0.5).astype(int)
    final_accuracy = (pred_classes == y.data).mean()

    # Get metrics
    initial_loss = history["loss"][0]
    final_loss = history["loss"][-1]
    initial_acc = history["accuracy"][0]
    final_acc = history["accuracy"][-1]

    console.print("[bold]ğŸ“Š The Results:[/bold]\n")

    table = Table(title="Training Outcome", box=box.ROUNDED)
    table.add_column("Metric", style="cyan", width=18)
    table.add_column("Before Training", style="yellow", width=16)
    table.add_column("After Training", style="green", width=16)
    table.add_column("Improvement", style="magenta", width=14)

    loss_improvement = f"-{initial_loss - final_loss:.4f}"
    acc_improvement = f"+{final_acc - initial_acc:.1%}"

    table.add_row("Loss", f"{initial_loss:.4f}", f"{final_loss:.4f}", loss_improvement)
    table.add_row("Accuracy", f"{initial_acc:.1%}", f"{final_acc:.1%}", acc_improvement)

    console.print(table)

    console.print("\n[bold]ğŸ” XOR Truth Table vs Predictions:[/bold]")
    console.print("[dim](The ultimate test - all 4 XOR cases!)[/dim]\n")
    test_inputs = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
    test_preds = model(Tensor(test_inputs))

    truth_table = Table(show_header=True, border_style="green")
    truth_table.add_column("xâ‚", style="cyan")
    truth_table.add_column("xâ‚‚", style="cyan")
    truth_table.add_column("XOR (True)", style="green")
    truth_table.add_column("Predicted", style="yellow")
    truth_table.add_column("Correct?", style="white")

    all_correct = True
    for i, (x1, x2) in enumerate(test_inputs):
        true_xor = int(x1 != x2)
        pred_prob = test_preds.data[i, 0]
        pred = int(pred_prob > 0.5)
        correct = pred == true_xor
        all_correct = all_correct and correct

        truth_table.add_row(
            f"{int(x1)}",
            f"{int(x2)}",
            f"{true_xor}",
            f"{pred} ({pred_prob:.3f})",
            "âœ…" if correct else "âŒ"
        )

    console.print(truth_table)

    if all_correct:
        console.print("\n[bold green]âœ¨ Perfect! All XOR cases correctly predicted![/bold green]")

    console.print("\n[bold]ğŸ’¡ Key Insights:[/bold]")
    console.print("  â€¢ Hidden layer transformed XOR into a solvable problem")
    console.print("  â€¢ Network learned non-linear decision boundary")
    console.print("  â€¢ Multi-layer networks can solve ANY classification problem!")


# ============================================================================
# ğŸ¯ MAIN EXECUTION
# ============================================================================

def main():
    """Demonstrate solving XOR with multi-layer networks."""

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 1: THE CHALLENGE ğŸ¯
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print(Panel.fit(
        "[bold cyan]ğŸ¯ 1986 - Ending the AI Winter[/bold cyan]\n\n"
        "[dim]Can neural networks solve non-linearly separable problems?[/dim]\n"
        "[dim]The XOR problem that stumped AI for 17 years![/dim]",
        title="ğŸ”¥ 1986 AI Renaissance",
        border_style="cyan",
        box=box.DOUBLE
    ))

    console.print("\n[bold]ğŸ“Š The Data:[/bold]")
    X, y = generate_xor_data(n_samples=100)
    console.print("  â€¢ Dataset: XOR problem (4 distinct cases)")
    console.print(f"  â€¢ Samples: {len(X.data)} (with slight noise)")
    console.print("  â€¢ Pattern: (0,0)â†’0, (0,1)â†’1, (1,0)â†’1, (1,1)â†’0")
    console.print("  â€¢ Challenge: [bold red]NOT linearly separable![/bold red]")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 2: THE SETUP ğŸ—ï¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("[bold]ğŸ—ï¸ The Architecture:[/bold]")
    console.print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Input â”‚    â”‚  Hidden   â”‚    â”‚ ReLU â”‚    â”‚ Output  â”‚    â”‚Sigmoid â”‚
    â”‚  (2)  â”‚â”€â”€â”€â–¶â”‚    (4)    â”‚â”€â”€â”€â–¶â”‚  Act â”‚â”€â”€â”€â–¶â”‚   (1)   â”‚â”€â”€â”€â–¶â”‚  Å·     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†‘ THE KEY!
             Learns non-linear features
    """)

    console.print("[bold]ğŸ”§ Components:[/bold]")
    console.print("  â€¢ Hidden layer: Transforms data into new space")
    console.print("  â€¢ [bold green]ReLU activation: Adds non-linearity (the secret!)[/bold green]")
    console.print("  â€¢ Output layer: Makes final decision")
    console.print("  â€¢ Total parameters: ~17 (vs 3 for single-layer)")

    console.print("\n[bold]âš™ï¸ Hyperparameters:[/bold]")
    console.print("  â€¢ Hidden size: 4")
    console.print("  â€¢ Learning rate: 0.5 (aggressive!)")
    console.print("  â€¢ Epochs: 500")
    console.print("  â€¢ Optimizer: SGD with backprop through hidden layer")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 3: THE EXPERIMENT ğŸ”¬
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Set seed before model creation to guarantee reproducible 100% convergence
    np.random.seed(1986)  # The year backprop was published!

    model = XORNetwork(hidden_size=4)
    initial_preds = model(X)
    initial_acc = ((initial_preds.data > 0.5).astype(int) == y.data).mean()

    console.print("[bold]ğŸ“Œ Before Training:[/bold]")
    console.print(f"  Initial accuracy: {initial_acc:.1%} (random guessing)")
    console.print("  XOR is impossible for single-layer networks!")
    console.print("  Let's see if hidden layers change the game...")

    console.print("\n[bold]ğŸ”¥ Training in Progress...[/bold]")
    console.print("[dim](This will work - hidden layers solve XOR!)[/dim]\n")

    history = train_network(model, X, y, epochs=500, lr=0.5)

    console.print("\n[green]âœ… Training Complete - XOR Solved![/green]")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 4: THE DIAGNOSIS ğŸ“Š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    evaluate_and_celebrate(model, X, y, history)

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 5: THE REFLECTION ğŸŒŸ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    final_acc = history["accuracy"][-1]

    console.print("")
    console.print(Panel.fit(
        "[bold green]ğŸ‰ Success! You Ended the AI Winter![/bold green]\n\n"

        f"Final accuracy: [bold]{final_acc:.1%}[/bold] (Perfect XOR solution!)\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸ’¡ What YOU Just Accomplished:[/bold]\n"
        "  âœ“ Solved the problem that killed AI for 17 years!\n"
        "  âœ“ Built multi-layer network with YOUR components\n"
        "  âœ“ Hidden layer learns non-linear features\n"
        "  âœ“ Backprop through multiple layers works perfectly!\n"
        "  âœ“ Proved that deep networks CAN work!\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸ“ Why This Matters:[/bold]\n"
        "  This ENDED the 17-year AI Winter!\n"
        "  [bold red]1969:[/bold red] XOR crisis â†’ single layers fail\n"
        "  [bold yellow]1970-1986:[/bold yellow] AI Winter - research funding dries up\n"
        "  [bold green]1986:[/bold green] Backprop + hidden layers solve it\n"
        "  [bold cyan]TODAY:[/bold cyan] YOU recreated this breakthrough!\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸ“Œ The Key Insight:[/bold]\n"
        "  Hidden layers are the KEY to modern AI.\n"
        "  They learn new features that make problems solvable.\n"
        "  Every deep network (GPT, AlphaGo, etc.) uses this pattern!\n"
        "  \n"
        "  [green]Breakthrough:[/green] Non-linear activation functions (ReLU)\n"
        "  enable networks to learn non-linear decision boundaries.\n\n"

        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

        "[bold]ğŸš€ What's Next:[/bold]\n"
        "[dim]Milestone 03 applies this to digit images with YOUR DataLoader!\n"
        "Train on handwritten digits and see modern ML in action![/dim]",

        title="ğŸŒŸ 1986 AI Renaissance Complete",
        border_style="green",
        box=box.DOUBLE
    ))


if __name__ == "__main__":
    main()
