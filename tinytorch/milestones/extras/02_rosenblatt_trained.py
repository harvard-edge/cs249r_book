#!/usr/bin/env python3
"""
The Perceptron (1957) - Frank Rosenblatt [WITH TRAINING]
=========================================================

ğŸ“š HISTORICAL CONTEXT:
In 1957, Frank Rosenblatt created the Perceptron - the first machine that could
LEARN from examples. Before this, all pattern recognition was hand-coded!
This breakthrough proved machines could improve themselves through training.

ğŸ¯ MILESTONE 1 PART 2: TRAINED PERCEPTRON (After Modules 01-08)

Now that you've completed training modules, let's see the SAME architecture
actually LEARN! Watch random weights â†’ intelligent predictions through training.

âœ… REQUIRED MODULES (Run after Module 08):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Module 01 (Tensor)        : YOUR data structure
  Module 02 (Activations)   : YOUR sigmoid activation
  Module 03 (Layers)        : YOUR Linear layer
  Module 04 (Losses)        : YOUR loss function (BinaryCrossEntropyLoss)
  Module 06 (Autograd)      : YOUR automatic differentiation
  Module 07 (Optimizers)    : YOUR SGD optimizer
  Module 08 (Training)      : YOUR training loop
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ—ï¸ ARCHITECTURE (Single-Layer Perceptron):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Input       â”‚     â”‚ Linear      â”‚     â”‚ Sigmoid     â”‚     â”‚ Output  â”‚
    â”‚  (xâ‚, xâ‚‚)   â”‚â”€â”€â”€â”€â–¶â”‚ wâ‚xâ‚+wâ‚‚xâ‚‚+bâ”‚â”€â”€â”€â”€â–¶â”‚  Ïƒ(z)       â”‚â”€â”€â”€â”€â–¶â”‚  Å·      â”‚
    â”‚ YOUR M1     â”‚     â”‚ YOUR M3     â”‚     â”‚ YOUR M2     â”‚     â”‚ [0,1]   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†‘                   â†‘
           â”‚                   â”‚ Gradients flow backward!
           â”‚                   â”‚ (YOUR Module 06 Autograd)
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# =============================================================================
# ğŸ“Š YOUR MODULES IN ACTION
# =============================================================================
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ What You Built      â”‚ How It's Used Here             â”‚ Systems Impact              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Module 06: Autograd â”‚ Computes âˆ‚Loss/âˆ‚w for each     â”‚ Enables automatic learning! â”‚
# â”‚                     â”‚ weight automatically           â”‚ No manual derivatives       â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 07: SGD      â”‚ Updates weights: w = w - lr*âˆ‚L â”‚ The learning algorithm!     â”‚
# â”‚                     â”‚ after each batch               â”‚                             â”‚
# â”‚                     â”‚                                â”‚                             â”‚
# â”‚ Module 08: Training â”‚ Orchestrates forward â†’ loss â†’  â”‚ The complete learning       â”‚
# â”‚                     â”‚ backward â†’ update cycle        â”‚ loop you control!           â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# =============================================================================

ğŸ”¥ THE TRANSFORMATION:
  Before Training:  50% accuracy (random guessing) âŒ
  After Training:   95%+ accuracy (intelligent!)   âœ…

  SAME architecture. SAME data. Just add LEARNING.

ğŸ’¡ KEY INSIGHT:
The magic isn't in the architecture (it's just 3 parameters!)
The magic is in YOUR training loop that adjusts those parameters to fit data.
"""

import sys
import os
import numpy as np

# Add project root to path
sys.path.insert(0, os.getcwd())

# Import TinyTorch components YOU BUILT!
from tinytorch import Tensor, Linear, Sigmoid, BinaryCrossEntropyLoss, SGD

# Rich for beautiful output
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.live import Live
from rich.spinner import Spinner
from rich.text import Text
from rich import box

console = Console()

# ============================================================================
# ğŸ“ STUDENT CODE: Same Perceptron class from forward_pass.py
# ============================================================================

class Perceptron:
    """
    Simple perceptron: Linear + Sigmoid

    SAME as forward_pass.py - but now we'll TRAIN it!
    """

    def __init__(self, input_size=2, output_size=1):
        self.linear = Linear(input_size, output_size)
        self.activation = Sigmoid()

    def forward(self, x):
        x = self.linear(x)
        x = self.activation(x)
        return x

    def __call__(self, x):
        return self.forward(x)

    def parameters(self):
        """Return all trainable parameters."""
        return self.linear.parameters()


# ============================================================================
# ğŸ“Š DATA GENERATION: Linearly separable 2D data
# ============================================================================

def generate_data(n_samples=100, seed=None):
    """Generate linearly separable data."""
    if seed is not None:
        np.random.seed(seed)

    # Class 1: Top-right cluster (high x1, high x2)
    class1 = np.random.randn(n_samples // 2, 2) * 0.5 + np.array([3, 3])
    labels1 = np.ones((n_samples // 2, 1))

    # Class 0: Bottom-left cluster (low x1, low x2)
    class0 = np.random.randn(n_samples // 2, 2) * 0.5 + np.array([1, 1])
    labels0 = np.zeros((n_samples // 2, 1))

    # Combine
    X = np.vstack([class1, class0])
    y = np.vstack([labels1, labels0])

    # Shuffle
    indices = np.random.permutation(n_samples)
    X = X[indices]
    y = y[indices]

    return Tensor(X), Tensor(y)


# ============================================================================
# ğŸ¯ TRAINING FUNCTION
# ============================================================================

def train_perceptron(model, X, y, epochs=100, lr=0.1):
    """Train the perceptron using SGD."""

    # Setup training components
    loss_fn = BinaryCrossEntropyLoss()
    optimizer = SGD(model.parameters(), lr=lr)

    console.print("\n[bold cyan]ğŸ”¥ Starting Training...[/bold cyan]\n")

    history = {"loss": [], "accuracy": []}

    # Use Live display with spinner for real-time feedback
    with Live(console=console, refresh_per_second=10) as live:
        for epoch in range(epochs):
            # Forward pass
            predictions = model(X)
            loss = loss_fn(predictions, y)

            # Backward pass
            loss.backward()

            # Update weights
            optimizer.step()
            optimizer.zero_grad()

            # Calculate accuracy
            pred_classes = (predictions.data > 0.5).astype(int)
            accuracy = (pred_classes == y.data).mean()

            history["loss"].append(loss.data.item())
            history["accuracy"].append(accuracy)

            # Update spinner with current progress
            spinner_text = Text()
            spinner_text.append("â ‹ ", style="cyan")
            spinner_text.append(f"Epoch {epoch+1:3d}/{epochs}  Loss: {loss.data:.4f}  Accuracy: {accuracy:.1%}")
            live.update(spinner_text)

            # Print progress every 10 epochs
            if (epoch + 1) % 10 == 0:
                live.console.print(f"Epoch {epoch+1:3d}/{epochs}  Loss: {loss.data:.4f}  Accuracy: {accuracy:.1%}")

    console.print("\n[bold green]âœ… Training Complete![/bold green]\n")

    return history


# ============================================================================
# ğŸ“ˆ EVALUATION & VISUALIZATION
# ============================================================================

def evaluate_model(model, X, y):
    """Evaluate trained model."""
    predictions = model(X)
    pred_classes = (predictions.data > 0.5).astype(int)
    accuracy = (pred_classes == y.data).mean()

    # Get model weights
    weights = model.linear.weight.data
    bias = model.linear.bias.data

    return accuracy, weights, bias, predictions


def main():
    """Main training pipeline."""

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 1: THE CHALLENGE ğŸ¯
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print(Panel.fit(
        "[bold cyan]ğŸ¯ 1957 - The First Neural Network[/bold cyan]\n\n"
        "[dim]Can a machine learn from examples to classify data?[/dim]\n"
        "[dim]Frank Rosenblatt's perceptron attempts to answer this![/dim]",
        title="ğŸ”¥ 1957 Perceptron Revolution",
        border_style="cyan",
        box=box.DOUBLE
    ))

    console.print("\n[bold]ğŸ“Š The Data:[/bold]")
    X, y = generate_data(n_samples=100, seed=42)
    console.print("  â€¢ Dataset: Linearly separable 2D points")
    console.print(f"  â€¢ Samples: {len(X.data)} (50 per class)")
    console.print("  â€¢ Challenge: Learn decision boundary from examples")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 2: THE SETUP ğŸ—ï¸
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("[bold]ğŸ—ï¸ The Architecture:[/bold]")
    console.print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Input     â”‚    â”‚   Weights    â”‚    â”‚  Output  â”‚
    â”‚   (xâ‚, xâ‚‚)  â”‚â”€â”€â”€â–¶â”‚ wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚â”‚â”€â”€â”€â–¶â”‚    Å·     â”‚
    â”‚  2 features â”‚    â”‚   + bias     â”‚    â”‚ binary   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    console.print("[bold]ğŸ”§ Components:[/bold]")
    console.print("  â€¢ Single layer: Maps 2D input â†’ 1D output")
    console.print("  â€¢ Linear transformation: Weighted sum")
    console.print("  â€¢ Total parameters: 3 (2 weights + 1 bias)")

    console.print("\n[bold]âš™ï¸ Hyperparameters:[/bold]")
    console.print("  â€¢ Learning rate: 0.1")
    console.print("  â€¢ Epochs: 100")
    console.print("  â€¢ Optimizer: Gradient descent")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 3: THE EXPERIMENT ğŸ”¬
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    model = Perceptron(input_size=2, output_size=1)
    acc_before, w_before, b_before, _ = evaluate_model(model, X, y)

    console.print("[bold]ğŸ“Œ Before Training:[/bold]")
    console.print(f"  Initial accuracy: {acc_before:.1%} (random guessing)")
    console.print(f"  Weights: wâ‚={w_before.flatten()[0]:.3f}, wâ‚‚={w_before.flatten()[1]:.3f}, b={b_before.flatten()[0]:.3f}")
    console.print("  Model has random weights - no knowledge yet!")

    console.print("\n[bold]ğŸ”¥ Training in Progress...[/bold]")
    console.print("[dim](Watch gradient descent optimize the weights!)[/dim]\n")

    history = train_perceptron(model, X, y, epochs=100, lr=0.1)

    console.print("\n[green]âœ… Training Complete![/green]")

    acc_after, w_after, b_after, predictions = evaluate_model(model, X, y)

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 4: THE DIAGNOSIS ğŸ“Š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("[bold]ğŸ“Š The Results:[/bold]\n")

    table = Table(title="Training Outcome", box=box.ROUNDED)
    table.add_column("Metric", style="cyan", width=18)
    table.add_column("Before Training", style="yellow", width=16)
    table.add_column("After Training", style="green", width=16)
    table.add_column("Improvement", style="magenta", width=14)

    table.add_row(
        "Accuracy",
        f"{acc_before:.1%}",
        f"{acc_after:.1%}",
        f"+{(acc_after - acc_before):.1%}" if acc_after > acc_before else "â€”"
    )

    table.add_row(
        "Weight wâ‚",
        f"{w_before.flatten()[0]:.4f}",
        f"{w_after.flatten()[0]:.4f}",
        "â†’ Learned" if abs(w_after.flatten()[0] - w_before.flatten()[0]) > 0.1 else "â€”"
    )

    table.add_row(
        "Weight wâ‚‚",
        f"{w_before.flatten()[1]:.4f}",
        f"{w_after.flatten()[1]:.4f}",
        "â†’ Learned" if abs(w_after.flatten()[1] - w_before.flatten()[1]) > 0.1 else "â€”"
    )

    table.add_row(
        "Bias b",
        f"{b_before.flatten()[0]:.4f}",
        f"{b_after.flatten()[0]:.4f}",
        "â†’ Learned" if abs(b_after.flatten()[0] - b_before.flatten()[0]) > 0.1 else "â€”"
    )

    console.print(table)

    console.print("\n[bold]ğŸ” Sample Predictions:[/bold]")
    console.print("[dim](First 10 samples - seeing is believing!)[/dim]\n")

    n_samples = min(10, len(y.data))
    for i in range(n_samples):
        true_val = int(y.data.flatten()[i])
        pred_val = int(predictions.data.flatten()[i])
        status = "âœ“" if pred_val == true_val else "âœ—"
        color = "green" if pred_val == true_val else "red"
        console.print(f"  {status} True: {true_val}, Predicted: {pred_val}", style=color)

    console.print("\n[bold]ğŸ’¡ Key Insights:[/bold]")
    console.print("  â€¢ The model LEARNED from data (not programmed!)")
    console.print(f"  â€¢ Weights changed: random â†’ optimized values")
    console.print("  â€¢ Simple gradient descent found the solution")

    console.print("\n" + "â”€" * 70 + "\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ACT 5: THE REFLECTION ğŸŒŸ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    console.print("")
    if acc_after >= 0.9:
        console.print(Panel.fit(
            "[bold green]ğŸ‰ Success! Your Perceptron Learned to Classify![/bold green]\n\n"

            f"Final accuracy: [bold]{acc_after:.1%}[/bold]\n\n"

            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

            "[bold]ğŸ’¡ What YOU Just Accomplished:[/bold]\n"
            "  âœ“ Built the FIRST neural network (1957 Rosenblatt)\n"
            "  âœ“ Implemented forward pass with YOUR Tensor\n"
            "  âœ“ Used gradient descent to optimize weights\n"
            "  âœ“ Watched machine learning happen in real-time!\n\n"

            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

            "[bold]ğŸ“ Why This Matters:[/bold]\n"
            "  This is the FOUNDATION of all neural networks.\n"
            "  Every model from GPT-4 to AlphaGo uses this same core idea:\n"
            "  Adjust weights via gradients to minimize error.\n\n"

            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

            "[bold]ğŸ“Œ The Key Insight:[/bold]\n"
            "  The architecture is simple (~10 lines of code).\n"
            "  The MAGIC is the training loop:\n"
            "    Forward â†’ Loss â†’ Backward â†’ Update\n"
            "  \n"
            "  [yellow]Limitation:[/yellow] Single layers can only solve\n"
            "  linearly separable problems.\n\n"

            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

            "[bold]ğŸš€ What's Next:[/bold]\n"
            "[dim]Milestone 02 shows what happens when data ISN'T\n"
            "linearly separable... the 17-year AI Winter begins![/dim]",

            title="ğŸŒŸ 1957 Perceptron Complete",
            border_style="green",
            box=box.DOUBLE
        ))
    else:
        console.print(Panel.fit(
            "[bold yellow]âš ï¸  Training in Progress[/bold yellow]\n\n"
            f"Current accuracy: {acc_after:.1%}\n\n"
            "Your perceptron is learning but needs more training.\n"
            "Try: More epochs (500+) or different learning rate (0.5).\n\n"
            "[dim]The gradient descent algorithm is working - just needs more steps![/dim]",
            title="ğŸ”„ Learning in Progress",
            border_style="yellow",
            box=box.DOUBLE
        ))


if __name__ == "__main__":
    main()
