#!/usr/bin/env python3
"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    üó£Ô∏è TINYTALKS: Your First Language Model                   ‚ïë
‚ïë              Watch YOUR Transformer Complete Simple Phrases                  ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üìö HISTORICAL CONTEXT:
After "Attention Is All You Need" (2017), researchers showed transformers could
learn language patterns without explicit rules - just predict the next token!
This is the foundation of GPT, ChatGPT, and all modern language models.

üéØ THE TASK: Next Character Prediction
    Given: "helo"  ‚Üí Predict: "l" (to form "hello")
    Given: "hello" ‚Üí Predict: " " (to form "hello ")

This is exactly how GPT works - predict the next token!

‚úÖ REQUIRED MODULES:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
  Module 01 (Tensor)          : YOUR data structure with autograd
  Module 02 (Activations)     : YOUR ReLU activation
  Module 03 (Layers)          : YOUR Linear layers
  Module 07 (Optimizers)      : YOUR Adam optimizer
  Module 11 (Embeddings)      : YOUR token + positional embeddings
  Module 12 (Attention)       : YOUR multi-head self-attention
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üèóÔ∏è ARCHITECTURE (Character-Level LM):
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Input: "ca"     ‚îÇ ‚Üê Characters you type
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Embedding       ‚îÇ ‚Üê Module 11: chars ‚Üí vectors
    ‚îÇ (vocab_size, d) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Positional      ‚îÇ ‚Üê Module 11: add position info
    ‚îÇ Encoding        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Self-Attention  ‚îÇ ‚Üê Module 12: which chars matter?
    ‚îÇ + LayerNorm     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Output Linear   ‚îÇ ‚Üê Module 03: vectors ‚Üí char probs
    ‚îÇ (d, vocab_size) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Predict: "t"    ‚îÇ ‚Üê Complete to "cat"!
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

# =============================================================================
# üìä YOUR MODULES IN ACTION
# =============================================================================
#
# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# ‚îÇ What You Built      ‚îÇ How It's Used Here             ‚îÇ Systems Impact              ‚îÇ
# ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
# ‚îÇ Module 11: Embed    ‚îÇ Converts characters to dense   ‚îÇ Learned representations     ‚îÇ
# ‚îÇ                     ‚îÇ vector representations         ‚îÇ capture char relationships  ‚îÇ
# ‚îÇ                     ‚îÇ                                ‚îÇ                             ‚îÇ
# ‚îÇ Module 11: PosEnc   ‚îÇ Adds position information      ‚îÇ Model knows char order      ‚îÇ
# ‚îÇ                     ‚îÇ to embeddings                  ‚îÇ without recurrence          ‚îÇ
# ‚îÇ                     ‚îÇ                                ‚îÇ                             ‚îÇ
# ‚îÇ Module 12: Attn     ‚îÇ Each position attends to       ‚îÇ Captures dependencies       ‚îÇ
# ‚îÇ                     ‚îÇ previous positions (causal)    ‚îÇ across the sequence         ‚îÇ
# ‚îÇ                     ‚îÇ                                ‚îÇ                             ‚îÇ
# ‚îÇ Module 03: Linear   ‚îÇ Projects to vocabulary for     ‚îÇ Converts understanding      ‚îÇ
# ‚îÇ                     ‚îÇ next-char prediction           ‚îÇ to predictions              ‚îÇ
# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
#
# =============================================================================

üìä EXPECTED PERFORMANCE:
- Training: ~300 epochs in 1-2 minutes
- Words learned: cat, dog, red, blue, sun, moon, star
- Generation: Character-by-character completion

üí° KEY INSIGHT:
"All of language modeling is just predicting the next token" - This simple
idea powers ChatGPT, and YOUR implementation makes it work!
"""

import sys
import os
import time
import numpy as np
from pathlib import Path

sys.path.insert(0, os.getcwd())

from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich import box

console = Console()


def main():
    # ========================================================================
    # WELCOME
    # ========================================================================

    console.print(Panel(
        "[bold magenta]‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] [bold]üó£Ô∏è TINYTALKS                  [/bold][bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] [bold]   Phrase Completion Demo     [/bold][bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta]                               [bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] YOUR Transformer predicts    [bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïë[/bold magenta] the next character!          [bold magenta]‚ïë[/bold magenta]\n"
        "[bold magenta]‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù[/bold magenta]",
        border_style="bright_magenta"
    ))

    # ========================================================================
    # IMPORT YOUR IMPLEMENTATIONS
    # ========================================================================

    console.print("\n[bold cyan]üì¶ Loading YOUR Tinyüî•Torch...[/bold cyan]\n")

    try:
        from tinytorch import Tensor, Linear, ReLU, CrossEntropyLoss
        from tinytorch import LayerNorm, create_causal_mask
        from tinytorch.core.optimizers import Adam
        from tinytorch.core.embeddings import Embedding, PositionalEncoding
        from tinytorch.core.attention import MultiHeadAttention

        console.print("  [green]‚úì[/green] All YOUR implementations loaded!")

    except ImportError as e:
        console.print(f"[red]Import Error: {e}[/red]")
        return 1

    # ========================================================================
    # TRAINING DATA
    # ========================================================================

    console.print(Panel(
        "[bold cyan]üìö Training Data: Simple Words[/bold cyan]\n\n"
        "Teaching the model to complete:\n"
        "  [cyan]'ca'[/cyan]  ‚Üí [green]'cat'[/green]\n"
        "  [cyan]'do'[/cyan]  ‚Üí [green]'dog'[/green]\n"
        "  [cyan]'su'[/cyan]  ‚Üí [green]'sun'[/green]\n"
        "  [cyan]'sta'[/cyan] ‚Üí [green]'star'[/green]",
        border_style="cyan"
    ))

    # Training words - distinct patterns to avoid confusion
    words = ["cat", "dog", "red", "blue", "sun", "moon", "star"]

    # Build vocabulary
    all_chars = set()
    for word in words:
        all_chars.update(word)
    all_chars.add('_')  # Padding

    chars = sorted(list(all_chars))
    char_to_idx = {c: i for i, c in enumerate(chars)}
    idx_to_char = {i: c for c, i in char_to_idx.items()}
    vocab_size = len(chars)
    pad_idx = char_to_idx['_']

    console.print(f"  [green]‚úì[/green] Vocabulary: {vocab_size} characters\n")

    # ========================================================================
    # BUILD MODEL
    # ========================================================================

    console.print(Panel(
        "[bold cyan]üèóÔ∏è Building Model[/bold cyan]\n\n"
        "Using YOUR implementations:\n"
        "  ‚Ä¢ Embedding (Module 11)\n"
        "  ‚Ä¢ MultiHeadAttention (Module 12)\n"
        "  ‚Ä¢ Linear, LayerNorm (Modules 03, 13)",
        border_style="cyan"
    ))

    # Small but capable config
    embed_dim = 32
    num_heads = 2
    max_len = 12

    # Build components
    embedding = Embedding(vocab_size, embed_dim)
    pos_encoding = PositionalEncoding(max_len, embed_dim)
    attention = MultiHeadAttention(embed_dim, num_heads)
    ln = LayerNorm(embed_dim)
    output_proj = Linear(embed_dim, vocab_size)

    all_params = (embedding.parameters() + attention.parameters() +
                  ln.parameters() + output_proj.parameters())

    param_count = sum(p.data.size for p in all_params)
    console.print(f"  [green]‚úì[/green] Model: {param_count:,} parameters\n")

    # Using create_causal_mask from tinytorch.core.transformer (Module 13)

    def forward(tokens):
        """Forward pass with causal masking for autoregressive generation."""
        batch, seq_len = tokens.shape[0], tokens.data.shape[1]

        x = embedding(tokens)
        x = pos_encoding(x)

        # Create causal mask - each position can only see past + current
        mask = create_causal_mask(seq_len)
        attn_out = attention(x, mask)

        x = ln(x + attn_out)  # Residual connection

        # Reshape for output projection
        batch, seq, embed = x.shape
        x_2d = x.reshape(batch * seq, embed)
        logits_2d = output_proj(x_2d)
        logits = logits_2d.reshape(batch, seq, vocab_size)
        return logits

    # ========================================================================
    # PREPARE TRAINING DATA
    # ========================================================================

    def encode(text):
        """Convert text to indices."""
        return [char_to_idx.get(c, pad_idx) for c in text]

    def pad(seq, length):
        """Pad sequence to length."""
        return seq + [pad_idx] * (length - len(seq))

    # Create training examples: for each word, train to predict next char
    # Input: "hel__" Target at each position: "ello_"
    train_inputs = []
    train_targets = []

    for word in words:
        # Pad word
        word_padded = word + '_' * (max_len - len(word))

        # Input is word, target is shifted by 1
        inp = encode(word_padded[:max_len])
        tgt = encode(word_padded[1:max_len] + '_')

        train_inputs.append(inp)
        train_targets.append(tgt)

    X = Tensor(np.array(train_inputs))
    y = Tensor(np.array(train_targets))

    console.print(f"  [dim]Training examples: {len(words)} words[/dim]\n")

    # ========================================================================
    # TRAINING
    # ========================================================================

    console.print(Panel(
        "[bold yellow]üèãÔ∏è Training: Next Character Prediction[/bold yellow]\n\n"
        "For 'star': s‚Üít, t‚Üía, a‚Üír, r‚Üí_",
        border_style="yellow"
    ))

    optimizer = Adam(all_params, lr=0.03)
    loss_fn = CrossEntropyLoss()

    num_epochs = 300  # More training for better completion

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("{task.completed}/{task.total}"),
        transient=True
    ) as progress:
        task = progress.add_task("Training...", total=num_epochs)

        for epoch in range(num_epochs):
            total_loss = 0

            for i in range(len(words)):
                # Get batch
                inp = Tensor(X.data[i:i+1])
                tgt = Tensor(y.data[i:i+1])

                # Forward
                logits = forward(inp)

                # Reshape for loss (batch*seq, vocab)
                batch, seq, vocab = logits.shape
                logits_2d = logits.reshape(batch * seq, vocab)
                target_1d = tgt.reshape(-1)

                # Compute loss over all positions
                loss = loss_fn(logits_2d, target_1d)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += float(loss.data)

            progress.advance(task)

    console.print(f"  [green]‚úì[/green] Training complete! (Loss: {total_loss/len(words):.4f})\n")

    # ========================================================================
    # GENERATION DEMO
    # ========================================================================

    console.print(Panel(
        "[bold green]üéâ PHRASE COMPLETION DEMO[/bold green]\n\n"
        "Watch YOUR transformer complete words!",
        border_style="green"
    ))

    def complete(prefix, max_chars=10):
        """Complete a word character by character."""
        text = prefix

        console.print(f"\n  [bold cyan]Start:[/bold cyan] [yellow]{prefix}[/yellow]", end="")

        for _ in range(max_chars):
            # Encode and pad
            inp = pad(encode(text), max_len)
            tokens = Tensor(np.array([inp]))

            # Forward
            logits = forward(tokens)

            # Get prediction for next position
            pos = len(text) - 1
            if pos >= max_len - 1:
                break

            next_logits = logits.data[0, pos, :]

            # Softmax + sample
            probs = np.exp(next_logits - np.max(next_logits))
            probs = probs / probs.sum()
            next_idx = np.argmax(probs)
            next_char = idx_to_char[next_idx]

            if next_char == '_':
                break

            console.print(f"[green]{next_char}[/green]", end="")
            text += next_char
            time.sleep(0.1)

        console.print()
        return text

    # Test completions
    test_prefixes = ["ca", "do", "re", "blu", "su", "sta"]

    for prefix in test_prefixes:
        complete(prefix)
        time.sleep(0.2)

    # ========================================================================
    # SUCCESS
    # ========================================================================

    console.print(Panel(
        "[bold green]üèÜ TINYTALKS COMPLETE![/bold green]\n\n"
        "[green]YOUR transformer completed words![/green]\n\n"
        "[bold]How it works:[/bold]\n"
        "  1. [cyan]Embedding[/cyan]: Characters ‚Üí Vectors\n"
        "  2. [cyan]Attention[/cyan]: Look at previous chars\n"
        "  3. [cyan]Predict[/cyan]: What comes next?\n"
        "  4. [cyan]Repeat[/cyan]: Generate char by char\n\n"
        "[dim]This is exactly how GPT works![/dim]\n\n"
        "[bold]üéì You've built a language model![/bold]",
        title="üó£Ô∏è TinyTalks",
        border_style="bright_green",
        box=box.DOUBLE,
        padding=(1, 2)
    ))

    return 0


if __name__ == "__main__":
    sys.exit(main())
