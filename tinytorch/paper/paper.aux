\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sutton2019bitter}
\citation{vaswani2017attention}
\citation{roberthalf2024talent,keller2025ai}
\citation{keller2025ai}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}{}{}{}}
\citation{meadows2008thinking}
\citation{reddi2024mlsysbook}
\citation{aho2006compilers}
\citation{tanenbaum1987minix}
\citation{aho2006compilers,appel2004tiger}
\citation{appel2004tiger}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{lst:pytorch-usage}{{1a}{3}{PyTorch: Black box usage}{figure.caption.1}{}}
\newlabel{lst:pytorch-usage@cref}{{[subfigure][1][1]1a}{[1][2][]3}{}{}{}}
\newlabel{sub@lst:pytorch-usage}{{a}{3}{PyTorch: Black box usage}{figure.caption.1}{}}
\newlabel{sub@lst:pytorch-usage@cref}{{[subfigure][1][1]1a}{[1][2][]3}{}{}{}}
\newlabel{lst:tinytorch-build}{{1b}{3}{TinyTorch: Build internals}{figure.caption.1}{}}
\newlabel{lst:tinytorch-build@cref}{{[subfigure][2][1]1b}{[1][2][]3}{}{}{}}
\newlabel{sub@lst:tinytorch-build}{{b}{3}{TinyTorch: Build internals}{figure.caption.1}{}}
\newlabel{sub@lst:tinytorch-build@cref}{{[subfigure][2][1]1b}{[1][2][]3}{}{}{}}
\newlabel{lst:tinytorch-culmination}{{1c}{3}{TinyTorch: The culmination}{figure.caption.1}{}}
\newlabel{lst:tinytorch-culmination@cref}{{[subfigure][3][1]1c}{[1][2][]3}{}{}{}}
\newlabel{sub@lst:tinytorch-culmination}{{c}{3}{TinyTorch: The culmination}{figure.caption.1}{}}
\newlabel{sub@lst:tinytorch-culmination@cref}{{[subfigure][3][1]1c}{[1][2][]3}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {From User to Engineer.} (a) PyTorch's high-level APIs hide framework internals. (b) TinyTorch students implement components like Adam, learning memory costs and update rules firsthand. (c) By Module 13, every import is student-built code---transformers train on infrastructure they fully understand.}}{3}{figure.caption.1}\protected@file@percent }
\newlabel{fig:code-comparison}{{1}{3}{\textbf {From User to Engineer.} (a) PyTorch's high-level APIs hide framework internals. (b) TinyTorch students implement components like Adam, learning memory costs and update rules firsthand. (c) By Module 13, every import is student-built code---transformers train on infrastructure they fully understand}{figure.caption.1}{}}
\newlabel{fig:code-comparison@cref}{{[figure][1][]1}{[1][2][]3}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{3}{Related Work}{section.2}{}}
\newlabel{sec:related@cref}{{[section][2][]2}{[1][3][]3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Pedagogical Precedents in Systems Edu}{3}{subsection.2.1}\protected@file@percent }
\citation{christopher1993nachos}
\citation{pfaff2004pintos}
\citation{kaashoek2023xv6}
\citation{abelson1996sicp}
\citation{karpathy2022micrograd}
\citation{schneider2020minitorch}
\citation{hotz2023tinygrad}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Module Flow.} Foundation (blue): core tensor operations through training loop. Architecture (purple): branches into vision (CNNs) and language (Tokenization$\rightarrow $Transformers) paths. Optimization (orange): profiling feeds parallel techniques---model-level (quantization, compression) and runtime (acceleration, memoization)---all converging at benchmarking before the capstone.}}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:module-flow}{{2}{4}{\textbf {Module Flow.} Foundation (blue): core tensor operations through training loop. Architecture (purple): branches into vision (CNNs) and language (Tokenization$\rightarrow $Transformers) paths. Optimization (orange): profiling feeds parallel techniques---model-level (quantization, compression) and runtime (acceleration, memoization)---all converging at benchmarking before the capstone}{figure.caption.2}{}}
\newlabel{fig:module-flow@cref}{{[figure][2][]2}{[1][3][]4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Educational ML Frameworks}{4}{subsection.2.2}\protected@file@percent }
\citation{johnson2016cs231n}
\citation{chen2022dlsyscourse}
\citation{banbury2021widening}
\citation{zhang2021dive}
\citation{howard2020fastai}
\citation{papert1980mindstorms}
\citation{collins1989cognitive}
\citation{kapur2008productive}
\citation{meyer2003threshold}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Positioning and Unique Contributions}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}TinyTorch Architecture}{5}{section.3}\protected@file@percent }
\newlabel{sec:curriculum}{{3}{5}{TinyTorch Architecture}{section.3}{}}
\newlabel{sec:curriculum@cref}{{[section][3][]3}{[1][5][]5}{}{}{}}
\citation{rosenblatt1958perceptron}
\citation{rumelhart1986learning}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Framework comparison positions TinyTorch's pedagogical role. Educational frameworks (micrograd, MiniTorch, tinygrad) prioritize learning over production use. Production frameworks (PyTorch, TensorFlow) prioritize performance and scalability. TinyTorch bridges both: students learn framework internals through implementation, then transfer that knowledge to production frameworks with deeper systems understanding.}}{6}{table.caption.3}\protected@file@percent }
\newlabel{tab:framework-comparison}{{1}{6}{Framework comparison positions TinyTorch's pedagogical role. Educational frameworks (micrograd, MiniTorch, tinygrad) prioritize learning over production use. Production frameworks (PyTorch, TensorFlow) prioritize performance and scalability. TinyTorch bridges both: students learn framework internals through implementation, then transfer that knowledge to production frameworks with deeper systems understanding}{table.caption.3}{}}
\newlabel{tab:framework-comparison@cref}{{[table][1][]1}{[1][5][]6}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The ML Systems Competency Matrix}{6}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:competency-matrix}{{3.1}{6}{The ML Systems Competency Matrix}{subsection.3.1}{}}
\newlabel{subsec:competency-matrix@cref}{{[subsection][1][3]3.1}{[1][6][]6}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The 3-Tier Learning Journey + Capstone}{6}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Competency Matrix.} The 40 cells (8 knowledge areas $\times $ 5 capability levels) characterize ML systems fundamentals. Rows progress from foundational concepts (tensors, operations) through learning mechanics (graphs, optimization) to production concerns (efficiency, deployment). Columns progress from conceptual understanding (Knows) through quantitative reasoning (Measures) and implementation (Implements) to diagnostic skill (Debugs) and performance engineering (Optimizes). This matrix guided TinyTorch's curriculum design. \Cref  {subsec:coverage} maps the 20 modules to this framework.}}{7}{table.caption.4}\protected@file@percent }
\newlabel{tab:competency-matrix}{{2}{7}{\textbf {Competency Matrix.} The 40 cells (8 knowledge areas $\times $ 5 capability levels) characterize ML systems fundamentals. Rows progress from foundational concepts (tensors, operations) through learning mechanics (graphs, optimization) to production concerns (efficiency, deployment). Columns progress from conceptual understanding (Knows) through quantitative reasoning (Measures) and implementation (Implements) to diagnostic skill (Debugs) and performance engineering (Optimizes). This matrix guided TinyTorch's curriculum design. \Cref {subsec:coverage} maps the 20 modules to this framework}{table.caption.4}{}}
\newlabel{tab:competency-matrix@cref}{{[table][2][]2}{[1][6][]7}{}{}{}}
\newlabel{lst:tensor-memory}{{1}{7}{\textbf {Memory Profiling.} Tensor implementation from Module 01 with explicit memory tracking}{lstlisting.1}{}}
\newlabel{lst:tensor-memory@cref}{{[listing][1][]1}{[1][6][]7}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}{\ignorespaces \textbf  {Memory Profiling.} Tensor implementation from Module 01 with explicit memory tracking.}}{7}{lstlisting.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Module Progression.} Each module teaches both ``what'' (ML technique) and ``how much'' (memory/compute costs). Foundation tier (M01--08) establishes core operations with explicit resource tracking. Architecture tier (M09--13) applies these foundations to CNNs and transformers. Optimization tier (M14--19) adds production concerns: profiling, quantization, deployment. This dual-concept approach ensures students never learn algorithms without understanding their systems implications.}}{8}{table.caption.5}\protected@file@percent }
\newlabel{tab:objectives}{{3}{8}{\textbf {Module Progression.} Each module teaches both ``what'' (ML technique) and ``how much'' (memory/compute costs). Foundation tier (M01--08) establishes core operations with explicit resource tracking. Architecture tier (M09--13) applies these foundations to CNNs and transformers. Optimization tier (M14--19) adds production concerns: profiling, quantization, deployment. This dual-concept approach ensures students never learn algorithms without understanding their systems implications}{table.caption.5}{}}
\newlabel{tab:objectives@cref}{{[table][3][]3}{[1][6][]8}{}{}{}}
\citation{krizhevsky2009cifar,lecun1998gradient}
\citation{vaswani2017attention}
\citation{mattson2020mlperf,reddi2020mlperf}
\citation{meadows2008thinking}
\citation{perkins1992transfer}
\citation{bruner1960process}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Module Structure}{10}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:module-pedagogy}{{3.3}{10}{Module Structure}{subsection.3.3}{}}
\newlabel{subsec:module-pedagogy@cref}{{[subsection][3][3]3.3}{[1][9][]10}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Milestone Arcs}{10}{subsection.3.4}\protected@file@percent }
\newlabel{subsec:milestones}{{3.4}{10}{Milestone Arcs}{subsection.3.4}{}}
\newlabel{subsec:milestones@cref}{{[subsection][4][3]3.4}{[1][10][]10}{}{}{}}
\citation{krizhevsky2009cifar,lecun1998gradient}
\citation{mattson2020mlperf,reddi2020mlperf}
\citation{sweller1988cognitive}
\citation{meyer2003threshold}
\citation{pytorch04release}
\citation{tensorflow20}
\@writefile{toc}{\contentsline {section}{\numberline {4}Progressive Disclosure}{12}{section.4}\protected@file@percent }
\newlabel{sec:progressive}{{4}{12}{Progressive Disclosure}{section.4}{}}
\newlabel{sec:progressive@cref}{{[section][4][]4}{[1][12][]12}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Pattern Implementation}{12}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Pedagogical Justification}{12}{subsection.4.2}\protected@file@percent }
\newlabel{lst:dormant-tensor}{{2}{12}{\textbf {Dormant Gradient Infrastructure.} Module 01 Tensor includes \texttt {.backward()}, \texttt {.grad}, and \texttt {.requires\_grad}---visible but inactive until Module 06 activation}{lstlisting.2}{}}
\newlabel{lst:dormant-tensor@cref}{{[listing][2][]2}{[1][12][]12}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}{\ignorespaces \textbf  {Dormant Gradient Infrastructure.} Module 01 Tensor includes \texttt  {.backward()}, \texttt  {.grad}, and \texttt  {.requires\_grad}---visible but inactive until Module 06 activation.}}{12}{lstlisting.2}\protected@file@percent }
\newlabel{lst:activation}{{3}{12}{\textbf {Autograd Activation.} Module 06 monkey-patches Tensor to enable gradient computation}{lstlisting.3}{}}
\newlabel{lst:activation@cref}{{[listing][3][]3}{[1][12][]12}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}{\ignorespaces \textbf  {Autograd Activation.} Module 06 monkey-patches Tensor to enable gradient computation.}}{12}{lstlisting.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Production Framework Alignment}{12}{subsection.4.3}\protected@file@percent }
\citation{lave1991situated}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Progressive Disclosure.} Runtime feature activation manages cognitive load. From Module 01, students see the complete Tensor API including gradient methods (\texttt  {.backward()}, \texttt  {.grad}, \texttt  {.requires\_grad}), but these features remain dormant (gray, dashed). In Module 06, runtime enhancement activates full autograd functionality (orange, solid) without breaking earlier code. Three learning benefits: (1) students learn the complete API early, avoiding interface surprise later; (2) Module 01 code continues working unchanged when autograd activates (forward compatibility); (3) visible but inactive features create curiosity-driven questions motivating curriculum progression.}}{13}{figure.caption.9}\protected@file@percent }
\newlabel{fig:progressive-timeline}{{3}{13}{\textbf {Progressive Disclosure.} Runtime feature activation manages cognitive load. From Module 01, students see the complete Tensor API including gradient methods (\texttt {.backward()}, \texttt {.grad}, \texttt {.requires\_grad}), but these features remain dormant (gray, dashed). In Module 06, runtime enhancement activates full autograd functionality (orange, solid) without breaking earlier code. Three learning benefits: (1) students learn the complete API early, avoiding interface surprise later; (2) Module 01 code continues working unchanged when autograd activates (forward compatibility); (3) visible but inactive features create curiosity-driven questions motivating curriculum progression}{figure.caption.9}{}}
\newlabel{fig:progressive-timeline@cref}{{[figure][3][]3}{[1][12][]13}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Systems-First Integration}{13}{section.5}\protected@file@percent }
\newlabel{sec:systems}{{5}{13}{Systems-First Integration}{section.5}{}}
\newlabel{sec:systems@cref}{{[section][5][]5}{[1][13][]13}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Phase 1: Understanding and Characterizing Memory Usage}{13}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Phase 2: Analyzing Complexity Through Transparent Implementations}{13}{subsection.5.2}\protected@file@percent }
\citation{kapur2008productive}
\newlabel{lst:conv-explicit}{{4}{14}{\textbf {Explicit Convolution.} Seven nested loops reveal $O(C_{out} \times H \times W \times C_{in} \times K^2)$ complexity}{lstlisting.4}{}}
\newlabel{lst:conv-explicit@cref}{{[listing][4][]4}{[1][13][]14}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}{\ignorespaces \textbf  {Explicit Convolution.} Seven nested loops reveal $O(C_{out} \times H \times W \times C_{in} \times K^2)$ complexity.}}{14}{lstlisting.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {Performance Comparison.} TinyTorch vs PyTorch (CPU). Pure Python implementations with explicit loops are 100--10,000$\times $ slower, making performance costs visible and optimization meaningful. Benchmarks measured on Intel i7 CPU; PyTorch uses optimized BLAS libraries while TinyTorch uses pure Python for pedagogical transparency.}}{14}{table.caption.11}\protected@file@percent }
\newlabel{tab:performance}{{4}{14}{\textbf {Performance Comparison.} TinyTorch vs PyTorch (CPU). Pure Python implementations with explicit loops are 100--10,000$\times $ slower, making performance costs visible and optimization meaningful. Benchmarks measured on Intel i7 CPU; PyTorch uses optimized BLAS libraries while TinyTorch uses pure Python for pedagogical transparency}{table.caption.11}{}}
\newlabel{tab:performance@cref}{{[table][4][]4}{[1][14][]14}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Phase 3: Optimizing Systems Through Measurement-Driven Iteration}{14}{subsection.5.3}\protected@file@percent }
\citation{mlsysbook2025}
\@writefile{toc}{\contentsline {section}{\numberline {6}Course Deployment}{15}{section.6}\protected@file@percent }
\newlabel{sec:deployment}{{6}{15}{Course Deployment}{section.6}{}}
\newlabel{sec:deployment@cref}{{[section][6][]6}{[1][15][]15}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Integration Models}{15}{subsection.6.1}\protected@file@percent }
\newlabel{subsec:integration}{{6.1}{15}{Integration Models}{subsection.6.1}{}}
\newlabel{subsec:integration@cref}{{[subsection][1][6]6.1}{[1][15][]15}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Tier-Based Curriculum Configurations}{15}{subsection.6.2}\protected@file@percent }
\newlabel{subsec:tier-configs}{{6.2}{15}{Tier-Based Curriculum Configurations}{subsection.6.2}{}}
\newlabel{subsec:tier-configs@cref}{{[subsection][2][6]6.2}{[1][15][]15}{}{}{}}
\citation{banbury2021widening}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}ML Systems Competency Coverage}{16}{subsection.6.3}\protected@file@percent }
\newlabel{subsec:coverage}{{6.3}{16}{ML Systems Competency Coverage}{subsection.6.3}{}}
\newlabel{subsec:coverage@cref}{{[subsection][3][6]6.3}{[1][16][]16}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Infrastructure and Accessibility}{16}{subsection.6.4}\protected@file@percent }
\newlabel{subsec:infrastructure}{{6.4}{16}{Infrastructure and Accessibility}{subsection.6.4}{}}
\newlabel{subsec:infrastructure@cref}{{[subsection][4][6]6.4}{[1][16][]16}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Jupyter Environment Options}{16}{subsubsection.6.4.1}\protected@file@percent }
\citation{vygotsky1978mind}
\citation{blank2019nbgrader}
\citation{thompson2008bloom}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {Competency Coverage.} Each cell shows which modules address that competency. \text  {$\mathsurround \z@ \mathchar "458$}~= full coverage through implementation and assessment. $\circ $~= partial coverage through conceptual instruction. --~= intentional gap where production infrastructure exceeds accessibility-first scope. Of 40 cells, 36 receive full coverage, 3 receive partial coverage, and 1 represents an intentional gap.}}{17}{table.caption.12}\protected@file@percent }
\newlabel{tab:coverage}{{5}{17}{\textbf {Competency Coverage.} Each cell shows which modules address that competency. \fullmark ~= full coverage through implementation and assessment. \halfmark ~= partial coverage through conceptual instruction. \emptymark ~= intentional gap where production infrastructure exceeds accessibility-first scope. Of 40 cells, 36 receive full coverage, 3 receive partial coverage, and 1 represents an intentional gap}{table.caption.12}{}}
\newlabel{tab:coverage@cref}{{[table][5][]5}{[1][16][]17}{}{}{}}
\newlabel{lst:nbgrader-example}{{5}{17}{\textbf {NBGrader Structure.} Cell metadata defines point allocation and solution delimiters}{lstlisting.5}{}}
\newlabel{lst:nbgrader-example@cref}{{[listing][5][]5}{[1][17][]17}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5}{\ignorespaces \textbf  {NBGrader Structure.} Cell metadata defines point allocation and solution delimiters.}}{17}{lstlisting.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}NBGrader Autograding Workflow}{17}{subsubsection.6.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Automated Assessment Infrastructure}{17}{subsection.6.5}\protected@file@percent }
\citation{howard2020fastai}
\newlabel{lst:progressive-imports}{{6}{18}{\textbf {Progressive Imports.} Framework capabilities grow module-by-module as students complete implementations}{lstlisting.6}{}}
\newlabel{lst:progressive-imports@cref}{{[listing][6][]6}{[1][18][]18}{}{}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6}{\ignorespaces \textbf  {Progressive Imports.} Framework capabilities grow module-by-module as students complete implementations.}}{18}{lstlisting.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Package Organization}{18}{subsection.6.6}\protected@file@percent }
\newlabel{subsec:package}{{6.6}{18}{Package Organization}{subsection.6.6}{}}
\newlabel{subsec:package@cref}{{[subsection][6][6]6.6}{[1][18][]18}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Open Source Infrastructure}{18}{subsection.6.7}\protected@file@percent }
\newlabel{subsec:opensource}{{6.7}{18}{Open Source Infrastructure}{subsection.6.7}{}}
\newlabel{subsec:opensource@cref}{{[subsection][7][6]6.7}{[1][18][]18}{}{}{}}
\citation{kapur2008productive}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}Teaching Assistant Support}{19}{subsection.6.8}\protected@file@percent }
\newlabel{subsec:ta-support}{{6.8}{19}{Teaching Assistant Support}{subsection.6.8}{}}
\newlabel{subsec:ta-support@cref}{{[subsection][8][6]6.8}{[1][19][]19}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9}Student Learning Support}{19}{subsection.6.9}\protected@file@percent }
\newlabel{subsec:student-support}{{6.9}{19}{Student Learning Support}{subsection.6.9}{}}
\newlabel{subsec:student-support@cref}{{[subsection][9][6]6.9}{[1][19][]19}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and Limitations}{19}{section.7}\protected@file@percent }
\newlabel{sec:discussion}{{7}{19}{Discussion and Limitations}{section.7}{}}
\newlabel{sec:discussion@cref}{{[section][7][]7}{[1][19][]19}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Pedagogical Scope as Design Decision}{19}{subsection.7.1}\protected@file@percent }
\newlabel{subsec:scope}{{7.1}{19}{Pedagogical Scope as Design Decision}{subsection.7.1}{}}
\newlabel{subsec:scope@cref}{{[subsection][1][7]7.1}{[1][19][]19}{}{}{}}
\citation{sweller1988cognitive}
\citation{collins1989cognitive}
\citation{paas1992training}
\citation{sorva2012visual}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Limitations}{20}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Future Directions}{20}{section.8}\protected@file@percent }
\newlabel{sec:future-work}{{8}{20}{Future Directions}{section.8}{}}
\newlabel{sec:future-work@cref}{{[section][8][]8}{[1][20][]20}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Empirical Validation Roadmap}{20}{subsection.8.1}\protected@file@percent }
\citation{chen2022dlsyscourse}
\citation{williams2009roofline}
\citation{strubell2019energy,patterson2021carbon}
\citation{banbury2021benchmarking}
\citation{samajdar2018scale}
\citation{parashar2019timeloop}
\citation{kannan2022astrasim}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Curriculum Evolution}{21}{subsection.8.2}\protected@file@percent }
\citation{mattson2020mlperf,reddi2020mlperf}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Community and Sustainability}{22}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{22}{section.9}\protected@file@percent }
\newlabel{sec:conclusion}{{9}{22}{Conclusion}{section.9}{}}
\newlabel{sec:conclusion@cref}{{[section][9][]9}{[1][22][]22}{}{}{}}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{aho2006compilers}{{1}{2006}{{Aho et~al.}}{{Aho, Lam, Sethi, and Ullman}}}
\bibcite{appel2004tiger}{{2}{}{{Appel and Palsberg}}{{}}}
\bibcite{banbury2021benchmarking}{{3}{}{{Banbury et~al.}}{{Banbury, Reddi, Lam, Fu, Fazel, Holleman, Huang, Hurtado, Kanter, Lokhmotov, Patterson, Pau, Seo, Sieracki, Thakker, Verhelst, and Yadav}}}
\bibcite{perkins1992transfer}{{4}{}{{Burstein et~al.}}{{Burstein, Henry, Collison, Marczak, Sligar, Watson, Marquez, Abbasalizad-Farhangi, Abbasi, Abd-Allah, Abdoli, Abdollahi, Abdollahpour, Abdulkader, Abrigo, Acharya, Adebayo, Adekanmbi, Adham, Afshari, Aghaali, Ahmadi, Ahmadi, Ahmadpour, Ahmed, Akal, Akinyemi, Alahdab, Alam, Alamene, Alene, Alijanzadeh, Alinia, Alipour, Aljunid, Almalki, Al-Mekhlafi, Altirkawi, Alvis-Guzman, Amegah, Amini, Amit, Anbari, Androudi, Anjomshoa, Ansari, Antonio, Arabloo, Arefi, Aremu, Armoon, Arora, Artaman, Asadi, Asadi-Aliabadi, Ashraf-Ganjouei, Assadi, Ataeinia, Atre, Quintanilla, Ayanore, Azari, Babaee, Babazadeh, Badawi, Bagheri, Bagherzadeh, Baheiraei, Balouchi, Barac, Bassat, Baune, Bayati, Bedi, Beghi, Behzadifar, Behzadifar, Belay, Bell, Bell, Berbada, Bernstein, Bhattacharjee, Bhattarai, Bhutta, Bijani, Bohlouli, Breitborde, Britton, Browne, Nagaraja, Busse, Butt, Car, C\'{a}rdenas, Casta\~{n}eda Orjuela, Cerin, Chanie, Chatterjee, Chu, Cooper, Costa, Dalal, Dandona, Dandona, Daoud, Daryani, Das~Gupta, Davis, Davis~Weaver, Davitoiu, De~Neve, Demeke, Demoz, Deribe, Desai, Deshpande, Desyibelew, Dey, Dharmaratne, Dhimal, Diaz, Doshmangir, Duraes, Dwyer-Lindgren, Earl, Ebrahimi, Ebrahimpour, Effiong, Eftekhari, Ehsani-Chimeh, El~Sayed, El~Sayed~Zaki, El~Tantawi, El-Khatib, Emamian, Enany, Eskandarieh, Eyawo, Ezalarab, Faramarzi, Fareed, Faridnia, Faro, Fazaeli, Fazlzadeh, Fentahun, Fereshtehnejad, Fernandes, Filip, Fischer, Foigt, Foroutan, Francis, Fukumoto, Fullman, Gallus, Gebre, Gebrehiwot, Gebremeskel, Gessner, Geta, Gething, Ghadimi, Ghadiri, Ghajarzadeh, Ghashghaee, Gill, Gill, Golding, Gomes, Gona, Gopalani, Gorini, Goulart, Graetz, Greaves, Green, Guo, Haj-Mirzaian, Haj-Mirzaian, Hall, Hamidi, Haririan, Haro, Hasankhani, Hasanpoor, Hasanzadeh, Hassankhani, Hassen, Hegazy, Hendrie, Heydarpour, Hird, Hoang, Hollerich, Rad, Hoseini-Ghahfarokhi, Hossain, Hosseini, Hosseinzadeh, Hostiuc, Hostiuc, Househ, Hsairi, Ilesanmi, Imani-Nasab, Iqbal, Irvani, Islam, Islam, J\"{u}risson, Balalami, Jalali, Javidnia, Jayatilleke, Jenabi, Ji, Jobanputra, Johnson, Jonas, Shushtari, Jozwiak, Kabir, Kahsay, Kalani, Kalhor, Karami, Karki, Kasaeian, Kassebaum, Keiyoro, Kemp, Khabiri, Khader, Khafaie, Khan, Khan, Khan, Khang, Khatab, Khater, Khater, Khatony, Khazaei, Khazaei, Khazaei-Pool, Khubchandani, Kianipour, Kim, Kimokoti, Kinyoki, Kisa, Kisa, Kolola, Kosen, Koul, Koyanagi, Kraemer, Krishan, Krohn, Kugbey, Kumar, Kumar, Kumar, Kuupiel, Lacey, Lad, Lami, Larsson, Lee, Leili, Levine, Li, Lim, Listl, Longbottom, Lopez, Lorkowski, Magdeldin, Abd El~Razek, Abd El~Razek, Majeed, Maleki, Malekzadeh, Malta, Mamun, Manafi, Manda, Mansourian, Martins-Melo, Masaka, Massenburg, Maulik, Mayala, Mazidi, McKee, Mehrotra, Mehta, Meles, Mendoza, Menezes, Meretoja, Meretoja, Mestrovic, Miller, Miller-Petrie, Mills, Milne, Mini, Mir, Mirjalali, Mirrakhimov, Mohamadi, Mohammad, Darwesh, Mezerji, Mohammed, Mohammed, Mokdad, Molokhia, Monasta, Moodley, Moosazadeh, Moradi, Moradi, Moradi, Moradi-Lakeh, Moradinazar, Moraga, Morawska, Mosapour, Mousavi, Mueller, Muluneh, Mustafa, Nabavizadeh, Naderi, Nagarajan, Nahvijou, Najafi, Nangia, Ndwandwe, Neamati, Negoi, Negoi, Ngunjiri, Thi~Nguyen, Nguyen, Nguyen, Nielsen, Ningrum, Nirayo, Nixon, Nnaji, Nojomi, Noroozi, Nosratnejad, Noubiap, Motlagh, Ofori-Asenso, Ogbo, Oladimeji, Olagunju, Olfatifar, Olum, Olusanya, Oluwasanu, Onwujekwe, Oren, Ortega-Altamirano, Ortiz, Osarenotor, Osei, Osgood-Zimmerman, Otstavnov, Owolabi, P.~A., Pagheh, Pakhale, Panda-Jonas, Pandey, Park, Parsian, Pashaei, Patel, Pepito, Pereira, Perkins, Pickering, Pilgrim, Pirestani, Piroozi, Pirsaheb, Plana-Ripoll, Pourjafar, Puri, Qorbani, Quintana, Rabiee, Rabiee, Radfar, Rafiei, Rahim, Rahimi, Rahimi-Movaghar, Rahimzadeh, Rajati, Raju, Ramezankhani, Ranabhat, Rasella, Rashedi, Rawal, Reiner~Jr, Renzaho, Rezaei, Rezapour, Riahi, Ribeiro, Roever, Roro, Roser, Roshandel, Roshani, Rostami, Rubagotti, Rubino, Sabour, Sadat, Sadeghi, Saeedi, Safari, Safari-Faramani, Safdarian, Sahebkar, Salahshoor, Salam, Salamati, Salehi, Zahabi, Salimi, Salimzadeh, Salomon, Sambala, Samy, Santric~Milicevic, Jose, Saraswathy, Sarmiento-Su\'{a}rez, Sartorius, Sathian, Saxena, Sbarra, Schaeffer, Schwebel, Sepanlou, Seyedmousavi, Shaahmadi, Shaikh, Shams-Beyranvand, Shamshirian, Shamsizadeh, Sharafi, Sharif, Sharif-Alhoseini, Sharifi, Sharma, Sharma, Sheikh, Shields, Shigematsu, Shiri, Shiue, Shuval, Siddiqi, Silva, Singh, Sinha, Sisay, Sisay, Sliwa, Smith, Somayaji, Soofi, Soriano, Sreeramareddy, Sudaryanto, Sufiyan, Sykes, Sylaja, Tabar\'{e}s-Seisdedos, Tabb, Tabuchi, Taveira, Temsah, Terkawi, Tessema, Thankappan, Thirunavukkarasu, To, Tovani-Palone, Tran, Tran, Ullah, Usman, Uthman, Vahedian-Azimi, Valdez, van Boven, Vasankari, Vasseghian, Veisani, Venketasubramanian, Violante, Vladimirov, Vlassov, Vos, Vu, Vujcic, Waheed, Wakefield, Wang, Wang, Wang, Ward, Weintraub, Weldegwergs, Weldesamuel, Westerman, Wiysonge, Wondafrash, Woyczynski, Wu, Xu, Yadegar, Yamada, Yazdi-Feyzabadi, Yilgwan, Yip, Yonemoto, Lebni, Younis, Yousefifard, Yousof, Yu, Yusefzadeh, Zabeh, Moghadam, Bin~Zaman, Zamani, Zandian, Zangeneh, Zerfu, Zhang, Ziapour, Zodpey, Murray, and Hay}}}
\bibcite{chen2022dlsyscourse}{{5}{2022}{{Chen and Zheng}}{{}}}
\bibcite{christopher1993nachos}{{6}{1993}{{Christopher et~al.}}{{Christopher, Procter, and Anderson}}}
\bibcite{collins1989cognitive}{{7}{}{{Collins et~al.}}{{Collins, Brown, and Newman}}}
\bibcite{bruner1960process}{{8}{}{{Frolli et~al.}}{{Frolli, Cerciello, Ciotola, Ricci, Esposito, and Sica}}}
\bibcite{abelson1996sicp}{{9}{}{{Fry et~al.}}{{Fry, Abelson, Sussman, and Sussman}}}
\bibcite{roberthalf2024talent}{{10}{2024}{{Half}}{{}}}
\bibcite{howard2020fastai}{{11}{}{{Howard and Gugger}}{{}}}
\bibcite{johnson2016cs231n}{{12}{2016}{{Johnson et~al.}}{{Johnson, Karpathy, and Fei-Fei}}}
\bibcite{blank2019nbgrader}{{13}{}{{Jupyter et~al.}}{{Jupyter, Blank, Bourgin, Brown, Bussonnier, Frederic, Granger, Griffiths, Hamrick, Kelley, Pacer, Page, P\'{e}rez, Ragan-Kelley, Suchow, and Willing}}}
\bibcite{kaashoek2023xv6}{{14}{2023}{{Kaashoek et~al.}}{{Kaashoek, Morris, and Cox}}}
\bibcite{kapur2008productive}{{15}{}{{Kapur}}{{}}}
\bibcite{karpathy2022micrograd}{{16}{2022}{{Karpathy}}{{}}}
\bibcite{tanenbaum1987minix}{{17}{}{{Khodayari~Moez et~al.}}{{Khodayari~Moez, Warkentin, Brhane, Lam, Field, Liu, Zulueta, Valencia, Mesa-Guzman, Nialet, Atkar-Khattra, Davies, Grant, Murison, Montuenga, Amos, Robbins, Johansson, and Hung}}}
\bibcite{krizhevsky2009cifar}{{18}{2009}{{Krizhevsky and Hinton}}{{}}}
\bibcite{lave1991situated}{{19}{}{{Lave and Wenger}}{{}}}
\bibcite{lecun1998gradient}{{20}{}{{Lecun et~al.}}{{Lecun, Bottou, Bengio, and Haffner}}}
\bibcite{mattson2020mlperf}{{21}{2020}{{Mattson et~al.}}{{Mattson, Cheng, Coleman, Diamos, Micikevicius, Patterson, Tang, Wei, Bailis, Bittorf, Brooks, Chen, Dutta, Gupta, Hazelwood, Hock, Huang, Ike, Jia, Kang, Kanter, Kumar, Liao, Ma, Narayanan, Oguntebi, Pekhimenko, Pentecost, Reddi, Robie, John, Tabber, Wu, Xu, Yamazaki, Young, and Zaharia}}}
\bibcite{meadows2008thinking}{{22}{2008}{{Meadows}}{{}}}
\bibcite{hotz2023tinygrad}{{23}{}{{Messina et~al.}}{{Messina, Bianco, and Fedi}}}
\bibcite{meyer2003threshold}{{24}{2003}{{Meyer and Land}}{{}}}
\bibcite{paas1992training}{{25}{}{{Paas}}{{}}}
\bibcite{parashar2019timeloop}{{26}{}{{Parashar et~al.}}{{Parashar, Raina, Shao, Chen, Ying, Mukkara, Venkatesan, Khailany, Keckler, and Emer}}}
\bibcite{patterson2021carbon}{{27}{}{{Patterson et~al.}}{{Patterson, Gonzalez, Le, Liang, Munguia, Rothchild, So, Texier, and Dean}}}
\bibcite{pfaff2004pintos}{{28}{}{{Pfaff et~al.}}{{Pfaff, Romano, and Back}}}
\bibcite{reddi2024mlsysbook}{{29}{}{{Reddi}}{{}}}
\bibcite{mlsysbook2025}{{30}{2025}{{Reddi}}{{}}}
\bibcite{reddi2020mlperf}{{31}{a}{{Reddi et~al.}}{{Reddi, Cheng, Kanter, Mattson, Schmuelling, Wu, Anderson, Breughe, Charlebois, Chou, Chukka, Coleman, Davis, Deng, Diamos, Duke, Fick, Gardner, Hubara, Idgunji, Jablin, Jiao, John, Kanwar, Lee, Liao, Lokhmotov, Massa, Meng, Micikevicius, Osborne, Pekhimenko, Rajan, Sequeira, Sirasao, Sun, Tang, Thomson, Wei, Wu, Xu, Yamada, Yu, Yuan, Zhong, Zhang, and Zhou}}}
\bibcite{banbury2021widening}{{32}{b}{{Reddi et~al.}}{{Reddi, Plancher, Kennedy, Moroney, Warden, Agarwal, Banbury, Banzi, Bennett, Brown, Chitlangia, Ghosal, Grafman, Jaeger, Krishnan, Lam, Leiker, Mann, Mazumder, Pajak, Ramaprasad, Smith, Stewart, and Tingley}}}
\bibcite{rosenblatt1958perceptron}{{33}{}{{Rosenblatt}}{{}}}
\bibcite{rumelhart1986learning}{{34}{}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{schneider2020minitorch}{{35}{2020}{{Rush}}{{}}}
\bibcite{samajdar2018scale}{{36}{2018}{{Samajdar et~al.}}{{Samajdar, Zhu, Whatmough, Mattina, and Krishna}}}
\bibcite{keller2025ai}{{37}{2025}{{Search}}{{}}}
\bibcite{sorva2012visual}{{38}{2012}{{Sorva}}{{}}}
\bibcite{strubell2019energy}{{39}{}{{Strubell et~al.}}{{Strubell, Ganesh, and McCallum}}}
\bibcite{sutton2019bitter}{{40}{2019}{{Sutton}}{{}}}
\bibcite{sweller1988cognitive}{{41}{}{{Sweller}}{{}}}
\bibcite{pytorch04release}{{42}{2018}{{Team}}{{}}}
\bibcite{tensorflow20}{{43}{2019}{{Team}}{{}}}
\bibcite{thompson2008bloom}{{44}{2008}{{Thompson et~al.}}{{Thompson, Luxton-Reilly, Whalley, Hu, and Robbins}}}
\bibcite{vaswani2017attention}{{45}{}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{vygotsky1978mind}{{46}{}{{VYGOTSKY}}{{}}}
\bibcite{williams2009roofline}{{47}{}{{Williams et~al.}}{{Williams, Waterman, and Patterson}}}
\bibcite{kannan2022astrasim}{{48}{}{{Won et~al.}}{{Won, Heo, Rashidi, Sridharan, Srinivasan, and Krishna}}}
\bibcite{papert1980mindstorms}{{49}{}{{Wooster and Papert}}{{}}}
\bibcite{zhang2021dive}{{50}{}{{Zhang et~al.}}{{Zhang, Lipton, Li, and Smola}}}
\gdef \@abspage@last{28}
