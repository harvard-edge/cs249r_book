% Horizontal Module Dependency Diagram for TinyTorch Paper
% Use with: \input{module_flow_horizontal.tex}
% Requires: \usepackage{tikz}, \usetikzlibrary{shapes,arrows,positioning,fit,backgrounds}

\begin{figure*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[
    node distance=0.4cm and 0.6cm,
    every node/.style={font=\scriptsize},
    % Tier styles
    foundation/.style={rectangle, draw=blue!70, fill=blue!20, minimum width=1.4cm, minimum height=0.55cm, rounded corners=2pt},
    architecture/.style={rectangle, draw=purple!70, fill=purple!20, minimum width=1.4cm, minimum height=0.55cm, rounded corners=2pt},
    optimization/.style={rectangle, draw=orange!70, fill=orange!20, minimum width=1.4cm, minimum height=0.55cm, rounded corners=2pt},
    capstone/.style={rectangle, draw=red!70, fill=red!20, minimum width=1.4cm, minimum height=0.55cm, rounded corners=2pt},
    % Arrow style
    arr/.style={->, >=stealth, thick},
    tier/.style={draw=gray!40, dashed, rounded corners=5pt, inner sep=8pt}
]

% === FOUNDATION TIER (01-07) ===
\node[foundation] (T) {01 Tensor};
\node[foundation, right=of T] (A) {02 Activ.};
\node[foundation, right=of A] (L) {03 Layers};
\node[foundation, right=of L] (Loss) {04 Losses};
\node[foundation, right=of Loss] (Auto) {05 Autograd};
\node[foundation, right=of Auto] (Opt) {06 Optim.};
\node[foundation, right=of Opt] (Train) {07 Training};

% Foundation arrows
\draw[arr] (T) -- (A);
\draw[arr] (A) -- (L);
\draw[arr] (L) -- (Loss);
\draw[arr] (Loss) -- (Auto);
\draw[arr] (Auto) -- (Opt);
\draw[arr] (Opt) -- (Train);

% === ARCHITECTURE TIER (08-13) ===
% DataLoader - the branch point
\node[architecture, right=1.2cm of Train] (Data) {08 DataLoader};
\draw[arr] (Train) -- (Data);

% Vision path (top branch)
\node[architecture, above right=0.6cm and 0.8cm of Data] (Spatial) {09 CNNs};
\draw[arr] (Data) -- (Spatial);

% Language path (bottom branch)
\node[architecture, below right=0.6cm and 0.8cm of Data] (Tok) {10 Token.};
\node[architecture, right=of Tok] (Emb) {11 Embed.};
\node[architecture, right=of Emb] (Att) {12 Attention};
\node[architecture, right=of Att] (Trans) {13 Transform.};

\draw[arr] (Data) -- (Tok);
\draw[arr] (Tok) -- (Emb);
\draw[arr] (Emb) -- (Att);
\draw[arr] (Att) -- (Trans);

% === OPTIMIZATION TIER (14-19) ===
% Profiling - convergence point
\node[optimization, right=1.5cm of Spatial, yshift=-0.6cm] (Prof) {14 Profiling};
\draw[arr] (Spatial) -- (Prof);
\draw[arr] (Trans) -- (Prof);

% Parallel optimization branches
\node[optimization, above right=0.4cm and 0.6cm of Prof] (Quant) {15 Quant.};
\node[optimization, right=0.4cm of Quant] (Comp) {16 Compress.};
\node[optimization, below right=0.4cm and 0.6cm of Prof] (Accel) {17 Accel.};
\node[optimization, right=0.4cm of Accel] (Memo) {18 Memo.};

\draw[arr] (Prof) -- (Quant);
\draw[arr] (Prof) -- (Memo);
\draw[arr] (Quant) -- (Comp);
\draw[arr] (Accel) -- (Memo);

% Benchmarking - convergence
\node[optimization, right=0.8cm of Prof, xshift=2.8cm] (Bench) {19 Benchmark};
\draw[arr] (Comp) -- (Bench);
\draw[arr] (Accel) -- (Bench);

% === CAPSTONE (20) ===
\node[capstone, right=0.8cm of Bench] (Cap) {20 Capstone};
\draw[arr] (Bench) -- (Cap);

% === TIER LABELS ===
\node[above=0.8cm of L, font=\scriptsize\bfseries, blue!70] {FOUNDATION (01-07)};
\node[above=0.9cm of Att, font=\scriptsize\bfseries, purple!70] {ARCHITECTURE (08-13)};
\node[above=0.9cm of Comp, font=\scriptsize\bfseries, orange!70] {OPTIMIZATION (14-19)};

% === PATH LABELS ===
\node[above=0.15cm of Spatial, font=\tiny, gray] {Vision};
\node[below=0.15cm of Tok, font=\tiny, gray] {Language};
\node[above=0.1cm of Quant, font=\tiny, gray] {Size};
\node[below=0.1cm of Memo, font=\tiny, gray] {Speed};

\end{tikzpicture}%
}
\caption{\textbf{Module Dependency Graph.} TinyTorch's 20 modules form a directed acyclic graph with two architectural paths. Foundation modules (blue, M01--08) build core infrastructure sequentially. At DataLoader (M05), students continue through training infrastructure. The \emph{Vision} path (M09) builds CNNs for spatial processing; the \emph{Language} path (M10--13) builds tokenization through transformers. Both paths converge at Profiling (M14), then branch into parallel optimization tracks---\emph{Model-level} (quantization, compression) and \emph{Runtime} (acceleration, memoization)---before final convergence at Benchmarking (M19) and Capstone (M20).}
\label{fig:module-dag}
\end{figure*}
