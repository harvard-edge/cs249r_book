
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 19: Benchmarking" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/19_benchmarking_ABOUT.html" />
<meta property="og:site_name" content="Tinyüî•Torch" />
<meta property="og:description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: B..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="Tinyüî•Torch" />
<meta name="description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: B..." />

    <title>Module 19: Benchmarking &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a6ab0a36" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";

const defaultStyle = document.createElement('style');
defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}
`;
document.head.appendChild(defaultStyle);

const fullscreenStyle = document.createElement('style');
fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
document.head.appendChild(fullscreenStyle);

// Detect if page has dark background
const isDarkTheme = () => {
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        return brightness < 128;
    }
    return false;
};

const load = async () => {
    await mermaid.run();

    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(load, 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(load, 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(load, 200);
        return;
    }

    const darkTheme = isDarkTheme();

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    const modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    const modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    let previousScrollOffset = [window.scrollX, window.scrollY];

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    const allButtons = [];

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });

        container.appendChild(fullscreenBtn);
        allButtons.push(fullscreenBtn);
    });

    // Update theme classes when theme changes
    const updateTheme = () => {
        const dark = isDarkTheme();
        allButtons.forEach(btn => {
            if (dark) {
                btn.classList.add('dark-theme');
            } else {
                btn.classList.remove('dark-theme');
            }
        });
        if (dark) {
            modal.classList.add('dark-theme');
            modalContent.classList.add('dark-theme');
            closeBtn.classList.add('dark-theme');
        } else {
            modal.classList.remove('dark-theme');
            modalContent.classList.remove('dark-theme');
            closeBtn.classList.remove('dark-theme');
        }
    };

    // Watch for theme changes
    const observer = new MutationObserver(updateTheme);
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    observer.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style']
    });
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/19_benchmarking_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=0d27a1a4"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=ee94e95f"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/subscribe-modal.js?v=c8499bec"></script>
    <link rel="canonical" href="https://mlsysbook.ai/tinytorch/modules/19_benchmarking_ABOUT.html" />
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Torch Olympics" href="../tiers/olympics.html" />
    <link rel="prev" title="Module 18: Memoization" href="18_memoization_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../book/" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mlsysbook" title="Support" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-heart fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Support</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-08)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader_ABOUT.html">05. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_autograd_ABOUT.html">06. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimizers_ABOUT.html">07. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_training_ABOUT.html">08. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (09-13)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_acceleration_ABOUT.html">17. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_memoization_ABOUT.html">18. Memoization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/19_benchmarking_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 19: Benchmarking</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarkresult-dataclass">BenchmarkResult Dataclass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#timing-context-manager">Timing Context Manager</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-class">Benchmark Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarksuite-class">BenchmarkSuite Class</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking-methodology">Benchmarking Methodology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-selection">Metrics Selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-validity">Statistical Validity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reporting-results">Reporting Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#insufficient-measurement-runs">Insufficient Measurement Runs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skipping-warmup">Skipping Warmup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-different-input-shapes">Comparing Different Input Shapes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ignoring-variance">Ignoring Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-industry-benchmarks">Your Implementation vs. Industry Benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-benchmarking-matters-at-scale">Why Benchmarking Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-19-benchmarking">
<h1>Module 19: Benchmarking<a class="headerlink" href="#module-19-benchmarking" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>OPTIMIZATION TIER</strong> | Difficulty: ‚óè‚óè‚óè‚óã | Time: 5-7 hours | Prerequisites: 01-18</p>
<p>This module assumes familiarity with the complete TinyTorch stack (Modules 01-13), profiling (Module 14), and optimization techniques (Modules 15-18). You should understand how to build, profile, and optimize models before tackling systematic benchmarking and statistical comparison of optimizations.</p>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run interactively in your browser.</p>
<p class="sd-card-text"><a href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F19_benchmarking%2F19_benchmarking.ipynb" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #f97316; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">Open in Binder ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìÑ View Source</div>
<p class="sd-card-text">Browse the source code on GitHub.</p>
<p class="sd-card-text"><a href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/19_benchmarking/19_benchmarking.py" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #6b7280; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">View on GitHub ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üéß Audio Overview</div>
<p class="sd-card-text">Listen to an AI-generated overview.</p>
<audio controls style="width: 100%; height: 54px; margin-top: auto;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/19_benchmarking.mp3" type="audio/mpeg">
</audio>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Benchmarking transforms performance optimization from guesswork into engineering discipline. You have learned individual optimization techniques in Modules 14-18, but how do you know which optimizations actually work? How do you compare a quantized model against a pruned one? How do you ensure your measurements are statistically valid rather than random noise?</p>
<p>In this module, you will build the infrastructure that powers TorchPerf Olympics, the capstone competition framework. You will implement professional benchmarking tools that measure latency, accuracy, and memory with statistical rigor, generate Pareto frontiers showing optimization trade-offs, and produce reproducible results that guide real engineering decisions.</p>
<p>By the end, you will have the evaluation framework needed to systematically combine optimizations and compete in the capstone challenge.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> professional benchmarking infrastructure with statistical analysis including confidence intervals and variance control</p></li>
<li><p><strong>Master</strong> multi-objective optimization trade-offs between accuracy, latency, and memory through Pareto frontier analysis</p></li>
<li><p><strong>Understand</strong> measurement uncertainty, warmup protocols, and reproducibility requirements for fair model comparison</p></li>
<li><p><strong>Connect</strong> optimization techniques from Modules 14-18 into systematic evaluation workflows for the TorchPerf Olympics capstone</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart TB
    subgraph &quot;Benchmarking Infrastructure&quot;
        A[&quot;BenchmarkResult&lt;br/&gt;Statistical container&quot;]
        B[&quot;Benchmark&lt;br/&gt;Single metric runner&quot;]
        C[&quot;BenchmarkSuite&lt;br/&gt;Multi-metric evaluation&quot;]
        D[&quot;TorchPerf Olympics&lt;br/&gt;Competition framework&quot;]
    end

    E[&quot;Models to Compare&quot;] --&gt; B
    F[&quot;Test Datasets&quot;] --&gt; B
    B --&gt; A
    A --&gt; C
    C --&gt; D

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
    </pre><figcaption>
<p><span class="caption-number">Fig. 28 </span><span class="caption-text">Benchmarking Infrastructure</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Part</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BenchmarkResult</span></code> dataclass</p></td>
<td><p>Statistical analysis with confidence intervals</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">precise_timer()</span></code> context manager</p></td>
<td><p>High-precision timing for microsecond measurements</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Benchmark.run_latency_benchmark()</span></code></p></td>
<td><p>Warmup protocols and latency measurement</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Benchmark.run_accuracy_benchmark()</span></code></p></td>
<td><p>Model quality evaluation across datasets</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Benchmark.run_memory_benchmark()</span></code></p></td>
<td><p>Peak memory tracking during inference</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BenchmarkSuite</span></code> for multi-metric analysis</p></td>
<td><p>Pareto frontier generation and trade-off visualization</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare baseline vs optimized model with statistical rigor</span>
<span class="n">benchmark</span> <span class="o">=</span> <span class="n">Benchmark</span><span class="p">([</span><span class="n">baseline_model</span><span class="p">,</span> <span class="n">optimized_model</span><span class="p">])</span>
<span class="n">latency_results</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">run_latency_benchmark</span><span class="p">()</span>
<span class="c1"># Output: baseline: 12.3ms ¬± 0.8ms, optimized: 4.1ms ¬± 0.3ms (67% reduction, p &lt; 0.01)</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>Hardware-specific benchmarks (GPU profiling requires CUDA, covered in production frameworks)</p></li>
<li><p>Energy measurement (requires specialized hardware like power meters)</p></li>
<li><p>Distributed benchmarking (multi-node coordination is beyond scope)</p></li>
<li><p>Automated hyperparameter tuning for optimization (that is Module 20: Capstone)</p></li>
</ul>
<p><strong>You are building the statistical foundation for fair comparison.</strong> Advanced deployment scenarios come in production systems.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section documents the benchmarking API you will implement. Use it as your reference while building the statistical analysis and measurement infrastructure.</p>
<section id="benchmarkresult-dataclass">
<h3>BenchmarkResult Dataclass<a class="headerlink" href="#benchmarkresult-dataclass" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BenchmarkResult</span><span class="p">(</span><span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">metadata</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{})</span>
</pre></div>
</div>
<p>Statistical container for benchmark measurements with automatic computation of mean, standard deviation, median, and 95% confidence intervals.</p>
<p><strong>Properties computed in <code class="docutils literal notranslate"><span class="pre">__post_init__</span></code>:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">mean</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>Average of all measurements</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">std</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>Standard deviation (0 if single measurement)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">median</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>Middle value, less sensitive to outliers</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">min_val</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>Minimum observed value</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">max_val</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>Maximum observed value</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">count</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p>Number of measurements</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ci_lower</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>Lower bound of 95% confidence interval</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ci_upper</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">float</span></code></p></td>
<td><p>Upper bound of 95% confidence interval</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Methods:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">to_dict</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">to_dict()</span> <span class="pre">-&gt;</span> <span class="pre">Dict[str,</span> <span class="pre">Any]</span></code></p></td>
<td><p>Serialize to dictionary for JSON export</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__str__</span></code></p></td>
<td><p>Returns formatted summary</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;metric:</span> <span class="pre">mean</span> <span class="pre">¬±</span> <span class="pre">std</span> <span class="pre">(n=count)&quot;</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="timing-context-manager">
<h3>Timing Context Manager<a class="headerlink" href="#timing-context-manager" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">precise_timer</span><span class="p">()</span> <span class="k">as</span> <span class="n">timer</span><span class="p">:</span>
    <span class="c1"># Your code to measure</span>
    <span class="o">...</span>
<span class="c1"># Access elapsed time</span>
<span class="n">elapsed_seconds</span> <span class="o">=</span> <span class="n">timer</span><span class="o">.</span><span class="n">elapsed</span>
</pre></div>
</div>
<p>High-precision timing context manager using <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code> for monotonic, nanosecond-resolution measurements.</p>
</section>
<section id="benchmark-class">
<h3>Benchmark Class<a class="headerlink" href="#benchmark-class" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Benchmark</span><span class="p">(</span><span class="n">models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">datasets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
          <span class="n">warmup_runs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">measurement_runs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>Core benchmarking engine for single-metric evaluation across multiple models.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">models</span></code>: List of models to benchmark (supports any object with forward/predict/<strong>call</strong>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">datasets</span></code>: List of datasets for accuracy benchmarking (required)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">warmup_runs</span></code>: Number of warmup iterations before measurement (default: 5)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">measurement_runs</span></code>: Number of measurement iterations for statistical analysis (default: 10)</p></li>
</ul>
<p><strong>Core Methods:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">run_latency_benchmark</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">run_latency_benchmark(input_shape=(1,28,28))</span> <span class="pre">-&gt;</span> <span class="pre">Dict[str,</span> <span class="pre">BenchmarkResult]</span></code></p></td>
<td><p>Measure inference time per model</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">run_accuracy_benchmark</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">run_accuracy_benchmark()</span> <span class="pre">-&gt;</span> <span class="pre">Dict[str,</span> <span class="pre">BenchmarkResult]</span></code></p></td>
<td><p>Measure prediction accuracy on datasets</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">run_memory_benchmark</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">run_memory_benchmark(input_shape=(1,28,28))</span> <span class="pre">-&gt;</span> <span class="pre">Dict[str,</span> <span class="pre">BenchmarkResult]</span></code></p></td>
<td><p>Track peak memory usage during inference</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">compare_models</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">compare_models(metric:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">&quot;latency&quot;)</span> <span class="pre">-&gt;</span> <span class="pre">List[Dict]</span></code></p></td>
<td><p>Compare models across a specific metric</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="benchmarksuite-class">
<h3>BenchmarkSuite Class<a class="headerlink" href="#benchmarksuite-class" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BenchmarkSuite</span><span class="p">(</span><span class="n">models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">datasets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">output_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;benchmark_results&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Comprehensive multi-metric evaluation suite for generating Pareto frontiers and optimization trade-off analysis.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">run_full_benchmark</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">run_full_benchmark()</span> <span class="pre">-&gt;</span> <span class="pre">Dict[str,</span> <span class="pre">Dict[str,</span> <span class="pre">BenchmarkResult]]</span></code></p></td>
<td><p>Run all benchmark types and aggregate results</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">plot_pareto_frontier</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">plot_pareto_frontier(x_metric='latency',</span> <span class="pre">y_metric='accuracy')</span></code></p></td>
<td><p>Plot Pareto frontier for two competing objectives</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">plot_results</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">plot_results(save_plots=True)</span></code></p></td>
<td><p>Generate visualization plots for benchmark results</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">generate_report</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">generate_report()</span> <span class="pre">-&gt;</span> <span class="pre">str</span></code></p></td>
<td><p>Generate comprehensive benchmark report</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental principles that make benchmarking scientifically valid. Understanding these concepts separates professional performance engineering from naive timing measurements.</p>
<section id="benchmarking-methodology">
<h3>Benchmarking Methodology<a class="headerlink" href="#benchmarking-methodology" title="Link to this heading">#</a></h3>
<p>Single measurements are meaningless in performance engineering. Consider timing a model inference once: you might get 1.2ms. Run it again and you might get 3.1ms because a background process started. Which number represents the model‚Äôs true performance? Neither.</p>
<p>Professional benchmarking treats measurements as samples from a noisy distribution. The true performance is the distribution‚Äôs mean, and your job is to estimate it with statistical confidence. This requires understanding measurement variance and controlling for confounding factors.</p>
<p>The methodology follows a structured protocol:</p>
<p><strong>Warmup Phase</strong>: Modern systems adapt to workloads. JIT compilers optimize hot code paths after several executions. CPU frequency scales up under sustained load. Caches fill with frequently accessed data. Without warmup, your first measurements capture cold-start behavior, not steady-state performance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Warmup protocol in action</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">warmup_runs</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>  <span class="c1"># Run but discard measurements</span>
</pre></div>
</div>
<p><strong>Measurement Phase</strong>: After warmup, run the operation multiple times and collect timing data. The Central Limit Theorem tells us that with enough samples, the sample mean approaches the true mean, and we can compute confidence intervals.</p>
<p>Here is how the Benchmark class implements the complete protocol:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_latency_benchmark</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">BenchmarkResult</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmark model inference latency using Profiler.&quot;&quot;&quot;</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">):</span>
        <span class="n">model_name</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;model_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Create input tensor for this benchmark</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">input_shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

        <span class="c1"># Warmup runs (discard results)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_runs</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;forward&#39;</span><span class="p">):</span>
                <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
                <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

        <span class="c1"># Measurement runs (collect statistics)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">measurement_runs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">precise_timer</span><span class="p">()</span> <span class="k">as</span> <span class="n">timer</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;forward&#39;</span><span class="p">):</span>
                    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
                    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
            <span class="n">latencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">timer</span><span class="o">.</span><span class="n">elapsed</span><span class="p">)</span>

        <span class="n">results</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">BenchmarkResult</span><span class="p">(</span>
            <span class="n">metric_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">_latency&quot;</span><span class="p">,</span>
            <span class="n">values</span><span class="o">=</span><span class="n">latencies</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;input_shape&#39;</span><span class="p">:</span> <span class="n">input_shape</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">system_info</span><span class="p">}</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p>Notice the clear separation between warmup (discarded) and measurement (collected) phases. This ensures measurements reflect steady-state performance.</p>
<p><strong>Statistical Analysis</strong>: Raw measurements get transformed into confidence intervals that quantify uncertainty. The 95% confidence interval tells you: ‚ÄúIf we ran this benchmark 100 times, 95 of those runs would produce a mean within this range.‚Äù</p>
</section>
<section id="metrics-selection">
<h3>Metrics Selection<a class="headerlink" href="#metrics-selection" title="Link to this heading">#</a></h3>
<p>Different optimization goals require different metrics. Choosing the wrong metric leads to optimizing for the wrong objective.</p>
<p><strong>Latency (Time per Inference)</strong>: Measures how fast a model processes a single input. Critical for real-time systems like autonomous vehicles where a prediction must complete in 30ms before the next camera frame arrives. Measured in milliseconds or microseconds.</p>
<p><strong>Throughput (Inputs per Second)</strong>: Measures total processing capacity. Critical for batch processing systems like translating millions of documents. Higher throughput means more efficient hardware utilization. Measured in samples per second or frames per second.</p>
<p><strong>Accuracy (Prediction Quality)</strong>: Measures how often the model makes correct predictions. The fundamental quality metric. No point having a 1ms model if it is wrong 50% of the time. Measured as percentage of correct predictions on a held-out test set.</p>
<p><strong>Memory Footprint</strong>: Measures peak RAM usage during inference. Critical for edge devices with limited memory. A 100MB model cannot run on a device with 64MB RAM. Measured in megabytes.</p>
<p><strong>Model Size</strong>: Measures storage size of model parameters. Critical for over-the-air updates and storage-constrained devices. A 500MB model takes minutes to download on slow networks. Measured in megabytes.</p>
<p>The key insight: <strong>these metrics trade off against each other</strong>. Quantization reduces memory but may reduce accuracy. Pruning reduces latency but may require retraining. Professional benchmarking reveals these trade-offs quantitatively.</p>
</section>
<section id="statistical-validity">
<h3>Statistical Validity<a class="headerlink" href="#statistical-validity" title="Link to this heading">#</a></h3>
<p>Statistics transforms noisy measurements into reliable conclusions. Without statistical validity, you cannot distinguish true performance differences from random noise.</p>
<p><strong>Why Variance Matters</strong>: Consider two models. Model A runs in 10.2ms, 10.3ms, 10.1ms (low variance). Model B runs in 9.5ms, 12.1ms, 8.9ms (high variance). Is B faster? Looking at means (10.2ms vs 10.2ms) suggests they are identical, but B‚Äôs high variance makes it unpredictable. Statistical analysis reveals both the mean and the reliability.</p>
<p>The BenchmarkResult class computes statistical properties automatically:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">BenchmarkResult</span><span class="p">:</span>
    <span class="n">metric_name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="n">metadata</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute statistics after initialization.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;BenchmarkResult requires at least one measurement.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">stdev</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">median</span> <span class="o">=</span> <span class="n">statistics</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

        <span class="c1"># 95% confidence interval for the mean</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">t_score</span> <span class="o">=</span> <span class="mf">1.96</span>  <span class="c1"># Approximate for large samples</span>
            <span class="n">margin_error</span> <span class="o">=</span> <span class="n">t_score</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ci_lower</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">-</span> <span class="n">margin_error</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ci_upper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+</span> <span class="n">margin_error</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ci_lower</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ci_upper</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</pre></div>
</div>
<p>The confidence interval calculation uses the standard error of the mean (std / sqrt(n)) scaled by the t-score. For large samples, a t-score of 1.96 corresponds to 95% confidence. This means: ‚ÄúWe are 95% confident the true mean latency lies between ci_lower and ci_upper.‚Äù</p>
<p><strong>Coefficient of Variation</strong>: The ratio std / mean measures relative noise. A CV of 0.05 means standard deviation is 5% of the mean, indicating stable measurements. A CV of 0.30 means 30% relative noise, indicating unstable or noisy measurements that need more samples.</p>
<p><strong>Outlier Detection</strong>: Extreme values can skew the mean. The median is robust to outliers. If mean and median differ significantly, investigate outliers. They might indicate thermal throttling, background processes, or measurement errors.</p>
</section>
<section id="reproducibility">
<h3>Reproducibility<a class="headerlink" href="#reproducibility" title="Link to this heading">#</a></h3>
<p>Reproducibility means another engineer can run your benchmark and get the same results. Without reproducibility, your optimization insights are not transferable.</p>
<p><strong>System Metadata</strong>: Recording system configuration ensures results are interpreted correctly. A benchmark on a 2020 laptop will differ from a 2024 server, not because the model changed, but because the hardware did.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">datasets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">warmup_runs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_WARMUP_RUNS</span><span class="p">,</span>
             <span class="n">measurement_runs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_MEASUREMENT_RUNS</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="n">models</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">datasets</span> <span class="o">=</span> <span class="n">datasets</span> <span class="ow">or</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_runs</span> <span class="o">=</span> <span class="n">warmup_runs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">measurement_runs</span> <span class="o">=</span> <span class="n">measurement_runs</span>

    <span class="c1"># Capture system information for reproducibility</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">system_info</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;platform&#39;</span><span class="p">:</span> <span class="n">platform</span><span class="o">.</span><span class="n">platform</span><span class="p">(),</span>
        <span class="s1">&#39;python_version&#39;</span><span class="p">:</span> <span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">(),</span>
        <span class="s1">&#39;cpu_count&#39;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span> <span class="ow">or</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>This metadata gets embedded in every BenchmarkResult, allowing you to understand why a benchmark run produced specific numbers.</p>
<p><strong>Controlled Environment</strong>: Background processes, thermal state, and power settings all affect measurements. Professional benchmarking controls these factors:</p>
<ul class="simple">
<li><p>Close unnecessary applications before benchmarking</p></li>
<li><p>Let the system reach thermal equilibrium (run warmup)</p></li>
<li><p>Use the same hardware configuration across runs</p></li>
<li><p>Document any anomalies or environmental changes</p></li>
</ul>
<p><strong>Versioning</strong>: Record the versions of all dependencies. A NumPy update might change BLAS library behavior, affecting performance. Recording versions ensures results remain interpretable months later.</p>
</section>
<section id="reporting-results">
<h3>Reporting Results<a class="headerlink" href="#reporting-results" title="Link to this heading">#</a></h3>
<p>Raw data is useless without clear communication. Professional benchmarking generates reports that guide engineering decisions.</p>
<p><strong>Comparison Tables</strong>: Show mean, standard deviation, and confidence intervals for each model on each metric. This lets stakeholders quickly identify winners and understand uncertainty.</p>
<p><strong>Pareto Frontiers</strong>: When metrics trade off, visualize the Pareto frontier to show which models are optimal for different constraints. A model is Pareto-optimal if no other model is better on all metrics simultaneously.</p>
<p>Consider three models:</p>
<ul class="simple">
<li><p>Model A: 10ms latency, 90% accuracy</p></li>
<li><p>Model B: 15ms latency, 95% accuracy</p></li>
<li><p>Model C: 12ms latency, 91% accuracy</p></li>
</ul>
<p>Model C is dominated by Model A (faster and only 1% less accurate). It is not Pareto-optimal. Models A and B are both Pareto-optimal: if you want maximum accuracy, choose B; if you want minimum latency, choose A.</p>
<p><strong>Visualization</strong>: Scatter plots reveal relationships between metrics. Plot latency vs accuracy and you immediately see the trade-off frontier. Add annotations showing model names and you have an actionable decision tool.</p>
<p><strong>Statistical Significance</strong>: Report not just means but confidence intervals. Saying ‚ÄúModel A is 2ms faster‚Äù is incomplete. Saying ‚ÄúModel A is 2ms faster with 95% confidence interval [1.5ms, 2.5ms], p &lt; 0.01‚Äù provides the statistical rigor needed for engineering decisions.</p>
</section>
</section>
<section id="common-errors">
<h2>Common Errors<a class="headerlink" href="#common-errors" title="Link to this heading">#</a></h2>
<p>These are the mistakes students commonly make when implementing benchmarking infrastructure. Understanding these patterns will save you debugging time.</p>
<section id="insufficient-measurement-runs">
<h3>Insufficient Measurement Runs<a class="headerlink" href="#insufficient-measurement-runs" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Running a benchmark only 3 times produces unreliable statistics.</p>
<p><strong>Symptom</strong>: Results change dramatically between benchmark runs. Confidence intervals are extremely wide.</p>
<p><strong>Cause</strong>: The Central Limit Theorem requires sufficient samples. With only 3 measurements, the sample mean is a poor estimator of the true mean.</p>
<p><strong>Fix</strong>: Use at least 10 measurement runs (the default in this module). For high-variance operations, increase to 30+ runs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># BAD: Only 3 measurements</span>
<span class="n">benchmark</span> <span class="o">=</span> <span class="n">Benchmark</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">measurement_runs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># Unreliable!</span>

<span class="c1"># GOOD: 10+ measurements</span>
<span class="n">benchmark</span> <span class="o">=</span> <span class="n">Benchmark</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">measurement_runs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Statistical confidence</span>
</pre></div>
</div>
</section>
<section id="skipping-warmup">
<h3>Skipping Warmup<a class="headerlink" href="#skipping-warmup" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Measuring performance without warmup captures cold-start behavior, not steady-state performance.</p>
<p><strong>Symptom</strong>: First measurement is much slower than subsequent ones. Results do not reflect production performance.</p>
<p><strong>Cause</strong>: JIT compilation, cache warming, and CPU frequency scaling all require several iterations to stabilize.</p>
<p><strong>Fix</strong>: Always run warmup iterations before measurement.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Already handled in Benchmark class</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_runs</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>  <span class="c1"># Warmup (discarded)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">measurement_runs</span><span class="p">):</span>
    <span class="c1"># Now measure steady-state performance</span>
</pre></div>
</div>
</section>
<section id="comparing-different-input-shapes">
<h3>Comparing Different Input Shapes<a class="headerlink" href="#comparing-different-input-shapes" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Benchmarking Model A on 28x28 images and Model B on 224x224 images, then comparing latency.</p>
<p><strong>Symptom</strong>: Misleading conclusions about which model is faster.</p>
<p><strong>Cause</strong>: Larger inputs require more computation. You are measuring input size effects, not model efficiency.</p>
<p><strong>Fix</strong>: Use identical input shapes across all models in a benchmark.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ensure all models use same input shape</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">run_latency_benchmark</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="ignoring-variance">
<h3>Ignoring Variance<a class="headerlink" href="#ignoring-variance" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Reporting only the mean, ignoring standard deviation and confidence intervals.</p>
<p><strong>Symptom</strong>: Cannot determine if performance differences are statistically significant or just noise.</p>
<p><strong>Cause</strong>: Treating measurements as deterministic when they are actually stochastic.</p>
<p><strong>Fix</strong>: Always report confidence intervals, not just means.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># BenchmarkResult automatically computes confidence intervals</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">BenchmarkResult</span><span class="p">(</span><span class="s2">&quot;latency&quot;</span><span class="p">,</span> <span class="n">measurements</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">mean</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">ms ¬± </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">std</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">ms, 95% CI: [</span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">ci_lower</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">ci_upper</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-industry-benchmarks">
<h3>Your Implementation vs. Industry Benchmarks<a class="headerlink" href="#your-implementation-vs-industry-benchmarks" title="Link to this heading">#</a></h3>
<p>Your TinyTorch benchmarking infrastructure implements the same statistical principles used in production ML benchmarking frameworks. The difference is scale and automation.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>MLPerf / Industry</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Statistical Analysis</strong></p></td>
<td><p>Mean, std, 95% CI</p></td>
<td><p>Same + hypothesis testing, ANOVA</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Metrics</strong></p></td>
<td><p>Latency, accuracy, memory</p></td>
<td><p>Same + energy, throughput, tail latency</p></td>
</tr>
<tr class="row-even"><td><p><strong>Warmup Protocol</strong></p></td>
<td><p>Fixed warmup runs</p></td>
<td><p>Same + adaptive warmup until convergence</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Reproducibility</strong></p></td>
<td><p>System metadata</p></td>
<td><p>Same + hardware specs, thermal state</p></td>
</tr>
<tr class="row-even"><td><p><strong>Automation</strong></p></td>
<td><p>Manual benchmark runs</p></td>
<td><p>CI/CD integration, regression detection</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Scale</strong></p></td>
<td><p>Single machine</p></td>
<td><p>Distributed benchmarks across clusters</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following shows equivalent benchmarking patterns in TinyTorch and production frameworks like MLPerf.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.benchmarking</span><span class="w"> </span><span class="kn">import</span> <span class="n">Benchmark</span>

<span class="c1"># Setup models and benchmark</span>
<span class="n">benchmark</span> <span class="o">=</span> <span class="n">Benchmark</span><span class="p">(</span>
    <span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="n">baseline_model</span><span class="p">,</span> <span class="n">optimized_model</span><span class="p">],</span>
    <span class="n">warmup_runs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">measurement_runs</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>

<span class="c1"># Run latency benchmark</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">run_latency_benchmark</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>

<span class="c1"># Analyze results</span>
<span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">mean</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">ms ¬± </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">std</span><span class="o">*</span><span class="mi">1000</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">ms&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
MLPerf (Industry Standard)</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">mlperf_loadgen</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lg</span>

<span class="c1"># Configure benchmark scenario</span>
<span class="n">settings</span> <span class="o">=</span> <span class="n">lg</span><span class="o">.</span><span class="n">TestSettings</span><span class="p">()</span>
<span class="n">settings</span><span class="o">.</span><span class="n">scenario</span> <span class="o">=</span> <span class="n">lg</span><span class="o">.</span><span class="n">TestScenario</span><span class="o">.</span><span class="n">SingleStream</span>
<span class="n">settings</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">lg</span><span class="o">.</span><span class="n">TestMode</span><span class="o">.</span><span class="n">PerformanceOnly</span>

<span class="c1"># Run standardized benchmark</span>
<span class="n">sut</span> <span class="o">=</span> <span class="n">SystemUnderTest</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">lg</span><span class="o">.</span><span class="n">StartTest</span><span class="p">(</span><span class="n">sut</span><span class="p">,</span> <span class="n">qsl</span><span class="p">,</span> <span class="n">settings</span><span class="p">)</span>

<span class="c1"># Results include latency percentiles, throughput, accuracy</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs understand the comparison:</p>
<ul class="simple">
<li><p><strong>Line 1-3 (Setup)</strong>: TinyTorch uses a simple class-based API. MLPerf uses a loadgen library with standardized scenarios (SingleStream, Server, MultiStream, Offline). Both ensure fair comparison.</p></li>
<li><p><strong>Line 6-8 (Configuration)</strong>: TinyTorch exposes warmup_runs and measurement_runs directly. MLPerf abstracts this into TestSettings with scenario-specific defaults. Same concept, different abstraction level.</p></li>
<li><p><strong>Line 11-13 (Execution)</strong>: TinyTorch returns BenchmarkResult objects with statistics. MLPerf logs results to standardized formats that compare across hardware vendors. Both provide statistical analysis.</p></li>
<li><p><strong>Statistical Rigor</strong>: Both use repeated measurements, warmup, and confidence intervals. TinyTorch teaches the foundations; MLPerf adds industry-specific requirements.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>The statistical methodology, warmup protocols, and reproducibility requirements are identical. Production frameworks add automation, standardization across organizations, and hardware-specific optimizations. Understanding TinyTorch benchmarking gives you the foundation to work with any industry benchmarking framework.</p>
</div>
</section>
<section id="why-benchmarking-matters-at-scale">
<h3>Why Benchmarking Matters at Scale<a class="headerlink" href="#why-benchmarking-matters-at-scale" title="Link to this heading">#</a></h3>
<p>Production ML systems operate at scales where small performance differences compound into massive resource consumption:</p>
<ul class="simple">
<li><p><strong>Cost</strong>: A data center running 10,000 GPUs 24/7 consumes $50 million in electricity annually. Reducing latency 10% saves $5 million per year.</p></li>
<li><p><strong>User Experience</strong>: Search engines must return results in under 200ms. A 50ms latency reduction is the difference between keeping or losing users.</p></li>
<li><p><strong>Sustainability</strong>: Training GPT-3 consumed 1,287 MWh of energy, equivalent to the annual energy use of 120 US homes. Optimization reduces carbon footprint.</p></li>
</ul>
<p>Fair benchmarking ensures optimization efforts focus on changes that produce measurable, statistically significant improvements.</p>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test your understanding of benchmarking statistics and methodology with these quantitative questions.</p>
<p><strong>Q1: Statistical Significance</strong></p>
<p>You benchmark a baseline model and an optimized model 10 times each. Baseline: mean=12.5ms, std=1.2ms. Optimized: mean=11.8ms, std=1.5ms. Is the optimized model statistically significantly faster?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Calculate 95% confidence intervals:</strong></p>
<p>Baseline: CI = mean ¬± 1.96 * (std / sqrt(n)) = 12.5 ¬± 1.96 * (1.2 / sqrt(10)) = 12.5 ¬± 0.74 = [11.76, 13.24]</p>
<p>Optimized: CI = 11.8 ¬± 1.96 * (1.5 / sqrt(10)) = 11.8 ¬± 0.93 = [10.87, 12.73]</p>
<p><strong>Result</strong>: The confidence intervals OVERLAP (baseline goes as low as 11.76, optimized goes as high as 12.73). This means the difference is <strong>NOT statistically significant</strong> at the 95% confidence level. You cannot confidently claim the optimized model is faster.</p>
<p><strong>Lesson</strong>: Always compute confidence intervals. A 0.7ms difference in means might seem meaningful, but with these variances and sample sizes, it could be random noise.</p>
</div>
<p><strong>Q2: Sample Size Calculation</strong></p>
<p>You measure latency with standard deviation of 2.0ms. How many measurements do you need to achieve a 95% confidence interval width of ¬±0.5ms?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Confidence interval formula</strong>: margin = 1.96 * (std / sqrt(n))</p>
<p><strong>Solve for n</strong>: 0.5 = 1.96 * (2.0 / sqrt(n))</p>
<p>sqrt(n) = 1.96 * 2.0 / 0.5 = 7.84</p>
<p>n = 7.84¬≤ = <strong>61.5 ‚âà 62 measurements</strong></p>
<p><strong>Lesson</strong>: Achieving tight confidence intervals requires many measurements. Quadrupling precision (from ¬±1.0ms to ¬±0.5ms) requires 4x more samples (15 to 60). This is why professional benchmarks run hundreds of iterations.</p>
</div>
<p><strong>Q3: Warmup Impact</strong></p>
<p>Without warmup, your measurements are: [15.2, 12.1, 10.8, 10.5, 10.6, 10.4] ms. With 3 warmup runs discarded, your measurements are: [10.5, 10.6, 10.4, 10.7, 10.5, 10.6] ms. How much does warmup reduce measured latency and variance?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Without warmup:</strong></p>
<ul class="simple">
<li><p>Mean = (15.2 + 12.1 + 10.8 + 10.5 + 10.6 + 10.4) / 6 = <strong>11.6ms</strong></p></li>
<li><p>Std = 1.8ms (high variance due to warmup effects)</p></li>
</ul>
<p><strong>With warmup:</strong></p>
<ul class="simple">
<li><p>Mean = (10.5 + 10.6 + 10.4 + 10.7 + 10.5 + 10.6) / 6 = <strong>10.55ms</strong></p></li>
<li><p>Std = 0.1ms (low variance, stable measurements)</p></li>
</ul>
<p><strong>Impact:</strong></p>
<ul class="simple">
<li><p>Latency reduced: 11.6 - 10.55 = <strong>1.05ms (9% reduction)</strong></p></li>
<li><p>Variance reduced: 1.8 ‚Üí 0.1ms = <strong>95% reduction in noise</strong></p></li>
</ul>
<p><strong>Lesson</strong>: Warmup eliminates cold-start effects and dramatically reduces measurement variance. Without warmup, you are measuring system startup behavior, not steady-state performance.</p>
</div>
<p><strong>Q4: Pareto Frontier</strong></p>
<p>You have three models with (latency, accuracy): A=(5ms, 88%), B=(8ms, 92%), C=(6ms, 89%). Which are Pareto-optimal?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Pareto-optimal definition</strong>: A model is Pareto-optimal if no other model is better on ALL metrics simultaneously.</p>
<p><strong>Analysis:</strong></p>
<ul class="simple">
<li><p>Model A vs B: A is faster (5ms &lt; 8ms) but less accurate (88% &lt; 92%). Neither dominates. Both Pareto-optimal.</p></li>
<li><p>Model A vs C: A is faster (5ms &lt; 6ms) but less accurate (88% &lt; 89%). Neither dominates. Both Pareto-optimal.</p></li>
<li><p>Model B vs C: B is slower (8ms &gt; 6ms) but more accurate (92% &gt; 89%). Neither dominates. Both Pareto-optimal.</p></li>
</ul>
<p><strong>Result</strong>: <strong>All three models are Pareto-optimal</strong>. None is strictly dominated by another.</p>
<p><strong>Lesson</strong>: The Pareto frontier shows trade-off options. If you need minimum latency, choose A. If you need maximum accuracy, choose B. If you want balanced performance, choose C.</p>
</div>
<p><strong>Q5: Measurement Overhead</strong></p>
<p>Your timer has 1Œºs overhead per measurement. You measure a 50Œºs operation 1000 times. What percentage of measured time is overhead?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Total true operation time</strong>: 50Œºs √ó 1000 = 50,000Œºs = 50ms</p>
<p><strong>Total timer overhead</strong>: 1Œºs √ó 1000 = 1,000Œºs = 1ms</p>
<p><strong>Total measured time</strong>: 50ms + 1ms = 51ms</p>
<p><strong>Overhead percentage</strong>: (1ms / 51ms) √ó 100% = <strong>1.96%</strong></p>
<p><strong>Lesson</strong>: Timer overhead is negligible for operations longer than ~50Œºs, but becomes significant for microsecond-scale operations. This is why we use <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code> with nanosecond resolution and minimal overhead. For operations under 10Œºs, consider measuring batches and averaging.</p>
</div>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>For students who want to understand the academic and industry foundations of ML benchmarking:</p>
<section id="seminal-papers">
<h3>Seminal Papers<a class="headerlink" href="#seminal-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance</strong> - Mattson et al. (2020). The definitive industry benchmark framework that establishes fair comparison methodology across hardware vendors. Your benchmarking infrastructure follows the same statistical principles MLPerf uses for standardized evaluation. <a class="reference external" href="https://arxiv.org/abs/1910.01500">arXiv:1910.01500</a></p></li>
<li><p><strong>How to Benchmark Code Execution Times on Intel IA-32 and IA-64 Instruction Set Architectures</strong> - Paoloni (2010). White paper explaining low-level timing mechanisms, measurement overhead, and sources of timing variance. Essential reading for understanding what happens when you call <code class="docutils literal notranslate"><span class="pre">time.perf_counter()</span></code>. <a class="reference external" href="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/ia-32-ia-64-benchmark-code-execution-paper.pdf">Intel</a></p></li>
<li><p><strong>Statistical Tests for Comparing Performance</strong> - Georges et al. (2007). Academic treatment of statistical methodology for benchmarking, including hypothesis testing, sample size calculation, and handling measurement noise. <a class="reference external" href="https://doi.org/10.1145/1297027.1297033">ACM OOPSLA</a></p></li>
<li><p><strong>Benchmarking Deep Learning Inference on Mobile Devices</strong> - Ignatov et al. (2018). Comprehensive study of mobile ML benchmarking challenges including thermal throttling, battery constraints, and heterogeneous hardware. Shows how benchmarking methodology changes for resource-constrained devices. <a class="reference external" href="https://arxiv.org/abs/1812.01328">arXiv:1812.01328</a></p></li>
</ul>
</section>
<section id="additional-resources">
<h3>Additional Resources<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Industry Standard</strong>: <a class="reference external" href="https://mlcommons.org/en/inference-edge-11/">MLCommons MLPerf</a> - Browse actual benchmark results across hardware vendors to see professional benchmarking in practice</p></li>
<li><p><strong>Textbook</strong>: ‚ÄúThe Art of Computer Systems Performance Analysis‚Äù by Raj Jain - Comprehensive treatment of experimental design, statistical analysis, and benchmarking methodology for systems engineering</p></li>
</ul>
</section>
</section>
<section id="whats-next">
<h2>What‚Äôs Next<a class="headerlink" href="#whats-next" title="Link to this heading">#</a></h2>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Coming Up: Module 20 - Capstone</p>
<p>Apply everything you have learned in Modules 01-19 to compete in the TorchPerf Olympics! You will strategically combine optimization techniques, benchmark your results, and compete for the fastest, smallest, or most accurate model on the leaderboard.</p>
</div>
<p><strong>Preview - How Your Benchmarking Gets Used in the Capstone:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Competition Event</p></th>
<th class="head"><p>Metric Optimized</p></th>
<th class="head"><p>Your Benchmark In Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Latency Sprint</strong></p></td>
<td><p>Minimize inference time</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">benchmark.run_latency_benchmark()</span></code> determines winners</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Memory Challenge</strong></p></td>
<td><p>Minimize model size</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">benchmark.run_memory_benchmark()</span></code> tracks footprint</p></td>
</tr>
<tr class="row-even"><td><p><strong>Accuracy Contest</strong></p></td>
<td><p>Maximize accuracy under constraints</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">benchmark.run_accuracy_benchmark()</span></code> validates quality</p></td>
</tr>
<tr class="row-odd"><td><p><strong>All-Around</strong></p></td>
<td><p>Balanced Pareto frontier</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">suite.generate_pareto_frontier()</span></code> finds optimal trade-offs</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/19_benchmarking/19_benchmarking.ipynb">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/19_benchmarking/19_benchmarking.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="18_memoization_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 18: Memoization</p>
      </div>
    </a>
    <a class="right-next"
       href="../tiers/olympics.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Torch Olympics</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarkresult-dataclass">BenchmarkResult Dataclass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#timing-context-manager">Timing Context Manager</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmark-class">Benchmark Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarksuite-class">BenchmarkSuite Class</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking-methodology">Benchmarking Methodology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-selection">Metrics Selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-validity">Statistical Validity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reporting-results">Reporting Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#insufficient-measurement-runs">Insufficient Measurement Runs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skipping-warmup">Skipping Warmup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-different-input-shapes">Comparing Different Input Shapes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ignoring-variance">Ignoring Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-industry-benchmarks">Your Implementation vs. Industry Benchmarks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-benchmarking-matters-at-scale">Why Benchmarking Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>