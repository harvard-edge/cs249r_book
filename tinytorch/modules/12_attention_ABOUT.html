
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 12: Attention" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/12_attention_ABOUT.html" />
<meta property="og:site_name" content="Tinyüî•Torch" />
<meta property="og:description" content="üöÄ Launch Binder Run interactively in your browser. No setup required. https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F12_attention%2F12_attention.ipynb üìÑ View..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="Tinyüî•Torch" />
<meta name="description" content="üöÄ Launch Binder Run interactively in your browser. No setup required. https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F12_attention%2F12_attention.ipynb üìÑ View..." />

    <title>Module 12: Attention &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1a2c4ab3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";

const defaultStyle = document.createElement('style');
defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}
`;
document.head.appendChild(defaultStyle);

const fullscreenStyle = document.createElement('style');
fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
document.head.appendChild(fullscreenStyle);

// Detect if page has dark background
const isDarkTheme = () => {
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        return brightness < 128;
    }
    return false;
};

const load = async () => {
    await mermaid.run();

    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(load, 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(load, 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(load, 200);
        return;
    }

    const darkTheme = isDarkTheme();

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    const modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    const modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    let previousScrollOffset = [window.scrollX, window.scrollY];

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    const allButtons = [];

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });

        container.appendChild(fullscreenBtn);
        allButtons.push(fullscreenBtn);
    });

    // Update theme classes when theme changes
    const updateTheme = () => {
        const dark = isDarkTheme();
        allButtons.forEach(btn => {
            if (dark) {
                btn.classList.add('dark-theme');
            } else {
                btn.classList.remove('dark-theme');
            }
        });
        if (dark) {
            modal.classList.add('dark-theme');
            modalContent.classList.add('dark-theme');
            closeBtn.classList.add('dark-theme');
        } else {
            modal.classList.remove('dark-theme');
            modalContent.classList.remove('dark-theme');
            closeBtn.classList.remove('dark-theme');
        }
    };

    // Watch for theme changes
    const observer = new MutationObserver(updateTheme);
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    observer.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style']
    });
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/12_attention_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=b021a6d4"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=ee94e95f"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/subscribe-modal.js?v=82641629"></script>
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Module 13: Transformers" href="13_transformers_ABOUT.html" />
    <link rel="prev" title="Module 11: Embeddings" href="11_embeddings_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="/tinytorch/_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="/tinytorch/_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://mlsysbook.ai" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mlsysbook" title="Support" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-heart fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Support</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-07)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_autograd_ABOUT.html">05. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_optimizers_ABOUT.html">06. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_training_ABOUT.html">07. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (08-13)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_dataloader_ABOUT.html">08. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_memoization_ABOUT.html">17. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_acceleration_ABOUT.html">18. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/12_attention_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 12: Attention</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention-function">Scaled Dot-Product Attention Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiheadattention-class">MultiHeadAttention Class</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-key-value-the-information-retrieval-paradigm">Query, Key, Value: The Information Retrieval Paradigm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention-similarity-as-relevance">Scaled Dot-Product Attention: Similarity as Relevance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-weights-and-softmax-normalization">Attention Weights and Softmax Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-parallel-relationship-learning">Multi-Head Attention: Parallel Relationship Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#causal-masking-preventing-information-leakage">Causal Masking: Preventing Information Leakage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity-the-o-n2-reality">Computational Complexity: The O(n¬≤) Reality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatch-in-attention">Shape Mismatch in Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-weights-dont-sum-to-1">Attention Weights Don‚Äôt Sum to 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-dimension-mismatch">Multi-Head Dimension Mismatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mask-broadcasting-errors">Mask Broadcasting Errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-flow-issues">Gradient Flow Issues</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-attention-matters-at-scale">Why Attention Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-12-attention">
<h1>Module 12: Attention<a class="headerlink" href="#module-12-attention" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>ARCHITECTURE TIER</strong> | Difficulty: ‚óè‚óè‚óè‚óã | Time: 5-7 hours | Prerequisites: 01-07, 10-11</p>
<p><strong>Prerequisites: Modules 01-07 and 10-11</strong> means you should understand:</p>
<ul class="simple">
<li><p>Tensor operations and shape manipulation (Module 01)</p></li>
<li><p>Activations, particularly softmax (Module 02)</p></li>
<li><p>Linear layers and weight projections (Module 03)</p></li>
<li><p>Autograd for gradient computation (Module 05)</p></li>
<li><p>Tokenization and embeddings (Modules 10-11)</p></li>
</ul>
<p>If you can explain why <code class="docutils literal notranslate"><span class="pre">softmax(x).sum(axis=-1)</span></code> equals 1.0 and how embeddings convert token IDs to dense vectors, you‚Äôre ready.</p>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run interactively in your browser. No setup required.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F12_attention%2F12_attention.ipynb"><span>https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F12_attention%2F12_attention.ipynb</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìÑ View Source</div>
<p class="sd-card-text">Browse the implementation code on GitHub.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/12_attention/12_attention.py"><span>https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/12_attention/12_attention.py</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üéß Audio Overview</div>
<p class="sd-card-text">Listen to an AI-generated overview.</p>
<audio controls style="width: 100%; margin-top: 8px;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/12_attention.mp3" type="audio/mpeg">
</audio>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>The attention mechanism revolutionized deep learning by solving a fundamental problem: how can models focus on relevant information when processing sequences? Before attention, models like RNNs compressed entire sequences into fixed-size hidden states, creating an information bottleneck. Attention changes this by allowing every position in a sequence to directly access information from every other position, weighted by relevance.</p>
<p>You‚Äôll build scaled dot-product attention and multi-head attention from scratch, the exact mechanisms powering GPT, BERT, and modern transformers. By implementing the core formula <code class="docutils literal notranslate"><span class="pre">Attention(Q,</span> <span class="pre">K,</span> <span class="pre">V)</span> <span class="pre">=</span> <span class="pre">softmax(QK^T</span> <span class="pre">/</span> <span class="pre">‚àöd_k)</span> <span class="pre">V</span></code> with vectorized matrix operations, you‚Äôll witness the O(n¬≤) memory complexity that makes attention both powerful and challenging at scale. This hands-on implementation reveals why research into efficient attention variants like FlashAttention is crucial for production systems.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> scaled dot-product attention with vectorized operations that reveal O(n¬≤) memory complexity</p></li>
<li><p><strong>Build</strong> multi-head attention for parallel processing of different relationship types across representation subspaces</p></li>
<li><p><strong>Master</strong> attention weight computation, normalization, and the query-key-value paradigm</p></li>
<li><p><strong>Understand</strong> quadratic memory scaling and why attention becomes the bottleneck in long-context transformers</p></li>
<li><p><strong>Connect</strong> your implementation to production frameworks and understand why efficient attention research matters at scale</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Your Attention System&quot;
        A[&quot;Query (Q)&lt;br/&gt;What to look for&quot;]
        B[&quot;Key (K)&lt;br/&gt;What's available&quot;]
        C[&quot;Value (V)&lt;br/&gt;What to retrieve&quot;]
        D[&quot;Similarity Scores&lt;br/&gt;QK^T / ‚àöd_k&quot;]
        E[&quot;Attention Weights&lt;br/&gt;softmax(scores)&quot;]
        F[&quot;Weighted Output&lt;br/&gt;weights &#64; V&quot;]
    end

    A --&gt; D
    B --&gt; D
    D --&gt; E
    E --&gt; F
    C --&gt; F

    style A fill:#e1f5ff
    style B fill:#e1f5ff
    style C fill:#e1f5ff
    style D fill:#fff3cd
    style E fill:#f8d7da
    style F fill:#d4edda
    </pre><figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Your Attention System</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Part</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention()</span></code></p></td>
<td><p>Core attention mechanism with QK^T similarity</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Attention weight normalization</p></td>
<td><p>Softmax converts scores to probability distribution</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>Causal masking support</p></td>
<td><p>Preventing attention to future positions</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MultiHeadAttention.__init__()</span></code></p></td>
<td><p>Linear projections and head configuration</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MultiHeadAttention.forward()</span></code></p></td>
<td><p>Split, attend, concatenate pattern</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Multi-head attention for sequence processing</span>
<span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>  <span class="c1"># Learn different relationship types in parallel</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>Full transformer blocks (that‚Äôs Module 13: Transformers)</p></li>
<li><p>Positional encoding (you built this in Module 11: Embeddings)</p></li>
<li><p>Efficient attention variants like FlashAttention (production optimization beyond scope)</p></li>
<li><p>Cross-attention for encoder-decoder models (PyTorch does this with separate Q vs K/V inputs)</p></li>
</ul>
<p><strong>You are building the core attention mechanism.</strong> Complete transformer architectures come next.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section provides a quick reference for the attention functions and classes you‚Äôll build. Use this as your implementation guide and debugging reference.</p>
<section id="scaled-dot-product-attention-function">
<h3>Scaled Dot-Product Attention Function<a class="headerlink" href="#scaled-dot-product-attention-function" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span><span class="p">)</span>
</pre></div>
</div>
<p>Computes the fundamental attention operation that powers all transformers.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Q</span></code>: Query tensor <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">d_model)</span></code> - what each position is looking for</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">K</span></code>: Key tensor <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">d_model)</span></code> - what‚Äôs available at each position</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">V</span></code>: Value tensor <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">d_model)</span></code> - actual content to retrieve</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mask</span></code>: Optional <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">seq_len)</span></code> - 1.0 for allowed positions, 0.0 for masked</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">output</span></code>: Attended values <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">d_model)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attention_weights</span></code>: Attention matrix <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">seq_len,</span> <span class="pre">seq_len)</span></code> showing focus patterns</p></li>
</ul>
</section>
<section id="multiheadattention-class">
<h3>MultiHeadAttention Class<a class="headerlink" href="#multiheadattention-class" title="Link to this heading">#</a></h3>
<p>Multi-head attention runs multiple attention mechanisms in parallel, each learning to focus on different types of relationships.</p>
<p><strong>Constructor:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiHeadAttention</span>
</pre></div>
</div>
<p>Creates multi-head attention with <code class="docutils literal notranslate"><span class="pre">embed_dim</span> <span class="pre">//</span> <span class="pre">num_heads</span></code> dimensions per head.</p>
<p><strong>Core Methods:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(x,</span> <span class="pre">mask=None)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Apply multi-head attention to input</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">parameters()</span> <span class="pre">-&gt;</span> <span class="pre">List[Tensor]</span></code></p></td>
<td><p>Return all trainable parameters</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Attributes:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span></code>: Total embedding dimension</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_heads</span></code>: Number of parallel attention heads</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">head_dim</span></code>: Dimension per head (embed_dim // num_heads)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">q_proj</span></code>, <code class="docutils literal notranslate"><span class="pre">k_proj</span></code>, <code class="docutils literal notranslate"><span class="pre">v_proj</span></code>: Linear projections for queries, keys, values</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_proj</span></code>: Output linear layer to mix information across heads</p></li>
</ul>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental ideas you need to understand attention deeply. These concepts apply to every transformer-based model in production today.</p>
<section id="query-key-value-the-information-retrieval-paradigm">
<h3>Query, Key, Value: The Information Retrieval Paradigm<a class="headerlink" href="#query-key-value-the-information-retrieval-paradigm" title="Link to this heading">#</a></h3>
<p>Attention models sequence processing as an information retrieval problem. Think of it like searching a database: you have a query describing what you need, keys that describe what each entry contains, and values representing the actual data. When you search ‚Äúmachine learning papers,‚Äù the search engine compares your query against document descriptions (keys) to determine relevance, then returns the actual documents (values) weighted by how well they match.</p>
<p>The same pattern applies to transformers. For each position in a sequence, the query asks ‚Äúwhat information do I need?‚Äù, keys describe ‚Äúwhat information do I have?‚Äù, and values contain the actual representations to retrieve. The beauty is that queries, keys, and values are all learned projections of the same input embeddings, allowing the model to discover what aspects of meaning to search for and retrieve.</p>
<p>Here‚Äôs how your implementation creates these three components through linear projections:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># From MultiHeadAttention.__init__</span>
<span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>  <span class="c1"># Learn what to search for</span>
<span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>  <span class="c1"># Learn what to index by</span>
<span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>  <span class="c1"># Learn what to retrieve</span>

<span class="c1"># From MultiHeadAttention.forward</span>
<span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Transform input to queries</span>
<span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Transform input to keys</span>
<span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Transform input to values</span>
</pre></div>
</div>
<p>Each linear projection learns a different perspective on the input. During training, the model discovers that queries might emphasize semantic meaning, keys might emphasize syntactic roles, and values might emphasize contextual information, all optimized end-to-end for the task.</p>
</section>
<section id="scaled-dot-product-attention-similarity-as-relevance">
<h3>Scaled Dot-Product Attention: Similarity as Relevance<a class="headerlink" href="#scaled-dot-product-attention-similarity-as-relevance" title="Link to this heading">#</a></h3>
<p>The core attention computation answers a simple question: how similar is each query to each key? The dot product between vectors naturally measures similarity, where higher values indicate more aligned directions in embedding space. For a query at position i and key at position j, the dot product <code class="docutils literal notranslate"><span class="pre">Q[i]</span> <span class="pre">¬∑</span> <span class="pre">K[j]</span></code> quantifies their relevance.</p>
<p>But raw dot products grow with embedding dimension, creating numerical instability in softmax. With 512-dimensional embeddings, dot products can reach hundreds, causing softmax to saturate (output probabilities near 0 or 1 with tiny gradients). Scaling by <code class="docutils literal notranslate"><span class="pre">1/‚àöd_k</span></code> normalizes the variance, keeping values in a stable range regardless of embedding dimension.</p>
<p>Your implementation computes this using vectorized matrix operations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># From scaled_dot_product_attention (lines 303-319)</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Compute all query-key similarities at once using matmul</span>
<span class="c1"># This is mathematically equivalent to nested loops computing Q[i] ¬∑ K[j]</span>
<span class="c1"># for all i,j pairs, but vectorized for efficiency</span>
<span class="n">K_t</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Transpose to align dimensions</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">K_t</span><span class="p">)</span>     <span class="c1"># (batch, seq_len, seq_len) - the O(n¬≤) matrix</span>

<span class="c1"># Scale by 1/‚àöd_k for numerical stability</span>
<span class="n">scale_factor</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">*</span> <span class="n">scale_factor</span>
</pre></div>
</div>
<p>The resulting <code class="docutils literal notranslate"><span class="pre">scores</span></code> tensor is the attention matrix before normalization. Element <code class="docutils literal notranslate"><span class="pre">[i,j]</span></code> represents how much position i should attend to position j. The vectorized <code class="docutils literal notranslate"><span class="pre">matmul</span></code> operation computes all n¬≤ query-key pairs simultaneously‚Äîwhile much faster than Python loops, it still creates the full O(n¬≤) attention matrix that dominates memory usage at scale.</p>
</section>
<section id="attention-weights-and-softmax-normalization">
<h3>Attention Weights and Softmax Normalization<a class="headerlink" href="#attention-weights-and-softmax-normalization" title="Link to this heading">#</a></h3>
<p>Raw similarity scores need to become a probability distribution. Softmax transforms scores into positive values that sum to 1.0 along each row, creating a proper weighted average. This ensures that for each query position, the attention weights over all key positions form valid mixing coefficients.</p>
<p>The softmax operation <code class="docutils literal notranslate"><span class="pre">exp(scores[i,j])</span> <span class="pre">/</span> <span class="pre">Œ£_k</span> <span class="pre">exp(scores[i,k])</span></code> has important properties. It‚Äôs differentiable, allowing gradients to flow during training. It amplifies differences: a score of 2.0 becomes much more prominent than 1.0 after exponentiation. And it‚Äôs translation-invariant: adding the same constant to all scores doesn‚Äôt change the output (exploited for numerical stability).</p>
<p>Here‚Äôs the complete attention weight computation with masking support:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># From scaled_dot_product_attention</span>

<span class="c1"># Apply causal mask if provided (set masked positions to large negative)</span>
<span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">mask_data</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">data</span>
    <span class="n">adder_mask</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mask_data</span><span class="p">)</span> <span class="o">*</span> <span class="n">MASK_VALUE</span>  <span class="c1"># MASK_VALUE = -1e9</span>
    <span class="n">adder_mask_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">adder_mask</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">adder_mask_tensor</span>

<span class="c1"># Softmax converts scores to probability distribution</span>
<span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">()</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Normalize along last dimension</span>

<span class="c1"># Apply to values: weighted combination</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
</pre></div>
</div>
<p>The mask addition is clever: for positions where <code class="docutils literal notranslate"><span class="pre">mask=0</span></code> (masked), we add -1e9 to the score. After softmax, <code class="docutils literal notranslate"><span class="pre">exp(-1e9)</span></code> is effectively zero, so masked positions get zero attention weight. For positions where <code class="docutils literal notranslate"><span class="pre">mask=1</span></code> (allowed), adding zero leaves scores unchanged. This preserves differentiability while enforcing hard constraints.</p>
</section>
<section id="multi-head-attention-parallel-relationship-learning">
<h3>Multi-Head Attention: Parallel Relationship Learning<a class="headerlink" href="#multi-head-attention-parallel-relationship-learning" title="Link to this heading">#</a></h3>
<p>Single-head attention learns one similarity function between queries and keys. But sequences have multiple types of relationships: syntactic dependencies, semantic similarity, positional patterns, long-range coreference. Multi-head attention addresses this by running multiple attention mechanisms in parallel, each with different learned projections.</p>
<p>The key insight is splitting the embedding dimension across heads rather than duplicating it. For <code class="docutils literal notranslate"><span class="pre">embed_dim=512</span></code> and <code class="docutils literal notranslate"><span class="pre">num_heads=8</span></code>, each head operates on <code class="docutils literal notranslate"><span class="pre">512/8=64</span></code> dimensions. This keeps parameter count constant while allowing diverse specialization. One head might learn to focus on adjacent tokens (local syntax), another on semantically similar words (meaning), another on specific positional offsets (structured patterns).</p>
<p>Your implementation handles this through reshape and transpose operations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># From MultiHeadAttention.forward</span>

<span class="c1"># Project to Q, K, V (each is batch, seq, embed_dim)</span>
<span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Reshape to separate heads: (batch, seq, num_heads, head_dim)</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

<span class="c1"># Transpose to (batch, num_heads, seq, head_dim) for parallel processing</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Apply attention to all heads at once</span>
<span class="n">attended</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask_reshaped</span><span class="p">)</span>

<span class="c1"># Transpose back and concatenate heads</span>
<span class="n">attended</span> <span class="o">=</span> <span class="n">attended</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch, seq, num_heads, head_dim)</span>
<span class="n">concat_output</span> <span class="o">=</span> <span class="n">attended</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

<span class="c1"># Mix information across heads with output projection</span>
<span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">concat_output</span><span class="p">)</span>
</pre></div>
</div>
<p>The reshape-transpose-attend-transpose-reshape dance separates heads for independent processing, then recombines their outputs. The final output projection learns how to mix information discovered by different heads, creating a rich representation that captures multiple relationship types simultaneously.</p>
</section>
<section id="causal-masking-preventing-information-leakage">
<h3>Causal Masking: Preventing Information Leakage<a class="headerlink" href="#causal-masking-preventing-information-leakage" title="Link to this heading">#</a></h3>
<p>In language modeling, predicting the next token requires a strict causality constraint: position i can only attend to positions 0 through i, never to future positions. Without this, the model could ‚Äúcheat‚Äù by looking at the answer during training. Causal masking enforces this by zeroing attention weights for all positions j &gt; i.</p>
<p>The implementation uses a lower triangular mask: ones below and on the diagonal, zeros above. For a sequence length of 4, the mask looks like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>   <span class="c1"># Position 0 can only see itself</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>   <span class="c1"># Position 1 sees 0 and 1</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>   <span class="c1"># Position 2 sees 0, 1, 2</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>   <span class="c1"># Position 3 sees all positions</span>
</pre></div>
</div>
<p>When combined with the masking logic in attention (adding -1e9 to masked scores before softmax), this creates a structured sparsity pattern: exactly half the attention matrix becomes zero. This is crucial for autoregressive models like GPT, where generation must proceed left-to-right without access to future tokens.</p>
</section>
<section id="computational-complexity-the-o-n2-reality">
<h3>Computational Complexity: The O(n¬≤) Reality<a class="headerlink" href="#computational-complexity-the-o-n2-reality" title="Link to this heading">#</a></h3>
<p>Attention‚Äôs power comes from all-to-all connectivity: every position can attend to every other position. But this creates quadratic scaling in both computation and memory. For sequence length n, the attention matrix has n¬≤ elements. The vectorized <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K^T</span></code> operation computes all n¬≤ similarity scores in one matrix multiplication, softmax normalizes n¬≤ values, and applying attention to values multiplies n¬≤ weights by the value vectors.</p>
<p>The memory cost is particularly severe. For GPT-3 with 2048-token context, a single attention matrix stores 2048¬≤ = 4,194,304 float32 values, requiring 16 MB. With 96 layers, attention matrices alone need 1.5 GB, excluding activations, gradients, and other tensors. This quadratic wall is why long-context AI remains an active research challenge.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Time Complexity</p></th>
<th class="head"><p>Memory Complexity</p></th>
<th class="head"><p>Dominates When</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>QK^T</p></td>
<td><p>O(n¬≤ √ó d)</p></td>
<td><p>O(n¬≤)</p></td>
<td><p>Long sequences</p></td>
</tr>
<tr class="row-odd"><td><p>Softmax</p></td>
<td><p>O(n¬≤)</p></td>
<td><p>O(n¬≤)</p></td>
<td><p>Always stores full matrix</p></td>
</tr>
<tr class="row-even"><td><p>Weights &#64; V</p></td>
<td><p>O(n¬≤ √ó d)</p></td>
<td><p>O(n √ó d)</p></td>
<td><p>Output reuses attention weights</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Total</strong></p></td>
<td><p><strong>O(n¬≤ √ó d)</strong></p></td>
<td><p><strong>O(n¬≤)</strong></p></td>
<td><p>n &gt; d (long sequences)</p></td>
</tr>
</tbody>
</table>
</div>
<p>For comparison, feed-forward networks in transformers have O(n √ó d¬≤) complexity. When sequence length n exceeds embedding dimension d (common in modern models), attention‚Äôs O(n¬≤) term dominates, making it the primary bottleneck. This explains why research into efficient attention variants like sparse attention, linear attention, and FlashAttention is crucial for production systems.</p>
</section>
</section>
<section id="common-errors">
<h2>Common Errors<a class="headerlink" href="#common-errors" title="Link to this heading">#</a></h2>
<p>These are the errors you‚Äôll encounter most often when implementing attention. Understanding them will save hours of debugging.</p>
<section id="shape-mismatch-in-attention">
<h3>Shape Mismatch in Attention<a class="headerlink" href="#shape-mismatch-in-attention" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">Cannot</span> <span class="pre">perform</span> <span class="pre">matrix</span> <span class="pre">multiplication:</span> <span class="pre">(2,</span> <span class="pre">4,</span> <span class="pre">64)</span> <span class="pre">&#64;</span> <span class="pre">(2,</span> <span class="pre">4,</span> <span class="pre">64).</span> <span class="pre">Inner</span> <span class="pre">dimensions</span> <span class="pre">must</span> <span class="pre">match</span></code></p>
<p>When computing <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K^T</span></code>, the key tensor needs transposing. The matrix multiplication <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K</span></code> has shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq_len,</span> <span class="pre">d_model)</span> <span class="pre">&#64;</span> <span class="pre">(batch,</span> <span class="pre">seq_len,</span> <span class="pre">d_model)</span></code>, which fails because the inner dimensions are both <code class="docutils literal notranslate"><span class="pre">d_model</span></code>. You need <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">&#64;</span> <span class="pre">K.transpose()</span></code> to get <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq_len,</span> <span class="pre">d_model)</span> <span class="pre">&#64;</span> <span class="pre">(batch,</span> <span class="pre">d_model,</span> <span class="pre">seq_len)</span></code>, producing the correct <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq_len,</span> <span class="pre">seq_len)</span></code> attention matrix.</p>
<p><strong>Fix</strong>: Always transpose K before the matmul: <code class="docutils literal notranslate"><span class="pre">scores</span> <span class="pre">=</span> <span class="pre">Q.matmul(K.transpose(-2,</span> <span class="pre">-1))</span></code></p>
</section>
<section id="attention-weights-dont-sum-to-1">
<h3>Attention Weights Don‚Äôt Sum to 1<a class="headerlink" href="#attention-weights-dont-sum-to-1" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">AssertionError:</span> <span class="pre">Attention</span> <span class="pre">weights</span> <span class="pre">don't</span> <span class="pre">sum</span> <span class="pre">to</span> <span class="pre">1</span></code></p>
<p>This happens when softmax is applied to the wrong axis. Attention weights must form a probability distribution over key positions for each query position. If you apply softmax along the wrong dimension, you‚Äôll get values that don‚Äôt sum to 1.0 per row.</p>
<p><strong>Fix</strong>: Use <code class="docutils literal notranslate"><span class="pre">softmax(scores,</span> <span class="pre">dim=-1)</span></code> to normalize along the last dimension (across keys for each query)</p>
</section>
<section id="multi-head-dimension-mismatch">
<h3>Multi-Head Dimension Mismatch<a class="headerlink" href="#multi-head-dimension-mismatch" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">embed_dim</span> <span class="pre">(512)</span> <span class="pre">must</span> <span class="pre">be</span> <span class="pre">divisible</span> <span class="pre">by</span> <span class="pre">num_heads</span> <span class="pre">(7)</span></code></p>
<p>Multi-head attention splits the embedding dimension across heads. If <code class="docutils literal notranslate"><span class="pre">embed_dim=512</span></code> and <code class="docutils literal notranslate"><span class="pre">num_heads=7</span></code>, you‚Äôd get <code class="docutils literal notranslate"><span class="pre">512/7=73.14</span></code> dimensions per head, which doesn‚Äôt work with integer tensor shapes. The architecture requires exact divisibility.</p>
<p><strong>Fix</strong>: Choose num_heads that evenly divides embed_dim. Common pairs: (512, 8), (768, 12), (1024, 16)</p>
</section>
<section id="mask-broadcasting-errors">
<h3>Mask Broadcasting Errors<a class="headerlink" href="#mask-broadcasting-errors" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">operands</span> <span class="pre">could</span> <span class="pre">not</span> <span class="pre">be</span> <span class="pre">broadcast</span> <span class="pre">together</span> <span class="pre">with</span> <span class="pre">shapes</span> <span class="pre">(2,1,4,4)</span> <span class="pre">(2,4,4)</span></code></p>
<p>Multi-head attention expects masks with a head dimension. If you pass a 3D mask <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq,</span> <span class="pre">seq)</span></code> but the implementation expects 4D <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">heads,</span> <span class="pre">seq,</span> <span class="pre">seq)</span></code>, broadcasting fails. The mask needs reshaping to add a dimension that broadcasts across all heads.</p>
<p><strong>Fix</strong>: Reshape mask: <code class="docutils literal notranslate"><span class="pre">mask.reshape(batch,</span> <span class="pre">1,</span> <span class="pre">seq_len,</span> <span class="pre">seq_len)</span></code> to broadcast over heads</p>
</section>
<section id="gradient-flow-issues">
<h3>Gradient Flow Issues<a class="headerlink" href="#gradient-flow-issues" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Loss doesn‚Äôt decrease during training despite correct forward pass</p>
<p>This can happen if you create new Tensor objects incorrectly, breaking the autograd graph. When applying masks or performing intermediate computations, ensure tensors maintain <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> appropriately.</p>
<p><strong>Fix</strong>: Check that operations preserve gradient flow: <code class="docutils literal notranslate"><span class="pre">Tensor(result,</span> <span class="pre">requires_grad=True)</span></code> when needed</p>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-pytorch">
<h3>Your Implementation vs. PyTorch<a class="headerlink" href="#your-implementation-vs-pytorch" title="Link to this heading">#</a></h3>
<p>Your TinyTorch attention and PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> implement the same mathematical operations. The differences are in implementation efficiency, features, and flexibility. PyTorch uses highly optimized C++ kernels, supports additional attention variants, and integrates with production training systems.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Core Algorithm</strong></p></td>
<td><p>Scaled dot-product attention</p></td>
<td><p>Same mathematical operation</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Multi-Head</strong></p></td>
<td><p>Split-attend-concat pattern</p></td>
<td><p>Identical architecture</p></td>
</tr>
<tr class="row-even"><td><p><strong>Backend</strong></p></td>
<td><p>NumPy (Python loops)</p></td>
<td><p>C++ CUDA kernels</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Speed</strong></p></td>
<td><p>1x (baseline)</p></td>
<td><p>50-100x faster on GPU</p></td>
</tr>
<tr class="row-even"><td><p><strong>Memory Optimization</strong></p></td>
<td><p>Stores full attention matrix</p></td>
<td><p>Optional FlashAttention integration</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Batch First</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq,</span> <span class="pre">embed)</span></code></p></td>
<td><p>Configurable via <code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code></p></td>
</tr>
<tr class="row-even"><td><p><strong>Cross-Attention</strong></p></td>
<td><p>Self-attention only</p></td>
<td><p>Separate Q vs K/V inputs supported</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Key Padding Mask</strong></p></td>
<td><p>Manual mask creation</p></td>
<td><p>Built-in mask utilities</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows equivalent attention operations in TinyTorch and PyTorch. Notice how the high-level API and shape conventions match almost exactly.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.attention</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultiHeadAttention</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Create multi-head attention</span>
<span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># Input embeddings (batch=2, seq=10, dim=512)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>

<span class="c1"># Apply attention</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">mha</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (2, 10, 512)</span>

<span class="c1"># With causal masking</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))))</span>
<span class="n">output_masked</span> <span class="o">=</span> <span class="n">mha</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
‚ö° PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Create multi-head attention</span>
<span class="n">mha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                             <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Input embeddings (batch=2, seq=10, dim=512)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>

<span class="c1"># Apply attention (PyTorch returns output + weights)</span>
<span class="n">output</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># Self-attention: Q=K=V=x</span>

<span class="c1"># With causal masking (upper triangle = -inf)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output_masked</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs walk through the key differences:</p>
<ul class="simple">
<li><p><strong>Line 1-2 (Imports)</strong>: TinyTorch separates attention into its own module; PyTorch includes it in <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>. Both follow modular design patterns.</p></li>
<li><p><strong>Line 4-5 (Construction)</strong>: API is nearly identical. PyTorch adds <code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code> for compatibility with older code that expected <code class="docutils literal notranslate"><span class="pre">(seq,</span> <span class="pre">batch,</span> <span class="pre">embed)</span></code> order.</p></li>
<li><p><strong>Line 8 (Input)</strong>: Shape conventions match exactly: <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">seq,</span> <span class="pre">embed)</span></code>. This is the modern standard.</p></li>
<li><p><strong>Line 11 (Forward Pass)</strong>: TinyTorch uses <code class="docutils literal notranslate"><span class="pre">mha.forward(x)</span></code> with x as both Q, K, V (self-attention). PyTorch makes this explicit with <code class="docutils literal notranslate"><span class="pre">mha(x,</span> <span class="pre">x,</span> <span class="pre">x)</span></code>, allowing cross-attention where Q differs from K/V.</p></li>
<li><p><strong>Line 14-15 (Masking)</strong>: TinyTorch uses 0/1 masks (0=masked). PyTorch uses additive masks (-inf=masked). Both work, but PyTorch‚Äôs convention integrates better with certain optimizations.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>The mathematical operations, architectural patterns, and shape conventions are identical. Multi-head attention works the same way in production. Understanding your implementation means understanding PyTorch‚Äôs attention.</p>
</div>
</section>
<section id="why-attention-matters-at-scale">
<h3>Why Attention Matters at Scale<a class="headerlink" href="#why-attention-matters-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate why attention research is crucial, consider the scaling characteristics of modern language models:</p>
<ul class="simple">
<li><p><strong>GPT-3</strong> (96 layers, 2048 context): ~1.5 GB just for attention matrices during forward pass, ~6 GB with gradients during training</p></li>
<li><p><strong>GPT-4</strong> (estimated 120 layers, 32K context): Would require ~480 GB for attention alone without optimization, exceeding single-GPU memory</p></li>
<li><p><strong>Long-context models</strong> (100K+ tokens): Attention becomes computationally prohibitive without algorithmic improvements</p></li>
</ul>
<p>These constraints drive modern attention research:</p>
<ul class="simple">
<li><p><strong>FlashAttention</strong>: Reformulates computation to reduce memory from O(n¬≤) to O(n) without approximation, enabling 8x longer contexts</p></li>
<li><p><strong>Sparse Attention</strong>: Only compute attention for specific patterns (local windows, strided access), reducing complexity to O(n log n) or O(n‚àön)</p></li>
<li><p><strong>Linear Attention</strong>: Approximate attention with linear complexity O(n), trading accuracy for scale</p></li>
<li><p><strong>State Space Models</strong>: Alternative architectures (Mamba, RWKV) that avoid attention‚Äôs quadratic cost entirely</p></li>
</ul>
<p>The attention mechanism you built is mathematically identical to production systems, but the O(n¬≤) wall explains why so much research focuses on making it tractable at scale.</p>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions. They‚Äôre designed to build intuition for the performance characteristics you‚Äôll encounter in production ML.</p>
<p><strong>Q1: Memory Calculation</strong></p>
<p>For sequence length 1024, how much memory does a single attention matrix require (float32)? What about sequence length 2048?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Sequence length 1024:</strong></p>
<ul class="simple">
<li><p>Attention matrix: 1024 √ó 1024 = 1,048,576 elements</p></li>
<li><p>Memory: 1,048,576 √ó 4 bytes = <strong>4.2 MB</strong></p></li>
</ul>
<p><strong>Sequence length 2048:</strong></p>
<ul class="simple">
<li><p>Attention matrix: 2048 √ó 2048 = 4,194,304 elements</p></li>
<li><p>Memory: 4,194,304 √ó 4 bytes = <strong>16.8 MB</strong></p></li>
</ul>
<p><strong>Scaling factor:</strong> Doubling sequence length quadruples memory (2¬≤ = 4√ó)</p>
<p>For GPT-3 (96 layers, 2048 context):</p>
<ul class="simple">
<li><p>96 layers √ó 16.8 MB = <strong>1.6 GB</strong> just for attention matrices!</p></li>
<li><p>This excludes Q/K/V projections, gradients, and all other tensors.</p></li>
</ul>
</div>
<p><strong>Q2: Attention Bottleneck</strong></p>
<p>A transformer layer has attention (O(n¬≤ √ó d)) and feed-forward network (O(n √ó d¬≤)). For embed_dim=512, at what sequence length does attention dominate?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Complexity comparison:</strong></p>
<ul class="simple">
<li><p>Attention: O(n¬≤ √ó d) = O(n¬≤ √ó 512)</p></li>
<li><p>FFN: O(n √ó d¬≤) = O(n √ó 512¬≤) = O(n √ó 262,144)</p></li>
</ul>
<p><strong>Crossover point:</strong> n¬≤ √ó 512 &gt; n √ó 262,144</p>
<ul class="simple">
<li><p>Simplify: n &gt; 262,144 / 512 = <strong>512</strong></p></li>
</ul>
<p><strong>When n &gt; 512</strong>, attention becomes the memory bottleneck.</p>
<p><strong>Real-world implications:</strong></p>
<ul class="simple">
<li><p>Short sequences (n=128): FFN dominates, 262K vs 8K operations</p></li>
<li><p>Medium sequences (n=512): Break-even point</p></li>
<li><p>Long sequences (n=2048): Attention dominates, 2M vs 262K operations</p></li>
<li><p><strong>This is why GPT-3 (2048 context) needed attention optimization!</strong></p></li>
</ul>
</div>
<p><strong>Q3: Multi-Head Efficiency</strong></p>
<p>Why use 8 heads of 64 dimensions instead of 1 head of 512 dimensions? Parameters are the same‚Äîwhat‚Äôs the systems difference?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Parameter count (both are identical):</strong></p>
<ul class="simple">
<li><p>8 heads √ó 64 dims: Linear(512‚Üí512) for Q, K, V, Out = 4 √ó (512√ó512 + 512) weights+biases</p></li>
<li><p>1 head √ó 512 dims: Same projection parameters</p></li>
</ul>
<p><strong>Key differences:</strong></p>
<p><strong>1. Parallelization:</strong></p>
<ul class="simple">
<li><p>8 heads can process in parallel on modern GPUs (separate CUDA streams)</p></li>
<li><p>Each head‚Äôs smaller matmul operations utilize GPU cores more efficiently</p></li>
</ul>
<p><strong>2. Representation diversity:</strong></p>
<ul class="simple">
<li><p>8 heads learn 8 different similarity functions (syntax, semantics, position, etc.)</p></li>
<li><p>1 head learns a single monolithic similarity function</p></li>
<li><p>Training discovers specialization automatically</p></li>
</ul>
<p><strong>3. Cache efficiency:</strong></p>
<ul class="simple">
<li><p>Smaller head_dim (64) fits better in GPU cache/shared memory</p></li>
<li><p>Larger single head (512) causes more cache misses</p></li>
</ul>
<p><strong>4. Gradient flow:</strong></p>
<ul class="simple">
<li><p>Multiple heads provide diverse gradient signals during backpropagation</p></li>
<li><p>Single head has one gradient path, slower learning</p></li>
</ul>
<p><strong>Empirical result:</strong> 8 heads consistently outperform 1 head despite same parameter count. Diversity matters!</p>
</div>
<p><strong>Q4: Causal Masking Computation</strong></p>
<p>Causal masking zeros out the upper triangle (roughly half the attention matrix). Do we save computation, or just ensure correctness?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>In your implementation: NO computation saved</strong></p>
<p>Your code computes the full attention matrix, then adds -1e9 to masked positions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">K_t</span><span class="p">)</span>  <span class="c1"># Full n¬≤ computation</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">adder_mask_tensor</span>  <span class="c1"># Masking happens after</span>
</pre></div>
</div>
</div>
<p><strong>Why no savings:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Q.matmul(K_t)</span></code> computes all n¬≤ scores</p></li>
<li><p>Masking only affects softmax, not the initial computation</p></li>
<li><p>We still store and normalize the full matrix</p></li>
</ul>
<p><strong>To actually save computation, you‚Äôd need:</strong></p>
<ol class="arabic simple">
<li><p>Sparse matrix multiplication (skip masked positions in matmul)</p></li>
<li><p>Only compute lower triangle of scores</p></li>
<li><p>Specialized CUDA kernels that exploit sparsity</p></li>
</ol>
<p><strong>Production optimizations:</strong></p>
<ul class="simple">
<li><p>PyTorch‚Äôs standard attention: Also computes full matrix (same as yours)</p></li>
<li><p>FlashAttention: Uses tiling to avoid full matrix but doesn‚Äôt exploit sparsity</p></li>
<li><p>Sparse attention (BigBird, Longformer): Actually skips computation for sparse patterns</p></li>
</ul>
<p><strong>Memory could be saved:</strong> Store only lower triangle (n¬≤/2 elements), but requires custom indexing</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
**Q5: Gradient Memory**

Training attention requires storing activations for backpropagation. How much memory does training need compared to inference?

```{admonition} Answer
:class: dropdown

**Forward pass (inference):**
- Attention matrix: n¬≤ values

**Backward pass (training) additional memory:**
- Gradient of attention weights: n¬≤ values
- Gradient of Q, K, V: 3 √ó (n √ó d) values
- Intermediate gradients from softmax: n¬≤ values

**With Adam optimizer (standard for transformers):**
- First moment (momentum): n¬≤ values
- Second moment (velocity): n¬≤ values

**Total multiplier for attention matrix alone:**
- Forward: 1√ó (attention weights)
- Backward: +2√ó (gradients)
- Optimizer: +2√ó (Adam state)
- **Total: 5√ó inference memory**

**For GPT-3 scale (96 layers, 2048 context):**
- Inference: 96 √ó 16 MB = 1.5 GB
- Training: 96 √ó 16 MB √ó 5 = **7.5 GB** just for attention gradients and optimizer state!

This excludes Q/K/V matrices, feed-forward networks, embeddings, and activations from other layers. Full GPT-3 training requires 350+ GB.
</pre></div>
</div>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>For students who want to understand the academic foundations and explore the research that created modern transformers:</p>
<section id="seminal-papers">
<h3>Seminal Papers<a class="headerlink" href="#seminal-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Attention Is All You Need</strong> - Vaswani et al. (2017). The paper that introduced transformers and the multi-head attention mechanism you just built. Shows how attention alone, without recurrence, achieves state-of-the-art results. <a class="reference external" href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></p></li>
<li><p><strong>BERT: Pre-training of Deep Bidirectional Transformers</strong> - Devlin et al. (2018). Demonstrates how bidirectional attention (no causal mask) enables powerful language understanding. <a class="reference external" href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></p></li>
<li><p><strong>Language Models are Unsupervised Multitask Learners (GPT-2)</strong> - Radford et al. (2019). Shows how causal attention with your masking pattern enables autoregressive language modeling at scale. <a class="reference external" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">OpenAI</a></p></li>
<li><p><strong>FlashAttention: Fast and Memory-Efficient Exact Attention</strong> - Dao et al. (2022). Addresses the O(n¬≤) memory bottleneck you experienced, achieving 2-4√ó speedups without approximation. <a class="reference external" href="https://arxiv.org/abs/2205.14135">arXiv:2205.14135</a></p></li>
</ul>
</section>
<section id="additional-resources">
<h3>Additional Resources<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Blog post</strong>: ‚ÄúThe Illustrated Transformer‚Äù by Jay Alammar - Visual explanations of attention mechanics that complement your implementation</p></li>
<li><p><strong>Interactive tool</strong>: BertViz - Visualize attention patterns in trained models to see the specialization you enabled with multi-head attention</p></li>
<li><p><strong>Textbook</strong>: ‚ÄúSpeech and Language Processing‚Äù (Jurafsky &amp; Martin, Chapter 9) - Formal treatment of attention in sequence-to-sequence models</p></li>
</ul>
</section>
</section>
<section id="whats-next">
<h2>What‚Äôs Next<a class="headerlink" href="#whats-next" title="Link to this heading">#</a></h2>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Coming Up: Module 13 - Transformers</p>
<p>Build complete transformer blocks by combining your attention mechanism with feed-forward networks, layer normalization, and residual connections. You‚Äôll assemble the architecture behind GPT, BERT, and modern language models.</p>
</div>
<p><strong>Preview - How Your Attention Gets Used in Future Modules:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>What It Does</p></th>
<th class="head"><p>Your Attention In Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>13: Transformers</strong></p></td>
<td><p>Complete transformer blocks</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">TransformerLayer(attention</span> <span class="pre">+</span> <span class="pre">FFN</span> <span class="pre">+</span> <span class="pre">LayerNorm)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>13: Transformers</strong></p></td>
<td><p>Positional encoding</p></td>
<td><p>Attention on position-aware embeddings</p></td>
</tr>
<tr class="row-even"><td><p><strong>13: Transformers</strong></p></td>
<td><p>Stacked layers</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention</span> <span class="pre">‚Üí</span> <span class="pre">FFN</span> <span class="pre">‚Üí</span> <span class="pre">attention</span> <span class="pre">‚Üí</span> <span class="pre">FFN...</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/12_attention/12_attention.ipynb">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/12_attention/12_attention.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="11_embeddings_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 11: Embeddings</p>
      </div>
    </a>
    <a class="right-next"
       href="13_transformers_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Module 13: Transformers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention-function">Scaled Dot-Product Attention Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiheadattention-class">MultiHeadAttention Class</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-key-value-the-information-retrieval-paradigm">Query, Key, Value: The Information Retrieval Paradigm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention-similarity-as-relevance">Scaled Dot-Product Attention: Similarity as Relevance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-weights-and-softmax-normalization">Attention Weights and Softmax Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-parallel-relationship-learning">Multi-Head Attention: Parallel Relationship Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#causal-masking-preventing-information-leakage">Causal Masking: Preventing Information Leakage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity-the-o-n2-reality">Computational Complexity: The O(n¬≤) Reality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatch-in-attention">Shape Mismatch in Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-weights-dont-sum-to-1">Attention Weights Don‚Äôt Sum to 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-dimension-mismatch">Multi-Head Dimension Mismatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mask-broadcasting-errors">Mask Broadcasting Errors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-flow-issues">Gradient Flow Issues</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-attention-matters-at-scale">Why Attention Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>