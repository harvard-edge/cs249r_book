
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 11: Embeddings" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/11_embeddings_ABOUT.html" />
<meta property="og:site_name" content="TinyTorch" />
<meta property="og:description" content="Overview: Embeddings are the crucial bridge between discrete tokens and continuous neural network operations. Your tokenizer from Module 10 converts text into token IDs like[42, 7, 15], but neural ..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="TinyTorch" />
<meta name="description" content="Overview: Embeddings are the crucial bridge between discrete tokens and continuous neural network operations. Your tokenizer from Module 10 converts text into token IDs like[42, 7, 15], but neural ..." />

    <title>Module 11: Embeddings &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=6f4f0411" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";

const defaultStyle = document.createElement('style');
defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}
`;
document.head.appendChild(defaultStyle);

const fullscreenStyle = document.createElement('style');
fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
document.head.appendChild(fullscreenStyle);

// Detect if page has dark background
const isDarkTheme = () => {
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        return brightness < 128;
    }
    return false;
};

const load = async () => {
    await mermaid.run();

    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(load, 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(load, 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(load, 200);
        return;
    }

    const darkTheme = isDarkTheme();

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    const modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    const modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    let previousScrollOffset = [window.scrollX, window.scrollY];

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    const allButtons = [];

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });

        container.appendChild(fullscreenBtn);
        allButtons.push(fullscreenBtn);
    });

    // Update theme classes when theme changes
    const updateTheme = () => {
        const dark = isDarkTheme();
        allButtons.forEach(btn => {
            if (dark) {
                btn.classList.add('dark-theme');
            } else {
                btn.classList.remove('dark-theme');
            }
        });
        if (dark) {
            modal.classList.add('dark-theme');
            modalContent.classList.add('dark-theme');
            closeBtn.classList.add('dark-theme');
        } else {
            modal.classList.remove('dark-theme');
            modalContent.classList.remove('dark-theme');
            closeBtn.classList.remove('dark-theme');
        }
    };

    // Watch for theme changes
    const observer = new MutationObserver(updateTheme);
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    observer.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style']
    });
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/11_embeddings_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=90b45b94"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=404b701b"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/subscribe-modal.js?v=173232fd"></script>
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Module 12: Attention" href="12_attention_ABOUT.html" />
    <link rel="prev" title="Module 10: Tokenization" href="10_tokenization_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://mlsysbook.ai" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-07)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_autograd_ABOUT.html">05. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_optimizers_ABOUT.html">06. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_training_ABOUT.html">07. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (08-13)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_dataloader_ABOUT.html">08. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_memoization_ABOUT.html">17. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_acceleration_ABOUT.html">18. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/11_embeddings_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 11: Embeddings</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-class">Embedding Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positionalencoding-class">PositionalEncoding Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sinusoidal-embeddings-function">Sinusoidal Embeddings Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddinglayer-class">EmbeddingLayer Class</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-indices-to-vectors">From Indices to Vectors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-table-mechanics">Embedding Table Mechanics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-vs-fixed-embeddings">Learned vs Fixed Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">Positional Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-dimension-trade-offs">Embedding Dimension Trade-offs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-out-of-range">Index Out of Range</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-length-exceeds-maximum">Sequence Length Exceeds Maximum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-dimension-mismatch">Embedding Dimension Mismatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-errors-with-batching">Shape Errors with Batching</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-embeddings-matter-at-scale">Why Embeddings Matter at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-11-embeddings">
<h1>Module 11: Embeddings<a class="headerlink" href="#module-11-embeddings" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>ARCHITECTURE TIER</strong> | Difficulty: ‚óè‚óè‚óã‚óã | Time: 3-5 hours | Prerequisites: 01-07, 10</p>
<p><strong>Prerequisites: Modules 01-07 and 10</strong> means you should understand:</p>
<ul class="simple">
<li><p>Tensor operations (shape manipulation, matrix operations, broadcasting)</p></li>
<li><p>Training fundamentals (forward/backward, optimization)</p></li>
<li><p>Tokenization (converting text to token IDs, vocabularies)</p></li>
</ul>
<p>If you can explain how a tokenizer converts ‚Äúhello‚Äù to token IDs and how to multiply matrices, you‚Äôre ready.</p>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Embeddings are the crucial bridge between discrete tokens and continuous neural network operations. Your tokenizer from Module 10 converts text into token IDs like <code class="docutils literal notranslate"><span class="pre">[42,</span> <span class="pre">7,</span> <span class="pre">15]</span></code>, but neural networks operate on dense vectors of real numbers. Embeddings transform each integer token ID into a learned dense vector that captures semantic meaning. By the end of this module, you‚Äôll implement the embedding systems that power modern language models, from basic lookup tables to sophisticated positional encodings.</p>
<p>Every transformer model, from BERT to GPT-4, relies on embeddings to convert language into learnable representations. You‚Äôll build the exact patterns used in production, understanding not just how they work but why they‚Äôre designed this way.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> embedding layers that convert token IDs to dense vectors through efficient table lookup</p></li>
<li><p><strong>Master</strong> positional encoding strategies including learned and sinusoidal approaches</p></li>
<li><p><strong>Understand</strong> memory scaling for embedding tables and the trade-offs between vocabulary size and embedding dimension</p></li>
<li><p><strong>Connect</strong> your implementation to production transformer architectures used in GPT and BERT</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Your Embedding System&quot;
        A[&quot;Embedding&lt;br/&gt;Token ‚Üí Vector&quot;]
        B[&quot;PositionalEncoding&lt;br/&gt;Learned Positions&quot;]
        C[&quot;Sinusoidal PE&lt;br/&gt;Math Patterns&quot;]
        D[&quot;EmbeddingLayer&lt;br/&gt;Complete System&quot;]
    end

    A --&gt; D
    B --&gt; D
    C --&gt; D

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
    </pre><figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Your Embedding System</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Part</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Embedding</span></code> class</p></td>
<td><p>Token ID to vector lookup via indexing</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">PositionalEncoding</span></code> class</p></td>
<td><p>Learnable position embeddings</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">create_sinusoidal_embeddings()</span></code></p></td>
<td><p>Mathematical position encoding</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">EmbeddingLayer</span></code> class</p></td>
<td><p>Complete token + position system</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Converting tokens to position-aware dense vectors</span>
<span class="n">embed_layer</span> <span class="o">=</span> <span class="n">EmbeddingLayer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>  <span class="c1"># Token IDs from tokenizer</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embed_layer</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>  <span class="c1"># (1, 3, 512) dense vectors ready for attention</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>Attention mechanisms (that‚Äôs Module 12: Attention)</p></li>
<li><p>Full transformer architectures (that‚Äôs Module 13: Transformers)</p></li>
<li><p>Word2Vec or GloVe pretrained embeddings (you‚Äôre building learnable embeddings)</p></li>
<li><p>Subword embedding composition (PyTorch handles this at the tokenization level)</p></li>
</ul>
<p><strong>You are building the foundation for sequence models.</strong> Context-aware representations come next.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section documents the embedding components you‚Äôll build. Use this as your reference while implementing and debugging.</p>
<section id="embedding-class">
<h3>Embedding Class<a class="headerlink" href="#embedding-class" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</pre></div>
</div>
<p>Learnable embedding layer that maps token indices to dense vectors through table lookup.</p>
<p><strong>Constructor Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> (int): Size of vocabulary (number of unique tokens)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> (int): Dimension of embedding vectors</p></li>
</ul>
<p><strong>Core Methods:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(indices:</span> <span class="pre">Tensor)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Lookup embeddings for token indices</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">parameters()</span> <span class="pre">-&gt;</span> <span class="pre">List[Tensor]</span></code></p></td>
<td><p>Return weight matrix for optimization</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span></code>: Tensor of shape <code class="docutils literal notranslate"><span class="pre">(vocab_size,</span> <span class="pre">embed_dim)</span></code> containing learnable embeddings</p></li>
</ul>
</section>
<section id="positionalencoding-class">
<h3>PositionalEncoding Class<a class="headerlink" href="#positionalencoding-class" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</pre></div>
</div>
<p>Learnable positional encoding that adds trainable position-specific vectors to embeddings.</p>
<p><strong>Constructor Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> (int): Maximum sequence length to support</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> (int): Embedding dimension (must match token embeddings)</p></li>
</ul>
<p><strong>Core Methods:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(x:</span> <span class="pre">Tensor)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Add positional encodings to input embeddings</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">parameters()</span> <span class="pre">-&gt;</span> <span class="pre">List[Tensor]</span></code></p></td>
<td><p>Return position embedding matrix</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="sinusoidal-embeddings-function">
<h3>Sinusoidal Embeddings Function<a class="headerlink" href="#sinusoidal-embeddings-function" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">create_sinusoidal_embeddings</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
<p>Creates fixed sinusoidal positional encodings using trigonometric functions. No parameters to learn.</p>
<p><strong>Mathematical Formula:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PE</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="mi">2</span><span class="n">i</span><span class="p">)</span>   <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="mi">10000</span><span class="o">^</span><span class="p">(</span><span class="mi">2</span><span class="n">i</span><span class="o">/</span><span class="n">embed_dim</span><span class="p">))</span>
<span class="n">PE</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="mi">2</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="mi">10000</span><span class="o">^</span><span class="p">(</span><span class="mi">2</span><span class="n">i</span><span class="o">/</span><span class="n">embed_dim</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="embeddinglayer-class">
<h3>EmbeddingLayer Class<a class="headerlink" href="#embeddinglayer-class" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">EmbeddingLayer</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
               <span class="n">pos_encoding</span><span class="o">=</span><span class="s1">&#39;learned&#39;</span><span class="p">,</span> <span class="n">scale_embeddings</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Complete embedding system combining token embeddings and positional encoding.</p>
<p><strong>Constructor Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> (int): Size of vocabulary</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> (int): Embedding dimension</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> (int): Maximum sequence length for positional encoding</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pos_encoding</span></code> (str): Type of positional encoding (‚Äòlearned‚Äô, ‚Äòsinusoidal‚Äô, or None)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scale_embeddings</span></code> (bool): Whether to scale by sqrt(embed_dim) (transformer convention)</p></li>
</ul>
<p><strong>Core Methods:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(tokens:</span> <span class="pre">Tensor)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Complete embedding pipeline</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">parameters()</span> <span class="pre">-&gt;</span> <span class="pre">List[Tensor]</span></code></p></td>
<td><p>All trainable parameters</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section explores the fundamental ideas behind embeddings and positional encoding. These concepts apply to every modern language model, from small research experiments to production systems.</p>
<section id="from-indices-to-vectors">
<h3>From Indices to Vectors<a class="headerlink" href="#from-indices-to-vectors" title="Link to this heading">#</a></h3>
<p>Neural networks operate on continuous vectors, but language consists of discrete tokens. After your tokenizer converts ‚Äúthe cat sat‚Äù to token IDs <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">42,</span> <span class="pre">7]</span></code>, you need a way to represent these discrete integers as dense vectors that can capture semantic meaning.</p>
<p>The embedding layer solves this through a simple but powerful idea: maintain a learnable table where each token ID maps to a vector. For a vocabulary of 50,000 tokens with 512-dimensional embeddings, you create a matrix of shape <code class="docutils literal notranslate"><span class="pre">(50000,</span> <span class="pre">512)</span></code> initialized with small random values. During training, these vectors adjust to capture semantic relationships: similar words learn similar vectors.</p>
<p>Here‚Äôs the core implementation showing how efficiently this works:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass: lookup embeddings for given indices.&quot;&quot;&quot;</span>
    <span class="c1"># Validate indices are in range</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">data</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Index out of range. Expected 0 &lt;= indices &lt; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;got min=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2">, max=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Perform embedding lookup using advanced indexing</span>
    <span class="c1"># This is equivalent to one-hot multiplication but much more efficient</span>
    <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">indices</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)]</span>

    <span class="c1"># Create result tensor with gradient tracking</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">result</span><span class="o">.</span><span class="n">_grad_fn</span> <span class="o">=</span> <span class="n">EmbeddingBackward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
<p>The beauty is in the simplicity: <code class="docutils literal notranslate"><span class="pre">self.weight.data[indices.data.astype(int)]</span></code> uses NumPy‚Äôs advanced indexing (also called fancy indexing) to look up multiple embeddings simultaneously. For input indices <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">42,</span> <span class="pre">7]</span></code>, this single operation retrieves rows 1, 42, and 7 from the weight matrix in one efficient step, automatically handling batched inputs of any shape. While conceptually equivalent to creating one-hot vectors and matrix multiplication, direct indexing is orders of magnitude faster and requires no intermediate allocations.</p>
</section>
<section id="embedding-table-mechanics">
<h3>Embedding Table Mechanics<a class="headerlink" href="#embedding-table-mechanics" title="Link to this heading">#</a></h3>
<p>The embedding table is a learnable parameter matrix initialized with small random values. For vocabulary size V and embedding dimension D, the table has shape <code class="docutils literal notranslate"><span class="pre">(V,</span> <span class="pre">D)</span></code>. Each row represents one token‚Äôs learned representation.</p>
<p>Initialization matters for training stability. The implementation uses Xavier initialization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Xavier initialization for better gradient flow</span>
<span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="n">embed_dim</span><span class="p">))</span>
<span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)),</span>
    <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This initialization scale ensures gradients neither explode nor vanish at the start of training. The limit is computed from both vocabulary size and embedding dimension, balancing the fan-in and fan-out of the embedding layer.</p>
<p>During training, gradients flow back through the lookup operation to update only the embeddings that were accessed. If your batch contains tokens <code class="docutils literal notranslate"><span class="pre">[5,</span> <span class="pre">10,</span> <span class="pre">10,</span> <span class="pre">5]</span></code>, only rows 5 and 10 of the embedding table receive gradient updates. This sparse gradient pattern is handled by the <code class="docutils literal notranslate"><span class="pre">EmbeddingBackward</span></code> gradient function, making embedding updates extremely efficient even for vocabularies with millions of tokens.</p>
</section>
<section id="learned-vs-fixed-embeddings">
<h3>Learned vs Fixed Embeddings<a class="headerlink" href="#learned-vs-fixed-embeddings" title="Link to this heading">#</a></h3>
<p>Positional information can be added to token embeddings in two fundamentally different ways: learned position embeddings and fixed sinusoidal encodings.</p>
<p><strong>Learned positional encoding</strong> treats each position as a trainable parameter, just like token embeddings. For maximum sequence length M and embedding dimension D, you create a second embedding table of shape <code class="docutils literal notranslate"><span class="pre">(M,</span> <span class="pre">D)</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize position embedding matrix</span>
<span class="c1"># Smaller initialization than token embeddings since these are additive</span>
<span class="n">limit</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">embed_dim</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)),</span>
    <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<p>During forward pass, you slice position embeddings and add them to token embeddings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add positional encodings to input embeddings.&quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Slice position embeddings for this sequence length</span>
    <span class="c1"># Tensor slicing preserves gradient flow (from Module 01&#39;s __getitem__)</span>
    <span class="n">pos_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">]</span>

    <span class="c1"># Reshape to add batch dimension: (1, seq_len, embed_dim)</span>
    <span class="n">pos_data</span> <span class="o">=</span> <span class="n">pos_embeddings</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="n">pos_embeddings_batched</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">pos_data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">pos_embeddings</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="c1"># Copy gradient function to preserve backward connection</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">pos_embeddings</span><span class="p">,</span> <span class="s1">&#39;_grad_fn&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">pos_embeddings</span><span class="o">.</span><span class="n">_grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pos_embeddings_batched</span><span class="o">.</span><span class="n">_grad_fn</span> <span class="o">=</span> <span class="n">pos_embeddings</span><span class="o">.</span><span class="n">_grad_fn</span>

    <span class="c1"># Add positional information - gradients flow through both x and pos_embeddings!</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">pos_embeddings_batched</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
</div>
<p>The slicing operation <code class="docutils literal notranslate"><span class="pre">self.position_embeddings[:seq_len]</span></code> preserves gradient tracking because TinyTorch‚Äôs Tensor <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> (from Module 01) maintains the connection to the original parameter. This allows backpropagation to update only the position embeddings actually used in the forward pass.</p>
<p>The advantage is flexibility: the model can learn task-specific positional patterns. The disadvantage is memory cost and a hard maximum sequence length.</p>
<p><strong>Fixed sinusoidal encoding</strong> uses mathematical patterns requiring no parameters. The formula creates unique position signatures using sine and cosine at different frequencies:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create position indices [0, 1, 2, ..., max_seq_len-1]</span>
<span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="c1"># Create dimension indices for calculating frequencies</span>
<span class="n">div_term</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span>
    <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">embed_dim</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Initialize the positional encoding matrix</span>
<span class="n">pe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Apply sine to even indices, cosine to odd indices</span>
<span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
<span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
</pre></div>
</div>
<p>This creates a unique vector for each position where low-index dimensions oscillate rapidly (high frequency) and high-index dimensions change slowly (low frequency). The pattern allows the model to learn relative positions through dot products, and crucially, can extrapolate to sequences longer than seen during training.</p>
</section>
<section id="positional-encodings">
<h3>Positional Encodings<a class="headerlink" href="#positional-encodings" title="Link to this heading">#</a></h3>
<p>Without positional information, embeddings have no notion of order. The sentence ‚Äúcat sat on mat‚Äù would have identical representation to ‚Äúmat on sat cat‚Äù because you‚Äôd sum or average the same four token embeddings regardless of order.</p>
<p>Positional encodings solve this by adding position-specific information to each token‚Äôs embedding. After embedding lookup, token 42 at position 0 gets different positional information than token 42 at position 5, making the model position-aware.</p>
<p>The Transformer paper introduced sinusoidal positional encoding with a clever mathematical structure. For position <code class="docutils literal notranslate"><span class="pre">pos</span></code> and dimension <code class="docutils literal notranslate"><span class="pre">i</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PE</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="mi">2</span><span class="n">i</span><span class="p">)</span>   <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="mi">10000</span><span class="o">^</span><span class="p">(</span><span class="mi">2</span><span class="n">i</span><span class="o">/</span><span class="n">embed_dim</span><span class="p">))</span>  <span class="c1"># Even dimensions</span>
<span class="n">PE</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="mi">2</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="mi">10000</span><span class="o">^</span><span class="p">(</span><span class="mi">2</span><span class="n">i</span><span class="o">/</span><span class="n">embed_dim</span><span class="p">))</span>  <span class="c1"># Odd dimensions</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">10000</span></code> base creates different wavelengths across dimensions. Dimension 0 oscillates rapidly (frequency ‚âà 1), while dimension 510 changes extremely slowly (frequency ‚âà 1/10000). This multi-scale structure gives each position a unique ‚Äúfingerprint‚Äù and enables the model to learn relative position through simple vector arithmetic.</p>
<p>At position 0, all sine terms equal 0 and all cosine terms equal 1: <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">...]</span></code>. At position 1, the pattern shifts based on each dimension‚Äôs frequency. The combination of many frequencies creates distinct encodings where nearby positions have similar (but not identical) vectors, providing smooth positional gradients.</p>
<p>The trigonometric identity enables learning relative positions: <code class="docutils literal notranslate"><span class="pre">PE(pos+k)</span></code> can be expressed as a linear function of <code class="docutils literal notranslate"><span class="pre">PE(pos)</span></code> using sine and cosine addition formulas. This allows attention mechanisms to implicitly learn positional offsets (like ‚Äúthe token 3 positions ahead‚Äù) through learned weights on the position encodings, without the model needing separate relative position parameters.</p>
</section>
<section id="embedding-dimension-trade-offs">
<h3>Embedding Dimension Trade-offs<a class="headerlink" href="#embedding-dimension-trade-offs" title="Link to this heading">#</a></h3>
<p>The embedding dimension D controls the capacity of your learned representations. Larger D provides more expressiveness but costs memory and compute. The choice involves several interacting factors.</p>
<p><strong>Memory scaling</strong>: Embedding tables scale as <code class="docutils literal notranslate"><span class="pre">vocab_size</span> <span class="pre">√ó</span> <span class="pre">embed_dim</span> <span class="pre">√ó</span> <span class="pre">4</span> <span class="pre">bytes</span></code> (for float32). A vocabulary of 50,000 tokens with 512-dimensional embeddings requires 100 MB. Double the dimension to 1024, and memory doubles to 200 MB. For large vocabularies, the embedding table often dominates total model memory. GPT-3‚Äôs 50,257 token vocabulary with 12,288-dimensional embeddings uses approximately 2.4 GB just for token embeddings.</p>
<p><strong>Semantic capacity</strong>: Higher dimensions allow finer-grained semantic distinctions. With 64 dimensions, you might capture basic categories (animals, actions, objects). With 512 dimensions, you can encode subtle relationships (synonyms, antonyms, part-of-speech, contextual variations). With 1024+ dimensions, you have capacity for highly nuanced semantic features discovered through training.</p>
<p><strong>Computational cost</strong>: Every attention head in transformers performs operations over the embedding dimension. Memory bandwidth becomes the bottleneck: transferring embedding vectors from RAM to cache dominates the time to process them. Larger embeddings mean more memory traffic per token, reducing throughput.</p>
<p><strong>Typical scales in production</strong>:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Vocabulary</p></th>
<th class="head"><p>Embed Dim</p></th>
<th class="head"><p>Embedding Memory</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Small BERT</p></td>
<td><p>30,000</p></td>
<td><p>768</p></td>
<td><p>92 MB</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-2</p></td>
<td><p>50,257</p></td>
<td><p>1,024</p></td>
<td><p>206 MB</p></td>
</tr>
<tr class="row-even"><td><p>GPT-3</p></td>
<td><p>50,257</p></td>
<td><p>12,288</p></td>
<td><p>2,471 MB</p></td>
</tr>
<tr class="row-odd"><td><p>Large Transformer</p></td>
<td><p>100,000</p></td>
<td><p>1,024</p></td>
<td><p>410 MB</p></td>
</tr>
</tbody>
</table>
</div>
<p>The embedding dimension typically matches the model‚Äôs hidden dimension since embeddings feed directly into the first transformer layer. You rarely see models with embedding dimension different from hidden dimension (though it‚Äôs technically possible with a projection layer).</p>
</section>
</section>
<section id="common-errors">
<h2>Common Errors<a class="headerlink" href="#common-errors" title="Link to this heading">#</a></h2>
<p>These are the errors you‚Äôll encounter most often when working with embeddings. Understanding why they happen will save you hours of debugging.</p>
<section id="index-out-of-range">
<h3>Index Out of Range<a class="headerlink" href="#index-out-of-range" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">Index</span> <span class="pre">out</span> <span class="pre">of</span> <span class="pre">range.</span> <span class="pre">Expected</span> <span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">indices</span> <span class="pre">&lt;</span> <span class="pre">50000,</span> <span class="pre">got</span> <span class="pre">max=50001</span></code></p>
<p>Embedding lookup expects token IDs in the range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">vocab_size-1]</span></code>. If your tokenizer produces an ID of 50001 but your embedding layer has <code class="docutils literal notranslate"><span class="pre">vocab_size=50000</span></code>, the lookup fails.</p>
<p><strong>Cause</strong>: Mismatch between tokenizer vocabulary size and embedding layer vocabulary size. This often happens when you train a tokenizer separately and forget to sync the vocabulary size when creating the embedding layer.</p>
<p><strong>Fix</strong>: Ensure <code class="docutils literal notranslate"><span class="pre">embed_layer.vocab_size</span></code> matches your tokenizer‚Äôs vocabulary size exactly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">50000</span><span class="p">)</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="sequence-length-exceeds-maximum">
<h3>Sequence Length Exceeds Maximum<a class="headerlink" href="#sequence-length-exceeds-maximum" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">Sequence</span> <span class="pre">length</span> <span class="pre">1024</span> <span class="pre">exceeds</span> <span class="pre">maximum</span> <span class="pre">512</span></code></p>
<p>Learned positional encodings have a fixed maximum sequence length set during initialization. If you try to process a sequence longer than this maximum, the forward pass fails because there are no position embeddings for those positions.</p>
<p><strong>Cause</strong>: Input sequences longer than <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> parameter used when creating the positional encoding layer.</p>
<p><strong>Fix</strong>: Either increase <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> during initialization, truncate your sequences, or use sinusoidal positional encoding which can handle arbitrary lengths:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Option 1: Increase max_seq_len</span>
<span class="n">pos_enc</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

<span class="c1"># Option 2: Use sinusoidal (no length limit)</span>
<span class="n">embed_layer</span> <span class="o">=</span> <span class="n">EmbeddingLayer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                             <span class="n">pos_encoding</span><span class="o">=</span><span class="s1">&#39;sinusoidal&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="embedding-dimension-mismatch">
<h3>Embedding Dimension Mismatch<a class="headerlink" href="#embedding-dimension-mismatch" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">Embedding</span> <span class="pre">dimension</span> <span class="pre">mismatch:</span> <span class="pre">expected</span> <span class="pre">512,</span> <span class="pre">got</span> <span class="pre">768</span></code></p>
<p>When adding positional encodings to token embeddings, the dimensions must match exactly. If your token embeddings are 512-dimensional but your positional encoding expects 768-dimensional inputs, element-wise addition fails.</p>
<p><strong>Cause</strong>: Creating embedding components with inconsistent <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> values.</p>
<p><strong>Fix</strong>: Use the same <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> for all embedding components:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">token_embed</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">)</span>
<span class="n">pos_enc</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="shape-errors-with-batching">
<h3>Shape Errors with Batching<a class="headerlink" href="#shape-errors-with-batching" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">Expected</span> <span class="pre">3D</span> <span class="pre">input</span> <span class="pre">(batch,</span> <span class="pre">seq,</span> <span class="pre">embed),</span> <span class="pre">got</span> <span class="pre">shape</span> <span class="pre">(128,</span> <span class="pre">512)</span></code></p>
<p>Positional encoding expects 3D tensors with batch dimension. If you pass a 2D tensor (sequence, embedding), the forward pass fails.</p>
<p><strong>Cause</strong>: Forgetting to add batch dimension when processing single sequences, or using raw embedding output without reshaping.</p>
<p><strong>Fix</strong>: Ensure inputs have batch dimension, even for single sequences:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Wrong: 2D input</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>  <span class="c1"># Shape: (3, 512) - missing batch dim</span>

<span class="c1"># Right: 3D input</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>  <span class="c1"># Added batch dimension</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>  <span class="c1"># Shape: (1, 3, 512) - correct</span>
</pre></div>
</div>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-pytorch">
<h3>Your Implementation vs. PyTorch<a class="headerlink" href="#your-implementation-vs-pytorch" title="Link to this heading">#</a></h3>
<p>Your TinyTorch embedding system and PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code> share the same conceptual design and API patterns. The differences are in scale, optimization, and device support.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Backend</strong></p></td>
<td><p>NumPy (CPU only)</p></td>
<td><p>C++/CUDA (CPU/GPU)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Lookup Speed</strong></p></td>
<td><p>1x (baseline)</p></td>
<td><p>10-100x faster on GPU</p></td>
</tr>
<tr class="row-even"><td><p><strong>Max Vocabulary</strong></p></td>
<td><p>Limited by RAM</p></td>
<td><p>Billions (with techniques)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Positional Encoding</strong></p></td>
<td><p>Learned + sinusoidal</p></td>
<td><p>Must implement yourself*</p></td>
</tr>
<tr class="row-even"><td><p><strong>Sparse Gradients</strong></p></td>
<td><p>Via custom backward</p></td>
<td><p>Native sparse gradient support</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Memory Optimization</strong></p></td>
<td><p>Standard</p></td>
<td><p>Quantization, pruning, sharing</p></td>
</tr>
</tbody>
</table>
</div>
<p>*PyTorch provides building blocks but you implement positional encoding patterns yourself (as you did here)</p>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows equivalent embedding operations in TinyTorch and PyTorch. Notice how the APIs mirror each other closely.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.embeddings</span><span class="w"> </span><span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">EmbeddingLayer</span>

<span class="c1"># Create embedding layer</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

<span class="c1"># Token lookup</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">99</span><span class="p">]])</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>  <span class="c1"># (1, 4, 512)</span>

<span class="c1"># Complete system with position encoding</span>
<span class="n">embed_layer</span> <span class="o">=</span> <span class="n">EmbeddingLayer</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="n">pos_encoding</span><span class="o">=</span><span class="s1">&#39;learned&#39;</span>
<span class="p">)</span>
<span class="n">position_aware</span> <span class="o">=</span> <span class="n">embed_layer</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Create embedding layer</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

<span class="c1"># Token lookup</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">99</span><span class="p">]])</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>  <span class="c1"># (1, 4, 512)</span>

<span class="c1"># Complete system (you implement positional encoding yourself)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">EmbeddingWithPosition</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embed</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>

<span class="n">embed_layer</span> <span class="o">=</span> <span class="n">EmbeddingWithPosition</span><span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">2048</span><span class="p">)</span>
<span class="n">position_aware</span> <span class="o">=</span> <span class="n">embed_layer</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs walk through each section to understand the comparison:</p>
<ul class="simple">
<li><p><strong>Line 1-2 (Import)</strong>: Both frameworks provide dedicated embedding modules. TinyTorch packages everything in <code class="docutils literal notranslate"><span class="pre">embeddings</span></code>; PyTorch uses <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> as the base class.</p></li>
<li><p><strong>Line 4-5 (Embedding Creation)</strong>: Your <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> class closely mirrors PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code>. The parameter names differ (<code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> vs <code class="docutils literal notranslate"><span class="pre">num_embeddings</span></code>) but the concept is identical.</p></li>
<li><p><strong>Line 7-9 (Token Lookup)</strong>: Both use identical calling patterns. The embedding layer acts as a function, taking token IDs and returning dense vectors. Shape semantics are identical.</p></li>
<li><p><strong>Line 11-20 (Complete System)</strong>: Your <code class="docutils literal notranslate"><span class="pre">EmbeddingLayer</span></code> provides a complete system in one class. In PyTorch, you implement this pattern yourself by composing <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layers for tokens and positions. The HuggingFace Transformers library implements this exact pattern for BERT, GPT, and other models.</p></li>
<li><p><strong>Line 22-24 (Forward Pass)</strong>: Both systems add token and position embeddings element-wise. Your implementation handles this internally; PyTorch requires you to manage position indices explicitly.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>Embedding lookup semantics, gradient flow patterns, and the addition of positional information. When you debug PyTorch transformer models, you‚Äôll recognize these exact patterns because you built them yourself.</p>
</div>
</section>
<section id="why-embeddings-matter-at-scale">
<h3>Why Embeddings Matter at Scale<a class="headerlink" href="#why-embeddings-matter-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate embedding systems, consider the scale of modern language models:</p>
<ul class="simple">
<li><p><strong>GPT-3 embeddings</strong>: 50,257 token vocabulary √ó 12,288 dimensions = <strong>618 million parameters</strong> = 2.4 GB of memory (just for token embeddings, not counting position embeddings)</p></li>
<li><p><strong>Lookup throughput</strong>: Processing 32 sequences of 2048 tokens requires <strong>65,536 embedding lookups</strong> per batch. At 1000 batches per second (typical training), that‚Äôs 65 million lookups per second.</p></li>
<li><p><strong>Memory bandwidth</strong>: Each lookup transfers 512-1024 dimensions √ó 4 bytes = <strong>2-4 KB from RAM to cache</strong>. At scale, memory bandwidth (not compute) becomes the bottleneck.</p></li>
<li><p><strong>Gradient sparsity</strong>: In a batch with 65,536 tokens, only a small fraction of the 50,257 vocabulary is accessed. Efficient training exploits this sparsity, updating only the accessed embeddings‚Äô gradients.</p></li>
</ul>
<p>Modern transformer training spends approximately <strong>10-15% of total time</strong> in embedding operations (lookup + position encoding). The remaining 85-90% goes to attention and feedforward layers. However, embeddings consume <strong>30-40% of model memory</strong> for models with large vocabularies, making them critical for deployment.</p>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions. They‚Äôre designed to build intuition for the performance and memory characteristics you‚Äôll encounter in production.</p>
<p><strong>Q1: Memory Calculation</strong></p>
<p>An embedding layer has <code class="docutils literal notranslate"><span class="pre">vocab_size=50000</span></code> and <code class="docutils literal notranslate"><span class="pre">embed_dim=512</span></code>. How much memory does the embedding table use (in MB)?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>50,000 √ó 512 √ó 4 bytes = <strong>102,400,000 bytes = 97.7 MB</strong></p>
<p>Calculation breakdown:</p>
<ul class="simple">
<li><p>Parameters: 50,000 √ó 512 = 25,600,000</p></li>
<li><p>Memory: 25,600,000 √ó 4 bytes (float32) = 102,400,000 bytes</p></li>
<li><p>In MB: 102,400,000 / (1024 √ó 1024) = 97.7 MB</p></li>
</ul>
<p>This is why vocabulary size matters for model deployment!</p>
</div>
<p><strong>Q2: Positional Encoding Memory</strong></p>
<p>Compare memory requirements for learned vs sinusoidal positional encoding with <code class="docutils literal notranslate"><span class="pre">max_seq_len=2048</span></code> and <code class="docutils literal notranslate"><span class="pre">embed_dim=512</span></code>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Learned PE</strong>: 2,048 √ó 512 √ó 4 = <strong>4,194,304 bytes = 4.0 MB</strong> (1,048,576 parameters)</p>
<p><strong>Sinusoidal PE</strong>: <strong>0 bytes</strong> (0 parameters - computed mathematically)</p>
<p>For large models, learned PE adds significant memory. GPT-3 uses learned PE with 2048 positions √ó 12,288 dimensions = 100 MB additional memory. Some models use sinusoidal to save this memory.</p>
</div>
<p><strong>Q3: Lookup Complexity</strong></p>
<p>What is the time complexity of looking up embeddings for a batch of 32 sequences, each with 128 tokens?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>O(1) per token</strong>, or <strong>O(batch_size √ó seq_len)</strong> = O(32 √ó 128) = O(4096) total</p>
<p>The lookup operation is constant time per token because it‚Äôs just array indexing: <code class="docutils literal notranslate"><span class="pre">weight[token_id]</span></code>. For 4,096 tokens, you perform 4,096 constant-time lookups.</p>
<p>Importantly, vocabulary size does NOT affect lookup time. Looking up tokens from a 1,000 word vocabulary is the same speed as from a 100,000 word vocabulary (assuming cache effects are comparable). The memory access is direct indexing, not search.</p>
</div>
<p><strong>Q4: Embedding Dimension Scaling</strong></p>
<p>You have an embedding layer with <code class="docutils literal notranslate"><span class="pre">vocab_size=50000,</span> <span class="pre">embed_dim=512</span></code> using 100 MB. If you double <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> to 1024, what happens to memory?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Memory <strong>doubles to 200 MB</strong></p>
<p>Embedding memory scales linearly with embedding dimension:</p>
<ul class="simple">
<li><p>Original: 50,000 √ó 512 √ó 4 = 100 MB</p></li>
<li><p>Doubled: 50,000 √ó 1,024 √ó 4 = 200 MB</p></li>
</ul>
<p>This is why you can‚Äôt arbitrarily increase embedding dimensions. Each doubling doubles memory and memory bandwidth requirements. Large models carefully balance embedding dimension against available memory.</p>
</div>
<p><strong>Q5: Sinusoidal Extrapolation</strong></p>
<p>You trained a model with sinusoidal positional encoding and <code class="docutils literal notranslate"><span class="pre">max_seq_len=512</span></code>. Can you process sequences of length 1024 at inference time? What about with learned positional encoding?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Sinusoidal PE: Yes</strong> - can extrapolate to length 1024 (or any length)</p>
<p>The mathematical formula creates unique encodings for any position. You simply compute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pe_1024</span> <span class="o">=</span> <span class="n">create_sinusoidal_embeddings</span><span class="p">(</span><span class="n">max_seq_len</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><strong>Learned PE: No</strong> - cannot handle sequences longer than training maximum</p>
<p>Learned PE creates a fixed embedding table of shape <code class="docutils literal notranslate"><span class="pre">(max_seq_len,</span> <span class="pre">embed_dim)</span></code>. For positions beyond 512, there are no learned embeddings. You must either:</p>
<ul class="simple">
<li><p>Retrain with larger <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code></p></li>
<li><p>Interpolate position embeddings (advanced technique)</p></li>
<li><p>Truncate sequences to 512 tokens</p></li>
</ul>
<p>This is why many production models use sinusoidal or relative positional encodings that can handle variable lengths.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
## Further Reading

For students who want to understand the academic foundations and mathematical underpinnings of embeddings and positional encoding:

### Seminal Papers

- **Word2Vec** - Mikolov et al. (2013). Introduced efficient learned word embeddings through context prediction. Though your implementation learns embeddings end-to-end, Word2Vec established the idea that similar words should have similar vectors. [arXiv:1301.3781](https://arxiv.org/abs/1301.3781)

- **Attention Is All You Need** - Vaswani et al. (2017). Introduced sinusoidal positional encoding and demonstrated that learned embeddings combined with positional information enable powerful sequence models. Section 3.5 explains the positional encoding formula you implemented. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

- **BERT: Pre-training of Deep Bidirectional Transformers** - Devlin et al. (2018). Shows how embeddings combine with positional and segment encodings for language understanding tasks. BERT uses learned positional embeddings rather than sinusoidal. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)

### Additional Resources

- **Blog Post**: &quot;The Illustrated Word2Vec&quot; by Jay Alammar - Visual explanation of learned word embeddings and semantic relationships
- **Documentation**: [PyTorch nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) - See production embedding implementation
- **Paper**: &quot;GloVe: Global Vectors for Word Representation&quot; - Pennington et al. (2014) - Alternative embedding approach based on co-occurrence statistics

## What&#39;s Next

```{seealso} Coming Up: Module 12 - Attention

Implement attention mechanisms that let embeddings interact with each other. You&#39;ll build the scaled dot-product attention that enables transformers to learn which tokens should influence each other, creating context-aware representations.
</pre></div>
</div>
<p><strong>Preview - How Your Embeddings Get Used in Future Modules:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>What It Does</p></th>
<th class="head"><p>Your Embeddings In Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>12: Attention</strong></p></td>
<td><p>Context-aware representations</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention(embed_layer(tokens))</span></code> creates query, key, value</p></td>
</tr>
<tr class="row-odd"><td><p><strong>13: Transformers</strong></p></td>
<td><p>Complete sequence-to-sequence</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">transformer(embed_layer(src),</span> <span class="pre">embed_layer(tgt))</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?filepath=tinytorch/src/11_embeddings/11_embeddings.py">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://colab.research.google.com/github/harvard-edge/cs249r_book/blob/main/tinytorch/src/11_embeddings/11_embeddings.py">Open in Colab</a></strong> - Use Google Colab for cloud compute</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/src/11_embeddings/11_embeddings.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder and Colab sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10_tokenization_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 10: Tokenization</p>
      </div>
    </a>
    <a class="right-next"
       href="12_attention_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Module 12: Attention</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-class">Embedding Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positionalencoding-class">PositionalEncoding Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sinusoidal-embeddings-function">Sinusoidal Embeddings Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embeddinglayer-class">EmbeddingLayer Class</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-indices-to-vectors">From Indices to Vectors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-table-mechanics">Embedding Table Mechanics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learned-vs-fixed-embeddings">Learned vs Fixed Embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings">Positional Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-dimension-trade-offs">Embedding Dimension Trade-offs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#index-out-of-range">Index Out of Range</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-length-exceeds-maximum">Sequence Length Exceeds Maximum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-dimension-mismatch">Embedding Dimension Mismatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-errors-with-batching">Shape Errors with Batching</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-embeddings-matter-at-scale">Why Embeddings Matter at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>