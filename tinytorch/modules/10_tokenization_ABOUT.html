
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 10: Tokenization" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/10_tokenization_ABOUT.html" />
<meta property="og:site_name" content="TinyTorch" />
<meta property="og:description" content="Overview: Neural networks operate on numbers, but humans communicate with text. Tokenization is the crucial bridge that converts text into numerical sequences that models can process. Every languag..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="TinyTorch" />
<meta name="description" content="Overview: Neural networks operate on numbers, but humans communicate with text. Tokenization is the crucial bridge that converts text into numerical sequences that models can process. Every languag..." />

    <title>Module 10: Tokenization &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=6f4f0411" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";

const defaultStyle = document.createElement('style');
defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}
`;
document.head.appendChild(defaultStyle);

const fullscreenStyle = document.createElement('style');
fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
document.head.appendChild(fullscreenStyle);

// Detect if page has dark background
const isDarkTheme = () => {
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        return brightness < 128;
    }
    return false;
};

const load = async () => {
    await mermaid.run();

    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(load, 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(load, 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(load, 200);
        return;
    }

    const darkTheme = isDarkTheme();

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    const modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    const modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    let previousScrollOffset = [window.scrollX, window.scrollY];

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    const allButtons = [];

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });

        container.appendChild(fullscreenBtn);
        allButtons.push(fullscreenBtn);
    });

    // Update theme classes when theme changes
    const updateTheme = () => {
        const dark = isDarkTheme();
        allButtons.forEach(btn => {
            if (dark) {
                btn.classList.add('dark-theme');
            } else {
                btn.classList.remove('dark-theme');
            }
        });
        if (dark) {
            modal.classList.add('dark-theme');
            modalContent.classList.add('dark-theme');
            closeBtn.classList.add('dark-theme');
        } else {
            modal.classList.remove('dark-theme');
            modalContent.classList.remove('dark-theme');
            closeBtn.classList.remove('dark-theme');
        }
    };

    // Watch for theme changes
    const observer = new MutationObserver(updateTheme);
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    observer.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style']
    });
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/10_tokenization_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=90b45b94"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=404b701b"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/subscribe-modal.js?v=173232fd"></script>
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Module 11: Embeddings" href="11_embeddings_ABOUT.html" />
    <link rel="prev" title="Module 09: Spatial" href="09_convolutions_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://mlsysbook.ai" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">The Big Picture</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-07)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_autograd_ABOUT.html">05. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_optimizers_ABOUT.html">06. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_training_ABOUT.html">07. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (08-13)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_dataloader_ABOUT.html">08. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_memoization_ABOUT.html">17. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_acceleration_ABOUT.html">18. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/10_tokenization_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 10: Tokenization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#base-tokenizer-interface">Base Tokenizer Interface</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chartokenizer">CharTokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bpetokenizer">BPETokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-functions">Utility Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-numbers">Text to Numbers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary-building">Vocabulary Building</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-tokens">Special Tokens</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding-and-decoding">Encoding and Decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity">Computational Complexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary-size-versus-sequence-length">Vocabulary Size Versus Sequence Length</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-production-tokenizers">Your Implementation vs. Production Tokenizers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-tokenization-matters-at-scale">Why Tokenization Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-10-tokenization">
<h1>Module 10: Tokenization<a class="headerlink" href="#module-10-tokenization" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>ARCHITECTURE TIER</strong> | Difficulty: ‚óè‚óè‚óã‚óã | Time: 4-6 hours | Prerequisites: 01-07</p>
<p><strong>Prerequisites: Foundation tier (Modules 01-07)</strong> means you should have completed:</p>
<ul class="simple">
<li><p>Tensor operations (Module 01)</p></li>
<li><p>Basic neural network components (Modules 02-04)</p></li>
<li><p>Training fundamentals (Modules 05-07)</p></li>
</ul>
<p>Tokenization is relatively independent and works primarily with strings and numbers. If you can manipulate Python strings and dictionaries, you‚Äôre ready.</p>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Neural networks operate on numbers, but humans communicate with text. Tokenization is the crucial bridge that converts text into numerical sequences that models can process. Every language model from GPT to BERT starts with tokenization, transforming raw strings like ‚ÄúHello, world!‚Äù into sequences of integers that neural networks can consume.</p>
<p>In this module, you‚Äôll build two tokenization systems from scratch: a simple character-level tokenizer that treats each character as a token, and a sophisticated Byte Pair Encoding (BPE) tokenizer that learns efficient subword representations. You‚Äôll discover the fundamental trade-off in text processing: vocabulary size versus sequence length. Small vocabularies produce long sequences; large vocabularies produce short sequences but require huge embedding tables.</p>
<p>By the end, you‚Äôll understand why GPT uses 50,000 tokens, how tokenizers handle unknown words, and the memory implications of vocabulary choices in production systems.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> character-level tokenization for robust text coverage and BPE tokenization for efficient subword representation</p></li>
<li><p><strong>Understand</strong> the vocabulary size versus sequence length trade-off and its impact on memory and computation</p></li>
<li><p><strong>Master</strong> encoding and decoding operations that convert between text and numerical token IDs</p></li>
<li><p><strong>Connect</strong> your implementation to production tokenizers used in GPT, BERT, and modern language models</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Your Tokenization System&quot;
        A[&quot;Base Interface&lt;br/&gt;encode/decode&quot;]
        B[&quot;CharTokenizer&lt;br/&gt;character-level&quot;]
        C[&quot;BPETokenizer&lt;br/&gt;subword units&quot;]
        D[&quot;Vocabulary&lt;br/&gt;management&quot;]
        E[&quot;Utilities&lt;br/&gt;dataset processing&quot;]
    end

    A --&gt; B
    A --&gt; C
    B --&gt; D
    C --&gt; D
    D --&gt; E

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
    style E fill:#e2d5f1
    </pre><figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Your Tokenization System</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Part</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Tokenizer</span></code> base class</p></td>
<td><p>Interface contract: encode/decode</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CharTokenizer</span></code></p></td>
<td><p>Character-level vocabulary, perfect coverage</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BPETokenizer</span></code></p></td>
<td><p>Byte Pair Encoding, learning merges</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>Vocabulary building</p></td>
<td><p>Unique character extraction, frequency analysis</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Utility functions</p></td>
<td><p>Dataset processing, analysis tools</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Converting text to numbers for neural networks</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BPETokenizer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)</span>  <span class="c1"># [142, 1847, 2341]</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>GPU-accelerated tokenization (production tokenizers use Rust/C++)</p></li>
<li><p>Advanced segmentation algorithms (SentencePiece, Unigram models)</p></li>
<li><p>Language-specific preprocessing (that‚Äôs Module 11: Embeddings)</p></li>
<li><p>Tokenizer serialization and loading (PyTorch handles this with <code class="docutils literal notranslate"><span class="pre">save_pretrained()</span></code>)</p></li>
</ul>
<p><strong>You are building the conceptual foundation.</strong> Production optimizations come later.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section provides a quick reference for the tokenization classes you‚Äôll build. Think of it as your cheat sheet while implementing and debugging.</p>
<section id="base-tokenizer-interface">
<h3>Base Tokenizer Interface<a class="headerlink" href="#base-tokenizer-interface" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Tokenizer</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Abstract base class defining the tokenizer contract</p></li>
<li><p>All tokenizers must implement <code class="docutils literal notranslate"><span class="pre">encode()</span></code> and <code class="docutils literal notranslate"><span class="pre">decode()</span></code></p></li>
</ul>
</section>
<section id="chartokenizer">
<h3>CharTokenizer<a class="headerlink" href="#chartokenizer" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">CharTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Character-level tokenizer treating each character as a token</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab</span></code>: Optional list of characters to include in vocabulary</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">build_vocab</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">build_vocab(corpus:</span> <span class="pre">List[str])</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Extract unique characters from corpus</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">encode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">encode(text:</span> <span class="pre">str)</span> <span class="pre">-&gt;</span> <span class="pre">List[int]</span></code></p></td>
<td><p>Convert text to character IDs</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">decode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">decode(tokens:</span> <span class="pre">List[int])</span> <span class="pre">-&gt;</span> <span class="pre">str</span></code></p></td>
<td><p>Convert character IDs back to text</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vocab</span></code>: List of characters in vocabulary</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>: Total number of unique characters + special tokens</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">char_to_id</span></code>: Mapping from characters to IDs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id_to_char</span></code>: Mapping from IDs to characters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">unk_id</span></code>: ID for unknown characters (always 0)</p></li>
</ul>
</section>
<section id="bpetokenizer">
<h3>BPETokenizer<a class="headerlink" href="#bpetokenizer" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BPETokenizer</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Byte Pair Encoding tokenizer learning subword units</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>: Target vocabulary size after training</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">train</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">train(corpus:</span> <span class="pre">List[str],</span> <span class="pre">vocab_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">None)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Learn BPE merges from corpus</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">encode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">encode(text:</span> <span class="pre">str)</span> <span class="pre">-&gt;</span> <span class="pre">List[int]</span></code></p></td>
<td><p>Convert text to subword token IDs</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">decode</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">decode(tokens:</span> <span class="pre">List[int])</span> <span class="pre">-&gt;</span> <span class="pre">str</span></code></p></td>
<td><p>Convert token IDs back to text</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Helper Methods:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">_get_word_tokens</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">_get_word_tokens(word:</span> <span class="pre">str)</span> <span class="pre">-&gt;</span> <span class="pre">List[str]</span></code></p></td>
<td><p>Convert word to character list with end-of-word marker</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">_get_pairs</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">_get_pairs(word_tokens:</span> <span class="pre">List[str])</span> <span class="pre">-&gt;</span> <span class="pre">Set[Tuple[str,</span> <span class="pre">str]]</span></code></p></td>
<td><p>Extract all adjacent character pairs</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">_apply_merges</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">_apply_merges(tokens:</span> <span class="pre">List[str])</span> <span class="pre">-&gt;</span> <span class="pre">List[str]</span></code></p></td>
<td><p>Apply learned merge rules to token sequence</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">_build_mappings</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">_build_mappings()</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Build token-to-ID and ID-to-token dictionaries</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vocab</span></code>: List of tokens (characters + learned merges)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_size</span></code>: Total vocabulary size</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">merges</span></code>: List of learned merge rules (pair tuples)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">token_to_id</span></code>: Mapping from tokens to IDs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id_to_token</span></code>: Mapping from IDs to tokens</p></li>
</ul>
</section>
<section id="utility-functions">
<h3>Utility Functions<a class="headerlink" href="#utility-functions" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">create_tokenizer</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">create_tokenizer(strategy:</span> <span class="pre">str,</span> <span class="pre">vocab_size:</span> <span class="pre">int,</span> <span class="pre">corpus:</span> <span class="pre">List[str])</span> <span class="pre">-&gt;</span> <span class="pre">Tokenizer</span></code></p></td>
<td><p>Factory for creating tokenizers</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">tokenize_dataset</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">tokenize_dataset(texts:</span> <span class="pre">List[str],</span> <span class="pre">tokenizer:</span> <span class="pre">Tokenizer,</span> <span class="pre">max_length:</span> <span class="pre">int)</span> <span class="pre">-&gt;</span> <span class="pre">List[List[int]]</span></code></p></td>
<td><p>Batch tokenization with length limits</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">analyze_tokenization</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">analyze_tokenization(texts:</span> <span class="pre">List[str],</span> <span class="pre">tokenizer:</span> <span class="pre">Tokenizer)</span> <span class="pre">-&gt;</span> <span class="pre">Dict[str,</span> <span class="pre">float]</span></code></p></td>
<td><p>Compute statistics and metrics</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental ideas you need to understand tokenization deeply. These concepts apply to every NLP system, from simple chatbots to large language models.</p>
<section id="text-to-numbers">
<h3>Text to Numbers<a class="headerlink" href="#text-to-numbers" title="Link to this heading">#</a></h3>
<p>Neural networks process numbers, not text. When you pass the string ‚ÄúHello‚Äù to a model, it must first become a sequence of integers. This transformation happens in four steps: split text into tokens (units of meaning), build a vocabulary mapping each unique token to an integer ID, encode text by looking up each token‚Äôs ID, and enable decoding to reconstruct the original text from IDs.</p>
<p>The simplest approach treats each character as a token. Consider the word ‚Äúhello‚Äù: split into characters <code class="docutils literal notranslate"><span class="pre">['h',</span> <span class="pre">'e',</span> <span class="pre">'l',</span> <span class="pre">'l',</span> <span class="pre">'o']</span></code>, build a vocabulary with IDs <code class="docutils literal notranslate"><span class="pre">{'h':</span> <span class="pre">1,</span> <span class="pre">'e':</span> <span class="pre">2,</span> <span class="pre">'l':</span> <span class="pre">3,</span> <span class="pre">'o':</span> <span class="pre">4}</span></code>, encode to <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">3,</span> <span class="pre">4]</span></code>, and decode back by reversing the lookup. This implementation is beautifully simple:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Encode text to list of character IDs.&quot;&quot;&quot;</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">char_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">char</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_id</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tokens</span>
</pre></div>
</div>
<p>The elegance is in the simplicity: iterate through each character, look up its ID in the vocabulary dictionary, and use the unknown token ID for unseen characters. This gives perfect coverage: any text can be encoded without errors, though the sequences can be long.</p>
</section>
<section id="vocabulary-building">
<h3>Vocabulary Building<a class="headerlink" href="#vocabulary-building" title="Link to this heading">#</a></h3>
<p>Before encoding text, you need a vocabulary: the complete set of tokens your tokenizer recognizes. For character-level tokenization, this means extracting all unique characters from a training corpus.</p>
<p>Here‚Äôs how the vocabulary building process works:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">build_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Build vocabulary from a corpus of text.&quot;&quot;&quot;</span>
    <span class="c1"># Collect all unique characters</span>
    <span class="n">all_chars</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
        <span class="n">all_chars</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1"># Sort for consistent ordering</span>
    <span class="n">unique_chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">all_chars</span><span class="p">))</span>

    <span class="c1"># Rebuild vocabulary with &lt;UNK&gt; token first</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;&lt;UNK&gt;&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">unique_chars</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

    <span class="c1"># Rebuild mappings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">char_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">char</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">id_to_char</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">char</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">char</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)}</span>
</pre></div>
</div>
<p>The special <code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code> token at position 0 handles characters not in the vocabulary. When encoding text with unknown characters, they all map to ID 0. This graceful degradation prevents crashes while signaling that information was lost.</p>
<p>Character vocabularies are tiny (typically 50-200 tokens depending on language), which means small embedding tables. A 100-character vocabulary with 512-dimensional embeddings requires only 51,200 parameters, about 200 KB of memory. This is dramatically smaller than word-level vocabularies with 100,000+ entries.</p>
</section>
<section id="byte-pair-encoding-bpe">
<h3>Byte Pair Encoding (BPE)<a class="headerlink" href="#byte-pair-encoding-bpe" title="Link to this heading">#</a></h3>
<p>Character tokenization has a fatal flaw for neural networks: sequences are too long. A 50-word sentence might produce 250 character tokens. Processing 250 tokens through self-attention layers is computationally expensive, and the model must learn to compose characters into words from scratch.</p>
<p>BPE solves this by learning subword units. The algorithm is elegant: start with a character-level vocabulary, count all adjacent character pairs in the corpus, merge the most frequent pair into a new token, and repeat until reaching the target vocabulary size.</p>
<p>Consider training BPE on the corpus <code class="docutils literal notranslate"><span class="pre">[&quot;hello&quot;,</span> <span class="pre">&quot;hello&quot;,</span> <span class="pre">&quot;help&quot;]</span></code>. Each word starts with end-of-word markers: <code class="docutils literal notranslate"><span class="pre">['h','e','l','l','o&lt;/w&gt;']</span></code>, <code class="docutils literal notranslate"><span class="pre">['h','e','l','l','o&lt;/w&gt;']</span></code>, <code class="docutils literal notranslate"><span class="pre">['h','e','l','p&lt;/w&gt;']</span></code>. Count all pairs: <code class="docutils literal notranslate"><span class="pre">('h','e')</span></code> appears 3 times, <code class="docutils literal notranslate"><span class="pre">('e','l')</span></code> appears 3 times, <code class="docutils literal notranslate"><span class="pre">('l','l')</span></code> appears 2 times. The most frequent is <code class="docutils literal notranslate"><span class="pre">('h','e')</span></code>, so merge it:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># Merge operation: (&#39;h&#39;, &#39;e&#39;) ‚Üí &#39;he&#39;
# Before:
[&#39;h&#39;,&#39;e&#39;,&#39;l&#39;,&#39;l&#39;,&#39;o&lt;/w&gt;&#39;]  ‚Üí  [&#39;he&#39;,&#39;l&#39;,&#39;l&#39;,&#39;o&lt;/w&gt;&#39;]
[&#39;h&#39;,&#39;e&#39;,&#39;l&#39;,&#39;l&#39;,&#39;o&lt;/w&gt;&#39;]  ‚Üí  [&#39;he&#39;,&#39;l&#39;,&#39;l&#39;,&#39;o&lt;/w&gt;&#39;]
[&#39;h&#39;,&#39;e&#39;,&#39;l&#39;,&#39;p&lt;/w&gt;&#39;]      ‚Üí  [&#39;he&#39;,&#39;l&#39;,&#39;p&lt;/w&gt;&#39;]
</pre></div>
</div>
<p>The vocabulary grows from <code class="docutils literal notranslate"><span class="pre">['h','e','l','o','p','&lt;/w&gt;']</span></code> to <code class="docutils literal notranslate"><span class="pre">['h','e','l','o','p','&lt;/w&gt;','he']</span></code>. Continue merging: next most frequent is <code class="docutils literal notranslate"><span class="pre">('l','l')</span></code>, so merge to get <code class="docutils literal notranslate"><span class="pre">'ll'</span></code>. The vocabulary becomes <code class="docutils literal notranslate"><span class="pre">['h','e','l','o','p','&lt;/w&gt;','he','ll']</span></code>. After sufficient merges, ‚Äúhello‚Äù encodes as <code class="docutils literal notranslate"><span class="pre">['he','ll','o&lt;/w&gt;']</span></code> (3 tokens instead of 5 characters).</p>
<p>Here‚Äôs how the training loop works:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">:</span>
    <span class="c1"># Count all pairs across all words</span>
    <span class="n">pair_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">word_freq</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokens</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="n">pairs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_pairs</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
            <span class="n">pair_counts</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freq</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">pair_counts</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Get most frequent pair</span>
    <span class="n">best_pair</span> <span class="o">=</span> <span class="n">pair_counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Merge this pair in all words</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_tokens</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokens</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span>
                <span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span>
                <span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="c1"># Merge pair</span>
                <span class="n">new_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">word_tokens</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_tokens</span>

    <span class="c1"># Add merged token to vocabulary</span>
    <span class="n">merged_token</span> <span class="o">=</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">best_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">merged_token</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">merges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_pair</span><span class="p">)</span>
</pre></div>
</div>
<p>This iterative merging automatically discovers linguistic patterns: common prefixes (‚Äúun‚Äù, ‚Äúre‚Äù), suffixes (‚Äúing‚Äù, ‚Äúed‚Äù), and frequent words become single tokens. The algorithm requires no linguistic knowledge, learning purely from statistics.</p>
</section>
<section id="special-tokens">
<h3>Special Tokens<a class="headerlink" href="#special-tokens" title="Link to this heading">#</a></h3>
<p>Production tokenizers include special tokens beyond <code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code>. Common ones include <code class="docutils literal notranslate"><span class="pre">&lt;PAD&gt;</span></code> for padding sequences to equal length, <code class="docutils literal notranslate"><span class="pre">&lt;BOS&gt;</span></code> (beginning of sequence) and <code class="docutils literal notranslate"><span class="pre">&lt;EOS&gt;</span></code> (end of sequence) for marking boundaries, and <code class="docutils literal notranslate"><span class="pre">&lt;SEP&gt;</span></code> for separating multiple text segments. GPT-style models often use <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> to mark document boundaries.</p>
<p>The choice of special tokens affects the embedding table size. If you reserve 10 special tokens and have a 50,000 token vocabulary, your embedding table has 50,010 rows. Each special token needs learned parameters just like regular tokens.</p>
</section>
<section id="encoding-and-decoding">
<h3>Encoding and Decoding<a class="headerlink" href="#encoding-and-decoding" title="Link to this heading">#</a></h3>
<p>Encoding converts text to token IDs; decoding reverses the process. For BPE, encoding requires applying learned merge rules in order:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Encode text using BPE.&quot;&quot;&quot;</span>
    <span class="c1"># Split text into words</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">all_tokens</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="c1"># Get character-level tokens</span>
        <span class="n">word_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_word_tokens</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

        <span class="c1"># Apply BPE merges</span>
        <span class="n">merged_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_merges</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)</span>

        <span class="n">all_tokens</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">merged_tokens</span><span class="p">)</span>

    <span class="c1"># Convert to IDs</span>
    <span class="n">token_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">all_tokens</span><span class="p">:</span>
        <span class="n">token_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">token_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>  <span class="c1"># 0 = &lt;UNK&gt;</span>

    <span class="k">return</span> <span class="n">token_ids</span>
</pre></div>
</div>
<p>Decoding is simpler: look up each ID, join the tokens, and clean up markers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decode token IDs back to text.&quot;&quot;&quot;</span>
    <span class="c1"># Convert IDs to tokens</span>
    <span class="n">token_strings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">id_to_token</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token_id</span><span class="p">,</span> <span class="s1">&#39;&lt;UNK&gt;&#39;</span><span class="p">)</span>
        <span class="n">token_strings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>

    <span class="c1"># Join and clean up</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">token_strings</span><span class="p">)</span>

    <span class="c1"># Replace end-of-word markers with spaces</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;&lt;/w&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>

    <span class="c1"># Clean up extra spaces</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">text</span>
</pre></div>
</div>
<p>The round-trip text ‚Üí IDs ‚Üí text should be lossless for known vocabulary. Unknown tokens degrade gracefully, mapping to <code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code> in both directions.</p>
</section>
<section id="computational-complexity">
<h3>Computational Complexity<a class="headerlink" href="#computational-complexity" title="Link to this heading">#</a></h3>
<p>Character tokenization is fast: encoding is O(n) where n is the string length (one dictionary lookup per character), and decoding is also O(n) (one reverse lookup per ID). The operations are embarrassingly parallel since each character processes independently.</p>
<p>BPE is slower due to merge rule application. Training BPE scales approximately O(n¬≤ √ó m) where n is corpus size and m is the number of merges. Each merge iteration requires counting all pairs across the entire corpus, then updating token sequences. For a 10,000-word corpus learning 5,000 merges, this can take seconds to minutes depending on implementation.</p>
<p>Encoding with trained BPE is O(n √ó m) where n is text length and m is the number of merge rules. Each merge rule must scan the token sequence looking for applicable pairs. Production tokenizers optimize this with trie data structures and caching, achieving near-linear time.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Character</p></th>
<th class="head"><p>BPE Training</p></th>
<th class="head"><p>BPE Encoding</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Complexity</strong></p></td>
<td><p>O(n)</p></td>
<td><p>O(n¬≤ √ó m)</p></td>
<td><p>O(n √ó m)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Typical Speed</strong></p></td>
<td><p>1-5 ms/1K chars</p></td>
<td><p>100-1000 ms/10K corpus</p></td>
<td><p>5-20 ms/1K chars</p></td>
</tr>
<tr class="row-even"><td><p><strong>Bottleneck</strong></p></td>
<td><p>Dictionary lookup</p></td>
<td><p>Pair frequency counting</p></td>
<td><p>Merge rule application</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="vocabulary-size-versus-sequence-length">
<h3>Vocabulary Size Versus Sequence Length<a class="headerlink" href="#vocabulary-size-versus-sequence-length" title="Link to this heading">#</a></h3>
<p>The fundamental trade-off in tokenization creates a spectrum of choices. Small vocabularies (100-500 tokens) produce long sequences because each token represents little information (individual characters or very common subwords). Large vocabularies (50,000+ tokens) produce short sequences because each token represents more information (whole words or meaningful subword units).</p>
<p>Memory and computation scale oppositely:</p>
<p><strong>Embedding table memory</strong> = vocabulary size √ó embedding dimension √ó bytes per parameter
<strong>Sequence processing cost</strong> = sequence length¬≤ √ó embedding dimension (for attention)</p>
<p>A character tokenizer with vocabulary 100 and embedding dimension 512 needs 100 √ó 512 √ó 4 = 204 KB for embeddings. But a 50-word sentence produces roughly 250 character tokens, requiring 250¬≤ = 62,500 attention computations per layer.</p>
<p>A BPE tokenizer with vocabulary 50,000 and embedding dimension 512 needs 50,000 √ó 512 √ó 4 = 102 MB for embeddings. But that same 50-word sentence might produce only 75 BPE tokens, requiring 75¬≤ = 5,625 attention computations per layer.</p>
<p>The attention cost savings (62,500 vs 5,625) dwarf the embedding memory cost (204 KB vs 102 MB) for models with multiple layers. This is why production language models use large vocabularies: the embedding table fits easily in memory, while shorter sequences dramatically reduce training and inference time.</p>
<p>Modern language models balance these factors:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Vocabulary</p></th>
<th class="head"><p>Strategy</p></th>
<th class="head"><p>Sequence Length (typical)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>GPT-2/3</strong></p></td>
<td><p>50,257</p></td>
<td><p>BPE</p></td>
<td><p>~50-200 tokens per sentence</p></td>
</tr>
<tr class="row-odd"><td><p><strong>BERT</strong></p></td>
<td><p>30,522</p></td>
<td><p>WordPiece</p></td>
<td><p>~40-150 tokens per sentence</p></td>
</tr>
<tr class="row-even"><td><p><strong>T5</strong></p></td>
<td><p>32,128</p></td>
<td><p>SentencePiece</p></td>
<td><p>~40-180 tokens per sentence</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Character</strong></p></td>
<td><p>~100</p></td>
<td><p>Character</p></td>
<td><p>~250-1000 tokens per sentence</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-production-tokenizers">
<h3>Your Implementation vs. Production Tokenizers<a class="headerlink" href="#your-implementation-vs-production-tokenizers" title="Link to this heading">#</a></h3>
<p>Your TinyTorch tokenizers demonstrate the core algorithms, but production tokenizers optimize for speed and scale. The conceptual differences are minimal: the same BPE algorithm, the same vocabulary mappings, the same encode/decode operations. The implementation differences are dramatic.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>Hugging Face Tokenizers</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Language</strong></p></td>
<td><p>Pure Python</p></td>
<td><p>Rust (compiled to native code)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Speed</strong></p></td>
<td><p>1-10 ms/sentence</p></td>
<td><p>0.01-0.1 ms/sentence (100√ó faster)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Parallelization</strong></p></td>
<td><p>Single-threaded</p></td>
<td><p>Multi-threaded with Rayon</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Vocabulary storage</strong></p></td>
<td><p>Python dict</p></td>
<td><p>Trie data structure</p></td>
</tr>
<tr class="row-even"><td><p><strong>Special features</strong></p></td>
<td><p>Basic encode/decode</p></td>
<td><p>Padding, truncation, attention masks</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Pretrained models</strong></p></td>
<td><p>Train from scratch</p></td>
<td><p>Load from Hugging Face Hub</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows equivalent tokenization in TinyTorch and Hugging Face. Notice how the high-level API mirrors production tools, making your learning transferable.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.tokenization</span><span class="w"> </span><span class="kn">import</span> <span class="n">BPETokenizer</span>

<span class="c1"># Train tokenizer on corpus</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;hello world&quot;</span><span class="p">,</span> <span class="s2">&quot;machine learning&quot;</span><span class="p">]</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BPETokenizer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># Encode text</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;hello machine&quot;</span>
<span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># Decode back to text</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
‚ö° Hugging Face</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">trainers</span>

<span class="c1"># Train tokenizer on corpus (same algorithm!)</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;hello world&quot;</span><span class="p">,</span> <span class="s2">&quot;machine learning&quot;</span><span class="p">]</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">BPE</span><span class="p">())</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="o">.</span><span class="n">BpeTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>

<span class="c1"># Encode text</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;hello machine&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">token_ids</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">ids</span>

<span class="c1"># Decode back to text</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs walk through each section to understand the comparison:</p>
<ul class="simple">
<li><p><strong>Lines 1-3 (Imports)</strong>: TinyTorch exposes <code class="docutils literal notranslate"><span class="pre">BPETokenizer</span></code> directly from the tokenization module. Hugging Face uses a more modular design with separate <code class="docutils literal notranslate"><span class="pre">models</span></code> and <code class="docutils literal notranslate"><span class="pre">trainers</span></code> for flexibility across algorithms (BPE, WordPiece, Unigram).</p></li>
<li><p><strong>Lines 5-8 (Training)</strong>: Both train on the same corpus using the same BPE algorithm. TinyTorch uses a simpler API with <code class="docutils literal notranslate"><span class="pre">train()</span></code> method. Hugging Face separates model definition from training for composability, but the underlying algorithm is identical.</p></li>
<li><p><strong>Lines 10-12 (Encoding)</strong>: TinyTorch returns a list of integers directly. Hugging Face returns an <code class="docutils literal notranslate"><span class="pre">Encoding</span></code> object with additional metadata (attention masks, offsets, etc.), and you extract the IDs with <code class="docutils literal notranslate"><span class="pre">.ids</span></code> attribute. Same numerical result.</p></li>
<li><p><strong>Lines 14-15 (Decoding)</strong>: Both use <code class="docutils literal notranslate"><span class="pre">decode()</span></code> with the token ID list. Output is identical. The core operation is the same: look up each ID in the vocabulary and join the tokens.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>The BPE algorithm, merge rule learning, vocabulary structure, and encode/decode logic. When you debug tokenization issues in production, you‚Äôll understand exactly what‚Äôs happening because you built the same system.</p>
</div>
</section>
<section id="why-tokenization-matters-at-scale">
<h3>Why Tokenization Matters at Scale<a class="headerlink" href="#why-tokenization-matters-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate why tokenization choices matter, consider the scale of modern systems:</p>
<ul class="simple">
<li><p><strong>GPT-3 training</strong>: Processing 300 billion tokens required careful vocabulary selection. Using character tokenization would have increased sequence lengths by 3-4√ó, multiplying training time by 9-16√ó (quadratic attention cost).</p></li>
<li><p><strong>Embedding table memory</strong>: A 50,000 token vocabulary with 12,288-dimensional embeddings (GPT-3 size) requires 50,000 √ó 12,288 √ó 4 bytes = <strong>2.4 GB</strong> just for the embedding layer. This is ~0.14% of GPT-3‚Äôs 175 billion total parameters, a reasonable fraction.</p></li>
<li><p><strong>Real-time inference</strong>: Chatbots must tokenize user input in milliseconds. Python tokenizers take 5-20 ms per sentence; Rust tokenizers take 0.05-0.2 ms. At 1 million requests per day, this saves ~5 hours of compute time daily.</p></li>
</ul>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions. They‚Äôre designed to build intuition for the performance characteristics you‚Äôll encounter in production NLP systems.</p>
<p><strong>Q1: Vocabulary Memory Calculation</strong></p>
<p>You train a BPE tokenizer with <code class="docutils literal notranslate"><span class="pre">vocab_size=30,000</span></code> for a production model. If using 768-dimensional embeddings with float32 precision, how much memory does the embedding table require?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>30,000 √ó 768 √ó 4 bytes = <strong>92,160,000 bytes ‚âà 92.16 MB</strong></p>
<p>Breakdown:</p>
<ul class="simple">
<li><p>Vocabulary size: 30,000 tokens</p></li>
<li><p>Embedding dimension: 768 (BERT-base size)</p></li>
<li><p>Float32: 4 bytes per parameter</p></li>
<li><p>Total parameters: 30,000 √ó 768 = 23,040,000</p></li>
<li><p>Memory: 23.04M √ó 4 = 92.16 MB</p></li>
</ul>
<p>This is why vocabulary size matters! Doubling to 60K vocab would double embedding memory to ~184 MB.</p>
</div>
<p><strong>Q2: Sequence Length Trade-offs</strong></p>
<p>A sentence contains 200 characters. With character tokenization it produces 200 tokens. With BPE it produces 50 tokens (4:1 compression). If processing batch size 32 with attention:</p>
<ul class="simple">
<li><p>How many attention computations for character tokenization per batch?</p></li>
<li><p>How many for BPE tokenization per batch?</p></li>
</ul>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Character tokenization:</strong></p>
<ul class="simple">
<li><p>Sequence length: 200 tokens</p></li>
<li><p>Attention per sequence: 200¬≤ = 40,000 operations</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total: 32 √ó 40,000 = <strong>1,280,000 attention operations</strong></p></li>
</ul>
<p><strong>BPE tokenization:</strong></p>
<ul class="simple">
<li><p>Sequence length: 50 tokens (200 chars √∑ 4)</p></li>
<li><p>Attention per sequence: 50¬≤ = 2,500 operations</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total: 32 √ó 2,500 = <strong>80,000 attention operations</strong></p></li>
</ul>
<p>BPE is <strong>16√ó faster</strong> for attention! This is why modern models use subword tokenization despite larger embedding tables.</p>
</div>
<p><strong>Q3: Unknown Token Handling</strong></p>
<p>Your BPE tokenizer encounters the word ‚Äúsupercalifragilistic‚Äù (not in training corpus). Character tokenizer maps it to 22 known tokens. BPE tokenizer decomposes it into subwords like <code class="docutils literal notranslate"><span class="pre">['super',</span> <span class="pre">'cal',</span> <span class="pre">'ifr',</span> <span class="pre">'ag',</span> <span class="pre">'il',</span> <span class="pre">'istic']</span></code> (6 tokens). Which is better?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>BPE is better for production:</strong></p>
<ul class="simple">
<li><p><strong>Efficiency</strong>: 6 tokens vs 22 tokens = 3.7√ó shorter sequence</p></li>
<li><p><strong>Semantics</strong>: Subwords like ‚Äúsuper‚Äù and ‚Äúistic‚Äù carry meaning; individual characters don‚Äôt</p></li>
<li><p><strong>Generalization</strong>: Model learns that ‚Äúsuper‚Äù prefix modifies meaning (superman, supermarket)</p></li>
<li><p><strong>Memory</strong>: 6¬≤ = 36 attention computations vs 22¬≤ = 484 (13√ó faster)</p></li>
</ul>
<p><strong>Character tokenization advantages:</strong></p>
<ul class="simple">
<li><p><strong>Perfect coverage</strong>: Never maps to <code class="docutils literal notranslate"><span class="pre">&lt;UNK&gt;</span></code>, always recovers original text</p></li>
<li><p><strong>Simplicity</strong>: No complex merge rules or training</p></li>
</ul>
<p>For rare/unknown words, BPE‚Äôs subword decomposition provides better semantic understanding and efficiency, which is why GPT, BERT, and T5 all use variants of subword tokenization.</p>
</div>
<p><strong>Q4: Compression Ratio Analysis</strong></p>
<p>You analyze two tokenizers on a 10,000 character corpus:</p>
<ul class="simple">
<li><p>Character tokenizer: 10,000 tokens</p></li>
<li><p>BPE tokenizer: 2,500 tokens</p></li>
</ul>
<p>What‚Äôs the compression ratio, and what does it tell you about efficiency?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Compression ratio: 10,000 √∑ 2,500 = 4.0</strong></p>
<p>This means each BPE token represents an average of 4 characters.</p>
<p><strong>Efficiency implications:</strong></p>
<ul class="simple">
<li><p><strong>Sequence processing</strong>: 4√ó shorter sequences = 16√ó faster attention (quadratic scaling)</p></li>
<li><p><strong>Context window</strong>: With max length 512, character tokenizer handles 512 chars (~100 words); BPE handles 2,048 chars (~400 words)</p></li>
<li><p><strong>Information density</strong>: Each BPE token carries more semantic information (subword vs character)</p></li>
</ul>
<p><strong>Trade-off</strong>: BPE vocabulary is ~100√ó larger (10K tokens vs 100), increasing embedding memory from ~200 KB to ~20 MB. This trade-off heavily favors BPE for models with multiple transformer layers where attention cost dominates.</p>
</div>
<p><strong>Q5: Training Corpus Size Impact</strong></p>
<p>Training BPE on 1,000 words takes 100ms. How long will 10,000 words take? What about 100,000 words?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>BPE training scales approximately <strong>O(n¬≤)</strong> where n is corpus size (due to repeated pair counting across the corpus).</p>
<ul class="simple">
<li><p><strong>1,000 words</strong>: 100 ms (baseline)</p></li>
<li><p><strong>10,000 words</strong>: ~10,000 ms = 10 seconds (100√ó longer, due to 10¬≤ scaling)</p></li>
<li><p><strong>100,000 words</strong>: ~1,000,000 ms = 1,000 seconds ‚âà <strong>16.7 minutes</strong> (10,000√ó longer)</p></li>
</ul>
<p><strong>Production strategies to handle this:</strong></p>
<ul class="simple">
<li><p>Sample representative subset (~50K-100K sentences is usually sufficient)</p></li>
<li><p>Use incremental/online BPE that doesn‚Äôt recount all pairs each iteration</p></li>
<li><p>Parallelize pair counting across corpus chunks</p></li>
<li><p>Cache frequent pair statistics</p></li>
<li><p>Use optimized implementations (Rust, C++) that are 100-1000√ó faster</p></li>
</ul>
<p>Note: Encoding with trained BPE is much faster (linear time), only training is slow.</p>
</div>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>For students who want to understand the academic foundations and production implementations of tokenization:</p>
<section id="seminal-papers">
<h3>Seminal Papers<a class="headerlink" href="#seminal-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Neural Machine Translation of Rare Words with Subword Units</strong> - Sennrich et al. (2016). The original BPE paper that introduced subword tokenization for neural machine translation. Shows how BPE handles rare words through decomposition and achieves better translation quality. <a class="reference external" href="https://arxiv.org/abs/1508.07909">arXiv:1508.07909</a></p></li>
<li><p><strong>SentencePiece: A simple and language independent approach to subword tokenization</strong> - Kudo &amp; Richardson (2018). Extends BPE with language-agnostic tokenization that handles raw text without pre-tokenization. Used in T5, ALBERT, and many multilingual models. <a class="reference external" href="https://arxiv.org/abs/1808.06226">arXiv:1808.06226</a></p></li>
<li><p><strong>BERT: Pre-training of Deep Bidirectional Transformers</strong> - Devlin et al. (2018). While primarily about transformers, introduces WordPiece tokenization used in BERT family models. Section 4.1 discusses vocabulary and tokenization choices. <a class="reference external" href="https://arxiv.org/abs/1810.04805">arXiv:1810.04805</a></p></li>
</ul>
</section>
<section id="additional-resources">
<h3>Additional Resources<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Library</strong>: <a class="reference external" href="https://github.com/huggingface/tokenizers">Hugging Face Tokenizers</a> - Production Rust implementation with Python bindings. Explore the source to see optimized BPE.</p></li>
<li><p><strong>Tutorial</strong>: ‚ÄúByte Pair Encoding Tokenization‚Äù - <a class="reference external" href="https://huggingface.co/learn/nlp-course/chapter6/5">Hugging Face Course</a> - Interactive tutorial showing BPE in action with visualizations</p></li>
<li><p><strong>Textbook</strong>: ‚ÄúSpeech and Language Processing‚Äù by Jurafsky &amp; Martin - Chapter 2 covers tokenization, including Unicode handling and language-specific issues</p></li>
</ul>
</section>
</section>
<section id="whats-next">
<h2>What‚Äôs Next<a class="headerlink" href="#whats-next" title="Link to this heading">#</a></h2>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Coming Up: Module 11 - Embeddings</p>
<p>Convert your token IDs into learnable dense vector representations. You‚Äôll implement embedding tables that transform discrete tokens into continuous vectors, enabling neural networks to capture semantic relationships in text.</p>
</div>
<p><strong>Preview - How Your Tokenizer Gets Used in Future Modules:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>What It Does</p></th>
<th class="head"><p>Your Tokenization In Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>11: Embeddings</strong></p></td>
<td><p>Learnable lookup tables</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">embedding</span> <span class="pre">=</span> <span class="pre">Embedding(vocab_size=1000,</span> <span class="pre">dim=128)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>12: Attention</strong></p></td>
<td><p>Sequence-to-sequence processing</p></td>
<td><p>Token sequences attend to each other</p></td>
</tr>
<tr class="row-even"><td><p><strong>13: Transformers</strong></p></td>
<td><p>Complete language models</p></td>
<td><p>Full pipeline: tokenize ‚Üí embed ‚Üí attend ‚Üí predict</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?filepath=tinytorch/src/10_tokenization/10_tokenization.py">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://colab.research.google.com/github/harvard-edge/cs249r_book/blob/main/tinytorch/src/10_tokenization/10_tokenization.py">Open in Colab</a></strong> - Use Google Colab for cloud compute</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/src/10_tokenization/10_tokenization.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder and Colab sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="09_convolutions_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 09: Spatial</p>
      </div>
    </a>
    <a class="right-next"
       href="11_embeddings_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Module 11: Embeddings</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#base-tokenizer-interface">Base Tokenizer Interface</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chartokenizer">CharTokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bpetokenizer">BPETokenizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utility-functions">Utility Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-numbers">Text to Numbers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary-building">Vocabulary Building</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#special-tokens">Special Tokens</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#encoding-and-decoding">Encoding and Decoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity">Computational Complexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary-size-versus-sequence-length">Vocabulary Size Versus Sequence Length</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-production-tokenizers">Your Implementation vs. Production Tokenizers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-tokenization-matters-at-scale">Why Tokenization Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>