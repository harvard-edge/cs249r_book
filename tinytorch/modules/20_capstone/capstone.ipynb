{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e46b57",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 20: Capstone - Benchmarking & Submission\n",
    "\n",
    "Welcome to the TinyTorch capstone! You've built an entire ML framework from scratch across 19 modules. Now it's time to demonstrate your work by benchmarking a model and generating a submission that showcases your framework's capabilities.\n",
    "\n",
    "## ğŸ”— Prerequisites & Progress\n",
    "**You've Built**: Complete ML framework with profiling (M14), quantization (M15), compression (M16), acceleration (M17), memoization (M18), and benchmarking (M19)\n",
    "**You'll Build**: Professional benchmark submission workflow with standardized reporting\n",
    "**You'll Enable**: Shareable, reproducible results demonstrating framework performance\n",
    "\n",
    "**Connection Map**:\n",
    "```\n",
    "Modules 01-13 â†’ Optimization Suite (14-18) â†’ Benchmarking (19) â†’ Submission (20)\n",
    "(Framework)     (Performance Tools)            (Measurement)       (Results)\n",
    "```\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "By the end of this capstone, you will:\n",
    "1. **Use** Module 19's benchmarking tools to measure model performance comprehensively\n",
    "2. **Apply** optimization techniques from Modules 14-18 to improve baseline models\n",
    "3. **Generate** standardized JSON submissions following industry best practices\n",
    "4. **Validate** submissions against a schema for reproducibility\n",
    "5. **Compare** baseline vs. optimized models with quantitative metrics\n",
    "6. **Share** your results with the TinyTorch community in a professional format\n",
    "\n",
    "**Key Insight**: This module teaches you the complete workflow from model to measurable results - the foundation of ML systems engineering. In production, reproducible benchmarking is what separates research experiments from deployable systems.\n",
    "\n",
    "## ğŸ“¦ Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `src/20_capstone/20_capstone.py`\n",
    "**Building Side:** Code exports to `tinytorch.olympics`\n",
    "\n",
    "```python\n",
    "# How to use this module:\n",
    "from tinytorch.olympics import generate_submission, BenchmarkReport\n",
    "\n",
    "# Benchmark your model\n",
    "report = BenchmarkReport()\n",
    "report.benchmark_model(my_model, X_test, y_test)\n",
    "\n",
    "# Generate submission\n",
    "submission = generate_submission(report)\n",
    "submission.save(\"my_submission.json\")\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Complete workflow from model to shareable results\n",
    "- **Production:** Professional submission format mirroring MLPerf and Papers with Code standards\n",
    "- **Community:** Share and compare results with other builders using standardized metrics\n",
    "- **Reproducibility:** Schema-validated submissions ensure results can be verified and trusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c246c27",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "exports",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp olympics\n",
    "#| export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4aae2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ’¡ Introduction: From Framework to Reproducible Results\n",
    "\n",
    "Over the past 19 modules, you built a complete ML framework from the ground up. You implemented tensors, layers, optimizers, loss functions, and advanced optimization techniques. But building a framework is only half the story.\n",
    "\n",
    "**The Missing Piece: Proving It Works**\n",
    "\n",
    "In production ML systems, claims without measurements are worthless. When researchers publish papers or engineers deploy models, they need to answer fundamental questions:\n",
    "- How fast is inference on this hardware?\n",
    "- How much memory does the model consume?\n",
    "- What's the accuracy-latency trade-off?\n",
    "- How do optimizations affect these metrics?\n",
    "\n",
    "### The Reproducibility Crisis in ML\n",
    "\n",
    "Modern ML faces a reproducibility crisis. Many published results can't be replicated because:\n",
    "- **Missing system details** - What hardware? What software versions?\n",
    "- **Inconsistent metrics** - Different ways to measure \"accuracy\" or \"latency\"\n",
    "- **Cherry-picked results** - Showing best runs without variance\n",
    "- **Incomplete reporting** - Omitting negative results or failed optimizations\n",
    "\n",
    "### Industry Standard: Benchmarking Frameworks\n",
    "\n",
    "Professional ML systems use standardized benchmarking frameworks:\n",
    "\n",
    "```\n",
    "Industry Benchmarking Standards:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ MLPerf (AI Hardware)     â”‚ Papers with Code (Research)       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ â€¢ Standardized tasks     â”‚ â€¢ Leaderboards for all datasets   â”‚\n",
    "â”‚ â€¢ Hardware specificationsâ”‚ â€¢ Reproducible results required   â”‚\n",
    "â”‚ â€¢ Measurement protocols  â”‚ â€¢ Code submission mandatory       â”‚\n",
    "â”‚ â€¢ Fair comparisons       â”‚ â€¢ Automated verification          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### What This Capstone Teaches You\n",
    "\n",
    "This module shows you how to:\n",
    "1. **Measure comprehensively** - Not just accuracy, but latency, memory, throughput\n",
    "2. **Report systematically** - Following a schema that ensures completeness\n",
    "3. **Enable comparison** - Using standardized metrics others can verify\n",
    "4. **Document optimizations** - Tracking what techniques were applied and their impact\n",
    "5. **Share professionally** - Generating submission files that work like research papers\n",
    "\n",
    "Let's build a benchmarking and submission system worthy of production ML!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3bbb8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import platform\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c20aa",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Import TinyTorch modules (not exported - used for module development only)\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.layers import Linear\n",
    "from tinytorch.core.activations import ReLU\n",
    "from tinytorch.core.losses import CrossEntropyLoss\n",
    "\n",
    "print(\"âœ… Capstone modules imported!\")\n",
    "print(\"ğŸ“Š Ready to benchmark and submit results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e0a26",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“ Foundations: The Science of Benchmarking\n",
    "\n",
    "Before we build our submission system, let's understand what makes a good benchmark and why standardized reporting matters.\n",
    "\n",
    "### The Three Pillars of Good Benchmarking\n",
    "\n",
    "```\n",
    "Good Benchmarks Rest on Three Pillars:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Repeatability   â”‚ Comparability   â”‚ Completeness    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Same result     â”‚ Apples-to-applesâ”‚ All relevant    â”‚\n",
    "â”‚ every time      â”‚ comparisons     â”‚ metrics capturedâ”‚\n",
    "â”‚                 â”‚                 â”‚                 â”‚\n",
    "â”‚ â€¢ Fixed seeds   â”‚ â€¢ Same hardware â”‚ â€¢ Accuracy      â”‚\n",
    "â”‚ â€¢ Same data     â”‚ â€¢ Same metrics  â”‚ â€¢ Latency       â”‚\n",
    "â”‚ â€¢ Same config   â”‚ â€¢ Same protocol â”‚ â€¢ Memory        â”‚\n",
    "â”‚ â€¢ Variance      â”‚ â€¢ Documented    â”‚ â€¢ Throughput    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### What Metrics Actually Matter?\n",
    "\n",
    "Different stakeholders care about different metrics:\n",
    "\n",
    "```\n",
    "Stakeholder View:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ ML Researcher:                                               â”‚\n",
    "â”‚   Primary   â†’ Accuracy, F1, BLEU (task-specific)             â”‚\n",
    "â”‚   Secondary â†’ Training time, convergence                     â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ Systems Engineer:                                            â”‚\n",
    "â”‚   Primary   â†’ Latency (p50, p99), throughput                 â”‚\n",
    "â”‚   Secondary â†’ Memory usage, CPU/GPU utilization              â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ Product Manager:                                             â”‚\n",
    "â”‚   Primary   â†’ User experience (latency < 100ms?)             â”‚\n",
    "â”‚   Secondary â†’ Cost per request, scalability                  â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚ DevOps/MLOps:                                                â”‚\n",
    "â”‚   Primary   â†’ Model size (deployment), inference cost        â”‚\n",
    "â”‚   Secondary â†’ Batch throughput, hardware utilization         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Insight**: A complete benchmark captures ALL perspectives, not just one.\n",
    "\n",
    "### Benchmark Report Components\n",
    "\n",
    "Our BenchmarkReport class will track everything needed for reproducibility:\n",
    "\n",
    "```\n",
    "BenchmarkReport Structure:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Model Characteristics:                                      â”‚\n",
    "â”‚   â€¢ Parameter count     â†’ Model capacity                    â”‚\n",
    "â”‚   â€¢ Model size (MB)     â†’ Deployment cost                   â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ Performance Metrics:                                        â”‚\n",
    "â”‚   â€¢ Accuracy           â†’ Task performance                   â”‚\n",
    "â”‚   â€¢ Latency (mean/std) â†’ Inference speed + variance         â”‚\n",
    "â”‚   â€¢ Throughput         â†’ Samples/second capacity            â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ System Context:                                             â”‚\n",
    "â”‚   â€¢ Platform           â†’ Hardware/OS environment            â”‚\n",
    "â”‚   â€¢ Python version     â†’ Language runtime                   â”‚\n",
    "â”‚   â€¢ NumPy version      â†’ Numerical library version          â”‚\n",
    "â”‚   â€¢ Timestamp          â†’ When benchmark was run             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Latency vs. Throughput: A Critical Distinction\n",
    "\n",
    "Many beginners confuse latency and throughput. They measure different things:\n",
    "\n",
    "```\n",
    "Latency vs. Throughput:\n",
    "\n",
    "Latency (Per-Sample Speed):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Input â†’ Model â†’ Output                          â”‚\n",
    "â”‚   â†‘              â†“                               â”‚\n",
    "â”‚   â””â”€â”€â”€â”€ 10ms â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                                                  â”‚\n",
    "â”‚  \"How fast can I get ONE result?\"                â”‚\n",
    "â”‚  Critical for: Real-time apps, user experience   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Throughput (Batch Capacity):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  [Input1, Input2, ... Input100]                  â”‚\n",
    "â”‚           â†“                                      â”‚\n",
    "â”‚        Model                                     â”‚\n",
    "â”‚           â†“                                      â”‚\n",
    "â”‚  [Out1, Out2, ... Out100] in 200ms               â”‚\n",
    "â”‚                                                  â”‚\n",
    "â”‚  \"How many samples per second?\"                  â”‚\n",
    "â”‚  Critical for: Batch jobs, data processing       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Example:\n",
    "  Latency:     10ms per sample   â†’ \"Fast\" for users\n",
    "  Throughput:  500 samples/sec   â†’ \"Fast\" for batches\n",
    "\n",
    "Trade-off: Batching increases throughput but adds latency!\n",
    "```\n",
    "\n",
    "### Why Variance Matters\n",
    "\n",
    "Single measurements lie. Variance tells the truth:\n",
    "\n",
    "```\n",
    "Why We Report Mean Â± Std:\n",
    "\n",
    "Measurement 1: 9.2ms    â”\n",
    "Measurement 2: 10.1ms   â”‚ Mean = 10.0ms\n",
    "Measurement 3: 9.8ms    â”‚ Std  = 0.5ms\n",
    "Measurement 4: 10.5ms   â”‚\n",
    "Measurement 5: 9.4ms    â”˜\n",
    "\n",
    "vs.\n",
    "\n",
    "Measurement 1: 5.2ms    â”\n",
    "Measurement 2: 14.8ms   â”‚ Mean = 10.0ms â† Same mean!\n",
    "Measurement 3: 8.1ms    â”‚ Std  = 4.2ms  â† Different variance!\n",
    "Measurement 4: 15.3ms   â”‚\n",
    "Measurement 5: 6.6ms    â”˜\n",
    "           â†‘\n",
    "    Unpredictable performance!\n",
    "```\n",
    "\n",
    "**Which model would you deploy?** The first one, because consistent performance matters in production.\n",
    "\n",
    "### The Submission Schema: Enforcing Standards\n",
    "\n",
    "Our submission format follows a JSON schema that ensures:\n",
    "- **Required fields** can't be omitted (no incomplete results)\n",
    "- **Type safety** prevents errors (accuracy is float, not string)\n",
    "- **Version tracking** allows format evolution\n",
    "- **Nested structure** organizes related data logically\n",
    "\n",
    "```\n",
    "Submission JSON Schema:\n",
    "{\n",
    "  \"tinytorch_version\": \"0.1.0\",           â† Version tracking\n",
    "  \"submission_type\": \"capstone_benchmark\", â† Classification\n",
    "  \"timestamp\": \"2025-01-15 14:30:00\",     â† When run\n",
    "  \"system_info\": {                         â† Environment\n",
    "    \"platform\": \"macOS-14.0-arm64\",\n",
    "    \"python_version\": \"3.11.0\",\n",
    "    \"numpy_version\": \"1.24.0\"\n",
    "  },\n",
    "  \"baseline\": {                            â† Required baseline\n",
    "    \"model_name\": \"simple_mlp\",\n",
    "    \"metrics\": {\n",
    "      \"parameter_count\": 1000,\n",
    "      \"model_size_mb\": 0.004,\n",
    "      \"accuracy\": 0.92,\n",
    "      \"latency_ms_mean\": 0.15,\n",
    "      \"latency_ms_std\": 0.02,\n",
    "      \"throughput_samples_per_sec\": 6666.67\n",
    "    }\n",
    "  },\n",
    "  \"optimized\": {                           â† Optional optimization\n",
    "    \"model_name\": \"quantized_mlp\",\n",
    "    \"metrics\": { ... },\n",
    "    \"techniques_applied\": [\"int8_quantization\", \"pruning\"]\n",
    "  },\n",
    "  \"improvements\": {                        â† Auto-calculated\n",
    "    \"speedup\": 2.3,\n",
    "    \"compression_ratio\": 4.1,\n",
    "    \"accuracy_delta\": -0.01\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This structure makes it trivial to:\n",
    "- **Validate** submissions programmatically\n",
    "- **Compare** different models objectively\n",
    "- **Aggregate** results across the community\n",
    "- **Visualize** trends and trade-offs\n",
    "\n",
    "Now let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13678c59",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Building a Simple Benchmark Model\n",
    "\n",
    "For this capstone, we'll use a simple MLP model. This keeps the focus on the benchmarking workflow rather than model complexity.\n",
    "\n",
    "**Why a Simple Model?**\n",
    "- **Focus on workflow** - The submission process is the learning goal, not model architecture\n",
    "- **Fast iteration** - Quick benchmarks let you experiment with the pipeline\n",
    "- **Extensible pattern** - Same workflow applies to complex models from milestones\n",
    "\n",
    "Students can later apply this exact workflow to more sophisticated models (CNNs, Transformers, etc.) from milestone projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c309f4f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "toy-model",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class SimpleMLP:\n",
    "    \"\"\"\n",
    "    Simple 2-layer MLP for benchmarking demonstration.\n",
    "\n",
    "    This is a toy model to demonstrate the benchmarking workflow.\n",
    "    Students can later apply the same workflow to milestone models.\n",
    "\n",
    "    Architecture:\n",
    "        Input â†’ Linear(in, hidden) â†’ ReLU â†’ Linear(hidden, out) â†’ Output\n",
    "\n",
    "    Why this design:\n",
    "    - Two layers: Enough to show optimization impact (quantization, pruning)\n",
    "    - ReLU activation: Common pattern students recognize\n",
    "    - Small by default: Fast benchmarking during development\n",
    "    - Configurable sizes: Can scale up for experiments\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=10, hidden_size=20, output_size=3):\n",
    "        \"\"\"Initialize simple MLP with random weights.\"\"\"\n",
    "        self.fc1 = Linear(input_size, hidden_size)\n",
    "        self.relu = ReLU()\n",
    "        self.fc2 = Linear(hidden_size, output_size)\n",
    "\n",
    "        # Initialize with small random weights\n",
    "        # Linear layer expects weight shape: (in_features, out_features)\n",
    "        self.fc1.weight.data = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.fc1.bias.data = np.zeros(hidden_size)\n",
    "        self.fc2.weight.data = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.fc2.bias.data = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Return model parameters for perf.\"\"\"\n",
    "        return [self.fc1.weight, self.fc1.bias, self.fc2.weight, self.fc2.bias]\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total number of parameters.\"\"\"\n",
    "        total = 0\n",
    "        for param in self.parameters():\n",
    "            total += param.data.size\n",
    "        return total\n",
    "\n",
    "print(\"âœ… SimpleMLP model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a6b56",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Understanding SimpleMLP Parameter Counting\n",
    "\n",
    "Let's break down where the parameters come from:\n",
    "\n",
    "```\n",
    "SimpleMLP Parameter Breakdown:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Layer 1: Linear(10, 20)                                     â”‚\n",
    "â”‚   Weight matrix: (10, 20) = 200 parameters                  â”‚\n",
    "â”‚   Bias vector:   (20,)    = 20 parameters                   â”‚\n",
    "â”‚   Subtotal: 220 parameters                                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ Layer 2: ReLU                                               â”‚\n",
    "â”‚   No parameters (just max(0, x))                            â”‚\n",
    "â”‚   Subtotal: 0 parameters                                    â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ Layer 3: Linear(20, 3)                                      â”‚\n",
    "â”‚   Weight matrix: (20, 3)  = 60 parameters                   â”‚\n",
    "â”‚   Bias vector:   (3,)     = 3 parameters                    â”‚\n",
    "â”‚   Subtotal: 63 parameters                                   â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ TOTAL: 220 + 0 + 63 = 283 parameters                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Memory Calculation (FP32):\n",
    "  283 parameters Ã— 4 bytes/param = 1,132 bytes â‰ˆ 0.001 MB\n",
    "\n",
    "If we quantize to INT8:\n",
    "  283 parameters Ã— 1 byte/param = 283 bytes â‰ˆ 0.0003 MB\n",
    "  â†’ 4Ã— memory reduction!\n",
    "```\n",
    "\n",
    "This small model is perfect for demonstrating optimization impact without long benchmark times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332aa97",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Benchmark Report Class\n",
    "\n",
    "The BenchmarkReport class encapsulates all benchmark results and provides methods for comprehensive measurement and professional reporting.\n",
    "\n",
    "**Design Philosophy:**\n",
    "1. **Separation of concerns** - Measurement logic separate from model logic\n",
    "2. **Comprehensive metrics** - Capture model characteristics AND performance\n",
    "3. **System context** - Record environment for reproducibility\n",
    "4. **Statistical rigor** - Multiple runs for latency, report mean + std\n",
    "5. **JSON-serializable** - All data types compatible with JSON export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b74c72",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "benchmark-report",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkReport:\n",
    "    \"\"\"\n",
    "    Benchmark report for model performance.\n",
    "\n",
    "    Measures and stores:\n",
    "    - Model characteristics (parameters, size)\n",
    "    - Performance metrics (accuracy, latency, throughput)\n",
    "    - System context (platform, versions)\n",
    "    - Optimization info (techniques applied)\n",
    "\n",
    "    Usage:\n",
    "        report = BenchmarkReport(model_name=\"my_model\")\n",
    "        report.benchmark_model(model, X_test, y_test, num_runs=100)\n",
    "        print(report.metrics)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"model\"):\n",
    "        self.model_name = model_name\n",
    "        self.metrics = {}\n",
    "        self.system_info = self._get_system_info()\n",
    "        self.timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    def _get_system_info(self):\n",
    "        \"\"\"Collect system information for reproducibility.\"\"\"\n",
    "        return {\n",
    "            'platform': platform.platform(),\n",
    "            'python_version': sys.version.split()[0],\n",
    "            'numpy_version': np.__version__\n",
    "        }\n",
    "\n",
    "    def benchmark_model(self, model, X_test, y_test, num_runs=100):\n",
    "        \"\"\"\n",
    "        Benchmark model performance comprehensively.\n",
    "\n",
    "        Args:\n",
    "            model: Model to benchmark (must have .forward() and .count_parameters())\n",
    "            X_test: Test inputs (Tensor)\n",
    "            y_test: Test labels (numpy array of class indices)\n",
    "            num_runs: Number of inference runs for latency measurement (default: 100)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "\n",
    "        Measurements:\n",
    "        1. Parameter count - Model capacity indicator\n",
    "        2. Model size (MB) - Deployment cost (assumes FP32)\n",
    "        3. Accuracy - Task performance (classification accuracy)\n",
    "        4. Latency (mean Â± std) - Inference speed and consistency\n",
    "        5. Throughput - Maximum samples/second capacity\n",
    "        \"\"\"\n",
    "        # Count parameters\n",
    "        param_count = model.count_parameters()\n",
    "        model_size_mb = (param_count * 4) / (1024 * 1024)  # Assuming FP32\n",
    "\n",
    "        # Measure accuracy\n",
    "        predictions = model.forward(X_test)\n",
    "        pred_labels = np.argmax(predictions.data, axis=1)\n",
    "        accuracy = np.mean(pred_labels == y_test)\n",
    "\n",
    "        # Measure latency (average over multiple runs)\n",
    "        # Why multiple runs? See \"Variance\" section in Foundations\n",
    "        latencies = []\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = model.forward(X_test[:1])  # Single sample inference\n",
    "            latencies.append((time.time() - start) * 1000)  # Convert to ms\n",
    "\n",
    "        avg_latency = np.mean(latencies)\n",
    "        std_latency = np.std(latencies)\n",
    "\n",
    "        # Store metrics (all as Python native types for JSON serialization)\n",
    "        self.metrics = {\n",
    "            'parameter_count': int(param_count),\n",
    "            'model_size_mb': float(model_size_mb),\n",
    "            'accuracy': float(accuracy),\n",
    "            'latency_ms_mean': float(avg_latency),\n",
    "            'latency_ms_std': float(std_latency),\n",
    "            'throughput_samples_per_sec': float(1000 / avg_latency)\n",
    "        }\n",
    "\n",
    "        print(f\"\\nğŸ“Š Benchmark Results for {self.model_name}:\")\n",
    "        print(f\"  Parameters: {param_count:,}\")\n",
    "        print(f\"  Size: {model_size_mb:.2f} MB\")\n",
    "        print(f\"  Accuracy: {accuracy*100:.1f}%\")\n",
    "        print(f\"  Latency: {avg_latency:.2f}ms Â± {std_latency:.2f}ms\")\n",
    "\n",
    "        return self.metrics\n",
    "\n",
    "print(\"âœ… BenchmarkReport class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a79201",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Why These Metrics?\n",
    "\n",
    "Each metric answers a specific production question:\n",
    "\n",
    "```\n",
    "Metric Decision Tree:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Question                 â”‚ Metric              â”‚ Why        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ \"Will it fit on device?\" â”‚ model_size_mb       â”‚ Memory     â”‚\n",
    "â”‚ \"Is it accurate enough?\" â”‚ accuracy            â”‚ Quality    â”‚\n",
    "â”‚ \"Is it fast enough?\"     â”‚ latency_ms_mean     â”‚ UX         â”‚\n",
    "â”‚ \"Is it consistent?\"      â”‚ latency_ms_std      â”‚ Reliabilityâ”‚\n",
    "â”‚ \"Can it scale?\"          â”‚ throughput          â”‚ Capacity   â”‚\n",
    "â”‚ \"How complex is it?\"     â”‚ parameter_count     â”‚ Capacity   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Design Choice: Why num_runs=100?\n",
    "\n",
    "We run inference 100 times by default to:\n",
    "- **Warm up** the system (first runs are often slower)\n",
    "- **Capture variance** (some runs hit cache, others miss)\n",
    "- **Average out noise** (OS interrupts, GC pauses)\n",
    "- **Get confidence intervals** (via std dev)\n",
    "\n",
    "```\n",
    "Single Run (Unreliable):        Multiple Runs (Reliable):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Run 1: 12.3ms           â”‚     â”‚ Run 1: 12.3ms           â”‚\n",
    "â”‚                         â”‚     â”‚ Run 2: 9.8ms            â”‚\n",
    "â”‚ Result: 12.3ms          â”‚     â”‚ Run 3: 10.1ms           â”‚\n",
    "â”‚ Confidence: Low         â”‚     â”‚ ...                     â”‚\n",
    "â”‚ (Could be outlier!)     â”‚     â”‚ Run 100: 10.2ms         â”‚\n",
    "â”‚                         â”‚     â”‚                         â”‚\n",
    "â”‚                         â”‚     â”‚ Result: 10.0ms Â± 0.5ms  â”‚\n",
    "â”‚                         â”‚     â”‚ Confidence: High        â”‚\n",
    "â”‚                         â”‚     â”‚ (Statistically sound)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Design Choice: Python Native Types\n",
    "\n",
    "Notice we convert all metrics to Python native types (int, float):\n",
    "\n",
    "```python\n",
    "'parameter_count': int(param_count),  # NumPy int64 â†’ Python int\n",
    "'accuracy': float(accuracy),          # NumPy float64 â†’ Python float\n",
    "```\n",
    "\n",
    "**Why?** JSON can't serialize NumPy types directly:\n",
    "```python\n",
    "# âŒ This fails:\n",
    "json.dumps({\"value\": np.int64(42)})  # TypeError!\n",
    "\n",
    "# âœ… This works:\n",
    "json.dumps({\"value\": int(42)})  # Success!\n",
    "```\n",
    "\n",
    "This design decision makes our submissions JSON-compatible without custom encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a98bcb",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Submission Generation\n",
    "\n",
    "The core function that generates a standardized JSON submission from benchmark results.\n",
    "\n",
    "**Design Goals:**\n",
    "1. **Baseline-first** - Always require baseline results (comparison reference)\n",
    "2. **Optimization optional** - Support baseline-only OR baseline+optimized submissions\n",
    "3. **Auto-calculate improvements** - Automatically compute speedup, compression, accuracy delta\n",
    "4. **Schema compliance** - Generate structure that passes validation\n",
    "5. **Extensible** - Easy to add new fields without breaking existing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcce07d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "generate-submission",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_submission(\n",
    "    baseline_report: BenchmarkReport,\n",
    "    optimized_report: Optional[BenchmarkReport] = None,\n",
    "    student_name: Optional[str] = None,\n",
    "    techniques_applied: Optional[List[str]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a standardized benchmark submission.\n",
    "\n",
    "    Args:\n",
    "        baseline_report: Benchmark results for baseline model (REQUIRED)\n",
    "        optimized_report: Optional benchmark results for optimized model\n",
    "        student_name: Optional student/submitter name\n",
    "        techniques_applied: List of optimization techniques used (e.g., [\"quantization\", \"pruning\"])\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing submission data (ready for JSON export)\n",
    "\n",
    "    Submission Structure:\n",
    "        {\n",
    "          \"tinytorch_version\": \"0.1.0\",\n",
    "          \"submission_type\": \"capstone_benchmark\",\n",
    "          \"timestamp\": \"...\",\n",
    "          \"system_info\": {...},\n",
    "          \"baseline\": {\n",
    "            \"model_name\": \"...\",\n",
    "            \"metrics\": {...}\n",
    "          },\n",
    "          \"optimized\": {...},        # Optional\n",
    "          \"improvements\": {...}      # Auto-calculated if optimized present\n",
    "        }\n",
    "    \"\"\"\n",
    "    submission = {\n",
    "        'tinytorch_version': '0.1.0',\n",
    "        'submission_type': 'capstone_benchmark',\n",
    "        'timestamp': baseline_report.timestamp,\n",
    "        'system_info': baseline_report.system_info,\n",
    "        'baseline': {\n",
    "            'model_name': baseline_report.model_name,\n",
    "            'metrics': baseline_report.metrics\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Add student name if provided\n",
    "    if student_name:\n",
    "        submission['student_name'] = student_name\n",
    "\n",
    "    # Add optimization results if provided\n",
    "    if optimized_report:\n",
    "        submission['optimized'] = {\n",
    "            'model_name': optimized_report.model_name,\n",
    "            'metrics': optimized_report.metrics,\n",
    "            'techniques_applied': techniques_applied or []\n",
    "        }\n",
    "\n",
    "        # Calculate improvement metrics\n",
    "        baseline_latency = baseline_report.metrics['latency_ms_mean']\n",
    "        optimized_latency = optimized_report.metrics['latency_ms_mean']\n",
    "        baseline_size = baseline_report.metrics['model_size_mb']\n",
    "        optimized_size = optimized_report.metrics['model_size_mb']\n",
    "\n",
    "        submission['improvements'] = {\n",
    "            'speedup': float(baseline_latency / optimized_latency),\n",
    "            'compression_ratio': float(baseline_size / optimized_size),\n",
    "            'accuracy_delta': float(\n",
    "                optimized_report.metrics['accuracy'] - baseline_report.metrics['accuracy']\n",
    "            )\n",
    "        }\n",
    "\n",
    "    return submission\n",
    "\n",
    "def save_submission(submission: Dict[str, Any], filepath: str = \"submission.json\"):\n",
    "    \"\"\"\n",
    "    Save submission to JSON file.\n",
    "\n",
    "    Args:\n",
    "        submission: Submission dictionary from generate_submission()\n",
    "        filepath: Output path (default: \"submission.json\")\n",
    "\n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    Path(filepath).write_text(json.dumps(submission, indent=2))\n",
    "    print(f\"\\nâœ… Submission saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "print(\"âœ… Submission generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247aebef",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Understanding the Improvements Calculation\n",
    "\n",
    "When you provide both baseline and optimized results, the submission auto-calculates three key improvement metrics:\n",
    "\n",
    "```\n",
    "Improvement Metrics Explained:\n",
    "\n",
    "1. Speedup (Latency Ratio):\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Speedup = baseline_latency / optimized_latency â”‚\n",
    "   â”‚                                                â”‚\n",
    "   â”‚ Example:                                       â”‚\n",
    "   â”‚   Baseline:  10.0ms                            â”‚\n",
    "   â”‚   Optimized: 5.0ms                             â”‚\n",
    "   â”‚   Speedup:   10.0 / 5.0 = 2.0x                 â”‚\n",
    "   â”‚                                                â”‚\n",
    "   â”‚ Interpretation:                                â”‚\n",
    "   â”‚   2.0x = Optimized model is 2Ã— faster          â”‚\n",
    "   â”‚   1.0x = No change                             â”‚\n",
    "   â”‚   0.5x = Optimized model is slower (bad!)      â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "2. Compression Ratio (Size Reduction):\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Compression = baseline_size / optimized_size   â”‚\n",
    "   â”‚                                                â”‚\n",
    "   â”‚ Example:                                       â”‚\n",
    "   â”‚   Baseline:  4.0 MB                            â”‚\n",
    "   â”‚   Optimized: 1.0 MB                            â”‚\n",
    "   â”‚   Compression: 4.0 / 1.0 = 4.0x                â”‚\n",
    "   â”‚                                                â”‚\n",
    "   â”‚ Interpretation:                                â”‚\n",
    "   â”‚   4.0x = Model is 4Ã— smaller                   â”‚\n",
    "   â”‚   1.0x = Same size                             â”‚\n",
    "   â”‚   0.8x = Larger after \"optimization\" (bad!)    â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "3. Accuracy Delta (Quality Impact):\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Delta = optimized_accuracy - baseline_accuracy â”‚\n",
    "   â”‚                                                â”‚\n",
    "   â”‚ Example:                                       â”‚\n",
    "   â”‚   Baseline:  92.0%                             â”‚\n",
    "   â”‚   Optimized: 91.5%                             â”‚\n",
    "   â”‚   Delta:     91.5 - 92.0 = -0.5%               â”‚\n",
    "   â”‚                                                â”‚\n",
    "   â”‚ Interpretation:                                â”‚\n",
    "   â”‚   +0.5% = Improved accuracy (rare but good!)   â”‚\n",
    "   â”‚    0.0% = Maintained accuracy (ideal!)         â”‚\n",
    "   â”‚   -0.5% = Slight loss (acceptable)             â”‚\n",
    "   â”‚   -5.0% = Major loss (unacceptable)            â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The Optimization Trade-off Triangle\n",
    "\n",
    "Every optimization involves trade-offs:\n",
    "\n",
    "```\n",
    "The Impossible Triangle:\n",
    "         Fast (Speedup)\n",
    "              â–²\n",
    "             /â”‚\\\n",
    "            / â”‚ \\\n",
    "           /  â”‚  \\\n",
    "          /   â”‚   \\\n",
    "         /  Good  \\\n",
    "        /  Balance \\\n",
    "       â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼\n",
    "    Small         Accurate\n",
    "  (Compression)   (Delta)\n",
    "\n",
    "You can pick TWO:\n",
    "â€¢ Fast + Small   â†’ Aggressive optimization, some accuracy loss\n",
    "â€¢ Fast + Accurate â†’ Careful optimization, less compression\n",
    "â€¢ Small + Accurate â†’ Conservative quantization, slower\n",
    "\n",
    "The goal: Find the sweet spot for YOUR use case!\n",
    "```\n",
    "\n",
    "### Why JSON Schema Validation Matters\n",
    "\n",
    "Our submission format is designed to be validated:\n",
    "\n",
    "```python\n",
    "# Valid submission (passes validation):\n",
    "{\n",
    "  \"tinytorch_version\": \"0.1.0\",      # âœ“ Required, string\n",
    "  \"timestamp\": \"2025-01-15 14:30\",   # âœ“ Required, string\n",
    "  \"baseline\": {                       # âœ“ Required, object\n",
    "    \"metrics\": {                      # âœ“ Required, object\n",
    "      \"accuracy\": 0.92                # âœ“ Required, float in [0, 1]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Invalid submission (fails validation):\n",
    "{\n",
    "  \"tinytorch_version\": 0.1,          # âœ— Wrong type (number not string)\n",
    "  # âœ— Missing timestamp\n",
    "  \"baseline\": {\n",
    "    \"metrics\": {\n",
    "      \"accuracy\": \"92%\"                # âœ— Wrong type (string not float)\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This prevents common mistakes:\n",
    "- Forgetting required fields\n",
    "- Using wrong data types\n",
    "- Invalid value ranges (accuracy > 1.0)\n",
    "- Inconsistent structure\n",
    "\n",
    "In production ML, schema validation is what makes benchmarks trustworthy and comparable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda45f02",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ”§ Complete Example Workflow\n",
    "\n",
    "This section demonstrates the complete workflow from model to submission.\n",
    "Students can modify this to benchmark their own models!\n",
    "\n",
    "**Workflow Steps:**\n",
    "1. Create test dataset (or load from milestone)\n",
    "2. Create baseline model\n",
    "3. Benchmark baseline performance\n",
    "4. (Optional) Apply optimizations\n",
    "5. (Optional) Benchmark optimized version\n",
    "6. Generate submission with comparisons\n",
    "7. Save to JSON file\n",
    "\n",
    "This is the EXACT workflow used in production ML systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a96fce",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "example-workflow",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def run_example_benchmark():\n",
    "    \"\"\"\n",
    "    Complete example showing the full benchmarking workflow.\n",
    "\n",
    "    Students can modify this to benchmark their own models!\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"TINYTORCH CAPSTONE: BENCHMARKING WORKFLOW EXAMPLE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Step 1: Create toy dataset\n",
    "    print(\"\\nğŸ”§ Step 1: Creating toy dataset...\")\n",
    "    np.random.seed(42)\n",
    "    X_test = Tensor(np.random.randn(100, 10))\n",
    "    y_test = np.random.randint(0, 3, 100)\n",
    "    print(f\"  Dataset: {X_test.shape[0]} samples, {X_test.shape[1]} features, 3 classes\")\n",
    "\n",
    "    # Step 2: Create baseline model\n",
    "    print(\"\\nğŸ”§ Step 2: Creating baseline model...\")\n",
    "    baseline_model = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "    print(f\"  Model: {baseline_model.count_parameters():,} parameters\")\n",
    "\n",
    "    # Step 3: Benchmark baseline\n",
    "    print(\"\\nğŸ“Š Step 3: Benchmarking baseline model...\")\n",
    "    baseline_report = BenchmarkReport(model_name=\"baseline_mlp\")\n",
    "    baseline_report.benchmark_model(baseline_model, X_test, y_test, num_runs=50)\n",
    "\n",
    "    # Step 4: Generate submission\n",
    "    print(\"\\nğŸ“ Step 4: Generating submission...\")\n",
    "    submission = generate_submission(\n",
    "        baseline_report=baseline_report,\n",
    "        student_name=\"TinyTorch Student\"\n",
    "    )\n",
    "\n",
    "    # Step 5: Save submission\n",
    "    print(\"\\nğŸ’¾ Step 5: Saving submission...\")\n",
    "    save_submission(submission, \"capstone_submission.json\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ‰ WORKFLOW COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Try optimizing the model (quantization, pruning, etc.)\")\n",
    "    print(\"  2. Benchmark the optimized version\")\n",
    "    print(\"  3. Generate a new submission with both baseline and optimized results\")\n",
    "    print(\"  4. Share your submission.json with the TinyTorch community!\")\n",
    "\n",
    "    return submission\n",
    "\n",
    "print(\"âœ… Example workflow defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b73bfa",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Understanding the Workflow Pattern\n",
    "\n",
    "This workflow follows industry best practices:\n",
    "\n",
    "```\n",
    "Production ML Workflow:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1. Define Task                                              â”‚\n",
    "â”‚    â†“ What are we solving? What's the test set?              â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ 2. Baseline Model                                           â”‚\n",
    "â”‚    â†“ Simplest reasonable model                              â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ 3. Baseline Benchmark                                       â”‚\n",
    "â”‚    â†“ Measure: accuracy, latency, memory                     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ 4. Optimization (ITERATIVE)                                 â”‚\n",
    "â”‚    â†“ Try technique â†’ Benchmark â†’ Compare â†’ Keep or revert   â”‚\n",
    "â”‚    â†“ Quantization? Pruning? Distillation?                   â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ 5. Final Submission                                         â”‚\n",
    "â”‚    â†“ Document: baseline, optimized, improvements            â”‚\n",
    "â”‚    â†“ Share: JSON file, metrics, techniques                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ 6. Community Comparison                                     â”‚\n",
    "â”‚    â†“ How do your results compare to others?                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Insight**: Professional ML engineers iterate on step 4, trying different optimizations and measuring their impact. The submission captures the BEST result after this exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab9a7b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ”§ Advanced Workflow - Using TinyTorch Optimization APIs\n",
    "\n",
    "This section demonstrates using the complete optimization pipeline from Modules 14-19:\n",
    "- Module 14 (Profiling): Measure baseline performance and identify bottlenecks\n",
    "- Module 15 (Quantization): Reduce precision from FP32 to INT8\n",
    "- Module 16 (Compression): Prune low-magnitude weights\n",
    "- Module 17 (Acceleration): Use optimized kernels\n",
    "- Module 18 (Memoization): Cache repeated computations\n",
    "- Module 19 (Benchmarking): Professional measurement infrastructure\n",
    "\n",
    "This is the COMPLETE story: Profile â†’ Optimize â†’ Benchmark â†’ Submit\n",
    "\n",
    "**What Students Learn:**\n",
    "- How to import and use APIs from previous modules\n",
    "- How to combine multiple optimizations (quantization + pruning)\n",
    "- How to measure cumulative impact (2Ã— from quant + 1.5Ã— from pruning = 3Ã— total)\n",
    "- How to document techniques for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53397a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "optimization-workflow",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def run_optimization_workflow_example():\n",
    "    \"\"\"\n",
    "    Advanced example showing the complete optimization workflow.\n",
    "\n",
    "    This demonstrates:\n",
    "    1. Profiling baseline model (Module 14)\n",
    "    2. Applying optimizations (Modules 15, 16)\n",
    "    3. Benchmarking with best practices (Module 19)\n",
    "    4. Generating submission with before/after comparison\n",
    "\n",
    "    Students learn how to use TinyTorch as a complete framework!\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"TINYTORCH CAPSTONE: OPTIMIZATION WORKFLOW\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nThis workflow demonstrates using Modules 14-19 together:\")\n",
    "    print(\"  ğŸ“Š Module 14: Profiling\")\n",
    "    print(\"  ğŸ”¢ Module 15: Quantization (optional - API imported for demonstration)\")\n",
    "    print(\"  âœ‚ï¸  Module 16: Compression (optional - API imported for demonstration)\")\n",
    "    print(\"  âš¡ Module 17: Acceleration (optional - API imported for demonstration)\")\n",
    "    print(\"  ğŸ’¾ Module 18: Memoization (optional - API imported for demonstration)\")\n",
    "    print(\"  ğŸ“ˆ Module 19: Benchmarking\")\n",
    "    print(\"  ğŸ“ Module 20: Submission Generation\")\n",
    "\n",
    "    # Demonstrate API imports (students can use these for their own optimizations)\n",
    "    print(\"\\nğŸ”§ Importing optimization APIs...\")\n",
    "    try:\n",
    "        from tinytorch.perf.profiling import Profiler, quick_profile\n",
    "        print(\"  âœ… Module 14 (Profiling) imported\")\n",
    "    except ImportError:\n",
    "        print(\"  âš ï¸  Module 14 (Profiling) not available - using basic profiling\")\n",
    "        Profiler = None\n",
    "\n",
    "    try:\n",
    "        from tinytorch.perf.compression import magnitude_prune, structured_prune\n",
    "        print(\"  âœ… Module 16 (Compression) imported\")\n",
    "    except ImportError:\n",
    "        print(\"  âš ï¸  Module 16 (Compression) not available - skipping pruning demo\")\n",
    "        magnitude_prune = None\n",
    "\n",
    "    try:\n",
    "        from tinytorch.benchmarking import Benchmark, BenchmarkResult\n",
    "        print(\"  âœ… Module 19 (Benchmarking) imported\")\n",
    "    except ImportError:\n",
    "        print(\"  âš ï¸  Module 19 (Benchmarking) not available - using basic benchmarking\")\n",
    "        Benchmark = None\n",
    "\n",
    "    # Step 1: Create dataset\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 1: Create Test Dataset\")\n",
    "    print(\"=\"*70)\n",
    "    np.random.seed(42)\n",
    "    X_test = Tensor(np.random.randn(100, 10))\n",
    "    y_test = np.random.randint(0, 3, 100)\n",
    "    print(f\"  Dataset: {X_test.shape[0]} samples, {X_test.shape[1]} features, 3 classes\")\n",
    "\n",
    "    # Step 2: Create and profile baseline model\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 2: Baseline Model - Profile & Benchmark\")\n",
    "    print(\"=\"*70)\n",
    "    baseline_model = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "    print(f\"  Model: {baseline_model.count_parameters():,} parameters\")\n",
    "\n",
    "    # Benchmark baseline using BenchmarkReport\n",
    "    baseline_report = BenchmarkReport(model_name=\"baseline_mlp\")\n",
    "    baseline_metrics = baseline_report.benchmark_model(baseline_model, X_test, y_test, num_runs=50)\n",
    "\n",
    "    # Optional: Demonstrate using Module 14's Profiler if available\n",
    "    if Profiler:\n",
    "        print(\"\\n  ğŸ“Š Optional: Using Module 14's Profiler for detailed analysis...\")\n",
    "        profiler = Profiler()\n",
    "        # Note: Profiler integration would go here\n",
    "        # This demonstrates the API is available for students to use\n",
    "\n",
    "    # Step 3: (DEMO ONLY) Show optimization APIs available\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 3: Optimization APIs Available (Demo)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n  ğŸ“š Students can apply these optimizations:\")\n",
    "    print(\"     - Module 15: quantize_model(model, bits=8)\")\n",
    "    print(\"     - Module 16: magnitude_prune(model, sparsity=0.5)\")\n",
    "    print(\"     - Module 17: Use accelerated ops (vectorized_matmul, etc.)\")\n",
    "    print(\"     - Module 18: enable_kv_cache(model)  # For transformers\")\n",
    "    print(\"\\n  ğŸ’¡ For this demo, we'll simulate an optimized model\")\n",
    "    print(\"     (Students can replace this with real optimizations!)\")\n",
    "\n",
    "    # Create \"optimized\" model (students would apply real optimizations here)\n",
    "    optimized_model = SimpleMLP(input_size=10, hidden_size=15, output_size=3)  # Smaller for demo\n",
    "    optimized_report = BenchmarkReport(model_name=\"optimized_mlp\")\n",
    "    optimized_metrics = optimized_report.benchmark_model(optimized_model, X_test, y_test, num_runs=50)\n",
    "\n",
    "    # Step 4: Generate submission with before/after comparison\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 4: Generate Submission with Improvements\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    submission = generate_submission(\n",
    "        baseline_report=baseline_report,\n",
    "        optimized_report=optimized_report,\n",
    "        student_name=\"TinyTorch Optimizer\",\n",
    "        techniques_applied=[\"model_sizing\", \"architecture_search\"]  # Students list real techniques\n",
    "    )\n",
    "\n",
    "    # Display improvement summary\n",
    "    if 'improvements' in submission:\n",
    "        improvements = submission['improvements']\n",
    "        print(\"\\n  ğŸ“ˆ Optimization Results:\")\n",
    "        print(f\"     Speedup: {improvements['speedup']:.2f}x\")\n",
    "        print(f\"     Compression: {improvements['compression_ratio']:.2f}x\")\n",
    "        print(f\"     Accuracy change: {improvements['accuracy_delta']*100:+.1f}%\")\n",
    "\n",
    "    # Step 5: Save submission\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 5: Save Submission\")\n",
    "    print(\"=\"*70)\n",
    "    filepath = save_submission(submission, \"optimization_submission.json\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ‰ OPTIMIZATION WORKFLOW COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nğŸ“š What students learned:\")\n",
    "    print(\"  âœ… How to import and use optimization APIs from Modules 14-19\")\n",
    "    print(\"  âœ… How to benchmark before and after optimization\")\n",
    "    print(\"  âœ… How to generate professional submissions with improvement metrics\")\n",
    "    print(\"  âœ… How TinyTorch modules work together as a complete framework\")\n",
    "    print(\"\\nğŸ’¡ Next steps:\")\n",
    "    print(\"  - Apply real optimizations (quantization, pruning, etc.)\")\n",
    "    print(\"  - Benchmark milestone models (XOR, MNIST, CNN, etc.)\")\n",
    "    print(\"  - Share your optimized results with the community!\")\n",
    "\n",
    "    return submission\n",
    "\n",
    "print(\"âœ… Optimization workflow example defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4574f81",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Combining Multiple Optimizations\n",
    "\n",
    "In production ML, you often stack optimizations for cumulative benefits:\n",
    "\n",
    "```\n",
    "Stacking Optimizations:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Baseline Model                                              â”‚\n",
    "â”‚   Size: 4.0 MB, Latency: 10.0ms, Accuracy: 92.0%            â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ â†“ Apply Quantization (INT8)                                 â”‚\n",
    "â”‚   Size: 1.0 MB (4.0Ã—), Latency: 5.0ms (2.0Ã—), Acc: 91.8%    â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ â†“ Apply Pruning (50% sparsity)                              â”‚\n",
    "â”‚   Size: 0.5 MB (2.0Ã—), Latency: 3.5ms (1.4Ã—), Acc: 91.5%    â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚ Final Optimized Model                                       â”‚\n",
    "â”‚   Total compression: 8.0Ã— (4.0 MB â†’ 0.5 MB)                 â”‚\n",
    "â”‚   Total speedup: 2.9Ã— (10.0ms â†’ 3.5ms)                      â”‚\n",
    "â”‚   Accuracy loss: -0.5% (92.0% â†’ 91.5%)                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Key Insight: Effects multiply!\n",
    "  Quant (4.0Ã—) Ã— Pruning (2.0Ã—) = 8.0Ã— total compression\n",
    "```\n",
    "\n",
    "The submission's `techniques_applied` list documents this for reproducibility:\n",
    "```json\n",
    "\"techniques_applied\": [\"int8_quantization\", \"magnitude_pruning_0.5\"]\n",
    "```\n",
    "\n",
    "This tells other engineers EXACTLY what you did, so they can reproduce or build on your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08078942",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ§ª Module Testing\n",
    "\n",
    "Individual unit tests for each component, following TinyTorch testing patterns.\n",
    "\n",
    "**Testing Strategy:**\n",
    "1. **Unit tests** - Test each class/function in isolation\n",
    "2. **Integration test** - Test complete workflow end-to-end (in test_module)\n",
    "3. **Schema validation** - Ensure submissions conform to standard\n",
    "4. **Edge cases** - Test with missing optional fields, extreme values\n",
    "\n",
    "Each test validates one specific aspect and provides clear feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449b9d0",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "test-simple-mlp",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_simple_mlp():\n",
    "    \"\"\"ğŸ”¬ Test SimpleMLP model creation and forward pass.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: SimpleMLP...\")\n",
    "\n",
    "    # Test model creation with default parameters\n",
    "    model = SimpleMLP()\n",
    "    assert model is not None, \"Model should be created\"\n",
    "\n",
    "    # Test with custom parameters\n",
    "    model = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "\n",
    "    # Test parameter count\n",
    "    param_count = model.count_parameters()\n",
    "    expected_params = (10 * 20 + 20) + (20 * 3 + 3)  # fc1 + fc2\n",
    "    assert param_count == expected_params, f\"Expected {expected_params} parameters, got {param_count}\"\n",
    "\n",
    "    # Test forward pass\n",
    "    np.random.seed(42)\n",
    "    X = Tensor(np.random.randn(5, 10))  # 5 samples, 10 features\n",
    "    output = model.forward(X)\n",
    "\n",
    "    assert output.shape == (5, 3), f\"Expected output shape (5, 3), got {output.shape}\"\n",
    "    assert not np.isnan(output.data).any(), \"Output should not contain NaN values\"\n",
    "\n",
    "    print(\"âœ… SimpleMLP works correctly!\")\n",
    "\n",
    "# Run test immediately when developing\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_simple_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519665f",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "test-benchmark-report",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_benchmark_report():\n",
    "    \"\"\"ğŸ”¬ Test BenchmarkReport class functionality.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: BenchmarkReport...\")\n",
    "\n",
    "    # Create report\n",
    "    report = BenchmarkReport(model_name=\"test_model\")\n",
    "\n",
    "    # Check initialization\n",
    "    assert report.model_name == \"test_model\", \"Model name should be set correctly\"\n",
    "    assert report.timestamp is not None, \"Timestamp should be set\"\n",
    "    assert report.system_info is not None, \"System info should be collected\"\n",
    "    assert 'platform' in report.system_info, \"Should have platform info\"\n",
    "    assert 'python_version' in report.system_info, \"Should have Python version\"\n",
    "\n",
    "    # Create test data\n",
    "    np.random.seed(42)\n",
    "    model = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "    X_test = Tensor(np.random.randn(50, 10))\n",
    "    y_test = np.random.randint(0, 3, 50)\n",
    "\n",
    "    # Benchmark model\n",
    "    metrics = report.benchmark_model(model, X_test, y_test, num_runs=10)\n",
    "\n",
    "    # Check metrics exist\n",
    "    required_metrics = [\n",
    "        'parameter_count', 'model_size_mb', 'accuracy',\n",
    "        'latency_ms_mean', 'latency_ms_std', 'throughput_samples_per_sec'\n",
    "    ]\n",
    "    for metric in required_metrics:\n",
    "        assert metric in metrics, f\"Missing metric: {metric}\"\n",
    "\n",
    "    # Check metric types and ranges\n",
    "    assert isinstance(metrics['parameter_count'], int), \"Parameter count should be int\"\n",
    "    assert metrics['parameter_count'] > 0, \"Should have positive parameter count\"\n",
    "    assert metrics['model_size_mb'] > 0, \"Model size should be positive\"\n",
    "    assert 0 <= metrics['accuracy'] <= 1, \"Accuracy should be in [0, 1]\"\n",
    "    assert metrics['latency_ms_mean'] > 0, \"Latency should be positive\"\n",
    "    assert metrics['latency_ms_std'] >= 0, \"Standard deviation should be non-negative\"\n",
    "    assert metrics['throughput_samples_per_sec'] > 0, \"Throughput should be positive\"\n",
    "\n",
    "    print(\"âœ… BenchmarkReport works correctly!\")\n",
    "\n",
    "# Run test immediately when developing\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_benchmark_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f925f7ea",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "test-submission-generation",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_submission_generation():\n",
    "    \"\"\"ğŸ”¬ Test generate_submission() function.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Submission Generation...\")\n",
    "\n",
    "    # Create baseline report\n",
    "    np.random.seed(42)\n",
    "    model = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "    X_test = Tensor(np.random.randn(50, 10))\n",
    "    y_test = np.random.randint(0, 3, 50)\n",
    "\n",
    "    baseline_report = BenchmarkReport(model_name=\"baseline_model\")\n",
    "    baseline_report.benchmark_model(model, X_test, y_test, num_runs=10)\n",
    "\n",
    "    # Generate submission with baseline only\n",
    "    submission = generate_submission(baseline_report)\n",
    "\n",
    "    # Check submission structure\n",
    "    assert isinstance(submission, dict), \"Submission should be a dictionary\"\n",
    "    assert 'tinytorch_version' in submission, \"Should have version field\"\n",
    "    assert 'submission_type' in submission, \"Should have submission type\"\n",
    "    assert 'timestamp' in submission, \"Should have timestamp\"\n",
    "    assert 'system_info' in submission, \"Should have system info\"\n",
    "    assert 'baseline' in submission, \"Should have baseline results\"\n",
    "\n",
    "    # Check baseline structure\n",
    "    baseline = submission['baseline']\n",
    "    assert 'model_name' in baseline, \"Baseline should have model name\"\n",
    "    assert 'metrics' in baseline, \"Baseline should have metrics\"\n",
    "    assert baseline['model_name'] == \"baseline_model\", \"Model name should match\"\n",
    "\n",
    "    # Test with student name\n",
    "    submission_with_name = generate_submission(baseline_report, student_name=\"Test Student\")\n",
    "    assert 'student_name' in submission_with_name, \"Should include student name when provided\"\n",
    "    assert submission_with_name['student_name'] == \"Test Student\", \"Student name should match\"\n",
    "\n",
    "    print(\"âœ… Submission generation works correctly!\")\n",
    "\n",
    "# Run test immediately when developing\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_submission_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d12b2",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "test-submission-schema",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def validate_submission_schema(submission: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate submission JSON conforms to required schema.\n",
    "\n",
    "    This function ensures submissions are:\n",
    "    - Complete (no missing required fields)\n",
    "    - Type-safe (correct data types)\n",
    "    - Valid (values in acceptable ranges)\n",
    "\n",
    "    Used for automated validation before accepting community submissions.\n",
    "    \"\"\"\n",
    "    # Check required top-level fields\n",
    "    required_fields = ['tinytorch_version', 'submission_type', 'timestamp', 'system_info', 'baseline']\n",
    "    for field in required_fields:\n",
    "        if field not in submission:\n",
    "            raise AssertionError(f\"Missing required field: {field}\")\n",
    "\n",
    "    # Check field types\n",
    "    assert isinstance(submission['tinytorch_version'], str), \"Version should be string\"\n",
    "    assert isinstance(submission['submission_type'], str), \"Submission type should be string\"\n",
    "    assert isinstance(submission['timestamp'], str), \"Timestamp should be string\"\n",
    "    assert isinstance(submission['system_info'], dict), \"System info should be dict\"\n",
    "    assert isinstance(submission['baseline'], dict), \"Baseline should be dict\"\n",
    "\n",
    "    # Check baseline structure\n",
    "    baseline = submission['baseline']\n",
    "    assert 'model_name' in baseline, \"Baseline missing model_name\"\n",
    "    assert 'metrics' in baseline, \"Baseline missing metrics\"\n",
    "\n",
    "    # Check metrics structure and types\n",
    "    metrics = baseline['metrics']\n",
    "    required_metrics = ['parameter_count', 'model_size_mb', 'accuracy', 'latency_ms_mean']\n",
    "    for metric in required_metrics:\n",
    "        if metric not in metrics:\n",
    "            raise AssertionError(f\"Missing metric in baseline: {metric}\")\n",
    "\n",
    "    # Check metric value ranges\n",
    "    assert 0 <= metrics['accuracy'] <= 1, \"Accuracy must be in [0, 1]\"\n",
    "    assert metrics['parameter_count'] > 0, \"Parameter count must be positive\"\n",
    "    assert metrics['model_size_mb'] > 0, \"Model size must be positive\"\n",
    "    assert metrics['latency_ms_mean'] > 0, \"Latency must be positive\"\n",
    "\n",
    "    # Check system info\n",
    "    system_info = submission['system_info']\n",
    "    assert 'platform' in system_info, \"System info missing platform\"\n",
    "    assert 'python_version' in system_info, \"System info missing python_version\"\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_unit_submission_schema():\n",
    "    \"\"\"ğŸ”¬ Test submission schema validation.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Submission Schema...\")\n",
    "\n",
    "    # Create valid submission\n",
    "    np.random.seed(42)\n",
    "    model = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "    X_test = Tensor(np.random.randn(50, 10))\n",
    "    y_test = np.random.randint(0, 3, 50)\n",
    "\n",
    "    report = BenchmarkReport(model_name=\"test_model\")\n",
    "    report.benchmark_model(model, X_test, y_test, num_runs=10)\n",
    "\n",
    "    submission = generate_submission(report)\n",
    "\n",
    "    # Validate schema\n",
    "    assert validate_submission_schema(submission), \"Submission should pass schema validation\"\n",
    "\n",
    "    # Test with optimized results\n",
    "    optimized_model = SimpleMLP(input_size=10, hidden_size=15, output_size=3)\n",
    "    optimized_report = BenchmarkReport(model_name=\"optimized_model\")\n",
    "    optimized_report.benchmark_model(optimized_model, X_test, y_test, num_runs=10)\n",
    "\n",
    "    submission_with_opt = generate_submission(\n",
    "        report,\n",
    "        optimized_report,\n",
    "        techniques_applied=[\"pruning\"]\n",
    "    )\n",
    "\n",
    "    # Validate optimized submission\n",
    "    assert validate_submission_schema(submission_with_opt), \"Optimized submission should pass validation\"\n",
    "    assert 'optimized' in submission_with_opt, \"Should have optimized section\"\n",
    "    assert 'improvements' in submission_with_opt, \"Should have improvements section\"\n",
    "\n",
    "    print(\"âœ… Submission schema validation works correctly!\")\n",
    "\n",
    "# Run test immediately when developing\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_submission_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f23c6",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "test-submission-with-optimization",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_submission_with_optimization():\n",
    "    \"\"\"ğŸ”¬ Test submission with baseline + optimized comparison.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Submission with Optimization...\")\n",
    "\n",
    "    # Create baseline\n",
    "    np.random.seed(42)\n",
    "    baseline_model = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "    X_test = Tensor(np.random.randn(50, 10))\n",
    "    y_test = np.random.randint(0, 3, 50)\n",
    "\n",
    "    baseline_report = BenchmarkReport(model_name=\"baseline\")\n",
    "    baseline_report.benchmark_model(baseline_model, X_test, y_test, num_runs=10)\n",
    "\n",
    "    # Create optimized version (smaller model for demo)\n",
    "    optimized_model = SimpleMLP(input_size=10, hidden_size=15, output_size=3)\n",
    "    optimized_report = BenchmarkReport(model_name=\"optimized\")\n",
    "    optimized_report.benchmark_model(optimized_model, X_test, y_test, num_runs=10)\n",
    "\n",
    "    # Generate submission with both\n",
    "    techniques = [\"model_sizing\", \"pruning\"]\n",
    "    submission = generate_submission(\n",
    "        baseline_report,\n",
    "        optimized_report,\n",
    "        student_name=\"Test Student\",\n",
    "        techniques_applied=techniques\n",
    "    )\n",
    "\n",
    "    # Check optimized section exists\n",
    "    assert 'optimized' in submission, \"Should have optimized section\"\n",
    "    optimized = submission['optimized']\n",
    "    assert 'model_name' in optimized, \"Optimized section should have model name\"\n",
    "    assert 'metrics' in optimized, \"Optimized section should have metrics\"\n",
    "    assert 'techniques_applied' in optimized, \"Should have techniques list\"\n",
    "    assert optimized['techniques_applied'] == techniques, \"Techniques should match\"\n",
    "\n",
    "    # Check improvements section\n",
    "    assert 'improvements' in submission, \"Should have improvements section\"\n",
    "    improvements = submission['improvements']\n",
    "    assert 'speedup' in improvements, \"Should have speedup metric\"\n",
    "    assert 'compression_ratio' in improvements, \"Should have compression ratio\"\n",
    "    assert 'accuracy_delta' in improvements, \"Should have accuracy delta\"\n",
    "\n",
    "    # Check improvement values are reasonable\n",
    "    assert improvements['speedup'] > 0, \"Speedup should be positive\"\n",
    "    assert improvements['compression_ratio'] > 0, \"Compression ratio should be positive\"\n",
    "    assert -1 <= improvements['accuracy_delta'] <= 1, \"Accuracy delta should be in [-1, 1]\"\n",
    "\n",
    "    print(\"âœ… Submission with optimization works correctly!\")\n",
    "\n",
    "# Run test immediately when developing\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_submission_with_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc99670",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "test-improvements-calculation",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_improvements_calculation():\n",
    "    \"\"\"ğŸ”¬ Test speedup/compression/accuracy calculations are correct.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Improvements Calculation...\")\n",
    "\n",
    "    # Create baseline with known metrics\n",
    "    baseline_report = BenchmarkReport(model_name=\"baseline\")\n",
    "    baseline_report.metrics = {\n",
    "        'parameter_count': 1000,\n",
    "        'model_size_mb': 4.0,\n",
    "        'accuracy': 0.80,\n",
    "        'latency_ms_mean': 10.0,\n",
    "        'latency_ms_std': 1.0,\n",
    "        'throughput_samples_per_sec': 100.0\n",
    "    }\n",
    "    baseline_report.timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    baseline_report.system_info = {'platform': 'test', 'python_version': '3.9', 'numpy_version': '1.20'}\n",
    "\n",
    "    # Create optimized with 2x speedup, 2x compression, 5% accuracy loss\n",
    "    optimized_report = BenchmarkReport(model_name=\"optimized\")\n",
    "    optimized_report.metrics = {\n",
    "        'parameter_count': 500,\n",
    "        'model_size_mb': 2.0,\n",
    "        'accuracy': 0.75,\n",
    "        'latency_ms_mean': 5.0,\n",
    "        'latency_ms_std': 0.5,\n",
    "        'throughput_samples_per_sec': 200.0\n",
    "    }\n",
    "    optimized_report.timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    optimized_report.system_info = baseline_report.system_info\n",
    "\n",
    "    # Generate submission\n",
    "    submission = generate_submission(baseline_report, optimized_report)\n",
    "\n",
    "    improvements = submission['improvements']\n",
    "\n",
    "    # Verify calculations\n",
    "    # Speedup = baseline_latency / optimized_latency = 10.0 / 5.0 = 2.0\n",
    "    assert abs(improvements['speedup'] - 2.0) < 0.01, f\"Expected speedup 2.0, got {improvements['speedup']}\"\n",
    "\n",
    "    # Compression = baseline_size / optimized_size = 4.0 / 2.0 = 2.0\n",
    "    assert abs(improvements['compression_ratio'] - 2.0) < 0.01, f\"Expected compression 2.0, got {improvements['compression_ratio']}\"\n",
    "\n",
    "    # Accuracy delta = 0.75 - 0.80 = -0.05\n",
    "    assert abs(improvements['accuracy_delta'] - (-0.05)) < 0.001, f\"Expected accuracy delta -0.05, got {improvements['accuracy_delta']}\"\n",
    "\n",
    "    print(\"âœ… Improvements calculation is correct!\")\n",
    "\n",
    "# Run test immediately when developing\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_improvements_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432d594",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "test-json-serialization",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_json_serialization():\n",
    "    \"\"\"ğŸ”¬ Test save_submission() creates valid JSON files.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: JSON Serialization...\")\n",
    "\n",
    "    # Create submission\n",
    "    np.random.seed(42)\n",
    "    model = SimpleMLP(input_size=10, hidden_size=20, output_size=3)\n",
    "    X_test = Tensor(np.random.randn(50, 10))\n",
    "    y_test = np.random.randint(0, 3, 50)\n",
    "\n",
    "    report = BenchmarkReport(model_name=\"test_model\")\n",
    "    report.benchmark_model(model, X_test, y_test, num_runs=10)\n",
    "\n",
    "    submission = generate_submission(report, student_name=\"Test Student\")\n",
    "\n",
    "    # Save to file\n",
    "    test_file = \"/tmp/test_submission_unit.json\"\n",
    "    filepath = save_submission(submission, test_file)\n",
    "\n",
    "    # Check file exists\n",
    "    assert Path(filepath).exists(), \"Submission file should exist\"\n",
    "\n",
    "    # Load and verify JSON is valid\n",
    "    loaded_json = json.loads(Path(test_file).read_text())\n",
    "\n",
    "    # Verify structure is preserved\n",
    "    assert loaded_json['tinytorch_version'] == submission['tinytorch_version'], \"Version should match\"\n",
    "    assert loaded_json['student_name'] == submission['student_name'], \"Student name should match\"\n",
    "    assert loaded_json['baseline']['model_name'] == submission['baseline']['model_name'], \"Model name should match\"\n",
    "\n",
    "    # Verify metrics are preserved\n",
    "    baseline_metrics = loaded_json['baseline']['metrics']\n",
    "    original_metrics = submission['baseline']['metrics']\n",
    "    assert baseline_metrics['accuracy'] == original_metrics['accuracy'], \"Accuracy should match\"\n",
    "    assert baseline_metrics['parameter_count'] == original_metrics['parameter_count'], \"Parameter count should match\"\n",
    "\n",
    "    # Verify JSON can be dumped again (round-trip test)\n",
    "    round_trip = json.dumps(loaded_json, indent=2)\n",
    "    assert len(round_trip) > 0, \"JSON should serialize again\"\n",
    "\n",
    "    # Clean up\n",
    "    Path(test_file).unlink()\n",
    "\n",
    "    print(\"âœ… JSON serialization works correctly!\")\n",
    "\n",
    "# Run test immediately when developing\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_json_serialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b439fa1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "test-module",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test_module():\n",
    "    \"\"\"\n",
    "    ğŸ§ª Test Module 20: Capstone submission infrastructure.\n",
    "\n",
    "    Runs all unit tests to validate complete functionality.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODULE 20: CAPSTONE - UNIT TESTS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    test_unit_simple_mlp()\n",
    "    test_unit_benchmark_report()\n",
    "    test_unit_submission_generation()\n",
    "    test_unit_submission_schema()\n",
    "    test_unit_submission_with_optimization()\n",
    "    test_unit_improvements_calculation()\n",
    "    test_unit_json_serialization()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ‰ ALL TESTS PASSED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nModule 20 validation complete!\")\n",
    "    print(\"Run: tito module complete 20\")\n",
    "\n",
    "print(\"âœ… Test module defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8837cc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ¤” ML Systems Thinking\n",
    "\n",
    "### Reflecting on the Complete ML Systems Journey\n",
    "\n",
    "You've built an entire ML framework across 20 modules. This capstone asks you to step back and reflect on the complete systems journeyâ€”from tensors to production-ready benchmarking.\n",
    "\n",
    "### End-to-End System Integration\n",
    "\n",
    "Modern ML systems aren't just individual components working in isolationâ€”they're carefully orchestrated pipelines where each piece connects to form a cohesive whole.\n",
    "\n",
    "**The Complete Pipeline You Built:**\n",
    "\n",
    "```\n",
    "Data â†’ Tensor (M01) â†’ Layers (M03) â†’ Model â†’ Training (M07)\n",
    "                â†“                      â†“           â†“\n",
    "          Activations (M02)      Autograd (M05)  DataLoader (M08)\n",
    "                â†“                      â†“           â†“\n",
    "          Losses (M04)           Optimizers (M06) Spatial Ops (M09)\n",
    "                                       â†“\n",
    "                              Advanced Architectures\n",
    "                         (Tokenization, Embeddings, Attention,\n",
    "                          Transformers: M10-M13)\n",
    "                                       â†“\n",
    "                              Optimization Pipeline\n",
    "                         (Profiling, Quantization, Compression,\n",
    "                          KV Cache, Acceleration: M14-M18)\n",
    "                                       â†“\n",
    "                           Measurement & Validation\n",
    "                         (Benchmarking M19, Submission M20)\n",
    "```\n",
    "\n",
    "**Systems Integration Lessons:**\n",
    "\n",
    "1. **Dependency Management** - Each module imports from previous modules, creating a proper dependency graph\n",
    "2. **API Consistency** - Tensor operations work the same whether in Module 01 or Module 20\n",
    "3. **Composability** - Complex systems (transformers) built from simple primitives (linear layers)\n",
    "4. **Progressive Enhancement** - Module 06 activated gradients dormant since Module 01\n",
    "\n",
    "**Reflection Question:** When you imported `from tinytorch.core.tensor import Tensor` in Module 15 (Quantization), the Tensor already had gradient tracking from Module 06. How does this \"single source of truth\" design simplify system integration compared to having separate BasicTensor and GradTensor classes?\n",
    "\n",
    "### Benchmarking Methodology: Science Meets Engineering\n",
    "\n",
    "Effective benchmarking requires rigorous methodology that bridges scientific measurement with engineering pragmatism.\n",
    "\n",
    "**The Three Pillars of Reliable Benchmarking:**\n",
    "\n",
    "```\n",
    "1. REPEATABILITY (Same Experiment â†’ Same Result)\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ â€¢ Fixed random seeds (np.random.seed)   â”‚\n",
    "   â”‚ â€¢ Same test dataset across runs         â”‚\n",
    "   â”‚ â€¢ Consistent environment (same hardware)â”‚\n",
    "   â”‚ â€¢ Multiple runs to capture variance     â”‚\n",
    "   â”‚                                         â”‚\n",
    "   â”‚ Why: Single measurements lie            â”‚\n",
    "   â”‚ 10.3ms once vs 10.0ms Â± 0.5ms (100Ã—)    â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "2. COMPARABILITY (Fair Comparisons)\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ â€¢ Same hardware platform                â”‚\n",
    "   â”‚ â€¢ Same test data for baseline/optimized â”‚\n",
    "   â”‚ â€¢ Same metrics (latency, accuracy)      â”‚\n",
    "   â”‚ â€¢ Documented environment (sys.platform) â”‚\n",
    "   â”‚                                         â”‚\n",
    "   â”‚ Why: Apples-to-apples decisions         â”‚\n",
    "   â”‚ Can't compare GPU timing to CPU timing  â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "3. COMPLETENESS (Capture All Dimensions)\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ â€¢ Accuracy (quality metric)             â”‚\n",
    "   â”‚ â€¢ Latency (speed metric)                â”‚\n",
    "   â”‚ â€¢ Memory (resource metric)              â”‚\n",
    "   â”‚ â€¢ Throughput (capacity metric)          â”‚\n",
    "   â”‚                                         â”‚\n",
    "   â”‚ Why: Optimizations have trade-offs      â”‚\n",
    "   â”‚ Fast + Small might mean Less Accurate   â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Measurement Best Practices You Implemented:**\n",
    "\n",
    "1. **Warm-up runs** - First inference is often slower (cold cache)\n",
    "2. **Statistical aggregation** - Report mean Â± std, not single values\n",
    "3. **Multiple metrics** - Never optimize for just one dimension\n",
    "4. **System context** - Platform, Python version, library versions matter\n",
    "\n",
    "**The Variance Story:**\n",
    "\n",
    "```python\n",
    "# Why we run 100 iterations instead of 1:\n",
    "\n",
    "Single measurement: 12.3ms\n",
    "  â†’ Could be outlier (GC pause? OS interrupt?)\n",
    "  â†’ No confidence interval\n",
    "  â†’ Can't detect performance regressions\n",
    "\n",
    "100 measurements: 10.0ms Â± 0.5ms\n",
    "  â†’ Statistically valid\n",
    "  â†’ Confidence: \"Next run will likely be 9.5-10.5ms\"\n",
    "  â†’ Can detect if update made things worse\n",
    "```\n",
    "\n",
    "**Reflection Question:** Your benchmark runs inference 100 times and reports mean latency. A production API serves 1 million requests/day. Which percentile (p50, p90, p99) matters more for user experience, and why isn't mean sufficient?\n",
    "\n",
    "### Performance Measurement Traps and How to Avoid Them\n",
    "\n",
    "Real-world benchmarking is full of subtle traps that can invalidate your measurements.\n",
    "\n",
    "**Common Measurement Pitfalls:**\n",
    "\n",
    "```\n",
    "TRAP 1: Measuring the Wrong Thing\n",
    "  âŒ Timing model creation instead of inference\n",
    "  âŒ Including data loading in latency measurement\n",
    "  âŒ Measuring batch=32 when production uses batch=1\n",
    "\n",
    "  âœ… FIX: Isolate exactly what you're measuring\n",
    "     start = time.time()\n",
    "     output = model.forward(x)  # ONLY this\n",
    "     latency = time.time() - start\n",
    "\n",
    "TRAP 2: Ignoring System Noise\n",
    "  âŒ Running benchmarks while streaming video\n",
    "  âŒ Single measurement (affected by GC, OS)\n",
    "  âŒ Not warming up (first run is slow)\n",
    "\n",
    "  âœ… FIX: Multiple runs, discard outliers\n",
    "     for _ in range(100):  # Warm up + measure\n",
    "         measure_latency()\n",
    "     report mean Â± std\n",
    "\n",
    "TRAP 3: Cherry-Picking Results\n",
    "  âŒ \"Ran 10 times, best was 8.2ms!\" (reporting min)\n",
    "  âŒ Rerunning until you get good numbers\n",
    "  âŒ Omitting variance in reporting\n",
    "\n",
    "  âœ… FIX: Report full distribution\n",
    "     \"10.0ms Â± 0.5ms (n=100, p99=11.2ms)\"\n",
    "\n",
    "TRAP 4: Wrong Hardware Baseline\n",
    "  âŒ Benchmarking on MacBook, deploying to server\n",
    "  âŒ Comparing GPU results to CPU results\n",
    "  âŒ Not documenting hardware (can't reproduce)\n",
    "\n",
    "  âœ… FIX: Benchmark on deployment hardware\n",
    "     submission['system_info'] = {\n",
    "       'platform': platform.platform(),\n",
    "       'cpu': 'Intel Xeon Gold',\n",
    "       'gpu': 'NVIDIA A100'\n",
    "     }\n",
    "\n",
    "TRAP 5: Confusing Latency and Throughput\n",
    "  âŒ \"Processes 1000 samples in 10s = 0.01s per sample\"\n",
    "     (Batch processing != per-sample latency!)\n",
    "  âŒ Optimizing throughput hurts latency (big batches)\n",
    "\n",
    "  âœ… FIX: Measure both separately\n",
    "     latency = measure_single_sample()\n",
    "     throughput = measure_batch_processing()\n",
    "```\n",
    "\n",
    "**Real Example from TinyTorch:**\n",
    "\n",
    "```python\n",
    "# âŒ WRONG: Measures more than inference\n",
    "def bad_benchmark():\n",
    "    start = time.time()\n",
    "    x = create_random_input()      # Includes data generation!\n",
    "    output = model.forward(x)\n",
    "    result = postprocess(output)   # Includes postprocessing!\n",
    "    return time.time() - start\n",
    "\n",
    "# âœ… CORRECT: Isolates inference\n",
    "def good_benchmark():\n",
    "    x = create_random_input()      # Setup (not timed)\n",
    "\n",
    "    start = time.time()\n",
    "    output = model.forward(x)      # ONLY inference\n",
    "    latency = time.time() - start\n",
    "\n",
    "    postprocess(output)            # Cleanup (not timed)\n",
    "    return latency\n",
    "```\n",
    "\n",
    "**Reflection Question:** You benchmark a model at batch_size=32 and report 50ms latency (1.56ms per sample). A production API serves requests one at a time. Will real users experience 1.56ms latency? Why or why not?\n",
    "\n",
    "### Schema Validation: Making Results Machine-Readable\n",
    "\n",
    "Your submission format uses JSON Schema validationâ€”a powerful pattern for ensuring data quality and enabling automation.\n",
    "\n",
    "**Why Schema Validation Matters:**\n",
    "\n",
    "```\n",
    "WITHOUT Schema:                     WITH Schema:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ {                        â”‚       â”‚ {                        â”‚\n",
    "â”‚   \"accuracy\": \"92%\",     â”‚ âŒ    â”‚   \"accuracy\": 0.92,      â”‚ âœ…\n",
    "â”‚   \"latency\": 10.5,       â”‚ âŒ    â”‚   \"latency_ms_mean\": 10.5â”‚ âœ…\n",
    "â”‚   \"time\": \"today\"        â”‚ âŒ    â”‚   \"timestamp\": \"2025...\" â”‚ âœ…\n",
    "â”‚ }                        â”‚       â”‚ }                        â”‚\n",
    "â”‚                          â”‚       â”‚                          â”‚\n",
    "â”‚ Problems:                â”‚       â”‚ Benefits:                â”‚\n",
    "â”‚ â€¢ Wrong type (string %)  â”‚       â”‚ â€¢ Enforced types (float) â”‚\n",
    "â”‚ â€¢ Ambiguous name         â”‚       â”‚ â€¢ Clear field names      â”‚\n",
    "â”‚ â€¢ Unparsable time        â”‚       â”‚ â€¢ Standard format        â”‚\n",
    "â”‚ â€¢ Can't aggregate        â”‚       â”‚ â€¢ Automated validation   â”‚\n",
    "â”‚ â€¢ No automation possible â”‚       â”‚ â€¢ Aggregation works      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Schema Design Principles:**\n",
    "\n",
    "1. **Required fields** - Baseline metrics are mandatory, optimized optional\n",
    "2. **Type safety** - `accuracy: float` not `accuracy: any`\n",
    "3. **Value constraints** - `accuracy in [0.0, 1.0]` catches errors\n",
    "4. **Nested structure** - Group related fields (`baseline: {metrics: {...}}`)\n",
    "5. **Version tracking** - `tinytorch_version: \"0.1.0\"` enables evolution\n",
    "\n",
    "**The Power of Machine-Readable Data:**\n",
    "\n",
    "```python\n",
    "# With schema-validated submissions, you can:\n",
    "\n",
    "# 1. Automatically aggregate community results\n",
    "all_submissions = load_all_submissions()\n",
    "avg_accuracy = np.mean([s['baseline']['metrics']['accuracy']\n",
    "                       for s in all_submissions])\n",
    "\n",
    "# 2. Build leaderboards\n",
    "sorted_by_speedup = sorted(all_submissions,\n",
    "                          key=lambda s: s['improvements']['speedup'],\n",
    "                          reverse=True)\n",
    "\n",
    "# 3. Detect regressions\n",
    "if new_latency > baseline_latency * 1.1:\n",
    "    alert(\"Performance regression detected!\")\n",
    "\n",
    "# 4. Generate visualizations\n",
    "plot_accuracy_vs_speedup(all_submissions)\n",
    "```\n",
    "\n",
    "**Reflection Question:** Your submission schema requires `model_size_mb` as a float. Why is this better than allowing users to write \"4MB\" or \"4.0 megabytes\" as strings? Think about aggregation and comparison.\n",
    "\n",
    "### The Complete ML Systems Lifecycle\n",
    "\n",
    "This capstone represents the final stage of the ML systems lifecycleâ€”but it's also the beginning of the next iteration.\n",
    "\n",
    "**The Never-Ending Loop:**\n",
    "\n",
    "```\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚    1. RESEARCH & DEVELOPMENT     â”‚\n",
    "            â”‚  (Modules 01-13: Build framework)â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â†“\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚     2. BASELINE MEASUREMENT      â”‚\n",
    "            â”‚   (Module 19: Benchmark baseline)â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â†“\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚      3. OPTIMIZATION PHASE       â”‚\n",
    "            â”‚ (Modules 14-18: Apply techniques)â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â†“\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚    4. VALIDATION & COMPARISON    â”‚\n",
    "            â”‚  (Module 20: Benchmark optimized)â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â†“\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚     5. DECISION & SUBMISSION     â”‚\n",
    "            â”‚  (Keep? Deploy? Iterate? Share?) â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â†“\n",
    "                   Did we meet goals?\n",
    "                         â†“\n",
    "                    No â”€â”€â”€â”€â”€â†’ (Loop back to step 3)\n",
    "                         â†“ Yes\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚      6. PRODUCTION DEPLOY        â”‚\n",
    "            â”‚   (Model serves real traffic)    â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â†“\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚     7. MONITORING & FEEDBACK     â”‚\n",
    "            â”‚  (Is performance degrading? New  â”‚\n",
    "            â”‚   optimization opportunities?)   â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â†“\n",
    "                   (Loop back to step 1)\n",
    "```\n",
    "\n",
    "**Key Insight:** Production ML is iterative. Your submission captures a snapshot, but the system keeps evolving. This is why reproducibility (schema, environment documentation) is criticalâ€”you need to know what changed when performance shifts.\n",
    "\n",
    "**Reflection Question:** You deploy a model with 92% accuracy and 10ms latency. Three months later, users complain it's slow. Monitoring shows 30ms latency now (same model, same code). You didn't save system_info in your original benchmark. What went wrong, and how does proper benchmarking prevent this?\n",
    "\n",
    "### Your Path Forward: From Learning to Production\n",
    "\n",
    "You've completed an educational framework, but the patterns you learned apply directly to production systems.\n",
    "\n",
    "**Translating TinyTorch Skills to Production:**\n",
    "\n",
    "```\n",
    "TinyTorch Pattern          â†’  Production Equivalent\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BenchmarkReport            â†’  MLflow Tracking\n",
    "generate_submission()      â†’  Experiment logging\n",
    "validate_schema()          â†’  JSON Schema / Protobuf\n",
    "system_info collection     â†’  Environment containers (Docker)\n",
    "baseline vs optimized      â†’  A/B testing framework\n",
    "improvements calculation   â†’  Regression detection\n",
    "```\n",
    "\n",
    "**Real-World Applications:**\n",
    "\n",
    "1. **Model Comparison** - Same workflow as Module 20, scaled to dozens of experiments\n",
    "2. **Performance Monitoring** - Continuous benchmarking in CI/CD pipelines\n",
    "3. **Reproducible Research** - Papers with Code submissions use similar schemas\n",
    "4. **Team Collaboration** - Shared benchmark format enables comparison across engineers\n",
    "\n",
    "**Next Steps for Production Systems:**\n",
    "\n",
    "- **Scale beyond toy models** - Apply to CNNs, Transformers from milestones\n",
    "- **Automated pipelines** - Trigger benchmarks on every commit (CI/CD)\n",
    "- **Visualization dashboards** - Plot accuracy vs latency trade-off curves\n",
    "- **Multi-hardware comparison** - Benchmark on CPU, GPU, TPU\n",
    "- **Production monitoring** - Track deployed model performance over time\n",
    "\n",
    "Congratulations! You've gone from implementing basic tensors to understanding end-to-end ML systems. The benchmarking methodology and systems thinking you learned here will serve you throughout your career in ML engineering. ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d32820",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ”§ Main Execution\n",
    "\n",
    "When run as a script, this demonstrates the complete workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddcd99",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## â­ Aha Moment: You Built a Complete ML System\n",
    "\n",
    "**What you built:** A professional benchmarking and submission system for your TinyTorch models.\n",
    "\n",
    "**Why it matters:** You've gone from raw tensors to complete ML systems! Your capstone ties\n",
    "together everything: models, training, optimization, profiling, and benchmarking. The\n",
    "submission format you created is how real ML competitions and production deployments work.\n",
    "\n",
    "Congratulationsâ€”you've built a deep learning framework from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0912f5c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demo_capstone():\n",
    "    \"\"\"ğŸ¯ See your complete system come together.\"\"\"\n",
    "    print(\"ğŸ¯ AHA MOMENT: You Built a Complete ML System\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    print(\"ğŸ“š Your TinyğŸ”¥Torch Journey:\")\n",
    "    print()\n",
    "    print(\"  Modules 01-08: Foundation\")\n",
    "    print(\"    Tensor â†’ Activations â†’ Layers â†’ Losses\")\n",
    "    print(\"    â†’ Autograd â†’ Optimizers â†’ Training â†’ DataLoader\")\n",
    "    print()\n",
    "    print(\"  Modules 09-13: Neural Architectures\")\n",
    "    print(\"    Conv2d â†’ Tokenization â†’ Embeddings\")\n",
    "    print(\"    â†’ Attention â†’ Transformers\")\n",
    "    print()\n",
    "    print(\"  Modules 14-19: Production Optimization\")\n",
    "    print(\"    Profiling â†’ Quantization â†’ Compression\")\n",
    "    print(\"    â†’ KV Caching â†’ Acceleration â†’ Benchmarking\")\n",
    "    print()\n",
    "    print(\"  Module 20: Capstone\")\n",
    "    print(\"    Complete benchmarking and submission system\")\n",
    "    print()\n",
    "    print(\"âœ¨ From np.array to production MLâ€”congratulations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca737ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_module()\n",
    "    print(\"\\n\")\n",
    "    demo_capstone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc417a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸš€ MODULE SUMMARY: Capstone - Benchmarking & Submission\n",
    "\n",
    "Congratulations! You've completed the TinyTorch capstone by building a professional benchmarking and submission system!\n",
    "\n",
    "### Key Accomplishments\n",
    "\n",
    "**What You Built:**\n",
    "- âœ… **BenchmarkReport class** - Comprehensive performance measurement (accuracy, latency, throughput, memory)\n",
    "- âœ… **Submission generation** - Standardized JSON format with schema validation\n",
    "- âœ… **Comparison infrastructure** - Automatic calculation of speedup, compression, accuracy delta\n",
    "- âœ… **Complete workflows** - From baseline to optimized models with reproducible results\n",
    "\n",
    "**What You Learned:**\n",
    "- ğŸ“Š **Benchmarking science** - Repeatability, comparability, completeness principles\n",
    "- ğŸ“ˆ **Metrics that matter** - Latency vs throughput, mean vs variance, accuracy vs efficiency\n",
    "- ğŸ” **Reproducibility** - System context, schema validation, standardized reporting\n",
    "- ğŸš€ **Production patterns** - How real ML systems measure and compare model performance\n",
    "\n",
    "**Technical Skills Gained:**\n",
    "- Measuring inference latency with statistical rigor (mean Â± std over multiple runs)\n",
    "- Calculating model memory footprint (parameters Ã— bytes per parameter)\n",
    "- Generating schema-compliant JSON for automated validation\n",
    "- Comparing baseline vs optimized models quantitatively\n",
    "\n",
    "### The Complete TinyTorch Journey\n",
    "\n",
    "```\n",
    "Module 01: Tensor          â†’ Built foundation\n",
    "Modules 02-13: Framework   â†’ Implemented ML components\n",
    "Modules 14-18: Optimization â†’ Learned performance techniques\n",
    "Module 19: Benchmarking    â†’ Measured performance\n",
    "Module 20: Submission      â†’ Proved it works! âœ¨\n",
    "```\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "The skills you practiced in this capstone are used daily in production ML:\n",
    "\n",
    "**Research Labs:**\n",
    "- Publishing papers with reproducible benchmarks\n",
    "- Comparing architectures on standardized leaderboards\n",
    "- Validating claims with measurable improvements\n",
    "\n",
    "**ML Engineering Teams:**\n",
    "- A/B testing model versions before deployment\n",
    "- Tracking latency/accuracy trade-offs across experiments\n",
    "- Documenting optimization wins for stakeholders\n",
    "\n",
    "**MLOps Platforms:**\n",
    "- Automated model evaluation pipelines\n",
    "- Performance regression detection\n",
    "- Multi-metric decision making (speed vs accuracy vs cost)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Benchmark milestone models** - Apply this workflow to your MNIST CNN, XOR network, etc.\n",
    "2. **Apply optimizations** - Use Modules 14-18 techniques and measure their impact\n",
    "3. **Share your results** - Submit your JSON to the TinyTorch community\n",
    "4. **Compare with others** - See how your optimizations stack up\n",
    "5. **Build production systems** - Use these patterns in real ML projects\n",
    "\n",
    "### Final Reflection\n",
    "\n",
    "You started Module 01 with a simple Tensor class. Now you have:\n",
    "- âœ… A complete ML framework\n",
    "- âœ… Advanced optimization techniques\n",
    "- âœ… Professional benchmarking infrastructure\n",
    "- âœ… Reproducible, shareable results\n",
    "\n",
    "**You didn't just learn ML systems - you BUILT one from scratch.** ğŸ‰\n",
    "\n",
    "Export your capstone module:\n",
    "```bash\n",
    "tito module complete 20\n",
    "```\n",
    "\n",
    "Then share your submission with the community and celebrate your achievement! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
