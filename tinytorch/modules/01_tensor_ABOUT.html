
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 01: Tensor" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/01_tensor_ABOUT.html" />
<meta property="og:site_name" content="Tinyüî•Torch" />
<meta property="og:description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: T..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="Tinyüî•Torch" />
<meta name="description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: T..." />

    <title>Module 01: Tensor &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=cd3a79b9" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";





const initStyles = () => {
    const defaultStyle = document.createElement('style');
    defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}`;
    document.head.appendChild(defaultStyle);

    const fullscreenStyle = document.createElement('style');
    fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
    document.head.appendChild(fullscreenStyle);
}

// Detect if page has dark background
const isDarkTheme = () => {
    // We use a set of heuristics:
    // 1. Check for common dark mode classes or attributes
    // 2. Check computed background color brightness
    if (document.documentElement.classList.contains('dark') ||
        document.documentElement.getAttribute('data-theme') === 'dark' ||
        document.body.classList.contains('dark') ||
        document.body.getAttribute('data-theme') === 'dark') {
        // console.log("Dark theme detected via class/attribute");
        return true;
    }
    if (document.documentElement.classList.contains('light') ||
        document.documentElement.getAttribute('data-theme') === 'light' ||
        document.body.classList.contains('light') ||
        document.body.getAttribute('data-theme') === 'light') {
        // console.log("Light theme detected via class/attribute");
        return false;
    }
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
        // console.log("Dark theme detected via prefers-color-scheme");
        return true;
    }
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        // console.log("Background color brightness:", brightness);
        return brightness < 128;
    }
    // console.log("No dark or light theme detected, defaulting to light theme");
    return false;
};

let darkTheme = isDarkTheme();
let modal = null;
let modalContent = null;
let previousScrollOffset = [window.scrollX, window.scrollY];

const runMermaid = async (rerun) => {
    console.log("Running mermaid diagrams, rerun =", rerun);
    // clear all existing mermaid charts
    let all_mermaids = document.querySelectorAll(".mermaid");

    if (rerun) {
        all_mermaids.forEach((el) => {
            if(!el.hasAttribute("data-original-code")) {
                // store original code
                // console.log(`Storing original code for first run: `, el.innerHTML);
                el.setAttribute('data-original-code', el.innerHTML);
            }
            if(el.getAttribute("data-processed") === "true") {
                // remove and restore original
                el.removeAttribute("data-processed");
                // console.log(`Restoring original code for re-run: `, el.getAttribute('data-original-code'));
                el.innerHTML = el.getAttribute('data-original-code');
            } else {
                // store original code
                // console.log(`Storing original code for re-run: `, el.innerHTML);
                el.setAttribute('data-original-code', el.innerHTML);
            }
        });
        await mermaid.run();
    }

    all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(() => runMermaid(false), 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(() => runMermaid(false), 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(() => runMermaid(false), 200);
        return;
    }

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    if (modal !== null ) {
        // Destroy existing modal
        modal.remove();
        modal = null;
        modalContent = null;
    }

    modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            // Already processed, adjust button class if needed
            const existingBtn = mermaidDiv.parentNode.querySelector('.mermaid-fullscreen-btn');
            if (existingBtn) {
                existingBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
            }
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });
        container.appendChild(fullscreenBtn);
    });
};

const load = async () => {
    initStyles();

    await runMermaid(true);

    const reRunIfThemeChanges = async () => {
        const newDarkTheme = isDarkTheme();
        if (newDarkTheme !== darkTheme) {
            darkTheme = newDarkTheme;
            console.log("Theme change detected, re-running mermaid with", darkTheme ? "dark" : "default", "theme");
            await mermaid.initialize(
                {...JSON.parse(
                    `{"startOnLoad": false}`
                ),
                ...{ darkMode: darkTheme, theme: darkTheme ? 'dark' : 'default' },
                }
            );
            await runMermaid(true);
        }
    };

    // Update theme classes when theme changes
    const themeObserver = new MutationObserver(reRunIfThemeChanges);
    themeObserver.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    themeObserver.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
};





console.log("Initializing mermaid with", darkTheme ? "dark" : "default", "theme");
mermaid.initialize(
    {...JSON.parse(
        `{"startOnLoad": false}`
    ),
    ...{ darkMode: darkTheme, theme: darkTheme ? 'dark' : 'default' },
    }
);

window.addEventListener("load", load);</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/01_tensor_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=0d27a1a4"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=ee94e95f"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/version-badge.js?v=04e6b7f0"></script>
    <script src="../_static/subscribe-modal.js?v=c8499bec"></script>
    <script src="../_static/announcement-bar.js?v=49ee9c9d"></script>
    <link rel="canonical" href="https://mlsysbook.ai/tinytorch/modules/01_tensor_ABOUT.html" />
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Module 02: Activations" href="02_activations_ABOUT.html" />
    <link rel="prev" title="Foundation Tier (Modules 01-08)" href="../tiers/foundation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../book/" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mlsysbook" title="Support" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-heart fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Support</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-08)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader_ABOUT.html">05. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_autograd_ABOUT.html">06. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimizers_ABOUT.html">07. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_training_ABOUT.html">08. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (09-13)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_acceleration_ABOUT.html">17. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_memoization_ABOUT.html">18. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/01_tensor_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 01: Tensor</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building">What You‚Äôre NOT Building</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constructor">Constructor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties">Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arithmetic-operations">Arithmetic Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-shape-operations">Matrix &amp; Shape Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reductions">Reductions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-dimensionality">Tensor Dimensionality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#views-vs-copies">Views vs. Copies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication">Matrix Multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-manipulation">Shape Manipulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity">Computational Complexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#axis-semantics">Axis Semantics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture">Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-integration">Module Integration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors-debugging">Common Errors &amp; Debugging</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatch-in-matmul">Shape Mismatch in matmul</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting-failures">Broadcasting Failures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshape-size-mismatch">Reshape Size Mismatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-attributes">Missing Attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-errors-in-arithmetic">Type Errors in Arithmetic</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-tensors-matter-at-scale">Why Tensors Matter at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-01-tensor">
<h1>Module 01: Tensor<a class="headerlink" href="#module-01-tensor" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>FOUNDATION TIER</strong> | Difficulty: ‚óè‚óã‚óã‚óã | Time: 4-6 hours | Prerequisites: None</p>
<p><strong>Prerequisites: None</strong> means exactly that. This module assumes:</p>
<ul class="simple">
<li><p>Basic Python (lists, classes, methods)</p></li>
<li><p>Basic math (matrix multiplication from linear algebra)</p></li>
<li><p>No machine learning background required</p></li>
</ul>
<p>If you can multiply two matrices by hand and write a Python class, you‚Äôre ready.</p>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run interactively in your browser.</p>
<p class="sd-card-text"><a href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F01_tensor%2F01_tensor.ipynb" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #f97316; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">Open in Binder ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìÑ View Source</div>
<p class="sd-card-text">Browse the source code on GitHub.</p>
<p class="sd-card-text"><a href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/01_tensor/01_tensor.py" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #6b7280; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">View on GitHub ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üéß Audio Overview</div>
<p class="sd-card-text">Listen to an AI-generated overview.</p>
<audio controls style="width: 100%; height: 54px; margin-top: auto;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/01_tensor.mp3" type="audio/mpeg">
</audio>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>The Tensor class is the foundational data structure of machine learning. Every neural network, from recognizing handwritten digits to translating languages, operates on tensors. These networks process millions of numbers per second, and tensors are the data structure that makes this possible. In this module, you‚Äôll build N-dimensional arrays from scratch, gaining deep insight into how PyTorch works under the hood.</p>
<p>By the end, your tensor will support arithmetic, broadcasting, matrix multiplication, and shape manipulation - exactly like <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> a complete Tensor class with arithmetic, matrix multiplication, shape manipulation, and reductions</p></li>
<li><p><strong>Master</strong> broadcasting semantics that enable efficient computation without data copying</p></li>
<li><p><strong>Understand</strong> computational complexity (O(n¬≥) for matmul) and memory trade-offs (views vs copies)</p></li>
<li><p><strong>Connect</strong> your implementation to production PyTorch patterns and design decisions</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Your Tensor Class&quot;
        A[&quot;Properties&lt;br/&gt;shape, size, dtype&quot;]
        B[&quot;Arithmetic&lt;br/&gt;+, -, *, /&quot;]
        C[&quot;Matrix Ops&lt;br/&gt;matmul()&quot;]
        D[&quot;Shape Ops&lt;br/&gt;reshape, transpose&quot;]
        E[&quot;Reductions&lt;br/&gt;sum, mean, max&quot;]
    end

    A --&gt; B --&gt; C --&gt; D --&gt; E

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
    style E fill:#e2d5f1
    </pre><figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Your Tensor Class</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Part</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">shape</span></code>, <code class="docutils literal notranslate"><span class="pre">size</span></code>, <code class="docutils literal notranslate"><span class="pre">dtype</span></code></p></td>
<td><p>Tensor as NumPy wrapper</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__add__</span></code>, <code class="docutils literal notranslate"><span class="pre">__sub__</span></code>, <code class="docutils literal notranslate"><span class="pre">__mul__</span></code>, <code class="docutils literal notranslate"><span class="pre">__truediv__</span></code></p></td>
<td><p>Operator overloading + broadcasting</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">matmul()</span></code></p></td>
<td><p>Matrix multiplication with shape validation</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">reshape()</span></code>, <code class="docutils literal notranslate"><span class="pre">transpose()</span></code></p></td>
<td><p>Shape manipulation, views vs copies</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sum()</span></code>, <code class="docutils literal notranslate"><span class="pre">mean()</span></code>, <code class="docutils literal notranslate"><span class="pre">max()</span></code></p></td>
<td><p>Reductions along axes</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Computing predictions from data</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>  <span class="c1"># Matrix multiplication + bias (used in every neural network)</span>
</pre></div>
</div>
<section id="what-youre-not-building">
<h3>What You‚Äôre NOT Building<a class="headerlink" href="#what-youre-not-building" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>GPU support (NumPy runs on CPU only)</p></li>
<li><p>Automatic differentiation</p></li>
<li><p>Hundreds of tensor operations (PyTorch has 2000+, you‚Äôll build ~15 core ones)</p></li>
<li><p>Memory optimization tricks (PyTorch uses lazy evaluation, memory pools, etc.)</p></li>
</ul>
<p><strong>You are building the conceptual foundation.</strong></p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section provides a quick reference for the Tensor class you‚Äôll build. Think of it as your cheat sheet while implementing and debugging. Each method is documented with its signature and expected behavior.</p>
<section id="constructor">
<h3>Constructor<a class="headerlink" href="#constructor" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">data</span></code>: list, numpy array, or scalar</p></li>
</ul>
</section>
<section id="properties">
<h3>Properties<a class="headerlink" href="#properties" title="Link to this heading">#</a></h3>
<p>Your Tensor wraps a NumPy array and exposes several properties that describe its structure. These properties are read-only and computed from the underlying data.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">data</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code></p></td>
<td><p>Underlying NumPy array</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">shape</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">tuple</span></code></p></td>
<td><p>Dimensions, e.g., <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3)</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">size</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">int</span></code></p></td>
<td><p>Total number of elements</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">dtype</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.dtype</span></code></p></td>
<td><p>Data type (float32)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="arithmetic-operations">
<h3>Arithmetic Operations<a class="headerlink" href="#arithmetic-operations" title="Link to this heading">#</a></h3>
<p>Python lets you override operators like <code class="docutils literal notranslate"><span class="pre">+</span></code> and <code class="docutils literal notranslate"><span class="pre">*</span></code> by implementing special methods. When you write <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">y</span></code>, Python calls <code class="docutils literal notranslate"><span class="pre">x.__add__(y)</span></code>. Your implementations should handle both Tensor-Tensor operations and Tensor-scalar operations, letting NumPy‚Äôs broadcasting do the heavy lifting.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Method</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Addition</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__add__</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">y</span></code> or <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">2</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Subtraction</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__sub__</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">-</span> <span class="pre">y</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Multiplication</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__mul__</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">*</span> <span class="pre">y</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Division</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__truediv__</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">/</span> <span class="pre">y</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="matrix-shape-operations">
<h3>Matrix &amp; Shape Operations<a class="headerlink" href="#matrix-shape-operations" title="Link to this heading">#</a></h3>
<p>These methods transform tensors without changing their data (for views) or perform mathematical operations that produce new data (for matmul).</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">matmul</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">matmul(other)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Matrix multiplication</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">reshape</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">reshape(*shape)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Change shape (-1 to infer)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">transpose</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">transpose(dim0=None,</span> <span class="pre">dim1=None)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Swap dimensions (defaults to last two)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="reductions">
<h3>Reductions<a class="headerlink" href="#reductions" title="Link to this heading">#</a></h3>
<p>Reduction operations collapse one or more dimensions by aggregating values. The <code class="docutils literal notranslate"><span class="pre">axis</span></code> parameter controls which dimension gets collapsed. If <code class="docutils literal notranslate"><span class="pre">axis=None</span></code>, all dimensions collapse to a single scalar.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sum</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sum(axis=None,</span> <span class="pre">keepdims=False)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Sum elements</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mean</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mean(axis=None,</span> <span class="pre">keepdims=False)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Average elements</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">max</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">max(axis=None,</span> <span class="pre">keepdims=False)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Maximum element</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental ideas you need to understand tensors deeply. These concepts apply to every ML framework, not just TinyTorch, so mastering them here will serve you throughout your career.</p>
<section id="tensor-dimensionality">
<h3>Tensor Dimensionality<a class="headerlink" href="#tensor-dimensionality" title="Link to this heading">#</a></h3>
<p>Tensors generalize the familiar concepts you already know. A scalar is just a single number, like a temperature reading of 72.5 degrees. Stack scalars into a list and you get a vector, like a series of temperature measurements throughout the day. Arrange vectors into rows and you get a matrix, like a spreadsheet where each row is a different day‚Äôs measurements. Keep stacking and you reach 3D and 4D tensors that can represent video frames or collections of images.</p>
<p>The beauty of the Tensor abstraction is that your single class handles all of these cases. The same code that adds two scalars can add two 4D tensors, thanks to broadcasting.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Rank</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Shape</p></th>
<th class="head"><p>Concrete Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0D</p></td>
<td><p>Scalar</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">()</span></code></p></td>
<td><p>Temperature reading: <code class="docutils literal notranslate"><span class="pre">72.5</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>1D</p></td>
<td><p>Vector</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(768,)</span></code></p></td>
<td><p>Audio sample: 768 measurements</p></td>
</tr>
<tr class="row-even"><td><p>2D</p></td>
<td><p>Matrix</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(128,</span> <span class="pre">768)</span></code></p></td>
<td><p>Spreadsheet: 128 rows √ó 768 columns</p></td>
</tr>
<tr class="row-odd"><td><p>3D</p></td>
<td><p>3D Tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">224,</span> <span class="pre">224)</span></code></p></td>
<td><p>Video frames: 32 grayscale images</p></td>
</tr>
<tr class="row-even"><td><p>4D</p></td>
<td><p>4D Tensor</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">3,</span> <span class="pre">224,</span> <span class="pre">224)</span></code></p></td>
<td><p>Video frames: 32 color (RGB) images</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="broadcasting">
<h3>Broadcasting<a class="headerlink" href="#broadcasting" title="Link to this heading">#</a></h3>
<p>When you add a vector to a matrix, the shapes don‚Äôt match. Should this fail? In most programming contexts, yes. But many computations need to apply the same operation across rows or columns. For example, if you want to adjust all values in a spreadsheet by adding a different offset to each column, you need to add a vector to a matrix. NumPy and PyTorch implement broadcasting to handle this: automatically expanding smaller tensors to match larger ones without actually copying data.</p>
<p>Consider adding a bias vector <code class="docutils literal notranslate"><span class="pre">[10,</span> <span class="pre">20,</span> <span class="pre">30]</span></code> to every row of a matrix. Without broadcasting, you‚Äôd need to manually tile the vector into a matrix first, wasting memory. With broadcasting, the operation just works, and the framework handles alignment internally.</p>
<p>Here‚Äôs how your <code class="docutils literal notranslate"><span class="pre">__add__</span></code> implementation handles this elegantly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add two tensors element-wise with broadcasting support.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># NumPy handles broadcasting!</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="p">)</span>  <span class="c1"># Scalar broadcast</span>
</pre></div>
</div>
<p>The elegance is that NumPy‚Äôs broadcasting rules apply automatically when you write <code class="docutils literal notranslate"><span class="pre">self.data</span> <span class="pre">+</span> <span class="pre">other.data</span></code>. NumPy aligns the shapes from right to left and expands dimensions where needed, all without copying data.</p>
<figure class="align-center" id="id2">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Broadcasting Example&quot;
        M[&quot;Matrix (2,3)&lt;br/&gt;[[1,2,3], [4,5,6]]&quot;]
        V[&quot;Vector (3,)&lt;br/&gt;[10,20,30]&quot;]
        R[&quot;Result (2,3)&lt;br/&gt;[[11,22,33], [14,25,36]]&quot;]
    end

    M --&gt; |&quot;+&quot;| R
    V --&gt; |&quot;expands&quot;| R

    style M fill:#e1f5ff
    style V fill:#fff3cd
    style R fill:#d4edda
    </pre><figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Broadcasting Example</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The rules are simpler than they look. Compare shapes from right to left. At each position, dimensions are compatible if they‚Äôre equal or if one of them is 1. Missing dimensions on the left are treated as 1. If any position fails this check, broadcasting fails.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Shape A</p></th>
<th class="head"><p>Shape B</p></th>
<th class="head"><p>Result</p></th>
<th class="head"><p>Valid?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">4)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(4,)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">4)</span></code></p></td>
<td><p>‚úì</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">4)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">1)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">4)</span></code></p></td>
<td><p>‚úì</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">4)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(3,)</span></code></p></td>
<td><p>Error</p></td>
<td><p>‚úó (3 ‚â† 4)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3,</span> <span class="pre">4)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">4)</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">3,</span> <span class="pre">4)</span></code></p></td>
<td><p>‚úì</p></td>
</tr>
</tbody>
</table>
</div>
<p>The memory savings are dramatic. Adding a <code class="docutils literal notranslate"><span class="pre">(768,)</span></code> vector to a <code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">512,</span> <span class="pre">768)</span></code> tensor would require copying the vector 32√ó512 times without broadcasting, allocating 50 MB of redundant data (12.5 million float32 numbers). With broadcasting, you store just the original 3 KB vector.</p>
</section>
<section id="views-vs-copies">
<h3>Views vs. Copies<a class="headerlink" href="#views-vs-copies" title="Link to this heading">#</a></h3>
<p>When you reshape a tensor, does it allocate new memory or just create a different view of the same data? The answer has huge implications for both performance and correctness.</p>
<p>A view shares memory with its source. Reshaping a 1 GB tensor is instant because you‚Äôre just changing the metadata that describes how to interpret the bytes, not copying the bytes themselves. But this creates an important gotcha: modifying a view modifies the original.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># y is a VIEW of x</span>
<span class="n">y</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">99</span>    <span class="c1"># This also changes x!</span>
</pre></div>
</div>
<p>Arithmetic operations like addition always create copies because they compute new values. This is safer but uses more memory. Production code carefully manages views to avoid both memory blowup (too many copies) and silent bugs (unexpected mutations through views).</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Memory</p></th>
<th class="head"><p>Time</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">reshape()</span></code></p></td>
<td><p>View*</p></td>
<td><p>Shared</p></td>
<td><p>O(1)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">transpose()</span></code></p></td>
<td><p>View*</p></td>
<td><p>Shared</p></td>
<td><p>O(1)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">+</span> <span class="pre">-</span> <span class="pre">*</span> <span class="pre">/</span></code></p></td>
<td><p>Copy</p></td>
<td><p>New allocation</p></td>
<td><p>O(n)</p></td>
</tr>
</tbody>
</table>
</div>
<p>*When data is contiguous in memory</p>
</section>
<section id="matrix-multiplication">
<h3>Matrix Multiplication<a class="headerlink" href="#matrix-multiplication" title="Link to this heading">#</a></h3>
<p>Matrix multiplication is the computational workhorse of neural networks. Every linear layer, every attention head, every embedding lookup involves matmul. Understanding its mechanics and cost is essential.</p>
<p>The operation is simple in concept: for each output element, compute a dot product of a row from the first matrix with a column from the second. But this simplicity hides cubic complexity. Multiplying two n√ón matrices requires n¬≥ multiplications and n¬≥ additions.</p>
<p>Here‚Äôs how the educational implementation in your module works:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">matmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Matrix multiplication of two tensors.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected Tensor for matrix multiplication, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Shape validation: inner dimensions must match</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot perform matrix multiplication: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> @ </span><span class="si">{</span><span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Inner dimensions must match: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> ‚â† </span><span class="si">{</span><span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span>

    <span class="c1"># Handle 2D matrices with explicit loops (educational)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">K2</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">result_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Each output element is a dot product</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
                <span class="n">result_data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">b</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># For batched operations, use np.matmul</span>
        <span class="n">result_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result_data</span><span class="p">)</span>
</pre></div>
</div>
<p>The explicit loops in the 2D case are intentionally slower than <code class="docutils literal notranslate"><span class="pre">np.matmul</span></code> because they reveal exactly what matrix multiplication does: each output element requires K operations, and there are M√óN outputs, giving O(M√óK√óN) total operations. For square matrices, this is O(n¬≥).</p>
</section>
<section id="shape-manipulation">
<h3>Shape Manipulation<a class="headerlink" href="#shape-manipulation" title="Link to this heading">#</a></h3>
<p>Shape manipulation operations change how data is interpreted without changing the values themselves. Understanding when data is copied versus viewed is crucial for both correctness and performance.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">reshape</span></code> method reinterprets the same data with different dimensions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reshape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reshape tensor to new dimensions.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="n">shape</span>

    <span class="c1"># Handle -1 for automatic dimension inference</span>
    <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">in</span> <span class="n">new_shape</span><span class="p">:</span>
        <span class="n">known_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">unknown_idx</span> <span class="o">=</span> <span class="n">new_shape</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">new_shape</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">unknown_idx</span><span class="p">:</span>
                <span class="n">known_size</span> <span class="o">*=</span> <span class="n">dim</span>
        <span class="n">unknown_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">//</span> <span class="n">known_size</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
        <span class="n">new_shape</span><span class="p">[</span><span class="n">unknown_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">unknown_dim</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>

    <span class="c1"># Validate total elements match</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Total elements must match: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s2"> ‚â† </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">new_shape</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">reshaped_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">reshaped_data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">-1</span></code> syntax is particularly useful: it tells NumPy to infer one dimension automatically. When flattening a batch of images, <code class="docutils literal notranslate"><span class="pre">x.reshape(batch_size,</span> <span class="pre">-1)</span></code> lets NumPy calculate the feature dimension.</p>
</section>
<section id="computational-complexity">
<h3>Computational Complexity<a class="headerlink" href="#computational-complexity" title="Link to this heading">#</a></h3>
<p>Not all tensor operations are equal. Element-wise operations like addition visit each element once: O(n) time where n is the total number of elements. Reductions like sum also visit each element once. But matrix multiplication is fundamentally different.</p>
<p>Multiplying two n√ón matrices requires n¬≥ operations: for each of the n¬≤ output elements, you compute a dot product of n values. This cubic scaling is why a 2000√ó2000 matmul takes 8x longer than a 1000√ó1000 matmul, not 4x. In neural networks, matrix multiplications consume over 90% of compute time. This is precisely why GPUs exist for ML: a modern GPU has thousands of cores that can compute thousands of dot products simultaneously, turning an 800ms CPU operation into an 8ms GPU operation.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Complexity</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Element-wise (<code class="docutils literal notranslate"><span class="pre">+</span></code>, <code class="docutils literal notranslate"><span class="pre">-</span></code>, <code class="docutils literal notranslate"><span class="pre">*</span></code>)</p></td>
<td><p>O(n)</p></td>
<td><p>Linear in tensor size</p></td>
</tr>
<tr class="row-odd"><td><p>Reductions (<code class="docutils literal notranslate"><span class="pre">sum</span></code>, <code class="docutils literal notranslate"><span class="pre">mean</span></code>)</p></td>
<td><p>O(n)</p></td>
<td><p>Must visit every element</p></td>
</tr>
<tr class="row-even"><td><p>Matrix multiply (<code class="docutils literal notranslate"><span class="pre">matmul</span></code>)</p></td>
<td><p>O(n¬≥)</p></td>
<td><p>Dominates training time</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="axis-semantics">
<h3>Axis Semantics<a class="headerlink" href="#axis-semantics" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">axis</span></code> parameter in reductions specifies which dimension to collapse. Think of it as ‚Äúsum along this axis‚Äù or ‚Äúaverage out this dimension.‚Äù The result has one fewer dimension than the input.</p>
<p>For a 2D tensor with shape <code class="docutils literal notranslate"><span class="pre">(rows,</span> <span class="pre">columns)</span></code>, summing along axis 0 collapses the rows, giving you column totals. Summing along axis 1 collapses the columns, giving you row totals. Summing with <code class="docutils literal notranslate"><span class="pre">axis=None</span></code> collapses everything to a single scalar.</p>
<p>Your reduction implementations simply pass the axis to NumPy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sum tensor along specified axis.&quot;&quot;&quot;</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">keepdims=True</span></code> option preserves the reduced dimension as size 1, which is useful for broadcasting the result back.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>For shape (rows, columns) = (32, 768):

sum(axis=0) ‚Üí collapse rows    ‚Üí shape (768,)  - column totals
sum(axis=1) ‚Üí collapse columns ‚Üí shape (32,)   - row totals
sum(axis=None) ‚Üí collapse all  ‚Üí scalar

Visual:
[[1, 2, 3],      sum(axis=0)     sum(axis=1)
 [4, 5, 6]]  ‚Üí   [5, 7, 9]   or  [6, 15]
                 (down cols)     (across rows)
</pre></div>
</div>
</section>
</section>
<section id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Link to this heading">#</a></h2>
<p>Your Tensor sits at the top of a stack that reaches down to hardware. When you call <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">y</span></code>, Python calls your <code class="docutils literal notranslate"><span class="pre">__add__</span></code> method, which delegates to NumPy, which calls optimized BLAS libraries written in C and Fortran, which use CPU SIMD instructions that process multiple numbers in a single clock cycle.</p>
<figure class="align-center" id="id3">
<pre  class="mermaid">
        flowchart TB
    subgraph &quot;Your Code&quot;
        A[&quot;Python Interface&lt;br/&gt;x = Tensor([[1,2],[3,4]])&quot;]
    end

    subgraph &quot;TinyTorch&quot;
        B[&quot;Tensor Class&lt;br/&gt;shape, data, operations&quot;]
    end

    subgraph &quot;Backend&quot;
        C[&quot;NumPy&lt;br/&gt;Vectorized operations&quot;]
        D[&quot;BLAS/LAPACK&lt;br/&gt;C/Fortran libraries&quot;]
    end

    subgraph &quot;Hardware&quot;
        E[&quot;CPU SIMD&lt;br/&gt;Cache optimization&quot;]
    end

    A --&gt; B --&gt; C --&gt; D --&gt; E

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#d4edda
    style E fill:#f8d7da
    </pre><figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">Your Code</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>This is the same architecture used by PyTorch and TensorFlow, just with different backends. PyTorch replaces NumPy with a C++ engine and BLAS with CUDA kernels running on GPUs. But the Python interface and the abstractions are identical. When you understand TinyTorch‚Äôs Tensor, you understand PyTorch‚Äôs Tensor.</p>
<section id="module-integration">
<h3>Module Integration<a class="headerlink" href="#module-integration" title="Link to this heading">#</a></h3>
<p>Your Tensor is the foundation for everything that follows in TinyTorch. Every subsequent module builds on the work you do here.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Module 01: Tensor (THIS MODULE)
    ‚Üì provides foundation
    All other modules build on this
</pre></div>
</div>
</section>
</section>
<section id="common-errors-debugging">
<h2>Common Errors &amp; Debugging<a class="headerlink" href="#common-errors-debugging" title="Link to this heading">#</a></h2>
<p>These are the errors you‚Äôll encounter most often when working with tensors. Understanding why they happen will save you hours of debugging, both in this module and throughout your ML career.</p>
<section id="shape-mismatch-in-matmul">
<h3>Shape Mismatch in matmul<a class="headerlink" href="#shape-mismatch-in-matmul" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">shapes</span> <span class="pre">(2,3)</span> <span class="pre">and</span> <span class="pre">(2,2)</span> <span class="pre">not</span> <span class="pre">aligned</span></code></p>
<p>Matrix multiplication requires the inner dimensions to match. If you‚Äôre multiplying <code class="docutils literal notranslate"><span class="pre">(M,</span> <span class="pre">K)</span></code> by <code class="docutils literal notranslate"><span class="pre">(K,</span> <span class="pre">N)</span></code>, both K values must be equal. The error above happens when trying to multiply a (2,3) matrix by a (2,2) matrix: 3 ‚â† 2.</p>
<p><strong>Fix</strong>: Check your shapes. The rule is <code class="docutils literal notranslate"><span class="pre">a.shape[-1]</span></code> must equal <code class="docutils literal notranslate"><span class="pre">b.shape[-2]</span></code>.</p>
</section>
<section id="broadcasting-failures">
<h3>Broadcasting Failures<a class="headerlink" href="#broadcasting-failures" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">operands</span> <span class="pre">could</span> <span class="pre">not</span> <span class="pre">be</span> <span class="pre">broadcast</span> <span class="pre">together</span></code></p>
<p>Broadcasting fails when shapes can‚Äôt be aligned according to the rules. Remember: compare right to left, and dimensions must be equal or one must be 1.</p>
<p><strong>Examples</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">(2,3)</span> <span class="pre">+</span> <span class="pre">(3,)</span></code> ‚úì works - 3 matches 3, and the missing dimension becomes 1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">(2,3)</span> <span class="pre">+</span> <span class="pre">(2,)</span></code> ‚úó fails - comparing right to left: 3 ‚â† 2</p></li>
</ul>
<p><strong>Fix</strong>: Reshape to make dimensions compatible: <code class="docutils literal notranslate"><span class="pre">vector.reshape(-1,</span> <span class="pre">1)</span></code> or <code class="docutils literal notranslate"><span class="pre">vector.reshape(1,</span> <span class="pre">-1)</span></code></p>
</section>
<section id="reshape-size-mismatch">
<h3>Reshape Size Mismatch<a class="headerlink" href="#reshape-size-mismatch" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">cannot</span> <span class="pre">reshape</span> <span class="pre">array</span> <span class="pre">of</span> <span class="pre">size</span> <span class="pre">X</span> <span class="pre">into</span> <span class="pre">shape</span> <span class="pre">Y</span></code></p>
<p>Reshape only rearranges elements; it can‚Äôt create or destroy them. If you have 12 elements, you can reshape to (3, 4) or (2, 6) or (2, 2, 3), but not to (5, 5).</p>
<p><strong>Fix</strong>: Ensure <code class="docutils literal notranslate"><span class="pre">np.prod(old_shape)</span> <span class="pre">==</span> <span class="pre">np.prod(new_shape)</span></code></p>
</section>
<section id="missing-attributes">
<h3>Missing Attributes<a class="headerlink" href="#missing-attributes" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">AttributeError:</span> <span class="pre">'Tensor'</span> <span class="pre">has</span> <span class="pre">no</span> <span class="pre">attribute</span> <span class="pre">'shape'</span></code></p>
<p>Your <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method needs to set all required attributes. If you forget to set <code class="docutils literal notranslate"><span class="pre">self.shape</span></code>, any code that accesses <code class="docutils literal notranslate"><span class="pre">tensor.shape</span></code> will fail.</p>
<p><strong>Fix</strong>: Add <code class="docutils literal notranslate"><span class="pre">self.shape</span> <span class="pre">=</span> <span class="pre">self.data.shape</span></code> in your constructor</p>
</section>
<section id="type-errors-in-arithmetic">
<h3>Type Errors in Arithmetic<a class="headerlink" href="#type-errors-in-arithmetic" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">TypeError:</span> <span class="pre">unsupported</span> <span class="pre">operand</span> <span class="pre">type(s)</span> <span class="pre">for</span> <span class="pre">+:</span> <span class="pre">'Tensor'</span> <span class="pre">and</span> <span class="pre">'int'</span></code></p>
<p>Your arithmetic methods need to handle both Tensor and scalar operands. When someone writes <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">2</span></code>, your <code class="docutils literal notranslate"><span class="pre">__add__</span></code> receives the integer 2, not a Tensor.</p>
<p><strong>Fix</strong>: Check for scalars: <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">isinstance(other,</span> <span class="pre">(int,</span> <span class="pre">float)):</span> <span class="pre">...</span></code></p>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-pytorch">
<h3>Your Implementation vs. PyTorch<a class="headerlink" href="#your-implementation-vs-pytorch" title="Link to this heading">#</a></h3>
<p>Your TinyTorch Tensor and PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> share the same conceptual design. The differences are in implementation: PyTorch uses a C++ backend for speed, supports GPUs for massive parallelism, and implements thousands of specialized operations. But the Python API, broadcasting rules, and shape semantics are identical.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Backend</strong></p></td>
<td><p>NumPy (Python)</p></td>
<td><p>C++/CUDA</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Speed</strong></p></td>
<td><p>1x (baseline)</p></td>
<td><p>10-100x faster</p></td>
</tr>
<tr class="row-even"><td><p><strong>GPU</strong></p></td>
<td><p>‚úó CPU only</p></td>
<td><p>‚úì CUDA, Metal, ROCm</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Operations</strong></p></td>
<td><p>~15 core ops</p></td>
<td><p>2000+ operations</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows equivalent operations in TinyTorch and PyTorch. Notice how closely the APIs mirror each other. This is intentional: by learning TinyTorch‚Äôs patterns, you‚Äôre simultaneously learning PyTorch‚Äôs patterns.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
‚ö° PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs walk through each line to understand the comparison:</p>
<ul class="simple">
<li><p><strong>Line 1 (Import)</strong>: Both frameworks use a simple import. TinyTorch exposes <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> from <code class="docutils literal notranslate"><span class="pre">core.tensor</span></code>; PyTorch uses <code class="docutils literal notranslate"><span class="pre">torch.tensor()</span></code> as a factory function.</p></li>
<li><p><strong>Line 3 (Creation)</strong>: TinyTorch infers dtype from input; PyTorch requires explicit <code class="docutils literal notranslate"><span class="pre">dtype=torch.float32</span></code> for floating-point operations. This explicitness matters for performance tuning in production.</p></li>
<li><p><strong>Line 4 (Broadcasting)</strong>: Both handle <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">2</span></code> identically, broadcasting the scalar across all elements. Same semantics, same result.</p></li>
<li><p><strong>Line 5 (Matrix multiply)</strong>: TinyTorch uses <code class="docutils literal notranslate"><span class="pre">.matmul()</span></code> method; PyTorch supports both <code class="docutils literal notranslate"><span class="pre">.matmul()</span></code> and the <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> operator. The operation is identical.</p></li>
<li><p><strong>Line 6 (Reduction)</strong>: Both use <code class="docutils literal notranslate"><span class="pre">.mean()</span></code> to reduce the tensor to a scalar. Reductions like this are fundamental to computing loss values.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>Broadcasting rules, shape semantics, and API design patterns. When you debug PyTorch shape errors, you‚Äôll understand exactly what‚Äôs happening because you built the same abstractions.</p>
</div>
</section>
<section id="why-tensors-matter-at-scale">
<h3>Why Tensors Matter at Scale<a class="headerlink" href="#why-tensors-matter-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate why tensor operations matter, consider the scale of modern ML systems:</p>
<ul class="simple">
<li><p><strong>Large language models</strong>: 175 billion numbers stored as tensors = <strong>350 GB</strong> (like storing 70,000 full-resolution photos)</p></li>
<li><p><strong>Image processing</strong>: A batch of 128 images = <strong>77 MB</strong> of tensor data</p></li>
<li><p><strong>Self-driving cars</strong>: Process tensor operations at <strong>36 FPS</strong> across multiple cameras (each frame = millions of operations in 28 milliseconds)</p></li>
</ul>
<p>A single matrix multiplication can consume <strong>90% of computation time</strong> in neural networks. Understanding tensor operations isn‚Äôt just academic; it‚Äôs essential for building and debugging real ML systems.</p>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions. They‚Äôre designed to build intuition for the performance characteristics you‚Äôll encounter in production ML.</p>
<p><strong>Q1: Memory Calculation</strong></p>
<p>A batch of 32 RGB images (224√ó224 pixels) stored as float32. How much memory?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>32 √ó 3 √ó 224 √ó 224 √ó 4 = <strong>77,070,336 bytes ‚âà 77 MB</strong></p>
<p>This is why batch size matters - double the batch, double the memory!</p>
</div>
<p><strong>Q2: Broadcasting Savings</strong></p>
<p>Adding a vector <code class="docutils literal notranslate"><span class="pre">(768,)</span></code> to a 3D tensor <code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">512,</span> <span class="pre">768)</span></code>. How much memory does broadcasting save?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Without broadcasting: 32 √ó 512 √ó 768 √ó 4 = <strong>50.3 MB</strong></p>
<p>With broadcasting: 768 √ó 4 = <strong>3 KB</strong></p>
<p>Savings: <strong>~50 MB per operation</strong> - this adds up across hundreds of operations in a neural network!</p>
</div>
<p><strong>Q3: Matmul Scaling</strong></p>
<p>If a 1000√ó1000 matmul takes 100ms, how long will 2000√ó2000 take?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Matmul is O(n¬≥). Doubling n ‚Üí 2¬≥ = <strong>8x longer</strong> ‚Üí ~800ms</p>
<p>This is why matrix size matters so much for transformer scaling!</p>
</div>
<p><strong>Q4: Shape Prediction</strong></p>
<p>What‚Äôs the output shape of <code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">1,</span> <span class="pre">768)</span> <span class="pre">+</span> <span class="pre">(512,</span> <span class="pre">768)</span></code>?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Broadcasting aligns right-to-left:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">(32,</span>&#160;&#160; <span class="pre">1,</span> <span class="pre">768)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">(</span>&#160;&#160;&#160; <span class="pre">512,</span> <span class="pre">768)</span></code></p></li>
</ul>
<p>Result: <strong>(32, 512, 768)</strong></p>
<p>The 1 broadcasts to 512, and 32 is prepended.</p>
</div>
<p><strong>Q5: Views vs Copies</strong></p>
<p>You reshape a 1GB tensor, then modify one element in the reshaped version. What happens to the original tensor? What if you had used <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">0</span></code> instead of reshape?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Reshape (view)</strong>: The original tensor IS modified. Reshape creates a view that shares memory with the original. Changing <code class="docutils literal notranslate"><span class="pre">y.data[0,0]</span> <span class="pre">=</span> <span class="pre">99</span></code> also changes <code class="docutils literal notranslate"><span class="pre">x.data[0]</span></code>.</p>
<p><strong>Addition (copy)</strong>: The original tensor is NOT modified. <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">0</span></code> creates a new tensor with freshly allocated memory. The values are identical but stored in different locations.</p>
<p>This distinction matters enormously for:</p>
<ul class="simple">
<li><p><strong>Memory</strong>: Views use 0 extra bytes; copies use n extra bytes</p></li>
<li><p><strong>Performance</strong>: Views are O(1); copies are O(n)</p></li>
<li><p><strong>Correctness</strong>: Unexpected mutations through views are a common source of bugs</p></li>
</ul>
</div>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>For students who want to understand the academic foundations and mathematical underpinnings of tensor operations:</p>
<section id="seminal-papers">
<h3>Seminal Papers<a class="headerlink" href="#seminal-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>NumPy: Array Programming</strong> - Harris et al. (2020). The definitive reference for NumPy, which underlies your Tensor implementation. Explains broadcasting, views, and the design philosophy. <a class="reference external" href="https://doi.org/10.1038/s41586-020-2649-2">Nature</a></p></li>
<li><p><strong>BLAS (Basic Linear Algebra Subprograms)</strong> - Lawson et al. (1979). The foundation of all high-performance matrix operations. Your <code class="docutils literal notranslate"><span class="pre">np.matmul</span></code> ultimately calls BLAS routines optimized over 40+ years. Understanding BLAS levels (1, 2, 3) explains why matmul is special. <a class="reference external" href="https://doi.org/10.1145/355841.355847">ACM TOMS</a></p></li>
<li><p><strong>Automatic Differentiation in ML</strong> - Baydin et al. (2018). Survey of automatic differentiation techniques. <a class="reference external" href="https://www.jmlr.org/papers/v18/17-468.html">JMLR</a></p></li>
</ul>
</section>
<section id="additional-resources">
<h3>Additional Resources<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Textbook</strong>: ‚ÄúDeep Learning‚Äù by Goodfellow, Bengio, and Courville - Chapter 2 covers linear algebra foundations including tensor operations</p></li>
<li><p><strong>Documentation</strong>: <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html">PyTorch Tensor Tutorial</a> - See how production frameworks implement similar concepts</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../tiers/foundation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Foundation Tier (Modules 01-08)</p>
      </div>
    </a>
    <a class="right-next"
       href="02_activations_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Module 02: Activations</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building">What You‚Äôre NOT Building</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constructor">Constructor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties">Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arithmetic-operations">Arithmetic Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-shape-operations">Matrix &amp; Shape Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reductions">Reductions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-dimensionality">Tensor Dimensionality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#views-vs-copies">Views vs. Copies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication">Matrix Multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-manipulation">Shape Manipulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity">Computational Complexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#axis-semantics">Axis Semantics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture">Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-integration">Module Integration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors-debugging">Common Errors &amp; Debugging</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatch-in-matmul">Shape Mismatch in matmul</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting-failures">Broadcasting Failures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reshape-size-mismatch">Reshape Size Mismatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-attributes">Missing Attributes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-errors-in-arithmetic">Type Errors in Arithmetic</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-tensors-matter-at-scale">Why Tensors Matter at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>