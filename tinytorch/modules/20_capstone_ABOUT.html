
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 20: Capstone" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/20_capstone_ABOUT.html" />
<meta property="og:site_name" content="Tinyüî•Torch" />
<meta property="og:description" content="üöÄ Launch Binder Run interactively in your browser. No setup required. https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F20_capstone%2F20_capstone.ipynb üìÑ View S..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="Tinyüî•Torch" />
<meta name="description" content="üöÄ Launch Binder Run interactively in your browser. No setup required. https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F20_capstone%2F20_capstone.ipynb üìÑ View S..." />

    <title>Module 20: Capstone &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1a2c4ab3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";

const defaultStyle = document.createElement('style');
defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}
`;
document.head.appendChild(defaultStyle);

const fullscreenStyle = document.createElement('style');
fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
document.head.appendChild(fullscreenStyle);

// Detect if page has dark background
const isDarkTheme = () => {
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        return brightness < 128;
    }
    return false;
};

const load = async () => {
    await mermaid.run();

    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(load, 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(load, 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(load, 200);
        return;
    }

    const darkTheme = isDarkTheme();

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    const modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    const modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    let previousScrollOffset = [window.scrollX, window.scrollY];

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    const allButtons = [];

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });

        container.appendChild(fullscreenBtn);
        allButtons.push(fullscreenBtn);
    });

    // Update theme classes when theme changes
    const updateTheme = () => {
        const dark = isDarkTheme();
        allButtons.forEach(btn => {
            if (dark) {
                btn.classList.add('dark-theme');
            } else {
                btn.classList.remove('dark-theme');
            }
        });
        if (dark) {
            modal.classList.add('dark-theme');
            modalContent.classList.add('dark-theme');
            closeBtn.classList.add('dark-theme');
        } else {
            modal.classList.remove('dark-theme');
            modalContent.classList.remove('dark-theme');
            closeBtn.classList.remove('dark-theme');
        }
    };

    // Watch for theme changes
    const observer = new MutationObserver(updateTheme);
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    observer.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style']
    });
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/20_capstone_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=b021a6d4"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=ee94e95f"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/subscribe-modal.js?v=82641629"></script>
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="TITO Command Reference" href="../tito/overview.html" />
    <link rel="prev" title="Torch Olympics (Module 20)" href="../tiers/olympics.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="/tinytorch/_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="/tinytorch/_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://mlsysbook.ai" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mlsysbook" title="Support" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-heart fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Support</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-07)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_autograd_ABOUT.html">05. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_optimizers_ABOUT.html">06. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_training_ABOUT.html">07. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (08-13)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_dataloader_ABOUT.html">08. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_memoization_ABOUT.html">17. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_acceleration_ABOUT.html">18. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/20_capstone_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 20: Capstone</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarkreport-constructor">BenchmarkReport Constructor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarkreport-properties">BenchmarkReport Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-methods">Core Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-dependencies-and-imports">Module Dependencies and Imports</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reproducibility-crisis-in-ml">The Reproducibility Crisis in ML</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-three-pillars-of-reliable-benchmarking">The Three Pillars of Reliable Benchmarking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latency-vs-throughput-a-critical-distinction">Latency vs Throughput: A Critical Distinction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-rigor-why-variance-matters">Statistical Rigor: Why Variance Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-optimization-trade-off-triangle">The Optimization Trade-off Triangle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#schema-validation-enabling-automation">Schema Validation: Enabling Automation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-measurement-traps">Performance Measurement Traps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#system-integration-the-complete-ml-lifecycle">System Integration: The Complete ML Lifecycle</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-industry-standards">Your Implementation vs. Industry Standards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-benchmarking-matters-at-scale">Why Benchmarking Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-20-capstone">
<h1>Module 20: Capstone<a class="headerlink" href="#module-20-capstone" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>OPTIMIZATION TIER</strong> | Difficulty: ‚óè‚óè‚óè‚óè | Time: 6-8 hours | Prerequisites: All modules (01-19)</p>
<p><strong>Prerequisites: All modules</strong> means you‚Äôve built a complete ML framework. This capstone assumes:</p>
<ul class="simple">
<li><p>Complete TinyTorch framework (Modules 01-13) - <strong>Required</strong></p></li>
<li><p>Optimization techniques (Modules 14-18) - <strong>Optional but recommended</strong></p></li>
<li><p>Benchmarking methodology (Module 19) - <strong>Required</strong></p></li>
</ul>
<p>The core benchmarking functionality (Parts 1-4) works with just Modules 01-13 and 19. Modules 14-18 enable the advanced optimization workflow (Part 4b), which demonstrates how to integrate all TinyTorch components. If optimization modules aren‚Äôt available, the system gracefully degrades to baseline benchmarking only.</p>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run interactively in your browser. No setup required.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F20_capstone%2F20_capstone.ipynb"><span>https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F20_capstone%2F20_capstone.ipynb</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìÑ View Source</div>
<p class="sd-card-text">Browse the implementation code on GitHub.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/20_capstone/20_capstone.py"><span>https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/20_capstone/20_capstone.py</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üéß Audio Overview</div>
<p class="sd-card-text">Listen to an AI-generated overview.</p>
<audio controls style="width: 100%; margin-top: 8px;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/20_capstone.mp3" type="audio/mpeg">
</audio>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>You‚Äôve built an entire machine learning framework from scratch across 19 modules. You can train neural networks, implement transformers, optimize models with quantization and pruning, and measure performance with profiling tools. But there‚Äôs one critical piece missing: proving your work with reproducible results.</p>
<p>In production ML systems, claims without measurements are worthless. This capstone module teaches you to benchmark models comprehensively, document optimizations systematically, and generate standardized submissions that mirror industry practices like MLPerf and Papers with Code. You‚Äôll learn the complete workflow from baseline measurement through optimization to final submission, using the same patterns employed by ML research labs and production engineering teams.</p>
<p>By the end, you‚Äôll have a professional benchmarking system that demonstrates your framework‚Äôs capabilities and enables fair comparisons with others‚Äô implementations.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> comprehensive benchmarking infrastructure measuring accuracy, latency, throughput, and memory</p></li>
<li><p><strong>Master</strong> the three pillars of reliable benchmarking: repeatability, comparability, and completeness</p></li>
<li><p><strong>Understand</strong> performance measurement traps (variance, cold starts, batch effects) and how to avoid them</p></li>
<li><p><strong>Connect</strong> your TinyTorch implementation to production ML workflows (experiment tracking, A/B testing, regression detection)</p></li>
<li><p><strong>Generate</strong> schema-validated JSON submissions that enable reproducible comparisons and community sharing</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Benchmarking System&quot;
        A[&quot;BenchmarkReport&lt;br/&gt;measure performance&quot;]
        B[&quot;Submission&lt;br/&gt;standardized format&quot;]
        C[&quot;Validation&lt;br/&gt;schema compliance&quot;]
    end

    subgraph &quot;Workflow&quot;
        D[&quot;Baseline Model&quot;]
        E[&quot;Optimization&quot;]
        F[&quot;Comparison&quot;]
    end

    D --&gt; A
    E --&gt; A
    A --&gt; B
    B --&gt; C
    A --&gt; F

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
    style E fill:#e2d5f1
    style F fill:#ffe4e1
    </pre><figcaption>
<p><span class="caption-number">Fig. 29 </span><span class="caption-text">Benchmarking System</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Part</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SimpleMLP</span></code></p></td>
<td><p>Baseline model for benchmarking demonstrations</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BenchmarkReport</span></code></p></td>
<td><p>Comprehensive performance measurement with statistical rigor</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">generate_submission()</span></code></p></td>
<td><p>Standardized JSON generation with schema compliance</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">validate_submission_schema()</span></code></p></td>
<td><p>Automated validation ensuring data quality</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Complete workflows</p></td>
<td><p>Baseline, optimization, comparison, submission pipeline</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Professional ML workflow</span>
<span class="n">report</span> <span class="o">=</span> <span class="n">BenchmarkReport</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;my_model&quot;</span><span class="p">)</span>
<span class="n">report</span><span class="o">.</span><span class="n">benchmark_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">num_runs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">submission</span> <span class="o">=</span> <span class="n">generate_submission</span><span class="p">(</span>
    <span class="n">baseline_report</span><span class="o">=</span><span class="n">baseline_report</span><span class="p">,</span>
    <span class="n">optimized_report</span><span class="o">=</span><span class="n">optimized_report</span><span class="p">,</span>
    <span class="n">techniques_applied</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;quantization&quot;</span><span class="p">,</span> <span class="s2">&quot;pruning&quot;</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">save_submission</span><span class="p">(</span><span class="n">submission</span><span class="p">,</span> <span class="s2">&quot;results.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this capstone focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>Automated CI/CD pipelines (production systems run benchmarks on every commit)</p></li>
<li><p>Multi-hardware comparison (benchmarking across CPU/GPU/TPU)</p></li>
<li><p>Visualization dashboards (plotting accuracy vs latency trade-off curves)</p></li>
<li><p>Leaderboard aggregation (combining community submissions)</p></li>
</ul>
<p><strong>You are building the measurement and reporting foundation.</strong> Automation and visualization come later in production MLOps.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section provides a quick reference for the benchmarking classes and functions you‚Äôll build. Use this while implementing and debugging.</p>
<section id="benchmarkreport-constructor">
<h3>BenchmarkReport Constructor<a class="headerlink" href="#benchmarkreport-constructor" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BenchmarkReport</span><span class="p">(</span><span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BenchmarkReport</span>
</pre></div>
</div>
<p>Creates a benchmark report instance that measures and stores model performance metrics along with system context for reproducibility.</p>
</section>
<section id="benchmarkreport-properties">
<h3>BenchmarkReport Properties<a class="headerlink" href="#benchmarkreport-properties" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Property</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p>Identifier for the model being benchmarked</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">metrics</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">dict</span></code></p></td>
<td><p>Performance measurements (accuracy, latency, etc.)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">system_info</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">dict</span></code></p></td>
<td><p>Platform, Python version, NumPy version</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">timestamp</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p>When benchmark was run (ISO format)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="core-methods">
<h3>Core Methods<a class="headerlink" href="#core-methods" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">benchmark_model</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">benchmark_model(model,</span> <span class="pre">X_test,</span> <span class="pre">y_test,</span> <span class="pre">num_runs=100)</span> <span class="pre">-&gt;</span> <span class="pre">dict</span></code></p></td>
<td><p>Measures accuracy, latency (mean ¬± std), throughput, memory</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">generate_submission</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">generate_submission(baseline_report,</span> <span class="pre">optimized_report=None,</span> <span class="pre">...)</span> <span class="pre">-&gt;</span> <span class="pre">dict</span></code></p></td>
<td><p>Creates standardized JSON with baseline, optimized, improvements</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">save_submission</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">save_submission(submission,</span> <span class="pre">filepath=&quot;submission.json&quot;)</span> <span class="pre">-&gt;</span> <span class="pre">str</span></code></p></td>
<td><p>Writes JSON to file with validation</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">validate_submission_schema</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">validate_submission_schema(submission)</span> <span class="pre">-&gt;</span> <span class="pre">bool</span></code></p></td>
<td><p>Validates structure and value ranges</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="module-dependencies-and-imports">
<h3>Module Dependencies and Imports<a class="headerlink" href="#module-dependencies-and-imports" title="Link to this heading">#</a></h3>
<p>This capstone integrates components from across TinyTorch:</p>
<p><strong>Core dependencies (required):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Linear</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.activations</span><span class="w"> </span><span class="kn">import</span> <span class="n">ReLU</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.losses</span><span class="w"> </span><span class="kn">import</span> <span class="n">CrossEntropyLoss</span>
</pre></div>
</div>
<p><strong>Optimization modules (optional):</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># These imports use try/except blocks for graceful degradation</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.perf.profiling</span><span class="w"> </span><span class="kn">import</span> <span class="n">Profiler</span><span class="p">,</span> <span class="n">quick_profile</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.perf.compression</span><span class="w"> </span><span class="kn">import</span> <span class="n">magnitude_prune</span><span class="p">,</span> <span class="n">structured_prune</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.benchmarking</span><span class="w"> </span><span class="kn">import</span> <span class="n">Benchmark</span><span class="p">,</span> <span class="n">BenchmarkResult</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># Core benchmarking still works without optimization modules</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>The advanced optimization workflow (Part 4b) demonstrates these optional integrations, but the core benchmarking system (Parts 1-4) works with just the foundation modules (01-13) and basic benchmarking (19).</p>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental principles of professional ML benchmarking. These concepts apply to every ML system, from research papers to production deployments.</p>
<section id="the-reproducibility-crisis-in-ml">
<h3>The Reproducibility Crisis in ML<a class="headerlink" href="#the-reproducibility-crisis-in-ml" title="Link to this heading">#</a></h3>
<p>Modern machine learning faces a credibility problem. Many published results cannot be replicated because researchers omit critical details. When a paper claims ‚Äú92% accuracy with 10ms latency,‚Äù can you reproduce that result? Often not, because the paper doesn‚Äôt specify hardware platform, software versions, batch size, measurement methodology, or variance across runs.</p>
<p>Industry standards like MLPerf and Papers with Code emerged to address this crisis by requiring:</p>
<ul class="simple">
<li><p><strong>Standardized tasks</strong> with fixed datasets</p></li>
<li><p><strong>Hardware specifications</strong> documented completely</p></li>
<li><p><strong>Measurement protocols</strong> defined precisely</p></li>
<li><p><strong>Code submissions</strong> for automated verification</p></li>
</ul>
<p>Your benchmarking system implements these same principles. When you generate a submission, it captures everything needed for someone else to verify your claims or build on your work.</p>
</section>
<section id="the-three-pillars-of-reliable-benchmarking">
<h3>The Three Pillars of Reliable Benchmarking<a class="headerlink" href="#the-three-pillars-of-reliable-benchmarking" title="Link to this heading">#</a></h3>
<p>Professional benchmarking rests on three foundational principles: repeatability, comparability, and completeness.</p>
<p><strong>Repeatability</strong> means running the same experiment multiple times produces the same result. This requires fixed random seeds, consistent test datasets, and measuring variance across runs. A single measurement of ‚Äú10.3ms‚Äù is worthless because you don‚Äôt know if that‚Äôs typical or an outlier. Measuring 100 times and reporting ‚Äú10.0ms ¬± 0.5ms‚Äù tells you the true performance and its variability.</p>
<p>Here‚Äôs how your implementation ensures repeatability:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Measure latency with statistical rigor</span>
<span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Single sample inference</span>
    <span class="n">latencies</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>  <span class="c1"># Convert to ms</span>

<span class="n">avg_latency</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
<span class="n">std_latency</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
</pre></div>
</div>
<p>The loop runs inference 100 times (by default) to capture variance. The first few runs may be slower due to cold caches, and occasional runs may hit garbage collection pauses. By aggregating many measurements, you get a statistically valid estimate.</p>
<p><strong>Comparability</strong> means different people can fairly compare results. This requires documenting the environment completely:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_get_system_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Collect system information for reproducibility.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;platform&#39;</span><span class="p">:</span> <span class="n">platform</span><span class="o">.</span><span class="n">platform</span><span class="p">(),</span>
        <span class="s1">&#39;python_version&#39;</span><span class="p">:</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
        <span class="s1">&#39;numpy_version&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">__version__</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>When someone sees your submission claiming 10ms latency, they need to know if that was measured on a MacBook or a server with 32 CPU cores. Platform differences can cause 10x performance variations, making cross-platform comparisons meaningless without context.</p>
<p><strong>Completeness</strong> means capturing all relevant metrics, not cherry-picking favorable ones. Your <code class="docutils literal notranslate"><span class="pre">benchmark_model</span></code> method measures six distinct metrics:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;parameter_count&#39;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">param_count</span><span class="p">),</span>
    <span class="s1">&#39;model_size_mb&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">model_size_mb</span><span class="p">),</span>
    <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">accuracy</span><span class="p">),</span>
    <span class="s1">&#39;latency_ms_mean&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">avg_latency</span><span class="p">),</span>
    <span class="s1">&#39;latency_ms_std&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">std_latency</span><span class="p">),</span>
    <span class="s1">&#39;throughput_samples_per_sec&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="mi">1000</span> <span class="o">/</span> <span class="n">avg_latency</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Each metric answers a different question. Parameter count indicates model capacity. Model size determines deployment cost. Accuracy measures task performance. Latency affects user experience. Throughput determines batch processing capacity. Optimizations create trade-offs between these dimensions, so measuring all of them prevents hiding downsides.</p>
</section>
<section id="latency-vs-throughput-a-critical-distinction">
<h3>Latency vs Throughput: A Critical Distinction<a class="headerlink" href="#latency-vs-throughput-a-critical-distinction" title="Link to this heading">#</a></h3>
<p>Many beginners confuse latency and throughput because both relate to speed. They measure fundamentally different things.</p>
<p><strong>Latency</strong> measures per-sample speed: how long does it take to process one input? This matters for real-time applications where users wait for results. Your implementation measures latency by timing single-sample inference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Latency: time for ONE sample</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Shape: (1, features)</span>
<span class="n">latency_ms</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
</pre></div>
</div>
<p>A model with 10ms latency processes one input in 10 milliseconds. If a user submits a query, they wait 10ms for a response. This directly impacts user experience.</p>
<p><strong>Throughput</strong> measures batch capacity: how many inputs can you process per second? This matters for offline batch jobs processing millions of examples. Your implementation derives throughput from latency:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">throughput_samples_per_sec</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">/</span> <span class="n">avg_latency</span>
</pre></div>
</div>
<p>If latency is 10ms per sample, throughput is 1000ms / 10ms = 100 samples/second. But this assumes processing samples one at a time. In practice, batching increases throughput significantly while adding latency. Processing a batch of 32 samples might take 50ms total, giving 640 samples/second throughput but 50ms per-request latency.</p>
<p>The trade-off: <strong>Batching increases throughput but hurts latency.</strong> A production API serving individual user requests optimizes for latency. A batch processing pipeline optimizes for throughput.</p>
</section>
<section id="statistical-rigor-why-variance-matters">
<h3>Statistical Rigor: Why Variance Matters<a class="headerlink" href="#statistical-rigor-why-variance-matters" title="Link to this heading">#</a></h3>
<p>Single measurements lie. Variance tells the truth about performance consistency.</p>
<p>Consider two models, both with mean latency of 10.0ms. Model A has standard deviation of 0.5ms. Model B has standard deviation of 4.2ms. Which would you deploy?</p>
<p>Model A‚Äôs predictable performance (9.5-10.5ms range) provides consistent user experience. Model B‚Äôs erratic performance (sometimes 6ms, sometimes 15ms) creates frustration. Users prefer reliable slowness over unpredictable speed.</p>
<p>Your implementation captures this variance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">latencies</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">avg_latency</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>
<span class="n">std_latency</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">latencies</span><span class="p">)</span>  <span class="c1"># Captures variance</span>
</pre></div>
</div>
<p>Running 100 iterations isn‚Äôt just for accuracy of the mean. It also characterizes the distribution. High standard deviation indicates performance varies significantly run-to-run, perhaps due to garbage collection pauses, cache effects, or OS scheduling.</p>
<p>In production systems, engineers track percentiles (p50, p90, p99) to understand tail latency. The p99 latency tells you ‚Äú99% of requests complete within this time,‚Äù which matters more for user experience than mean latency. One user experiencing a 100ms delay (because they hit p99) has a worse experience than if all users consistently saw 20ms.</p>
</section>
<section id="the-optimization-trade-off-triangle">
<h3>The Optimization Trade-off Triangle<a class="headerlink" href="#the-optimization-trade-off-triangle" title="Link to this heading">#</a></h3>
<p>Every optimization involves trade-offs between three competing objectives: speed (latency), size (memory), and accuracy. You can optimize for any two, but achieving all three simultaneously is impossible with current techniques.</p>
<p><strong>Fast + Small</strong> means aggressive optimization. Quantizing to INT8 reduces model size 4x and speeds up inference 2x, but typically costs 1-2% accuracy. Pruning 50% of weights halves memory and adds another speedup, but may lose another 1-2% accuracy. You‚Äôve traded accuracy for efficiency.</p>
<p><strong>Fast + Accurate</strong> means careful optimization. You might quantize only certain layers, or use INT16 instead of INT8. You preserve accuracy but achieve less compression. The model is faster but not dramatically smaller.</p>
<p><strong>Small + Accurate</strong> means conservative techniques. Knowledge distillation transfers accuracy from a large teacher to a small student. The student is smaller and maintains accuracy, but may be slower than aggressive quantization because it still operates in FP32.</p>
<p>Your submission captures these trade-offs automatically:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">submission</span><span class="p">[</span><span class="s1">&#39;improvements&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;speedup&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">baseline_latency</span> <span class="o">/</span> <span class="n">optimized_latency</span><span class="p">),</span>
    <span class="s1">&#39;compression_ratio&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">baseline_size</span> <span class="o">/</span> <span class="n">optimized_size</span><span class="p">),</span>
    <span class="s1">&#39;accuracy_delta&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
        <span class="n">optimized_report</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">baseline_report</span><span class="o">.</span><span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
    <span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A speedup of 2.3x with compression of 4.1x but accuracy delta of -0.01 (-1%) shows you chose the ‚Äúfast + small‚Äù corner of the triangle. A speedup of 1.2x with compression of 1.5x but accuracy delta of 0.00 shows you chose ‚Äúaccurate + moderately fast.‚Äù</p>
</section>
<section id="schema-validation-enabling-automation">
<h3>Schema Validation: Enabling Automation<a class="headerlink" href="#schema-validation-enabling-automation" title="Link to this heading">#</a></h3>
<p>Your submission format uses a structured JSON schema that enforces completeness and type safety. This isn‚Äôt bureaucracy‚Äîit enables powerful automation.</p>
<p>Without schema validation, submissions become inconsistent. One person reports accuracy as a percentage string (‚Äú92%‚Äù), another as a float (0.92), another as an integer (92). Aggregating these results requires manual cleaning. With schema validation, every submission uses the same format:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Schema-enforced format</span>
<span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>  <span class="c1"># Always 0.0-1.0 float</span>

<span class="c1"># Validation catches errors</span>
<span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Accuracy must be in [0, 1]&quot;</span>
</pre></div>
</div>
<p>This enables automated processing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Aggregate community results automatically</span>
<span class="n">all_submissions</span> <span class="o">=</span> <span class="p">[</span><span class="n">load_json</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">submission_files</span><span class="p">]</span>
<span class="n">avg_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="s1">&#39;baseline&#39;</span><span class="p">][</span><span class="s1">&#39;metrics&#39;</span><span class="p">][</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_submissions</span><span class="p">])</span>

<span class="c1"># Build leaderboards</span>
<span class="n">sorted_by_speedup</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">all_submissions</span><span class="p">,</span>
                           <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="p">[</span><span class="s1">&#39;improvements&#39;</span><span class="p">][</span><span class="s1">&#39;speedup&#39;</span><span class="p">],</span>
                           <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Detect regressions in CI/CD</span>
<span class="k">if</span> <span class="n">new_latency</span> <span class="o">&gt;</span> <span class="n">baseline_latency</span> <span class="o">*</span> <span class="mf">1.1</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Performance regression: 10</span><span class="si">% s</span><span class="s2">lower!&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The schema also enables forward compatibility. When you add new optional fields, old submissions remain valid. When you require new fields, the version number increments, and validation enforces the migration.</p>
</section>
<section id="performance-measurement-traps">
<h3>Performance Measurement Traps<a class="headerlink" href="#performance-measurement-traps" title="Link to this heading">#</a></h3>
<p>Real-world benchmarking is full of subtle traps that invalidate measurements. Understanding these pitfalls is crucial for accurate results.</p>
<p><strong>Trap 1: Measuring the Wrong Thing.</strong> If you time model creation instead of just inference, you‚Äôre measuring initialization overhead, not runtime performance. If you include data loading in the timing loop, you‚Äôre measuring I/O speed, not model speed. The fix is isolating exactly what you want to measure:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prepare data BEFORE timing</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">create_test_input</span><span class="p">()</span>

<span class="c1"># Time ONLY the operation you care about</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Only this is timed</span>
<span class="n">latency</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="c1"># Process output AFTER timing</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">postprocess</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Trap 2: Ignoring System Noise.</strong> Operating systems multitask. Your benchmark might get interrupted by background processes, garbage collection, or CPU thermal throttling. Single measurements capture noise. Multiple measurements average it out. Your implementation runs 100 iterations by default to handle this.</p>
<p><strong>Trap 3: Cold Start Effects.</strong> The first inference is often slower because caches are cold and JIT compilers haven‚Äôt optimized yet. Production benchmarks typically discard the first N runs as ‚Äúwarm-up.‚Äù Your implementation includes warm-up inherently by averaging all runs‚Äîthe few slow cold starts get averaged with many fast warm runs.</p>
<p><strong>Trap 4: Batch Size Confusion.</strong> Measuring latency on batch_size=32 then dividing by 32 doesn‚Äôt give per-sample latency. Batching amortizes overhead, so batch latency / batch_size underestimates per-sample latency. Always measure with the same batch size as production deployment.</p>
</section>
<section id="system-integration-the-complete-ml-lifecycle">
<h3>System Integration: The Complete ML Lifecycle<a class="headerlink" href="#system-integration-the-complete-ml-lifecycle" title="Link to this heading">#</a></h3>
<p>This capstone represents the final stage of the ML systems lifecycle, but it‚Äôs also the beginning of the next iteration. Production ML systems operate in a never-ending loop:</p>
<ol class="arabic simple">
<li><p><strong>Research &amp; Development</strong> - Build models (Modules 01-13)</p></li>
<li><p><strong>Baseline Measurement</strong> - Benchmark unoptimized performance (Module 19)</p></li>
<li><p><strong>Optimization</strong> - Apply techniques like quantization and pruning (Modules 14-18)</p></li>
<li><p><strong>Validation</strong> - Benchmark optimized version (Module 19)</p></li>
<li><p><strong>Decision</strong> - Keep optimization if improvements outweigh costs (Module 20)</p></li>
<li><p><strong>Deployment</strong> - Serve model in production</p></li>
<li><p><strong>Monitoring</strong> - Track performance over time, detect regressions</p></li>
<li><p><strong>Iteration</strong> - When performance degrades or requirements change, loop back to step 3</p></li>
</ol>
<p>Your submission captures a snapshot of this cycle. The baseline metrics document performance before optimization. The optimized metrics show results after applying techniques. The improvements section quantifies the delta. The techniques_applied list enables reproducibility.</p>
<p>In production, engineers maintain this documentation across hundreds of experiments. When a deployment‚Äôs latency increases from 10ms to 30ms three months later, they consult the original benchmark to understand what changed. Without system_info and reproducible measurements, debugging becomes guesswork.</p>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-industry-standards">
<h3>Your Implementation vs. Industry Standards<a class="headerlink" href="#your-implementation-vs-industry-standards" title="Link to this heading">#</a></h3>
<p>Your TinyTorch benchmarking system implements the same principles used by production ML frameworks and research competitions, just at educational scale.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>Production Systems</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Metrics</strong></p></td>
<td><p>6 core metrics (accuracy, latency, etc.)</p></td>
<td><p>20+ metrics including p99 latency, memory bandwidth</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Runs</strong></p></td>
<td><p>100 iterations for variance</p></td>
<td><p>1000+ runs, discard outliers</p></td>
</tr>
<tr class="row-even"><td><p><strong>Validation</strong></p></td>
<td><p>Python assertions</p></td>
<td><p>JSON Schema, automated CI checks</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Format</strong></p></td>
<td><p>Simple JSON</p></td>
<td><p>Protobuf, versioned schemas</p></td>
</tr>
<tr class="row-even"><td><p><strong>Scale</strong></p></td>
<td><p>Single model benchmarks</p></td>
<td><p>Automated pipelines tracking 1000s of experiments</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows how your educational implementation translates to production tools.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.olympics</span><span class="w"> </span><span class="kn">import</span> <span class="n">BenchmarkReport</span><span class="p">,</span> <span class="n">generate_submission</span>

<span class="c1"># Benchmark baseline</span>
<span class="n">baseline_report</span> <span class="o">=</span> <span class="n">BenchmarkReport</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;my_model&quot;</span><span class="p">)</span>
<span class="n">baseline_report</span><span class="o">.</span><span class="n">benchmark_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">num_runs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Benchmark optimized</span>
<span class="n">optimized_report</span> <span class="o">=</span> <span class="n">BenchmarkReport</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;optimized_model&quot;</span><span class="p">)</span>
<span class="n">optimized_report</span><span class="o">.</span><span class="n">benchmark_model</span><span class="p">(</span><span class="n">opt_model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">num_runs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Generate submission</span>
<span class="n">submission</span> <span class="o">=</span> <span class="n">generate_submission</span><span class="p">(</span>
    <span class="n">baseline_report</span><span class="o">=</span><span class="n">baseline_report</span><span class="p">,</span>
    <span class="n">optimized_report</span><span class="o">=</span><span class="n">optimized_report</span><span class="p">,</span>
    <span class="n">techniques_applied</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;quantization&quot;</span><span class="p">,</span> <span class="s2">&quot;pruning&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">save_submission</span><span class="p">(</span><span class="n">submission</span><span class="p">,</span> <span class="s2">&quot;results.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
Production MLflow</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">mlflow</span>

<span class="c1"># Track baseline experiment</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">(</span><span class="n">run_name</span><span class="o">=</span><span class="s2">&quot;baseline&quot;</span><span class="p">):</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">({</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;my_model&quot;</span><span class="p">})</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">benchmark_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="s2">&quot;model.pkl&quot;</span><span class="p">)</span>

<span class="c1"># Track optimized experiment</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">(</span><span class="n">run_name</span><span class="o">=</span><span class="s2">&quot;optimized&quot;</span><span class="p">):</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">({</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;optimized_model&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;techniques&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;quantization&quot;</span><span class="p">,</span> <span class="s2">&quot;pruning&quot;</span><span class="p">]})</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">benchmark_model</span><span class="p">(</span><span class="n">opt_model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="s2">&quot;optimized_model.pkl&quot;</span><span class="p">)</span>

<span class="c1"># Compare experiments in MLflow UI</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs walk through the comparison line by line:</p>
<ul class="simple">
<li><p><strong>Line 1 (Import)</strong>: TinyTorch uses a simple module import. MLflow provides enterprise-grade experiment tracking with databases and web UIs.</p></li>
<li><p><strong>Line 4 (Benchmark baseline)</strong>: TinyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">BenchmarkReport</span></code> mirrors MLflow‚Äôs experiment runs. Both capture metrics and system context.</p></li>
<li><p><strong>Line 8 (Benchmark optimized)</strong>: Same API in both‚Äîcreate report, benchmark model. This consistency makes transitioning to production tools natural.</p></li>
<li><p><strong>Line 12 (Generate submission)</strong>: TinyTorch generates JSON. MLflow logs to a database that supports querying, visualization, and comparison.</p></li>
<li><p><strong>Line 18 (Save)</strong>: TinyTorch saves to file. MLflow persists to SQL database with version control and artifact storage.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>The workflow pattern: baseline ‚Üí optimize ‚Üí benchmark ‚Üí compare ‚Üí decide. Whether you use TinyTorch or MLflow, this cycle is fundamental to production ML. The tools scale, but the methodology remains the same.</p>
</div>
</section>
<section id="why-benchmarking-matters-at-scale">
<h3>Why Benchmarking Matters at Scale<a class="headerlink" href="#why-benchmarking-matters-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate why professional benchmarking matters, consider the scale of production ML systems:</p>
<ul class="simple">
<li><p><strong>Model serving</strong>: A recommendation system handles 10 million requests/day. If you reduce latency from 20ms to 10ms, you save 100,000 seconds of compute daily = 1.16 days of compute per day = 42% cost reduction.</p></li>
<li><p><strong>Training efficiency</strong>: Training a large language model costs $1 million in GPU time. Profiling reveals 60% of time is spent in data loading. Optimizing the data pipeline saves $600,000.</p></li>
<li><p><strong>Deployment constraints</strong>: A mobile app‚Äôs model must fit in 50MB. Quantization compresses a 200MB model to 50MB with 1% accuracy loss. The app ships; without benchmarking, you wouldn‚Äôt know the trade-off was acceptable.</p></li>
</ul>
<p>Systematic benchmarking with reproducible results isn‚Äôt academic exercise‚Äîit‚Äôs how engineers justify technical decisions and demonstrate business impact.</p>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions about benchmarking and performance measurement.</p>
<p><strong>Q1: Memory Calculation</strong></p>
<p>A model has 5 million parameters stored as FP32. After INT8 quantization, how much memory is saved?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>FP32: 5,000,000 parameters √ó 4 bytes = 20,000,000 bytes = <strong>20 MB</strong></p>
<p>INT8: 5,000,000 parameters √ó 1 byte = 5,000,000 bytes = <strong>5 MB</strong></p>
<p>Savings: 20 MB - 5 MB = <strong>15 MB</strong> (75% reduction)</p>
<p>Compression ratio: 20 MB / 5 MB = <strong>4.0x</strong></p>
<p>This is why quantization is standard in mobile deployment‚Äîmodels must fit in tight memory budgets.</p>
</div>
<p><strong>Q2: Latency Variance Analysis</strong></p>
<p>Model A: 10.0ms ¬± 0.3ms latency. Model B: 10.0ms ¬± 3.0ms latency. Both have same accuracy. Which do you deploy and why?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Deploy Model A.</strong></p>
<p>Same mean latency (10.0ms) but Model A has 10x lower variance (0.3ms vs 3.0ms std).</p>
<p>Model A‚Äôs latency range: ~9.4-10.6ms (95% confidence: ¬± 2 std)
Model B‚Äôs latency range: ~4.0-16.0ms (95% confidence: ¬± 2 std)</p>
<p><strong>Why consistency matters:</strong></p>
<ul class="simple">
<li><p>Users prefer predictable performance over erratic speed</p></li>
<li><p>High variance suggests GC pauses, cache misses, or resource contention</p></li>
<li><p>Production SLAs commit to p99 latency‚ÄîModel B‚Äôs p99 could be 16ms vs Model A‚Äôs 11ms</p></li>
</ul>
<p>In production, <strong>reliability &gt; mean performance</strong>. A consistently decent experience beats an unreliable fast one.</p>
</div>
<p><strong>Q3: Batch Size Trade-off</strong></p>
<p>Measuring latency with batch_size=32 gives 100ms total. Can you claim 100ms / 32 = 3.1ms per-sample latency?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>No.</strong> This underestimates per-sample latency.</p>
<p>Batching amortizes fixed overhead (data transfer, kernel launch). Per-sample latency at batch=1 is higher than batch=32 divided by 32.</p>
<p>Example reality:</p>
<ul class="simple">
<li><p>Batch=32: 100ms total ‚Üí 3.1ms per sample (amortized)</p></li>
<li><p>Batch=1: 8ms total ‚Üí 8ms per sample (actual)</p></li>
</ul>
<p><strong>Why the discrepancy?</strong></p>
<ul class="simple">
<li><p>Fixed overhead: 10ms (data transfer, setup)</p></li>
<li><p>Variable cost: 90ms / 32 = 2.8ms per sample</p></li>
<li><p>At batch=1: 10ms fixed + 2.8ms variable = 12.8ms</p></li>
</ul>
<p><strong>Always benchmark at deployment batch size.</strong> If production serves single requests, measure with batch=1.</p>
</div>
<p><strong>Q4: Speedup Calculation</strong></p>
<p>Baseline: 20ms latency. Optimized: 5ms latency. What is the speedup and what does it mean?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Speedup = baseline_latency / optimized_latency = 20ms / 5ms = <strong>4.0x</strong></p>
<p><strong>What it means:</strong></p>
<ul class="simple">
<li><p>Optimized model is <strong>4 times faster</strong></p></li>
<li><p>Processes same input in 1/4 the time</p></li>
<li><p>Can handle 4x more traffic with same hardware</p></li>
</ul>
<p><strong>Real-world impact:</strong></p>
<ul class="simple">
<li><p>If baseline served 100 requests/sec, optimized serves 400 requests/sec</p></li>
<li><p>If baseline cost $1000/month in compute, optimized costs $250/month</p></li>
<li><p>If baseline met latency SLA at 60% utilization, optimized has 85% headroom</p></li>
</ul>
<p><strong>Note:</strong> Speedup alone doesn‚Äôt tell the full story. Check accuracy_delta and compression_ratio to understand trade-offs.</p>
</div>
<p><strong>Q5: Schema Validation Value</strong></p>
<p>Why does the submission schema require <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> as float in [0, 1] instead of allowing any format?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Type safety enables automation.</strong></p>
<p>Without schema:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Different submissions, different formats (breaks aggregation)</span>
<span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="s2">&quot;92%&quot;</span><span class="p">}</span>      <span class="c1"># String</span>
<span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mi">92</span><span class="p">}</span>         <span class="c1"># Integer (out of 100)</span>
<span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.92</span><span class="p">}</span>       <span class="c1"># Float</span>
<span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="s2">&quot;good&quot;</span><span class="p">}</span>     <span class="c1"># Non-numeric</span>
</pre></div>
</div>
</div>
<p>Aggregating these requires manual parsing and error handling.</p>
<p>With schema:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># All submissions use same format (aggregation works)</span>
<span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.92</span><span class="p">}</span>  <span class="c1"># Always float in [0.0, 1.0]</span>
</pre></div>
</div>
<p><strong>Benefits:</strong></p>
<ol class="arabic simple">
<li><p><strong>Automated validation</strong> - Reject invalid submissions immediately</p></li>
<li><p><strong>Aggregation</strong> - <code class="docutils literal notranslate"><span class="pre">np.mean([s['accuracy']</span> <span class="pre">for</span> <span class="pre">s</span> <span class="pre">in</span> <span class="pre">submissions])</span></code> just works</p></li>
<li><p><strong>Comparison</strong> - Sort by accuracy without parsing different formats</p></li>
<li><p><strong>APIs</strong> - Other tools can consume submissions without custom parsers</p></li>
</ol>
<p><strong>Real example:</strong> Papers with Code leaderboards require strict schemas. Thousands of submissions from different teams aggregate automatically because everyone follows the same format.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
## Further Reading

For students who want to understand the academic foundations and industry standards for ML benchmarking:

### Seminal Papers

- **MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance** - Mattson et al. (2020). Defines standardized ML benchmarks for hardware comparison. The gold standard for fair performance comparisons. [arXiv:1910.01500](https://arxiv.org/abs/1910.01500)

- **A Step Toward Quantifying Independently Reproducible Machine Learning Research** - Pineau et al. (2021). Analyzes reproducibility crisis in ML and proposes requirements for verifiable claims. Introduces reproducibility checklist adopted by NeurIPS. [arXiv:2104.05563](https://arxiv.org/abs/2104.05563)

- **Hidden Technical Debt in Machine Learning Systems** - Sculley et al. (2015). Identifies systems challenges in production ML, including monitoring, versioning, and reproducibility. Required reading for ML systems engineers. [NeurIPS 2015](https://papers.nips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html)

### Additional Resources

- **MLflow Documentation**: [https://mlflow.org/](https://mlflow.org/) - Production experiment tracking system implementing patterns from this module
- **Papers with Code**: [https://paperswithcode.com/](https://paperswithcode.com/) - See how research papers submit benchmarks with reproducible code
- **Weights &amp; Biases Best Practices**: [https://wandb.ai/site/experiment-tracking](https://wandb.ai/site/experiment-tracking) - Industry standard for ML experiment management

## What&#39;s Next

```{seealso} Congratulations: You&#39;ve Completed TinyTorch!

You&#39;ve built a complete ML framework from scratch‚Äîfrom basic tensors to production-ready benchmarking. You understand how PyTorch works under the hood, how optimizations affect performance, and how to measure and document results professionally. These skills transfer directly to production ML engineering.
</pre></div>
</div>
<p><strong>Next Steps - Applying Your Knowledge:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Direction</p></th>
<th class="head"><p>What To Build</p></th>
<th class="head"><p>Skills Applied</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Advanced Optimizations</strong></p></td>
<td><p>Benchmark milestone models (MNIST CNN, Transformer) with Modules 14-18 techniques</p></td>
<td><p>Apply learned optimizations to real models</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Production Systems</strong></p></td>
<td><p>Integrate MLflow or Weights &amp; Biases into your projects</p></td>
<td><p>Scale benchmarking to team workflows</p></td>
</tr>
<tr class="row-even"><td><p><strong>Research Contributions</strong></p></td>
<td><p>Submit to Papers with Code using your schema validation patterns</p></td>
<td><p>Share reproducible results with community</p></td>
</tr>
<tr class="row-odd"><td><p><strong>MLOps Automation</strong></p></td>
<td><p>Build CI/CD pipelines that run benchmarks on every commit</p></td>
<td><p>Detect performance regressions automatically</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/20_capstone/20_capstone.ipynb">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/20_capstone/20_capstone.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../tiers/olympics.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Torch Olympics (Module 20)</p>
      </div>
    </a>
    <a class="right-next"
       href="../tito/overview.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">TITO Command Reference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarkreport-constructor">BenchmarkReport Constructor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarkreport-properties">BenchmarkReport Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-methods">Core Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-dependencies-and-imports">Module Dependencies and Imports</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reproducibility-crisis-in-ml">The Reproducibility Crisis in ML</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-three-pillars-of-reliable-benchmarking">The Three Pillars of Reliable Benchmarking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latency-vs-throughput-a-critical-distinction">Latency vs Throughput: A Critical Distinction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-rigor-why-variance-matters">Statistical Rigor: Why Variance Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-optimization-trade-off-triangle">The Optimization Trade-off Triangle</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#schema-validation-enabling-automation">Schema Validation: Enabling Automation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-measurement-traps">Performance Measurement Traps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#system-integration-the-complete-ml-lifecycle">System Integration: The Complete ML Lifecycle</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-industry-standards">Your Implementation vs. Industry Standards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-benchmarking-matters-at-scale">Why Benchmarking Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>