{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp perf.acceleration\n",
    "#| export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dac111",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 17: Acceleration - Hardware-Aware Optimization\n",
    "\n",
    "Welcome to Module 17! You're about to master the art of neural network acceleration through vectorization and kernel fusion.\n",
    "\n",
    "## ğŸ”— Prerequisites & Progress\n",
    "**You've Built**: Complete neural network foundation with tensors (01), layers (03), autograd (06), training (08), and CNNs (09)\n",
    "**You'll Build**: Acceleration techniques including vectorization and operation fusion\n",
    "**You'll Enable**: Hardware-efficient execution for production deployment\n",
    "\n",
    "**Connection Map**:\n",
    "```\n",
    "Layers (03) â†’ Training (08) â†’ CNNs (09) â†’ Acceleration (17) â†’ Memoization (18)\n",
    "(building blocks) (learning)   (spatial)  (speed up)         (KV-cache)\n",
    "```\n",
    "\n",
    "**Prerequisites**: Modules 01-15 must be working\n",
    "Before starting, verify:\n",
    "- [ ] Module 01 (Tensor): Tensor class works\n",
    "- [ ] Module 06 (Autograd): Gradients work\n",
    "- [ ] Module 09 (Convolutions): Conv2d works (optional)\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "1. Implement vectorized operations for maximum throughput\n",
    "2. Create fused operations to reduce memory bandwidth\n",
    "3. Understand the relationship between compute and memory bandwidth\n",
    "4. Analyze acceleration trade-offs in production systems\n",
    "\n",
    "Let's optimize for speed!\n",
    "\n",
    "## ğŸ“¦ Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/17_acceleration/acceleration_dev.py`\n",
    "**Building Side:** Code exports to `tinytorch.perf.acceleration`\n",
    "\n",
    "```python\n",
    "# How to use this module:\n",
    "from tinytorch.perf.acceleration import vectorized_matmul, fused_gelu\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Complete acceleration system in one focused module for deep understanding\n",
    "- **Production:** Proper organization like PyTorch's torch.cuda and torch.backends with optimization components\n",
    "- **Consistency:** All acceleration operations and optimization components in perf.acceleration\n",
    "- **Integration:** Works seamlessly with neural network layers for complete performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19444cf6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-imports-core",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "import warnings\n",
    "\n",
    "# Constants for performance measurement\n",
    "DEFAULT_WARMUP_ITERATIONS = 2  # Default warmup iterations for timing\n",
    "DEFAULT_TIMING_ITERATIONS = 5  # Default timing iterations for measurement\n",
    "BYTES_PER_FLOAT32 = 4  # Standard float32 size in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a22f8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ’¡ Motivation: Why Acceleration Matters\n",
    "\n",
    "Before we learn acceleration techniques, let's understand the performance gap.\n",
    "Neural networks often underutilize hardware due to:\n",
    "- Sequential operations (no parallelism)\n",
    "- Poor memory access patterns (cache misses)\n",
    "- Missing SIMD (Single Instruction, Multiple Data) opportunities\n",
    "- Separate operations (memory bandwidth waste)\n",
    "\n",
    "We'll fix these issues with vectorization and kernel fusion, achieving 2-5Ã— speedups!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c8d92",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ’¡ Introduction - The Performance Challenge\n",
    "\n",
    "Modern neural networks face two fundamental bottlenecks that limit their speed:\n",
    "\n",
    "### The Two Enemies of Performance\n",
    "\n",
    "**1. Compute Bound Operations:**\n",
    "```\n",
    "CPU/GPU Cores: [====BUSY====] [====BUSY====] [====BUSY====]\n",
    "Memory Bus:    [---idle---] [---idle---] [---idle---]\n",
    "\n",
    "When: Matrix multiplication, convolutions\n",
    "Solution: Vectorization, better algorithms\n",
    "```\n",
    "\n",
    "**2. Memory Bound Operations:**\n",
    "```\n",
    "CPU/GPU Cores: [--idle--] [--idle--] [--idle--]\n",
    "Memory Bus:    [========SATURATED========]\n",
    "\n",
    "When: Element-wise operations, small tensors\n",
    "Solution: Kernel fusion, memory layout optimization\n",
    "```\n",
    "\n",
    "### The Roofline Model - Your Performance Compass\n",
    "\n",
    "Every processor has fundamental limits:\n",
    "\n",
    "```\n",
    "Performance   â”‚   Compute Bound Region\n",
    "(GFLOPS)      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "              â”‚  â”‚ Peak Performance\n",
    "              â”‚  â”‚\n",
    "              â”‚ â•±â”‚ Memory Bound Region\n",
    "              â”‚â•± â”‚\n",
    "             â•±â”‚  â”‚\n",
    "            â•± â”‚  â”‚\n",
    "           â•±  â”‚  â”‚\n",
    "          â•±â”€â”€â”€â”‚â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "         â•±    â”‚  â”‚\n",
    "        â•±     â”‚  â”‚\n",
    "       â•±â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Arithmetic Intensity\n",
    "              â”‚  â”‚        (FLOPs/Byte)\n",
    "           Lowâ”‚  â”‚High\n",
    "```\n",
    "\n",
    "**Key Insight**: Understand where your operations live on this graph to optimize effectively.\n",
    "\n",
    "### Why This Module Matters\n",
    "\n",
    "Real-world performance wins:\n",
    "- **2-5Ã— speedup** from vectorization\n",
    "- **2-3Ã— throughput** from kernel fusion\n",
    "- **10Ã— scaling improvement** for large models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a123e5b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "tensor-import",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "# Import from TinyTorch package (previous modules must be completed and exported)\n",
    "from tinytorch.core.tensor import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479709a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ“ Foundations - Vectorization: From Loops to Lightning\n",
    "\n",
    "### The SIMD Revolution\n",
    "\n",
    "Modern processors can execute **Single Instruction, Multiple Data** operations:\n",
    "\n",
    "```\n",
    "Traditional Loop (Scalar):               SIMD Vectorized:\n",
    "for i in range(4):        â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
    "    c[i] = a[i] + b[i]    â”‚ ALU â”‚  â†’   â”‚ALU 0â”‚ALU 1â”‚ALU 2â”‚ALU 3â”‚\n",
    "                          â””â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n",
    "                          1 element     4 elements per cycle\n",
    "                          per cycle\n",
    "```\n",
    "\n",
    "### Memory Access Patterns: The Hidden Performance Killer\n",
    "\n",
    "```\n",
    "Sequential Access (FAST):\n",
    "Memory: [A][B][C][D][E][F][G][H]\n",
    "Access:  â†“  â†“  â†“  â†“  â†’ Cache friendly\n",
    "\n",
    "Strided Access (SLOWER):\n",
    "Memory: [A][ ][B][ ][C][ ][D][ ]\n",
    "Access:  â†“     â†“     â†“     â†“   â†’ Cache misses\n",
    "\n",
    "Random Access (SLOWEST):\n",
    "Memory: [A][B][C][D][E][F][G][H]\n",
    "Access:  â†“     â†‘  â†“     â†‘       â†’ Cache chaos\n",
    "```\n",
    "\n",
    "### Matrix Multiplication: The King of Vectorization\n",
    "\n",
    "Matrix multiplication is **perfectly suited** for vectorization:\n",
    "\n",
    "```\n",
    "Matrix A (MÃ—K) Ã— Matrix B (KÃ—N) = Matrix C (MÃ—N)\n",
    "\n",
    "Computation Pattern:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ aâ‚â‚ aâ‚â‚‚ aâ‚â‚ƒ aâ‚â‚„ â”‚ Ã— â”‚ bâ‚â‚ bâ‚â‚‚ bâ‚â‚ƒ bâ‚â‚„ â”‚ = â”‚ câ‚â‚ câ‚â‚‚ câ‚â‚ƒ câ‚â‚„ â”‚\n",
    "â”‚ aâ‚‚â‚ aâ‚‚â‚‚ aâ‚‚â‚ƒ aâ‚‚â‚„ â”‚   â”‚ bâ‚‚â‚ bâ‚‚â‚‚ bâ‚‚â‚ƒ bâ‚‚â‚„ â”‚   â”‚ câ‚‚â‚ câ‚‚â‚‚ câ‚‚â‚ƒ câ‚‚â‚„ â”‚\n",
    "â”‚ aâ‚ƒâ‚ aâ‚ƒâ‚‚ aâ‚ƒâ‚ƒ aâ‚ƒâ‚„ â”‚   â”‚ bâ‚ƒâ‚ bâ‚ƒâ‚‚ bâ‚ƒâ‚ƒ bâ‚ƒâ‚„ â”‚   â”‚ câ‚ƒâ‚ câ‚ƒâ‚‚ câ‚ƒâ‚ƒ câ‚ƒâ‚„ â”‚\n",
    "â”‚ aâ‚„â‚ aâ‚„â‚‚ aâ‚„â‚ƒ aâ‚„â‚„ â”‚   â”‚ bâ‚„â‚ bâ‚„â‚‚ bâ‚„â‚ƒ bâ‚„â‚„ â”‚   â”‚ câ‚„â‚ câ‚„â‚‚ câ‚„â‚ƒ câ‚„â‚„ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "For câ‚â‚: Rowâ‚ Â· Columnâ‚ = aâ‚â‚Ã—bâ‚â‚ + aâ‚â‚‚Ã—bâ‚‚â‚ + aâ‚â‚ƒÃ—bâ‚ƒâ‚ + aâ‚â‚„Ã—bâ‚„â‚\n",
    "                                    â†‘\n",
    "                              VECTORIZABLE!\n",
    "```\n",
    "\n",
    "**Why vectorization wins:**\n",
    "- **High arithmetic intensity**: 2NÂ³ FLOPs for NÂ³ data\n",
    "- **Predictable memory access**: Sequential row/column reads\n",
    "- **Parallelizable**: Independent dot products\n",
    "- **Cache-friendly**: Data reuse in inner loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479204ef",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "vectorized-matmul",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def vectorized_matmul(a: Tensor, b: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    High-performance matrix multiplication using vectorized operations.\n",
    "\n",
    "    This implementation leverages optimized BLAS libraries that use:\n",
    "    - SIMD instructions for parallel computation\n",
    "    - Cache-blocking for memory efficiency\n",
    "    - Multi-threading for CPU parallelization\n",
    "\n",
    "    TODO: Implement production-grade matrix multiplication\n",
    "\n",
    "    APPROACH:\n",
    "    1. Validate shapes are compatible for matrix multiplication\n",
    "    2. Use NumPy's optimized dot product (calls BLAS GEMM)\n",
    "    3. Return result wrapped in Tensor\n",
    "\n",
    "    Args:\n",
    "        a: First tensor for multiplication (MÃ—K or batchÃ—MÃ—K)\n",
    "        b: Second tensor for multiplication (KÃ—N or batchÃ—KÃ—N)\n",
    "\n",
    "    Returns:\n",
    "        Result tensor of shape (MÃ—N or batchÃ—MÃ—N)\n",
    "\n",
    "    EXAMPLE:\n",
    "    Matrix multiplication visualization:\n",
    "    >>> a = Tensor([[1, 2], [3, 4]])  # 2Ã—2\n",
    "    >>> b = Tensor([[5, 6], [7, 8]])  # 2Ã—2\n",
    "    >>> result = vectorized_matmul(a, b)\n",
    "    >>> print(result.data)\n",
    "    [[19 22]    # [1Ã—5+2Ã—7, 1Ã—6+2Ã—8] = [19, 22]\n",
    "     [43 50]]   # [3Ã—5+4Ã—7, 3Ã—6+4Ã—8] = [43, 50]\n",
    "\n",
    "    PERFORMANCE CHARACTERISTICS:\n",
    "    - Time Complexity: O(NÂ³) but highly optimized\n",
    "    - Space Complexity: O(NÂ²) for result\n",
    "    - Arithmetic Intensity: 2NÂ³ FLOPs / 3NÂ² bytes = 2N/3 (good for large N)\n",
    "\n",
    "    HINTS:\n",
    "    - Check a.shape[-1] == b.shape[-2] for inner dimension match\n",
    "    - Use np.matmul() for batch support and optimization\n",
    "    - Trust BLAS to handle the vectorization magic\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Input validation for matrix multiplication\n",
    "    if len(a.shape) < 2 or len(b.shape) < 2:\n",
    "        raise ValueError(\n",
    "            f\"Matrix multiplication requires 2D+ tensors, got shapes {a.shape} and {b.shape}. \"\n",
    "            f\"ğŸ’¡ HINT: Use reshape() to add dimensions if needed.\"\n",
    "        )\n",
    "\n",
    "    if a.shape[-1] != b.shape[-2]:\n",
    "        raise ValueError(\n",
    "            f\"Matrix multiplication shape mismatch: {a.shape} @ {b.shape}. \"\n",
    "            f\"Inner dimensions must match: a.shape[-1]={a.shape[-1]} != b.shape[-2]={b.shape[-2]}. \"\n",
    "            f\"ğŸ’¡ HINT: For A@B, A's columns must equal B's rows.\"\n",
    "        )\n",
    "\n",
    "    # Use NumPy's highly optimized matrix multiplication\n",
    "    # This calls BLAS GEMM (General Matrix Multiply), which uses:\n",
    "    # - SIMD vectorization for parallel arithmetic\n",
    "    # - Cache blocking for memory efficiency\n",
    "    # - Multi-threading on multi-core systems\n",
    "    result_data = np.matmul(a.data, b.data)\n",
    "\n",
    "    return Tensor(result_data)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949c435",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-vectorized-matmul",
     "locked": true,
     "points": 10
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_vectorized_matmul():\n",
    "    \"\"\"ğŸ”¬ Test vectorized matrix multiplication implementation.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Vectorized Matrix Multiplication...\")\n",
    "\n",
    "    # Test basic 2D multiplication\n",
    "    a = Tensor([[1, 2], [3, 4]])\n",
    "    b = Tensor([[5, 6], [7, 8]])\n",
    "    result = vectorized_matmul(a, b)\n",
    "\n",
    "    expected = np.array([[19, 22], [43, 50]])\n",
    "    assert np.allclose(result.data, expected), f\"Basic matmul failed: expected {expected}, got {result.data}\"\n",
    "\n",
    "    # Test batch multiplication (3D tensors)\n",
    "    batch_size, m, k, n = 2, 3, 4, 5\n",
    "    a_batch = Tensor(np.random.randn(batch_size, m, k))\n",
    "    b_batch = Tensor(np.random.randn(batch_size, k, n))\n",
    "    result_batch = vectorized_matmul(a_batch, b_batch)\n",
    "\n",
    "    assert result_batch.shape == (batch_size, m, n), f\"Wrong batch shape: {result_batch.shape}\"\n",
    "\n",
    "    # Test broadcasting (different batch dimensions)\n",
    "    a_single = Tensor(np.random.randn(m, k))\n",
    "    b_batch = Tensor(np.random.randn(batch_size, k, n))\n",
    "    result_broadcast = vectorized_matmul(a_single, b_batch)\n",
    "\n",
    "    assert result_broadcast.shape == (batch_size, m, n), f\"Broadcasting failed: {result_broadcast.shape}\"\n",
    "\n",
    "    # Test error cases\n",
    "    try:\n",
    "        vectorized_matmul(Tensor([1, 2, 3]), Tensor([4, 5]))  # 1D tensors\n",
    "        assert False, \"Should reject 1D tensors\"\n",
    "    except ValueError as e:\n",
    "        assert \"2D+\" in str(e)\n",
    "\n",
    "    try:\n",
    "        vectorized_matmul(Tensor([[1, 2]]), Tensor([[1], [2], [3]]))  # Shape mismatch\n",
    "        assert False, \"Should reject incompatible shapes\"\n",
    "    except ValueError as e:\n",
    "        assert \"shape mismatch\" in str(e).lower()\n",
    "\n",
    "    print(\"âœ… vectorized_matmul works correctly!\")\n",
    "\n",
    "# Test is callable but runs via test_module() in main block below\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_unit_vectorized_matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281db080",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Implementation - Kernel Fusion: Eliminating Memory Bottlenecks\n",
    "\n",
    "### The Memory Bandwidth Crisis\n",
    "\n",
    "Consider this innocent-looking computation: `y = gelu(x * weight + bias)`\n",
    "\n",
    "**Naive Implementation (Memory Intensive):**\n",
    "```\n",
    "Step 1: temp1 = x * weight     â†’ Write 4GB to memory\n",
    "Step 2: temp2 = temp1 + bias   â†’ Read 4GB, Write 4GB\n",
    "Step 3: y = gelu(temp2)        â†’ Read 4GB, Write 4GB\n",
    "                                 Total: 20GB memory traffic!\n",
    "```\n",
    "\n",
    "**Fused Implementation (Memory Efficient):**\n",
    "```\n",
    "Single Step: y = gelu(x * weight + bias)  â†’ Read 8GB, Write 4GB\n",
    "                                            Total: 12GB memory traffic!\n",
    "                                            60% memory bandwidth reduction!\n",
    "```\n",
    "\n",
    "### Understanding GELU: The Smooth Activation\n",
    "\n",
    "GELU (Gaussian Error Linear Unit) is used in transformers because it's **smooth** (differentiable everywhere):\n",
    "\n",
    "```\n",
    "Activation Functions Compared:\n",
    "\n",
    "ReLU:           GELU:           Sigmoid:\n",
    "     |               |                 1 â”Œâ”€â”€â”€â”€â”€\n",
    "     |               |               â•±   â”‚\n",
    "     |           â•±â”€â”€â”€â”‚â”€â”€â”€            â•±   â”‚\n",
    "â”€â”€â”€â”€â”€â”˜       â•±â”€â”€â”€    â”‚         â”€â”€â”€â•±      â”‚\n",
    " Discontinuous   Smooth Curve    â”‚ Smooth but saturates\n",
    " gradient at 0   everywhere      â”‚\n",
    "```\n",
    "\n",
    "**GELU Formula**: `GELU(x) = x * Î¦(x)` where Î¦ is the standard normal CDF\n",
    "\n",
    "**Fast Approximation**: `GELU(x) â‰ˆ 0.5 * x * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715 * xÂ³)))`\n",
    "\n",
    "### Kernel Fusion Strategy\n",
    "\n",
    "```\n",
    "Unfused Operations:                    Fused Operation:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ xÂ³ computation  â”‚ â†’ temp1           â”‚                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                    â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                    â”‚\n",
    "â”‚ polynomial part â”‚ â†’ temp2           â”‚   All operations   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   combined in      â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   single kernel    â”‚\n",
    "â”‚ tanh computationâ”‚ â†’ temp3           â”‚                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚                    â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                    â”‚\n",
    "â”‚ final multiply  â”‚ â†’ result          â”‚                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "5 memory round-trips                   1 memory round-trip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb206e",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "fused-gelu",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def fused_gelu(x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Fused GELU activation that combines all operations in a single kernel.\n",
    "\n",
    "    GELU combines the benefits of ReLU and sigmoid:\n",
    "    - Smooth everywhere (unlike ReLU's discontinuity at 0)\n",
    "    - Non-saturating for positive values (unlike sigmoid)\n",
    "    - Probabilistic interpretation: x * P(X â‰¤ x) where X ~ N(0,1)\n",
    "\n",
    "    Mathematical Definition:\n",
    "    GELU(x) = x * Î¦(x) where Î¦(x) is the standard normal CDF\n",
    "\n",
    "    Fast Approximation (used here):\n",
    "    GELU(x) â‰ˆ 0.5 * x * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715 * xÂ³)))\n",
    "\n",
    "    TODO: Implement fused GELU to minimize memory bandwidth\n",
    "\n",
    "    APPROACH:\n",
    "    1. Compute all intermediate values in a single expression\n",
    "    2. Avoid creating temporary arrays\n",
    "    3. Let NumPy's broadcasting handle vectorization\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor to apply GELU activation\n",
    "\n",
    "    Returns:\n",
    "        GELU-activated tensor (same shape as input)\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> x = Tensor([-2, -1, 0, 1, 2])\n",
    "    >>> result = fused_gelu(x)\n",
    "    >>> print(result.data)\n",
    "    [-0.04550026 -0.15865526  0.          0.8413447   1.9544997 ]\n",
    "    # Notice: smooth transition through 0, positive bias\n",
    "\n",
    "    MEMORY EFFICIENCY:\n",
    "    - Unfused: 5 temporary arrays Ã— input_size Ã— 4 bytes\n",
    "    - Fused: 0 temporary arrays, direct computation\n",
    "    - Bandwidth reduction: ~80% for memory-bound operations\n",
    "\n",
    "    HINTS:\n",
    "    - Use np.sqrt(2.0 / np.pi) for the constant\n",
    "    - Keep entire expression in one line for maximum fusion\n",
    "    - NumPy will optimize the expression tree automatically\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Mathematical constant for GELU approximation\n",
    "    sqrt_2_over_pi = np.sqrt(2.0 / np.pi)\n",
    "\n",
    "    # Fused GELU computation - all operations in single expression\n",
    "    # This minimizes memory bandwidth by avoiding intermediate arrays\n",
    "    # NumPy's expression evaluator will optimize this into efficient machine code\n",
    "    result_data = 0.5 * x.data * (\n",
    "        1.0 + np.tanh(sqrt_2_over_pi * (x.data + 0.044715 * x.data**3))\n",
    "    )\n",
    "\n",
    "    return Tensor(result_data)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344b6d2",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-fused-gelu",
     "locked": true,
     "points": 10
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_fused_gelu():\n",
    "    \"\"\"ğŸ”¬ Test fused GELU activation implementation.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Fused GELU...\")\n",
    "\n",
    "    # Test basic properties\n",
    "    x = Tensor([-3, -1, 0, 1, 3])\n",
    "    result = fused_gelu(x)\n",
    "\n",
    "    # GELU(0) = 0 (exact property)\n",
    "    assert abs(result.data[2]) < 1e-6, f\"GELU(0) should be 0, got {result.data[2]}\"\n",
    "\n",
    "    # GELU is smooth and increasing\n",
    "    assert result.data[4] > result.data[3] > result.data[2], \"GELU should be increasing\"\n",
    "\n",
    "    # GELU has positive bias (unlike ReLU)\n",
    "    assert result.data[3] > 0.8, \"GELU(1) should be close to 1\"\n",
    "    assert result.data[1] > -0.2, \"GELU(-1) should be slightly negative\"\n",
    "\n",
    "    # Test numerical stability with extreme values\n",
    "    x_extreme = Tensor([-10, -5, 0, 5, 10])\n",
    "    result_extreme = fused_gelu(x_extreme)\n",
    "\n",
    "    assert not np.any(np.isnan(result_extreme.data)), \"No NaN values allowed\"\n",
    "    assert not np.any(np.isinf(result_extreme.data)), \"No infinite values allowed\"\n",
    "\n",
    "    # Test large tensor processing\n",
    "    x_large = Tensor(np.random.randn(1000, 1000).astype(np.float32))\n",
    "    result_large = fused_gelu(x_large)\n",
    "\n",
    "    assert result_large.shape == x_large.shape, \"Shape preservation failed\"\n",
    "    assert result_large.data.dtype == np.float32, \"Data type preservation failed\"\n",
    "\n",
    "    # Test that positive inputs are mostly preserved (GELU â‰ˆ x for large positive x)\n",
    "    x_positive = Tensor([5.0])\n",
    "    result_positive = fused_gelu(x_positive)\n",
    "    assert result_positive.data[0] > 4.9, \"Large positive values should be nearly preserved\"\n",
    "\n",
    "    print(\"âœ… fused_gelu works correctly!\")\n",
    "\n",
    "# Test is callable but runs via test_module() in main block below\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_unit_fused_gelu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0411cb",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ğŸ”¬ Performance Analysis: Measuring Fusion Benefits\n",
    "\n",
    "Let's quantify the impact of kernel fusion by comparing fused vs unfused implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae559e9",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "unfused-gelu",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def unfused_gelu(x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Deliberately unfused GELU implementation for performance comparison.\n",
    "\n",
    "    This version creates multiple intermediate tensors to simulate\n",
    "    the memory bandwidth overhead of unfused operations.\n",
    "\n",
    "    TODO: Implement GELU with explicit intermediate steps\n",
    "\n",
    "    APPROACH:\n",
    "    1. Break computation into individual steps\n",
    "    2. Create temporary Tensor objects for each step\n",
    "    3. This simulates real memory allocation overhead\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "\n",
    "    Returns:\n",
    "        GELU-activated tensor (same shape as input)\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> x = Tensor([0.5, 1.0, -0.5])\n",
    "    >>> result = unfused_gelu(x)\n",
    "    >>> print(result.shape)\n",
    "    (3,)  # Same as input\n",
    "\n",
    "    PERFORMANCE IMPACT:\n",
    "    - Creates 7 temporary arrays\n",
    "    - Each array allocation/deallocation has overhead\n",
    "    - More memory bandwidth usage\n",
    "    - Potential cache misses between operations\n",
    "\n",
    "    HINTS:\n",
    "    - Create each step as: temp = Tensor(operation)\n",
    "    - This forces memory allocation for educational comparison\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Unfused version - creates many intermediate arrays\n",
    "    sqrt_2_over_pi = np.sqrt(2.0 / np.pi)\n",
    "\n",
    "    # Each operation creates a temporary array (simulating kernel launches)\n",
    "    temp1 = Tensor(x.data**3)  # xÂ³\n",
    "    temp2 = Tensor(0.044715 * temp1.data)  # 0.044715 * xÂ³\n",
    "    temp3 = Tensor(x.data + temp2.data)  # x + 0.044715 * xÂ³\n",
    "    temp4 = Tensor(sqrt_2_over_pi * temp3.data)  # âˆš(2/Ï€) * (...)\n",
    "    temp5 = Tensor(np.tanh(temp4.data))  # tanh(...)\n",
    "    temp6 = Tensor(1.0 + temp5.data)  # 1 + tanh(...)\n",
    "    temp7 = Tensor(x.data * temp6.data)  # x * (1 + tanh(...))\n",
    "    result = Tensor(0.5 * temp7.data)  # 0.5 * x * (...)\n",
    "\n",
    "    return result\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8226a3a",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-fusion-speedup",
     "locked": true,
     "points": 10
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_fusion_speedup():\n",
    "    \"\"\"ğŸ”¬ Measure the performance impact of kernel fusion.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Kernel Fusion Performance Impact...\")\n",
    "\n",
    "    # Create moderately large tensor for meaningful timing\n",
    "    size = 2000\n",
    "    x = Tensor(np.random.randn(size, size).astype(np.float32))\n",
    "    warmup_iterations = DEFAULT_WARMUP_ITERATIONS\n",
    "    timing_iterations = DEFAULT_TIMING_ITERATIONS\n",
    "\n",
    "    # Warmup both implementations\n",
    "    for _ in range(warmup_iterations):\n",
    "        _ = unfused_gelu(x)\n",
    "        _ = fused_gelu(x)\n",
    "\n",
    "    # Time unfused version\n",
    "    start = time.time()\n",
    "    for _ in range(timing_iterations):\n",
    "        result_unfused = unfused_gelu(x)\n",
    "    unfused_time = time.time() - start\n",
    "\n",
    "    # Time fused version\n",
    "    start = time.time()\n",
    "    for _ in range(timing_iterations):\n",
    "        result_fused = fused_gelu(x)\n",
    "    fused_time = time.time() - start\n",
    "\n",
    "    # Verify numerical correctness\n",
    "    assert np.allclose(result_unfused.data, result_fused.data, atol=1e-6), \\\n",
    "        \"Fused and unfused implementations must be numerically equivalent\"\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    speedup = unfused_time / fused_time if fused_time > 0 else 1.0\n",
    "    unfused_per_elem = (unfused_time / timing_iterations) / (size * size) * 1e9  # ns per element\n",
    "    fused_per_elem = (fused_time / timing_iterations) / (size * size) * 1e9\n",
    "\n",
    "    print(f\"ğŸ“Š Kernel Fusion Performance Analysis:\")\n",
    "    print(f\"   Tensor size: {size}Ã—{size} = {size*size:,} elements\")\n",
    "    print(f\"   Unfused time: {unfused_time/timing_iterations*1000:.2f} ms\")\n",
    "    print(f\"   Fused time:   {fused_time/timing_iterations*1000:.2f} ms\")\n",
    "    print(f\"   Speedup: {speedup:.2f}Ã— faster\")\n",
    "    print(f\"   Per-element: {unfused_per_elem:.1f} ns â†’ {fused_per_elem:.1f} ns\")\n",
    "\n",
    "    # Memory bandwidth estimate\n",
    "    bytes_per_elem = 4  # float32\n",
    "    unfused_memory_ops = 7  # 7 intermediate arrays\n",
    "    fused_memory_ops = 2   # read input, write output\n",
    "\n",
    "    unfused_bandwidth = (unfused_memory_ops * size * size * bytes_per_elem) / (unfused_time / timing_iterations) / 1e9\n",
    "    fused_bandwidth = (fused_memory_ops * size * size * bytes_per_elem) / (fused_time / timing_iterations) / 1e9\n",
    "\n",
    "    print(f\"   Memory efficiency: {unfused_memory_ops}â†’{fused_memory_ops} memory ops\")\n",
    "    print(f\"   Effective bandwidth: {unfused_bandwidth:.1f}â†’{fused_bandwidth:.1f} GB/s\")\n",
    "\n",
    "    # Interpret results\n",
    "    if speedup > 1.5:\n",
    "        print(\"ğŸš€ Excellent! Kernel fusion providing significant speedup\")\n",
    "    elif speedup > 1.1:\n",
    "        print(\"âœ… Good! Kernel fusion providing measurable benefit\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Limited speedup - may be compute-bound or small tensor size\")\n",
    "\n",
    "    print(\"âœ… Fusion performance analysis completed!\")\n",
    "\n",
    "# Test is callable but runs via test_module() in main block below\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_unit_fusion_speedup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8fb09",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Cache-Aware Matrix Multiplication\n",
    "\n",
    "For large matrices that don't fit in cache, we need **tiling** (also called blocking).\n",
    "This breaks the computation into cache-sized chunks for better performance.\n",
    "\n",
    "### Why Cache Awareness Matters\n",
    "\n",
    "Modern processors have a memory hierarchy:\n",
    "```\n",
    "L1 Cache:   32-64 KB   (fastest, 1-4 cycles)\n",
    "L2 Cache:   256 KB-1MB (fast, 10-20 cycles)\n",
    "L3 Cache:   8-32 MB    (moderate, 40-75 cycles)\n",
    "Main RAM:   8-64 GB    (slow, 100-300 cycles)\n",
    "```\n",
    "\n",
    "When matrices are larger than cache, we get **cache misses** that slow us down dramatically.\n",
    "Tiling keeps working set in cache for maximum reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c7ba3",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "tiled-matmul",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def tiled_matmul(a: Tensor, b: Tensor, tile_size: int = 64) -> Tensor:\n",
    "    \"\"\"\n",
    "    Cache-aware matrix multiplication using tiling/blocking.\n",
    "\n",
    "    Demonstrates blocking algorithm for cache optimization by breaking\n",
    "    large matrix multiplications into cache-sized chunks.\n",
    "\n",
    "    TODO: Implement cache-aware tiled matrix multiplication\n",
    "\n",
    "    APPROACH:\n",
    "    1. Validate inputs for matrix multiplication compatibility\n",
    "    2. Use NumPy's optimized matmul (which already implements tiling internally)\n",
    "    3. In production, explicit tiling would use nested loops over blocks\n",
    "\n",
    "    Args:\n",
    "        a: First matrix (MÃ—K)\n",
    "        b: Second matrix (KÃ—N)\n",
    "        tile_size: Block size for cache efficiency (default: 64)\n",
    "\n",
    "    Returns:\n",
    "        Result matrix (MÃ—N)\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> a = Tensor(np.random.randn(256, 256))\n",
    "    >>> b = Tensor(np.random.randn(256, 256))\n",
    "    >>> result = tiled_matmul(a, b, tile_size=64)\n",
    "    >>> # Same result as vectorized_matmul, but more cache-friendly for large matrices\n",
    "\n",
    "    PERFORMANCE CHARACTERISTICS:\n",
    "    - Reduces cache misses by working on blocks that fit in L1/L2\n",
    "    - Especially beneficial for matrices larger than cache size\n",
    "    - tile_size should match cache line size (typically 64 bytes)\n",
    "\n",
    "    HINTS:\n",
    "    - For educational purposes, we use NumPy's optimized BLAS\n",
    "    - BLAS libraries (MKL, OpenBLAS) already implement cache blocking\n",
    "    - Explicit tiling would use 6 nested loops (3 for tiles, 3 for elements)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Input validation\n",
    "    if len(a.shape) < 2 or len(b.shape) < 2:\n",
    "        raise ValueError(\n",
    "            f\"Tiled matmul requires 2D+ tensors, got shapes {a.shape} and {b.shape}. \"\n",
    "            f\"ğŸ’¡ HINT: Tiling works on matrix operations.\"\n",
    "        )\n",
    "\n",
    "    if a.shape[-1] != b.shape[-2]:\n",
    "        raise ValueError(\n",
    "            f\"Shape mismatch: {a.shape} @ {b.shape}. \"\n",
    "            f\"Inner dimensions must match for matrix multiplication. \"\n",
    "            f\"ğŸ’¡ HINT: a.shape[-1]={a.shape[-1]} != b.shape[-2]={b.shape[-2]}\"\n",
    "        )\n",
    "\n",
    "    # For educational purposes, we use NumPy's matmul which already\n",
    "    # implements cache-aware tiling via BLAS libraries (MKL, OpenBLAS)\n",
    "    # These libraries automatically partition large matrices into\n",
    "    # cache-sized blocks for optimal performance\n",
    "\n",
    "    # In a full educational implementation, you would write:\n",
    "    # for i_tile in range(0, M, tile_size):\n",
    "    #     for j_tile in range(0, N, tile_size):\n",
    "    #         for k_tile in range(0, K, tile_size):\n",
    "    #             # Multiply tile blocks that fit in cache\n",
    "    #             C[i_tile:i_tile+tile_size, j_tile:j_tile+tile_size] +=\n",
    "    #                 A[i_tile:i_tile+tile_size, k_tile:k_tile+tile_size] @\n",
    "    #                 B[k_tile:k_tile+tile_size, j_tile:j_tile+tile_size]\n",
    "\n",
    "    result_data = np.matmul(a.data, b.data)\n",
    "    return Tensor(result_data)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d55b5f",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-tiled-matmul",
     "locked": true,
     "points": 10
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_tiled_matmul():\n",
    "    \"\"\"ğŸ”¬ Test cache-aware tiled matrix multiplication.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Tiled Matrix Multiplication...\")\n",
    "\n",
    "    # Test correctness against vectorized version\n",
    "    a = Tensor(np.random.randn(128, 128).astype(np.float32))\n",
    "    b = Tensor(np.random.randn(128, 128).astype(np.float32))\n",
    "\n",
    "    result_tiled = tiled_matmul(a, b, tile_size=32)\n",
    "    result_reference = vectorized_matmul(a, b)\n",
    "\n",
    "    assert np.allclose(result_tiled.data, result_reference.data, atol=1e-5), \\\n",
    "        \"Tiled and vectorized results should match\"\n",
    "\n",
    "    # Test different tile sizes\n",
    "    for tile_size in [16, 32, 64]:\n",
    "        result = tiled_matmul(a, b, tile_size=tile_size)\n",
    "        assert result.shape == (128, 128), f\"Wrong shape for tile_size={tile_size}\"\n",
    "\n",
    "    # Test shape validation\n",
    "    try:\n",
    "        wrong_a = Tensor(np.random.randn(128, 64).astype(np.float32))\n",
    "        wrong_b = Tensor(np.random.randn(128, 64).astype(np.float32))\n",
    "        tiled_matmul(wrong_a, wrong_b)\n",
    "        assert False, \"Should have raised ValueError for shape mismatch\"\n",
    "    except ValueError as e:\n",
    "        assert \"Shape mismatch\" in str(e)\n",
    "\n",
    "    print(\"âœ… tiled_matmul works correctly!\")\n",
    "\n",
    "# Test is callable but runs via test_module() in main block below\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_unit_tiled_matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90c696",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ“Š Systems Analysis - Performance Scaling Patterns\n",
    "\n",
    "Let's analyze how our acceleration techniques perform across different scenarios and understand their scaling characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f917484e",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "analyze-vectorization",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_vectorization_scaling():\n",
    "    \"\"\"ğŸ“Š Analyze vectorization performance across different tensor sizes.\"\"\"\n",
    "    print(\"ğŸ“Š Analyzing vectorization scaling behavior...\")\n",
    "\n",
    "    # Test sizes spanning different cache regimes\n",
    "    sizes = [64, 128, 256, 512, 1024, 2048]\n",
    "\n",
    "    print(\"\\nğŸ” Vectorization Scaling Analysis:\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚  Size   â”‚ Time (ms)   â”‚ GFLOPS      â”‚ Bandwidth   â”‚ Efficiency  â”‚\")\n",
    "    print(\"â”‚         â”‚             â”‚             â”‚ (GB/s)      â”‚ (% of peak) â”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "\n",
    "    for size in sizes:\n",
    "        # Create test matrices\n",
    "        a = Tensor(np.random.randn(size, size).astype(np.float32))\n",
    "        b = Tensor(np.random.randn(size, size).astype(np.float32))\n",
    "\n",
    "        # Warm up\n",
    "        for _ in range(2):\n",
    "            _ = vectorized_matmul(a, b)\n",
    "\n",
    "        # Time vectorized implementation\n",
    "        iterations = max(1, 100 // (size // 64))  # Fewer iterations for larger sizes\n",
    "        start = time.time()\n",
    "        for _ in range(iterations):\n",
    "            result = vectorized_matmul(a, b)\n",
    "        elapsed = (time.time() - start) / iterations\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        flops = 2 * size**3  # 2NÂ³ FLOPs for matrix multiplication\n",
    "        gflops = flops / (elapsed * 1e9)\n",
    "\n",
    "        bytes_accessed = 3 * size * size * 4  # 3 matrices Ã— sizeÂ² Ã— 4 bytes\n",
    "        bandwidth = bytes_accessed / (elapsed * 1e9)\n",
    "\n",
    "        # Estimate efficiency (rough baseline: modern CPU ~100-500 GFLOPS peak)\n",
    "        estimated_peak_gflops = 200  # Conservative estimate\n",
    "        efficiency = min(100, gflops / estimated_peak_gflops * 100)\n",
    "\n",
    "        print(f\"â”‚ {size:6d}  â”‚ {elapsed*1000:9.2f}   â”‚ {gflops:9.1f}   â”‚ {bandwidth:9.1f}   â”‚ {efficiency:9.1f}   â”‚\")\n",
    "\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "    print(f\"\\nğŸ’¡ Vectorization insights:\")\n",
    "    print(f\"   â€¢ Small matrices: Limited by overhead and cache effects\")\n",
    "    print(f\"   â€¢ Medium matrices: Sweet spot for cache reuse\")\n",
    "    print(f\"   â€¢ Large matrices: Memory bandwidth becomes limiting factor\")\n",
    "    print(f\"   â€¢ BLAS libraries automatically optimize for each size regime\")\n",
    "    print(\"ğŸš€ Vectorization effectiveness depends on problem size and hardware\")\n",
    "\n",
    "# Analysis is callable but runs via main block below\n",
    "# if __name__ == \"__main__\":\n",
    "#     analyze_vectorization_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de9dc8",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "analyze-arithmetic-intensity",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_arithmetic_intensity():\n",
    "    \"\"\"ğŸ“Š Demonstrate the roofline model with different operations.\"\"\"\n",
    "    print(\"ğŸ“Š Analyzing arithmetic intensity patterns...\")\n",
    "\n",
    "    size = 1024\n",
    "    iterations = 10\n",
    "\n",
    "    operations = []\n",
    "\n",
    "    # Create test data\n",
    "    x = Tensor(np.random.randn(size, size).astype(np.float32))\n",
    "    y = Tensor(np.random.randn(size, size).astype(np.float32))\n",
    "\n",
    "    print(\"\\nğŸ¯ Arithmetic Intensity Analysis:\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚ Operation           â”‚ AI      â”‚ Time (ms)   â”‚ GFLOPS      â”‚ GB/s        â”‚\")\n",
    "    print(\"â”‚                     â”‚(FLOPs/B)â”‚             â”‚             â”‚             â”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "\n",
    "    # 1. Element-wise addition (very low arithmetic intensity)\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = Tensor(x.data + y.data)\n",
    "    add_time = (time.time() - start) / iterations\n",
    "\n",
    "    add_flops = size * size  # One addition per element\n",
    "    add_bytes = 3 * size * size * 4  # Read x, read y, write result\n",
    "    add_ai = add_flops / add_bytes\n",
    "    add_gflops = add_flops / (add_time * 1e9)\n",
    "    add_bandwidth = add_bytes / (add_time * 1e9)\n",
    "\n",
    "    print(f\"â”‚ Element-wise Add    â”‚ {add_ai:6.3f}  â”‚ {add_time*1000:9.2f}   â”‚ {add_gflops:9.1f}   â”‚ {add_bandwidth:9.1f}   â”‚\")\n",
    "\n",
    "    # 2. Element-wise multiply (still low, but slightly higher)\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = Tensor(x.data * y.data)\n",
    "    mul_time = (time.time() - start) / iterations\n",
    "\n",
    "    mul_flops = size * size\n",
    "    mul_bytes = 3 * size * size * 4\n",
    "    mul_ai = mul_flops / mul_bytes\n",
    "    mul_gflops = mul_flops / (mul_time * 1e9)\n",
    "    mul_bandwidth = mul_bytes / (mul_time * 1e9)\n",
    "\n",
    "    print(f\"â”‚ Element-wise Mult   â”‚ {mul_ai:6.3f}  â”‚ {mul_time*1000:9.2f}   â”‚ {mul_gflops:9.1f}   â”‚ {mul_bandwidth:9.1f}   â”‚\")\n",
    "\n",
    "    # 3. GELU (medium arithmetic intensity)\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = fused_gelu(x)\n",
    "    gelu_time = (time.time() - start) / iterations\n",
    "\n",
    "    gelu_flops = size * size * 8  # Approximate: xÂ³, add, mul, tanh, etc.\n",
    "    gelu_bytes = 2 * size * size * 4  # Read x, write result\n",
    "    gelu_ai = gelu_flops / gelu_bytes\n",
    "    gelu_gflops = gelu_flops / (gelu_time * 1e9)\n",
    "    gelu_bandwidth = gelu_bytes / (gelu_time * 1e9)\n",
    "\n",
    "    print(f\"â”‚ Fused GELU          â”‚ {gelu_ai:6.3f}  â”‚ {gelu_time*1000:9.2f}   â”‚ {gelu_gflops:9.1f}   â”‚ {gelu_bandwidth:9.1f}   â”‚\")\n",
    "\n",
    "    # 4. Matrix multiplication (high arithmetic intensity)\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        _ = vectorized_matmul(x, y)\n",
    "    matmul_time = (time.time() - start) / iterations\n",
    "\n",
    "    matmul_flops = 2 * size**3  # 2NÂ³ FLOPs\n",
    "    matmul_bytes = 3 * size * size * 4  # 3 matrices\n",
    "    matmul_ai = matmul_flops / matmul_bytes\n",
    "    matmul_gflops = matmul_flops / (matmul_time * 1e9)\n",
    "    matmul_bandwidth = matmul_bytes / (matmul_time * 1e9)\n",
    "\n",
    "    print(f\"â”‚ Matrix Multiply     â”‚ {matmul_ai:6.3f}  â”‚ {matmul_time*1000:9.2f}   â”‚ {matmul_gflops:9.1f}   â”‚ {matmul_bandwidth:9.1f}   â”‚\")\n",
    "\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "    print(f\"\\nğŸ’¡ Roofline Model Insights:\")\n",
    "    print(f\"   ğŸ“Š Low AI (< 1): Memory bound - limited by bandwidth\")\n",
    "    print(f\"   ğŸ“Š Med AI (1-10): Transitional - depends on implementation\")\n",
    "    print(f\"   ğŸ“Š High AI (> 10): Compute bound - limited by ALU throughput\")\n",
    "    print(f\"   ğŸ¯ Matrix multiplication ({matmul_ai:.1f} AI) is ideal for GPUs/TPUs\")\n",
    "    print(f\"   âš¡ Element-wise ops ({add_ai:.3f} AI) need memory optimization\")\n",
    "    print(\"ğŸš€ Design algorithms with high arithmetic intensity for performance\")\n",
    "\n",
    "# Analysis is callable but runs via main block below\n",
    "# if __name__ == \"__main__\":\n",
    "#     analyze_arithmetic_intensity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05287469",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ğŸ“Š Memory Efficiency Analysis\n",
    "\n",
    "Understanding memory allocation patterns is crucial for perf.\n",
    "Let's measure how different implementations use memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b54fd",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "analyze-memory",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def analyze_memory_efficiency():\n",
    "    \"\"\"ğŸ“Š Analyze memory allocation patterns for different operations.\"\"\"\n",
    "    print(\"ğŸ“Š Analyzing memory efficiency patterns...\")\n",
    "\n",
    "    import tracemalloc\n",
    "\n",
    "    sizes = [100, 500, 1000]\n",
    "\n",
    "    print(\"\\nğŸ” Memory Allocation Analysis:\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚  Size   â”‚ Vectorized   â”‚ Unfused GELU â”‚ Fused GELU   â”‚\")\n",
    "    print(\"â”‚         â”‚ Matmul (MB)  â”‚ (MB)         â”‚ (MB)         â”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "\n",
    "    for size in sizes:\n",
    "        x = Tensor(np.random.randn(size, size).astype(np.float32))\n",
    "        y = Tensor(np.random.randn(size, size).astype(np.float32))\n",
    "\n",
    "        # Measure vectorized matmul\n",
    "        tracemalloc.start()\n",
    "        _ = vectorized_matmul(x, y)\n",
    "        _, matmul_peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "\n",
    "        # Measure unfused GELU\n",
    "        tracemalloc.start()\n",
    "        _ = unfused_gelu(x)\n",
    "        _, unfused_peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "\n",
    "        # Measure fused GELU\n",
    "        tracemalloc.start()\n",
    "        _ = fused_gelu(x)\n",
    "        _, fused_peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "\n",
    "        print(f\"â”‚ {size:6d}  â”‚ {matmul_peak/1e6:10.2f}   â”‚ {unfused_peak/1e6:10.2f}   â”‚ {fused_peak/1e6:8.2f}   â”‚\")\n",
    "\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "    print(\"\\nğŸ’¡ Key insights:\")\n",
    "    print(\"   â€¢ Vectorized matmul: ~3Ã— input size (2 inputs + 1 output)\")\n",
    "    print(\"   â€¢ Unfused GELU: ~8-10Ã— input size (many intermediate tensors)\")\n",
    "    print(\"   â€¢ Fused GELU: ~2Ã— input size (1 input + 1 output only)\")\n",
    "    print(\"   â€¢ Fusion reduces memory allocations by 4-5Ã—\")\n",
    "    print(\"ğŸš€ Memory efficiency critical for large batch sizes and limited GPU memory\")\n",
    "\n",
    "# Analysis is callable but runs via main block below\n",
    "# if __name__ == \"__main__\":\n",
    "#     analyze_memory_efficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f652f5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ“Š Optimization Insights - Production Acceleration Strategy\n",
    "\n",
    "Understanding when and how to apply different acceleration techniques in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5a66b",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "acceleration-decision-framework",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_acceleration_decision_framework():\n",
    "    \"\"\"ğŸ“Š Decision framework for choosing acceleration techniques.\"\"\"\n",
    "    print(\"ğŸ“Š Acceleration Technique Decision Framework...\")\n",
    "\n",
    "    # Define workload characteristics\n",
    "    workloads = [\n",
    "        (\"Research Training\", {\n",
    "            \"memory_pressure\": \"medium\",\n",
    "            \"latency_sensitive\": False,\n",
    "            \"stability_critical\": False,\n",
    "            \"development_speed\": \"high\",\n",
    "            \"hardware_variety\": \"high\"\n",
    "        }),\n",
    "        (\"Production Training\", {\n",
    "            \"memory_pressure\": \"high\",\n",
    "            \"latency_sensitive\": False,\n",
    "            \"stability_critical\": True,\n",
    "            \"development_speed\": \"medium\",\n",
    "            \"hardware_variety\": \"low\"\n",
    "        }),\n",
    "        (\"Real-time Inference\", {\n",
    "            \"memory_pressure\": \"medium\",\n",
    "            \"latency_sensitive\": True,\n",
    "            \"stability_critical\": True,\n",
    "            \"development_speed\": \"low\",\n",
    "            \"hardware_variety\": \"medium\"\n",
    "        }),\n",
    "        (\"Edge Deployment\", {\n",
    "            \"memory_pressure\": \"very_high\",\n",
    "            \"latency_sensitive\": True,\n",
    "            \"stability_critical\": True,\n",
    "            \"development_speed\": \"low\",\n",
    "            \"hardware_variety\": \"very_high\"\n",
    "        }),\n",
    "        (\"Batch Inference\", {\n",
    "            \"memory_pressure\": \"low\",\n",
    "            \"latency_sensitive\": False,\n",
    "            \"stability_critical\": True,\n",
    "            \"development_speed\": \"medium\",\n",
    "            \"hardware_variety\": \"low\"\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    # Define technique characteristics\n",
    "    techniques = {\n",
    "        \"Vectorization\": {\n",
    "            \"implementation_cost\": \"low\",\n",
    "            \"memory_benefit\": \"none\",\n",
    "            \"latency_benefit\": \"high\",\n",
    "            \"stability_risk\": \"none\",\n",
    "            \"hardware_dependency\": \"low\"\n",
    "        },\n",
    "        \"Kernel Fusion\": {\n",
    "            \"implementation_cost\": \"medium\",\n",
    "            \"memory_benefit\": \"medium\",\n",
    "            \"latency_benefit\": \"medium\",\n",
    "            \"stability_risk\": \"low\",\n",
    "            \"hardware_dependency\": \"medium\"\n",
    "        },\n",
    "        \"Graph Optimization\": {\n",
    "            \"implementation_cost\": \"very_high\",\n",
    "            \"memory_benefit\": \"medium\",\n",
    "            \"latency_benefit\": \"very_high\",\n",
    "            \"stability_risk\": \"low\",\n",
    "            \"hardware_dependency\": \"very_high\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\\nğŸ¯ Acceleration Technique Recommendations:\")\n",
    "    print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚ Workload            â”‚ Vectorize   â”‚ Fuse Kernelsâ”‚ Mixed Prec  â”‚ Graph Opt   â”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "\n",
    "    for workload_name, workload_chars in workloads:\n",
    "        recommendations = []\n",
    "\n",
    "        for technique_name in [\"Vectorization\", \"Kernel Fusion\", \"Graph Optimization\"]:\n",
    "            tech_chars = techniques[technique_name]\n",
    "            score = 0\n",
    "\n",
    "            # Benefit vs requirement matching\n",
    "            if workload_chars[\"memory_pressure\"] in [\"high\", \"very_high\"]:\n",
    "                if tech_chars[\"memory_benefit\"] in [\"medium\", \"high\"]:\n",
    "                    score += 2\n",
    "\n",
    "            if workload_chars[\"latency_sensitive\"]:\n",
    "                if tech_chars[\"latency_benefit\"] in [\"medium\", \"high\", \"very_high\"]:\n",
    "                    score += 2\n",
    "\n",
    "            # Risk vs tolerance matching\n",
    "            if workload_chars[\"stability_critical\"]:\n",
    "                if tech_chars[\"stability_risk\"] in [\"none\", \"low\"]:\n",
    "                    score += 1\n",
    "                elif tech_chars[\"stability_risk\"] == \"medium\":\n",
    "                    score -= 1\n",
    "\n",
    "            # Implementation cost vs development speed\n",
    "            if workload_chars[\"development_speed\"] == \"high\":\n",
    "                if tech_chars[\"implementation_cost\"] in [\"low\", \"medium\"]:\n",
    "                    score += 1\n",
    "                elif tech_chars[\"implementation_cost\"] in [\"high\", \"very_high\"]:\n",
    "                    score -= 1\n",
    "\n",
    "            # Hardware dependency vs variety\n",
    "            if workload_chars[\"hardware_variety\"] in [\"high\", \"very_high\"]:\n",
    "                if tech_chars[\"hardware_dependency\"] in [\"low\", \"medium\"]:\n",
    "                    score += 1\n",
    "                elif tech_chars[\"hardware_dependency\"] in [\"high\", \"very_high\"]:\n",
    "                    score -= 2\n",
    "\n",
    "            # Convert score to recommendation\n",
    "            if score >= 3:\n",
    "                rec = \"âœ… High\"\n",
    "            elif score >= 1:\n",
    "                rec = \"âš¡ Medium\"\n",
    "            elif score >= 0:\n",
    "                rec = \"âš ï¸  Low\"\n",
    "            else:\n",
    "                rec = \"âŒ Skip\"\n",
    "\n",
    "            recommendations.append(rec)\n",
    "\n",
    "        rec_line = \" â”‚ \".join(f\"{rec:10s}\" for rec in recommendations)\n",
    "        print(f\"â”‚ {workload_name:18s}  â”‚ {rec_line} â”‚\")\n",
    "\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "    # Implementation priority framework\n",
    "    print(f\"\\nğŸ› ï¸  Implementation Priority Framework:\")\n",
    "    print(f\"   ğŸ“Š Phase 1 (Always): Vectorization\")\n",
    "    print(f\"      â€¢ Low risk, high reward\")\n",
    "    print(f\"      â€¢ Works on any hardware\")\n",
    "    print(f\"      â€¢ Foundation for other optimizations\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   ğŸ“Š Phase 2 (Memory constrained): Kernel Fusion\")\n",
    "    print(f\"      â€¢ Targets memory-bound operations\")\n",
    "    print(f\"      â€¢ Moderate complexity\")\n",
    "    print(f\"      â€¢ Significant wins on element-wise ops\")\n",
    "    print(f\"   \")\n",
    "    print(f\"      â€¢ Essential for large model training\")\n",
    "    print(f\"      â€¢ Requires careful validation\")\n",
    "    print(f\"      â€¢ Hardware-dependent benefits\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   ğŸ“Š Phase 4 (Production): Graph Optimization\")\n",
    "    print(f\"      â€¢ Maximum performance extraction\")\n",
    "    print(f\"      â€¢ High implementation cost\")\n",
    "    print(f\"      â€¢ Deployment-specific tuning\")\n",
    "\n",
    "    print(f\"\\nğŸ’¡ Key Decision Factors:\")\n",
    "    print(f\"   ğŸ¯ Start simple: Vectorization first, always\")\n",
    "    print(f\"   ğŸ“ˆ Scale up: Add complexity only when needed\")\n",
    "    print(f\"   âš¡ Measure impact: Profile before and after each optimization\")\n",
    "    print(f\"   ğŸ”„ Iterate: Optimization is an ongoing process, not one-time\")\n",
    "    print(\"ğŸš€ Systematic acceleration beats random optimization\")\n",
    "\n",
    "# Analysis is callable but runs via main block below\n",
    "# if __name__ == \"__main__\":\n",
    "#     analyze_acceleration_decision_framework()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a81d0d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“Š Measuring Acceleration Gains with Profiler\n",
    "\n",
    "Now let's use the **Profiler** tool you built in Module 15 to measure the actual performance improvements from vectorization. This demonstrates the full workflow: build profiling tools (M15), apply optimizations (M16), measure gains (M15+M16).\n",
    "\n",
    "This is how professional ML engineers work: profile â†’ optimize â†’ measure â†’ repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99770812",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "demo-profiler-acceleration",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Import Profiler from Module 14 (Module 17 comes after Module 14)\n",
    "from tinytorch.perf.profiling import Profiler\n",
    "\n",
    "def demo_acceleration_with_profiler():\n",
    "    \"\"\"ğŸ“Š Demonstrate acceleration gains using Profiler from Module 14.\"\"\"\n",
    "\n",
    "    print(\"ğŸ“Š Measuring Acceleration Gains with Profiler\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    profiler = Profiler()\n",
    "\n",
    "    # Create two simple models: one slow (loop-based), one fast (vectorized)\n",
    "    class SlowLinear:\n",
    "        \"\"\"Linear layer using explicit loops (slow).\"\"\"\n",
    "        def __init__(self, in_features, out_features):\n",
    "            self.weight = Tensor(np.random.randn(in_features, out_features).astype(np.float32) * 0.01)\n",
    "            self.name = \"slow_linear\"\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Explicit loop implementation (for demonstration)\n",
    "            batch_size = x.shape[0]\n",
    "            out_features = self.weight.shape[1]\n",
    "            result = np.zeros((batch_size, out_features), dtype=np.float32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                for j in range(out_features):\n",
    "                    for k in range(x.shape[1]):\n",
    "                        result[i, j] += x.data[i, k] * self.weight.data[k, j]\n",
    "\n",
    "            return Tensor(result)\n",
    "\n",
    "    class FastLinear:\n",
    "        \"\"\"Linear layer using vectorized matmul (fast).\"\"\"\n",
    "        def __init__(self, in_features, out_features):\n",
    "            self.weight = Tensor(np.random.randn(in_features, out_features).astype(np.float32) * 0.01)\n",
    "            self.name = \"fast_linear\"\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Vectorized implementation\n",
    "            return vectorized_matmul(x, self.weight)\n",
    "\n",
    "    in_features, out_features = 128, 64\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create models\n",
    "    slow_model = SlowLinear(in_features, out_features)\n",
    "    fast_model = FastLinear(in_features, out_features)\n",
    "\n",
    "    # Create input\n",
    "    input_tensor = Tensor(np.random.randn(batch_size, in_features).astype(np.float32))\n",
    "\n",
    "    print(\"\\nğŸ¢ BEFORE: Loop-based implementation\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Measure slow model\n",
    "    slow_latency = profiler.measure_latency(slow_model, input_tensor, warmup=3, iterations=10)\n",
    "    slow_flops = profiler.count_flops(slow_model, (batch_size, in_features))\n",
    "\n",
    "    print(f\"   Latency: {slow_latency:.2f} ms\")\n",
    "    print(f\"   FLOPs: {slow_flops:,}\")\n",
    "    print(f\"   Throughput: {slow_flops / (slow_latency / 1000) / 1e9:.2f} GFLOP/s\")\n",
    "\n",
    "    print(\"\\nğŸš€ AFTER: Vectorized implementation\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Measure fast model\n",
    "    fast_latency = profiler.measure_latency(fast_model, input_tensor, warmup=3, iterations=10)\n",
    "    fast_flops = profiler.count_flops(fast_model, (batch_size, in_features))\n",
    "\n",
    "    print(f\"   Latency: {fast_latency:.2f} ms\")\n",
    "    print(f\"   FLOPs: {fast_flops:,}\")\n",
    "    print(f\"   Throughput: {fast_flops / (fast_latency / 1000) / 1e9:.2f} GFLOP/s\")\n",
    "\n",
    "    print(\"\\nğŸ“ˆ ACCELERATION GAINS\")\n",
    "    print(\"=\" * 70)\n",
    "    speedup = slow_latency / fast_latency\n",
    "    print(f\"   Speedup: {speedup:.1f}x faster\")\n",
    "    print(f\"   Time saved: {slow_latency - fast_latency:.2f} ms per inference\")\n",
    "    print(f\"   Throughput improvement: {speedup:.1f}x more inferences/second\")\n",
    "\n",
    "    print(\"\\nğŸ’¡ Key Insight:\")\n",
    "    print(f\"   Vectorization with numpy.matmul leverages optimized BLAS libraries\")\n",
    "    print(f\"   that use SIMD instructions and cache-friendly memory access patterns.\")\n",
    "    print(f\"   This is why {speedup:.0f}x speedups are possible with the same FLOPs!\")\n",
    "    print(\"\\nâœ… This is the power of acceleration: same math, different execution!\")\n",
    "\n",
    "# Demo is callable but runs via main block below\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo_acceleration_with_profiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bcd496",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ¤” ML Systems Thinking: Acceleration and Performance\n",
    "\n",
    "### Question 1: Arithmetic Intensity Analysis\n",
    "You implemented vectorized matrix multiplication and fused GELU.\n",
    "- Matrix multiplication (1024Ã—1024): Performs ~2.1 billion FLOPs, reads ~12 MB data\n",
    "- Arithmetic intensity: _____ FLOPs/byte\n",
    "- Compared to element-wise addition (0.33 FLOPs/byte): _____Ã— higher intensity\n",
    "- Why does this make matrix multiplication ideal for GPUs? _____\n",
    "\n",
    "### Question 2: Kernel Fusion Memory Benefits\n",
    "Your fused_gelu combines 7 operations into a single expression.\n",
    "- Unfused version memory accesses: 7 reads + 7 writes = _____ per element\n",
    "- Fused version memory accesses: 1 read + 1 write = _____ per element\n",
    "- Memory bandwidth reduction: _____%\n",
    "- Why is this critical for transformer inference? _____\n",
    "\n",
    "### Question 4: Production Optimization Strategy\n",
    "Based on your decision framework analysis:\n",
    "For edge deployment (memory critical, stability required, hardware diverse):\n",
    "- Priority 1 technique: _____ (low risk, universal)\n",
    "- Priority 2 technique: _____ (memory benefits)\n",
    "- Skip technique: _____ (why: _____)\n",
    "- What's the primary constraint: memory, compute, or power? _____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95812b37",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ§ª Module Integration Test\n",
    "\n",
    "Final validation that all acceleration components work together correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13b980",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-module",
     "locked": true,
     "points": 20
    }
   },
   "outputs": [],
   "source": [
    "def test_module():\n",
    "    \"\"\"ğŸ§ª Module Test: Complete Integration\n",
    "\n",
    "    Comprehensive test of entire acceleration module functionality.\n",
    "\n",
    "    This final test ensures:\n",
    "    - All acceleration techniques work correctly\n",
    "    - Performance improvements are measurable\n",
    "    - Components integrate seamlessly\n",
    "    - Module is ready for production use\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_vectorized_matmul()\n",
    "    test_unit_fused_gelu()\n",
    "    test_unit_fusion_speedup()\n",
    "    test_unit_tiled_matmul()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test realistic acceleration pipeline\n",
    "    print(\"ğŸ”¬ Integration Test: Complete acceleration pipeline...\")\n",
    "\n",
    "    # Create realistic model scenario\n",
    "    batch_size, seq_len, hidden_dim = 16, 64, 256\n",
    "    print(f\"   Model config: batch={batch_size}, seq_len={seq_len}, hidden={hidden_dim}\")\n",
    "\n",
    "    # Test data\n",
    "    x = Tensor(np.random.randn(batch_size, seq_len, hidden_dim).astype(np.float32))\n",
    "    weight = Tensor(np.random.randn(hidden_dim, hidden_dim).astype(np.float32))\n",
    "    print(f\"   Input tensor: {x.shape}, Weight tensor: {weight.shape}\")\n",
    "\n",
    "    # Test complete pipeline: reshape â†’ matmul â†’ activation\n",
    "    print(\"   Testing vectorized operations...\")\n",
    "\n",
    "    # Reshape for matrix multiplication (flatten batch and sequence)\n",
    "    x_reshaped = Tensor(x.data.reshape(-1, hidden_dim))\n",
    "    assert x_reshaped.shape == (batch_size * seq_len, hidden_dim)\n",
    "\n",
    "    # Vectorized matrix multiplication\n",
    "    linear_output = vectorized_matmul(x_reshaped, weight)\n",
    "    assert linear_output.shape == (batch_size * seq_len, hidden_dim)\n",
    "    print(f\"   âœ… Matrix multiplication: {x_reshaped.shape} @ {weight.shape} â†’ {linear_output.shape}\")\n",
    "\n",
    "    # Fused activation\n",
    "    activated = fused_gelu(linear_output)\n",
    "    assert activated.shape == linear_output.shape\n",
    "    print(f\"   âœ… Fused GELU activation: {linear_output.shape} â†’ {activated.shape}\")\n",
    "\n",
    "    # Reshape back to original structure\n",
    "    final_output = Tensor(activated.data.reshape(batch_size, seq_len, hidden_dim))\n",
    "    assert final_output.shape == x.shape\n",
    "    print(f\"   âœ… Output reshape: {activated.shape} â†’ {final_output.shape}\")\n",
    "    class TransformerBlock:\n",
    "        def __init__(self, hidden_dim):\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.weight1 = Tensor(np.random.randn(hidden_dim, hidden_dim).astype(np.float32))\n",
    "            self.weight2 = Tensor(np.random.randn(hidden_dim, hidden_dim).astype(np.float32))\n",
    "            self.weight1.grad = None\n",
    "            self.weight2.grad = None\n",
    "\n",
    "        def __call__(self, x):\n",
    "            # Simulate transformer block: linear â†’ activation â†’ linear\n",
    "            batch_size, seq_len, hidden_dim = x.shape\n",
    "            x_flat = Tensor(x.data.reshape(-1, hidden_dim))\n",
    "\n",
    "            # First linear layer\n",
    "            h1 = vectorized_matmul(x_flat, self.weight1)\n",
    "            h1_activated = fused_gelu(h1)\n",
    "\n",
    "            # Second linear layer\n",
    "            h2 = vectorized_matmul(h1_activated, self.weight2)\n",
    "\n",
    "            # Reshape back\n",
    "            output = Tensor(h2.data.reshape(batch_size, seq_len, hidden_dim))\n",
    "            return output\n",
    "\n",
    "        def parameters(self):\n",
    "            return [self.weight1, self.weight2]\n",
    "\n",
    "    # Initialize model and test forward pass\n",
    "    model = TransformerBlock(hidden_dim)\n",
    "    print(f\"   Model parameters: {len(model.parameters())}\")\n",
    "\n",
    "    # Test model forward pass with accelerated operations\n",
    "    print(\"   Testing model forward pass with accelerated operations...\")\n",
    "    output = model(x)\n",
    "    assert output.shape == x.shape\n",
    "    print(f\"   âœ… Model forward pass: {x.shape} â†’ {output.shape}\")\n",
    "\n",
    "    # Verify accelerated operations provide correct results\n",
    "    print(\"   Validating numerical correctness...\")\n",
    "    # Check output is finite and has reasonable values\n",
    "    assert np.all(np.isfinite(output.data)), \"Model output contains NaN or Inf\"\n",
    "    output_mean = np.mean(np.abs(output.data))\n",
    "    # Random initialization can produce larger values - verify reasonable range\n",
    "    assert output_mean < 1000.0, f\"Output values unreasonably large: {output_mean}\"\n",
    "    print(f\"   âœ… Numerical validation passed (mean magnitude: {output_mean:.4f})\")\n",
    "\n",
    "    print(\"   Testing performance characteristics...\")\n",
    "\n",
    "    # Verify acceleration provides measurable benefits\n",
    "    test_sizes = [128, 256]\n",
    "    for size in test_sizes:\n",
    "        test_x = Tensor(np.random.randn(size, size).astype(np.float32))\n",
    "        test_y = Tensor(np.random.randn(size, size).astype(np.float32))\n",
    "\n",
    "        # Time operations and verify reasonable performance\n",
    "        start = time.time()\n",
    "        _ = vectorized_matmul(test_x, test_y)\n",
    "        matmul_time = time.time() - start\n",
    "\n",
    "        start = time.time()\n",
    "        _ = fused_gelu(test_x)\n",
    "        gelu_time = time.time() - start\n",
    "\n",
    "        # Verify operations complete in reasonable time\n",
    "        assert matmul_time < 1.0, f\"Matrix multiplication too slow: {matmul_time:.3f}s\"\n",
    "        assert gelu_time < 0.1, f\"GELU activation too slow: {gelu_time:.3f}s\"\n",
    "\n",
    "        print(f\"   âœ… Size {size}: matmul={matmul_time*1000:.1f}ms, gelu={gelu_time*1000:.1f}ms\")\n",
    "\n",
    "    print(\"   Testing memory efficiency...\")\n",
    "\n",
    "    print(\"âœ… End-to-end acceleration pipeline works!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ‰ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd5964",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "main-execution",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Main execution block - single clean entry point\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ Running Acceleration Module...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run comprehensive module test\n",
    "    test_module()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"âœ… Acceleration module validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a639186",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## â­ Aha Moment: Vectorization and Fusion Speed Things Up\n",
    "\n",
    "**What you built:** Vectorized operations and fused kernels that reduce memory traffic.\n",
    "\n",
    "**Why it matters:** Individual operations like x + y + z require reading and writing memory\n",
    "multiple times. Fused operations like fused_gelu do everything in one pass! This reduces\n",
    "memory bandwidth by 60-80%, a huge win since memory is often the bottleneck.\n",
    "\n",
    "Combined with vectorization (SIMD), these techniques make neural networks 2-5Ã— faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423878f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demo_acceleration():\n",
    "    \"\"\"ğŸ¯ See fused operations produce correct results.\"\"\"\n",
    "    print(\"ğŸ¯ AHA MOMENT: Fused Operations Match Reference\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    # Use concrete small values for clear demonstration\n",
    "    x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "    # Compute GELU using fused implementation\n",
    "    result_fused = fused_gelu(x)\n",
    "\n",
    "    # Compute reference using NumPy directly\n",
    "    sqrt_2_over_pi = np.sqrt(2.0 / np.pi)\n",
    "    result_reference = 0.5 * x.data * (\n",
    "        1.0 + np.tanh(sqrt_2_over_pi * (x.data + 0.044715 * x.data**3))\n",
    "    )\n",
    "\n",
    "    # Display inputs and outputs\n",
    "    print(f\"Input: {x.data}\")\n",
    "    print(f\"GELU output: {result_fused.data}\")\n",
    "    print(f\"Reference:   {result_reference}\")\n",
    "\n",
    "    # Validate results match\n",
    "    match = np.allclose(result_fused.data, result_reference)\n",
    "    print(f\"\\nResults match: {match}\")\n",
    "\n",
    "    print(\"\\nâœ¨ Same math, optimized execution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_module()\n",
    "    print(\"\\n\")\n",
    "    demo_acceleration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787b8b8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸš€ MODULE SUMMARY: Acceleration\n",
    "\n",
    "Congratulations! You've mastered the fundamental techniques for accelerating neural networks!\n",
    "\n",
    "### Key Accomplishments\n",
    "- Built **vectorized operations** leveraging SIMD and optimized BLAS for 2-5Ã— speedups\n",
    "- Implemented **kernel fusion** reducing memory bandwidth by 60-80% for element-wise operations\n",
    "- Created **cache-aware tiling** for efficient large matrix operations\n",
    "- Analyzed **arithmetic intensity patterns** and their impact on the roofline model\n",
    "- Measured **memory efficiency** across different operation types\n",
    "- Developed **production decision framework** for systematic optimization\n",
    "- All tests pass âœ… (validated by `test_module()`)\n",
    "\n",
    "### Systems Insights Discovered\n",
    "- **Roofline Model**: Operations with high arithmetic intensity (FLOPs/byte) scale better\n",
    "- **Memory Bandwidth**: Often the limiting factor for modern accelerators\n",
    "- **Cache Awareness**: Tiling keeps working sets in cache for better performance\n",
    "- **Kernel Fusion**: Critical for memory-bound workloads, reduces intermediate storage by 4-5Ã—\n",
    "- **Optimization Strategy**: Start simple (vectorization), add complexity as needed\n",
    "\n",
    "### Production Impact\n",
    "Your acceleration techniques enable:\n",
    "- **Training larger models** within memory constraints\n",
    "- **Faster iteration cycles** during research and development\n",
    "- **Better hardware utilization** across different deployment targets\n",
    "- **Cost reduction** through improved efficiency\n",
    "\n",
    "### Ready for Next Steps\n",
    "Your acceleration implementations provide the foundation for advanced optimization modules.\n",
    "The performance analysis skills transfer directly to production optimization workflows.\n",
    "\n",
    "Export with: `tito module complete 18`\n",
    "\n",
    "**Next**: Advanced modules will build on these acceleration techniques for specialized optimizations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
