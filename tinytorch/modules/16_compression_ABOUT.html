
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 16: Compression" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/16_compression_ABOUT.html" />
<meta property="og:site_name" content="Tinyüî•Torch" />
<meta property="og:description" content="üöÄ Launch Binder Run interactively in your browser. No setup required. https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F16_compression%2F16_compression.ipynb üìÑ ..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="Tinyüî•Torch" />
<meta name="description" content="üöÄ Launch Binder Run interactively in your browser. No setup required. https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F16_compression%2F16_compression.ipynb üìÑ ..." />

    <title>Module 16: Compression &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1a2c4ab3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";

const defaultStyle = document.createElement('style');
defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}
`;
document.head.appendChild(defaultStyle);

const fullscreenStyle = document.createElement('style');
fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
document.head.appendChild(fullscreenStyle);

// Detect if page has dark background
const isDarkTheme = () => {
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        return brightness < 128;
    }
    return false;
};

const load = async () => {
    await mermaid.run();

    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(load, 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(load, 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(load, 200);
        return;
    }

    const darkTheme = isDarkTheme();

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    const modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    const modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    let previousScrollOffset = [window.scrollX, window.scrollY];

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    const allButtons = [];

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });

        container.appendChild(fullscreenBtn);
        allButtons.push(fullscreenBtn);
    });

    // Update theme classes when theme changes
    const updateTheme = () => {
        const dark = isDarkTheme();
        allButtons.forEach(btn => {
            if (dark) {
                btn.classList.add('dark-theme');
            } else {
                btn.classList.remove('dark-theme');
            }
        });
        if (dark) {
            modal.classList.add('dark-theme');
            modalContent.classList.add('dark-theme');
            closeBtn.classList.add('dark-theme');
        } else {
            modal.classList.remove('dark-theme');
            modalContent.classList.remove('dark-theme');
            closeBtn.classList.remove('dark-theme');
        }
    };

    // Watch for theme changes
    const observer = new MutationObserver(updateTheme);
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    observer.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style']
    });
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/16_compression_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=b021a6d4"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=ee94e95f"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/subscribe-modal.js?v=82641629"></script>
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Module 17: Memoization" href="17_memoization_ABOUT.html" />
    <link rel="prev" title="Module 15: Quantization" href="15_quantization_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://mlsysbook.ai" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mlsysbook" title="Support" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-heart fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Support</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-07)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_autograd_ABOUT.html">05. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_optimizers_ABOUT.html">06. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_training_ABOUT.html">07. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (08-13)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_dataloader_ABOUT.html">08. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_memoization_ABOUT.html">17. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_acceleration_ABOUT.html">18. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/16_compression_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 16: Compression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparsity-measurement">Sparsity Measurement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruning-methods">Pruning Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-distillation">Knowledge Distillation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-approximation">Low-Rank Approximation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruning-fundamentals">Pruning Fundamentals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-vs-unstructured-pruning">Structured vs Unstructured Pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Knowledge Distillation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-approximation-theory">Low-Rank Approximation Theory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-trade-offs">Compression Trade-offs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-compression-matters-at-scale">Why Compression Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-16-compression">
<h1>Module 16: Compression<a class="headerlink" href="#module-16-compression" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>OPTIMIZATION TIER</strong> | Difficulty: ‚óè‚óè‚óè‚óã | Time: 5-7 hours | Prerequisites: 01-14</p>
<p><strong>Prerequisites: Modules 01-14</strong> means you should have:</p>
<ul class="simple">
<li><p>Built tensors, layers, and the complete training pipeline (Modules 01-07)</p></li>
<li><p>Implemented profiling tools to measure model characteristics (Module 14)</p></li>
<li><p>Comfort with weight distributions, parameter counting, and memory analysis</p></li>
</ul>
<p>If you can profile a model‚Äôs parameters and understand weight distributions, you‚Äôre ready.</p>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run interactively in your browser. No setup required.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F16_compression%2F16_compression.ipynb"><span>https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F16_compression%2F16_compression.ipynb</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìÑ View Source</div>
<p class="sd-card-text">Browse the implementation code on GitHub.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/16_compression/16_compression.py"><span>https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/16_compression/16_compression.py</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üéß Audio Overview</div>
<p class="sd-card-text">Listen to an AI-generated overview.</p>
<audio controls style="width: 100%; margin-top: 8px;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/16_compression.mp3" type="audio/mpeg">
</audio>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Model compression is the art of making neural networks smaller and faster while preserving their intelligence. Modern language models occupy 100GB+ of storage, but mobile devices have less than 1GB available for models. Edge devices have even tighter constraints at under 100MB. Compression techniques bridge this gap, enabling deployment of powerful models on resource-constrained devices.</p>
<p>In this module, you‚Äôll implement four fundamental compression techniques: magnitude-based pruning removes small weights, structured pruning eliminates entire channels for hardware efficiency, knowledge distillation trains compact student models from large teachers, and low-rank approximation factors matrices to reduce parameters. By the end, you‚Äôll achieve 80-90% sparsity with minimal accuracy loss, understanding the systems trade-offs between model size, inference speed, and prediction quality.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> magnitude-based pruning to remove 80-90% of small weights while preserving accuracy</p></li>
<li><p><strong>Master</strong> structured pruning that creates hardware-friendly sparsity patterns by removing entire channels</p></li>
<li><p><strong>Build</strong> knowledge distillation systems that compress models 10x through teacher-student training</p></li>
<li><p><strong>Understand</strong> compression trade-offs between sparsity ratio, inference speed, memory footprint, and accuracy preservation</p></li>
<li><p><strong>Analyze</strong> when to apply different compression techniques based on deployment constraints and performance requirements</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id2">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Your Compression System&quot;
        A[&quot;Sparsity&lt;br/&gt;Measurement&quot;]
        B[&quot;Magnitude&lt;br/&gt;Pruning&quot;]
        C[&quot;Structured&lt;br/&gt;Pruning&quot;]
        D[&quot;Knowledge&lt;br/&gt;Distillation&quot;]
        E[&quot;Low-Rank&lt;br/&gt;Approximation&quot;]
    end

    A --&gt; B --&gt; C --&gt; D --&gt; E

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
    style E fill:#e2d5f1
    </pre><figcaption>
<p><span class="caption-number">Fig. 25 </span><span class="caption-text">Your Compression System</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Step</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">measure_sparsity()</span></code></p></td>
<td><p>Calculate percentage of zero weights</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">magnitude_prune()</span></code></p></td>
<td><p>Remove weights below threshold</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">structured_prune()</span></code></p></td>
<td><p>Remove entire channels by importance</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KnowledgeDistillation</span></code></p></td>
<td><p>Train small model from large teacher</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">low_rank_approximate()</span></code></p></td>
<td><p>Compress matrices via SVD</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compress a model by removing 80% of smallest weights</span>
<span class="n">magnitude_prune</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sparsity</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">sparsity</span> <span class="o">=</span> <span class="n">measure_sparsity</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># Returns ~80%</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>Sparse storage formats like CSR (scipy.sparse handles this in production)</p></li>
<li><p>Fine-tuning after pruning (iterative pruning schedules)</p></li>
<li><p>Dynamic pruning during training (PyTorch does this with hooks and callbacks)</p></li>
<li><p>Combined quantization and pruning (advanced technique for maximum compression)</p></li>
</ul>
<p><strong>You are building compression algorithms.</strong> Sparse execution optimizations come from specialized libraries.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section provides a quick reference for the compression functions and classes you‚Äôll build. Use it as your guide while implementing and debugging.</p>
<section id="sparsity-measurement">
<h3>Sparsity Measurement<a class="headerlink" href="#sparsity-measurement" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">measure_sparsity</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span>
</pre></div>
</div>
<p>Calculate the percentage of zero weights in a model. Essential for tracking compression effectiveness.</p>
</section>
<section id="pruning-methods">
<h3>Pruning Methods<a class="headerlink" href="#pruning-methods" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">magnitude_prune</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">magnitude_prune(model,</span> <span class="pre">sparsity=0.9)</span></code></p></td>
<td><p>Remove smallest weights to achieve target sparsity</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">structured_prune</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">structured_prune(model,</span> <span class="pre">prune_ratio=0.5)</span></code></p></td>
<td><p>Remove entire channels based on L2 norm importance</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="knowledge-distillation">
<h3>Knowledge Distillation<a class="headerlink" href="#knowledge-distillation" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">KnowledgeDistillation</span><span class="p">(</span><span class="n">teacher_model</span><span class="p">,</span> <span class="n">student_model</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Constructor Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">teacher_model</span></code>: Large pre-trained model with high accuracy</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">student_model</span></code>: Smaller model to train via distillation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code>: Softening parameter for probability distributions (typical: 3-5)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">alpha</span></code>: Weight for soft targets (0.7 = 70% teacher, 30% hard labels)</p></li>
</ul>
<p><strong>Key Methods:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">distillation_loss</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">distillation_loss(student_logits,</span> <span class="pre">teacher_logits,</span> <span class="pre">true_labels)</span> <span class="pre">-&gt;</span> <span class="pre">float</span></code></p></td>
<td><p>Combined soft and hard target loss</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="low-rank-approximation">
<h3>Low-Rank Approximation<a class="headerlink" href="#low-rank-approximation" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">low_rank_approximate</span><span class="p">(</span><span class="n">weight_matrix</span><span class="p">,</span> <span class="n">rank_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">]</span>
</pre></div>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weight_matrix</span></code>: Weight matrix to compress (e.g., (512, 256) Linear layer weights)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank_ratio</span></code>: Fraction of original rank to keep (0.5 = keep 50% of singular values)</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">U</span></code>: Left singular vectors (shape: m √ó k)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">S</span></code>: Singular values (shape: k)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">V</span></code>: Right singular vectors (shape: k √ó n)</p></li>
</ul>
<p>Where k = rank_ratio √ó min(m, n). Reconstruct approximation with <code class="docutils literal notranslate"><span class="pre">U</span> <span class="pre">&#64;</span> <span class="pre">diag(S)</span> <span class="pre">&#64;</span> <span class="pre">V</span></code>.</p>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental ideas you need to understand model compression deeply. These concepts apply across all ML frameworks and deployment scenarios.</p>
<section id="pruning-fundamentals">
<h3>Pruning Fundamentals<a class="headerlink" href="#pruning-fundamentals" title="Link to this heading">#</a></h3>
<p>Neural networks are remarkably over-parameterized. Research shows that 50-90% of weights in trained models contribute minimally to predictions. Pruning exploits this redundancy by removing unimportant weights, creating sparse networks that maintain accuracy while using dramatically fewer parameters.</p>
<p>The core insight is simple: weights with small magnitudes have little effect on outputs. When you compute <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">W</span> <span class="pre">&#64;</span> <span class="pre">x</span></code>, a weight of 0.001 contributes almost nothing compared to weights of magnitude 2.0 or 3.0. Pruning identifies and zeros out these negligible weights.</p>
<p>Here‚Äôs how your magnitude pruning implementation works:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">magnitude_prune</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sparsity</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Remove weights with smallest magnitudes to achieve target sparsity.&quot;&quot;&quot;</span>
    <span class="c1"># Collect all weights from model (excluding biases)</span>
    <span class="n">all_weights</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">weight_params</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Only weight matrices, not bias vectors</span>
            <span class="n">all_weights</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
            <span class="n">weight_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

    <span class="c1"># Calculate magnitude threshold at desired percentile</span>
    <span class="n">magnitudes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">all_weights</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">magnitudes</span><span class="p">,</span> <span class="n">sparsity</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Apply pruning mask: zero out weights below threshold</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">weight_params</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">threshold</span>
        <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">mask</span>  <span class="c1"># In-place zeroing</span>

    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p>The elegance is in the percentile-based threshold. Setting <code class="docutils literal notranslate"><span class="pre">sparsity=0.9</span></code> means ‚Äúremove the bottom 90% of weights by magnitude.‚Äù NumPy‚Äôs <code class="docutils literal notranslate"><span class="pre">percentile</span></code> function finds the exact value that splits the distribution, and then a binary mask zeros out everything below that threshold.</p>
<p>To understand why this works, consider a typical weight distribution after training:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Weight Magnitudes (sorted):
[0.001, 0.002, 0.003, ..., 0.085, 0.087, ..., 0.95, 1.2, 2.3, 3.1]
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 90% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 10% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        Small, removable                    Large, important

90th percentile = 0.087
Threshold mask: magnitude &gt;= 0.087
Result: Keep only weights &gt;= 0.087 (top 10%)
</pre></div>
</div>
<p>The critical insight is that weight distributions in trained networks are heavily skewed toward zero. Most weights contribute minimally, so removing them preserves the essential computation while dramatically reducing storage and compute.</p>
<p>The memory impact is immediate. A model with 10 million parameters at 90% sparsity has only 1 million active weights. With sparse storage formats (like scipy‚Äôs CSR matrix), this translates directly to 90% memory reduction. The compute savings come from skipping zero multiplications, though realizing this speedup requires sparse computation libraries.</p>
</section>
<section id="structured-vs-unstructured-pruning">
<h3>Structured vs Unstructured Pruning<a class="headerlink" href="#structured-vs-unstructured-pruning" title="Link to this heading">#</a></h3>
<p>Magnitude pruning creates unstructured sparsity: zeros scattered randomly throughout weight matrices. This achieves high compression ratios but creates irregular memory access patterns that modern hardware struggles to accelerate. Structured pruning solves this by removing entire computational units like channels, neurons, or attention heads.</p>
<p>Think of the difference like editing text. Unstructured pruning removes random letters from words, making them hard to read quickly. Structured pruning removes entire words or sentences, preserving readability while reducing length.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Unstructured Sparsity (Magnitude Pruning):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Channel 0: [2.1, 0.0, 1.8, 0.0, 3.2]    ‚îÇ ‚Üê Scattered zeros
‚îÇ Channel 1: [0.0, 2.8, 0.0, 2.1, 0.0]    ‚îÇ ‚Üê Irregular pattern
‚îÇ Channel 2: [1.5, 0.0, 2.4, 0.0, 1.9]    ‚îÇ ‚Üê Hard to optimize
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Structured Sparsity (Channel Pruning):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Channel 0: [2.1, 1.3, 1.8, 0.9, 3.2]    ‚îÇ ‚Üê Fully dense
‚îÇ Channel 1: [0.0, 0.0, 0.0, 0.0, 0.0]    ‚îÇ ‚Üê Fully removed
‚îÇ Channel 2: [1.5, 2.2, 2.4, 1.1, 1.9]    ‚îÇ ‚Üê Fully dense
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</pre></div>
</div>
<p>Structured pruning requires deciding which channels to remove. Your implementation uses L2 norm as an importance metric:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">structured_prune</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prune_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Remove entire channels based on L2 norm importance.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Linear</span><span class="p">):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>

            <span class="c1"># Calculate L2 norm for each output channel (column)</span>
            <span class="n">channel_norms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="c1"># Find channels to prune (lowest importance)</span>
            <span class="n">num_channels</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">num_to_prune</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_channels</span> <span class="o">*</span> <span class="n">prune_ratio</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">num_to_prune</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Get indices of smallest channels</span>
                <span class="n">prune_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argpartition</span><span class="p">(</span><span class="n">channel_norms</span><span class="p">,</span> <span class="n">num_to_prune</span><span class="p">)[:</span><span class="n">num_to_prune</span><span class="p">]</span>

                <span class="c1"># Zero out entire channels</span>
                <span class="n">weight</span><span class="p">[:,</span> <span class="n">prune_indices</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="c1"># Also zero corresponding bias elements</span>
                <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">prune_indices</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p>The L2 norm <code class="docutils literal notranslate"><span class="pre">||W[:,i]||_2</span> <span class="pre">=</span> <span class="pre">sqrt(sum(w_j^2))</span></code> measures the total magnitude of all weights in a channel. Channels with small L2 norms contribute less to the output because all their weights are small. Removing entire channels creates block sparsity that hardware can exploit through vectorized operations on the remaining dense channels.</p>
<p>The key insight in structured pruning is that you remove entire computational units, not scattered weights. When you zero out channel <code class="docutils literal notranslate"><span class="pre">i</span></code> in layer <code class="docutils literal notranslate"><span class="pre">l</span></code>, you‚Äôre eliminating:</p>
<ul class="simple">
<li><p>All connections from that channel to the next layer (forward propagation)</p></li>
<li><p>All gradient computation for that channel (backward propagation)</p></li>
<li><p>The entire channel‚Äôs activation storage (memory savings)</p></li>
</ul>
<p>This creates contiguous blocks of zeros that enable:</p>
<ol class="arabic simple">
<li><p><strong>Memory coalescing</strong>: Hardware accesses dense remaining channels sequentially</p></li>
<li><p><strong>SIMD operations</strong>: CPUs/GPUs process multiple channels in parallel</p></li>
<li><p><strong>No indexing overhead</strong>: Don‚Äôt need sparse matrix formats to track zero locations</p></li>
<li><p><strong>Cache efficiency</strong>: Better spatial locality from accessing dense blocks</p></li>
</ol>
<p>The trade-off is clear: structured pruning achieves lower sparsity (typically 30-50%) than magnitude pruning (80-90%), but the sparsity it creates enables real hardware acceleration. On GPUs and specialized accelerators, structured sparsity can provide 2-3x speedup, while unstructured sparsity requires custom sparse kernels to see any speedup at all.</p>
</section>
<section id="id1">
<h3>Knowledge Distillation<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Knowledge distillation takes a different approach to compression: instead of removing weights from an existing model, train a smaller model to mimic a larger one‚Äôs behavior. The large ‚Äúteacher‚Äù model transfers its knowledge to the compact ‚Äústudent‚Äù model, achieving similar accuracy with dramatically fewer parameters.</p>
<p>The key innovation is using soft targets instead of hard labels. Traditional training uses one-hot labels: for a cat image, the label is <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">0]</span></code> (100% cat). But the teacher‚Äôs predictions are softer: <code class="docutils literal notranslate"><span class="pre">[0.02,</span> <span class="pre">0.05,</span> <span class="pre">0.85,</span> <span class="pre">0.08]</span></code> (85% cat, but some uncertainty about similar classes). These soft predictions contain richer information about class relationships that helps the student learn more effectively.</p>
<p>Temperature scaling controls how soft the distributions become:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute softmax with numerical stability.&quot;&quot;&quot;</span>
    <span class="n">exp_logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">exp_logits</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">distillation_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">student_logits</span><span class="p">,</span> <span class="n">teacher_logits</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate combined distillation loss.&quot;&quot;&quot;</span>
    <span class="c1"># Soften distributions with temperature</span>
    <span class="n">student_soft</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax</span><span class="p">(</span><span class="n">student_logits</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">)</span>
    <span class="n">teacher_soft</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax</span><span class="p">(</span><span class="n">teacher_logits</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">)</span>

    <span class="c1"># Soft target loss (KL divergence)</span>
    <span class="n">soft_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kl_divergence</span><span class="p">(</span><span class="n">student_soft</span><span class="p">,</span> <span class="n">teacher_soft</span><span class="p">)</span>

    <span class="c1"># Hard target loss (cross-entropy)</span>
    <span class="n">student_hard</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax</span><span class="p">(</span><span class="n">student_logits</span><span class="p">)</span>
    <span class="n">hard_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cross_entropy</span><span class="p">(</span><span class="n">student_hard</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">)</span>

    <span class="c1"># Combined loss</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">soft_loss</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">hard_loss</span>

    <span class="k">return</span> <span class="n">total_loss</span>
</pre></div>
</div>
<p>Dividing logits by temperature before softmax spreads probability mass across classes. With <code class="docutils literal notranslate"><span class="pre">temperature=1</span></code>, you get standard softmax with sharp peaks. With <code class="docutils literal notranslate"><span class="pre">temperature=3</span></code>, the distribution flattens, revealing the teacher‚Äôs uncertainty. The student learns both what the teacher predicts (highest probability class) and what it considers similar (non-zero probabilities on other classes).</p>
<p>The combined loss balances two objectives. The soft loss (with <code class="docutils literal notranslate"><span class="pre">alpha=0.7</span></code>) teaches the student to match the teacher‚Äôs reasoning process. The hard loss (with <code class="docutils literal notranslate"><span class="pre">1-alpha=0.3</span></code>) ensures the student still learns correct classifications. This combination typically achieves 10x compression with only 2-5% accuracy loss.</p>
</section>
<section id="low-rank-approximation-theory">
<h3>Low-Rank Approximation Theory<a class="headerlink" href="#low-rank-approximation-theory" title="Link to this heading">#</a></h3>
<p>Weight matrices in neural networks often contain redundancy that can be captured through low-rank approximations. Singular Value Decomposition (SVD) provides the mathematically optimal way to approximate a matrix with fewer parameters while minimizing reconstruction error.</p>
<p>The core idea is matrix factorization. Instead of storing a full (512, 256) weight matrix with 131,072 parameters, you decompose it into smaller factors that capture the essential structure:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">low_rank_approximate</span><span class="p">(</span><span class="n">weight_matrix</span><span class="p">,</span> <span class="n">rank_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Approximate weight matrix using SVD-based low-rank decomposition.&quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">weight_matrix</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Perform SVD: W = U @ diag(S) @ V</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">weight_matrix</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Keep only top-k singular values</span>
    <span class="n">max_rank</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">target_rank</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">rank_ratio</span> <span class="o">*</span> <span class="n">max_rank</span><span class="p">))</span>

    <span class="c1"># Truncate to target rank</span>
    <span class="n">U_truncated</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">target_rank</span><span class="p">]</span>     <span class="c1"># (m, k)</span>
    <span class="n">S_truncated</span> <span class="o">=</span> <span class="n">S</span><span class="p">[:</span><span class="n">target_rank</span><span class="p">]</span>         <span class="c1"># (k,)</span>
    <span class="n">V_truncated</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:</span><span class="n">target_rank</span><span class="p">,</span> <span class="p">:]</span>      <span class="c1"># (k, n)</span>

    <span class="k">return</span> <span class="n">U_truncated</span><span class="p">,</span> <span class="n">S_truncated</span><span class="p">,</span> <span class="n">V_truncated</span>
</pre></div>
</div>
<p>SVD identifies the most important ‚Äúdirections‚Äù in the weight matrix through singular values. Larger singular values capture more variance, so keeping only the top k values preserves most of the matrix‚Äôs information while dramatically reducing parameters.</p>
<p>For a (512, 256) matrix with rank_ratio=0.5:</p>
<ul class="simple">
<li><p>Original: 512 √ó 256 = 131,072 parameters</p></li>
<li><p>Compressed: (512 √ó 128) + 128 + (128 √ó 256) = 98,432 parameters</p></li>
<li><p>Compression ratio: 1.33x (25% reduction)</p></li>
</ul>
<p>The compression ratio improves with larger matrices. For a (1024, 1024) matrix at rank_ratio=0.1:</p>
<ul class="simple">
<li><p>Original: 1,048,576 parameters</p></li>
<li><p>Compressed: (1024 √ó 102) + 102 + (102 √ó 1024) = 209,046 parameters</p></li>
<li><p>Compression ratio: 5.0x (80% reduction)</p></li>
</ul>
<p>Low-rank approximation trades accuracy for size. The reconstruction error depends on the discarded singular values. Choosing the right rank_ratio balances compression and accuracy preservation.</p>
</section>
<section id="compression-trade-offs">
<h3>Compression Trade-offs<a class="headerlink" href="#compression-trade-offs" title="Link to this heading">#</a></h3>
<p>Every compression technique trades accuracy for efficiency, but different techniques make different trade-offs. Understanding these helps you choose the right approach for your deployment constraints.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Technique</p></th>
<th class="head"><p>Compression Ratio</p></th>
<th class="head"><p>Accuracy Loss</p></th>
<th class="head"><p>Hardware Speedup</p></th>
<th class="head"><p>Training Required</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Magnitude Pruning</strong></p></td>
<td><p>5-10x</p></td>
<td><p>1-3%</p></td>
<td><p>Minimal (needs sparse libs)</p></td>
<td><p>No (prune pretrained)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Structured Pruning</strong></p></td>
<td><p>2-3x</p></td>
<td><p>2-5%</p></td>
<td><p>2-3x (hardware-friendly)</p></td>
<td><p>No (prune pretrained)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Knowledge Distillation</strong></p></td>
<td><p>10-50x</p></td>
<td><p>5-10%</p></td>
<td><p>Proportional to size</p></td>
<td><p>Yes (train student)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Low-Rank Approximation</strong></p></td>
<td><p>2-5x</p></td>
<td><p>3-7%</p></td>
<td><p>Minimal (depends on impl)</p></td>
<td><p>No (SVD decomposition)</p></td>
</tr>
</tbody>
</table>
</div>
<p>The systems insight is that compression ratio alone doesn‚Äôt determine deployment success. A 10x compressed model with magnitude pruning might run slower than a 3x compressed model with structured pruning because hardware can‚Äôt accelerate irregular sparsity. Similarly, knowledge distillation requires training infrastructure but achieves the best compression for a given accuracy target.</p>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-pytorch">
<h3>Your Implementation vs. PyTorch<a class="headerlink" href="#your-implementation-vs-pytorch" title="Link to this heading">#</a></h3>
<p>Your TinyTorch compression functions and PyTorch‚Äôs pruning utilities share the same core algorithms. The differences are in integration depth: PyTorch provides hooks for pruning during training, automatic mask management, and integration with quantization-aware training. But the fundamental magnitude-based and structured pruning logic is identical.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Magnitude Pruning</strong></p></td>
<td><p>Global threshold via percentile</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.nn.utils.prune.l1_unstructured</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Structured Pruning</strong></p></td>
<td><p>L2 norm channel removal</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.nn.utils.prune.ln_structured</span></code></p></td>
</tr>
<tr class="row-even"><td><p><strong>Knowledge Distillation</strong></p></td>
<td><p>Manual loss calculation</p></td>
<td><p>User-implemented (same approach)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Sparse Execution</strong></p></td>
<td><p>‚úó Dense NumPy arrays</p></td>
<td><p>‚úì Sparse tensors + kernels</p></td>
</tr>
<tr class="row-even"><td><p><strong>Pruning Schedules</strong></p></td>
<td><p>One-shot pruning</p></td>
<td><p>Iterative + fine-tuning</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows equivalent compression operations in TinyTorch and PyTorch. Notice how the core concepts translate directly while PyTorch provides additional automation for production workflows.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.perf.compression</span><span class="w"> </span><span class="kn">import</span> <span class="n">magnitude_prune</span><span class="p">,</span> <span class="n">measure_sparsity</span>

<span class="c1"># Create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Apply magnitude pruning</span>
<span class="n">magnitude_prune</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sparsity</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Measure results</span>
<span class="n">sparsity</span> <span class="o">=</span> <span class="n">measure_sparsity</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># Returns 80.0 (percentage)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sparsity: </span><span class="si">{</span><span class="n">sparsity</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
‚ö° PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.utils.prune</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">prune</span>

<span class="c1"># Create model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Apply magnitude pruning</span>
<span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="n">prune</span><span class="o">.</span><span class="n">l1_unstructured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
        <span class="n">prune</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span>  <span class="c1"># Make pruning permanent</span>

<span class="c1"># Measure results</span>
<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">zero_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">p</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">sparsity</span> <span class="o">=</span> <span class="n">zero_params</span> <span class="o">/</span> <span class="n">total_params</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sparsity: </span><span class="si">{</span><span class="n">sparsity</span><span class="si">:</span><span class="s2">.1%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs walk through the key differences:</p>
<ul class="simple">
<li><p><strong>Line 1 (Import)</strong>: TinyTorch provides compression in a dedicated <code class="docutils literal notranslate"><span class="pre">perf.compression</span></code> module. PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">torch.nn.utils.prune</span></code> offers similar functionality with additional hooks.</p></li>
<li><p><strong>Line 4-5 (Model)</strong>: Both create identical model architectures. PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> matches TinyTorch‚Äôs explicit layer composition.</p></li>
<li><p><strong>Line 8 (Pruning)</strong>: TinyTorch uses a simple function call that operates on the entire model. PyTorch requires iterating over modules and applying pruning individually, offering finer control.</p></li>
<li><p><strong>Line 13 (Permanence)</strong>: TinyTorch immediately zeros weights. PyTorch uses masks that can be removed or made permanent, enabling experimentation with different sparsity levels.</p></li>
<li><p><strong>Line 16-19 (Measurement)</strong>: TinyTorch provides a dedicated <code class="docutils literal notranslate"><span class="pre">measure_sparsity()</span></code> function. PyTorch requires manual counting, giving you full control over what counts as ‚Äúsparse.‚Äù</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>The core algorithms for magnitude thresholding, L2 norm channel ranking, and knowledge distillation loss are identical. When you understand TinyTorch compression, you understand PyTorch compression. The production differences are in automation, not algorithms.</p>
</div>
</section>
<section id="why-compression-matters-at-scale">
<h3>Why Compression Matters at Scale<a class="headerlink" href="#why-compression-matters-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate compression‚Äôs impact, consider real deployment constraints:</p>
<ul class="simple">
<li><p><strong>Mobile apps</strong>: Models must fit in &lt;10MB for reasonable download sizes and &lt;50MB runtime memory</p></li>
<li><p><strong>Edge devices</strong>: Raspberry Pi 4 has 4GB RAM total, shared across OS and all applications</p></li>
<li><p><strong>Cloud cost</strong>: GPT-3 inference at scale costs $millions/month; 10x compression = $millions saved</p></li>
<li><p><strong>Latency targets</strong>: Self-driving cars need &lt;100ms inference time; compression enables real-time decisions</p></li>
<li><p><strong>Energy efficiency</strong>: Smartphones have ~3000mAh batteries; model size directly impacts battery life</p></li>
</ul>
<p>A 100MB model pruned to 90% sparsity becomes 10MB with sparse storage, fitting mobile constraints. The same model distilled to a 1MB student runs 10x faster, meeting latency requirements. These aren‚Äôt theoretical gains; they‚Äôre necessary for deployment.</p>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions. They‚Äôre designed to build intuition for compression trade-offs you‚Äôll encounter in production.</p>
<p><strong>Q1: Sparsity Calculation</strong></p>
<p>A Linear layer with shape (512, 256) undergoes 80% magnitude pruning. How many weights remain active?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Total parameters: 512 √ó 256 = <strong>131,072</strong></p>
<p>After 80% pruning: 20% remain active = 131,072 √ó 0.2 = <strong>26,214 active weights</strong></p>
<p>Zeroed weights: 131,072 √ó 0.8 = <strong>104,858 zeros</strong></p>
<p>This is why sparsity creates memory savings - 80% of parameters are literally zero!</p>
</div>
<p><strong>Q2: Compression Ratio Analysis</strong></p>
<p>You apply magnitude pruning (90% sparsity) and structured pruning (50% channels) sequentially. What‚Äôs the final sparsity?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Trick question!</strong> Structured pruning zeros entire channels, which may already be partially sparse from magnitude pruning.</p>
<p>Approximation:</p>
<ul class="simple">
<li><p>After magnitude: 90% sparse ‚Üí 10% active weights</p></li>
<li><p>Structured removes 50% of channels ‚Üí removes 50% of rows/columns</p></li>
<li><p>Final active weights ‚âà 10% √ó 50% = <strong>5% active ‚Üí 95% sparse</strong></p></li>
</ul>
<p>Actual result depends on which channels structured pruning removes. If it removes already-sparse channels, sparsity increases less.</p>
</div>
<p><strong>Q3: Knowledge Distillation Efficiency</strong></p>
<p>Teacher model: 100M parameters, 95% accuracy, 500ms inference
Student model: 10M parameters, 92% accuracy, 50ms inference</p>
<p>What‚Äôs the compression ratio and speedup?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Compression ratio</strong>: 100M / 10M = <strong>10x smaller</strong></p>
<p><strong>Speedup</strong>: 500ms / 50ms = <strong>10x faster</strong></p>
<p><strong>Accuracy loss</strong>: 95% - 92% = <strong>3% degradation</strong></p>
<p>Why speedup matches compression: Student has 10x fewer parameters, so 10x fewer operations. Linear scaling!</p>
<p>Is this good? <strong>Yes</strong> - 10x compression with only 3% accuracy loss is excellent for mobile deployment.</p>
</div>
<p><strong>Q4: Low-Rank Decomposition Math</strong></p>
<p>A (1000, 1000) weight matrix gets low-rank approximation with rank=100. Calculate parameter reduction.</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Original: 1000 √ó 1000 = <strong>1,000,000 parameters</strong></p>
<p>SVD decomposition: W ‚âà U &#64; S &#64; V</p>
<ul class="simple">
<li><p>U: (1000, 100) = 100,000 parameters</p></li>
<li><p>S: (100,) = 100 parameters (diagonal)</p></li>
<li><p>V: (100, 1000) = 100,000 parameters</p></li>
</ul>
<p>Compressed: 100,000 + 100 + 100,000 = <strong>200,100 parameters</strong></p>
<p>Compression ratio: 1,000,000 / 200,100 = <strong>~5x reduction</strong></p>
<p>Memory savings: (1,000,000 - 200,100) √ó 4 bytes = <strong>3.2 MB saved</strong> (float32)</p>
</div>
<p><strong>Q5: Structured vs Unstructured Trade-offs</strong></p>
<p>For mobile deployment with tight latency constraints, would you choose magnitude pruning (90% sparsity) or structured pruning (30% sparsity)? Why?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Choose structured pruning (30% sparsity)</strong> despite lower compression.</p>
<p>Reasoning:</p>
<ol class="arabic simple">
<li><p><strong>Hardware acceleration</strong>: Mobile CPUs/GPUs can execute dense channels 2-3x faster than sparse patterns</p></li>
<li><p><strong>Latency guarantee</strong>: Structured sparsity gives predictable speedup; magnitude sparsity needs sparse libraries (often unavailable on mobile)</p></li>
<li><p><strong>Real speedup</strong>: 30% structured = ~1.5x actual speedup; 90% magnitude = no speedup without custom kernels</p></li>
<li><p><strong>Memory</strong>: Both save memory, but latency requirement dominates</p></li>
</ol>
<p><strong>Production insight</strong>: High sparsity ‚â† high speedup. Hardware capabilities matter more than compression ratio for latency-critical applications.</p>
</div>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>For students who want to understand the academic foundations and explore compression techniques further:</p>
<section id="seminal-papers">
<h3>Seminal Papers<a class="headerlink" href="#seminal-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Learning both Weights and Connections for Efficient Neural Networks</strong> - Han et al. (2015). Introduced magnitude-based pruning and demonstrated 90% sparsity with minimal accuracy loss. Foundation for modern pruning research. <a class="reference external" href="https://arxiv.org/abs/1506.02626">arXiv:1506.02626</a></p></li>
<li><p><strong>The Lottery Ticket Hypothesis</strong> - Frankle &amp; Carbin (2019). Showed that dense networks contain sparse subnetworks trainable to full accuracy from initialization. Changed how we think about pruning and network over-parameterization. <a class="reference external" href="https://arxiv.org/abs/1803.03635">arXiv:1803.03635</a></p></li>
<li><p><strong>Distilling the Knowledge in a Neural Network</strong> - Hinton et al. (2015). Introduced knowledge distillation with temperature scaling. Enables training compact models that match large model accuracy. <a class="reference external" href="https://arxiv.org/abs/1503.02531">arXiv:1503.02531</a></p></li>
<li><p><strong>Pruning Filters for Efficient ConvNets</strong> - Li et al. (2017). Demonstrated structured pruning by removing entire convolutional filters. Showed that L1-norm ranking identifies unimportant channels effectively. <a class="reference external" href="https://arxiv.org/abs/1608.08710">arXiv:1608.08710</a></p></li>
</ul>
</section>
<section id="additional-resources">
<h3>Additional Resources<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Survey</strong>: ‚ÄúModel Compression and Hardware Acceleration for Neural Networks‚Äù by Deng et al. (2020) - Comprehensive overview of compression techniques and hardware implications</p></li>
<li><p><strong>Tutorial</strong>: <a class="reference external" href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html">PyTorch Pruning Tutorial</a> - See how production frameworks implement these concepts</p></li>
<li><p><strong>Blog</strong>: ‚ÄúThe State of Sparsity in Deep Neural Networks‚Äù by Uber Engineering - Practical experiences deploying sparse models at scale</p></li>
</ul>
</section>
</section>
<section id="whats-next">
<h2>What‚Äôs Next<a class="headerlink" href="#whats-next" title="Link to this heading">#</a></h2>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Coming Up: Module 17 - Memoization</p>
<p>Implement caching and memoization strategies to eliminate redundant computations. You‚Äôll cache repeated forward passes, attention patterns, and embedding lookups for dramatic speedups in production inference.</p>
</div>
<p><strong>Preview - How Your Compression Gets Used in Future Modules:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>What It Does</p></th>
<th class="head"><p>Your Compression In Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>17: Memoization</strong></p></td>
<td><p>Cache repeated computations</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">compress_model()</span></code> before caching for memory efficiency</p></td>
</tr>
<tr class="row-odd"><td><p><strong>18: Acceleration</strong></p></td>
<td><p>Optimize computation kernels</p></td>
<td><p>Structured sparsity enables vectorized operations</p></td>
</tr>
<tr class="row-even"><td><p><strong>19: Benchmarking</strong></p></td>
<td><p>Measure end-to-end performance</p></td>
<td><p>Compare dense vs sparse model throughput</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/16_compression/16_compression.ipynb">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/16_compression/16_compression.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="15_quantization_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 15: Quantization</p>
      </div>
    </a>
    <a class="right-next"
       href="17_memoization_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Module 17: Memoization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sparsity-measurement">Sparsity Measurement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruning-methods">Pruning Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-distillation">Knowledge Distillation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-approximation">Low-Rank Approximation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruning-fundamentals">Pruning Fundamentals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-vs-unstructured-pruning">Structured vs Unstructured Pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Knowledge Distillation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-approximation-theory">Low-Rank Approximation Theory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-trade-offs">Compression Trade-offs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-compression-matters-at-scale">Why Compression Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>