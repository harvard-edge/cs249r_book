
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 18: Memoization" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/18_memoization_ABOUT.html" />
<meta property="og:site_name" content="Tinyüî•Torch" />
<meta property="og:description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: E..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="Tinyüî•Torch" />
<meta name="description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: E..." />

    <title>Module 18: Memoization &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=cd3a79b9" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";





const initStyles = () => {
    const defaultStyle = document.createElement('style');
    defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}`;
    document.head.appendChild(defaultStyle);

    const fullscreenStyle = document.createElement('style');
    fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
    document.head.appendChild(fullscreenStyle);
}

// Detect if page has dark background
const isDarkTheme = () => {
    // We use a set of heuristics:
    // 1. Check for common dark mode classes or attributes
    // 2. Check computed background color brightness
    if (document.documentElement.classList.contains('dark') ||
        document.documentElement.getAttribute('data-theme') === 'dark' ||
        document.body.classList.contains('dark') ||
        document.body.getAttribute('data-theme') === 'dark') {
        // console.log("Dark theme detected via class/attribute");
        return true;
    }
    if (document.documentElement.classList.contains('light') ||
        document.documentElement.getAttribute('data-theme') === 'light' ||
        document.body.classList.contains('light') ||
        document.body.getAttribute('data-theme') === 'light') {
        // console.log("Light theme detected via class/attribute");
        return false;
    }
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
        // console.log("Dark theme detected via prefers-color-scheme");
        return true;
    }
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        // console.log("Background color brightness:", brightness);
        return brightness < 128;
    }
    // console.log("No dark or light theme detected, defaulting to light theme");
    return false;
};

let darkTheme = isDarkTheme();
let modal = null;
let modalContent = null;
let previousScrollOffset = [window.scrollX, window.scrollY];

const runMermaid = async (rerun) => {
    console.log("Running mermaid diagrams, rerun =", rerun);
    // clear all existing mermaid charts
    let all_mermaids = document.querySelectorAll(".mermaid");

    if (rerun) {
        all_mermaids.forEach((el) => {
            if(!el.hasAttribute("data-original-code")) {
                // store original code
                // console.log(`Storing original code for first run: `, el.innerHTML);
                el.setAttribute('data-original-code', el.innerHTML);
            }
            if(el.getAttribute("data-processed") === "true") {
                // remove and restore original
                el.removeAttribute("data-processed");
                // console.log(`Restoring original code for re-run: `, el.getAttribute('data-original-code'));
                el.innerHTML = el.getAttribute('data-original-code');
            } else {
                // store original code
                // console.log(`Storing original code for re-run: `, el.innerHTML);
                el.setAttribute('data-original-code', el.innerHTML);
            }
        });
        await mermaid.run();
    }

    all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(() => runMermaid(false), 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(() => runMermaid(false), 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(() => runMermaid(false), 200);
        return;
    }

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    if (modal !== null ) {
        // Destroy existing modal
        modal.remove();
        modal = null;
        modalContent = null;
    }

    modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            // Already processed, adjust button class if needed
            const existingBtn = mermaidDiv.parentNode.querySelector('.mermaid-fullscreen-btn');
            if (existingBtn) {
                existingBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
            }
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });
        container.appendChild(fullscreenBtn);
    });
};

const load = async () => {
    initStyles();

    await runMermaid(true);

    const reRunIfThemeChanges = async () => {
        const newDarkTheme = isDarkTheme();
        if (newDarkTheme !== darkTheme) {
            darkTheme = newDarkTheme;
            console.log("Theme change detected, re-running mermaid with", darkTheme ? "dark" : "default", "theme");
            await mermaid.initialize(
                {...JSON.parse(
                    `{"startOnLoad": false}`
                ),
                ...{ darkMode: darkTheme, theme: darkTheme ? 'dark' : 'default' },
                }
            );
            await runMermaid(true);
        }
    };

    // Update theme classes when theme changes
    const themeObserver = new MutationObserver(reRunIfThemeChanges);
    themeObserver.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    themeObserver.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
};





console.log("Initializing mermaid with", darkTheme ? "dark" : "default", "theme");
mermaid.initialize(
    {...JSON.parse(
        `{"startOnLoad": false}`
    ),
    ...{ darkMode: darkTheme, theme: darkTheme ? 'dark' : 'default' },
    }
);

window.addEventListener("load", load);</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/18_memoization_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=0d27a1a4"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=ee94e95f"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/version-badge.js?v=4f0a3395"></script>
    <script src="../_static/subscribe-modal.js?v=c8499bec"></script>
    <script src="../_static/announcement-bar.js?v=49ee9c9d"></script>
    <link rel="canonical" href="https://mlsysbook.ai/tinytorch/modules/18_memoization_ABOUT.html" />
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Module 19: Benchmarking" href="19_benchmarking_ABOUT.html" />
    <link rel="prev" title="Module 17: Acceleration" href="17_acceleration_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../book/" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mlsysbook" title="Support" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-heart fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Support</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-08)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader_ABOUT.html">05. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_autograd_ABOUT.html">06. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimizers_ABOUT.html">07. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_training_ABOUT.html">08. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (09-13)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_acceleration_ABOUT.html">17. Acceleration</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">18. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/18_memoization_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 18: Memoization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-memoization-after-acceleration">Why Memoization After Acceleration?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kvcache-constructor">KVCache Constructor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-methods">Core Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#helper-functions">Helper Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caching-computation">Caching Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kv-cache-in-transformers">KV Cache in Transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-checkpointing">Gradient Checkpointing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cache-invalidation">Cache Invalidation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-compute-trade-offs">Memory-Compute Trade-offs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cache-position-out-of-bounds">Cache Position Out of Bounds</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forgetting-to-advance-position">Forgetting to Advance Position</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatches">Shape Mismatches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cache-not-reset-between-sequences">Cache Not Reset Between Sequences</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-memoization-matters-at-scale">Why Memoization Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-18-memoization">
<h1>Module 18: Memoization<a class="headerlink" href="#module-18-memoization" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>OPTIMIZATION TIER</strong> | Difficulty: ‚óè‚óè‚óã‚óã | Time: 3-5 hours | Prerequisites: 01-14</p>
<p><strong>Prerequisites: Modules 01-14</strong> means you should be comfortable with:</p>
<ul class="simple">
<li><p>Tensor operations, matrix multiplication, and shape manipulation (Module 01)</p></li>
<li><p>Transformer architectures and attention (Modules 12-13)</p></li>
<li><p>Profiling tools (Module 14) to measure speedup</p></li>
</ul>
<p>This module introduces optimization techniques that make production language model inference economically viable. If you understand how transformers compute attention and why it‚Äôs expensive, you‚Äôre ready to learn how to make inference dramatically faster.</p>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run interactively in your browser.</p>
<p class="sd-card-text"><a href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F18_memoization%2F18_memoization.ipynb" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #f97316; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">Open in Binder ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìÑ View Source</div>
<p class="sd-card-text">Browse the source code on GitHub.</p>
<p class="sd-card-text"><a href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/18_memoization/18_memoization.py" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #6b7280; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">View on GitHub ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üéß Audio Overview</div>
<p class="sd-card-text">Listen to an AI-generated overview.</p>
<audio controls style="width: 100%; height: 54px; margin-top: auto;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/18_memoization.mp3" type="audio/mpeg">
</audio>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Every time a language model generates a token, it performs the same computations over and over. When ChatGPT writes a 100-word response, it recomputes attention values for earlier words hundreds of times, wasting enormous computational resources. This inefficiency makes real-time conversational AI economically impossible without optimization.</p>
<p>Memoization solves this by caching computation results for reuse. In transformers, this manifests as KV caching: storing the key and value matrices from attention computations. Instead of recomputing these matrices for every token, the model computes them once and retrieves them from cache. This single optimization transforms generation from O(n¬≤) to O(n) complexity, enabling 10-15x speedup.</p>
<p>In this module, you‚Äôll implement a production-grade KV cache system that makes transformer inference practical at scale. You‚Äôll discover why every deployed language model uses this technique.</p>
</section>
<section id="why-memoization-after-acceleration">
<h2>Why Memoization After Acceleration?<a class="headerlink" href="#why-memoization-after-acceleration" title="Link to this heading">#</a></h2>
<p>The Optimization tier divides runtime optimizations into:</p>
<ul class="simple">
<li><p><strong>Acceleration (17)</strong>: General-purpose speedups that apply to ANY computation</p></li>
<li><p><strong>Memoization (18)</strong>: Domain-specific optimization for transformer generation</p></li>
</ul>
<p><strong>Why this order?</strong> Pedagogically, you learn general techniques before specialized applications:</p>
<ol class="arabic simple">
<li><p><strong>Acceleration</strong> teaches universal concepts: vectorization, cache locality, kernel fusion. These apply to matrix multiplication, convolutions, attention‚Äîeverything.</p></li>
<li><p><strong>Memoization (KV-cache)</strong> is specialized: it only helps autoregressive transformer generation, trading O(n) memory for O(n¬≤) ‚Üí O(n) speedup.</p></li>
</ol>
<p>Once you understand how to make any code fast (Module 17), you can appreciate this elegant optimization that makes ChatGPT and Claude economically viable (Module 18).</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> a KVCache class with efficient memory management and O(1) update operations</p></li>
<li><p><strong>Master</strong> the memory-compute trade-off: accepting O(n) memory overhead for O(n¬≤) to O(n) speedup</p></li>
<li><p><strong>Understand</strong> why memoization transforms generation complexity from quadratic to linear</p></li>
<li><p><strong>Connect</strong> your implementation to production systems like ChatGPT and Claude that rely on KV caching</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;KV Cache System&quot;
        A[&quot;Cache Storage&lt;br/&gt;Pre-allocated tensors&quot;]
        B[&quot;Update Logic&lt;br/&gt;O(1) append&quot;]
        C[&quot;Retrieval&lt;br/&gt;O(1) slice&quot;]
        D[&quot;Memory Tracking&lt;br/&gt;Usage analysis&quot;]
    end

    A --&gt; B --&gt; C --&gt; D

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#d4edda
    style D fill:#f8d7da
    </pre><figcaption>
<p><span class="caption-number">Fig. 27 </span><span class="caption-text">KV Cache System</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Step</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KVCache.__init__()</span></code></p></td>
<td><p>Pre-allocated cache storage per layer</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KVCache.update()</span></code></p></td>
<td><p>O(1) cache append without copying</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KVCache.get()</span></code></p></td>
<td><p>O(1) retrieval of cached values</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">enable_kv_cache()</span></code></p></td>
<td><p>Non-invasive model enhancement</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Performance analysis</p></td>
<td><p>Measure speedup and memory usage</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enable caching for dramatic speedup</span>
<span class="n">cache</span> <span class="o">=</span> <span class="n">enable_kv_cache</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># Generate with 10-15x faster inference</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>Multi-batch cache management (production systems handle thousands of concurrent sequences)</p></li>
<li><p>Cache eviction strategies (handling sequences longer than max_seq_len)</p></li>
<li><p>GPU memory optimization (production uses memory pools and paging)</p></li>
<li><p>Speculative decoding (advanced technique that builds on KV caching)</p></li>
</ul>
<p><strong>You are building the core memoization mechanism.</strong> Advanced cache management comes in production deployment.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section provides a quick reference for the KVCache class you‚Äôll build. Use this as your guide while implementing and debugging.</p>
<section id="kvcache-constructor">
<h3>KVCache Constructor<a class="headerlink" href="#kvcache-constructor" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">KVCache</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">KVCache</span>
</pre></div>
</div>
<p>Pre-allocates cache storage for all transformer layers. Each layer gets two tensors (keys and values) sized to hold the maximum sequence length.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>: Number of sequences to cache simultaneously</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code>: Maximum sequence length to support</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_layers</span></code>: Number of transformer layers in the model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_heads</span></code>: Number of attention heads per layer</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">head_dim</span></code>: Dimension of each attention head</p></li>
</ul>
</section>
<section id="core-methods">
<h3>Core Methods<a class="headerlink" href="#core-methods" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">update</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">update(layer_idx:</span> <span class="pre">int,</span> <span class="pre">key:</span> <span class="pre">Tensor,</span> <span class="pre">value:</span> <span class="pre">Tensor)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Append new K,V to cache for given layer</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">get</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">get(layer_idx:</span> <span class="pre">int)</span> <span class="pre">-&gt;</span> <span class="pre">Tuple[Tensor,</span> <span class="pre">Tensor]</span></code></p></td>
<td><p>Retrieve cached K,V for attention computation</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">advance</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">advance()</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Move sequence position forward after processing token</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">reset</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">reset()</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Clear cache for new generation sequence</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">get_memory_usage</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">get_memory_usage()</span> <span class="pre">-&gt;</span> <span class="pre">Dict[str,</span> <span class="pre">float]</span></code></p></td>
<td><p>Calculate cache memory consumption</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="helper-functions">
<h3>Helper Functions<a class="headerlink" href="#helper-functions" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">enable_kv_cache</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">enable_kv_cache(model)</span> <span class="pre">-&gt;</span> <span class="pre">KVCache</span></code></p></td>
<td><p>Non-invasively add caching to transformer</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">disable_kv_cache</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">disable_kv_cache(model)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Restore original attention behavior</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental ideas you need to understand memoization in transformers. These concepts explain why KV caching is the optimization that makes production language models economically viable.</p>
<section id="caching-computation">
<h3>Caching Computation<a class="headerlink" href="#caching-computation" title="Link to this heading">#</a></h3>
<p>Memoization trades memory for speed by storing computation results for reuse. When a function is called with the same inputs repeatedly, computing the result once and caching it eliminates redundant work. This trade-off makes sense when memory is cheaper than computation, which is almost always true for inference.</p>
<p>In transformers, attention is the perfect target for memoization. During autoregressive generation, each new token requires attention over all previous tokens. The naive approach recomputes key and value projections for every previous token at every step, leading to quadratic complexity. But these projections never change once computed, making them ideal candidates for caching.</p>
<p>Here‚Äôs the core insight in your implementation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update cache with new key-value pairs for given layer.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">layer_idx</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Layer index </span><span class="si">{</span><span class="n">layer_idx</span><span class="si">}</span><span class="s2"> &gt;= num_layers </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_pos</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sequence position </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_pos</span><span class="si">}</span><span class="s2"> &gt;= max_seq_len </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Get cache for this layer</span>
    <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">caches</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>

    <span class="c1"># Update cache at current position (efficient O(1) write)</span>
    <span class="n">key_cache</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_pos</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_pos</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">data</span>
    <span class="n">value_cache</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_pos</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_pos</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
<p>This O(1) update operation writes directly to a pre-allocated position in the cache. No array resizing, no data copying, just an indexed assignment. The use of <code class="docutils literal notranslate"><span class="pre">.data</span></code> accesses the underlying NumPy array directly, avoiding gradient tracking overhead since caching is inference-only.</p>
<p>The computational savings compound across generation steps. For a 100-token sequence:</p>
<ul class="simple">
<li><p>Without caching: 1 + 2 + 3 + ‚Ä¶ + 100 = 5,050 K,V computations</p></li>
<li><p>With caching: 100 K,V computations (one per token)</p></li>
<li><p>Speedup: 50x reduction in K,V computation alone</p></li>
</ul>
</section>
<section id="kv-cache-in-transformers">
<h3>KV Cache in Transformers<a class="headerlink" href="#kv-cache-in-transformers" title="Link to this heading">#</a></h3>
<p>Transformer attention computes three projections from the input: query (Q), key (K), and value (V). The attention output is computed as softmax(Q &#64; K^T / sqrt(d_k)) &#64; V. During generation, each new token produces a new query, but the keys and values from previous tokens remain constant.</p>
<p>Consider generating the sequence ‚ÄúHello world!‚Äù:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Step 1: Input = [&quot;Hello&quot;]
  Compute: Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ
  Attention: Q‚ÇÅ @ [K‚ÇÅ] @ [V‚ÇÅ]

Step 2: Input = [&quot;Hello&quot;, &quot;world&quot;]
  Compute: Q‚ÇÇ, K‚ÇÇ, V‚ÇÇ
  Attention: Q‚ÇÇ @ [K‚ÇÅ, K‚ÇÇ] @ [V‚ÇÅ, V‚ÇÇ]
  Problem: K‚ÇÅ and V‚ÇÅ are recomputed unnecessarily!

Step 3: Input = [&quot;Hello&quot;, &quot;world&quot;, &quot;!&quot;]
  Compute: Q‚ÇÉ, K‚ÇÉ, V‚ÇÉ
  Attention: Q‚ÇÉ @ [K‚ÇÅ, K‚ÇÇ, K‚ÇÉ] @ [V‚ÇÅ, V‚ÇÇ, V‚ÇÉ]
  Problem: K‚ÇÅ, V‚ÇÅ, K‚ÇÇ, V‚ÇÇ are all recomputed!
</pre></div>
</div>
<p>The cache eliminates this redundancy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Step 1: Compute K‚ÇÅ, V‚ÇÅ ‚Üí Cache them
Step 2: Compute K‚ÇÇ, V‚ÇÇ ‚Üí Append to cache
  Attention: Q‚ÇÇ @ cached[K‚ÇÅ, K‚ÇÇ] @ cached[V‚ÇÅ, V‚ÇÇ]
Step 3: Compute K‚ÇÉ, V‚ÇÉ ‚Üí Append to cache
  Attention: Q‚ÇÉ @ cached[K‚ÇÅ, K‚ÇÇ, K‚ÇÉ] @ cached[V‚ÇÅ, V‚ÇÇ, V‚ÇÉ]
</pre></div>
</div>
<p>Each step now computes only one new K,V pair instead of recomputing all previous pairs.</p>
</section>
<section id="gradient-checkpointing">
<h3>Gradient Checkpointing<a class="headerlink" href="#gradient-checkpointing" title="Link to this heading">#</a></h3>
<p>While KV caching optimizes inference, gradient checkpointing addresses the opposite problem: memory consumption during training. Training requires storing intermediate activations for backpropagation, but for very deep networks, this can exceed available memory. Gradient checkpointing trades compute for memory by not storing all activations.</p>
<p>The technique works by discarding some intermediate activations during the forward pass and recomputing them during backpropagation when needed. Instead of storing activations for all layers (requiring O(n) memory where n is the number of layers), checkpointing only stores activations at regular intervals (checkpoints). Between checkpoints, activations are recomputed from the last checkpoint during the backward pass.</p>
<p>For a transformer with 96 layers:</p>
<ul class="simple">
<li><p>Without checkpointing: Store 96 sets of activations</p></li>
<li><p>With checkpointing every 12 layers: Store 8 sets, recompute 11 sets during backward</p></li>
<li><p>Memory reduction: 12x decrease</p></li>
<li><p>Compute increase: ~33% slower training (recomputation overhead)</p></li>
</ul>
<p>This is the inverse trade-off from KV caching. KV caching spends memory to save compute during inference. Gradient checkpointing spends compute to save memory during training. Both techniques recognize that memory and compute are fungible resources with different costs in different contexts.</p>
</section>
<section id="cache-invalidation">
<h3>Cache Invalidation<a class="headerlink" href="#cache-invalidation" title="Link to this heading">#</a></h3>
<p>Cache invalidation is one of the hardest problems in computer science because deciding when cached data is still valid requires careful analysis. For KV caching in transformers, invalidation is straightforward because the cached values have well-defined lifetimes.</p>
<p>During generation, cached K,V pairs remain valid for the entire sequence being generated. The cache is invalidated and reset when starting a new generation sequence. This simplicity comes from the autoregressive property: each token depends only on previous tokens, and those dependencies are frozen once computed.</p>
<p>Here‚Äôs how your implementation handles cache lifecycle:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset cache for new generation sequence.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">seq_pos</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Zero out caches for clean state (helps with debugging)</span>
    <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">caches</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>
        <span class="n">key_cache</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="n">value_cache</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
<p>The reset operation returns the sequence position to zero and clears the cache data. This is called when starting to generate a new sequence, ensuring no stale data from previous generations affects the current one.</p>
<p>Production systems handle more complex invalidation scenarios:</p>
<ul class="simple">
<li><p><strong>Max length reached</strong>: When the sequence fills the cache, either error out or implement a sliding window</p></li>
<li><p><strong>Batch inference</strong>: Each sequence in a batch has independent cache state</p></li>
<li><p><strong>Multi-turn conversation</strong>: Some systems maintain cache across turns, others reset per turn</p></li>
</ul>
</section>
<section id="memory-compute-trade-offs">
<h3>Memory-Compute Trade-offs<a class="headerlink" href="#memory-compute-trade-offs" title="Link to this heading">#</a></h3>
<p>Every optimization involves trade-offs. KV caching trades memory for speed, and understanding this exchange quantitatively reveals when the technique makes sense.</p>
<p>For a transformer with L layers, H heads per layer, dimension D per head, and maximum sequence length S, the cache requires:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Memory = 2 √ó L √ó H √ó S √ó D √ó 4 bytes

Example (GPT-2 Small):
L = 12 layers
H = 12 heads
S = 1024 tokens
D = 64 dimensions
Memory = 2 √ó 12 √ó 12 √ó 1024 √ó 64 √ó 4 = 75,497,472 bytes ‚âà 75 MB
</pre></div>
</div>
<p>For a model with 125 million parameters (500 MB), the cache adds 15% memory overhead. This seems significant until you consider the computational savings.</p>
<p>Without caching, generating a sequence of length N requires computing K,V for:</p>
<ul class="simple">
<li><p>Step 1: 1 token</p></li>
<li><p>Step 2: 2 tokens</p></li>
<li><p>Step 3: 3 tokens</p></li>
<li><p>Step N: N tokens</p></li>
<li><p>Total: 1 + 2 + 3 + ‚Ä¶ + N = N(N+1)/2 ‚âà N¬≤/2 computations</p></li>
</ul>
<p>With caching:</p>
<ul class="simple">
<li><p>Step 1: 1 token (compute and cache)</p></li>
<li><p>Step 2: 1 token (compute and append)</p></li>
<li><p>Step 3: 1 token (compute and append)</p></li>
<li><p>Step N: 1 token (compute and append)</p></li>
<li><p>Total: N computations</p></li>
</ul>
<p>For N = 100 tokens, caching provides 50x reduction in K,V computation. For N = 1000 tokens, the reduction is 500x. The speedup grows with sequence length, making the memory trade-off increasingly favorable for longer generation.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Sequence Length</p></th>
<th class="head"><p>Cache Memory</p></th>
<th class="head"><p>Compute Reduction</p></th>
<th class="head"><p>Effective Speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>10 tokens</p></td>
<td><p>75 MB</p></td>
<td><p>5.5x</p></td>
<td><p>3-5x</p></td>
</tr>
<tr class="row-odd"><td><p>50 tokens</p></td>
<td><p>75 MB</p></td>
<td><p>25.5x</p></td>
<td><p>8-12x</p></td>
</tr>
<tr class="row-even"><td><p>100 tokens</p></td>
<td><p>75 MB</p></td>
<td><p>50.5x</p></td>
<td><p>10-15x</p></td>
</tr>
<tr class="row-odd"><td><p>500 tokens</p></td>
<td><p>75 MB</p></td>
<td><p>250.5x</p></td>
<td><p>12-20x</p></td>
</tr>
</tbody>
</table>
</div>
<p>The effective speedup is lower than the theoretical compute reduction because attention includes other operations beyond K,V projection, but the benefit is still dramatic.</p>
</section>
</section>
<section id="common-errors">
<h2>Common Errors<a class="headerlink" href="#common-errors" title="Link to this heading">#</a></h2>
<p>These are the errors you‚Äôll encounter most often when implementing KV caching. Understanding why they happen will save hours of debugging.</p>
<section id="cache-position-out-of-bounds">
<h3>Cache Position Out of Bounds<a class="headerlink" href="#cache-position-out-of-bounds" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">Sequence</span> <span class="pre">position</span> <span class="pre">128</span> <span class="pre">&gt;=</span> <span class="pre">max_seq_len</span> <span class="pre">128</span></code></p>
<p>This happens when you try to append to a full cache. The cache is pre-allocated with a maximum sequence length, and attempting to write beyond that length raises an error.</p>
<p><strong>Cause</strong>: Generation exceeded the maximum sequence length specified when creating the cache.</p>
<p><strong>Fix</strong>: Either increase <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code> when creating the cache, or implement cache eviction logic to handle sequences longer than the maximum.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create cache with sufficient capacity</span>
<span class="n">cache</span> <span class="o">=</span> <span class="n">KVCache</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>  <span class="c1"># Increased from 128</span>
                <span class="n">num_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">head_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="forgetting-to-advance-position">
<h3>Forgetting to Advance Position<a class="headerlink" href="#forgetting-to-advance-position" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Cache retrieval returns the same K,V repeatedly, or update overwrites previous values</p>
<p><strong>Symptom</strong>: Generated text repeats, or cache doesn‚Äôt grow as expected</p>
<p><strong>Cause</strong>: Forgetting to call <code class="docutils literal notranslate"><span class="pre">cache.advance()</span></code> after updating all layers for a token.</p>
<p><strong>Fix</strong>: Always advance the cache position after processing a complete token through all layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="n">cache</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">new_key</span><span class="p">,</span> <span class="n">new_value</span><span class="p">)</span>

<span class="n">cache</span><span class="o">.</span><span class="n">advance</span><span class="p">()</span>  <span class="c1"># Move to next position for next token</span>
</pre></div>
</div>
</section>
<section id="shape-mismatches">
<h3>Shape Mismatches<a class="headerlink" href="#shape-mismatches" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Broadcasting error or shape mismatch when updating cache</p>
<p><strong>Symptom</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">could</span> <span class="pre">not</span> <span class="pre">broadcast</span> <span class="pre">input</span> <span class="pre">array</span> <span class="pre">from</span> <span class="pre">shape</span> <span class="pre">(1,8,64,64)</span> <span class="pre">into</span> <span class="pre">shape</span> <span class="pre">(1,8,1,64)</span></code></p>
<p><strong>Cause</strong>: The key and value tensors passed to <code class="docutils literal notranslate"><span class="pre">update()</span></code> must have shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">heads,</span> <span class="pre">1,</span> <span class="pre">head_dim)</span></code> with sequence dimension equal to 1 (single new token).</p>
<p><strong>Fix</strong>: Ensure new K,V tensors represent a single token:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Correct: Single token (seq_len = 1)</span>
<span class="n">new_key</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span>
<span class="n">cache</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">new_key</span><span class="p">,</span> <span class="n">new_value</span><span class="p">)</span>

<span class="c1"># Wrong: Multiple tokens (seq_len = 64)</span>
<span class="n">wrong_key</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">))</span>
<span class="n">cache</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">wrong_key</span><span class="p">,</span> <span class="n">wrong_value</span><span class="p">)</span>  <span class="c1"># This will fail!</span>
</pre></div>
</div>
</section>
<section id="cache-not-reset-between-sequences">
<h3>Cache Not Reset Between Sequences<a class="headerlink" href="#cache-not-reset-between-sequences" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Second generation includes tokens from first generation</p>
<p><strong>Symptom</strong>: Model generates text that seems to continue from a previous unrelated sequence</p>
<p><strong>Cause</strong>: Forgetting to reset the cache when starting a new generation sequence.</p>
<p><strong>Fix</strong>: Always reset the cache before generating a new sequence:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate first sequence</span>
<span class="n">output1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt1</span><span class="p">)</span>

<span class="c1"># Reset cache before second sequence</span>
<span class="n">model</span><span class="o">.</span><span class="n">_kv_cache</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="c1"># Generate second sequence (independent of first)</span>
<span class="n">output2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt2</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-pytorch">
<h3>Your Implementation vs. PyTorch<a class="headerlink" href="#your-implementation-vs-pytorch" title="Link to this heading">#</a></h3>
<p>Your KVCache implementation uses the same conceptual design as production frameworks. The differences lie in scale, optimization level, and integration depth. PyTorch‚Äôs KV cache implementation is written in C++ and CUDA for speed, supports dynamic batching for serving multiple users, and includes sophisticated memory management with paging and eviction.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>PyTorch (Transformers library)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Backend</strong></p></td>
<td><p>NumPy (CPU)</p></td>
<td><p>C++/CUDA (GPU)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Pre-allocation</strong></p></td>
<td><p>Fixed max_seq_len</p></td>
<td><p>Dynamic growth + paging</p></td>
</tr>
<tr class="row-even"><td><p><strong>Batch support</strong></p></td>
<td><p>Single batch size</p></td>
<td><p>Dynamic batching</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Memory management</strong></p></td>
<td><p>Simple reset</p></td>
<td><p>LRU eviction, memory pools</p></td>
</tr>
<tr class="row-even"><td><p><strong>Update complexity</strong></p></td>
<td><p>O(1)</p></td>
<td><p>O(1) with optimized kernels</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows how KV caching is used in TinyTorch versus production PyTorch. The API patterns are similar because the underlying concept is identical.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.perf.memoization</span><span class="w"> </span><span class="kn">import</span> <span class="n">enable_kv_cache</span>

<span class="c1"># Enable caching</span>
<span class="n">cache</span> <span class="o">=</span> <span class="n">enable_kv_cache</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Generate with caching (10-15x faster)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_token</span><span class="p">)</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="c1"># Cache automatically used and updated</span>
    <span class="n">input_token</span> <span class="o">=</span> <span class="n">next_token</span>

<span class="c1"># Reset for new sequence</span>
<span class="n">cache</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
‚ö° PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="c1"># KV cache enabled automatically during generate()</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># KV caching enabled</span>
<span class="p">)</span>

<span class="c1"># Cache managed internally by HuggingFace</span>
<span class="c1"># Automatically reset between generate() calls</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs examine each approach to understand the similarities and differences:</p>
<ul class="simple">
<li><p><strong>Line 1-2 (Imports)</strong>: TinyTorch uses an explicit <code class="docutils literal notranslate"><span class="pre">enable_kv_cache()</span></code> function to opt-in to caching. PyTorch‚Äôs Transformers library integrates caching directly into the model architecture.</p></li>
<li><p><strong>Line 4-5 (Setup)</strong>: TinyTorch requires manually enabling the cache and storing the reference. PyTorch handles this transparently when you call <code class="docutils literal notranslate"><span class="pre">generate()</span></code>.</p></li>
<li><p><strong>Line 7-12 (Generation)</strong>: TinyTorch‚Äôs loop explicitly manages token generation with the cache working behind the scenes. PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">generate()</span></code> method encapsulates the entire loop and automatically uses caching when <code class="docutils literal notranslate"><span class="pre">use_cache=True</span></code>.</p></li>
<li><p><strong>Line 14-15 (Reset)</strong>: TinyTorch requires manual cache reset between sequences. PyTorch automatically resets the cache at the start of each <code class="docutils literal notranslate"><span class="pre">generate()</span></code> call.</p></li>
</ul>
<p>The core difference is abstraction level. TinyTorch exposes the cache as an explicit object you control, making the optimization visible for learning. PyTorch hides caching inside <code class="docutils literal notranslate"><span class="pre">generate()</span></code> for ease of use in production. Both implementations use the same O(1) append pattern you built.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>The fundamental algorithm: compute K,V once, append to cache, retrieve for attention. Production systems add memory management and batching, but the core optimization is exactly what you implemented.</p>
</div>
</section>
<section id="why-memoization-matters-at-scale">
<h3>Why Memoization Matters at Scale<a class="headerlink" href="#why-memoization-matters-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate the production impact of KV caching, consider the economics of language model serving:</p>
<ul class="simple">
<li><p><strong>ChatGPT</strong>: Serves millions of requests per day. Without KV caching, serving costs would be 10x higher, making the service economically unviable at current pricing.</p></li>
<li><p><strong>GitHub Copilot</strong>: Generates code completions in real-time. Without caching, latency would increase from 100ms to 1-2 seconds, breaking the developer experience.</p></li>
<li><p><strong>Production API serving</strong>: A single V100 GPU serving GPT-2 can handle 50-100 concurrent users with caching, but only 5-10 without it. This 10x difference determines infrastructure costs.</p></li>
</ul>
<p>The memory cost is modest compared to the benefit. For a GPT-2 model:</p>
<ul class="simple">
<li><p>Model parameters: 500 MB (loaded once, shared across all users)</p></li>
<li><p>KV cache per user: 75 MB</p></li>
<li><p>10 concurrent users: 750 MB cache + 500 MB model = 1.25 GB total</p></li>
<li><p>Fits comfortably on a 16 GB GPU while delivering 10x throughput</p></li>
</ul>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions. They‚Äôre designed to build intuition for the performance characteristics and trade-offs you‚Äôll encounter in production ML systems.</p>
<p><strong>Q1: Cache Memory Calculation</strong></p>
<p>A 12-layer transformer has 8 attention heads per layer, each head has 64 dimensions, maximum sequence length is 1024, and batch size is 4. Calculate the KV cache memory requirement.</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Shape per cache tensor: (batch=4, heads=8, seq=1024, dim=64)</p>
<p>Elements per tensor: 4 √ó 8 √ó 1024 √ó 64 = 2,097,152</p>
<p>Each layer has 2 tensors (K and V): 2 √ó 2,097,152 = 4,194,304 elements per layer</p>
<p>Total across 12 layers: 12 √ó 4,194,304 = 50,331,648 elements</p>
<p>Memory: 50,331,648 √ó 4 bytes = 201,326,592 bytes ‚âà <strong>192 MB</strong></p>
<p>This is why production systems carefully tune batch size and sequence length!</p>
</div>
<p><strong>Q2: Complexity Reduction</strong></p>
<p>Without caching, generating 200 tokens requires how many K,V computations? With caching?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Without caching</strong>: 1 + 2 + 3 + ‚Ä¶ + 200 = 200 √ó 201 / 2 = <strong>20,100 computations</strong></p>
<p><strong>With caching</strong>: 200 computations (one per token)</p>
<p><strong>Reduction</strong>: 20,100 / 200 = <strong>100.5x fewer K,V computations</strong></p>
<p>This is why the speedup grows with sequence length!</p>
</div>
<p><strong>Q3: Memory-Compute Trade-off</strong></p>
<p>A model uses 2 GB for parameters. Adding KV cache uses 300 MB. Is this trade-off worthwhile if it provides 12x speedup?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Memory overhead</strong>: 300 MB / 2000 MB = 15% increase</p>
<p><strong>Speedup</strong>: 12x faster generation</p>
<p><strong>Analysis</strong>:</p>
<ul class="simple">
<li><p>Cost: 15% more memory</p></li>
<li><p>Benefit: 12x more throughput (or 12x lower latency)</p></li>
<li><p>Result: You can serve 12x more users with 1.15x the memory</p></li>
</ul>
<p><strong>Verdict</strong>: Absolutely worthwhile! Memory is cheap, compute is expensive.</p>
<p>In production, this enables serving 120 users per GPU instead of 10 users, dramatically reducing infrastructure costs.</p>
</div>
<p><strong>Q4: Cache Hit Rate</strong></p>
<p>During generation, what percentage of K,V retrievals come from cache vs. fresh computation after 50 tokens?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>At token position 50:</p>
<ul class="simple">
<li><p>Fresh computation: 1 new K,V pair</p></li>
<li><p>Cache retrievals: 49 previous K,V pairs</p></li>
<li><p>Total: 50 K,V pairs needed</p></li>
</ul>
<p><strong>Cache hit rate</strong>: 49/50 = <strong>98%</strong></p>
<p>As generation continues:</p>
<ul class="simple">
<li><p>Token 100: 99/100 = 99% hit rate</p></li>
<li><p>Token 500: 499/500 = 99.8% hit rate</p></li>
</ul>
<p>The cache hit rate approaches 100% for long sequences, explaining why speedup increases with length!</p>
</div>
<p><strong>Q5: Batch Inference Scaling</strong></p>
<p>Cache memory for batch_size=1 is 75 MB. What is cache memory for batch_size=8?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Cache memory scales linearly with batch size:</p>
<p><strong>batch_size=8</strong>: 75 MB √ó 8 = <strong>600 MB</strong></p>
<p>This is why production systems carefully manage batch size:</p>
<ul class="simple">
<li><p>Larger batches ‚Üí higher throughput (more sequences per second)</p></li>
<li><p>Larger batches ‚Üí more memory (may hit GPU limits)</p></li>
</ul>
<p>Trade-off example on 16 GB GPU:</p>
<ul class="simple">
<li><p>Model: 2 GB</p></li>
<li><p>Available for cache: 14 GB</p></li>
<li><p>Max batch size: 14 GB / 75 MB ‚âà 186 sequences</p></li>
</ul>
<p>Production systems balance batch size against latency requirements and memory constraints.</p>
</div>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>For students who want to understand the academic foundations and production implementation of memoization in transformers:</p>
<section id="seminal-papers">
<h3>Seminal Papers<a class="headerlink" href="#seminal-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Attention Is All You Need</strong> - Vaswani et al. (2017). The original transformer paper that introduced the architecture requiring KV caching for efficient generation. Section 3.2 describes the attention mechanism that benefits from memoization. <a class="reference external" href="https://arxiv.org/abs/1706.03762">arXiv:1706.03762</a></p></li>
<li><p><strong>Generating Sequences With Recurrent Neural Networks</strong> - Graves (2013). Early work on autoregressive generation patterns, establishing the sequential token generation that creates the redundant computation KV caching eliminates. <a class="reference external" href="https://arxiv.org/abs/1308.0850">arXiv:1308.0850</a></p></li>
<li><p><strong>Training Compute-Optimal Large Language Models</strong> - Hoffmann et al. (2022). Analyzes the computational costs of training and inference, quantifying the importance of inference optimizations like KV caching at scale. <a class="reference external" href="https://arxiv.org/abs/2203.15556">arXiv:2203.15556</a></p></li>
<li><p><strong>FlashAttention: Fast and Memory-Efficient Exact Attention</strong> - Dao et al. (2022). Modern attention optimization that combines with KV caching in production systems, demonstrating complementary optimization strategies. <a class="reference external" href="https://arxiv.org/abs/2205.14135">arXiv:2205.14135</a></p></li>
</ul>
</section>
<section id="additional-resources">
<h3>Additional Resources<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>System</strong>: <a class="reference external" href="https://vllm.readthedocs.io/">vLLM documentation</a> - Production serving system that uses advanced KV cache management with paging</p></li>
<li><p><strong>Tutorial</strong>: <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/text_generation">Hugging Face Text Generation Guide</a> - See <code class="docutils literal notranslate"><span class="pre">use_cache</span></code> parameter in production API</p></li>
<li><p><strong>Blog</strong>: ‚ÄúThe Illustrated Transformer‚Äù by Jay Alammar - Visual explanation of attention mechanisms that benefit from caching</p></li>
</ul>
</section>
</section>
<section id="whats-next">
<h2>What‚Äôs Next<a class="headerlink" href="#whats-next" title="Link to this heading">#</a></h2>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Coming Up: Module 19 - Benchmarking</p>
<p>Implement kernel fusion, operator batching, and CPU/GPU optimization techniques. You‚Äôll combine multiple operations to reduce memory bandwidth bottlenecks and maximize hardware utilization.</p>
</div>
<p><strong>Preview - How Memoization Combines with Future Optimizations:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>What It Does</p></th>
<th class="head"><p>Works with Memoization</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>15: Quantization</strong></p></td>
<td><p>Reduce precision to save memory</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KVCache</span> <span class="pre">with</span> <span class="pre">int8</span> <span class="pre">keys/values</span> <span class="pre">‚Üí</span> <span class="pre">4x</span> <span class="pre">memory</span> <span class="pre">reduction</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>17: Acceleration</strong></p></td>
<td><p>Optimize computation kernels</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Fused</span> <span class="pre">attention</span> <span class="pre">+</span> <span class="pre">KV</span> <span class="pre">cache</span> <span class="pre">‚Üí</span> <span class="pre">minimal</span> <span class="pre">memory</span> <span class="pre">traffic</span></code></p></td>
</tr>
<tr class="row-even"><td><p><strong>19: Benchmarking</strong></p></td>
<td><p>Measure end-to-end performance</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Profile</span> <span class="pre">cache</span> <span class="pre">hit</span> <span class="pre">rates</span> <span class="pre">and</span> <span class="pre">speedup</span> <span class="pre">gains</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/18_memoization/18_memoization.ipynb">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/18_memoization/18_memoization.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="17_acceleration_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 17: Acceleration</p>
      </div>
    </a>
    <a class="right-next"
       href="19_benchmarking_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Module 19: Benchmarking</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-memoization-after-acceleration">Why Memoization After Acceleration?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kvcache-constructor">KVCache Constructor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-methods">Core Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#helper-functions">Helper Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caching-computation">Caching Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kv-cache-in-transformers">KV Cache in Transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-checkpointing">Gradient Checkpointing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cache-invalidation">Cache Invalidation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-compute-trade-offs">Memory-Compute Trade-offs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cache-position-out-of-bounds">Cache Position Out of Bounds</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forgetting-to-advance-position">Forgetting to Advance Position</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatches">Shape Mismatches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cache-not-reset-between-sequences">Cache Not Reset Between Sequences</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-memoization-matters-at-scale">Why Memoization Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>