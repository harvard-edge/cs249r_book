{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd8f818",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 16: Compression - Pruning and Model Compression\n",
    "\n",
    "Welcome to Module 16! You're about to build model compression techniques that make neural networks smaller and more efficient while preserving their intelligence.\n",
    "\n",
    "## ğŸ”— Prerequisites & Progress\n",
    "**You've Built**: Complete optimization pipeline with profiling (14) and quantization (15)\n",
    "**You'll Build**: Pruning (magnitude & structured), knowledge distillation, and low-rank approximation\n",
    "**You'll Enable**: Compressed models that maintain accuracy while using dramatically less storage and memory\n",
    "\n",
    "**Connection Map**:\n",
    "```\n",
    "Profiling (14) â†’ Quantization (15) â†’ Compression (16) â†’ Acceleration (17) â†’ Memoization (18)\n",
    "(measure size)   (reduce precision)  (remove weights)   (speed up compute) (cache compute)\n",
    "```\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "1. Implement magnitude-based and structured pruning\n",
    "2. Build knowledge distillation for model compression\n",
    "3. Create low-rank approximations of weight matrices\n",
    "4. Measure compression ratios and sparsity levels\n",
    "5. Understand structured vs unstructured sparsity trade-offs\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## ğŸ“¦ Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/16_compression/compression_dev.py`\n",
    "**Building Side:** Code exports to `tinytorch.perf.compression`\n",
    "\n",
    "```python\n",
    "# How to use this module:\n",
    "from tinytorch.perf.compression import magnitude_prune, structured_prune, measure_sparsity\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Complete compression system in one focused module for deep understanding\n",
    "- **Production:** Proper organization like real compression libraries with all techniques together\n",
    "- **Consistency:** All compression operations and sparsity management in perf.compression\n",
    "- **Integration:** Works seamlessly with models and quantization for complete optimization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f542626",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp perf.compression\n",
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import time\n",
    "\n",
    "# Import from TinyTorch package (previous modules must be completed and exported)\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.layers import Linear, Sequential\n",
    "from tinytorch.core.activations import ReLU\n",
    "\n",
    "# Constants for memory calculations\n",
    "BYTES_PER_FLOAT32 = 4  # Standard float32 size in bytes\n",
    "MB_TO_BYTES = 1024 * 1024  # Megabytes to bytes conversion\n",
    "\n",
    "# Sequential provides model container with .layers and .parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b1a18",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### ğŸš¨ CRITICAL: Why No Sequential Container in TinyTorch\n",
    "\n",
    "**TinyTorch teaches ATOMIC COMPONENTS, not compositions!**\n",
    "\n",
    "**FORBIDDEN Pattern:**\n",
    "```python\n",
    "model = Sequential([Linear(10, 20), ReLU(), Linear(20, 10)])\n",
    "y = model(x)  # Student can't see what's happening!\n",
    "```\n",
    "\n",
    "**CORRECT Pattern:**\n",
    "```python\n",
    "# Explicit composition - students see every step\n",
    "layer1 = Linear(10, 20)\n",
    "activation = ReLU()\n",
    "layer2 = Linear(20, 10)\n",
    "\n",
    "# Forward pass - nothing hidden\n",
    "x = layer1.forward(input)\n",
    "x = activation.forward(x)\n",
    "output = layer2.forward(x)\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "- Students MUST see explicit forward passes to understand data flow\n",
    "- Hidden abstractions prevent learning\n",
    "- Sequential belongs in helper utilities, NOT core modules\n",
    "- Educational value comes from seeing layer interactions explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e3d8e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ’¡ Motivation: Why Compression Matters\n",
    "\n",
    "Before we learn compression, let's profile a model to analyze its weight\n",
    "distribution. We'll discover that many weights are tiny and might not matter much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile weight distribution to discover pruning opportunities\n",
    "# Module 14 (Profiling) must be completed before Module 16\n",
    "from tinytorch.perf.profiling import Profiler, analyze_weight_distribution\n",
    "\n",
    "def show_weight_distribution_motivation():\n",
    "    \"\"\"Display weight distribution analysis - motivates compression techniques.\"\"\"\n",
    "    profiler = Profiler()\n",
    "\n",
    "    # Create a model and analyze its weights\n",
    "    model = Linear(512, 512)\n",
    "    input_data = Tensor(np.random.randn(1, 512))\n",
    "\n",
    "    # Profile basic characteristics\n",
    "    profile = profiler.profile_forward_pass(model, input_data)\n",
    "\n",
    "    print(\"ğŸ”¬ Profiling Parameter Distribution:\\n\")\n",
    "    print(f\"   Total parameters: {profile['parameters']:,}\")\n",
    "    print(f\"   Model memory: {profile['parameters'] * BYTES_PER_FLOAT32 / MB_TO_BYTES:.1f} MB (FP32)\")\n",
    "\n",
    "    # Analyze weight distribution\n",
    "    weights = model.weight.data.flatten()\n",
    "    abs_weights = np.abs(weights)\n",
    "\n",
    "    print(\"\\n   Weight Statistics:\")\n",
    "    print(f\"   Mean: {np.mean(abs_weights):.4f}\")\n",
    "    print(f\"   Std:  {np.std(abs_weights):.4f}\")\n",
    "    print(f\"   Min:  {np.min(abs_weights):.4f}\")\n",
    "    print(f\"   Max:  {np.max(abs_weights):.4f}\")\n",
    "\n",
    "    # Check how many weights are small\n",
    "    thresholds = [0.001, 0.01, 0.1, 0.5]\n",
    "    print(\"\\n   Weights Below Threshold:\")\n",
    "    print(\"   Threshold  |  Percentage\")\n",
    "    print(\"   -----------|--------------\")\n",
    "    for threshold in thresholds:\n",
    "        percentage = np.sum(abs_weights < threshold) / len(weights) * 100\n",
    "        print(f\"   < {threshold:<6}  |  {percentage:5.1f}%\")\n",
    "\n",
    "    print(\"\\nğŸ’¡ Key Observations:\")\n",
    "    print(\"   â€¢ Many weights are very small (close to zero)\")\n",
    "    print(\"   â€¢ Weight distribution typically: mean â‰ˆ 0, concentrated near zero\")\n",
    "    print(\"   â€¢ Small weights contribute little to final predictions\")\n",
    "    print(\"   â€¢ Typical finding: 50-90% of weights can be removed!\")\n",
    "\n",
    "    print(\"\\nğŸ¯ The Problem:\")\n",
    "    print(\"   Why store and compute with weights that barely matter?\")\n",
    "    print(\"   â€¢ They take memory\")\n",
    "    print(\"   â€¢ They require computation\")\n",
    "    print(\"   â€¢ They slow down inference\")\n",
    "    print(\"   â€¢ But removing them has minimal accuracy impact!\")\n",
    "\n",
    "    print(\"\\nâœ¨ The Solution:\")\n",
    "    print(\"   Prune (remove) small weights:\")\n",
    "    print(\"   â€¢ Magnitude pruning: Set small weights to zero\")\n",
    "    print(\"   â€¢ Structured pruning: Remove entire neurons/channels\")\n",
    "    print(\"   â€¢ Typical: 80-90% sparsity with <1% accuracy loss\")\n",
    "    print(\"   â€¢ Benefit: Smaller models, faster inference, less memory\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    show_weight_distribution_motivation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211da90",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ’¡ Introduction: What is Model Compression?\n",
    "\n",
    "Imagine you have a massive library with millions of books, but you only reference 10% of them regularly. Model compression is like creating a curated collection that keeps the essential knowledge while dramatically reducing storage space.\n",
    "\n",
    "Model compression reduces the size and computational requirements of neural networks while preserving their intelligence. It's the bridge between powerful research models and practical deployment.\n",
    "\n",
    "### Why Compression Matters in ML Systems\n",
    "\n",
    "**The Storage Challenge:**\n",
    "- Modern language models: 100GB+ (GPT-3 scale)\n",
    "- Mobile devices: <1GB available for models\n",
    "- Edge devices: <100MB realistic limits\n",
    "- Network bandwidth: Slow downloads kill user experience\n",
    "\n",
    "**The Speed Challenge:**\n",
    "- Research models: Designed for accuracy, not efficiency\n",
    "- Production needs: Sub-second response times\n",
    "- Battery life: Energy consumption matters for mobile\n",
    "- Cost scaling: Inference costs grow with model size\n",
    "\n",
    "### The Compression Landscape\n",
    "\n",
    "```\n",
    "Neural Network Compression Techniques:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         COMPRESSION METHODS                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  WEIGHT-BASED                       â”‚  ARCHITECTURE-BASED             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚ Magnitude Pruning              â”‚ â”‚  â”‚ Knowledge Distillation     â”‚ â”‚\n",
    "â”‚  â”‚ â€¢ Remove small weights         â”‚ â”‚  â”‚ â€¢ Teacher â†’ Student        â”‚ â”‚\n",
    "â”‚  â”‚ â€¢ 90% sparsity achievable      â”‚ â”‚  â”‚ â€¢ 10x size reduction       â”‚ â”‚\n",
    "â”‚  â”‚                                â”‚ â”‚  â”‚                            â”‚ â”‚\n",
    "â”‚  â”‚ Structured Pruning             â”‚ â”‚  â”‚ Neural Architecture        â”‚ â”‚\n",
    "â”‚  â”‚ â€¢ Remove entire channels       â”‚ â”‚  â”‚ Search (NAS)               â”‚ â”‚\n",
    "â”‚  â”‚ â€¢ Hardware-friendly            â”‚ â”‚  â”‚ â€¢ Automated design         â”‚ â”‚\n",
    "â”‚  â”‚                                â”‚ â”‚  â”‚                            â”‚ â”‚\n",
    "â”‚  â”‚ Low-Rank Approximation         â”‚ â”‚  â”‚ Early Exit                 â”‚ â”‚\n",
    "â”‚  â”‚ â€¢ Matrix factorization         â”‚ â”‚  â”‚ â€¢ Adaptive compute         â”‚ â”‚\n",
    "â”‚  â”‚ â€¢ SVD decomposition            â”‚ â”‚  â”‚                            â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Think of compression like optimizing a recipe - you want to keep the essential ingredients that create the flavor while removing anything that doesn't contribute to the final dish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564fcf8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“ Foundations: Mathematical Background\n",
    "\n",
    "Understanding the mathematics behind compression helps us choose the right technique for each situation and predict their effects on model performance.\n",
    "\n",
    "### Magnitude-Based Pruning: The Simple Approach\n",
    "\n",
    "The core insight: small weights contribute little to the final prediction. Magnitude pruning removes weights based on their absolute values.\n",
    "\n",
    "```\n",
    "Mathematical Foundation:\n",
    "For weight w_ij in layer l:\n",
    "    If |w_ij| < threshold_l â†’ w_ij = 0\n",
    "\n",
    "Threshold Selection:\n",
    "- Global: One threshold for entire model\n",
    "- Layer-wise: Different threshold per layer\n",
    "- Percentile-based: Remove bottom k% of weights\n",
    "\n",
    "Sparsity Calculation:\n",
    "    Sparsity = (Zero weights / Total weights) Ã— 100%\n",
    "```\n",
    "\n",
    "### Structured Pruning: Hardware-Friendly Compression\n",
    "\n",
    "Unlike magnitude pruning which creates scattered zeros, structured pruning removes entire computational units (neurons, channels, attention heads).\n",
    "\n",
    "```\n",
    "Channel Importance Metrics:\n",
    "\n",
    "Method 1: L2 Norm\n",
    "    Importance(channel_i) = ||W[:,i]||â‚‚ = âˆš(Î£â±¼ WÂ²â±¼áµ¢)\n",
    "\n",
    "Method 2: Gradient-based\n",
    "    Importance(channel_i) = |âˆ‚Loss/âˆ‚W[:,i]|\n",
    "\n",
    "Method 3: Activation-based\n",
    "    Importance(channel_i) = E[|activations_i|]\n",
    "\n",
    "Pruning Decision:\n",
    "    Remove bottom k% of channels based on importance ranking\n",
    "```\n",
    "\n",
    "### Knowledge Distillation: Learning from Teachers\n",
    "\n",
    "Knowledge distillation transfers knowledge from a large \"teacher\" model to a smaller \"student\" model. The student learns not just the correct answers, but the teacher's reasoning process.\n",
    "\n",
    "```\n",
    "Distillation Loss Function:\n",
    "    L_total = Î± Ã— L_soft + (1-Î±) Ã— L_hard\n",
    "\n",
    "Where:\n",
    "    L_soft = KL_divergence(Ïƒ(z_s/T), Ïƒ(z_t/T))  # Soft targets\n",
    "    L_hard = CrossEntropy(Ïƒ(z_s), y_true)        # Hard targets\n",
    "\n",
    "    Ïƒ(z/T) = Softmax with temperature T\n",
    "    z_s = Student logits, z_t = Teacher logits\n",
    "    Î± = Balance parameter (typically 0.7)\n",
    "    T = Temperature parameter (typically 3-5)\n",
    "\n",
    "Temperature Effect:\n",
    "    T=1: Standard softmax (sharp probabilities)\n",
    "    T>1: Softer distributions (reveals teacher's uncertainty)\n",
    "```\n",
    "\n",
    "### Low-Rank Approximation: Matrix Compression\n",
    "\n",
    "Large weight matrices often have redundancy that can be captured with lower-rank approximations using Singular Value Decomposition (SVD).\n",
    "\n",
    "```\n",
    "SVD Decomposition:\n",
    "    W_{mÃ—n} = U_{mÃ—k} Ã— Î£_{kÃ—k} Ã— V^T_{kÃ—n}\n",
    "\n",
    "Parameter Reduction:\n",
    "    Original: m Ã— n parameters\n",
    "    Compressed: (m Ã— k) + k + (k Ã— n) = k(m + n + 1) parameters\n",
    "\n",
    "    Compression achieved when: k < mn/(m+n+1)\n",
    "\n",
    "Reconstruction Error:\n",
    "    ||W - W_approx||_F = âˆš(Î£áµ¢â‚Œâ‚–â‚Šâ‚Ê³ Ïƒáµ¢Â²)\n",
    "\n",
    "    Where Ïƒáµ¢ are singular values, r = rank(W)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a20d6",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Sparsity Measurement - Understanding Model Density\n",
    "\n",
    "Before we can compress models, we need to understand how dense they are. Sparsity measurement tells us what percentage of weights are zero (or effectively zero).\n",
    "\n",
    "### Understanding Sparsity\n",
    "\n",
    "Sparsity is like measuring how much of a parking lot is empty. A 90% sparse model means 90% of its weights are zero - only 10% of the \"parking spaces\" are occupied.\n",
    "\n",
    "```\n",
    "Sparsity Visualization:\n",
    "\n",
    "Dense Matrix (0% sparse):           Sparse Matrix (75% sparse):\n",
    "â”Œâ”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€â”    â”Œâ”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€â”\n",
    "â”‚ 2.1 1.3 0.8 1.9 2.4 1.1 0.7 â”‚    â”‚ 2.1 0.0 0.0 1.9 0.0 0.0 0.0 â”‚\n",
    "â”‚ 1.5 2.8 1.2 0.9 1.6 2.2 1.4 â”‚    â”‚ 0.0 2.8 0.0 0.0 0.0 2.2 0.0 â”‚\n",
    "â”‚ 0.6 1.7 2.5 1.1 0.8 1.3 2.0 â”‚    â”‚ 0.0 0.0 2.5 0.0 0.0 0.0 2.0 â”‚\n",
    "â”‚ 1.9 1.0 1.6 2.3 1.8 0.9 1.2 â”‚    â”‚ 1.9 0.0 0.0 2.3 0.0 0.0 0.0 â”‚\n",
    "â””â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€â”˜    â””â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€â”˜\n",
    "All weights active                   Only 7/28 weights active\n",
    "Storage: 28 values                   Storage: 7 values + indices\n",
    "```\n",
    "\n",
    "Why this matters: Sparsity directly relates to memory savings, but achieving speedup requires special sparse computation libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4562af",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "measure-sparsity",
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def measure_sparsity(model) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of zero weights in a model.\n",
    "\n",
    "    TODO: Count zero weights and total weights across all layers\n",
    "\n",
    "    APPROACH:\n",
    "    1. Iterate through all model parameters\n",
    "    2. Count zeros using np.sum(weights == 0)\n",
    "    3. Count total parameters\n",
    "    4. Return percentage: zeros / total * 100\n",
    "\n",
    "    Args:\n",
    "        model: Model with .parameters() method\n",
    "\n",
    "    Returns:\n",
    "        Sparsity percentage (0.0-100.0)\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> # Create test model with explicit composition\n",
    "    >>> layer1 = Linear(10, 5)\n",
    "    >>> layer2 = Linear(5, 2)\n",
    "    >>> model = Sequential(layer1, layer2)\n",
    "    >>> sparsity = measure_sparsity(model)\n",
    "    >>> print(f\"Model sparsity: {sparsity:.1f}%\")\n",
    "    Model sparsity: 0.0%  # Before pruning\n",
    "\n",
    "    HINT: Use np.sum() to count zeros efficiently\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        # Only count weight matrices (2D), not biases (1D)\n",
    "        # Biases are often initialized to zero, which would skew sparsity\n",
    "        if len(param.shape) > 1:\n",
    "            total_params += param.size\n",
    "            zero_params += np.sum(param.data == 0)\n",
    "\n",
    "    if total_params == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (zero_params / total_params) * 100.0\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381134b7",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-measure-sparsity",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_measure_sparsity():\n",
    "    \"\"\"ğŸ”¬ Test sparsity measurement functionality.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Measure Sparsity...\")\n",
    "\n",
    "    # Test with dense model - explicit composition shows structure\n",
    "    layer1 = Linear(4, 3)\n",
    "    layer2 = Linear(3, 2)\n",
    "    model = Sequential(layer1, layer2)  # Test helper for parameter collection\n",
    "\n",
    "    initial_sparsity = measure_sparsity(model)\n",
    "    assert initial_sparsity < 1.0, f\"Expected <1% sparsity (dense model), got {initial_sparsity}%\"\n",
    "\n",
    "    # Test with manually sparse model - students see which weights are zeroed\n",
    "    layer1.weight.data[0, 0] = 0  # Zero out specific weight\n",
    "    layer1.weight.data[1, 1] = 0  # Zero out another weight\n",
    "    sparse_sparsity = measure_sparsity(model)\n",
    "    assert sparse_sparsity > 0, f\"Expected >0% sparsity, got {sparse_sparsity}%\"\n",
    "\n",
    "    print(\"âœ… measure_sparsity works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_measure_sparsity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8429c1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Magnitude-Based Pruning - Removing Small Weights\n",
    "\n",
    "Magnitude pruning is the simplest and most intuitive compression technique. It's based on the observation that weights with small magnitudes contribute little to the model's output.\n",
    "\n",
    "### How Magnitude Pruning Works\n",
    "\n",
    "Think of magnitude pruning like editing a document - you remove words that don't significantly change the meaning. In neural networks, we remove weights that don't significantly affect predictions.\n",
    "\n",
    "```\n",
    "Magnitude Pruning Process:\n",
    "\n",
    "Step 1: Collect All Weights\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Layer 1: [2.1, 0.1, -1.8, 0.05, 3.2, -0.02]      â”‚\n",
    "â”‚ Layer 2: [1.5, -0.03, 2.8, 0.08, -2.1, 0.01]     â”‚\n",
    "â”‚ Layer 3: [0.7, 2.4, -0.06, 1.9, 0.04, -1.3]      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â†“\n",
    "Step 2: Calculate Magnitudes\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Magnitudes: [2.1, 0.1, 1.8, 0.05, 3.2, 0.02,     â”‚\n",
    "â”‚              1.5, 0.03, 2.8, 0.08, 2.1, 0.01,    â”‚\n",
    "â”‚              0.7, 2.4, 0.06, 1.9, 0.04, 1.3]     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â†“\n",
    "Step 3: Find Threshold (e.g., 70th percentile)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Sorted: [0.01, 0.02, 0.03, 0.04, 0.05, 0.06,     â”‚\n",
    "â”‚          0.08, 0.1, 0.7, 1.3, 1.5, 1.8,          â”‚ Threshold: 0.1\n",
    "â”‚          1.9, 2.1, 2.1, 2.4, 2.8, 3.2]           â”‚ (70% of weights removed)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â†“\n",
    "Step 4: Apply Pruning Mask\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Layer 1: [2.1, 0.0, -1.8, 0.0, 3.2, 0.0]         â”‚\n",
    "â”‚ Layer 2: [1.5, 0.0, 2.8, 0.0, -2.1, 0.0]         â”‚ 70% weights â†’ 0\n",
    "â”‚ Layer 3: [0.7, 2.4, 0.0, 1.9, 0.0, -1.3]         â”‚ 30% preserved\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Memory Impact:\n",
    "- Dense storage: 18 values\n",
    "- Sparse storage: 6 values + 6 indices = 12 values (33% savings)\n",
    "- Theoretical limit: 70% savings with perfect sparse format\n",
    "```\n",
    "\n",
    "### Why Global Thresholding Works\n",
    "\n",
    "Global thresholding treats the entire model as one big collection of weights, finding a single threshold that achieves the target sparsity across all layers.\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement and understand\n",
    "- Preserves overall model capacity\n",
    "- Works well for uniform network architectures\n",
    "\n",
    "**Disadvantages:**\n",
    "- May over-prune some layers, under-prune others\n",
    "- Doesn't account for layer-specific importance\n",
    "- Can hurt performance if layers have very different weight distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d0c12",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "magnitude-prune",
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def magnitude_prune(model, sparsity=0.9):\n",
    "    \"\"\"\n",
    "    Remove weights with smallest magnitudes to achieve target sparsity.\n",
    "\n",
    "    TODO: Implement global magnitude-based pruning\n",
    "\n",
    "    APPROACH:\n",
    "    1. Collect all weights from the model\n",
    "    2. Calculate absolute values to get magnitudes\n",
    "    3. Find threshold at desired sparsity percentile\n",
    "    4. Set weights below threshold to zero (in-place)\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> # Create model with explicit layer composition\n",
    "    >>> layer1 = Linear(100, 50)\n",
    "    >>> layer2 = Linear(50, 10)\n",
    "    >>> model = Sequential(layer1, layer2)\n",
    "    >>> original_params = sum(p.size for p in model.parameters())\n",
    "    >>> magnitude_prune(model, sparsity=0.8)\n",
    "    >>> final_sparsity = measure_sparsity(model)\n",
    "    >>> print(f\"Achieved {final_sparsity:.1f}% sparsity\")\n",
    "    Achieved 80.0% sparsity\n",
    "\n",
    "    HINTS:\n",
    "    - Use np.percentile() to find threshold\n",
    "    - Modify model parameters in-place\n",
    "    - Consider only weight matrices, not biases\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Collect all weights (excluding biases)\n",
    "    all_weights = []\n",
    "    weight_params = []\n",
    "\n",
    "    for param in model.parameters():\n",
    "        # Skip biases (typically 1D)\n",
    "        if len(param.shape) > 1:\n",
    "            all_weights.extend(param.data.flatten())\n",
    "            weight_params.append(param)\n",
    "\n",
    "    if not all_weights:\n",
    "        return model\n",
    "\n",
    "    # Calculate magnitude threshold\n",
    "    magnitudes = np.abs(all_weights)\n",
    "    threshold = np.percentile(magnitudes, sparsity * 100)\n",
    "\n",
    "    # Apply pruning to each weight parameter\n",
    "    for param in weight_params:\n",
    "        mask = np.abs(param.data) >= threshold\n",
    "        param.data = param.data * mask\n",
    "\n",
    "    return model\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f0e5d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-magnitude-prune",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_magnitude_prune():\n",
    "    \"\"\"ğŸ”¬ Test magnitude-based pruning functionality.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Magnitude Prune...\")\n",
    "\n",
    "    # Create test model with explicit composition - students see structure\n",
    "    layer1 = Linear(4, 3)\n",
    "    layer2 = Linear(3, 2)\n",
    "    model = Sequential(layer1, layer2)\n",
    "\n",
    "    # Set specific weight values for predictable testing\n",
    "    # Students can see exactly which weights we're testing\n",
    "    layer1.weight.data = np.array([\n",
    "        [1.0, 2.0, 3.0],    # Large weights - should survive pruning\n",
    "        [0.1, 0.2, 0.3],    # Medium weights\n",
    "        [4.0, 5.0, 6.0],    # Large weights - should survive pruning\n",
    "        [0.01, 0.02, 0.03]  # Tiny weights - will be pruned\n",
    "    ])\n",
    "\n",
    "    initial_sparsity = measure_sparsity(model)\n",
    "    assert initial_sparsity < 1.0, \"Model should start with minimal sparsity (<1%)\"\n",
    "\n",
    "    # Apply 50% pruning - removes smallest 50% of weights\n",
    "    magnitude_prune(model, sparsity=0.5)\n",
    "    final_sparsity = measure_sparsity(model)\n",
    "\n",
    "    # Should achieve approximately 50% sparsity\n",
    "    assert 40 <= final_sparsity <= 60, f\"Expected ~50% sparsity, got {final_sparsity}%\"\n",
    "\n",
    "    # Verify largest weights survived - students understand pruning criteria\n",
    "    remaining_weights = layer1.weight.data[layer1.weight.data != 0]\n",
    "    assert len(remaining_weights) > 0, \"Some weights should remain\"\n",
    "    assert np.all(np.abs(remaining_weights) >= 0.1), \"Large weights should survive\"\n",
    "\n",
    "    print(\"âœ… magnitude_prune works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_magnitude_prune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb669f6e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Structured Pruning - Hardware-Friendly Compression\n",
    "\n",
    "While magnitude pruning creates scattered zeros throughout the network, structured pruning removes entire computational units (channels, neurons, heads). This creates sparsity patterns that modern hardware can actually accelerate.\n",
    "\n",
    "### Why Structured Pruning Matters\n",
    "\n",
    "Think of the difference between removing random words from a paragraph versus removing entire sentences. Structured pruning removes entire \"sentences\" (channels) rather than random \"words\" (individual weights).\n",
    "\n",
    "```\n",
    "Unstructured vs Structured Sparsity:\n",
    "\n",
    "UNSTRUCTURED (Magnitude Pruning):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Channel 0: [2.1, 0.0, 1.8, 0.0, 3.2]        â”‚ â† Sparse weights\n",
    "â”‚ Channel 1: [0.0, 2.8, 0.0, 2.1, 0.0]        â”‚ â† Sparse weights\n",
    "â”‚ Channel 2: [1.5, 0.0, 2.4, 0.0, 1.9]        â”‚ â† Sparse weights\n",
    "â”‚ Channel 3: [0.0, 1.7, 0.0, 2.0, 0.0]        â”‚ â† Sparse weights\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Issues: Irregular memory access, no hardware speedup\n",
    "\n",
    "STRUCTURED (Channel Pruning):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Channel 0: [2.1, 1.3, 1.8, 0.9, 3.2]        â”‚ â† Fully preserved\n",
    "â”‚ Channel 1: [0.0, 0.0, 0.0, 0.0, 0.0]        â”‚ â† Fully removed\n",
    "â”‚ Channel 2: [1.5, 2.2, 2.4, 1.1, 1.9]        â”‚ â† Fully preserved\n",
    "â”‚ Channel 3: [0.0, 0.0, 0.0, 0.0, 0.0]        â”‚ â† Fully removed\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Benefits: Regular patterns, hardware acceleration possible\n",
    "```\n",
    "\n",
    "### Channel Importance Ranking\n",
    "\n",
    "How do we decide which channels to remove? We rank them by importance using various metrics:\n",
    "\n",
    "```\n",
    "Channel Importance Metrics:\n",
    "\n",
    "Method 1: L2 Norm (Most Common)\n",
    "    For each output channel i:\n",
    "    Importance_i = ||W[:, i]||_2 = âˆš(Î£â±¼ wÂ²â±¼áµ¢)\n",
    "\n",
    "    Intuition: Channels with larger weights have bigger impact\n",
    "\n",
    "Method 2: Activation-Based\n",
    "    Importance_i = E[|activation_i|] over dataset\n",
    "\n",
    "    Intuition: Channels that activate more are more important\n",
    "\n",
    "Method 3: Gradient-Based\n",
    "    Importance_i = |âˆ‚Loss/âˆ‚W[:, i]|\n",
    "\n",
    "    Intuition: Channels with larger gradients affect loss more\n",
    "\n",
    "Ranking Process:\n",
    "    1. Calculate importance for all channels\n",
    "    2. Sort channels by importance (ascending)\n",
    "    3. Remove bottom k% (least important)\n",
    "    4. Zero out entire channels, not individual weights\n",
    "```\n",
    "\n",
    "### Hardware Benefits of Structured Sparsity\n",
    "\n",
    "Structured sparsity enables real hardware acceleration because:\n",
    "\n",
    "1. **Memory Coalescing**: Accessing contiguous memory chunks is faster\n",
    "2. **SIMD Operations**: Can process multiple remaining channels in parallel\n",
    "3. **No Indexing Overhead**: Don't need to track locations of sparse weights\n",
    "4. **Cache Efficiency**: Better spatial locality of memory access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57c8af",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "structured-prune",
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def structured_prune(model, prune_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Remove entire channels/neurons based on L2 norm importance.\n",
    "\n",
    "    TODO: Implement structured pruning for Linear layers\n",
    "\n",
    "    APPROACH:\n",
    "    1. For each Linear layer, calculate L2 norm of each output channel\n",
    "    2. Rank channels by importance (L2 norm)\n",
    "    3. Remove lowest importance channels by setting to zero\n",
    "    4. This creates block sparsity that's hardware-friendly\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> # Create model with explicit layers\n",
    "    >>> layer1 = Linear(100, 50)\n",
    "    >>> layer2 = Linear(50, 10)\n",
    "    >>> model = Sequential(layer1, layer2)\n",
    "    >>> original_shape = layer1.weight.shape\n",
    "    >>> structured_prune(model, prune_ratio=0.3)\n",
    "    >>> # 30% of channels are now completely zero\n",
    "    >>> final_sparsity = measure_sparsity(model)\n",
    "    >>> print(f\"Structured sparsity: {final_sparsity:.1f}%\")\n",
    "    Structured sparsity: 30.0%\n",
    "\n",
    "    HINTS:\n",
    "    - Calculate L2 norm along input dimension for each output channel\n",
    "    - Use np.linalg.norm(weights[:, channel]) for channel importance\n",
    "    - Set entire channels to zero (not just individual weights)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # All Linear layers have .weight attribute\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Linear):\n",
    "            weight = layer.weight.data\n",
    "\n",
    "            # Calculate L2 norm for each output channel (column)\n",
    "            channel_norms = np.linalg.norm(weight, axis=0)\n",
    "\n",
    "            # Find channels to prune (lowest importance)\n",
    "            num_channels = weight.shape[1]\n",
    "            num_to_prune = int(num_channels * prune_ratio)\n",
    "\n",
    "            if num_to_prune > 0:\n",
    "                # Get indices of channels to prune (smallest norms)\n",
    "                prune_indices = np.argpartition(channel_norms, num_to_prune)[:num_to_prune]\n",
    "\n",
    "                # Zero out entire channels\n",
    "                weight[:, prune_indices] = 0\n",
    "\n",
    "                # Also zero corresponding bias elements if bias exists\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data[prune_indices] = 0\n",
    "\n",
    "    return model\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6381d82",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-structured-prune",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_structured_prune():\n",
    "    \"\"\"ğŸ”¬ Test structured pruning functionality.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Structured Prune...\")\n",
    "\n",
    "    # Create test model with explicit layers - students see the architecture\n",
    "    layer1 = Linear(4, 6)\n",
    "    layer2 = Linear(6, 2)\n",
    "    model = Sequential(layer1, layer2)\n",
    "\n",
    "    # Set predictable weights for testing\n",
    "    # Students can see channel importance: col 0,2,4 = large, col 1,3,5 = small\n",
    "    layer1.weight.data = np.array([\n",
    "        [1.0, 0.1, 2.0, 0.05, 3.0, 0.01],  # Channels with varying importance\n",
    "        [1.1, 0.11, 2.1, 0.06, 3.1, 0.02],  # Large values in columns 0,2,4\n",
    "        [1.2, 0.12, 2.2, 0.07, 3.2, 0.03],  # Small values in columns 1,3,5\n",
    "        [1.3, 0.13, 2.3, 0.08, 3.3, 0.04]   # Pruning removes small channels\n",
    "    ])\n",
    "\n",
    "    initial_sparsity = measure_sparsity(model)\n",
    "    assert initial_sparsity < 1.0, \"Model should start with minimal sparsity (<1%)\"\n",
    "\n",
    "    # Apply 33% structured pruning (2 out of 6 channels)\n",
    "    # This removes entire channels, not scattered weights\n",
    "    structured_prune(model, prune_ratio=0.33)\n",
    "    final_sparsity = measure_sparsity(model)\n",
    "\n",
    "    # Check that some channels are completely zero\n",
    "    weight = layer1.weight.data\n",
    "    zero_channels = np.sum(np.all(weight == 0, axis=0))\n",
    "    assert zero_channels >= 1, f\"Expected at least 1 zero channel, got {zero_channels}\"\n",
    "\n",
    "    # Check that non-zero channels are completely preserved\n",
    "    # This is structured pruning - entire channels are zero or non-zero\n",
    "    for col in range(weight.shape[1]):\n",
    "        channel = weight[:, col]\n",
    "        assert np.all(channel == 0) or np.all(channel != 0), \"Channels should be fully zero or fully non-zero\"\n",
    "\n",
    "    print(\"âœ… structured_prune works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_structured_prune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef52629",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Low-Rank Approximation - Matrix Compression Through Factorization\n",
    "\n",
    "Low-rank approximation discovers that large weight matrices often contain redundant information that can be captured with much smaller matrices through mathematical decomposition.\n",
    "\n",
    "### The Intuition Behind Low-Rank Approximation\n",
    "\n",
    "Imagine you're storing a massive spreadsheet where many columns are highly correlated. Instead of storing all columns separately, you could store a few \"basis\" columns and coefficients for how to combine them to recreate the original data.\n",
    "\n",
    "```\n",
    "Low-Rank Decomposition Visualization:\n",
    "\n",
    "Original Matrix W (large):           Factorized Form (smaller):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 2.1  1.3  0.8  1.9  2.4 â”‚         â”‚ 1.1  â”‚    â”‚ 1.9  1.2  0.7â”‚\n",
    "â”‚ 1.5  2.8  1.2  0.9  1.6 â”‚    â‰ˆ    â”‚ 2.4  â”‚ @  â”‚ 0.6  1.2  0.5â”‚\n",
    "â”‚ 0.6  1.7  2.5  1.1  0.8 â”‚         â”‚ 0.8  â”‚    â”‚ 1.4  2.1  0.9â”‚\n",
    "â”‚ 1.9  1.0  1.6  2.3  1.8 â”‚         â”‚ 1.6  â”‚    â”‚ 0.5  0.6  1.1â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    W (4Ã—5) = 20 params           U (4Ã—2)=8  +  V (2Ã—5)=10  = 18 params\n",
    "\n",
    "Parameter Reduction:\n",
    "- Original: 4 Ã— 5 = 20 parameters\n",
    "- Compressed: (4 Ã— 2) + (2 Ã— 5) = 18 parameters\n",
    "- Compression ratio: 18/20 = 0.9 (10% savings)\n",
    "\n",
    "For larger matrices, savings become dramatic:\n",
    "- W (1000Ã—1000): 1M parameters â†’ U (1000Ã—100) + V (100Ã—1000): 200K parameters\n",
    "- Compression ratio: 0.2 (80% savings)\n",
    "```\n",
    "\n",
    "### SVD: The Mathematical Foundation\n",
    "\n",
    "Singular Value Decomposition (SVD) finds the optimal low-rank approximation by identifying the most important \"directions\" in the data:\n",
    "\n",
    "```\n",
    "SVD Decomposition:\n",
    "    W = U Ã— Î£ Ã— V^T\n",
    "\n",
    "Where:\n",
    "    U: Left singular vectors (input patterns)\n",
    "    Î£: Singular values (importance weights)\n",
    "    V^T: Right singular vectors (output patterns)\n",
    "\n",
    "Truncated SVD (Rank-k approximation):\n",
    "    W â‰ˆ U[:,:k] Ã— Î£[:k] Ã— V^T[:k,:]\n",
    "\n",
    "Quality vs Compression Trade-off:\n",
    "    Higher k â†’ Better approximation, less compression\n",
    "    Lower k â†’ More compression, worse approximation\n",
    "\n",
    "Choosing Optimal Rank:\n",
    "    Method 1: Fixed ratio (k = ratio Ã— min(m,n))\n",
    "    Method 2: Energy threshold (keep 90% of singular value energy)\n",
    "    Method 3: Error threshold (reconstruction error < threshold)\n",
    "```\n",
    "\n",
    "### When Low-Rank Works Best\n",
    "\n",
    "Low-rank approximation works well when:\n",
    "- **Matrices are large**: Compression benefits scale with size\n",
    "- **Data has structure**: Correlated patterns enable compression\n",
    "- **Moderate accuracy loss acceptable**: Some precision traded for efficiency\n",
    "\n",
    "It works poorly when:\n",
    "- **Matrices are already small**: Overhead exceeds benefits\n",
    "- **Data is random**: No patterns to exploit\n",
    "- **High precision required**: SVD introduces approximation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc28de",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "low-rank-approx",
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def low_rank_approximate(weight_matrix, rank_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Approximate weight matrix using low-rank decomposition (SVD).\n",
    "\n",
    "    TODO: Implement SVD-based low-rank approximation\n",
    "\n",
    "    APPROACH:\n",
    "    1. Perform SVD: W = U @ S @ V^T\n",
    "    2. Keep only top k singular values where k = rank_ratio * min(dimensions)\n",
    "    3. Reconstruct: W_approx = U[:,:k] @ diag(S[:k]) @ V[:k,:]\n",
    "    4. Return decomposed matrices for memory savings\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> weight = np.random.randn(100, 50)\n",
    "    >>> U, S, V = low_rank_approximate(weight, rank_ratio=0.3)\n",
    "    >>> # Original: 100*50 = 5000 params\n",
    "    >>> # Compressed: 100*15 + 15*50 = 2250 params (55% reduction)\n",
    "\n",
    "    HINTS:\n",
    "    - Use np.linalg.svd() for decomposition\n",
    "    - Choose k = int(rank_ratio * min(m, n))\n",
    "    - Return U[:,:k], S[:k], V[:k,:] for reconstruction\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    m, n = weight_matrix.shape\n",
    "\n",
    "    # Perform SVD\n",
    "    U, S, V = np.linalg.svd(weight_matrix, full_matrices=False)\n",
    "\n",
    "    # Determine target rank\n",
    "    max_rank = min(m, n)\n",
    "    target_rank = max(1, int(rank_ratio * max_rank))\n",
    "\n",
    "    # Truncate to target rank\n",
    "    U_truncated = U[:, :target_rank]\n",
    "    S_truncated = S[:target_rank]\n",
    "    V_truncated = V[:target_rank, :]\n",
    "\n",
    "    return U_truncated, S_truncated, V_truncated\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662fd40",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-low-rank",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_low_rank_approximate():\n",
    "    \"\"\"ğŸ”¬ Test low-rank approximation functionality.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Low-Rank Approximate...\")\n",
    "\n",
    "    # Create test weight matrix\n",
    "    original_weight = np.random.randn(20, 15)\n",
    "    original_params = original_weight.size\n",
    "\n",
    "    # Apply low-rank approximation\n",
    "    U, S, V = low_rank_approximate(original_weight, rank_ratio=0.4)\n",
    "\n",
    "    # Check dimensions\n",
    "    target_rank = int(0.4 * min(20, 15))  # min(20,15) = 15, so 0.4*15 = 6\n",
    "    assert U.shape == (20, target_rank), f\"Expected U shape (20, {target_rank}), got {U.shape}\"\n",
    "    assert S.shape == (target_rank,), f\"Expected S shape ({target_rank},), got {S.shape}\"\n",
    "    assert V.shape == (target_rank, 15), f\"Expected V shape ({target_rank}, 15), got {V.shape}\"\n",
    "\n",
    "    # Check parameter reduction\n",
    "    compressed_params = U.size + S.size + V.size\n",
    "    compression_ratio = compressed_params / original_params\n",
    "    assert compression_ratio < 1.0, f\"Should compress, but ratio is {compression_ratio}\"\n",
    "\n",
    "    # Check reconstruction quality\n",
    "    reconstructed = U @ np.diag(S) @ V\n",
    "    reconstruction_error = np.linalg.norm(original_weight - reconstructed)\n",
    "    relative_error = reconstruction_error / np.linalg.norm(original_weight)\n",
    "    # Low-rank approximation trades accuracy for compression - error is expected\n",
    "    assert relative_error < 0.7, f\"Reconstruction error too high: {relative_error}\"\n",
    "\n",
    "    print(\"âœ… low_rank_approximate works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_low_rank_approximate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a04ec",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ—ï¸ Knowledge Distillation - Learning from Teacher Models\n",
    "\n",
    "Knowledge distillation is like having an expert teacher simplify complex concepts for a student. The large \"teacher\" model shares its knowledge with a smaller \"student\" model, achieving similar performance with far fewer parameters.\n",
    "\n",
    "### The Teacher-Student Learning Process\n",
    "\n",
    "Unlike traditional training where models learn from hard labels (cat/dog), knowledge distillation uses \"soft\" targets that contain richer information about the teacher's decision-making process.\n",
    "\n",
    "```\n",
    "Knowledge Distillation Process:\n",
    "\n",
    "                    TEACHER MODEL (Large)\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "Input Data â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ 100M parameters     â”‚\n",
    "                    â”‚ 95% accuracy        â”‚\n",
    "                    â”‚ 500ms inference     â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â†“ Soft Targets\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Logits: [2.1, 0.3, â”‚\n",
    "                    â”‚           0.8, 4.2] â”‚ â† Rich information\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "                             â†“ Distillation Loss\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "Input Data â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ STUDENT MODEL       â”‚\n",
    "Hard Labels â”€â”€â”€â”€â”€â”€â”€â†’â”‚ 10M parameters      â”‚ â† 10x smaller\n",
    "                    â”‚ 93% accuracy        â”‚ â† 2% loss\n",
    "                    â”‚ 50ms inference      â”‚ â† 10x faster\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Benefits:\n",
    "â€¢ Size: 10x smaller models\n",
    "â€¢ Speed: 10x faster inference\n",
    "â€¢ Accuracy: Only 2-5% degradation\n",
    "â€¢ Knowledge transfer: Student learns teacher's \"reasoning\"\n",
    "```\n",
    "\n",
    "### Temperature Scaling: Softening Decisions\n",
    "\n",
    "Temperature scaling is a key innovation that makes knowledge distillation effective. It \"softens\" the teacher's confidence, revealing uncertainty that helps the student learn.\n",
    "\n",
    "```\n",
    "Temperature Effect on Probability Distributions:\n",
    "\n",
    "Without Temperature (T=1):           With Temperature (T=3):\n",
    "Teacher Logits: [1.0, 2.0, 0.5]    Teacher Logits: [1.0, 2.0, 0.5]\n",
    "                       â†“                               â†“ Ã· 3\n",
    "Softmax: [0.09, 0.67, 0.24]         Logits/T: [0.33, 0.67, 0.17]\n",
    "         ^      ^      ^                       â†“\n",
    "      Low   High   Med              Softmax: [0.21, 0.42, 0.17]\n",
    "                                             ^      ^      ^\n",
    "Sharp decisions (hard to learn)           Soft   decisions (easier to learn)\n",
    "\n",
    "Why Soft Targets Help:\n",
    "1. Reveal teacher's uncertainty about similar classes\n",
    "2. Provide richer gradients for student learning\n",
    "3. Transfer knowledge about class relationships\n",
    "4. Reduce overfitting to hard labels\n",
    "```\n",
    "\n",
    "### Loss Function Design\n",
    "\n",
    "The distillation loss balances learning from both the teacher's soft knowledge and the ground truth hard labels:\n",
    "\n",
    "```\n",
    "Combined Loss Function:\n",
    "\n",
    "L_total = Î± Ã— L_soft + (1-Î±) Ã— L_hard\n",
    "\n",
    "Where:\n",
    "    L_soft = KL_divergence(Student_soft, Teacher_soft)\n",
    "             â”‚\n",
    "             â””â”€ Measures how well student mimics teacher\n",
    "\n",
    "    L_hard = CrossEntropy(Student_predictions, True_labels)\n",
    "             â”‚\n",
    "             â””â”€ Ensures student still learns correct answers\n",
    "\n",
    "Balance Parameter Î±:\n",
    "â€¢ Î± = 0.7: Focus mainly on teacher (typical)\n",
    "â€¢ Î± = 0.9: Almost pure distillation\n",
    "â€¢ Î± = 0.3: Balance teacher and ground truth\n",
    "â€¢ Î± = 0.0: Ignore teacher (regular training)\n",
    "\n",
    "Temperature T:\n",
    "â€¢ T = 1: No softening (standard softmax)\n",
    "â€¢ T = 3-5: Good balance (typical range)\n",
    "â€¢ T = 10+: Very soft (may lose information)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345c788",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "distillation",
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class KnowledgeDistillation:\n",
    "    \"\"\"\n",
    "    Knowledge distillation for model compression.\n",
    "\n",
    "    Train a smaller student model to mimic a larger teacher model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, teacher_model, student_model, temperature=3.0, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Initialize knowledge distillation.\n",
    "\n",
    "        TODO: Set up teacher and student models with distillation parameters\n",
    "\n",
    "        APPROACH:\n",
    "        1. Store teacher and student models\n",
    "        2. Set temperature for softening probability distributions\n",
    "        3. Set alpha for balancing hard vs soft targets\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> # Create teacher with more capacity (explicit layers)\n",
    "        >>> teacher_l1 = Linear(100, 200)\n",
    "        >>> teacher_l2 = Linear(200, 50)\n",
    "        >>> teacher = Sequential(teacher_l1, teacher_l2)\n",
    "        >>>\n",
    "        >>> # Create smaller student (explicit layer)\n",
    "        >>> student = Sequential(Linear(100, 50))\n",
    "        >>>\n",
    "        >>> kd = KnowledgeDistillation(teacher, student, temperature=4.0, alpha=0.8)\n",
    "        >>> print(f\"Temperature: {kd.temperature}, Alpha: {kd.alpha}\")\n",
    "        Temperature: 4.0, Alpha: 0.8\n",
    "\n",
    "        HINTS:\n",
    "        - Simply assign the parameters to instance variables\n",
    "        - Temperature typically ranges from 3-5 for effective softening\n",
    "        - Alpha of 0.7 means 70% soft targets, 30% hard targets\n",
    "\n",
    "        Args:\n",
    "            teacher_model: Large, pre-trained model\n",
    "            student_model: Smaller model to train\n",
    "            temperature: Softening parameter for distributions\n",
    "            alpha: Weight for soft target loss (1-alpha for hard targets)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.teacher_model = teacher_model\n",
    "        self.student_model = student_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def distillation_loss(self, student_logits, teacher_logits, true_labels):\n",
    "        \"\"\"\n",
    "        Calculate combined distillation loss.\n",
    "\n",
    "        TODO: Implement knowledge distillation loss function\n",
    "\n",
    "        APPROACH:\n",
    "        1. Calculate hard target loss (student vs true labels)\n",
    "        2. Calculate soft target loss (student vs teacher, with temperature)\n",
    "        3. Combine losses: alpha * soft_loss + (1-alpha) * hard_loss\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> kd = KnowledgeDistillation(teacher, student)\n",
    "        >>> loss = kd.distillation_loss(student_out, teacher_out, labels)\n",
    "        >>> print(f\"Distillation loss: {loss:.4f}\")\n",
    "\n",
    "        HINTS:\n",
    "        - Use temperature to soften distributions: logits/temperature\n",
    "        - Soft targets use KL divergence or cross-entropy\n",
    "        - Hard targets use standard classification loss\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Extract numpy arrays from Tensors\n",
    "        # student_logits and teacher_logits are always Tensors from forward passes\n",
    "        student_logits = student_logits.data\n",
    "        teacher_logits = teacher_logits.data\n",
    "\n",
    "        # true_labels might be numpy array or Tensor\n",
    "        if isinstance(true_labels, Tensor):\n",
    "            true_labels = true_labels.data\n",
    "\n",
    "        # Soften distributions with temperature\n",
    "        student_soft = self._softmax(student_logits / self.temperature)\n",
    "        teacher_soft = self._softmax(teacher_logits / self.temperature)\n",
    "\n",
    "        # Soft target loss (KL divergence)\n",
    "        soft_loss = self._kl_divergence(student_soft, teacher_soft)\n",
    "\n",
    "        # Hard target loss (cross-entropy)\n",
    "        student_hard = self._softmax(student_logits)\n",
    "        hard_loss = self._cross_entropy(student_hard, true_labels)\n",
    "\n",
    "        # Combined loss\n",
    "        total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss\n",
    "\n",
    "        return total_loss\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def _softmax(self, logits):\n",
    "        \"\"\"Compute softmax with numerical stability.\"\"\"\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
    "        return exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "\n",
    "    def _kl_divergence(self, p, q):\n",
    "        \"\"\"Compute KL divergence between distributions.\"\"\"\n",
    "        return np.sum(p * np.log(p / (q + 1e-8) + 1e-8))\n",
    "\n",
    "    def _cross_entropy(self, predictions, labels):\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        # Simple implementation for integer labels\n",
    "        if labels.ndim == 1:\n",
    "            return -np.mean(np.log(predictions[np.arange(len(labels)), labels] + 1e-8))\n",
    "        else:\n",
    "            return -np.mean(np.sum(labels * np.log(predictions + 1e-8), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5511245",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-distillation",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_knowledge_distillation():\n",
    "    \"\"\"ğŸ”¬ Test knowledge distillation functionality.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Knowledge Distillation...\")\n",
    "\n",
    "    # Create teacher model with more capacity - explicit composition\n",
    "    teacher_l1 = Linear(10, 20)\n",
    "    teacher_l2 = Linear(20, 5)\n",
    "    teacher = Sequential(teacher_l1, teacher_l2)\n",
    "\n",
    "    # Create smaller student model - explicit composition shows size difference\n",
    "    student_l1 = Linear(10, 5)\n",
    "    student = Sequential(student_l1)  # Direct connection, no hidden layer\n",
    "\n",
    "    # Initialize knowledge distillation with temperature scaling\n",
    "    kd = KnowledgeDistillation(teacher, student, temperature=3.0, alpha=0.7)\n",
    "\n",
    "    # Create dummy data for testing\n",
    "    input_data = Tensor(np.random.randn(8, 10))  # Batch of 8 samples\n",
    "    true_labels = np.array([0, 1, 2, 3, 4, 0, 1, 2])  # Class labels\n",
    "\n",
    "    # Forward passes - students see explicit data flow through each model\n",
    "    teacher_output = teacher.forward(input_data)  # Large model predictions\n",
    "    student_output = student.forward(input_data)  # Small model predictions\n",
    "\n",
    "    # Calculate distillation loss - combines soft and hard targets\n",
    "    loss = kd.distillation_loss(student_output, teacher_output, true_labels)\n",
    "\n",
    "    # Verify loss is reasonable\n",
    "    assert isinstance(loss, (float, np.floating)), f\"Loss should be float, got {type(loss)}\"\n",
    "    assert loss > 0, f\"Loss should be positive, got {loss}\"\n",
    "    assert not np.isnan(loss), \"Loss should not be NaN\"\n",
    "\n",
    "    print(\"âœ… knowledge_distillation works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_knowledge_distillation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777359d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ”§ Integration: Complete Compression Pipeline\n",
    "\n",
    "Now let's combine all our compression techniques into a unified system that can apply multiple methods and track their cumulative effects.\n",
    "\n",
    "### Compression Strategy Design\n",
    "\n",
    "Real-world compression often combines multiple techniques in sequence, each targeting different types of redundancy:\n",
    "\n",
    "```\n",
    "Multi-Stage Compression Pipeline:\n",
    "\n",
    "Original Model (100MB, 100% accuracy)\n",
    "         â”‚\n",
    "         â†“ Stage 1: Magnitude Pruning (remove 80% of small weights)\n",
    "Sparse Model (20MB, 98% accuracy)\n",
    "         â”‚\n",
    "         â†“ Stage 2: Structured Pruning (remove 30% of channels)\n",
    "Compact Model (14MB, 96% accuracy)\n",
    "         â”‚\n",
    "         â†“ Stage 3: Low-Rank Approximation (compress large layers)\n",
    "Factorized Model (10MB, 95% accuracy)\n",
    "         â”‚\n",
    "         â†“ Stage 4: Knowledge Distillation (train smaller architecture)\n",
    "Student Model (5MB, 93% accuracy)\n",
    "\n",
    "Final Result: 20x size reduction, 7% accuracy loss\n",
    "```\n",
    "\n",
    "### Compression Configuration\n",
    "\n",
    "Different deployment scenarios require different compression strategies:\n",
    "\n",
    "```\n",
    "Deployment Scenarios and Strategies:\n",
    "\n",
    "MOBILE APP (Aggressive compression needed):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Target: <10MB, <100ms inference         â”‚\n",
    "â”‚ Strategy:                               â”‚\n",
    "â”‚ â€¢ Magnitude pruning: 95% sparsity       â”‚\n",
    "â”‚ â€¢ Structured pruning: 50% channels      â”‚\n",
    "â”‚ â€¢ Knowledge distillation: 10x reduction â”‚\n",
    "â”‚ â€¢ Quantization: 8-bit weights           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "EDGE DEVICE (Balanced compression):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Target: <50MB, <200ms inference         â”‚\n",
    "â”‚ Strategy:                               â”‚\n",
    "â”‚ â€¢ Magnitude pruning: 80% sparsity       â”‚\n",
    "â”‚ â€¢ Structured pruning: 30% channels      â”‚\n",
    "â”‚ â€¢ Low-rank: 50% rank reduction          â”‚\n",
    "â”‚ â€¢ Quantization: 16-bit weights          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "CLOUD SERVICE (Minimal compression):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Target: Maintain accuracy, reduce cost  â”‚\n",
    "â”‚ Strategy:                               â”‚\n",
    "â”‚ â€¢ Magnitude pruning: 50% sparsity       â”‚\n",
    "â”‚ â€¢ Structured pruning: 10% channels      â”‚\n",
    "â”‚ â€¢ Dynamic batching optimization         â”‚\n",
    "â”‚ â€¢ Mixed precision inference             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb8ffa",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "compress-model-comprehensive",
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compress_model(model, compression_config):\n",
    "    \"\"\"\n",
    "    Apply comprehensive model compression based on configuration.\n",
    "\n",
    "    TODO: Implement complete compression pipeline\n",
    "\n",
    "    APPROACH:\n",
    "    1. Apply magnitude pruning if specified\n",
    "    2. Apply structured pruning if specified\n",
    "    3. Apply low-rank approximation if specified\n",
    "    4. Return compression statistics\n",
    "\n",
    "    EXAMPLE:\n",
    "    >>> config = {\n",
    "    ...     'magnitude_prune': 0.8,\n",
    "    ...     'structured_prune': 0.3,\n",
    "    ...     'low_rank': 0.5\n",
    "    ... }\n",
    "    >>> stats = compress_model(model, config)\n",
    "    >>> print(f\"Final sparsity: {stats['sparsity']:.1f}%\")\n",
    "    Final sparsity: 85.0%\n",
    "\n",
    "    HINT: Apply techniques sequentially and measure results\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    original_params = sum(p.size for p in model.parameters())\n",
    "    original_sparsity = measure_sparsity(model)\n",
    "\n",
    "    stats = {\n",
    "        'original_params': original_params,\n",
    "        'original_sparsity': original_sparsity,\n",
    "        'applied_techniques': []\n",
    "    }\n",
    "\n",
    "    # Apply magnitude pruning\n",
    "    if 'magnitude_prune' in compression_config:\n",
    "        sparsity = compression_config['magnitude_prune']\n",
    "        magnitude_prune(model, sparsity=sparsity)\n",
    "        stats['applied_techniques'].append(f'magnitude_prune_{sparsity}')\n",
    "\n",
    "    # Apply structured pruning\n",
    "    if 'structured_prune' in compression_config:\n",
    "        ratio = compression_config['structured_prune']\n",
    "        structured_prune(model, prune_ratio=ratio)\n",
    "        stats['applied_techniques'].append(f'structured_prune_{ratio}')\n",
    "\n",
    "    # Apply low-rank approximation (conceptually - would need architecture changes)\n",
    "    if 'low_rank' in compression_config:\n",
    "        ratio = compression_config['low_rank']\n",
    "        # For demo, we'll just record that it would be applied\n",
    "        stats['applied_techniques'].append(f'low_rank_{ratio}')\n",
    "\n",
    "    # Final measurements\n",
    "    final_sparsity = measure_sparsity(model)\n",
    "    stats['final_sparsity'] = final_sparsity\n",
    "    stats['sparsity_increase'] = final_sparsity - original_sparsity\n",
    "\n",
    "    return stats\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e506f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-compression-integration",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_compress_model():\n",
    "    \"\"\"ğŸ”¬ Test comprehensive model compression.\"\"\"\n",
    "    print(\"ğŸ”¬ Unit Test: Compress Model...\")\n",
    "\n",
    "    # Create test model with explicit layers - students see the full architecture\n",
    "    layer1 = Linear(20, 15)\n",
    "    layer2 = Linear(15, 10)\n",
    "    layer3 = Linear(10, 5)\n",
    "    model = Sequential(layer1, layer2, layer3)\n",
    "\n",
    "    # Define compression configuration\n",
    "    # Students understand what each technique does\n",
    "    config = {\n",
    "        'magnitude_prune': 0.7,    # Remove 70% of smallest weights\n",
    "        'structured_prune': 0.2     # Remove 20% of least important channels\n",
    "    }\n",
    "\n",
    "    # Apply compression pipeline - multiple techniques sequentially\n",
    "    stats = compress_model(model, config)\n",
    "\n",
    "    # Verify statistics - students understand what was measured\n",
    "    assert 'original_params' in stats, \"Should track original parameter count\"\n",
    "    assert 'final_sparsity' in stats, \"Should track final sparsity\"\n",
    "    assert 'applied_techniques' in stats, \"Should track applied techniques\"\n",
    "\n",
    "    # Verify compression was applied successfully\n",
    "    assert stats['final_sparsity'] > stats['original_sparsity'], \"Sparsity should increase\"\n",
    "    assert len(stats['applied_techniques']) == 2, \"Should apply both techniques\"\n",
    "\n",
    "    # Verify model still has reasonable structure after compression\n",
    "    remaining_params = sum(np.count_nonzero(p.data) for p in model.parameters())\n",
    "    assert remaining_params > 0, \"Model should retain some parameters\"\n",
    "\n",
    "    print(\"âœ… compress_model works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_compress_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f44ce",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“Š Systems Analysis - Compression Techniques\n",
    "\n",
    "Understanding the real-world effectiveness of different compression techniques through systematic measurement and comparison.\n",
    "\n",
    "### Accuracy vs Compression Trade-offs\n",
    "\n",
    "The fundamental challenge in model compression is balancing three competing objectives: model size, inference speed, and prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada675ae",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ“Š Measuring Compression Impact with Profiler\n",
    "\n",
    "Now let's use the **Profiler** tool from Module 14 to measure the actual parameter reduction from pruning. This demonstrates the complete workflow: profile baseline (M14) â†’ apply compression (M16) â†’ measure impact (M14+M16).\n",
    "\n",
    "This is the production workflow: measure â†’ prune â†’ validate â†’ deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf33c76",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "demo-profiler-compression",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Import Profiler from Module 14 (already imported above)\n",
    "\n",
    "def demo_compression_with_profiler():\n",
    "    \"\"\"ğŸ“Š Demonstrate parameter reduction using Profiler from Module 14.\"\"\"\n",
    "    print(\"ğŸ“Š Measuring Compression Impact with Profiler\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    profiler = Profiler()\n",
    "\n",
    "    # Create a simple model (Linear already imported above)\n",
    "    model = Linear(512, 256)\n",
    "    model.name = \"baseline_model\"\n",
    "\n",
    "    print(\"\\nğŸ‹ï¸  BEFORE: Dense Model\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Measure baseline\n",
    "    param_count_before = profiler.count_parameters(model)\n",
    "    sparsity_before = measure_sparsity(model)\n",
    "    input_shape = (32, 512)\n",
    "    memory_before = profiler.measure_memory(model, input_shape)\n",
    "\n",
    "    print(f\"   Parameters: {param_count_before:,}\")\n",
    "    print(f\"   Sparsity: {sparsity_before*100:.1f}% (zeros)\")\n",
    "    print(f\"   Memory: {memory_before['parameter_memory_mb']:.2f} MB\")\n",
    "    print(f\"   Active parameters: {int(param_count_before * (1 - sparsity_before)):,}\")\n",
    "\n",
    "    # Apply magnitude pruning\n",
    "    target_sparsity = 0.7  # Remove 70% of parameters\n",
    "    print(f\"\\nâœ‚ï¸  Applying {target_sparsity*100:.0f}% Magnitude Pruning...\")\n",
    "    pruned_model = magnitude_prune(model, sparsity=target_sparsity)\n",
    "    pruned_model.name = \"pruned_model\"\n",
    "\n",
    "    print(\"\\nğŸª¶ AFTER: Pruned Model\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Measure after pruning\n",
    "    param_count_after = profiler.count_parameters(pruned_model)\n",
    "    sparsity_after = measure_sparsity(pruned_model)\n",
    "    memory_after = profiler.measure_memory(pruned_model, input_shape)\n",
    "\n",
    "    print(f\"   Parameters: {param_count_after:,} (same, but many are zero)\")\n",
    "    print(f\"   Sparsity: {sparsity_after*100:.1f}% (zeros)\")\n",
    "    print(f\"   Memory: {memory_after['parameter_memory_mb']:.2f} MB (same storage)\")\n",
    "    print(f\"   Active parameters: {int(param_count_after * (1 - sparsity_after)):,}\")\n",
    "\n",
    "    print(\"\\nğŸ“ˆ COMPRESSION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    sparsity_gain = (sparsity_after - sparsity_before) * 100\n",
    "    active_before = int(param_count_before * (1 - sparsity_before))\n",
    "    active_after = int(param_count_after * (1 - sparsity_after))\n",
    "    reduction_ratio = active_before / active_after if active_after > 0 else 1\n",
    "    params_removed = active_before - active_after\n",
    "\n",
    "    print(f\"   Sparsity increased: {sparsity_before*100:.1f}% â†’ {sparsity_after*100:.1f}%\")\n",
    "    print(f\"   Active params reduced: {active_before:,} â†’ {active_after:,}\")\n",
    "    print(f\"   Parameters removed: {params_removed:,} ({sparsity_gain:.1f}% of total)\")\n",
    "    print(f\"   Compression ratio: {reduction_ratio:.1f}x fewer active parameters\")\n",
    "\n",
    "    print(\"\\nğŸ’¡ Key Insight:\")\n",
    "    print(f\"   Magnitude pruning removes {sparsity_gain:.0f}% of parameters\")\n",
    "    print(f\"   With sparse storage formats, this means {reduction_ratio:.1f}x less memory!\")\n",
    "    print(f\"   Critical for: edge devices, mobile apps, energy efficiency\")\n",
    "    print(\"\\nâœ… This is the power of compression: remove what doesn't matter!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_compression_with_profiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d966a19",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ“Š Advanced Systems Analysis - Compression Techniques\n",
    "\n",
    "Understanding the real-world effectiveness of different compression techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b21b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_compression_techniques():\n",
    "    \"\"\"ğŸ“Š Compare compression ratios across different techniques.\"\"\"\n",
    "    print(\"ğŸ“Š Analyzing Compression Techniques\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create baseline model (Linear already imported above)\n",
    "    model_configs = [\n",
    "        (\"Small MLP\", [Linear(128, 64), Linear(64, 32)]),\n",
    "        (\"Medium MLP\", [Linear(512, 256), Linear(256, 128)]),\n",
    "        (\"Large MLP\", [Linear(1024, 512), Linear(512, 256)])\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n{'Model':<15} {'Technique':<20} {'Sparsity':<12} {'Compression':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for model_name, layers in model_configs:\n",
    "        # Create model with explicit composition\n",
    "        model = Sequential(*layers)\n",
    "        baseline_params = sum(p.size for p in model.parameters())\n",
    "\n",
    "        # Test magnitude pruning on copy of model\n",
    "        # Create fresh layers for magnitude pruning test\n",
    "        mag_layers = [Linear(l.weight.shape[0], l.weight.shape[1]) for l in layers]\n",
    "        for i, layer in enumerate(mag_layers):\n",
    "            layer.weight = layers[i].weight\n",
    "            # Linear layers always have bias (may be None)\n",
    "            layer.bias = layers[i].bias\n",
    "        mag_model = Sequential(*mag_layers)\n",
    "        magnitude_prune(mag_model, sparsity=0.8)\n",
    "        mag_sparsity = measure_sparsity(mag_model)\n",
    "        mag_ratio = 1.0 / (1.0 - mag_sparsity / 100) if mag_sparsity < 100 else float('inf')\n",
    "\n",
    "        print(f\"{model_name:<15} {'Magnitude (80%)':<20} {mag_sparsity:>10.1f}% {mag_ratio:>10.1f}x\")\n",
    "\n",
    "        # Test structured pruning on separate copy\n",
    "        # Create fresh layers for structured pruning test\n",
    "        struct_layers = [Linear(l.weight.shape[0], l.weight.shape[1]) for l in layers]\n",
    "        for i, layer in enumerate(struct_layers):\n",
    "            layer.weight = layers[i].weight\n",
    "            # Linear layers always have bias (may be None)\n",
    "            layer.bias = layers[i].bias\n",
    "        struct_model = Sequential(*struct_layers)\n",
    "        structured_prune(struct_model, prune_ratio=0.5)\n",
    "        struct_sparsity = measure_sparsity(struct_model)\n",
    "        struct_ratio = 1.0 / (1.0 - struct_sparsity / 100) if struct_sparsity < 100 else float('inf')\n",
    "\n",
    "        print(f\"{'':<15} {'Structured (50%)':<20} {struct_sparsity:>10.1f}% {struct_ratio:>10.1f}x\")\n",
    "        print()\n",
    "\n",
    "    print(\"ğŸ’¡ Key Insights:\")\n",
    "    print(\"   â€¢ Magnitude pruning achieves higher sparsity (80%+)\")\n",
    "    print(\"   â€¢ Structured pruning creates hardware-friendly patterns\")\n",
    "    print(\"   â€¢ Larger models compress more effectively\")\n",
    "    print(\"   â€¢ Compression ratio = 1 / (1 - sparsity)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_compression_techniques()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7be38c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Knowledge Distillation Analysis\n",
    "\n",
    "Now let's analyze how knowledge distillation compares to other compression techniques for different compression ratios and accuracy preservation goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distillation_effectiveness():\n",
    "    \"\"\"ğŸ“Š Analyze knowledge distillation compression and accuracy trade-offs.\"\"\"\n",
    "    print(\"\\nğŸ“Š Analyzing Knowledge Distillation Effectiveness\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Simulate teacher-student scenarios\n",
    "    scenarios = [\n",
    "        (\"Largeâ†’Small\", 100_000, 10_000, 0.95, 0.90, 10.0),\n",
    "        (\"Mediumâ†’Tiny\", 50_000, 5_000, 0.92, 0.87, 10.0),\n",
    "        (\"Smallâ†’Micro\", 10_000, 1_000, 0.88, 0.83, 10.0),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n{'Scenario':<15} {'Teacher':<12} {'Student':<12} {'Ratio':<10} {'Acc Loss':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for name, teacher_params, student_params, teacher_acc, student_acc, compression in scenarios:\n",
    "        acc_retention = (student_acc / teacher_acc) * 100\n",
    "        acc_loss = teacher_acc - student_acc\n",
    "\n",
    "        print(f\"{name:<15} {teacher_params:>10,}p {student_params:>10,}p {compression:>8.1f}x {acc_loss*100:>8.1f}%\")\n",
    "\n",
    "    print(\"\\nğŸ’¡ Knowledge Distillation Insights:\")\n",
    "    print(\"   â€¢ Achieves 10x+ compression with 5-10% accuracy loss\")\n",
    "    print(\"   â€¢ Student learns teacher's 'soft' predictions\")\n",
    "    print(\"   â€¢ More effective than naive pruning for large reductions\")\n",
    "    print(\"   â€¢ Requires retraining (unlike pruning/quantization)\")\n",
    "    print(\"\\nğŸš€ Best Use Case:\")\n",
    "    print(\"   Deploy small student models on edge devices\")\n",
    "    print(\"   Train expensive teacher once, distill many students\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_distillation_effectiveness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447aedd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bfaf012",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ”§ Consolidated Compression Classes for Export\n",
    "\n",
    "Now that we've implemented all compression techniques, let's create a consolidated class\n",
    "for export to the tinytorch package. This allows milestones to use the complete compression system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647ae51",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "compression_export",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Compressor:\n",
    "    \"\"\"\n",
    "    Complete compression system for milestone use.\n",
    "\n",
    "    Provides pruning, distillation, and low-rank approximation techniques.\n",
    "\n",
    "    This class delegates to the standalone functions (measure_sparsity, magnitude_prune, etc.)\n",
    "    that students implement, providing a clean OOP interface for milestones.\n",
    "\n",
    "    Note: Compressor methods return fractions (0-1) for consistency with benchmarking,\n",
    "    while standalone functions return percentages (0-100) for educational clarity.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def measure_sparsity(model) -> float:\n",
    "        \"\"\"Measure the sparsity of a model (returns fraction 0-1).\"\"\"\n",
    "        # Delegate to standalone function and convert percentage to fraction\n",
    "        return measure_sparsity(model) / 100.0\n",
    "\n",
    "    @staticmethod\n",
    "    def magnitude_prune(model, sparsity=0.5):\n",
    "        \"\"\"Prune model weights by magnitude. Delegates to standalone function.\"\"\"\n",
    "        return magnitude_prune(model, sparsity)\n",
    "\n",
    "    @staticmethod\n",
    "    def structured_prune(model, prune_ratio=0.5):\n",
    "        \"\"\"Prune entire neurons/channels. Delegates to standalone function.\"\"\"\n",
    "        return structured_prune(model, prune_ratio)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_model(model, compression_config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Apply complete compression pipeline to a model.\n",
    "\n",
    "        Args:\n",
    "            model: Model to compress\n",
    "            compression_config: Dictionary with compression settings\n",
    "                - 'magnitude_sparsity': float (0-1)\n",
    "                - 'structured_prune_ratio': float (0-1)\n",
    "\n",
    "        Returns:\n",
    "            Compressed model with sparsity stats (fractions 0-1)\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'original_sparsity': Compressor.measure_sparsity(model)\n",
    "        }\n",
    "\n",
    "        # Apply magnitude pruning\n",
    "        if 'magnitude_sparsity' in compression_config:\n",
    "            model = Compressor.magnitude_prune(\n",
    "                model, compression_config['magnitude_sparsity']\n",
    "            )\n",
    "\n",
    "        # Apply structured pruning\n",
    "        if 'structured_prune_ratio' in compression_config:\n",
    "            model = Compressor.structured_prune(\n",
    "                model, compression_config['structured_prune_ratio']\n",
    "            )\n",
    "\n",
    "        stats['final_sparsity'] = Compressor.measure_sparsity(model)\n",
    "        stats['compression_ratio'] = 1.0 / (1.0 - stats['final_sparsity']) if stats['final_sparsity'] < 1.0 else float('inf')\n",
    "\n",
    "        return model, stats\n",
    "\n",
    "# Note: measure_sparsity, magnitude_prune, structured_prune are defined earlier in this module.\n",
    "# The Compressor class above delegates to those functions, providing an OOP interface for milestones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd68a02",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ¤” ML Systems Thinking: Compression Foundations\n",
    "\n",
    "### Question 1: Compression Trade-offs\n",
    "You implemented magnitude pruning that removes 90% of weights from a 10M parameter model.\n",
    "- How many parameters remain active? _____ M parameters\n",
    "- If the original model was 40MB, what's the theoretical minimum storage? _____ MB\n",
    "- Why might actual speedup be less than 10x? _____________\n",
    "\n",
    "### Question 2: Structured vs Unstructured Sparsity\n",
    "Your structured pruning removes entire channels, while magnitude pruning creates scattered zeros.\n",
    "- Which enables better hardware acceleration? _____________\n",
    "- Which preserves accuracy better at high sparsity? _____________\n",
    "- Which creates more predictable memory access patterns? _____________\n",
    "\n",
    "### Question 3: Knowledge Distillation Efficiency\n",
    "A teacher model has 100M parameters, student has 10M parameters, both achieve 85% accuracy.\n",
    "- What's the compression ratio? _____x\n",
    "- If teacher inference takes 100ms, student takes 15ms, what's the speedup? _____x\n",
    "- Why is the speedup greater than the compression ratio? _____________\n",
    "\n",
    "### Question 4: Low-Rank Decomposition\n",
    "You approximate a (512, 256) weight matrix with rank 64 using SVD.\n",
    "- Original parameter count: _____ parameters\n",
    "- Decomposed parameter count: _____ parameters\n",
    "- Compression ratio: _____x\n",
    "- At what rank does compression become ineffective? rank > _____\n",
    "\n",
    "### Question 5: Pruning Strategy Selection\n",
    "For deploying on a mobile device with 50MB model limit and 100ms latency requirement:\n",
    "- Which pruning strategy optimizes for memory? [magnitude/structured/both]\n",
    "- Which pruning strategy optimizes for speed? [magnitude/structured/both]\n",
    "- What order should you apply compression techniques? _____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd267c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ”§ Verification: Prove Pruning Works\n",
    "\n",
    "Before running the full integration test, let's create a verification function that\n",
    "proves pruning actually creates zeros using real zero counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f942f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def verify_pruning_works(model, target_sparsity=0.8):\n",
    "    \"\"\"\n",
    "    Verify pruning actually creates zeros using real zero counting.\n",
    "\n",
    "    This is NOT a theoretical calculation - we count actual zero values\n",
    "    in parameter arrays and honestly report memory footprint (unchanged with dense storage).\n",
    "\n",
    "    Args:\n",
    "        model: Model with pruned parameters (Sequential with .parameters())\n",
    "        target_sparsity: Expected sparsity ratio (default 0.8 = 80%)\n",
    "\n",
    "    Returns:\n",
    "        dict: Verification results with sparsity, zeros, total, verified\n",
    "\n",
    "    Example:\n",
    "        >>> model = Sequential(Linear(100, 50))\n",
    "        >>> magnitude_prune(model, sparsity=0.8)\n",
    "        >>> results = verify_pruning_works(model, target_sparsity=0.8)\n",
    "        >>> assert results['verified']  # Pruning actually works!\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”¬ Verifying pruning sparsity with actual zero counting...\")\n",
    "\n",
    "    # Count actual zeros in model parameters\n",
    "    zeros = sum(np.sum(p.data == 0) for p in model.parameters())\n",
    "    total = sum(p.data.size for p in model.parameters())\n",
    "    sparsity = zeros / total\n",
    "    memory_bytes = sum(p.data.nbytes for p in model.parameters())\n",
    "\n",
    "    # Display results\n",
    "    print(f\"   Total parameters: {total:,}\")\n",
    "    print(f\"   Zero parameters: {zeros:,}\")\n",
    "    print(f\"   Active parameters: {total - zeros:,}\")\n",
    "    print(f\"   Sparsity achieved: {sparsity*100:.1f}%\")\n",
    "    print(f\"   Memory footprint: {memory_bytes / MB_TO_BYTES:.2f} MB (unchanged with dense storage)\")\n",
    "\n",
    "    # Verify target met (allow 15% tolerance for structured pruning variations)\n",
    "    verified = abs(sparsity - target_sparsity) < 0.15\n",
    "    status = 'âœ“' if verified else 'âœ—'\n",
    "    print(f\"   {status} Meets {target_sparsity*100:.0f}% sparsity target\")\n",
    "\n",
    "    assert verified, f\"Sparsity target not met: {sparsity:.2f} vs {target_sparsity:.2f}\"\n",
    "\n",
    "    print(f\"\\nâœ… VERIFIED: {sparsity*100:.1f}% sparsity achieved\")\n",
    "    print(f\"âš ï¸ Memory saved: 0 MB (dense numpy arrays)\")\n",
    "    print(f\"ğŸ’¡ LEARNING: Compute savings ~{sparsity*100:.1f}% (skip zero multiplications)\")\n",
    "    print(f\"   In production: Use sparse formats (scipy.sparse.csr_matrix) for memory savings\")\n",
    "\n",
    "    return {\n",
    "        'sparsity': sparsity,\n",
    "        'zeros': zeros,\n",
    "        'total': total,\n",
    "        'active': total - zeros,\n",
    "        'memory_mb': memory_bytes / MB_TO_BYTES,\n",
    "        'verified': verified\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49454fb2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ§ª Module Integration Test\n",
    "\n",
    "Final validation that all compression techniques work together correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ccf96a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_module():\n",
    "    \"\"\"ğŸ§ª Module Test: Complete Integration\n",
    "\n",
    "    Comprehensive test of entire compression module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - All unit tests pass\n",
    "    - Functions work together correctly\n",
    "    - Module is ready for integration with TinyTorch\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_measure_sparsity()\n",
    "    test_unit_magnitude_prune()\n",
    "    test_unit_structured_prune()\n",
    "    test_unit_low_rank_approximate()\n",
    "    test_unit_knowledge_distillation()\n",
    "    test_unit_compress_model()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test 1: Complete compression pipeline\n",
    "    print(\"ğŸ”¬ Integration Test: Complete compression pipeline...\")\n",
    "\n",
    "    # Create a realistic model with explicit layers - students see the architecture\n",
    "    input_layer = Linear(784, 512)    # Input layer (like MNIST)\n",
    "    hidden1 = Linear(512, 256)         # Hidden layer 1\n",
    "    hidden2 = Linear(256, 128)         # Hidden layer 2\n",
    "    output_layer = Linear(128, 10)     # Output layer\n",
    "    model = Sequential(input_layer, hidden1, hidden2, output_layer)\n",
    "\n",
    "    original_params = sum(p.size for p in model.parameters())\n",
    "    print(f\"Original model: {original_params:,} parameters\")\n",
    "\n",
    "    # Apply comprehensive compression - students see each technique\n",
    "    compression_config = {\n",
    "        'magnitude_prune': 0.8,    # Remove 80% of smallest weights\n",
    "        'structured_prune': 0.3     # Remove 30% of channels\n",
    "    }\n",
    "\n",
    "    stats = compress_model(model, compression_config)\n",
    "    final_sparsity = measure_sparsity(model)\n",
    "\n",
    "    # Validate compression results\n",
    "    assert final_sparsity > 70, f\"Expected >70% sparsity, got {final_sparsity:.1f}%\"\n",
    "    assert stats['sparsity_increase'] > 70, \"Should achieve significant compression\"\n",
    "    assert len(stats['applied_techniques']) == 2, \"Should apply both techniques\"\n",
    "\n",
    "    print(f\"âœ… Achieved {final_sparsity:.1f}% sparsity with {len(stats['applied_techniques'])} techniques\")\n",
    "\n",
    "    # Test 2: Knowledge distillation setup\n",
    "    print(\"ğŸ”¬ Integration Test: Knowledge distillation...\")\n",
    "\n",
    "    # Create teacher with more capacity - explicit layers show architecture\n",
    "    teacher_l1 = Linear(100, 200)\n",
    "    teacher_l2 = Linear(200, 50)\n",
    "    teacher = Sequential(teacher_l1, teacher_l2)\n",
    "\n",
    "    # Create smaller student - explicit shows size difference\n",
    "    student_l1 = Linear(100, 50)\n",
    "    student = Sequential(student_l1)  # 3x fewer parameters\n",
    "\n",
    "    kd = KnowledgeDistillation(teacher, student, temperature=4.0, alpha=0.8)\n",
    "\n",
    "    # Verify setup\n",
    "    teacher_params = sum(p.size for p in teacher.parameters())\n",
    "    student_params = sum(p.size for p in student.parameters())\n",
    "    compression_ratio = student_params / teacher_params\n",
    "\n",
    "    assert compression_ratio < 0.5, f\"Student should be <50% of teacher size, got {compression_ratio:.2f}\"\n",
    "    assert kd.temperature == 4.0, \"Temperature should be set correctly\"\n",
    "    assert kd.alpha == 0.8, \"Alpha should be set correctly\"\n",
    "\n",
    "    print(f\"âœ… Knowledge distillation: {compression_ratio:.2f}x size reduction\")\n",
    "\n",
    "    # Test 3: Low-rank approximation\n",
    "    print(\"ğŸ”¬ Integration Test: Low-rank approximation...\")\n",
    "\n",
    "    large_matrix = np.random.randn(200, 150)\n",
    "    U, S, V = low_rank_approximate(large_matrix, rank_ratio=0.3)\n",
    "\n",
    "    original_size = large_matrix.size\n",
    "    compressed_size = U.size + S.size + V.size\n",
    "    compression_ratio = compressed_size / original_size\n",
    "\n",
    "    assert compression_ratio < 0.7, f\"Should achieve compression, got ratio {compression_ratio:.2f}\"\n",
    "\n",
    "    # Test reconstruction\n",
    "    reconstructed = U @ np.diag(S) @ V\n",
    "    error = np.linalg.norm(large_matrix - reconstructed) / np.linalg.norm(large_matrix)\n",
    "    # Low-rank approximation trades accuracy for compression - some error is expected\n",
    "    assert error < 0.7, f\"Reconstruction error too high: {error:.3f}\"\n",
    "\n",
    "    print(f\"âœ… Low-rank: {compression_ratio:.2f}x compression, {error:.3f} error\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ‰ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867353fc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ Running Compression module...\")\n",
    "    test_module()\n",
    "    print(\"âœ… Module validation complete!\")\n",
    "\"\"\"\n",
    "## â­ Aha Moment: Pruning Removes Unimportant Weights\n",
    "\n",
    "**What you built:** Pruning that zeros out small weights, creating sparse models.\n",
    "\n",
    "**Why it matters:** Most neural network weights are close to zeroâ€”and removing them barely\n",
    "affects accuracy! At 50% sparsity, half your weights are gone, but the model still works.\n",
    "This is how you make models faster and smaller without retraining.\n",
    "\n",
    "Combined with quantization, pruning can shrink models 8Ã— or more.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c27c2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demo_compression():\n",
    "    \"\"\"ğŸ¯ See pruning create sparsity.\"\"\"\n",
    "    print(\"ğŸ¯ AHA MOMENT: Pruning Removes Weights\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    # Create a model\n",
    "    layer = Linear(128, 64)\n",
    "\n",
    "    original_nonzero = np.count_nonzero(layer.weight.data)\n",
    "    original_total = layer.weight.data.size\n",
    "\n",
    "    # Apply 50% pruning\n",
    "    Compressor.magnitude_prune(layer, sparsity=0.5)\n",
    "\n",
    "    pruned_nonzero = np.count_nonzero(layer.weight.data)\n",
    "    sparsity = 1 - (pruned_nonzero / original_total)\n",
    "\n",
    "    print(f\"Original: {original_nonzero:,} non-zero weights\")\n",
    "    print(f\"After 50% pruning: {pruned_nonzero:,} non-zero weights\")\n",
    "    print(f\"\\nActual sparsity: {sparsity:.1%}\")\n",
    "    print(f\"Half the weights are now zero!\")\n",
    "\n",
    "    print(\"\\nâœ¨ Smaller weights removedâ€”model still works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8efac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_module()\n",
    "    print(\"\\n\")\n",
    "    demo_compression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499142c7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸš€ MODULE SUMMARY: Compression\n",
    "\n",
    "Congratulations! You've built a comprehensive model compression system that can dramatically reduce model size while preserving intelligence!\n",
    "\n",
    "### Key Accomplishments\n",
    "- Built magnitude-based and structured pruning techniques with clear sparsity patterns\n",
    "- Implemented knowledge distillation for teacher-student compression with temperature scaling\n",
    "- Created low-rank approximation using SVD decomposition for matrix factorization\n",
    "- Developed sparsity measurement and comprehensive compression pipeline\n",
    "- Analyzed compression trade-offs between size, speed, and accuracy with real measurements\n",
    "- All tests pass âœ… (validated by `test_module()`)\n",
    "\n",
    "### Systems Insights Gained\n",
    "- **Structured vs Unstructured**: Hardware-friendly sparsity patterns vs maximum compression ratios\n",
    "- **Compression Cascading**: Multiple techniques compound benefits but require careful sequencing\n",
    "- **Accuracy Preservation**: Knowledge distillation maintains performance better than pruning alone\n",
    "- **Memory vs Speed**: Parameter reduction doesn't guarantee proportional speedup without sparse libraries\n",
    "- **Deployment Strategy**: Different scenarios (mobile, edge, cloud) require different compression approaches\n",
    "\n",
    "### Technical Mastery\n",
    "- **Sparsity Measurement**: Calculate and track zero weight percentages across models\n",
    "- **Magnitude Pruning**: Global thresholding based on weight importance ranking\n",
    "- **Structured Pruning**: Channel-wise removal using L2 norm importance metrics\n",
    "- **Knowledge Distillation**: Teacher-student training with temperature-scaled soft targets\n",
    "- **Low-Rank Approximation**: SVD-based matrix factorization for parameter reduction\n",
    "- **Pipeline Integration**: Sequential application of multiple compression techniques\n",
    "\n",
    "### Ready for Next Steps\n",
    "Your compression implementation enables efficient model deployment across diverse hardware constraints!\n",
    "Export with: `tito module complete 16`\n",
    "\n",
    "**Next**: Module 17 will add acceleration techniques including vectorization and kernel fusion, building on compression for maximum efficiency!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
