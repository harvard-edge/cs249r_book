
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 04: Losses" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/04_losses_ABOUT.html" />
<meta property="og:site_name" content="Tinyüî•Torch" />
<meta property="og:description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: L..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="Tinyüî•Torch" />
<meta name="description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: L..." />

    <title>Module 04: Losses &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=9c3e77be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=cd3a79b9" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=1ae7504c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";





const initStyles = () => {
    const defaultStyle = document.createElement('style');
    defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}`;
    document.head.appendChild(defaultStyle);

    const fullscreenStyle = document.createElement('style');
    fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
    document.head.appendChild(fullscreenStyle);
}

// Detect if page has dark background
const isDarkTheme = () => {
    // We use a set of heuristics:
    // 1. Check for common dark mode classes or attributes
    // 2. Check computed background color brightness
    if (document.documentElement.classList.contains('dark') ||
        document.documentElement.getAttribute('data-theme') === 'dark' ||
        document.body.classList.contains('dark') ||
        document.body.getAttribute('data-theme') === 'dark') {
        // console.log("Dark theme detected via class/attribute");
        return true;
    }
    if (document.documentElement.classList.contains('light') ||
        document.documentElement.getAttribute('data-theme') === 'light' ||
        document.body.classList.contains('light') ||
        document.body.getAttribute('data-theme') === 'light') {
        // console.log("Light theme detected via class/attribute");
        return false;
    }
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
        // console.log("Dark theme detected via prefers-color-scheme");
        return true;
    }
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        // console.log("Background color brightness:", brightness);
        return brightness < 128;
    }
    // console.log("No dark or light theme detected, defaulting to light theme");
    return false;
};

let darkTheme = isDarkTheme();
let modal = null;
let modalContent = null;
let previousScrollOffset = [window.scrollX, window.scrollY];

const runMermaid = async (rerun) => {
    console.log("Running mermaid diagrams, rerun =", rerun);
    // clear all existing mermaid charts
    let all_mermaids = document.querySelectorAll(".mermaid");

    if (rerun) {
        all_mermaids.forEach((el) => {
            if(!el.hasAttribute("data-original-code")) {
                // store original code
                // console.log(`Storing original code for first run: `, el.innerHTML);
                el.setAttribute('data-original-code', el.innerHTML);
            }
            if(el.getAttribute("data-processed") === "true") {
                // remove and restore original
                el.removeAttribute("data-processed");
                // console.log(`Restoring original code for re-run: `, el.getAttribute('data-original-code'));
                el.innerHTML = el.getAttribute('data-original-code');
            } else {
                // store original code
                // console.log(`Storing original code for re-run: `, el.innerHTML);
                el.setAttribute('data-original-code', el.innerHTML);
            }
        });
        await mermaid.run();
    }

    all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(() => runMermaid(false), 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(() => runMermaid(false), 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(() => runMermaid(false), 200);
        return;
    }

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    if (modal !== null ) {
        // Destroy existing modal
        modal.remove();
        modal = null;
        modalContent = null;
    }

    modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            // Already processed, adjust button class if needed
            const existingBtn = mermaidDiv.parentNode.querySelector('.mermaid-fullscreen-btn');
            if (existingBtn) {
                existingBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
            }
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });
        container.appendChild(fullscreenBtn);
    });
};

const load = async () => {
    initStyles();

    await runMermaid(true);

    const reRunIfThemeChanges = async () => {
        const newDarkTheme = isDarkTheme();
        if (newDarkTheme !== darkTheme) {
            darkTheme = newDarkTheme;
            console.log("Theme change detected, re-running mermaid with", darkTheme ? "dark" : "default", "theme");
            await mermaid.initialize(
                {...JSON.parse(
                    `{"startOnLoad": false}`
                ),
                ...{ darkMode: darkTheme, theme: darkTheme ? 'dark' : 'default' },
                }
            );
            await runMermaid(true);
        }
    };

    // Update theme classes when theme changes
    const themeObserver = new MutationObserver(reRunIfThemeChanges);
    themeObserver.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    themeObserver.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
};





console.log("Initializing mermaid with", darkTheme ? "dark" : "default", "theme");
mermaid.initialize(
    {...JSON.parse(
        `{"startOnLoad": false}`
    ),
    ...{ darkMode: darkTheme, theme: darkTheme ? 'dark' : 'default' },
    }
);

window.addEventListener("load", load);</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/04_losses_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=0d27a1a4"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=ee94e95f"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/version-badge.js?v=4f0a3395"></script>
    <script src="../_static/subscribe-modal.js?v=c8499bec"></script>
    <script src="../_static/announcement-bar.js?v=49ee9c9d"></script>
    <link rel="canonical" href="https://mlsysbook.ai/tinytorch/modules/04_losses_ABOUT.html" />
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Module 05: DataLoader" href="05_dataloader_ABOUT.html" />
    <link rel="prev" title="Module 03: Layers" href="03_layers_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../book/" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mlsysbook" title="Support" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-heart fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Support</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-08)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader_ABOUT.html">05. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_autograd_ABOUT.html">06. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimizers_ABOUT.html">07. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_training_ABOUT.html">08. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (09-13)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_acceleration_ABOUT.html">17. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_memoization_ABOUT.html">18. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/04_losses_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 04: Losses</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#helper-functions">Helper Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-output-shapes">Input/Output Shapes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-as-a-feedback-signal">Loss as a Feedback Signal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error">Mean Squared Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy-loss">Cross-Entropy Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-stability-in-loss-computation">Numerical Stability in Loss Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-strategies">Reduction Strategies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatch-in-cross-entropy">Shape Mismatch in Cross-Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nan-loss-from-numerical-instability">NaN Loss from Numerical Instability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusing-logits-and-probabilities">Confusing Logits and Probabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-loss-functions-matter-at-scale">Why Loss Functions Matter at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-04-losses">
<h1>Module 04: Losses<a class="headerlink" href="#module-04-losses" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>FOUNDATION TIER</strong> | Difficulty: ‚óè‚óè‚óã‚óã | Time: 4-6 hours | Prerequisites: 01, 02, 03</p>
<p><strong>Prerequisites:</strong> Modules 01 (Tensor), 02 (Activations), and 03 (Layers) must be completed. This module assumes you understand:</p>
<ul class="simple">
<li><p>Tensor operations and broadcasting (Module 01)</p></li>
<li><p>Activation functions and their role in neural networks (Module 02)</p></li>
<li><p>Layers and how they transform data (Module 03)</p></li>
</ul>
<p>If you can build a simple neural network that takes input and produces output, you‚Äôre ready to learn how to measure its quality.</p>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run interactively in your browser.</p>
<p class="sd-card-text"><a href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F04_losses%2F04_losses.ipynb" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #f97316; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">Open in Binder ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìÑ View Source</div>
<p class="sd-card-text">Browse the source code on GitHub.</p>
<p class="sd-card-text"><a href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/04_losses/04_losses.py" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #6b7280; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">View on GitHub ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üéß Audio Overview</div>
<p class="sd-card-text">Listen to an AI-generated overview.</p>
<audio controls style="width: 100%; height: 54px; margin-top: auto;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/04_losses.mp3" type="audio/mpeg">
</audio>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Loss functions are the mathematical conscience of machine learning. Every neural network needs to know when it‚Äôs right and when it‚Äôs wrong. Loss functions provide that feedback by measuring the distance between what your model predicts and what actually happened. Without loss functions, models have no way to improve - they‚Äôre like athletes training without knowing their score.</p>
<p>In this module, you‚Äôll implement three essential loss functions: Mean Squared Error (MSE) for regression, Cross-Entropy for multi-class classification, and Binary Cross-Entropy for binary decisions. You‚Äôll also master the log-sum-exp trick, a crucial numerical stability technique that prevents computational overflow with large numbers. These implementations will serve as the foundation for Module 06: Autograd, where gradients flow backward from these loss values to update model parameters.</p>
<p>By the end, you‚Äôll understand not just how to compute loss, but why different problems require different loss functions, and how numerical stability shapes production ML systems.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> MSELoss for regression, CrossEntropyLoss for multi-class classification, and BinaryCrossEntropyLoss for binary decisions</p></li>
<li><p><strong>Master</strong> the log-sum-exp trick for numerically stable softmax computation</p></li>
<li><p><strong>Understand</strong> computational complexity (O(B√óC) for cross-entropy with large vocabularies) and memory trade-offs</p></li>
<li><p><strong>Analyze</strong> loss function behavior across different prediction patterns and confidence levels</p></li>
<li><p><strong>Connect</strong> your implementation to production PyTorch patterns and engineering decisions at scale</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Your Loss Functions&quot;
        A[&quot;log_softmax()&lt;br/&gt;Numerical Stability&quot;]
        B[&quot;MSELoss&lt;br/&gt;Regression&quot;]
        C[&quot;CrossEntropyLoss&lt;br/&gt;Classification&quot;]
        D[&quot;BinaryCrossEntropyLoss&lt;br/&gt;Binary Decisions&quot;]
    end

    A --&gt; C

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
    </pre><figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">Your Loss Functions</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Step</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">log_softmax()</span></code></p></td>
<td><p>Log-sum-exp trick for numerical stability</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MSELoss.forward()</span></code></p></td>
<td><p>Mean squared error for continuous predictions</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss.forward()</span></code></p></td>
<td><p>Negative log-likelihood for multi-class classification</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss.forward()</span></code></p></td>
<td><p>Cross-entropy specialized for binary decisions</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Measuring prediction quality</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>  <span class="c1"># Scalar feedback signal for learning</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>Gradient computation (that‚Äôs Module 06: Autograd)</p></li>
<li><p>Advanced loss variants (Focal Loss, Label Smoothing, Huber Loss)</p></li>
<li><p>Hierarchical or sampled softmax for large vocabularies (PyTorch optimization)</p></li>
<li><p>Custom reduction strategies beyond mean</p></li>
</ul>
<p><strong>You are building the core feedback signal.</strong> Gradient-based learning comes next.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section provides a quick reference for the loss functions you‚Äôll build. Use it as your cheat sheet while implementing and debugging.</p>
<section id="helper-functions">
<h3>Helper Functions<a class="headerlink" href="#helper-functions" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span>
</pre></div>
</div>
<p>Computes numerically stable log-softmax using the log-sum-exp trick. This is the foundation for cross-entropy loss.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x</span></code> (Tensor): Input tensor containing logits (raw model outputs, unbounded values)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim</span></code> (int): Dimension along which to compute log-softmax (default: -1, last dimension)</p></li>
</ul>
<p><strong>Returns:</strong> Tensor with same shape as input, containing log-probabilities</p>
<p><strong>Note:</strong> Logits are raw, unbounded scores from your model before any activation function. CrossEntropyLoss expects logits, not probabilities.</p>
</section>
<section id="loss-functions">
<h3>Loss Functions<a class="headerlink" href="#loss-functions" title="Link to this heading">#</a></h3>
<p>All loss functions follow the same pattern:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Loss Function</p></th>
<th class="head"><p>Constructor</p></th>
<th class="head"><p>Forward Signature</p></th>
<th class="head"><p>Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">MSELoss</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">MSELoss()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(predictions:</span> <span class="pre">Tensor,</span> <span class="pre">targets:</span> <span class="pre">Tensor)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Regression</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(logits:</span> <span class="pre">Tensor,</span> <span class="pre">targets:</span> <span class="pre">Tensor)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Multi-class classification</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">BinaryCrossEntropyLoss()</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(predictions:</span> <span class="pre">Tensor,</span> <span class="pre">targets:</span> <span class="pre">Tensor)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Binary classification</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Common Pattern:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>  <span class="c1"># __call__ delegates to forward()</span>
</pre></div>
</div>
</section>
<section id="input-output-shapes">
<h3>Input/Output Shapes<a class="headerlink" href="#input-output-shapes" title="Link to this heading">#</a></h3>
<p>Understanding input shapes is crucial for correct loss computation:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Loss</p></th>
<th class="head"><p>Predictions Shape</p></th>
<th class="head"><p>Targets Shape</p></th>
<th class="head"><p>Output Shape</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>MSE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(N,)</span></code> or <code class="docutils literal notranslate"><span class="pre">(N,</span> <span class="pre">D)</span></code></p></td>
<td><p>Same as predictions</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">()</span></code> scalar</p></td>
</tr>
<tr class="row-odd"><td><p>CrossEntropy</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(N,</span> <span class="pre">C)</span></code> logits¬π</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(N,)</span></code> class indices¬≤</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">()</span></code> scalar</p></td>
</tr>
<tr class="row-even"><td><p>BinaryCrossEntropy</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(N,)</span></code> probabilities¬≥</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(N,)</span></code> binary labels (0 or 1)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">()</span></code> scalar</p></td>
</tr>
</tbody>
</table>
</div>
<p>Where N = batch size, D = feature dimension, C = number of classes</p>
<p><strong>Notes:</strong></p>
<ol class="arabic simple">
<li><p><strong>Logits</strong>: Raw unbounded values from your model (e.g., <code class="docutils literal notranslate"><span class="pre">[2.3,</span> <span class="pre">-1.2,</span> <span class="pre">5.1]</span></code>). Do NOT apply softmax before passing to CrossEntropyLoss.</p></li>
<li><p><strong>Class indices</strong>: Integer values from 0 to C-1 indicating the correct class (e.g., <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">2,</span> <span class="pre">1]</span></code> for 3 samples).</p></li>
<li><p><strong>Probabilities</strong>: Values between 0 and 1 after applying sigmoid activation. Must be in valid probability range.</p></li>
</ol>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental ideas you need to understand loss functions deeply. These concepts apply to every ML framework, not just TinyTorch.</p>
<section id="loss-as-a-feedback-signal">
<h3>Loss as a Feedback Signal<a class="headerlink" href="#loss-as-a-feedback-signal" title="Link to this heading">#</a></h3>
<p>Loss functions transform the abstract question ‚Äúhow good is my model?‚Äù into a concrete number that can drive improvement. Consider a simple example: predicting house prices. If your model predicts $250,000 for a house that sold for $245,000, how wrong is that? What about $150,000 when the actual price was $250,000? The loss function quantifies these errors in a way that optimization algorithms can use.</p>
<p>The key insight is that loss functions must be differentiable - you need to know not just the current error, but which direction to move parameters to reduce that error. This is why we use squared differences instead of absolute differences in MSE: the square function has a smooth derivative that points toward improvement.</p>
<p>Every training iteration follows the same pattern: forward pass produces predictions, loss function measures error, backward pass (Module 06) computes how to improve. The loss value itself becomes a single number summarizing model quality across an entire batch of examples.</p>
</section>
<section id="mean-squared-error">
<h3>Mean Squared Error<a class="headerlink" href="#mean-squared-error" title="Link to this heading">#</a></h3>
<p>MSE is the foundational loss for regression problems. It measures the average squared distance between predictions and targets. The squaring serves three purposes: it makes all errors positive (preventing cancellation), it heavily penalizes large errors, and it creates smooth gradients for optimization.</p>
<p>Here‚Äôs the complete implementation from your module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute mean squared error between predictions and targets.&quot;&quot;&quot;</span>
    <span class="c1"># Step 1: Compute element-wise difference</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">targets</span><span class="o">.</span><span class="n">data</span>

    <span class="c1"># Step 2: Square the differences</span>
    <span class="n">squared_diff</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="c1"># Step 3: Take mean across all elements</span>
    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
<p>The beauty of MSE is its simplicity: subtract, square, average. Yet this simple formula creates a quadratic error landscape. An error of 10 contributes 100 to the loss, while an error of 20 contributes 400. This quadratic growth means the loss function cares much more about fixing large errors than small ones, naturally prioritizing the worst predictions during optimization.</p>
<p>Consider predicting house prices. An error of $5,000 on a $200,000 house gets squared to 25,000,000. An error of $50,000 gets squared to 2,500,000,000 - one hundred times worse for an error only ten times larger. This sensitivity to outliers can be both a strength (quickly correcting large errors) and a weakness (vulnerable to noisy labels).</p>
</section>
<section id="cross-entropy-loss">
<h3>Cross-Entropy Loss<a class="headerlink" href="#cross-entropy-loss" title="Link to this heading">#</a></h3>
<p>Cross-entropy measures how wrong your probability predictions are for classification problems. Unlike MSE which measures distance, cross-entropy measures surprise: how unexpected is the true outcome given your model‚Äôs probability distribution?</p>
<p>The mathematical formula is deceptively simple: negative log-likelihood of the correct class. But implementing it correctly requires careful attention to numerical stability. Here‚Äôs how your implementation handles it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute cross-entropy loss between logits and target class indices.&quot;&quot;&quot;</span>
    <span class="c1"># Step 1: Compute log-softmax for numerical stability</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Step 2: Select log-probabilities for correct classes</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">target_indices</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="c1"># Select correct class log-probabilities using advanced indexing</span>
    <span class="n">selected_log_probs</span> <span class="o">=</span> <span class="n">log_probs</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">target_indices</span><span class="p">]</span>

    <span class="c1"># Step 3: Return negative mean (cross-entropy is negative log-likelihood)</span>
    <span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">selected_log_probs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
</pre></div>
</div>
<p>The critical detail is using <code class="docutils literal notranslate"><span class="pre">log_softmax</span></code> instead of computing softmax then taking the log. This seemingly minor choice prevents catastrophic overflow with large logits. Without it, a logit value of 100 would compute <code class="docutils literal notranslate"><span class="pre">exp(100)</span> <span class="pre">=</span> <span class="pre">2.7√ó10^43</span></code>, which exceeds float32 range and becomes infinity.</p>
<p>Cross-entropy‚Äôs power comes from its asymmetric penalty structure. If your model predicts 0.99 probability for the correct class, the loss is <code class="docutils literal notranslate"><span class="pre">-log(0.99)</span> <span class="pre">=</span> <span class="pre">0.01</span></code> - very small. But if you predict 0.01 for the correct class, the loss is <code class="docutils literal notranslate"><span class="pre">-log(0.01)</span> <span class="pre">=</span> <span class="pre">4.6</span></code> - much larger. This creates strong pressure to be confident when correct and uncertain when wrong.</p>
</section>
<section id="numerical-stability-in-loss-computation">
<h3>Numerical Stability in Loss Computation<a class="headerlink" href="#numerical-stability-in-loss-computation" title="Link to this heading">#</a></h3>
<p>The log-sum-exp trick is one of the most important numerical stability techniques in machine learning. It solves a fundamental problem: computing softmax directly causes overflow, but we need softmax for classification.</p>
<p>Consider what happens without the trick. Standard softmax computes <code class="docutils literal notranslate"><span class="pre">exp(x)</span> <span class="pre">/</span> <span class="pre">sum(exp(x))</span></code>. With logits <code class="docutils literal notranslate"><span class="pre">[100,</span> <span class="pre">200,</span> <span class="pre">300]</span></code>, you‚Äôd compute <code class="docutils literal notranslate"><span class="pre">exp(300)</span> <span class="pre">=</span> <span class="pre">1.97√ó10^130</span></code>, which is infinity in float32. The trick subtracts the maximum value first, making the largest exponent zero:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute log-softmax with numerical stability.&quot;&quot;&quot;</span>
    <span class="c1"># Step 1: Find max along dimension for numerical stability</span>
    <span class="n">max_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Step 2: Subtract max to prevent overflow</span>
    <span class="n">shifted</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">max_vals</span>

    <span class="c1"># Step 3: Compute log(sum(exp(shifted)))</span>
    <span class="n">log_sum_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">shifted</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="c1"># Step 4: Return log_softmax = input - max - log_sum_exp</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">max_vals</span> <span class="o">-</span> <span class="n">log_sum_exp</span>

    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p>After subtracting the max (300), the shifted logits become <code class="docutils literal notranslate"><span class="pre">[-200,</span> <span class="pre">-100,</span> <span class="pre">0]</span></code>. Now the largest exponent is <code class="docutils literal notranslate"><span class="pre">exp(0)</span> <span class="pre">=</span> <span class="pre">1.0</span></code>, perfectly safe. The smaller values like <code class="docutils literal notranslate"><span class="pre">exp(-200)</span></code> underflow to zero, but that‚Äôs acceptable - they contribute negligibly to the sum anyway.</p>
<p>This trick is mathematically exact, not an approximation. Subtracting the max from both numerator and denominator cancels out, leaving the result unchanged. But the computational difference is dramatic: infinity versus valid probabilities.</p>
</section>
<section id="reduction-strategies">
<h3>Reduction Strategies<a class="headerlink" href="#reduction-strategies" title="Link to this heading">#</a></h3>
<p>All three loss functions reduce a batch of per-sample errors to a single scalar by taking the mean. This reduction strategy affects both the loss magnitude and the resulting gradients during backpropagation.</p>
<p>The mean reduction has important properties. First, it normalizes by batch size, making loss values comparable across different batch sizes. A batch of 32 samples and a batch of 128 samples produce similar loss magnitudes if the per-sample errors are similar. Second, it makes gradients inversely proportional to batch size - with 128 samples, each sample contributes 1/128 to the total gradient, preventing gradient explosion with large batches.</p>
<p>Alternative reduction strategies exist but aren‚Äôt implemented in this module. Sum reduction (<code class="docutils literal notranslate"><span class="pre">np.sum</span></code> instead of <code class="docutils literal notranslate"><span class="pre">np.mean</span></code>) accumulates total error across the batch, making loss scale with batch size. No reduction (<code class="docutils literal notranslate"><span class="pre">reduction='none'</span></code>) returns per-sample losses, useful for weighted sampling or analyzing individual predictions. Production frameworks support all these modes, but mean reduction is the standard choice for stable training.</p>
<p>The choice of reduction interacts with learning rate. If you switch from mean to sum reduction, you must divide your learning rate by batch size to maintain equivalent optimization dynamics. This is why PyTorch defaults to mean reduction - it makes hyperparameters more transferable across different batch sizes.</p>
</section>
</section>
<section id="common-errors">
<h2>Common Errors<a class="headerlink" href="#common-errors" title="Link to this heading">#</a></h2>
<section id="shape-mismatch-in-cross-entropy">
<h3>Shape Mismatch in Cross-Entropy<a class="headerlink" href="#shape-mismatch-in-cross-entropy" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">IndexError:</span> <span class="pre">index</span> <span class="pre">5</span> <span class="pre">is</span> <span class="pre">out</span> <span class="pre">of</span> <span class="pre">bounds</span> <span class="pre">for</span> <span class="pre">axis</span> <span class="pre">1</span> <span class="pre">with</span> <span class="pre">size</span> <span class="pre">3</span></code></p>
<p>This happens when your target class indices exceed the number of classes in your logits. If you have 3 classes (indices 0, 1, 2) but your targets contain index 5, the indexing operation fails.</p>
<p><strong>Fix</strong>: Verify your target indices match your model‚Äôs output dimensions. For a 3-class problem, targets should only contain 0, 1, or 2.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ‚ùå Wrong - target index 5 doesn&#39;t exist for 3 classes</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span>  <span class="c1"># 3 classes</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">])</span>  <span class="c1"># Index out of bounds</span>

<span class="c1"># ‚úÖ Correct - target indices match number of classes</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># Index 2 is valid for 3 classes</span>
</pre></div>
</div>
</section>
<section id="nan-loss-from-numerical-instability">
<h3>NaN Loss from Numerical Instability<a class="headerlink" href="#nan-loss-from-numerical-instability" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">RuntimeWarning:</span> <span class="pre">invalid</span> <span class="pre">value</span> <span class="pre">encountered</span> <span class="pre">in</span> <span class="pre">log</span></code> followed by <code class="docutils literal notranslate"><span class="pre">loss.data</span> <span class="pre">=</span> <span class="pre">nan</span></code></p>
<p>This occurs when probabilities reach exactly 0.0 or 1.0, causing <code class="docutils literal notranslate"><span class="pre">log(0)</span> <span class="pre">=</span> <span class="pre">-‚àû</span></code>. Binary cross-entropy is particularly vulnerable because it computes both <code class="docutils literal notranslate"><span class="pre">log(prediction)</span></code> and <code class="docutils literal notranslate"><span class="pre">log(1-prediction)</span></code>.</p>
<p><strong>Fix</strong>: Clamp probabilities to a safe range using epsilon:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Already implemented in your BinaryCrossEntropyLoss:</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-7</span>
<span class="n">clamped_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>
</pre></div>
</div>
<p>This ensures you never compute <code class="docutils literal notranslate"><span class="pre">log(0)</span></code> while keeping values extremely close to the true probabilities.</p>
</section>
<section id="confusing-logits-and-probabilities">
<h3>Confusing Logits and Probabilities<a class="headerlink" href="#confusing-logits-and-probabilities" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">loss.data</span> <span class="pre">=</span> <span class="pre">inf</span></code> or unreasonably large loss values</p>
<p>Cross-entropy expects raw logits (unbounded values from your model), while binary cross-entropy expects probabilities (0 to 1 range). Mixing these up causes numerical explosions.</p>
<p><strong>Fix</strong>: Check what your model outputs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ‚úÖ CrossEntropyLoss: Use raw logits (no sigmoid/softmax!)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Raw outputs like [2.3, -1.2, 5.1]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="c1"># ‚úÖ BinaryCrossEntropyLoss: Use probabilities (apply sigmoid!)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>  <span class="c1"># Converts to [0, 1] range</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">BinaryCrossEntropyLoss</span><span class="p">()(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-pytorch">
<h3>Your Implementation vs. PyTorch<a class="headerlink" href="#your-implementation-vs-pytorch" title="Link to this heading">#</a></h3>
<p>Your TinyTorch loss functions and PyTorch‚Äôs implementations share the same mathematical foundations and numerical stability techniques. The differences are in performance optimizations, GPU support, and additional features for production use.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Backend</strong></p></td>
<td><p>NumPy (Python)</p></td>
<td><p>C++/CUDA</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Numerical Stability</strong></p></td>
<td><p>Log-sum-exp trick</p></td>
<td><p>Same trick, fused kernels</p></td>
</tr>
<tr class="row-even"><td><p><strong>Speed</strong></p></td>
<td><p>1x (baseline)</p></td>
<td><p>10-100x faster (GPU)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Reduction Modes</strong></p></td>
<td><p>Mean only</p></td>
<td><p>mean, sum, none</p></td>
</tr>
<tr class="row-even"><td><p><strong>Advanced Variants</strong></p></td>
<td><p>‚úó</p></td>
<td><p>Label smoothing, weights</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Memory Efficiency</strong></p></td>
<td><p>Standard</p></td>
<td><p>Fused operations reduce copies</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows equivalent loss computations in TinyTorch and PyTorch. Notice how the high-level API is nearly identical - you‚Äôre learning the same patterns used in production.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.losses</span><span class="w"> </span><span class="kn">import</span> <span class="n">MSELoss</span><span class="p">,</span> <span class="n">CrossEntropyLoss</span>

<span class="c1"># Regression</span>
<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">200.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">195.0</span><span class="p">,</span> <span class="mf">260.0</span><span class="p">,</span> <span class="mf">290.0</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="c1"># Classification</span>
<span class="n">ce_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">ce_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
‚ö° PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Regression</span>
<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">200.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">195.0</span><span class="p">,</span> <span class="mf">260.0</span><span class="p">,</span> <span class="mf">290.0</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

<span class="c1"># Classification</span>
<span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">ce_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs walk through the key similarities and differences:</p>
<ul class="simple">
<li><p><strong>Line 1 (Imports)</strong>: Both frameworks use modular imports. TinyTorch exposes loss functions from <code class="docutils literal notranslate"><span class="pre">core.losses</span></code>; PyTorch uses <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>.</p></li>
<li><p><strong>Line 3 (Construction)</strong>: Both use the same pattern: instantiate the loss function once, then call it multiple times. No parameters needed for basic usage.</p></li>
<li><p><strong>Line 4-5 (Data)</strong>: TinyTorch wraps Python lists in <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>; PyTorch uses <code class="docutils literal notranslate"><span class="pre">torch.tensor()</span></code>. The data structure concept is identical.</p></li>
<li><p><strong>Line 6 (Computation)</strong>: Both compute loss by calling the loss function object. Under the hood, this calls the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method you implemented.</p></li>
<li><p><strong>Line 9 (Classification)</strong>: Both expect raw logits (not probabilities) for cross-entropy. The <code class="docutils literal notranslate"><span class="pre">log_softmax</span></code> computation happens internally in both frameworks.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>The mathematical formulas, numerical stability techniques (log-sum-exp trick), and high-level API patterns. When you debug PyTorch loss functions, you‚Äôll understand exactly what‚Äôs happening because you built the same abstractions.</p>
</div>
</section>
<section id="why-loss-functions-matter-at-scale">
<h3>Why Loss Functions Matter at Scale<a class="headerlink" href="#why-loss-functions-matter-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate why loss functions matter in production, consider the scale of modern ML systems:</p>
<ul class="simple">
<li><p><strong>Language models</strong>: 50,000 token vocabulary √ó 128 batch size = <strong>6.4M exponential operations per loss computation</strong>. With sampled softmax, this reduces to ~128K operations (50√ó speedup).</p></li>
<li><p><strong>Computer vision</strong>: ImageNet with 1,000 classes processes <strong>256,000 softmax computations</strong> per batch. Fused CUDA kernels reduce this from 15ms to 0.5ms.</p></li>
<li><p><strong>Recommendation systems</strong>: Billions of items require specialized loss functions. YouTube‚Äôs recommendation system uses <strong>sampled softmax over 1M+ videos</strong>, making loss computation the primary bottleneck.</p></li>
</ul>
<p>Memory pressure is equally significant. A language model forward pass might consume 8GB for activations, 2GB for parameters, but <strong>768MB just for the cross-entropy loss computation</strong> (B=128, C=50000, float32). Using FP16 cuts this to 384MB. Using hierarchical softmax eliminates the materialization entirely.</p>
<p>The loss computation typically accounts for <strong>5-10% of total training time</strong> in well-optimized systems, but can dominate (30-50%) for large vocabularies without optimization. This is why production frameworks invest heavily in fused kernels, specialized data structures, and algorithmic improvements like hierarchical softmax.</p>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions. They‚Äôre designed to build intuition for the performance characteristics you‚Äôll encounter in production ML.</p>
<p><strong>Q1: Memory Calculation - Large Vocabulary Language Model</strong></p>
<p>A language model with 50,000 token vocabulary uses CrossEntropyLoss with batch size 128. Using float32, how much memory does the loss computation require for logits, softmax probabilities, and log-probabilities?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Calculation:</strong></p>
<ul class="simple">
<li><p>Logits: 128 √ó 50,000 √ó 4 bytes = 25.6 MB</p></li>
<li><p>Softmax probabilities: 128 √ó 50,000 √ó 4 bytes = 25.6 MB</p></li>
<li><p>Log-softmax: 128 √ó 50,000 √ó 4 bytes = 25.6 MB</p></li>
</ul>
<p><strong>Total: 76.8 MB</strong> just for loss computation (before model activations!)</p>
<p><strong>Key insight</strong>: Memory scales as B√óC. Doubling vocabulary doubles loss computation memory. This is why large language models use techniques like sampled softmax - they literally can‚Äôt afford to materialize the full vocabulary every forward pass.</p>
<p><strong>Production solution</strong>: Switch to FP16 (cuts to 38.4 MB) or use hierarchical/sampled softmax (reduces C from 50,000 to ~1,000).</p>
</div>
<p><strong>Q2: Complexity Analysis - Softmax Bottleneck</strong></p>
<p>Your training profile shows: Forward pass 80ms, Loss computation 120ms, Backward pass 150ms. Your model has 1,000 output classes and batch size 64. Why is loss computation so expensive, and what‚Äôs the fix?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Problem</strong>: Loss taking 120ms (34% of iteration time) is unusually high. Normal ratio is 5-10%.</p>
<p><strong>Root cause</strong>: CrossEntropyLoss is O(B√óC). With B=64 and C=1,000, that‚Äôs 64,000 exp/log operations. If implemented naively in Python loops (not vectorized), this becomes a bottleneck.</p>
<p><strong>Diagnosis steps</strong>:</p>
<ol class="arabic simple">
<li><p>Profile within loss: Is <code class="docutils literal notranslate"><span class="pre">log_softmax</span></code> the bottleneck? (Likely yes)</p></li>
<li><p>Check vectorization: Are you using NumPy broadcasting or Python loops?</p></li>
<li><p>Check batch size: Is B=64 too small to utilize vectorization?</p></li>
</ol>
<p><strong>Fixes</strong>:</p>
<ul class="simple">
<li><p><strong>Immediate</strong>: Ensure you‚Äôre using vectorized NumPy ops (not loops)</p></li>
<li><p><strong>Better</strong>: Use PyTorch with CUDA - GPU acceleration gives 10-50√ó speedup</p></li>
<li><p><strong>Advanced</strong>: For C&gt;10,000, use hierarchical softmax (reduces to O(B√ólog C))</p></li>
</ul>
<p><strong>Reality check</strong>: In optimized PyTorch on GPU, loss should be ~5ms for this size, not 120ms. Your implementation in pure Python/NumPy is expected to be slower, but vectorization is crucial.</p>
</div>
<p><strong>Q3: Numerical Stability - Why Log-Sum-Exp Matters</strong></p>
<p>Your model outputs logits <code class="docutils literal notranslate"><span class="pre">[50,</span> <span class="pre">100,</span> <span class="pre">150]</span></code>. Without the log-sum-exp trick, what happens when you compute softmax? With the trick, what values are actually computed?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Without the trick (naive softmax):</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>exp_vals = [exp(50), exp(100), exp(150)]
         = [5.2√ó10¬≤¬π, 2.7√ó10‚Å¥¬≥, 1.4√ó10‚Å∂‚Åµ]  # Last value overflows to inf!
softmax = exp_vals / sum(exp_vals)  # inf / inf = nan
</pre></div>
</div>
</div>
<p><strong>Result</strong>: NaN loss, training fails.</p>
<p><strong>With log-sum-exp trick:</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>max_val = 150
shifted = [50-150, 100-150, 150-150] = [-100, -50, 0]
exp_shifted = [exp(-100), exp(-50), exp(0)]
            = [3.7√ó10‚Åª‚Å¥‚Å¥, 1.9√ó10‚Åª¬≤¬≤, 1.0]  # All ‚â§ 1.0, safe!
sum_exp = 1.0 (others negligible)
log_sum_exp = log(1.0) = 0
log_softmax = shifted - log_sum_exp = [-100, -50, 0]
</pre></div>
</div>
<p><strong>Result</strong>: Valid log-probabilities, stable training.</p>
<p><strong>Key insight</strong>: Subtracting max makes largest value 0, so <code class="docutils literal notranslate"><span class="pre">exp(0)</span> <span class="pre">=</span> <span class="pre">1.0</span></code> is always safe. Smaller values underflow to 0, but that‚Äôs fine - they contribute negligibly anyway. This is why <strong>you must use log-sum-exp for any softmax computation</strong>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
**Q4: Loss Function Selection - Classification Problem**

You&#39;re building a medical diagnosis system with 5 disease categories. Should you use BinaryCrossEntropyLoss or CrossEntropyLoss? What if the categories aren&#39;t mutually exclusive (patient can have multiple diseases)?

```{admonition} Answer
:class: dropdown

**Case 1: Mutually exclusive diseases** (patient has exactly one)
- **Use**: CrossEntropyLoss
- **Model output**: Logits of shape (batch_size, 5)
- **Why**: Categories are mutually exclusive - softmax ensures probabilities sum to 1.0

**Case 2: Multi-label classification** (patient can have multiple diseases)
- **Use**: BinaryCrossEntropyLoss
- **Model output**: Probabilities of shape (batch_size, 5) after sigmoid
- **Why**: Each disease is an independent binary decision. Softmax would incorrectly force them to sum to 1.

**Example**:
```python
# ‚úÖ Mutually exclusive (one disease)
logits = Linear(features, 5)(x)  # Shape: (B, 5)
loss = CrossEntropyLoss()(logits, targets)  # targets: class index 0-4

# ‚úÖ Multi-label (can have multiple)
logits = Linear(features, 5)(x)  # Shape: (B, 5)
probs = sigmoid(logits)  # Independent probabilities
targets = Tensor([[1, 0, 1, 0, 0], ...])  # Binary labels for each disease
loss = BinaryCrossEntropyLoss()(probs, targets)
</pre></div>
</div>
<p><strong>Critical medical consideration</strong>: Multi-label is more realistic - patients often have comorbidities!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
**Q5: Batch Size Impact - Memory and Gradients**

You train with batch size 32, using 4GB GPU memory. You want to increase to batch size 128. Will memory usage be 16GB? What happens to the loss value and gradient quality?

```{admonition} Answer
:class: dropdown

**Memory usage**: Yes, approximately **16GB** (4√ó increase)
- Loss computation scales linearly: 4√ó batch ‚Üí 4√ó memory
- Activations scale linearly: 4√ó batch ‚Üí 4√ó memory
- Model parameters: Fixed (same regardless of batch size)

**Problem**: If your GPU only has 12GB, training will crash with OOM (out of memory).

**Loss value**: **Stays the same** (assuming similar data)
```python
# Both compute the mean over their batch:
batch_32_loss = mean(losses[:32])   # Average of 32 samples
batch_128_loss = mean(losses[:128]) # Average of 128 samples
# If data is similar, means are similar
</pre></div>
</div>
<p><strong>Gradient quality</strong>: <strong>Improves with larger batch</strong></p>
<ul class="simple">
<li><p>Batch 32: High variance, noisy gradient estimates</p></li>
<li><p>Batch 128: Lower variance, smoother gradient, more stable convergence</p></li>
<li><p>Trade-off: More computation per step, fewer steps per epoch</p></li>
</ul>
<p><strong>Production solution - Gradient Accumulation</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate batch_size=128 with only batch_size=32 memory:</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>  <span class="c1"># 4 micro-batches</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">32</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">32</span><span class="p">])</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Accumulate gradients</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update once with accumulated gradients (4√ó32 = 128 effective batch)</span>
</pre></div>
</div>
<p>This gives you the gradient quality of batch 128 with only the memory cost of batch 32!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
## Further Reading

For students who want to understand the academic foundations and explore deeper:

### Seminal Papers

- **Improving neural networks by preventing co-adaptation of feature detectors** - Hinton et al. (2012). Introduces dropout, but also discusses cross-entropy loss and its role in preventing overfitting. Understanding why cross-entropy works better than MSE for classification is fundamental. [arXiv:1207.0580](https://arxiv.org/abs/1207.0580)

- **Focal Loss for Dense Object Detection** - Lin et al. (2017). Addresses class imbalance by reshaping the loss curve to down-weight easy examples. Shows how loss function design directly impacts model performance on real problems. [arXiv:1708.02002](https://arxiv.org/abs/1708.02002)

- **When Does Label Smoothing Help?** - M√ºller et al. (2019). Analyzes why adding small noise to target labels (label smoothing) improves generalization. Demonstrates that loss function details matter beyond just basic formulation. [arXiv:1906.02629](https://arxiv.org/abs/1906.02629)

### Additional Resources

- **Tutorial**: [Understanding Cross-Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) - PyTorch documentation with mathematical details
- **Blog post**: &quot;The Softmax Function and Its Derivative&quot; - Excellent explanation of log-sum-exp trick and numerical stability
- **Textbook**: &quot;Deep Learning&quot; by Goodfellow, Bengio, and Courville - Chapter 5 covers loss functions and maximum likelihood

## What&#39;s Next

```{seealso} Coming Up: Module 05 - DataLoader

Build efficient data pipelines that handle batching, shuffling, and iteration over your datasets. DataLoader prepares your training data so that autograd and training loops can consume it efficiently.
</pre></div>
</div>
<p><strong>Preview - How Your Loss Functions Get Used in Future Modules:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>What It Does</p></th>
<th class="head"><p>Your Loss In Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>06: Autograd</strong></p></td>
<td><p>Automatic differentiation</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> computes gradients</p></td>
</tr>
<tr class="row-odd"><td><p><strong>07: Optimizers</strong></p></td>
<td><p>Parameter updates</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> uses loss gradients to improve weights</p></td>
</tr>
<tr class="row-even"><td><p><strong>08: Training</strong></p></td>
<td><p>Complete training loop</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">criterion(outputs,</span> <span class="pre">targets)</span></code> measures progress</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/04_losses/04_losses.ipynb">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/04_losses/04_losses.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="03_layers_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 03: Layers</p>
      </div>
    </a>
    <a class="right-next"
       href="05_dataloader_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Module 05: DataLoader</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#helper-functions">Helper Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions">Loss Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-output-shapes">Input/Output Shapes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-as-a-feedback-signal">Loss as a Feedback Signal</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error">Mean Squared Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy-loss">Cross-Entropy Loss</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-stability-in-loss-computation">Numerical Stability in Loss Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduction-strategies">Reduction Strategies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatch-in-cross-entropy">Shape Mismatch in Cross-Entropy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nan-loss-from-numerical-instability">NaN Loss from Numerical Instability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusing-logits-and-probabilities">Confusing Logits and Probabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-loss-functions-matter-at-scale">Why Loss Functions Matter at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>