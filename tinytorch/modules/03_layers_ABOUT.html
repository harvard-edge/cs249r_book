
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 03: Layers" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/03_layers_ABOUT.html" />
<meta property="og:site_name" content="Tinyüî•Torch" />
<meta property="og:description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: N..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="Tinyüî•Torch" />
<meta name="description" content="üöÄ Launch Binder Run interactively in your browser. Open in Binder ‚Üí üìÑ View Source Browse the source code on GitHub. View on GitHub ‚Üí üéß Audio Overview Listen to an AI-generated overview. Overview: N..." />

    <title>Module 03: Layers &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=101bb79e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";

const defaultStyle = document.createElement('style');
defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}
`;
document.head.appendChild(defaultStyle);

const fullscreenStyle = document.createElement('style');
fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
document.head.appendChild(fullscreenStyle);

// Detect if page has dark background
const isDarkTheme = () => {
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        return brightness < 128;
    }
    return false;
};

const load = async () => {
    await mermaid.run();

    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(load, 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(load, 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(load, 200);
        return;
    }

    const darkTheme = isDarkTheme();

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    const modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    const modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    let previousScrollOffset = [window.scrollX, window.scrollY];

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    const allButtons = [];

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });

        container.appendChild(fullscreenBtn);
        allButtons.push(fullscreenBtn);
    });

    // Update theme classes when theme changes
    const updateTheme = () => {
        const dark = isDarkTheme();
        allButtons.forEach(btn => {
            if (dark) {
                btn.classList.add('dark-theme');
            } else {
                btn.classList.remove('dark-theme');
            }
        });
        if (dark) {
            modal.classList.add('dark-theme');
            modalContent.classList.add('dark-theme');
            closeBtn.classList.add('dark-theme');
        } else {
            modal.classList.remove('dark-theme');
            modalContent.classList.remove('dark-theme');
            closeBtn.classList.remove('dark-theme');
        }
    };

    // Watch for theme changes
    const observer = new MutationObserver(updateTheme);
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    observer.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style']
    });
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/03_layers_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=0d27a1a4"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=ee94e95f"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/subscribe-modal.js?v=82641629"></script>
    <link rel="canonical" href="https://mlsysbook.ai/tinytorch/modules/03_layers_ABOUT.html" />
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Module 04: Losses" href="04_losses_ABOUT.html" />
    <link rel="prev" title="Module 02: Activations" href="02_activations_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="../_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://mlsysbook.ai" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mlsysbook" title="Support" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-heart fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Support</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-08)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_dataloader_ABOUT.html">05. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_autograd_ABOUT.html">06. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_optimizers_ABOUT.html">07. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_training_ABOUT.html">08. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (09-13)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_acceleration_ABOUT.html">17. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_memoization_ABOUT.html">18. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/03_layers_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 03: Layers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-base-class">Layer Base Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-layer">Linear Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-layer">Dropout Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-container">Sequential Container</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-linear-transformation">The Linear Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-initialization">Weight Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-management">Parameter Management</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-mechanics">Forward Pass Mechanics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-composition">Layer Composition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-and-computational-complexity">Memory and Computational Complexity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatch-in-layer-composition">Shape Mismatch in Layer Composition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-in-inference-mode">Dropout in Inference Mode</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-parameters">Missing Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialization-scale">Initialization Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-layers-matter-at-scale">Why Layers Matter at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-03-layers">
<h1>Module 03: Layers<a class="headerlink" href="#module-03-layers" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>FOUNDATION TIER</strong> | Difficulty: ‚óè‚óè‚óã‚óã | Time: 5-7 hours | Prerequisites: 01, 02</p>
<p><strong>Prerequisites: Modules 01 and 02</strong> means you have built:</p>
<ul class="simple">
<li><p>Tensor class with arithmetic, broadcasting, matrix multiplication, and shape manipulation</p></li>
<li><p>Activation functions (ReLU, Sigmoid, Tanh, Softmax) for introducing non-linearity</p></li>
<li><p>Understanding of element-wise operations and reductions</p></li>
</ul>
<p>If you can multiply tensors, apply activations, and understand shape transformations, you‚Äôre ready.</p>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run interactively in your browser.</p>
<p class="sd-card-text"><a href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F03_layers%2F03_layers.ipynb" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #f97316; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">Open in Binder ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìÑ View Source</div>
<p class="sd-card-text">Browse the source code on GitHub.</p>
<p class="sd-card-text"><a href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/03_layers/03_layers.py" target="_blank" style="display: flex; align-items: center; justify-content: center; width: 100%; height: 54px; margin-top: auto; background: #6b7280; color: white; text-align: center; text-decoration: none; border-radius: 27px; font-size: 14px; box-sizing: border-box;">View on GitHub ‚Üí</a></p>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üéß Audio Overview</div>
<p class="sd-card-text">Listen to an AI-generated overview.</p>
<audio controls style="width: 100%; height: 54px; margin-top: auto;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/03_layers.mp3" type="audio/mpeg">
</audio>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Neural network layers are the fundamental building blocks that transform data as it flows through a network. Each layer performs a specific computation: Linear layers apply learned transformations (<code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">xW</span> <span class="pre">+</span> <span class="pre">b</span></code>), while Dropout layers randomly zero elements for regularization. In this module, you‚Äôll build these essential components from scratch, gaining deep insight into how PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.Dropout</span></code> work under the hood.</p>
<p>Every neural network, from recognizing handwritten digits to translating languages, is built by stacking layers. The Linear layer learns which combinations of input features matter for the task at hand. Dropout prevents overfitting by forcing the network to not rely on any single neuron. Together, these layers enable multi-layer architectures that can learn complex patterns.</p>
<p>By the end, your layers will support parameter management, proper initialization, and seamless integration with the tensor and activation functions you built in previous modules.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> Linear layers with Xavier initialization and proper parameter management for gradient-based training</p></li>
<li><p><strong>Master</strong> the mathematical operation <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">xW</span> <span class="pre">+</span> <span class="pre">b</span></code> and understand how parameter counts scale with layer dimensions</p></li>
<li><p><strong>Understand</strong> memory usage patterns (parameter memory vs activation memory) and computational complexity of matrix operations</p></li>
<li><p><strong>Connect</strong> your implementation to production PyTorch patterns, including <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>, <code class="docutils literal notranslate"><span class="pre">nn.Dropout</span></code>, and parameter tracking</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Your Layer System&quot;
        A[&quot;Layer Base Class&lt;br/&gt;forward(), parameters()&quot;]
        B[&quot;Linear Layer&lt;br/&gt;y = xW + b&quot;]
        C[&quot;Dropout Layer&lt;br/&gt;regularization&quot;]
        D[&quot;Sequential Container&lt;br/&gt;layer composition&quot;]
    end

    A --&gt; B
    A --&gt; C
    D --&gt; B
    D --&gt; C

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
    </pre><figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">Your Layer System</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Part</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Layer</span></code> base class with <code class="docutils literal notranslate"><span class="pre">forward()</span></code>, <code class="docutils literal notranslate"><span class="pre">__call__()</span></code>, <code class="docutils literal notranslate"><span class="pre">parameters()</span></code></p></td>
<td><p>Consistent interface for all layers</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer with Xavier initialization</p></td>
<td><p>Learned transformation <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">xW</span> <span class="pre">+</span> <span class="pre">b</span></code></p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Dropout</span></code> with training/inference modes</p></td>
<td><p>Regularization through random masking</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Sequential</span></code> container for layer composition</p></td>
<td><p>Chaining layers together</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Building a multi-layer network</span>
<span class="n">layer1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Manual composition for explicit data flow</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>Automatic gradient computation (that‚Äôs Module 06: Autograd)</p></li>
<li><p>Parameter optimization (that‚Äôs Module 07: Optimizers)</p></li>
<li><p>Hundreds of layer types (PyTorch has Conv2d, LSTM, Attention - you‚Äôll build Linear and Dropout)</p></li>
<li><p>Automatic training/eval mode switching (PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">model.train()</span></code> - you‚Äôll manually pass <code class="docutils literal notranslate"><span class="pre">training</span></code> flag)</p></li>
</ul>
<p><strong>You are building the core building blocks.</strong> Training loops and optimizers come later.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section provides a quick reference for the Layer classes you‚Äôll build. Think of it as your cheat sheet while implementing and debugging. Each class is documented with its signature and expected behavior.</p>
<section id="layer-base-class">
<h3>Layer Base Class<a class="headerlink" href="#layer-base-class" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Layer</span><span class="p">()</span>
</pre></div>
</div>
<p>Base class providing consistent interface for all neural network layers. All layers inherit from this and implement <code class="docutils literal notranslate"><span class="pre">forward()</span></code> and <code class="docutils literal notranslate"><span class="pre">parameters()</span></code>.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(x)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Compute layer output (must override)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__call__</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">__call__(x)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Makes layer callable like a function</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">parameters()</span> <span class="pre">-&gt;</span> <span class="pre">List[Tensor]</span></code></p></td>
<td><p>Returns list of trainable parameters</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="linear-layer">
<h3>Linear Layer<a class="headerlink" href="#linear-layer" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Linear (fully connected) layer implementing <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">xW</span> <span class="pre">+</span> <span class="pre">b</span></code>.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">in_features</span></code>: Number of input features</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_features</span></code>: Number of output features</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias</span></code>: Whether to include bias term (default: True)</p></li>
</ul>
<p><strong>Attributes:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span></code>: Tensor of shape <code class="docutils literal notranslate"><span class="pre">(in_features,</span> <span class="pre">out_features)</span></code> with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias</span></code>: Tensor of shape <code class="docutils literal notranslate"><span class="pre">(out_features,)</span></code> with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> (or None)</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(x)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Apply linear transformation <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">xW</span> <span class="pre">+</span> <span class="pre">b</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">parameters()</span> <span class="pre">-&gt;</span> <span class="pre">List[Tensor]</span></code></p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">[weight,</span> <span class="pre">bias]</span></code> or <code class="docutils literal notranslate"><span class="pre">[weight]</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="dropout-layer">
<h3>Dropout Layer<a class="headerlink" href="#dropout-layer" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<p>Dropout layer for regularization. During training, randomly zeros elements with probability <code class="docutils literal notranslate"><span class="pre">p</span></code> and scales survivors by <code class="docutils literal notranslate"><span class="pre">1/(1-p)</span></code>. During inference, passes input unchanged.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">p</span></code>: Probability of zeroing each element (0.0 = no dropout, 1.0 = zero everything)</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(x,</span> <span class="pre">training=True)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Apply dropout during training, passthrough during inference</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">parameters()</span> <span class="pre">-&gt;</span> <span class="pre">List[Tensor]</span></code></p></td>
<td><p>Returns empty list (no trainable parameters)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="sequential-container">
<h3>Sequential Container<a class="headerlink" href="#sequential-container" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
<p>Container that chains layers together sequentially. Provides convenient way to compose multiple layers.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">forward(x)</span> <span class="pre">-&gt;</span> <span class="pre">Tensor</span></code></p></td>
<td><p>Forward pass through all layers in order</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">parameters</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">parameters()</span> <span class="pre">-&gt;</span> <span class="pre">List[Tensor]</span></code></p></td>
<td><p>Collects all parameters from all layers</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental ideas you need to understand neural network layers deeply. These concepts apply to every ML framework, not just TinyTorch, so mastering them here will serve you throughout your career.</p>
<section id="the-linear-transformation">
<h3>The Linear Transformation<a class="headerlink" href="#the-linear-transformation" title="Link to this heading">#</a></h3>
<p>Linear layers implement the mathematical operation <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">xW</span> <span class="pre">+</span> <span class="pre">b</span></code>, where <code class="docutils literal notranslate"><span class="pre">x</span></code> is your input, <code class="docutils literal notranslate"><span class="pre">W</span></code> is a weight matrix you learn, <code class="docutils literal notranslate"><span class="pre">b</span></code> is a bias vector you learn, and <code class="docutils literal notranslate"><span class="pre">y</span></code> is your output. This simple formula is the foundation of neural networks.</p>
<p>Think of the weight matrix as a feature detector. Each column of <code class="docutils literal notranslate"><span class="pre">W</span></code> learns to recognize a particular pattern in the input. When you multiply input <code class="docutils literal notranslate"><span class="pre">x</span></code> by <code class="docutils literal notranslate"><span class="pre">W</span></code>, you‚Äôre asking: ‚ÄúHow much of each learned pattern appears in this input?‚Äù The bias <code class="docutils literal notranslate"><span class="pre">b</span></code> shifts the output, providing a baseline independent of the input.</p>
<p>Consider recognizing handwritten digits. A flattened 28√ó28 image has 784 pixels. A Linear layer transforming 784 features to 10 classes creates a weight matrix of shape <code class="docutils literal notranslate"><span class="pre">(784,</span> <span class="pre">10)</span></code>. Each of the 10 columns learns which combination of those 784 pixels indicates a particular digit. The network discovers these patterns through training.</p>
<p>Here‚Äôs how your implementation performs this transformation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass through linear layer.&quot;&quot;&quot;</span>
    <span class="c1"># Linear transformation: y = xW</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="c1"># Add bias if present</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>The elegance is in the simplicity. Matrix multiplication handles all the feature combinations in one operation, and broadcasting handles adding the bias vector to every sample in the batch. This single method enables every linear transformation in neural networks.</p>
</section>
<section id="weight-initialization">
<h3>Weight Initialization<a class="headerlink" href="#weight-initialization" title="Link to this heading">#</a></h3>
<p>How you initialize weights determines whether your network can learn at all. Initialize too small and gradients vanish, making learning impossibly slow. Initialize too large and gradients explode, making training unstable. The sweet spot ensures stable gradient flow through the network.</p>
<p>Xavier (Glorot) initialization solves this by scaling weights based on the number of inputs. For a layer with <code class="docutils literal notranslate"><span class="pre">in_features</span></code> inputs, Xavier uses scale <code class="docutils literal notranslate"><span class="pre">sqrt(1/in_features)</span></code>. This keeps the variance of activations roughly constant as data flows through layers, preventing vanishing or exploding gradients.</p>
<p>Here‚Äôs your initialization code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize linear layer with proper weight initialization.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>

    <span class="c1"># Xavier/Glorot initialization for stable gradients</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">XAVIER_SCALE_FACTOR</span> <span class="o">/</span> <span class="n">in_features</span><span class="p">)</span>
    <span class="n">weight_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">weight_data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Initialize bias to zeros or None</span>
    <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
        <span class="n">bias_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">bias_data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> flag marks these tensors for gradient computation in Module 06. Even though you haven‚Äôt built autograd yet, your layers are already prepared for it. Bias starts at zero because the weight initialization already handles the scale, and zero is a neutral starting point for per-class adjustments.</p>
<p>For Linear(1000, 10), the scale is <code class="docutils literal notranslate"><span class="pre">sqrt(1/1000)</span> <span class="pre">‚âà</span> <span class="pre">0.032</span></code>. For Linear(10, 1000), the scale is <code class="docutils literal notranslate"><span class="pre">sqrt(1/10)</span> <span class="pre">‚âà</span> <span class="pre">0.316</span></code>. Layers with more inputs get smaller initial weights because each input contributes to the output, and you want their combined effect to remain stable.</p>
</section>
<section id="parameter-management">
<h3>Parameter Management<a class="headerlink" href="#parameter-management" title="Link to this heading">#</a></h3>
<p>Parameters are tensors that need gradients and optimizer updates. Your Linear layer manages two parameters: weights and biases. The <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> method collects them into a list that optimizers can iterate over.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return list of trainable parameters.&quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">params</span>
</pre></div>
</div>
<p>This simple method enables powerful workflows. When you build a multi-layer network, you can collect all parameters from all layers and pass them to an optimizer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">all_params</span> <span class="o">=</span> <span class="n">layer1</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="o">+</span> <span class="n">layer2</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
<span class="c1"># In Module 07, you&#39;ll pass all_params to optimizer.step()</span>
</pre></div>
</div>
<p>Each Linear layer independently manages its own parameters. The Sequential container extends this pattern by collecting parameters from all its contained layers, enabling hierarchical composition.</p>
</section>
<section id="forward-pass-mechanics">
<h3>Forward Pass Mechanics<a class="headerlink" href="#forward-pass-mechanics" title="Link to this heading">#</a></h3>
<p>The forward pass transforms input data through the layer‚Äôs computation. Every layer implements <code class="docutils literal notranslate"><span class="pre">forward()</span></code>, and the base class provides <code class="docutils literal notranslate"><span class="pre">__call__()</span></code> to make layers callable like functions. This matches PyTorch‚Äôs design exactly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Allow layer to be called like a function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>This lets you write <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">layer(input)</span></code> instead of <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">=</span> <span class="pre">layer.forward(input)</span></code>. The difference seems minor, but it‚Äôs a powerful abstraction. The <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method can add hooks, logging, or mode switching (like <code class="docutils literal notranslate"><span class="pre">training</span></code> vs <code class="docutils literal notranslate"><span class="pre">eval</span></code>), while <code class="docutils literal notranslate"><span class="pre">forward()</span></code> focuses purely on the computation.</p>
<p>For Dropout, the forward pass depends on whether you‚Äôre training or performing inference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass through dropout layer.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">training</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="n">DROPOUT_MIN_PROB</span><span class="p">:</span>
        <span class="c1"># During inference or no dropout, pass through unchanged</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="n">DROPOUT_MAX_PROB</span><span class="p">:</span>
        <span class="c1"># Drop everything (preserve requires_grad for gradient flow)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="c1"># During training, apply dropout</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span>

    <span class="c1"># Create random mask: True where we keep elements</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">keep_prob</span>

    <span class="c1"># Apply mask and scale using Tensor operations to preserve gradients</span>
    <span class="n">mask_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">keep_prob</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Use Tensor operations: x * mask * scale</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">mask_tensor</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>The key insight is the scaling factor <code class="docutils literal notranslate"><span class="pre">1/(1-p)</span></code>. If you drop 50% of neurons, the survivors need to be scaled by 2.0 to maintain the same expected value. This ensures that during inference (when no dropout is applied), the output magnitudes match training expectations.</p>
</section>
<section id="layer-composition">
<h3>Layer Composition<a class="headerlink" href="#layer-composition" title="Link to this heading">#</a></h3>
<p>Neural networks are built by chaining layers together. Data flows through each layer in sequence, with each transformation building on the previous one. Your Sequential container captures this pattern:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Sequential</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Container that chains layers together sequentially.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">layers</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize with layers to chain together.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass through all layers sequentially.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Collect all parameters from all layers.&quot;&quot;&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">params</span>
</pre></div>
</div>
<p>This simple container demonstrates a powerful principle: composition. Complex architectures emerge from simple building blocks. A 3-layer network is just three Linear layers with activations and dropout in between:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
    <span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The forward pass chains computations, and <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> collects all trainable tensors. This composability is a hallmark of good system design.</p>
</section>
<section id="memory-and-computational-complexity">
<h3>Memory and Computational Complexity<a class="headerlink" href="#memory-and-computational-complexity" title="Link to this heading">#</a></h3>
<p>Understanding the memory and computational costs of layers is essential for building efficient networks. Linear layers dominate both parameter memory and computation time in fully connected architectures.</p>
<p>Parameter memory for a Linear layer is straightforward: <code class="docutils literal notranslate"><span class="pre">in_features</span> <span class="pre">√ó</span> <span class="pre">out_features</span> <span class="pre">√ó</span> <span class="pre">4</span> <span class="pre">bytes</span></code> for weights, plus <code class="docutils literal notranslate"><span class="pre">out_features</span> <span class="pre">√ó</span> <span class="pre">4</span> <span class="pre">bytes</span></code> for bias (assuming float32). For Linear(784, 256):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Weights: 784 √ó 256 √ó 4 = 802,816 bytes ‚âà 803 KB
Bias:    256 √ó 4 = 1,024 bytes ‚âà 1 KB
Total:   ‚âà 804 KB
</pre></div>
</div>
<p>Activation memory depends on batch size. For batch size 32 and the same layer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Input:   32 √ó 784 √ó 4 = 100,352 bytes ‚âà 100 KB
Output:  32 √ó 256 √ó 4 = 32,768 bytes ‚âà 33 KB
</pre></div>
</div>
<p>The computational cost of the forward pass is dominated by matrix multiplication. For input shape <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">in_features)</span></code> and weight shape <code class="docutils literal notranslate"><span class="pre">(in_features,</span> <span class="pre">out_features)</span></code>, the operation requires <code class="docutils literal notranslate"><span class="pre">batch</span> <span class="pre">√ó</span> <span class="pre">in_features</span> <span class="pre">√ó</span> <span class="pre">out_features</span></code> multiplications and the same number of additions. Bias addition is just <code class="docutils literal notranslate"><span class="pre">batch</span> <span class="pre">√ó</span> <span class="pre">out_features</span></code> additions, negligible compared to matrix multiplication.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Complexity</p></th>
<th class="head"><p>Memory</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Linear forward</p></td>
<td><p>O(batch √ó in √ó out)</p></td>
<td><p>O(batch √ó (in + out)) activations</p></td>
</tr>
<tr class="row-odd"><td><p>Dropout forward</p></td>
<td><p>O(batch √ó features)</p></td>
<td><p>O(batch √ó features) mask</p></td>
</tr>
<tr class="row-even"><td><p>Parameter storage</p></td>
<td><p>O(in √ó out)</p></td>
<td><p>O(in √ó out) weights</p></td>
</tr>
</tbody>
</table>
</div>
<p>For a 3-layer network (784‚Üí256‚Üí128‚Üí10) with batch size 32:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Layer 1: 32 √ó 784 √ó 256 = 6,422,528 FLOPs
Layer 2: 32 √ó 256 √ó 128 = 1,048,576 FLOPs
Layer 3: 32 √ó 128 √ó 10  = 40,960 FLOPs
Total:   ‚âà 7.5 million FLOPs per forward pass
</pre></div>
</div>
<p>The first layer dominates because it has the largest input dimension. This is why production networks often use dimension reduction early to save computation in later layers.</p>
</section>
</section>
<section id="common-errors">
<h2>Common Errors<a class="headerlink" href="#common-errors" title="Link to this heading">#</a></h2>
<p>These are the errors you‚Äôll encounter most often when working with layers. Understanding why they happen will save you hours of debugging, both in this module and throughout your ML career.</p>
<section id="shape-mismatch-in-layer-composition">
<h3>Shape Mismatch in Layer Composition<a class="headerlink" href="#shape-mismatch-in-layer-composition" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">Cannot</span> <span class="pre">perform</span> <span class="pre">matrix</span> <span class="pre">multiplication:</span> <span class="pre">(32,</span> <span class="pre">128)</span> <span class="pre">&#64;</span> <span class="pre">(256,</span> <span class="pre">10).</span> <span class="pre">Inner</span> <span class="pre">dimensions</span> <span class="pre">must</span> <span class="pre">match:</span> <span class="pre">128</span> <span class="pre">‚â†</span> <span class="pre">256</span></code></p>
<p>This happens when you chain layers with incompatible dimensions. If <code class="docutils literal notranslate"><span class="pre">layer1</span></code> outputs 128 features but <code class="docutils literal notranslate"><span class="pre">layer2</span></code> expects 256 input features, the matrix multiplication in <code class="docutils literal notranslate"><span class="pre">layer2</span></code> fails.</p>
<p><strong>Fix</strong>: Ensure output features of one layer match input features of the next:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>  <span class="c1"># Outputs 128 features</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>   <span class="c1"># Expects 128 input features ‚úì</span>
</pre></div>
</div>
</section>
<section id="dropout-in-inference-mode">
<h3>Dropout in Inference Mode<a class="headerlink" href="#dropout-in-inference-mode" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Test accuracy is much lower than training accuracy, but loss curves suggest good learning</p>
<p><strong>Cause</strong>: You‚Äôre applying dropout during inference. Dropout should only zero elements during training. During inference, all neurons must be active.</p>
<p><strong>Fix</strong>: Always pass <code class="docutils literal notranslate"><span class="pre">training=False</span></code> during evaluation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Evaluation</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="missing-parameters">
<h3>Missing Parameters<a class="headerlink" href="#missing-parameters" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Optimizer has no parameters to update, or parameter count is wrong</p>
<p><strong>Cause</strong>: Your <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> method doesn‚Äôt return all trainable tensors, or you forgot to set <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>.</p>
<p><strong>Fix</strong>: Verify all tensors with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> are returned:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">params</span>  <span class="c1"># Must include all trainable tensors</span>
</pre></div>
</div>
</section>
<section id="initialization-scale">
<h3>Initialization Scale<a class="headerlink" href="#initialization-scale" title="Link to this heading">#</a></h3>
<p><strong>Error</strong>: Loss becomes NaN within a few iterations, or gradients vanish immediately</p>
<p><strong>Cause</strong>: Weights initialized too large (exploding gradients) or too small (vanishing gradients).</p>
<p><strong>Fix</strong>: Use Xavier initialization with proper scale:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">in_features</span><span class="p">)</span>  <span class="c1"># Not just random()!</span>
<span class="n">weight_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
</pre></div>
</div>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-pytorch">
<h3>Your Implementation vs. PyTorch<a class="headerlink" href="#your-implementation-vs-pytorch" title="Link to this heading">#</a></h3>
<p>Your TinyTorch layers and PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.Dropout</span></code> share the same conceptual design. The differences are in implementation details: PyTorch uses C++ for speed, supports GPU acceleration, and provides hundreds of specialized layer types. But the core abstractions are identical.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Backend</strong></p></td>
<td><p>NumPy (Python)</p></td>
<td><p>C++/CUDA</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Initialization</strong></p></td>
<td><p>Xavier manual</p></td>
<td><p>Multiple schemes (<code class="docutils literal notranslate"><span class="pre">init.xavier_uniform_</span></code>)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Parameter Management</strong></p></td>
<td><p>Manual <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> list</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> base class with auto-registration</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Training Mode</strong></p></td>
<td><p>Manual <code class="docutils literal notranslate"><span class="pre">training</span></code> flag</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model.train()</span></code> / <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code> state</p></td>
</tr>
<tr class="row-even"><td><p><strong>Layer Types</strong></p></td>
<td><p>Linear, Dropout</p></td>
<td><p>100+ layer types (Conv, LSTM, Attention, etc.)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>GPU Support</strong></p></td>
<td><p>‚úó CPU only</p></td>
<td><p>‚úì CUDA, Metal, ROCm</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows equivalent layer operations in TinyTorch and PyTorch. Notice how closely the APIs mirror each other.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Sequential</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.activations</span><span class="w"> </span><span class="kn">import</span> <span class="n">ReLU</span>

<span class="c1"># Build layers</span>
<span class="n">layer1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">()</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Manual composition</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Or use Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">ReLU</span><span class="p">(),</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Collect parameters</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
‚ö° PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Build layers</span>
<span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Manual composition</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Automatically uses model.training state</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Or use Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Collect parameters</span>
<span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs walk through each difference:</p>
<ul class="simple">
<li><p><strong>Line 1-2 (Import)</strong>: Both frameworks provide layers in a dedicated module. TinyTorch uses <code class="docutils literal notranslate"><span class="pre">tinytorch.core.layers</span></code>; PyTorch uses <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>.</p></li>
<li><p><strong>Line 4-7 (Layer Creation)</strong>: Identical API. Both use <code class="docutils literal notranslate"><span class="pre">Linear(in_features,</span> <span class="pre">out_features)</span></code> and <code class="docutils literal notranslate"><span class="pre">Dropout(p)</span></code>.</p></li>
<li><p><strong>Line 9-13 (Manual Composition)</strong>: TinyTorch requires explicit <code class="docutils literal notranslate"><span class="pre">training=True</span></code> flag for Dropout; PyTorch uses global model state (<code class="docutils literal notranslate"><span class="pre">model.train()</span></code>).</p></li>
<li><p><strong>Line 15-19 (Sequential)</strong>: Identical pattern for composing layers into a container.</p></li>
<li><p><strong>Line 22 (Parameters)</strong>: Both use <code class="docutils literal notranslate"><span class="pre">.parameters()</span></code> method to collect all trainable tensors. PyTorch returns a generator; TinyTorch returns a list.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>Layer initialization API, forward pass mechanics, and parameter collection patterns. When you debug PyTorch shape errors or parameter counts, you‚Äôll understand exactly what‚Äôs happening because you built the same abstractions.</p>
</div>
</section>
<section id="why-layers-matter-at-scale">
<h3>Why Layers Matter at Scale<a class="headerlink" href="#why-layers-matter-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate why layer design matters, consider the scale of modern ML systems:</p>
<ul class="simple">
<li><p><strong>GPT-3</strong>: 175 billion parameters across 96 Linear layers (each layer transforming 12,288 features) = <strong>350 GB</strong> of parameter memory</p></li>
<li><p><strong>ResNet-50</strong>: 25.5 million parameters with 50 convolutional and linear layers = <strong>100 MB</strong> of parameter memory</p></li>
<li><p><strong>BERT-Base</strong>: 110 million parameters with 12 transformer blocks (each containing multiple Linear layers) = <strong>440 MB</strong> of parameter memory</p></li>
</ul>
<p>Every Linear layer in these architectures follows the same <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">xW</span> <span class="pre">+</span> <span class="pre">b</span></code> pattern you implemented. Understanding parameter counts, memory scaling, and initialization strategies isn‚Äôt just academic; it‚Äôs essential for building and debugging real ML systems. When GPT-3 fails to converge, engineers debug the same weight initialization and layer composition issues you encountered in this module.</p>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions. They‚Äôre designed to build intuition for the performance characteristics you‚Äôll encounter in production ML.</p>
<p><strong>Q1: Parameter Scaling</strong></p>
<p>A Linear layer has <code class="docutils literal notranslate"><span class="pre">in_features=784</span></code> and <code class="docutils literal notranslate"><span class="pre">out_features=256</span></code>. How many parameters does it have? If you double <code class="docutils literal notranslate"><span class="pre">out_features</span></code> to 512, how many parameters now?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Original</strong>: 784 √ó 256 + 256 = 200,960 parameters</p>
<p><strong>Doubled</strong>: 784 √ó 512 + 512 = 401,920 parameters</p>
<p>Doubling <code class="docutils literal notranslate"><span class="pre">out_features</span></code> approximately doubles the parameter count because weights dominate (200,704 vs 401,408 for weights alone). This shows parameter count scales linearly with layer width.</p>
<p><strong>Memory</strong>: 200,960 √ó 4 = 803,840 bytes ‚âà 804 KB (original) vs 401,920 √ó 4 = 1,607,680 bytes ‚âà 1.6 MB (doubled)</p>
</div>
<p><strong>Q2: Multi-layer Memory</strong></p>
<p>A 3-layer network has architecture 784‚Üí256‚Üí128‚Üí10. Calculate total parameter count and memory usage (assume float32).</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Layer 1</strong>: 784 √ó 256 + 256 = 200,960 parameters
<strong>Layer 2</strong>: 256 √ó 128 + 128 = 32,896 parameters
<strong>Layer 3</strong>: 128 √ó 10 + 10 = 1,290 parameters</p>
<p><strong>Total</strong>: 235,146 parameters</p>
<p><strong>Memory</strong>: 235,146 √ó 4 = 940,584 bytes ‚âà 940 KB</p>
<p>This is parameter memory only. Add activation memory for batch processing: for batch size 32, you need space for intermediate tensors at each layer (32√ó784, 32√ó256, 32√ó128, 32√ó10 = approximately 260 KB more).</p>
</div>
<p><strong>Q3: Dropout Scaling</strong></p>
<p>Why do we scale surviving values by <code class="docutils literal notranslate"><span class="pre">1/(1-p)</span></code> during training? What happens if we don‚Äôt scale?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>With scaling</strong>: Expected value of output matches input. If <code class="docutils literal notranslate"><span class="pre">p=0.5</span></code>, half the neurons survive and are scaled by 2.0, so <code class="docutils literal notranslate"><span class="pre">E[output]</span> <span class="pre">=</span> <span class="pre">0.5</span> <span class="pre">√ó</span> <span class="pre">0</span> <span class="pre">+</span> <span class="pre">0.5</span> <span class="pre">√ó</span> <span class="pre">2x</span> <span class="pre">=</span> <span class="pre">x</span></code>.</p>
<p><strong>Without scaling</strong>: Expected value is halved. <code class="docutils literal notranslate"><span class="pre">E[output]</span> <span class="pre">=</span> <span class="pre">0.5</span> <span class="pre">√ó</span> <span class="pre">0</span> <span class="pre">+</span> <span class="pre">0.5</span> <span class="pre">√ó</span> <span class="pre">x</span> <span class="pre">=</span> <span class="pre">0.5x</span></code>. During inference (no dropout), output would be <code class="docutils literal notranslate"><span class="pre">x</span></code>, creating a mismatch.</p>
<p><strong>Result</strong>: Network sees different magnitude activations during training vs inference, leading to poor test performance. Scaling ensures consistent magnitudes.</p>
</div>
<p><strong>Q4: Computational Bottleneck</strong></p>
<p>For Linear layer forward pass <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">xW</span> <span class="pre">+</span> <span class="pre">b</span></code>, which operation dominates: matrix multiply or bias addition?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Matrix multiply</strong>: O(batch √ó in_features √ó out_features) operations
<strong>Bias addition</strong>: O(batch √ó out_features) operations</p>
<p>For Linear(784, 256) with batch size 32:</p>
<ul class="simple">
<li><p><strong>Matmul</strong>: 32 √ó 784 √ó 256 = 6,422,528 operations</p></li>
<li><p><strong>Bias</strong>: 32 √ó 256 = 8,192 operations</p></li>
</ul>
<p>Matrix multiply dominates by ~783x. This is why optimizing matmul (using BLAS, GPU kernels) is critical for neural network performance.</p>
</div>
<p><strong>Q5: Initialization Impact</strong></p>
<p>What happens if you initialize all weights to zero? To the same non-zero value?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>All zeros</strong>: Network can‚Äôt learn. All neurons compute identical outputs, receive identical gradients, and update identically. Symmetry is never broken. Training is stuck.</p>
<p><strong>Same non-zero value (e.g., all 1s)</strong>: Same problem - symmetry. All neurons remain identical throughout training. You need randomness to break symmetry.</p>
<p><strong>Xavier initialization</strong>: Random values scaled by <code class="docutils literal notranslate"><span class="pre">sqrt(1/in_features)</span></code> break symmetry AND maintain stable gradient variance. This is why proper initialization is essential for learning.</p>
</div>
<p><strong>Q6: Batch Size vs Throughput</strong></p>
<p>From your timing analysis, batch size 32 processes 10,000 samples/sec, while batch size 1 processes 800 samples/sec. Why is batching faster?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Overhead amortization</strong>: Setting up matrix operations has fixed cost per call. With batch=1, you pay this cost for every sample. With batch=32, you pay once for 32 samples.</p>
<p><strong>Vectorization</strong>: Modern CPUs/GPUs process vectors efficiently. Matrix operations on larger matrices utilize SIMD instructions and better cache locality.</p>
<p><strong>Throughput calculation</strong>:</p>
<ul class="simple">
<li><p>Batch=1: 800 samples/sec means each forward pass takes ~1.25ms</p></li>
<li><p>Batch=32: 10,000 samples/sec means each forward pass takes ~3.2ms for 32 samples = 0.1ms per sample</p></li>
</ul>
<p>Batching achieves 12.5x better per-sample performance by better utilizing hardware.</p>
<p><strong>Trade-off</strong>: Larger batches increase latency (time to process one sample) but dramatically improve throughput (samples processed per second).</p>
</div>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>For students who want to understand the academic foundations and mathematical underpinnings of neural network layers:</p>
<section id="seminal-papers">
<h3>Seminal Papers<a class="headerlink" href="#seminal-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Understanding the difficulty of training deep feedforward neural networks</strong> - Glorot and Bengio (2010). Introduces Xavier/Glorot initialization and analyzes why proper weight scaling matters for gradient flow. The foundation for modern initialization schemes. <a class="reference external" href="http://proceedings.mlr.press/v9/glorot10a.html">PMLR</a></p></li>
<li><p><strong>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</strong> - He et al. (2015). Introduces He initialization tailored for ReLU activations. Shows how initialization schemes must match activation functions for optimal training. <a class="reference external" href="https://arxiv.org/abs/1502.01852">arXiv:1502.01852</a></p></li>
<li><p><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong> - Srivastava et al. (2014). The original dropout paper demonstrating how random neuron dropping prevents overfitting. Includes theoretical analysis and extensive empirical validation. <a class="reference external" href="https://jmlr.org/papers/v15/srivastava14a.html">JMLR</a></p></li>
</ul>
</section>
<section id="additional-resources">
<h3>Additional Resources<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Textbook</strong>: ‚ÄúDeep Learning‚Äù by Goodfellow, Bengio, and Courville - Chapter 6 covers feedforward networks and linear layers in detail</p></li>
<li><p><strong>Documentation</strong>: <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">PyTorch nn.Linear</a> - See how production frameworks implement the same concepts</p></li>
<li><p><strong>Blog Post</strong>: ‚ÄúA Recipe for Training Neural Networks‚Äù by Andrej Karpathy - Practical advice on initialization, architecture design, and debugging</p></li>
</ul>
</section>
</section>
<section id="whats-next">
<h2>What‚Äôs Next<a class="headerlink" href="#whats-next" title="Link to this heading">#</a></h2>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Coming Up: Module 04 - Losses</p>
<p>Implement loss functions (MSELoss, CrossEntropyLoss) that measure prediction error. You‚Äôll combine your layers with loss computation to evaluate how wrong your model is - the foundation for learning.</p>
</div>
<p><strong>Preview - How Your Layers Get Used in Future Modules:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>What It Does</p></th>
<th class="head"><p>Your Layers In Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>04: Losses</strong></p></td>
<td><p>Measure prediction error</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">CrossEntropyLoss()(model(x),</span> <span class="pre">y)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>06: Autograd</strong></p></td>
<td><p>Compute gradients</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> fills <code class="docutils literal notranslate"><span class="pre">layer.weight.grad</span></code></p></td>
</tr>
<tr class="row-even"><td><p><strong>07: Optimizers</strong></p></td>
<td><p>Update parameters</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> uses <code class="docutils literal notranslate"><span class="pre">layer.parameters()</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/03_layers/03_layers.ipynb">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/03_layers/03_layers.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02_activations_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 02: Activations</p>
      </div>
    </a>
    <a class="right-next"
       href="04_losses_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Module 04: Losses</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-base-class">Layer Base Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-layer">Linear Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-layer">Dropout Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-container">Sequential Container</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-linear-transformation">The Linear Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-initialization">Weight Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-management">Parameter Management</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-mechanics">Forward Pass Mechanics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-composition">Layer Composition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-and-computational-complexity">Memory and Computational Complexity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-errors">Common Errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shape-mismatch-in-layer-composition">Shape Mismatch in Layer Composition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout-in-inference-mode">Dropout in Inference Mode</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#missing-parameters">Missing Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialization-scale">Initialization Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-layers-matter-at-scale">Why Layers Matter at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>