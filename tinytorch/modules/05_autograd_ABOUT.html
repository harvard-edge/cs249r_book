
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Module 05: Autograd" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://mlsysbook.ai/tinytorch/modules/05_autograd_ABOUT.html" />
<meta property="og:site_name" content="Tinyüî•Torch" />
<meta property="og:description" content="üöÄ Launch Binder Run interactively in your browser. No setup required. https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F05_autograd%2F05_autograd.ipynb üìÑ View S..." />
<meta property="og:image" content="https://mlsysbook.ai/tinytorch/_static/logos/logo-tinytorch.png" />
<meta property="og:image:alt" content="Tinyüî•Torch" />
<meta name="description" content="üöÄ Launch Binder Run interactively in your browser. No setup required. https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F05_autograd%2F05_autograd.ipynb üìÑ View S..." />

    <title>Module 05: Autograd &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1a2c4ab3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";

const defaultStyle = document.createElement('style');
defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}
`;
document.head.appendChild(defaultStyle);

const fullscreenStyle = document.createElement('style');
fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
document.head.appendChild(fullscreenStyle);

// Detect if page has dark background
const isDarkTheme = () => {
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        return brightness < 128;
    }
    return false;
};

const load = async () => {
    await mermaid.run();

    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(load, 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(load, 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(load, 200);
        return;
    }

    const darkTheme = isDarkTheme();

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    const modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    const modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    let previousScrollOffset = [window.scrollX, window.scrollY];

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    const allButtons = [];

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });

        container.appendChild(fullscreenBtn);
        allButtons.push(fullscreenBtn);
    });

    // Update theme classes when theme changes
    const updateTheme = () => {
        const dark = isDarkTheme();
        allButtons.forEach(btn => {
            if (dark) {
                btn.classList.add('dark-theme');
            } else {
                btn.classList.remove('dark-theme');
            }
        });
        if (dark) {
            modal.classList.add('dark-theme');
            modalContent.classList.add('dark-theme');
            closeBtn.classList.add('dark-theme');
        } else {
            modal.classList.remove('dark-theme');
            modalContent.classList.remove('dark-theme');
            closeBtn.classList.remove('dark-theme');
        }
    };

    // Watch for theme changes
    const observer = new MutationObserver(updateTheme);
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    observer.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style']
    });
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/05_autograd_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=50797cee"></script>
    <script src="../_static/wip-banner.js?v=b021a6d4"></script>
    <script src="../_static/marimo-badges.js?v=dca17944"></script>
    <script src="../_static/sidebar-link.js?v=ee94e95f"></script>
    <script src="../_static/hero-carousel.js?v=fa18433d"></script>
    <script src="../_static/subscribe-modal.js?v=82641629"></script>
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Module 06: Optimizers" href="06_optimizers_ABOUT.html" />
    <link rel="prev" title="Module 04: Losses" href="04_losses_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="_static/downloads/TinyTorch-Guide.pdf" title="PDF Guide" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-file-pdf fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PDF Guide</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="_static/downloads/TinyTorch-Paper.pdf" title="Paper" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-scroll fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Paper</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://mlsysbook.ai" title="MLSysBook" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-book fa-lg" aria-hidden="true"></i>
            <span class="sr-only">MLSysBook</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mlsysbook" title="Support" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-heart fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Support</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/harvard-edge/cs249r_book" title="Star" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Star</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/EZyaFgpB4F" title="Community" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Community</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preface.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="../big-picture.html">Big Picture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-07)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">05. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_optimizers_ABOUT.html">06. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_training_ABOUT.html">07. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (08-13)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_dataloader_ABOUT.html">08. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_convolutions_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_memoization_ABOUT.html">17. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_acceleration_ABOUT.html">18. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/05_autograd_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 05: Autograd</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-base-class">Function Base Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-function-classes">Core Function Classes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-tensor-methods">Enhanced Tensor Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#global-activation">Global Activation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-graphs">Computation Graphs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule">The Chain Rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass-implementation">Backward Pass Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-accumulation">Gradient Accumulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-management-in-autograd">Memory Management in Autograd</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-autograd-matters-at-scale">Why Autograd Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-05-autograd">
<h1>Module 05: Autograd<a class="headerlink" href="#module-05-autograd" title="Link to this heading">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Module Info</p>
<p><strong>FOUNDATION TIER</strong> | Difficulty: ‚óè‚óè‚óè‚óã | Time: 6-8 hours | Prerequisites: 01-04</p>
<p><strong>Prerequisites: Modules 01-04</strong> means you need:</p>
<ul class="simple">
<li><p>Tensor operations (matmul, broadcasting, reductions)</p></li>
<li><p>Activation functions (understanding non-linearity)</p></li>
<li><p>Neural network layers (what gradients flow through)</p></li>
<li><p>Loss functions (the ‚Äúwhy‚Äù behind gradients)</p></li>
</ul>
<p>If you can compute a forward pass through a neural network manually and understand why we need to minimize loss, you‚Äôre ready.</p>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 sd-g-3 sd-g-xs-3 sd-g-sm-3 sd-g-md-3 sd-g-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run interactively in your browser. No setup required.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F05_autograd%2F05_autograd.ipynb"><span>https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?labpath=tinytorch%2Fmodules%2F05_autograd%2F05_autograd.ipynb</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìÑ View Source</div>
<p class="sd-card-text">Browse the implementation code on GitHub.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/05_autograd/05_autograd.py"><span>https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/05_autograd/05_autograd.py</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üéß Audio Overview</div>
<p class="sd-card-text">Listen to an AI-generated overview.</p>
<audio controls style="width: 100%; margin-top: 8px;">
  <source src="https://github.com/harvard-edge/cs249r_book/releases/download/tinytorch-audio-v0.1.1/05_autograd.mp3" type="audio/mpeg">
</audio>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Autograd is the gradient engine that makes neural networks learn. Every modern deep learning framework‚ÄîPyTorch, TensorFlow, JAX‚Äîhas automatic differentiation at its core. Without autograd, training a neural network would require deriving and coding gradients by hand for every parameter in every layer. For a network with millions of parameters, this is impossible.</p>
<p>In this module, you‚Äôll build reverse-mode automatic differentiation from scratch. Your autograd system will track computation graphs during the forward pass, then flow gradients backward through every operation using the chain rule. By the end, calling <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> will automatically compute gradients for every parameter in your network, just like PyTorch.</p>
<p>This is the most conceptually challenging module in the Foundation tier, but it unlocks everything that follows: optimizers, training loops, and the ability to learn from data.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>By completing this module, you will:</p>
<ul class="simple">
<li><p><strong>Implement</strong> the Function base class that enables gradient computation for all operations</p></li>
<li><p><strong>Build</strong> computation graphs that track dependencies between tensors during forward pass</p></li>
<li><p><strong>Master</strong> the chain rule by implementing backward passes for arithmetic, matrix multiplication, and reductions</p></li>
<li><p><strong>Understand</strong> memory trade-offs between storing intermediate values and recomputing forward passes</p></li>
<li><p><strong>Connect</strong> your autograd implementation to PyTorch‚Äôs design patterns and production optimizations</p></li>
</ul>
</div>
</section>
<section id="what-youll-build">
<h2>What You‚Äôll Build<a class="headerlink" href="#what-youll-build" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id1">
<pre  class="mermaid">
        flowchart LR
    subgraph &quot;Autograd System&quot;
        A[&quot;Function&lt;br/&gt;Base Class&quot;]
        B[&quot;Operation Functions&lt;br/&gt;Add, Mul, Matmul&quot;]
        C[&quot;Backward Pass&lt;br/&gt;Gradient Flow&quot;]
        D[&quot;Computation Graph&lt;br/&gt;Tracking&quot;]
        E[&quot;enable_autograd()&lt;br/&gt;Global Activation&quot;]
    end

    A --&gt; B --&gt; C --&gt; D --&gt; E

    style A fill:#e1f5ff
    style B fill:#fff3cd
    style C fill:#f8d7da
    style D fill:#d4edda
    style E fill:#e2d5f1
    </pre><figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Autograd System</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Implementation roadmap:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Part</p></th>
<th class="head"><p>What You‚Äôll Implement</p></th>
<th class="head"><p>Key Concept</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Function</span></code> base class</p></td>
<td><p>Storing inputs for backward pass</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">AddBackward</span></code>, <code class="docutils literal notranslate"><span class="pre">MulBackward</span></code>, <code class="docutils literal notranslate"><span class="pre">MatmulBackward</span></code></p></td>
<td><p>Operation-specific gradient rules</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">backward()</span></code> method on Tensor</p></td>
<td><p>Reverse-mode differentiation</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">enable_autograd()</span></code> enhancement</p></td>
<td><p>Monkey-patching operations for gradient tracking</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>Integration tests</p></td>
<td><p>Multi-layer gradient flow</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>The pattern you‚Äôll enable:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Automatic gradient computation</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># y = 3x + 1</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>   <span class="c1"># Computes dy/dx = 3 automatically</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># [3.0]</span>
</pre></div>
</div>
<section id="what-youre-not-building-yet">
<h3>What You‚Äôre NOT Building (Yet)<a class="headerlink" href="#what-youre-not-building-yet" title="Link to this heading">#</a></h3>
<p>To keep this module focused, you will <strong>not</strong> implement:</p>
<ul class="simple">
<li><p>Higher-order derivatives (gradients of gradients)‚ÄîPyTorch supports this with <code class="docutils literal notranslate"><span class="pre">create_graph=True</span></code></p></li>
<li><p>Dynamic computation graphs‚Äîyour graphs are built during forward pass only</p></li>
<li><p>GPU kernel fusion‚ÄîPyTorch‚Äôs JIT compiler optimizes backward pass operations</p></li>
<li><p>Checkpointing for memory efficiency‚Äîthat‚Äôs an advanced optimization technique</p></li>
</ul>
<p><strong>You are building the core gradient engine.</strong> Advanced optimizations come in production frameworks.</p>
</section>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<p>This section documents the autograd components you‚Äôll build. These integrate with the existing Tensor class from Module 01.</p>
<section id="function-base-class">
<h3>Function Base Class<a class="headerlink" href="#function-base-class" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Function</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)</span>
</pre></div>
</div>
<p>Base class for all differentiable operations. Every operation (addition, multiplication, etc.) inherits from Function and implements gradient computation rules.</p>
</section>
<section id="core-function-classes">
<h3>Core Function Classes<a class="headerlink" href="#core-function-classes" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Class</p></th>
<th class="head"><p>Purpose</p></th>
<th class="head"><p>Gradient Rule</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">AddBackward</span></code></p></td>
<td><p>Addition gradients</p></td>
<td><p>‚àÇ(a+b)/‚àÇa = 1, ‚àÇ(a+b)/‚àÇb = 1</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SubBackward</span></code></p></td>
<td><p>Subtraction gradients</p></td>
<td><p>‚àÇ(a-b)/‚àÇa = 1, ‚àÇ(a-b)/‚àÇb = -1</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">MulBackward</span></code></p></td>
<td><p>Multiplication gradients</p></td>
<td><p>‚àÇ(a<em>b)/‚àÇa = b, ‚àÇ(a</em>b)/‚àÇb = a</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">DivBackward</span></code></p></td>
<td><p>Division gradients</p></td>
<td><p>‚àÇ(a/b)/‚àÇa = 1/b, ‚àÇ(a/b)/‚àÇb = -a/b¬≤</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">MatmulBackward</span></code></p></td>
<td><p>Matrix multiplication gradients</p></td>
<td><p>‚àÇ(A&#64;B)/‚àÇA = grad&#64;B.T, ‚àÇ(A&#64;B)/‚àÇB = A.T&#64;grad</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">SumBackward</span></code></p></td>
<td><p>Reduction gradients</p></td>
<td><p>‚àÇsum(a)/‚àÇa[i] = 1 for all i</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ReshapeBackward</span></code></p></td>
<td><p>Shape manipulation</p></td>
<td><p>‚àÇ(X.reshape(‚Ä¶))/‚àÇX = grad.reshape(X.shape)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">TransposeBackward</span></code></p></td>
<td><p>Transpose gradients</p></td>
<td><p>‚àÇ(X.T)/‚àÇX = grad.T</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Additional Backward Classes:</strong> The implementation includes backward functions for activations (<code class="docutils literal notranslate"><span class="pre">ReLUBackward</span></code>, <code class="docutils literal notranslate"><span class="pre">SigmoidBackward</span></code>, <code class="docutils literal notranslate"><span class="pre">SoftmaxBackward</span></code>, <code class="docutils literal notranslate"><span class="pre">GELUBackward</span></code>), losses (<code class="docutils literal notranslate"><span class="pre">MSEBackward</span></code>, <code class="docutils literal notranslate"><span class="pre">BCEBackward</span></code>, <code class="docutils literal notranslate"><span class="pre">CrossEntropyBackward</span></code>), and other operations (<code class="docutils literal notranslate"><span class="pre">PermuteBackward</span></code>, <code class="docutils literal notranslate"><span class="pre">EmbeddingBackward</span></code>, <code class="docutils literal notranslate"><span class="pre">SliceBackward</span></code>). These follow the same pattern as the core classes above.</p>
</section>
<section id="enhanced-tensor-methods">
<h3>Enhanced Tensor Methods<a class="headerlink" href="#enhanced-tensor-methods" title="Link to this heading">#</a></h3>
<p>Your implementation adds these methods to the Tensor class:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">backward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">backward(gradient=None)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Compute gradients via backpropagation</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">zero_grad</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">zero_grad()</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Reset gradients to None</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="global-activation">
<h3>Global Activation<a class="headerlink" href="#global-activation" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Signature</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">enable_autograd</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">enable_autograd(quiet=False)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code></p></td>
<td><p>Activate gradient tracking globally</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="core-concepts">
<h2>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h2>
<p>This section covers the fundamental ideas behind automatic differentiation. Understanding these concepts deeply will help you debug gradient issues in any framework, not just TinyTorch.</p>
<section id="computation-graphs">
<h3>Computation Graphs<a class="headerlink" href="#computation-graphs" title="Link to this heading">#</a></h3>
<p>A computation graph is a directed acyclic graph (DAG) where nodes represent tensors and edges represent operations. When you write <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">*</span> <span class="pre">3</span> <span class="pre">+</span> <span class="pre">1</span></code>, you‚Äôre implicitly building a graph with three nodes (x, intermediate result, y) and two operations (multiply, add).</p>
<p>Autograd systems build these graphs during the forward pass by recording each operation. Every tensor created by an operation stores a reference to the function that created it. This reference is the key to gradient flow: when you call <code class="docutils literal notranslate"><span class="pre">backward()</span></code>, the system traverses the graph in reverse, applying the chain rule at each node.</p>
<p>Here‚Äôs how a simple computation builds a graph:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Forward Pass:  x ‚Üí [Mul(*3)] ‚Üí temp ‚Üí [Add(+1)] ‚Üí y
Backward Pass: grad_x ‚Üê [MulBackward] ‚Üê grad_temp ‚Üê [AddBackward] ‚Üê grad_y
</pre></div>
</div>
<p>Each operation stores its inputs because backward pass needs them to compute gradients. For multiplication <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">b</span></code>, the gradient with respect to <code class="docutils literal notranslate"><span class="pre">a</span></code> is <code class="docutils literal notranslate"><span class="pre">grad_z</span> <span class="pre">*</span> <span class="pre">b</span></code>, so we must save <code class="docutils literal notranslate"><span class="pre">b</span></code> during forward pass. This is the core memory trade-off in autograd: storing intermediate values uses memory, but enables fast backward passes.</p>
<p>Your implementation tracks graphs with the <code class="docutils literal notranslate"><span class="pre">_grad_fn</span></code> attribute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AddBackward</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gradient computation for addition.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Store inputs needed for backward pass.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute gradients for both inputs.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">grad_output</span><span class="p">,</span> <span class="n">grad_output</span>  <span class="c1"># Addition distributes gradients equally</span>
</pre></div>
</div>
<p>When you compute <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">+</span> <span class="pre">y</span></code>, your enhanced Tensor class automatically creates an AddBackward instance and attaches it to <code class="docutils literal notranslate"><span class="pre">z</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">data</span>
<span class="n">result_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="n">result_tensor</span><span class="o">.</span><span class="n">_grad_fn</span> <span class="o">=</span> <span class="n">AddBackward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># Track operation</span>
</pre></div>
</div>
<p>This simple pattern enables arbitrarily complex computation graphs.</p>
</section>
<section id="the-chain-rule">
<h3>The Chain Rule<a class="headerlink" href="#the-chain-rule" title="Link to this heading">#</a></h3>
<p>The chain rule is the mathematical foundation of backpropagation. For composite functions, the derivative of the output with respect to any input equals the product of derivatives along the path connecting them.</p>
<p>Mathematically, if <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">f(g(x))</span></code>, then <code class="docutils literal notranslate"><span class="pre">dz/dx</span> <span class="pre">=</span> <span class="pre">(dz/dg)</span> <span class="pre">*</span> <span class="pre">(dg/dx)</span></code>. In computation graphs with multiple paths, gradients from all paths accumulate. This is gradient accumulation, and it‚Äôs why shared parameters (like embedding tables used multiple times) correctly receive gradients from all their uses.</p>
<p>Consider this computation: <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">(x</span> <span class="pre">*</span> <span class="pre">W</span> <span class="pre">+</span> <span class="pre">b)¬≤</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Forward:  x ‚Üí [Mul(W)] ‚Üí z1 ‚Üí [Add(b)] ‚Üí z2 ‚Üí [Square] ‚Üí loss

Backward chain rule:
  ‚àÇloss/‚àÇz2 = 2*z2              (square backward)
  ‚àÇloss/‚àÇz1 = ‚àÇloss/‚àÇz2 * 1     (addition backward)
  ‚àÇloss/‚àÇx  = ‚àÇloss/‚àÇz1 * W     (multiplication backward)
</pre></div>
</div>
<p>Each backward function multiplies the incoming gradient by the local derivative. Here‚Äôs how your MulBackward implements this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MulBackward</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gradient computation for element-wise multiplication.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For z = a * b:</span>
<span class="sd">        ‚àÇz/‚àÇa = b ‚Üí grad_a = grad_output * b</span>
<span class="sd">        ‚àÇz/‚àÇb = a ‚Üí grad_b = grad_output * a</span>

<span class="sd">        Uses vectorized element-wise multiplication (NumPy broadcasting).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># Vectorized element-wise multiplication</span>

        <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># NumPy handles broadcasting automatically</span>

        <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span>
</pre></div>
</div>
<p>The elegance is that each operation only knows its own derivative. The chain rule connects them all. NumPy‚Äôs vectorized operations handle all element-wise computations efficiently without explicit loops.</p>
</section>
<section id="backward-pass-implementation">
<h3>Backward Pass Implementation<a class="headerlink" href="#backward-pass-implementation" title="Link to this heading">#</a></h3>
<p>The backward pass traverses the computation graph in reverse topological order, computing gradients for each tensor. Your <code class="docutils literal notranslate"><span class="pre">backward()</span></code> method implements this as a recursive tree walk:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute gradients via backpropagation.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Initialize gradient for scalar outputs</span>
    <span class="k">if</span> <span class="n">gradient</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;backward() requires gradient for non-scalar tensors&quot;</span><span class="p">)</span>

    <span class="c1"># Accumulate gradient (vectorized NumPy operation)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">gradient</span>

    <span class="c1"># Propagate to parent tensors</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_grad_fn&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_fn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>  <span class="c1"># Compute input gradients using vectorized ops</span>

        <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grad_fn</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Recursive call</span>
</pre></div>
</div>
<p>The recursion naturally handles arbitrarily deep networks. For a 100-layer network, calling <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> triggers 100 recursive calls, one per layer, flowing gradients from output to input. Note that while the graph traversal uses recursion, the gradient computations within each <code class="docutils literal notranslate"><span class="pre">apply()</span></code> method use vectorized NumPy operations for efficiency.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">gradient</span></code> parameter deserves attention. For scalar losses (the typical case), you call <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> without arguments, and the method initializes the gradient to 1.0. This makes sense: <code class="docutils literal notranslate"><span class="pre">‚àÇloss/‚àÇloss</span> <span class="pre">=</span> <span class="pre">1</span></code>. For non-scalar tensors, you must provide the gradient from the next layer explicitly.</p>
</section>
<section id="gradient-accumulation">
<h3>Gradient Accumulation<a class="headerlink" href="#gradient-accumulation" title="Link to this heading">#</a></h3>
<p>Gradient accumulation is both a feature and a potential bug. When you call <code class="docutils literal notranslate"><span class="pre">backward()</span></code> multiple times on the same tensor, gradients add together. This is intentional: it enables mini-batch gradient descent and gradient checkpointing.</p>
<p>Consider processing a large batch in smaller chunks to fit in memory:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Large batch (doesn&#39;t fit in memory)</span>
<span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">split_batch</span><span class="p">(</span><span class="n">large_batch</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Gradients accumulate in model parameters</span>

<span class="c1"># Now gradients equal the sum over the entire large batch</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Reset for next iteration</span>
</pre></div>
</div>
<p>Without gradient accumulation, you‚Äôd need to store all mini-batch gradients and sum them manually. With accumulation, the autograd system handles it automatically.</p>
<p>But accumulation becomes a bug if you forget to call <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> between iterations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WRONG: Gradients accumulate across iterations</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Gradients keep adding!</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Updates use accumulated gradients from all previous batches</span>

<span class="c1"># CORRECT: Zero gradients after each update</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Reset gradients</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>Your <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> implementation is simple but crucial:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reset gradients to None.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>Setting to None instead of zeros saves memory: NumPy doesn‚Äôt allocate arrays until you accumulate the first gradient.</p>
</section>
<section id="memory-management-in-autograd">
<h3>Memory Management in Autograd<a class="headerlink" href="#memory-management-in-autograd" title="Link to this heading">#</a></h3>
<p>Autograd‚Äôs memory footprint comes from two sources: stored intermediate tensors and gradient storage. For a forward pass through an N-layer network, you store roughly N intermediate activations. During backward pass, you store gradients for every parameter.</p>
<p>Consider a simple linear layer: <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">&#64;</span> <span class="pre">W</span> <span class="pre">+</span> <span class="pre">b</span></code></p>
<p><strong>Forward pass stores:</strong></p>
<ul class="simple">
<li><p>x (needed for computing grad_W = x.T &#64; grad_y)</p></li>
<li><p>W (needed for computing grad_x = grad_y &#64; W.T)</p></li>
</ul>
<p><strong>Backward pass allocates:</strong></p>
<ul class="simple">
<li><p>grad_x (same shape as x)</p></li>
<li><p>grad_W (same shape as W)</p></li>
<li><p>grad_b (same shape as b)</p></li>
</ul>
<p>For a batch of 32 samples through a (512, 768) linear layer, the memory breakdown is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Forward storage:
  x: 32 √ó 512 √ó 4 bytes = 64 KB
  W: 512 √ó 768 √ó 4 bytes = 1,572 KB

Backward storage:
  grad_x: 32 √ó 512 √ó 4 bytes = 64 KB
  grad_W: 512 √ó 768 √ó 4 bytes = 1,572 KB
  grad_b: 768 √ó 4 bytes = 3 KB

Total: ~3.3 MB for one layer (2√ó parameter size + activation size)
</pre></div>
</div>
<p>Multiply by network depth and you see why memory limits batch size. A 100-layer transformer stores 100√ó the activations, which can easily exceed GPU memory.</p>
<p>Production frameworks mitigate this with gradient checkpointing: they discard intermediate activations during forward pass and recompute them during backward pass. This trades compute (recomputing activations) for memory (not storing them). Your implementation doesn‚Äôt do this‚Äîit‚Äôs an advanced optimization‚Äîbut understanding the trade-off is essential.</p>
<p>The implementation shows this memory overhead clearly in the MatmulBackward class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MatmulBackward</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradient computation for matrix multiplication.</span>

<span class="sd">    For Z = A @ B:</span>
<span class="sd">    - Must store A and B during forward pass</span>
<span class="sd">    - Backward computes: grad_A = grad_Z @ B.T and grad_B = A.T @ grad_Z</span>
<span class="sd">    - Uses vectorized NumPy operations (np.matmul, np.swapaxes)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>  <span class="c1"># Retrieved from memory</span>
        <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="c1"># Vectorized transpose and matmul (no explicit loops)</span>
            <span class="n">b_T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">grad_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">b_T</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="c1"># Vectorized operations for efficiency</span>
            <span class="n">a_T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">grad_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_T</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span>
</pre></div>
</div>
<p>Notice that both <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> must be saved during forward pass. For large matrices, this storage cost dominates memory usage. All gradient computations use vectorized NumPy operations, which are implemented in optimized C/Fortran code under the hood‚Äîno explicit Python loops are needed.</p>
</section>
</section>
<section id="production-context">
<h2>Production Context<a class="headerlink" href="#production-context" title="Link to this heading">#</a></h2>
<section id="your-implementation-vs-pytorch">
<h3>Your Implementation vs. PyTorch<a class="headerlink" href="#your-implementation-vs-pytorch" title="Link to this heading">#</a></h3>
<p>Your autograd system and PyTorch‚Äôs share the same design: computation graphs built during forward pass, reverse-mode differentiation during backward pass, and gradient accumulation in parameter tensors. The differences are in scale and optimization.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Your Implementation</p></th>
<th class="head"><p>PyTorch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Graph Building</strong></p></td>
<td><p>Python objects, <code class="docutils literal notranslate"><span class="pre">_grad_fn</span></code> attribute</p></td>
<td><p>C++ objects, compiled graph</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Memory</strong></p></td>
<td><p>Stores all intermediates</p></td>
<td><p>Gradient checkpointing, memory pools</p></td>
</tr>
<tr class="row-even"><td><p><strong>Speed</strong></p></td>
<td><p>Pure Python, NumPy backend</p></td>
<td><p>C++/CUDA, fused kernels</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Operations</strong></p></td>
<td><p>10 backward functions</p></td>
<td><p>2000+ optimized backward functions</p></td>
</tr>
<tr class="row-even"><td><p><strong>Debugging</strong></p></td>
<td><p>Direct Python inspection</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch.autograd.profiler</span></code>, graph visualization</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="code-comparison">
<h3>Code Comparison<a class="headerlink" href="#code-comparison" title="Link to this heading">#</a></h3>
<p>The following comparison shows identical conceptual patterns in TinyTorch and PyTorch. The APIs mirror each other because both implement the same autograd algorithm.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Your TinyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="c1"># Create tensors with gradient tracking</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Forward pass builds computation graph</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>  <span class="c1"># y = x @ W</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># loss = sum(y¬≤)</span>

<span class="c1"># Backward pass computes gradients</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># Access gradients</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x.grad: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># ‚àÇloss/‚àÇx</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W.grad: </span><span class="si">{</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># ‚àÇloss/‚àÇW</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
‚ö° PyTorch</label><div class="sd-tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Create tensors with gradient tracking</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Forward pass builds computation graph</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span>  <span class="c1"># PyTorch uses @ operator</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># Backward pass computes gradients</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># Access gradients</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x.grad: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W.grad: </span><span class="si">{</span><span class="n">W</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs walk through the comparison line by line:</p>
<ul class="simple">
<li><p><strong>Line 3-4 (Tensor creation)</strong>: Both frameworks use <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> to enable gradient tracking. This is an opt-in design: most tensors (data, labels) don‚Äôt need gradients, only parameters do.</p></li>
<li><p><strong>Line 7-8 (Forward pass)</strong>: Operations automatically build computation graphs. TinyTorch uses <code class="docutils literal notranslate"><span class="pre">.matmul()</span></code> method; PyTorch supports both <code class="docutils literal notranslate"><span class="pre">.matmul()</span></code> and the <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> operator.</p></li>
<li><p><strong>Line 11 (Backward pass)</strong>: Single method call triggers reverse-mode differentiation through the entire graph.</p></li>
<li><p><strong>Line 14-15 (Gradient access)</strong>: Both store gradients in the <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attribute. Gradients have the same shape as the original tensor.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>What‚Äôs Identical</p>
<p>Computation graph construction, chain rule implementation, and gradient accumulation semantics. When you debug PyTorch autograd issues, you‚Äôre debugging the same algorithm you implemented here.</p>
</div>
</section>
<section id="why-autograd-matters-at-scale">
<h3>Why Autograd Matters at Scale<a class="headerlink" href="#why-autograd-matters-at-scale" title="Link to this heading">#</a></h3>
<p>To appreciate why automatic differentiation is essential, consider the scale of modern networks:</p>
<ul class="simple">
<li><p><strong>GPT-3</strong>: 175 billion parameters = <strong>175,000,000,000 gradients</strong> to compute per training step</p></li>
<li><p><strong>Training time</strong>: Each backward pass takes roughly <strong>2√ó forward pass time</strong> (gradients require 2 matmuls per forward matmul)</p></li>
<li><p><strong>Memory</strong>: Storing computation graphs for a transformer can require <strong>10√ó model size</strong> in GPU memory</p></li>
</ul>
<p>Manual gradient derivation becomes impossible at this scale. Even for a 3-layer MLP with 1 million parameters, manually coding gradients would take weeks and inevitably contain bugs. Autograd makes training tractable by automating the most error-prone part of deep learning.</p>
</section>
</section>
<section id="check-your-understanding">
<h2>Check Your Understanding<a class="headerlink" href="#check-your-understanding" title="Link to this heading">#</a></h2>
<p>Test yourself with these systems thinking questions. They‚Äôre designed to build intuition for autograd‚Äôs performance characteristics and design decisions.</p>
<p><strong>Q1: Computation Graph Memory</strong></p>
<p>A 5-layer MLP processes a batch of 64 samples. Each layer stores its input activation for backward pass. Layer dimensions are: 784 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí 10. How much memory (in MB) is used to store activations for one batch?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Layer 1 input: 64 √ó 784 √ó 4 bytes = 200 KB
Layer 2 input: 64 √ó 512 √ó 4 bytes = 131 KB
Layer 3 input: 64 √ó 256 √ó 4 bytes = 66 KB
Layer 4 input: 64 √ó 128 √ó 4 bytes = 33 KB
Layer 5 input: 64 √ó 10 √ó 4 bytes = 3 KB</p>
<p><strong>Total: ~433 KB ‚âà 0.43 MB</strong></p>
<p>This is per forward pass! A 100-layer transformer would store 100√ó this amount, which is why gradient checkpointing trades compute for memory by recomputing activations during backward pass.</p>
</div>
<p><strong>Q2: Backward Pass Complexity</strong></p>
<p>A forward pass through a linear layer <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">&#64;</span> <span class="pre">W</span></code> (where x is 32√ó512 and W is 512√ó256) takes 8ms. How long will the backward pass take?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Forward: 1 matmul (x &#64; W)</p>
<p>Backward: 2 matmuls</p>
<ul class="simple">
<li><p>grad_x = grad_y &#64; W.T (32√ó256 &#64; 256√ó512)</p></li>
<li><p>grad_W = x.T &#64; grad_y (512√ó32 &#64; 32√ó256)</p></li>
</ul>
<p><strong>Backward takes ~2√ó forward time ‚âà 16ms</strong></p>
<p>This is why training (forward + backward) takes roughly 3√ó inference time. GPU parallelism and kernel fusion can reduce this, but the fundamental 2:1 ratio remains.</p>
</div>
<p><strong>Q3: Gradient Accumulation Memory</strong></p>
<p>You have 16GB GPU memory and a model with 1B parameters (float32). How much memory is available for activations and gradients during training?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Model parameters: 1B √ó 4 bytes = 4 GB
Gradients: 1B √ó 4 bytes = 4 GB
Optimizer state (Adam): 1B √ó 8 bytes = 8 GB (momentum + variance)</p>
<p><strong>Total framework overhead: 16 GB</strong></p>
<p><strong>Available for activations: 0 GB</strong> - you‚Äôve already exceeded memory!</p>
<p>This is why large models use gradient accumulation across multiple forward passes before updating parameters, or gradient checkpointing to reduce activation memory. The ‚Äú2√ó parameter size‚Äù rule (parameters + gradients) is a minimum; optimizers add more overhead.</p>
</div>
<p><strong>Q4: requires_grad Performance</strong></p>
<p>A typical training batch has: 32 images (input), 10M parameter tensors (weights), 50 intermediate activation tensors. If requires_grad defaults to True for all tensors, how many tensors unnecessarily track gradients?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Tensors that NEED gradients:</p>
<ul class="simple">
<li><p>Parameters: 10M tensors ‚úì</p></li>
</ul>
<p>Tensors that DON‚ÄôT need gradients:</p>
<ul class="simple">
<li><p>Input images: 32 tensors (no gradient needed for data)</p></li>
<li><p>Intermediate activations: 50 tensors (needed for backward but not updated)</p></li>
</ul>
<p><strong>32 input tensors unnecessarily track gradients</strong> if requires_grad defaults to True.</p>
<p>This is why PyTorch defaults requires_grad=False for new tensors and requires explicit opt-in for parameters. For image inputs with 32√ó3√ó224√ó224 = 4.8M values each, tracking gradients wastes 4.8M √ó 4 bytes = 19 MB per image √ó 32 = 608 MB for the batch!</p>
</div>
<p><strong>Q5: Graph Retention</strong></p>
<p>You forget to call <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> before each training iteration. After 10 iterations, how do the gradients compare to correct training?</p>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p><strong>Gradients accumulate across all 10 iterations.</strong></p>
<p>If correct gradient for iteration i is <code class="docutils literal notranslate"><span class="pre">g_i</span></code>, your accumulated gradient is:
<code class="docutils literal notranslate"><span class="pre">grad</span> <span class="pre">=</span> <span class="pre">g_1</span> <span class="pre">+</span> <span class="pre">g_2</span> <span class="pre">+</span> <span class="pre">g_3</span> <span class="pre">+</span> <span class="pre">...</span> <span class="pre">+</span> <span class="pre">g_10</span></code></p>
<p><strong>Effects:</strong></p>
<ol class="arabic simple">
<li><p><strong>Magnitude</strong>: Gradients are ~10√ó larger than they should be</p></li>
<li><p><strong>Direction</strong>: The sum of 10 different gradients, which may not point toward the loss minimum</p></li>
<li><p><strong>Learning</strong>: Parameter updates use the wrong direction and wrong magnitude</p></li>
<li><p><strong>Result</strong>: Training diverges or oscillates instead of converging</p></li>
</ol>
<p><strong>Bottom line</strong>: Always call <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> at the start of each iteration (or after <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>).</p>
</div>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>For students who want to understand the academic foundations and mathematical underpinnings of automatic differentiation:</p>
<section id="seminal-papers">
<h3>Seminal Papers<a class="headerlink" href="#seminal-papers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Automatic Differentiation in Machine Learning: a Survey</strong> - Baydin et al. (2018). Comprehensive survey of AD techniques, covering forward-mode, reverse-mode, and mixed-mode differentiation. Essential reading for understanding autograd theory. <a class="reference external" href="https://arxiv.org/abs/1502.05767">arXiv:1502.05767</a></p></li>
<li><p><strong>Automatic Differentiation of Algorithms</strong> - Griewank (1989). The foundational work on reverse-mode AD that underlies all modern deep learning frameworks. Introduces the mathematical formalism for gradient computation via the chain rule. <a class="reference external" href="https://doi.org/10.1007/BF00139316">Computational Optimization and Applications</a></p></li>
<li><p><strong>PyTorch: An Imperative Style, High-Performance Deep Learning Library</strong> - Paszke et al. (2019). Describes PyTorch‚Äôs autograd implementation and design philosophy. Shows how imperative programming (define-by-run) enables dynamic computation graphs. <a class="reference external" href="https://arxiv.org/abs/1912.01703">NeurIPS 2019</a></p></li>
</ul>
</section>
<section id="additional-resources">
<h3>Additional Resources<a class="headerlink" href="#additional-resources" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Textbook</strong>: ‚ÄúDeep Learning‚Äù by Goodfellow, Bengio, and Courville - Chapter 6 covers backpropagation and computational graphs with excellent visualizations</p></li>
<li><p><strong>Tutorial</strong>: <a class="reference external" href="https://cs231n.github.io/optimization-2/">CS231n: Backpropagation, Intuitions</a> - Stanford‚Äôs visual explanation of gradient flow through computation graphs</p></li>
<li><p><strong>Documentation</strong>: <a class="reference external" href="https://pytorch.org/docs/stable/notes/autograd.html">PyTorch Autograd Mechanics</a> - Official guide to PyTorch‚Äôs autograd implementation details</p></li>
</ul>
</section>
</section>
<section id="whats-next">
<h2>What‚Äôs Next<a class="headerlink" href="#whats-next" title="Link to this heading">#</a></h2>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Coming Up: Module 06 - Optimizers</p>
<p>Implement SGD, Adam, and other optimization algorithms that use your autograd gradients to update parameters and train neural networks. You‚Äôll complete the training loop and make your networks learn from data.</p>
</div>
<p><strong>Preview - How Your Autograd Gets Used in Future Modules:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>What It Does</p></th>
<th class="head"><p>Your Autograd In Action</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>06: Optimizers</strong></p></td>
<td><p>Update parameters using gradients</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> uses <code class="docutils literal notranslate"><span class="pre">param.grad</span></code> computed by backward()</p></td>
</tr>
<tr class="row-odd"><td><p><strong>07: Training</strong></p></td>
<td><p>Complete training loops</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> ‚Üí <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> ‚Üí repeat</p></td>
</tr>
<tr class="row-even"><td><p><strong>12: Attention</strong></p></td>
<td><p>Multi-head self-attention</p></td>
<td><p>Gradients flow through Q, K, V projections automatically</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Interactive Options</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://mybinder.org/v2/gh/harvard-edge/cs249r_book/main?urlpath=lab/tree/tinytorch/modules/05_autograd/05_autograd.ipynb">Launch Binder</a></strong> - Run interactively in browser, no setup required</p></li>
<li><p><strong><a class="reference external" href="https://github.com/harvard-edge/cs249r_book/blob/main/tinytorch/src/05_autograd/05_autograd.py">View Source</a></strong> - Browse the implementation code</p></li>
</ul>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Save Your Progress</p>
<p>Binder sessions are temporary. Download your completed notebook when done, or clone the repository for persistent local work.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="04_losses_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Module 04: Losses</p>
      </div>
    </a>
    <a class="right-next"
       href="06_optimizers_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Module 06: Optimizers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youll-build">What You‚Äôll Build</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-youre-not-building-yet">What You‚Äôre NOT Building (Yet)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-base-class">Function Base Class</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-function-classes">Core Function Classes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-tensor-methods">Enhanced Tensor Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#global-activation">Global Activation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-graphs">Computation Graphs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule">The Chain Rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass-implementation">Backward Pass Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-accumulation">Gradient Accumulation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-management-in-autograd">Memory Management in Autograd</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#production-context">Production Context</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#your-implementation-vs-pytorch">Your Implementation vs. PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-comparison">Code Comparison</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-autograd-matters-at-scale">Why Autograd Matters at Scale</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-your-understanding">Check Your Understanding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seminal-papers">Seminal Papers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-next">What‚Äôs Next</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#get-started">Get Started</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025 Harvard University.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>