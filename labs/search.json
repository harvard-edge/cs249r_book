[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Co-Labs",
    "section": "",
    "text": "Coming 2026\n    Co-Labs\n    See ML Systems in Action\n    Watch quantization compress models. Measure memory hierarchies. Profile gradient flow.\n  \n\n\n\n\nWhy Another Set of Notebooks?\n\nThere are many excellent Colab notebooks for ML. Most demonstrate algorithms. Few help you understand systems.\n\nWhen you quantize a model from FP32 to INT8, what actually happens to the weights? When you increase batch size, where does memory go? When you add a layer, how does gradient flow change?\n\nCo-Labs are designed to answer these questions. Each notebook is systematically aligned with textbook chapters, letting you experiment with the exact system concepts you just read about. The goal is not to teach you how to use PyTorch. It's to show you why PyTorch works the way it does.\n\nâ€” Vijay\n\nThe Systems Learning Path\n\nCo-Labs fit between conceptual understanding and building from scratch.\n\n\n  \n    ðŸ“–\n    Understand\n    Learn system design principles: memory, compute, parallelism, efficiency\n    Textbook â†’\n  \n  \n    ðŸ”¬\n    Experiment\n    Measure tradeoffs, profile bottlenecks, see system decisions ripple through models\n    Co-Labs\n  \n  \n    ðŸ”¥\n    Build\n    Implement tensors, autograd, and training loops from scratch\n    TinyTorch â†’\n  \n\n\nWhat You'll Explore\n\nEach Co-Lab maps directly to textbook chapters, focusing on the systems perspective:\n\n\n  \n    Memory Systems\n    \n      Batch size vs memory footprint\n      Activation checkpointing tradeoffs\n      Cache hierarchy effects on training\n    \n  \n  \n    Numerical Representation\n    \n      FP32 â†’ FP16 â†’ INT8 â†’ INT4\n      Quantization error propagation\n      Mixed precision training dynamics\n    \n  \n  \n    Compute Efficiency\n    \n      Pruning and sparsity patterns\n      Knowledge distillation mechanics\n      Operator fusion benefits\n    \n  \n  \n    Deployment Tradeoffs\n    \n      Latency vs throughput curves\n      Batching strategy impact\n      Hardware utilization profiling\n    \n  \n\n\nHelp Shape This\n\nI'm still figuring out what makes the most sense. If you have ideas for experiments that would help you understand ML systems better:\n\n\n  Share Ideas\n  Get Updates\n\n\n\n\n\n\n Back to top"
  }
]