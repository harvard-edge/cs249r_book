{
  "total_files": 64,
  "total_references": 1469,
  "total_definitions": 732,
  "patterns": {
    "total_definitions": 732,
    "with_bold_terms": 723,
    "average_length": 381,
    "common_prefixes": {
      "fn": 723,
      "immutable": 1,
      "stateless": 1,
      "jit": 1,
      "pure": 1,
      "gradient": 1,
      "batch": 1
    },
    "terms_used": [
      "tinyml device scale",
      "agi consensus complexity",
      "imagenet",
      "ai for climate action",
      "energy efficiency in tinyml",
      "tpu (tensor processing unit)",
      "mnist dataset",
      "air-gapped systems",
      "mutual tls (mtls)",
      "hazardous chemical quantities",
      "cassava disease impact",
      "operator fusion",
      "apple's neural engine strategy",
      "learning rate",
      "mechanical turk origins",
      "synthetic data growth",
      "convolutional operations",
      "convolutional neural networks",
      "tensor operations",
      "high-bandwidth memory (hbm)",
      "bigquery serverless power",
      "sparse energy savings",
      "transfer learning attacks",
      "etl vs elt performance",
      "bert compression",
      "gdpr article 22",
      "electrical grids",
      "e-waste from computing",
      "relu hardware efficiency",
      "artificial neurons",
      "tinyml in fitness trackers",
      "batch size effects",
      "api keys",
      "roofline model",
      "adversarial inputs",
      "ai-generated phishing",
      "3d chip stacking",
      "tinyml",
      "tops (tera operations per second)",
      "nccl (nvidia collective communications library)",
      "white-box attacks",
      "github actions for ml",
      "chiplet architecture",
      "gpt-3 training scale",
      "richard sutton",
      "systemic bias",
      "kserve (formerly kfserving)",
      "intel sgx constraints",
      "chinese rare earth dominance",
      "cloud genomics scale",
      "spec cpu",
      "transformer",
      "hsm certification",
      "mobile system-on-chip",
      "human-in-the-loop (hitl)",
      "petabytes",
      "energy-based models (ebms)",
      "onnx deployment",
      "global explanations",
      "deployment environments",
      "mixed-precision training",
      "nlp computational demands",
      "etl evolution",
      "alexnet's gpu revolution",
      "model compression",
      "critical material scarcity",
      "scan chains",
      "data versioning challenges",
      "edge computing for ai",
      "lidar",
      "just-in-time (jit) compilation",
      "arduino edge computing reality",
      "latency vs throughput",
      "non-iid (non-independent and identically distributed)",
      "microcontroller power budget reality",
      "attention mechanism notation",
      "training profiling tools",
      "systolic array renaissance",
      "industrial iot",
      "iot hubs",
      "simd (single instruction, multiple data)",
      "programmable logic controllers (plcs)",
      "the 80/20 rule in ml",
      "thermal imaging in disaster response",
      "homomorphic encryption breakthrough",
      "embodied carbon",
      "whetstone",
      "depthwise separable convolutions",
      "protected attributes",
      "crowdsourcing risks",
      "ml telemetry",
      "intelligence vs. performance",
      "theano",
      "carlini and wagner (c&w) attack",
      "error-correcting codes",
      "usb attack vectors",
      "mobile storage evolution",
      "gemm (general matrix multiply)",
      "mobile face detection",
      "hdfs origins",
      "attention maps",
      "batching in neural networks",
      "model drift detection",
      "digital infrastructure divide",
      "mlops business impact",
      "thermal stress",
      "asynchronous federated learning (fedasync)",
      "cuda programming model",
      "apache beam architecture",
      "cifar-10",
      "gradient descent",
      "large language models (llms)",
      "feature engineering",
      "systolic array architecture",
      "mel-frequency cepstral coefficients (mfccs)",
      "carbon emissions",
      "domain-specific architectures (dsa)",
      "parameters",
      "social good resource paradox",
      "backdoor attacks",
      "docker's revolution",
      "ml runtimes",
      "categorical encoding impact",
      "equalized odds",
      "fault models",
      "catastrophic forgetting severity",
      "f1 score",
      "on-device training constraints",
      "model repositories",
      "model uncertainty",
      "automl (automated machine learning)",
      "cudnn",
      "oauth protocol",
      "dvc creation story",
      "service level objectives (slos)",
      "receptive field",
      "federated learning birth",
      "agi infrastructure scale engineering analysis",
      "tensor processing unit (tpu)",
      "endpoint device constraints",
      "edge tpu",
      "regularization",
      "cloud inference latency",
      "tinyml optimization",
      "train-serve split economics",
      "gans for adversarial attacks",
      "model cards",
      "uci ml repository",
      "data quality reality",
      "hyperparameter optimization",
      "data parallelism scaling",
      "cuda cores",
      "vision transformers (vits)",
      "flops (floating-point operations per second)",
      "tensor core breakthrough",
      "tinyml model optimization",
      "squeezenet",
      "tpu origins",
      "quantization-aware training",
      "hardware-aware nas",
      "nvidia triton inference server",
      "core ml",
      "ml-based fraud detection evolution",
      "sdg global impact",
      "static model problem",
      "fips 140-2 standard",
      "raspberry pi development advantages",
      "synapses",
      "cascade of classifiers",
      "kubeflow production usage",
      "automatic differentiation",
      "tinyml market reality",
      "pipeline jungle metaphor",
      "training-serving skew impact",
      "model serialization",
      "perspective api",
      "shap in production",
      "tensorflow lite",
      "flops vs flops",
      "google tensor soc architecture",
      "normalization in ml",
      "glitches",
      "flops (floating-point operations)",
      "tokens",
      "sparsity",
      "backpropagation",
      "microservices in ml",
      "healthcare ai deployment reality",
      "latency",
      "nvidia ai gpus",
      "vanishing gradient problem",
      "beam testing",
      "stochastic gradient descent",
      "epoch",
      "rnns and lstms",
      "3dmark",
      "memory planning",
      "latency-critical applications",
      "household energy comparison",
      "memory hierarchy in ml",
      "stuxnet discovery",
      "mobilenet",
      "gpt-3 energy consumption",
      "gpu (graphics processing unit)",
      "data drift",
      "iot (internet of things) networks",
      "activation functions",
      "viola-jones algorithm",
      "distributed training",
      "edge computing",
      "neural processing units (npus)",
      "stateless function",
      "iot device vulnerabilities",
      "distributed infrastructure",
      "nas evaluation metrics",
      "resnet revolution",
      "microcontroller constraints",
      "transfer learning",
      "data center cooling costs",
      "automotive cybersecurity recalls",
      "waze crowdsourcing model",
      "model inversion attack",
      "data lake origins",
      "value alignment",
      "fp32",
      "gpt-3",
      "neural processing unit (npu)",
      "memory usage",
      "lora (low-rank adaptation)",
      "multimodal sensor data",
      "mysql at scale",
      "differential privacy origins",
      "jenkins origins",
      "huber loss",
      "application-specific integrated circuits (asics)",
      "lottery ticket hypothesis",
      "int8",
      "jevon's paradox",
      "voice recognition evolution",
      "ibm system/360",
      "structural inequities",
      "safety-critical applications",
      "real-time latency requirements",
      "superhuman ai capabilities",
      "fpga for ml",
      "canary deployment history",
      "redis performance",
      "parameter scaling",
      "directed acyclic graph (dag)",
      "nvidia jetson ecosystem",
      "data evolution in production",
      "energy-privacy tradeoffs",
      "simd evolution",
      "global fishing watch impact",
      "mlops maturity models",
      "perplexity",
      "iot device growth",
      "autoregressive models",
      "dropout mechanism",
      "dual-use dilemma",
      "transformer + nas environmental impact",
      "billion-parameter models",
      "datacenter environmental justice",
      "alexnet",
      "neuromorphic computing",
      "tiny ml",
      "wafer-scale engine specifications",
      "wireless communication reality",
      "pay-as-you-go pricing",
      "real-time translation",
      "end of moore's law",
      "imagenet revolution",
      "tensor cores",
      "human feedback bottlenecks",
      "mobilenet innovation",
      "data centers",
      "fbnet",
      "healthcare ml compliance",
      "cloud infrastructure evolution",
      "proxy metrics",
      "google data centers",
      "open images dataset",
      "collaborative filtering",
      "cryptographic hashing",
      "distributed systems",
      "scale-invariant feature transform (sift)",
      "stochastic computing",
      "feature store evolution",
      "lapack (linear algebra package)",
      "arm trustzone",
      "silent data corruption (sdc)",
      "gradient clipping",
      "model drift phenomenon",
      "cross-entropy loss",
      "agi network topology",
      "transformer batch size scaling",
      "elk stack",
      "concept drift challenge",
      "scaling laws",
      "hardening strategies",
      "google's carbon-free commitment",
      "batch normalization",
      "ml audit requirements",
      "blas (basic linear algebra subprograms)",
      "computational graphs",
      "lstm origins",
      "tvm",
      "electromagnetic interference (emi)",
      "systemic vulnerabilities",
      "microclimate monitoring",
      "ml autoscaling at scale",
      "dhrystone",
      "adversarial examples",
      "ai compute explosion",
      "basic linear algebra subprograms (blas)",
      "google carbon-aware scheduling results",
      "data parallelism",
      "multi-agent intelligence",
      "gpu manufacturing impact",
      "computational photography",
      "gpt-4 energy consumption",
      "burst buffers",
      "fast gradient sign method (fgsm)",
      "field-programmable gate array (fpga)",
      "moore's law",
      "eniac (electronic numerical integrator and computer)",
      "ai safety",
      "robotic system requirements",
      "cray-1 vector legacy",
      "model distillation",
      "memory hierarchy",
      "model watermarking",
      "principal component analysis (pca)",
      "backpropagation (historical context)",
      "datasheets for datasets",
      "predictive maintenance",
      "ensemble methods",
      "post-hoc explanations",
      "data drift detection",
      "serverless ai",
      "dartmouth conference (1956)",
      "cuda (compute unified device architecture)",
      "mosquito species detection",
      "ml system scaling complexity",
      "medical imaging ai revolution",
      "youtube recommendation impact",
      "risc-v for ai",
      "hot spares",
      "technical debt origins",
      "a/b testing for ml",
      "ddos attacks",
      "medical device security",
      "flops",
      "eager execution",
      "federated learning architecture",
      "distributed training costs",
      "power usage effectiveness (pue)",
      "data-centric ai",
      "bayesian neural networks",
      "32-bit floating point precision",
      "tensorrt",
      "explainability market growth",
      "self-supervised learning",
      "coprocessor",
      "membership inference attacks",
      "infrastructure efficiency gap",
      "general data protection regulation (gdpr)",
      "devops origins",
      "activation caching",
      "medical ai performance metrics",
      "workflow automation scale",
      "agi byzantine threats",
      "triple modular redundancy (tmr)",
      "wafer-scale integration",
      "agi compute requirements engineering analysis",
      "kubernetes origins",
      "esp32 edge computing",
      "lightweight transformers",
      "distilbert",
      "neural network theoretical foundations",
      "efficientnet pruning",
      "neural network learning mechanisms",
      "principle of least privilege",
      "mlops emergence",
      "l1 cache",
      "nhtsa cybersecurity guidelines",
      "relu (rectified linear unit)",
      "ci/cd security",
      "transformer attention",
      "matrix factorization",
      "evolutionary nas",
      "stochastic gradient descent (sgd)",
      "query-key-value attention",
      "rlhf effectiveness",
      "tensorrt optimization",
      "heterogeneous computing",
      "mobile power constraints",
      "parameter-efficient fine-tuning",
      "dp-sgd industry adoption",
      "linear transformations",
      "parallel processing in ml",
      "backpropagation algorithm",
      "particle accelerators",
      "gradient checkpointing",
      "serverless computing for ml",
      "ml apis",
      "etl vs elt in ml",
      "medical data annotation costs",
      "linpack",
      "processing-in-memory (pim)",
      "mirai botnet scale",
      "algorithmic fairness in healthcare",
      "large-scale training challenges",
      "power usage effectiveness",
      "tensorflow",
      "automatic vectorization",
      "ml reproducibility crisis",
      "high-stakes domains",
      "hsm performance",
      "acoustic gunshot detection",
      "xla (accelerated linear algebra)",
      "gflops (giga floating-point operations per second)",
      "edge latency advantage",
      "tensorflow model optimization",
      "agi agent scale",
      "arm processors",
      "advanced encryption standard (aes)",
      "fp16",
      "gradient quantization",
      "von neumann architecture",
      "tensorflow serving origins",
      "bert",
      "static computational graph",
      "constitutional ai method",
      "translation invariance",
      "role-based access control (rbac)",
      "pure function",
      "constitutional ai",
      "speculative execution",
      "jax",
      "2014-2016 ebola outbreak",
      "gdpr's ml impact",
      "hardware lottery",
      "convolutional neural network (cnn)",
      "the lab-to-clinic performance gap",
      "kaggle",
      "agi communication complexity",
      "cloud ml training economics",
      "field-programmable gate arrays (fpgas)",
      "immutable data structures",
      "mlp mathematical notation",
      "cybersecurity regulations",
      "fairlearn",
      "transformer training",
      "structured pruning",
      "resnet",
      "bayesian optimization",
      "efficientnet",
      "black box",
      "aws sagemaker",
      "checkpoint and restart mechanisms",
      "intermediate representation (ir)",
      "microcontrollers",
      "gradients and convergence",
      "mobile device constraints",
      "gdpr (general data protection regulation)",
      "memory bottleneck",
      "euv lithography",
      "netflix deanonymization",
      "double modular redundancy (dmr)",
      "mobilemark",
      "deep learning frameworks",
      "infrastructure as code",
      "hipaa (health insurance portability and accountability act)",
      "hyperscale data center scale",
      "edge processor",
      "tensorflow serving",
      "data augmentation",
      "memory optimization",
      "gradient accumulation",
      "data drift discovery",
      "chiplet",
      "reward hacking",
      "combinational logic",
      "brain energy efficiency",
      "recaptcha evolution",
      "foundation models",
      "agi resource coordination",
      "debug port vulnerabilities",
      "eliza",
      "saliency maps",
      "smpc performance",
      "smallholder farmers global impact",
      "optical interconnects",
      "lora technology",
      "pytorch",
      "diabetic retinopathy global impact",
      "quantum-classical hybrid",
      "mnist",
      "curriculum learning",
      "data center climate impact",
      "near-memory computing",
      "columnar format revolution",
      "microsoft fpga deployment",
      "nvidia nccl (collective communications library)",
      "feature store scale",
      "chatgpt user adoption",
      "systolic array",
      "covid-19 ml impact",
      "real-time grid carbon intensity",
      "\"hey siri\" technical reality",
      "apple neural engine evolution",
      "privacy-utility tension",
      "data sanitization",
      "attention mechanisms",
      "model parallelism",
      "privacy regulation timeline",
      "moore's law origins",
      "alexa voice service (avs)",
      "uci machine learning repository",
      "batch processing",
      "energy star",
      "plantvillage nuru real-world impact",
      "tensor processing units",
      "\"attention is all you need\"",
      "zero-day etymology",
      "transferability",
      "hyperparameter optimization complexity",
      "vanishing gradients",
      "gradient sparsification",
      "differential privacy",
      "learning rate schedules",
      "nuclear power for ai data centers",
      "gradient accumulation impact",
      "hipaa violations",
      "ci/cd for machine learning",
      "ai hypercomputers",
      "connection machine cm-5",
      "fairness impossibility theorems",
      "ai vs industrial emissions",
      "satellite disaster monitoring",
      "alignment technical challenges",
      "inference latency",
      "data lineage systems",
      "industry 4.0",
      "histogram of oriented gradients (hog)",
      "mainframes",
      "network protocols",
      "single event upsets (seus)",
      "recurrent neural networks",
      "phase transition",
      "edge network coordination",
      "pruning",
      "asic (application-specific integrated circuit)",
      "federated learning",
      "batch processing evolution",
      "esp32 capabilities",
      "side-channel attacks on ml",
      "demographic parity origins",
      "crosstalk",
      "imagenet competition progress",
      "google tpus",
      "green500",
      "multi-agent system",
      "cough analysis technology",
      "energy-aware ai frameworks",
      "model extraction threat",
      "parameter sharing",
      "knowledge distillation",
      "energy efficiency metrics",
      "failsafe mechanisms",
      "blockchain for ml governance",
      "dp-sgd (differentially private stochastic gradient descent)",
      "model parallelism memory scaling",
      "activation checkpointing trade-offs",
      "dynamic voltage and frequency scaling (dvfs)",
      "smart cities",
      "model scaling explosion",
      "minimax",
      "personalization technical foundations",
      "application-specific integrated circuit (asic)",
      "click-through rate (ctr) optimization",
      "kernel fusion",
      "data poisoning",
      "paradigm shift",
      "model optimization",
      "biological efficiency",
      "transformer architectures",
      "ml vs. traditional problem definition",
      "semiconductor water consumption scale",
      "fp16 dynamic range",
      "edge ml",
      "prometheus at scale",
      "microsoft farmbeats",
      "production alert thresholds",
      "arm cortex architecture spectrum",
      "ml team role evolution",
      "allreduce algorithm",
      "tvm (tensor virtual machine)",
      "google crowdsource",
      "overfitting",
      "puf market growth",
      "im2col (image to column)",
      "universal approximation theorem",
      "cnn convolution notation",
      "active learning",
      "operation fusion",
      "autoencoders",
      "gpu operational costs",
      "computer engineering",
      "lookup table",
      "cdc 6600",
      "federated averaging (fedavg)",
      "ultra-long battery life",
      "yann lecun and cnns",
      "alignment failure modes",
      "snappy compression trade-offs",
      "mlflow's creation",
      "perceptron",
      "memory scale comparison",
      "hyperscale data centers",
      "pseudonymization under gdpr",
      "healthcare algorithm scale",
      "strassen's algorithm",
      "service level agreements (slas)",
      "zillow ibuying failure",
      "local explanations",
      "amdahl's law",
      "autopilot",
      "few-shot learning",
      "memory bandwidth",
      "compas algorithm controversy",
      "portrait mode photography",
      "onnx runtime",
      "nvlink",
      "ai's sdg impact potential",
      "dennard scaling",
      "meltdown/spectre impact",
      "iot data explosion",
      "inference attack",
      "xor problem",
      "brittleness in ai systems",
      "smart glasses with tinyml",
      "a11 bionic breakthrough",
      "tinytl memory breakthrough",
      "reasoning performance cliff",
      "attention scaling",
      "intel 8087 impact",
      "sigmoid computational cost",
      "ml hardware cost spectrum",
      "sequential neural networks",
      "model memorization",
      "cloudsuite",
      "stm32f4 microcontroller reality",
      "memory coherence",
      "ibm watson health",
      "embedded systems",
      "coin-cell batteries",
      "transient vs permanent faults",
      "gradient-based attacks",
      "spec power",
      "reasoning architecture requirements",
      "docker",
      "rfm analysis origins",
      "reinforcement learning nas",
      "iot ecosystems"
    ]
  },
  "duplicates": {
    "duplicate_ids": {
      "fn-flops": [
        "benchmarking",
        "dl_primer",
        "hw_acceleration",
        "optimizations"
      ],
      "fn-npu": [
        "benchmarking",
        "ml_systems"
      ],
      "fn-fpga": [
        "benchmarking",
        "robust_ai"
      ],
      "fn-operator-fusion": [
        "benchmarking",
        "optimizations"
      ],
      "fn-model-compression": [
        "benchmarking",
        "ml_systems"
      ],
      "fn-reproducibility-crisis": [
        "data_engineering",
        "ops"
      ],
      "fn-batch-processing": [
        "dl_primer",
        "data_engineering",
        "efficient_ai"
      ],
      "fn-brain-efficiency": [
        "dl_primer",
        "frontiers"
      ],
      "fn-mnist-dataset": [
        "dl_primer",
        "dnn_architectures"
      ],
      "fn-systolic-array": [
        "frameworks",
        "training",
        "dnn_architectures"
      ],
      "fn-memory-bandwidth": [
        "efficient_ai",
        "hw_acceleration",
        "optimizations"
      ],
      "fn-gradient-accumulation": [
        "training",
        "frameworks"
      ],
      "fn-gradient-checkpointing": [
        "frameworks",
        "optimizations",
        "ondevice_learning"
      ],
      "fn-dennard-scaling": [
        "hw_acceleration",
        "sustainable_ai"
      ],
      "fn-data-centers": [
        "ml_systems",
        "introduction"
      ],
      "fn-microcontrollers": [
        "ml_systems",
        "introduction"
      ],
      "fn-latency": [
        "ml_systems",
        "introduction"
      ],
      "fn-embedded-systems": [
        "ml_systems",
        "robust_ai"
      ],
      "fn-inference-latency": [
        "ml_systems",
        "optimizations"
      ],
      "fn-ml-apis": [
        "ml_systems",
        "privacy_security"
      ],
      "fn-gdpr": [
        "ml_systems",
        "privacy_security"
      ],
      "fn-tflite": [
        "ml_systems",
        "ondevice_learning"
      ],
      "fn-mixed-precision": [
        "optimizations",
        "ondevice_learning"
      ],
      "fn-lora": [
        "optimizations",
        "ondevice_learning"
      ],
      "fn-differential-privacy": [
        "responsible_ai",
        "ondevice_learning"
      ],
      "fn-data-poisoning": [
        "privacy_security",
        "robust_ai"
      ],
      "fn-backdoor-attacks": [
        "privacy_security",
        "robust_ai"
      ],
      "fn-federated-learning": [
        "responsible_ai",
        "workflow"
      ],
      "fn-edge-computing": [
        "sustainable_ai",
        "robust_ai"
      ]
    },
    "duplicate_terms": {
      "imagenet": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-bench-imagenet"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-imagenet"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-intro-imagenet"
        }
      ],
      "alexnet": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-bench-alexnet"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-alexnet"
        },
        {
          "file": "training",
          "footnote_id": "fn-training-alexnet"
        }
      ],
      "resnet": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-bench-resnet"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-resnet"
        }
      ],
      "tensor processing unit (tpu)": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-bench-tpu"
        },
        {
          "file": "dl_primer",
          "footnote_id": "fn-dlprimer-tpu"
        },
        {
          "file": "frontiers",
          "footnote_id": "fn-tpu"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-intro-tpu"
        },
        {
          "file": "ml_systems",
          "footnote_id": "fn-mlsys-tpu"
        }
      ],
      "flops": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-flops"
        },
        {
          "file": "dl_primer",
          "footnote_id": "fn-flops"
        },
        {
          "file": "dl_primer",
          "footnote_id": "fn-dlprimer-flops"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-flops"
        }
      ],
      "gpt-3": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-bench-gpt3"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-gpt3"
        }
      ],
      "mixed-precision training": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-bench-mixed-precision"
        },
        {
          "file": "ondevice_learning",
          "footnote_id": "fn-mixed-precision"
        },
        {
          "file": "optimizations",
          "footnote_id": "fn-mixed-precision"
        },
        {
          "file": "training",
          "footnote_id": "fn-training-mixed-precision"
        }
      ],
      "data parallelism": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-data-parallel"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-data-parallelism"
        },
        {
          "file": "frameworks",
          "footnote_id": "fn-data-parallelism"
        }
      ],
      "model parallelism": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-model-parallel"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-model-parallelism"
        },
        {
          "file": "frameworks",
          "footnote_id": "fn-model-parallelism"
        }
      ],
      "neural processing unit (npu)": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-npu"
        },
        {
          "file": "ml_systems",
          "footnote_id": "fn-npu"
        }
      ],
      "operator fusion": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-operator-fusion"
        },
        {
          "file": "optimizations",
          "footnote_id": "fn-operator-fusion"
        }
      ],
      "ml reproducibility crisis": [
        {
          "file": "data_engineering",
          "footnote_id": "fn-reproducibility-crisis"
        },
        {
          "file": "ops",
          "footnote_id": "fn-reproducibility-crisis"
        }
      ],
      "data versioning challenges": [
        {
          "file": "data_engineering",
          "footnote_id": "fn-dataeng-data-versioning"
        },
        {
          "file": "workflow",
          "footnote_id": "fn-workflow-data-versioning"
        }
      ],
      "perceptron": [
        {
          "file": "dl_primer",
          "footnote_id": "fn-perceptron"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-early"
        }
      ],
      "backpropagation": [
        {
          "file": "dl_primer",
          "footnote_id": "fn-dlprimer-backpropagation"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-backprop"
        }
      ],
      "mnist dataset": [
        {
          "file": "dl_primer",
          "footnote_id": "fn-mnist-dataset"
        },
        {
          "file": "dnn_architectures",
          "footnote_id": "fn-mnist-dataset"
        }
      ],
      "batch processing": [
        {
          "file": "dl_primer",
          "footnote_id": "fn-batch-processing"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-batch-processing"
        }
      ],
      "backpropagation algorithm": [
        {
          "file": "dnn_architectures",
          "footnote_id": "fn-dnn-backpropagation"
        },
        {
          "file": "training",
          "footnote_id": "fn-backpropagation"
        }
      ],
      "systolic array": [
        {
          "file": "dnn_architectures",
          "footnote_id": "fn-systolic-array"
        },
        {
          "file": "frameworks",
          "footnote_id": "fn-systolic-array"
        }
      ],
      "memory bandwidth": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-memory-bandwidth"
        },
        {
          "file": "hw_acceleration",
          "footnote_id": "fn-memory-bandwidth"
        },
        {
          "file": "optimizations",
          "footnote_id": "fn-memory-bandwidth"
        }
      ],
      "stochastic gradient descent (sgd)": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-sgd"
        },
        {
          "file": "ondevice_learning",
          "footnote_id": "fn-sgd"
        }
      ],
      "moore's law": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-moores-law"
        },
        {
          "file": "hw_acceleration",
          "footnote_id": "fn-moores-law"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-mooreslaw"
        }
      ],
      "transfer learning": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-transfer-learning"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-intro-transfer-learning"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-transfer"
        },
        {
          "file": "workflow",
          "footnote_id": "fn-workflow-transfer-learning"
        }
      ],
      "foundation models": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-efficient-foundation-models"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-intro-foundation-models"
        }
      ],
      "gradient checkpointing": [
        {
          "file": "frameworks",
          "footnote_id": "fn-gradient-checkpointing"
        },
        {
          "file": "ondevice_learning",
          "footnote_id": "fn-gradient-checkpointing"
        },
        {
          "file": "optimizations",
          "footnote_id": "fn-gradient-checkpointing"
        }
      ],
      "just-in-time (jit) compilation": [
        {
          "file": "frameworks",
          "footnote_id": "fn-jit-ml"
        },
        {
          "file": "frameworks",
          "footnote_id": "jit-compilation"
        }
      ],
      "automatic differentiation": [
        {
          "file": "frameworks",
          "footnote_id": "fn-auto-diff"
        },
        {
          "file": "training",
          "footnote_id": "fn-autodiff"
        }
      ],
      "dennard scaling": [
        {
          "file": "hw_acceleration",
          "footnote_id": "fn-dennard-scaling"
        },
        {
          "file": "sustainable_ai",
          "footnote_id": "fn-dennard-scaling"
        }
      ],
      "data centers": [
        {
          "file": "introduction",
          "footnote_id": "fn-data-centers"
        },
        {
          "file": "ml_systems",
          "footnote_id": "fn-data-centers"
        }
      ],
      "microcontrollers": [
        {
          "file": "introduction",
          "footnote_id": "fn-microcontrollers"
        },
        {
          "file": "ml_systems",
          "footnote_id": "fn-microcontrollers"
        }
      ],
      "embedded systems": [
        {
          "file": "ml_systems",
          "footnote_id": "fn-embedded-systems"
        },
        {
          "file": "robust_ai",
          "footnote_id": "fn-embedded-systems"
        }
      ],
      "ml apis": [
        {
          "file": "ml_systems",
          "footnote_id": "fn-ml-apis"
        },
        {
          "file": "privacy_security",
          "footnote_id": "fn-ml-apis"
        }
      ],
      "tensorflow lite": [
        {
          "file": "ml_systems",
          "footnote_id": "fn-tflite"
        },
        {
          "file": "ondevice_learning",
          "footnote_id": "fn-tflite"
        }
      ],
      "federated learning architecture": [
        {
          "file": "ml_systems",
          "footnote_id": "fn-federated-architecture"
        },
        {
          "file": "workflow",
          "footnote_id": "fn-federated-learning"
        }
      ],
      "quantization-aware training": [
        {
          "file": "ondevice_learning",
          "footnote_id": "fn-quantization-aware"
        },
        {
          "file": "optimizations",
          "footnote_id": "fn-qat-performance"
        }
      ],
      "lora (low-rank adaptation)": [
        {
          "file": "ondevice_learning",
          "footnote_id": "fn-lora"
        },
        {
          "file": "optimizations",
          "footnote_id": "fn-lora"
        }
      ],
      "differential privacy": [
        {
          "file": "ondevice_learning",
          "footnote_id": "fn-differential-privacy"
        },
        {
          "file": "responsible_ai",
          "footnote_id": "fn-differential-privacy-table"
        },
        {
          "file": "responsible_ai",
          "footnote_id": "fn-differential-privacy"
        }
      ],
      "data poisoning": [
        {
          "file": "privacy_security",
          "footnote_id": "fn-data-poisoning"
        },
        {
          "file": "robust_ai",
          "footnote_id": "fn-data-poisoning"
        }
      ],
      "backdoor attacks": [
        {
          "file": "privacy_security",
          "footnote_id": "fn-backdoor-attacks"
        },
        {
          "file": "robust_ai",
          "footnote_id": "fn-backdoor-attacks"
        }
      ],
      "bayesian neural networks": [
        {
          "file": "robust_ai",
          "footnote_id": "fn-bayesian-neural-networks"
        },
        {
          "file": "robust_ai",
          "footnote_id": "fn-bayesian-nn"
        }
      ]
    },
    "undefined_references": [],
    "unused_definitions": []
  },
  "by_chapter": [
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "chapter_name": "ai_for_good",
      "total_references": 40,
      "total_definitions": 20,
      "footnote_ids": [
        "fn-cassava-impact",
        "fn-climate-action-ai",
        "fn-cough-detection",
        "fn-ebola-outbreak",
        "fn-esp32-constraints",
        "fn-farmbeats",
        "fn-genomics-cloud",
        "fn-global-fishing",
        "fn-gunshot-detection",
        "fn-lora-technology",
        "fn-microclimate-monitoring",
        "fn-mosquito-detection",
        "fn-plantvillage-nuru",
        "fn-raspberry-pi-development",
        "fn-resource-paradox",
        "fn-satellite-disaster",
        "fn-sdg-adoption",
        "fn-sdg-ai-potential",
        "fn-smallholder-farmers",
        "fn-thermal-imaging-rescue"
      ],
      "terms_defined": [
        "2014-2016 Ebola Outbreak",
        "AI for Climate Action",
        "AI's SDG Impact Potential",
        "Acoustic Gunshot Detection",
        "Cassava Disease Impact",
        "Cloud Genomics Scale",
        "Cough Analysis Technology",
        "ESP32 Capabilities",
        "Global Fishing Watch Impact",
        "LoRa Technology",
        "Microclimate Monitoring",
        "Microsoft FarmBeats",
        "Mosquito Species Detection",
        "PlantVillage Nuru Real-World Impact",
        "Raspberry Pi Development Advantages",
        "SDG Global Impact",
        "Satellite Disaster Monitoring",
        "Smallholder Farmers Global Impact",
        "Social Good Resource Paradox",
        "Thermal Imaging in Disaster Response"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "chapter_name": "benchmarking",
      "total_references": 78,
      "total_definitions": 38,
      "footnote_ids": [
        "fn-3dmark",
        "fn-asic",
        "fn-bench-alexnet",
        "fn-bench-gpt3",
        "fn-bench-imagenet",
        "fn-bench-mixed-precision",
        "fn-bench-resnet",
        "fn-bench-tpu",
        "fn-bert",
        "fn-cloudsuite",
        "fn-cudnn",
        "fn-data-parallel",
        "fn-dhrystone",
        "fn-docker",
        "fn-edge-tpu",
        "fn-energy-star",
        "fn-flops",
        "fn-fp16",
        "fn-fp32",
        "fn-fpga",
        "fn-green500",
        "fn-hardware-lottery",
        "fn-int8",
        "fn-linpack",
        "fn-mobilemark",
        "fn-model-compression",
        "fn-model-parallel",
        "fn-npu",
        "fn-onnx-runtime",
        "fn-operator-fusion",
        "fn-roofline-model",
        "fn-serverless-ai",
        "fn-spec",
        "fn-spec-power",
        "fn-tensor-ops",
        "fn-tensorrt",
        "fn-tvm",
        "fn-whetstone"
      ],
      "terms_defined": [
        "3DMark",
        "AlexNet",
        "Application-Specific Integrated Circuit (ASIC)",
        "BERT",
        "CloudSuite",
        "Data Parallelism",
        "Dhrystone",
        "Docker",
        "ENERGY STAR",
        "Edge TPU",
        "FLOPS",
        "FP16",
        "FP32",
        "Field-Programmable Gate Array (FPGA)",
        "GPT-3",
        "Green500",
        "Hardware Lottery",
        "INT8",
        "ImageNet",
        "LINPACK",
        "Mixed-Precision Training",
        "MobileMark",
        "Model Compression",
        "Model Parallelism",
        "Neural Processing Unit (NPU)",
        "ONNX Runtime",
        "Operator Fusion",
        "ResNet",
        "Roofline Model",
        "SPEC CPU",
        "SPEC Power",
        "Serverless AI",
        "TVM",
        "Tensor Operations",
        "Tensor Processing Unit (TPU)",
        "TensorRT",
        "Whetstone",
        "cuDNN"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "chapter_name": "conclusion",
      "total_references": 8,
      "total_definitions": 4,
      "footnote_ids": [
        "fn-chatgpt-adoption",
        "fn-gpt3-energy",
        "fn-pruning-sparsity",
        "fn-tinyml-constraints"
      ],
      "terms_defined": []
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "chapter_name": "data_engineering",
      "total_references": 74,
      "total_definitions": 37,
      "footnote_ids": [
        "fn-apache-beam",
        "fn-audit-trails",
        "fn-batch-processing",
        "fn-bigquery-performance",
        "fn-blockchain-governance",
        "fn-burst-buffers",
        "fn-categorical-encoding",
        "fn-columnar-formats",
        "fn-concept-drift",
        "fn-data-lake-origins",
        "fn-data-lineage",
        "fn-data-quality-stats",
        "fn-dataeng-data-versioning",
        "fn-etl-history",
        "fn-etl-vs-elt",
        "fn-feature-stores",
        "fn-google-crowdsource",
        "fn-hashing-security",
        "fn-hdfs-origins",
        "fn-iot-data-volume",
        "fn-kaggle",
        "fn-mechanical-turk",
        "fn-medical-imaging",
        "fn-model-scaling",
        "fn-mysql-scale",
        "fn-normalization-techniques",
        "fn-open-images",
        "fn-privacy-regulations",
        "fn-pseudonymization-gdpr",
        "fn-recaptcha-evolution",
        "fn-redis-performance",
        "fn-reproducibility-crisis",
        "fn-rfm-analysis",
        "fn-snappy-compression",
        "fn-uci-repository",
        "fn-watson-health",
        "fn-waze-crowdsourcing"
      ],
      "terms_defined": [
        "Apache Beam Architecture",
        "Batch Processing Evolution",
        "BigQuery Serverless Power",
        "Blockchain for ML Governance",
        "Burst Buffers",
        "Categorical Encoding Impact",
        "Columnar Format Revolution",
        "Concept Drift Challenge",
        "Cryptographic Hashing",
        "Data Lake Origins",
        "Data Lineage Systems",
        "Data Quality Reality",
        "Data Versioning Challenges",
        "ETL Evolution",
        "ETL vs ELT Performance",
        "Feature Store Evolution",
        "Google Crowdsource",
        "HDFS Origins",
        "IBM Watson Health",
        "IoT Data Explosion",
        "Kaggle",
        "ML Audit Requirements",
        "ML Reproducibility Crisis",
        "Mechanical Turk Origins",
        "Medical Imaging AI Revolution",
        "Model Scaling Explosion",
        "MySQL at Scale",
        "Normalization in ML",
        "Open Images Dataset",
        "Privacy Regulation Timeline",
        "Pseudonymization under GDPR",
        "RFM Analysis Origins",
        "Redis Performance",
        "Snappy Compression Trade-offs",
        "UCI ML Repository",
        "Waze Crowdsourcing Model",
        "reCAPTCHA Evolution"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "chapter_name": "dl_primer",
      "total_references": 46,
      "total_definitions": 23,
      "footnote_ids": [
        "fn-batch-processing",
        "fn-brain-efficiency",
        "fn-cross-entropy",
        "fn-dl-frameworks",
        "fn-dlprimer-backpropagation",
        "fn-dlprimer-flops",
        "fn-dlprimer-tpu",
        "fn-epoch-training",
        "fn-floating-point",
        "fn-flops",
        "fn-gpu-parallel",
        "fn-gradient-descent",
        "fn-hog-method",
        "fn-imagenet-progress",
        "fn-learning-rate",
        "fn-mnist-dataset",
        "fn-overfitting",
        "fn-perceptron",
        "fn-relu-function",
        "fn-sift",
        "fn-synapses",
        "fn-vanishing",
        "fn-xor-problem"
      ],
      "terms_defined": [
        "32-bit Floating Point Precision",
        "Backpropagation",
        "Batch Processing",
        "Brain Energy Efficiency",
        "Cross-Entropy Loss",
        "Deep Learning Frameworks",
        "Epoch",
        "FLOPS",
        "FLOPS",
        "GPU (Graphics Processing Unit)",
        "Gradient Descent",
        "Histogram of Oriented Gradients (HOG)",
        "ImageNet Competition Progress",
        "Learning Rate",
        "MNIST Dataset",
        "Overfitting",
        "Perceptron",
        "ReLU (Rectified Linear Unit)",
        "Scale-Invariant Feature Transform (SIFT)",
        "Synapses",
        "Tensor Processing Unit (TPU)",
        "Vanishing Gradients",
        "XOR Problem"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "chapter_name": "dnn_architectures",
      "total_references": 52,
      "total_definitions": 26,
      "footnote_ids": [
        "fn-activation-functions",
        "fn-attention-is-all-you-need",
        "fn-attention-notation",
        "fn-attention-qkv",
        "fn-attention-scaling",
        "fn-cnn-convolution-notation",
        "fn-dnn-backpropagation",
        "fn-dnn-blas",
        "fn-dnn-imagenet",
        "fn-dnn-resnet",
        "fn-dnn-tpu",
        "fn-gemm",
        "fn-im2col",
        "fn-lecun-cnn",
        "fn-lstm-invention",
        "fn-mlp-notation",
        "fn-mnist-dataset",
        "fn-parameter-scaling",
        "fn-parameter-sharing",
        "fn-receptive-field",
        "fn-simd",
        "fn-systolic-array",
        "fn-translation-invariance",
        "fn-uat",
        "fn-vanishing-gradient",
        "fn-vision-transformers"
      ],
      "terms_defined": [
        "\"Attention is All You Need\"",
        "Activation Functions",
        "Attention Mechanism Notation",
        "Attention Scaling",
        "Backpropagation Algorithm",
        "Basic Linear Algebra Subprograms (BLAS)",
        "CNN Convolution Notation",
        "GEMM (General Matrix Multiply)",
        "ImageNet Revolution",
        "LSTM Origins",
        "MLP Mathematical Notation",
        "MNIST Dataset",
        "Parameter Scaling",
        "Parameter Sharing",
        "Query-Key-Value Attention",
        "Receptive Field",
        "ResNet Revolution",
        "SIMD (Single Instruction, Multiple Data)",
        "Systolic Array",
        "Tensor Processing Units",
        "Translation Invariance",
        "Universal Approximation Theorem",
        "Vanishing Gradient Problem",
        "Vision Transformers (ViTs)",
        "Yann LeCun and CNNs",
        "im2col (Image to Column)"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "chapter_name": "efficient_ai",
      "total_references": 76,
      "total_definitions": 38,
      "footnote_ids": [
        "fn-active-learning",
        "fn-autoregressive",
        "fn-batch-processing",
        "fn-carbon-emissions",
        "fn-cifar10",
        "fn-cuda-cores",
        "fn-curriculum-learning",
        "fn-data-augmentation",
        "fn-data-centric-ai",
        "fn-distributed-infrastructure",
        "fn-efficient-alexnet",
        "fn-efficient-data-parallelism",
        "fn-efficient-flops",
        "fn-efficient-foundation-models",
        "fn-efficient-gpt3",
        "fn-efficient-imagenet",
        "fn-efficient-model-parallelism",
        "fn-efficient-moores-law",
        "fn-efficient-resnet",
        "fn-efficient-sgd",
        "fn-efficient-transfer-learning",
        "fn-efficientnet",
        "fn-lidar",
        "fn-memory-bandwidth",
        "fn-memory-usage",
        "fn-mnist",
        "fn-mobilenet",
        "fn-npus",
        "fn-param-efficient",
        "fn-pca",
        "fn-perplexity",
        "fn-scaling-laws",
        "fn-self-supervised",
        "fn-squeezenet",
        "fn-tinyml",
        "fn-tokens",
        "fn-transformer",
        "fn-uci"
      ],
      "terms_defined": [
        "Active Learning",
        "AlexNet",
        "Autoregressive Models",
        "Batch Processing",
        "CIFAR-10",
        "CUDA Cores",
        "Carbon Emissions",
        "Curriculum Learning",
        "Data Augmentation",
        "Data Parallelism",
        "Data-Centric AI",
        "Distributed Infrastructure",
        "EfficientNet",
        "FLOPs",
        "Foundation Models",
        "GPT-3",
        "ImageNet",
        "LiDAR",
        "MNIST",
        "Memory Bandwidth",
        "Memory Usage",
        "MobileNet",
        "Model Parallelism",
        "Moore's Law",
        "Neural Processing Units (NPUs)",
        "Parameter-Efficient Fine-tuning",
        "Perplexity",
        "Principal Component Analysis (PCA)",
        "ResNet",
        "Scaling Laws",
        "Self-Supervised Learning",
        "SqueezeNet",
        "Stochastic Gradient Descent (SGD)",
        "TinyML",
        "Tokens",
        "Transfer Learning",
        "Transformer",
        "UCI Machine Learning Repository"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "chapter_name": "frameworks",
      "total_references": 58,
      "total_definitions": 29,
      "footnote_ids": [
        "fn-asic-ml",
        "fn-auto-diff",
        "fn-comp-graphs",
        "fn-dag-ml",
        "fn-data-parallelism",
        "fn-eager-execution",
        "fn-frameworks-blas",
        "fn-frameworks-tpu",
        "fn-gradient-accumulation",
        "fn-gradient-checkpointing",
        "fn-intermediate-representation",
        "fn-jax",
        "fn-jit-ml",
        "fn-kernel-fusion",
        "fn-lapack",
        "fn-linear-transformations",
        "fn-memory-planning",
        "fn-model-parallelism",
        "fn-operation-fusion",
        "fn-pytorch",
        "fn-static-graph",
        "fn-systolic-array",
        "fn-tensorflow",
        "fn-theano",
        "immutable-data",
        "jit-compilation",
        "pure-function",
        "stateless-function",
        "vectorization"
      ],
      "terms_defined": [
        "ASIC (Application-Specific Integrated Circuit)",
        "Automatic Differentiation",
        "Automatic Vectorization",
        "BLAS (Basic Linear Algebra Subprograms)",
        "Computational Graphs",
        "Data Parallelism",
        "Directed Acyclic Graph (DAG)",
        "Eager Execution",
        "Gradient Accumulation",
        "Gradient Checkpointing",
        "Immutable Data Structures",
        "Intermediate Representation (IR)",
        "JAX",
        "Just-In-Time (JIT) Compilation",
        "Just-in-Time (JIT) Compilation",
        "Kernel Fusion",
        "LAPACK (Linear Algebra Package)",
        "Linear Transformations",
        "Memory Planning",
        "Model Parallelism",
        "Operation Fusion",
        "Pure Function",
        "PyTorch",
        "Stateless Function",
        "Static Computational Graph",
        "Systolic Array",
        "TPU (Tensor Processing Unit)",
        "TensorFlow",
        "Theano"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "chapter_name": "frontiers",
      "total_references": 80,
      "total_definitions": 40,
      "footnote_ids": [
        "fn-3d-stacking",
        "fn-agi-agent-scale",
        "fn-agi-byzantine",
        "fn-agi-communication",
        "fn-agi-compute-requirements",
        "fn-agi-consensus",
        "fn-agi-infrastructure-scale",
        "fn-agi-resource-coordination",
        "fn-agi-topology",
        "fn-alignment-challenge",
        "fn-alignment-components",
        "fn-brain-efficiency",
        "fn-catastrophic",
        "fn-chatgpt-growth",
        "fn-chiplet-benefits",
        "fn-constitutional-approach",
        "fn-constitutional-intro",
        "fn-deployment-freeze",
        "fn-embodiment-constraints",
        "fn-energy-models",
        "fn-explainability-demand",
        "fn-gpt4-energy",
        "fn-human-feedback-limits",
        "fn-infra-bottleneck",
        "fn-intelligence-theory",
        "fn-llm-definition",
        "fn-moores-end",
        "fn-multi-agent",
        "fn-neuromorphic-promise",
        "fn-optical-interconnects",
        "fn-personalization-tech",
        "fn-phase-transition",
        "fn-processing-in-memory",
        "fn-quantum-hybrid",
        "fn-realtime-requirements",
        "fn-reasoning-limitation",
        "fn-reasoning-requirements",
        "fn-rlhf-impact",
        "fn-tpu",
        "fn-workflow-automation"
      ],
      "terms_defined": [
        "3D Chip Stacking",
        "AGI Agent Scale",
        "AGI Byzantine Threats",
        "AGI Communication Complexity",
        "AGI Compute Requirements Engineering Analysis",
        "AGI Consensus Complexity",
        "AGI Infrastructure Scale Engineering Analysis",
        "AGI Network Topology",
        "AGI Resource Coordination",
        "Alignment Failure Modes",
        "Alignment Technical Challenges",
        "Biological Efficiency",
        "Catastrophic Forgetting Severity",
        "ChatGPT User Adoption",
        "Chiplet Architecture",
        "Constitutional AI",
        "Constitutional AI Method",
        "End of Moore's Law",
        "Energy-Based Models (EBMs)",
        "Explainability Market Growth",
        "GPT-4 Energy Consumption",
        "Human Feedback Bottlenecks",
        "Infrastructure Efficiency Gap",
        "Intelligence vs. Performance",
        "Large Language Models (LLMs)",
        "Multi-Agent Intelligence",
        "Neuromorphic Computing",
        "Optical Interconnects",
        "Personalization Technical Foundations",
        "Phase Transition",
        "Processing-in-Memory (PIM)",
        "Quantum-Classical Hybrid",
        "RLHF Effectiveness",
        "Real-Time Latency Requirements",
        "Reasoning Architecture Requirements",
        "Reasoning Performance Cliff",
        "Robotic System Requirements",
        "Static Model Problem",
        "Tensor Processing Unit (TPU)",
        "Workflow Automation Scale"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "chapter_name": "hw_acceleration",
      "total_references": 50,
      "total_definitions": 25,
      "footnote_ids": [
        "fn-asics",
        "fn-chiplet",
        "fn-coprocessor",
        "fn-cray-vector",
        "fn-dennard-scaling",
        "fn-dsa",
        "fn-flops",
        "fn-gflops",
        "fn-hbm",
        "fn-heterogeneous",
        "fn-hwacc-alexnet",
        "fn-hwacc-neural-engine",
        "fn-hwacc-tensor-cores",
        "fn-hwacc-tpu",
        "fn-intel-8087",
        "fn-l1-cache",
        "fn-memory-bandwidth",
        "fn-memory-coherence",
        "fn-moores-law",
        "fn-risc-v-ai",
        "fn-simd-evolution",
        "fn-systolic-origin",
        "fn-tops",
        "fn-von-neumann",
        "fn-wafer-scale"
      ],
      "terms_defined": [
        "AlexNet's GPU Revolution",
        "Apple's Neural Engine Strategy",
        "Application-Specific Integrated Circuits (ASICs)",
        "Chiplet",
        "Coprocessor",
        "Cray-1 Vector Legacy",
        "Dennard Scaling",
        "Domain-Specific Architectures (DSA)",
        "FLOPS (Floating-Point Operations Per Second)",
        "GFLOPS (Giga Floating-Point Operations Per Second)",
        "Heterogeneous Computing",
        "High-Bandwidth Memory (HBM)",
        "Intel 8087 Impact",
        "L1 Cache",
        "Memory Bandwidth",
        "Memory Coherence",
        "Moore's Law",
        "RISC-V for AI",
        "SIMD Evolution",
        "Systolic Array Renaissance",
        "TOPS (Tera Operations Per Second)",
        "TPU Origins",
        "Tensor Core Breakthrough",
        "Von Neumann Architecture",
        "Wafer-Scale Integration"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "chapter_name": "introduction",
      "total_references": 79,
      "total_definitions": 39,
      "footnote_ids": [
        "fn-amdahls-law",
        "fn-backprop",
        "fn-backprop-history",
        "fn-black-box",
        "fn-brittleness",
        "fn-cascade",
        "fn-cnn",
        "fn-computer-engineering",
        "fn-cuda",
        "fn-dartmouth-conference",
        "fn-data-centers",
        "fn-distributed-systems",
        "fn-drift",
        "fn-early",
        "fn-edge",
        "fn-electrical-grids",
        "fn-eliza",
        "fn-inference",
        "fn-intro-foundation-models",
        "fn-intro-imagenet",
        "fn-intro-tpu",
        "fn-intro-transfer-learning",
        "fn-iot-networks",
        "fn-latency",
        "fn-mainframes",
        "fn-mas",
        "fn-microcontrollers",
        "fn-mooreslaw",
        "fn-neurons",
        "fn-paradigm-shift",
        "fn-parameters",
        "fn-particle-accelerators",
        "fn-petabytes",
        "fn-rnn",
        "fn-superhuman-capabilities",
        "fn-sutton-turing",
        "fn-training-challenges",
        "fn-transfer",
        "fn-viola-jones"
      ],
      "terms_defined": [
        "Amdahl's Law",
        "Artificial Neurons",
        "Backpropagation",
        "Backpropagation (Historical Context)",
        "Black Box",
        "Brittleness in AI Systems",
        "CUDA (Compute Unified Device Architecture)",
        "Cascade of Classifiers",
        "Computer Engineering",
        "Convolutional Neural Network (CNN)",
        "Dartmouth Conference (1956)",
        "Data Centers",
        "Data Drift",
        "Distributed Systems",
        "ELIZA",
        "Edge Processor",
        "Electrical Grids",
        "Foundation Models",
        "ImageNet",
        "Inference Attack",
        "IoT (Internet of Things) Networks",
        "Large-Scale Training Challenges",
        "Latency",
        "Mainframes",
        "Microcontrollers",
        "Moore's Law",
        "Multi-Agent System",
        "Paradigm Shift",
        "Parameters",
        "Particle Accelerators",
        "Perceptron",
        "Petabytes",
        "Richard Sutton",
        "Sequential Neural Networks",
        "Superhuman AI Capabilities",
        "Tensor Processing Unit (TPU)",
        "Transfer Learning",
        "Transfer Learning",
        "Viola-Jones Algorithm"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "chapter_name": "ml_systems",
      "total_references": 112,
      "total_definitions": 56,
      "footnote_ids": [
        "fn-automl",
        "fn-aws-sagemaker",
        "fn-battery-life",
        "fn-billion-parameters",
        "fn-cloud-evolution",
        "fn-coin-cell",
        "fn-collaborative-filtering",
        "fn-computational-photography",
        "fn-coreml",
        "fn-cost-spectrum",
        "fn-data-centers",
        "fn-deployment-environments",
        "fn-device-size",
        "fn-edge-coordination",
        "fn-edge-latency",
        "fn-embedded-systems",
        "fn-endpoint-constraints",
        "fn-energy-efficiency",
        "fn-face-detection",
        "fn-federated-architecture",
        "fn-fitness-trackers",
        "fn-gdpr",
        "fn-hipaa",
        "fn-hyperscale",
        "fn-industrial-iot",
        "fn-industry-40",
        "fn-inference-latency",
        "fn-iot-ecosystems",
        "fn-iot-growth",
        "fn-iot-hubs",
        "fn-jetson-ecosystem",
        "fn-latency",
        "fn-latency-critical",
        "fn-memory-bottleneck",
        "fn-memory-comparison",
        "fn-microcontrollers",
        "fn-ml-apis",
        "fn-mlsys-quantization",
        "fn-mlsys-tpu",
        "fn-mobile-constraints",
        "fn-mobile-power",
        "fn-mobile-soc",
        "fn-mobile-storage",
        "fn-model-compression",
        "fn-nlp-compute",
        "fn-npu",
        "fn-on-device-training",
        "fn-paas-pricing",
        "fn-portrait-mode",
        "fn-predictive-maintenance",
        "fn-real-time-translation",
        "fn-smart-glasses",
        "fn-tflite",
        "fn-tinyml-optimization",
        "fn-train-serve-split",
        "fn-voice-recognition"
      ],
      "terms_defined": [
        "AWS SageMaker",
        "AutoML (Automated Machine Learning)",
        "Billion-Parameter Models",
        "Cloud Inference Latency",
        "Cloud Infrastructure Evolution",
        "Coin-Cell Batteries",
        "Collaborative Filtering",
        "Computational Photography",
        "Core ML",
        "Data Centers",
        "Deployment Environments",
        "Edge Latency Advantage",
        "Edge Network Coordination",
        "Embedded Systems",
        "Endpoint Device Constraints",
        "Energy Efficiency in TinyML",
        "Federated Learning Architecture",
        "GDPR (General Data Protection Regulation)",
        "HIPAA (Health Insurance Portability and Accountability Act)",
        "Hyperscale Data Centers",
        "Industrial IoT",
        "Industry 4.0",
        "IoT Device Growth",
        "IoT Ecosystems",
        "IoT Hubs",
        "Latency vs Throughput",
        "Latency-Critical Applications",
        "ML APIs",
        "ML Hardware Cost Spectrum",
        "Memory Bottleneck",
        "Memory Scale Comparison",
        "Microcontrollers",
        "Mobile Device Constraints",
        "Mobile Face Detection",
        "Mobile Power Constraints",
        "Mobile Storage Evolution",
        "Mobile System-on-Chip",
        "Model Optimization",
        "NLP Computational Demands",
        "NVIDIA Jetson Ecosystem",
        "Neural Processing Unit (NPU)",
        "On-Device Training Constraints",
        "Pay-as-You-Go Pricing",
        "Portrait Mode Photography",
        "Predictive Maintenance",
        "Real-Time Translation",
        "Smart Glasses with TinyML",
        "Tensor Processing Unit (TPU)",
        "TensorFlow Lite",
        "TinyML Device Scale",
        "TinyML Model Optimization",
        "TinyML Optimization",
        "TinyML in Fitness Trackers",
        "Train-Serve Split Economics",
        "Ultra-Long Battery Life",
        "Voice Recognition Evolution"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "chapter_name": "ondevice_learning",
      "total_references": 68,
      "total_definitions": 34,
      "footnote_ids": [
        "fn-a11-bionic-breakthrough",
        "fn-activation-caching",
        "fn-arduino-constraints",
        "fn-arm-cortex-spectrum",
        "fn-depthwise-separable",
        "fn-differential-privacy",
        "fn-dvfs-mobile",
        "fn-esp32-capabilities",
        "fn-fedasync",
        "fn-fedavg",
        "fn-federated-birth",
        "fn-few-shot-learning",
        "fn-gdpr-impact",
        "fn-gradient-checkpointing",
        "fn-gradient-quantization",
        "fn-gradient-sparsification",
        "fn-hey-siri-constraints",
        "fn-lora",
        "fn-mfcc",
        "fn-microcontroller-power",
        "fn-mixed-precision",
        "fn-mobilenet-innovation",
        "fn-near-memory-compute",
        "fn-non-iid",
        "fn-ondevice-neural-engine",
        "fn-quantization-aware",
        "fn-sgd",
        "fn-stm32-constraints",
        "fn-tensor-soc",
        "fn-tflite",
        "fn-tinyml-scale",
        "fn-tinytl-efficiency",
        "fn-transformer-mobile",
        "fn-wireless-constraints"
      ],
      "terms_defined": [
        "\"Hey Siri\" Technical Reality",
        "A11 Bionic Breakthrough",
        "ARM Cortex Architecture Spectrum",
        "Activation Caching",
        "Apple Neural Engine Evolution",
        "Arduino Edge Computing Reality",
        "Asynchronous Federated Learning (FedAsync)",
        "Depthwise Separable Convolutions",
        "Differential Privacy",
        "Dynamic Voltage and Frequency Scaling (DVFS)",
        "ESP32 Edge Computing",
        "Federated Averaging (FedAvg)",
        "Federated Learning Birth",
        "Few-Shot Learning",
        "GDPR's ML Impact",
        "Google Tensor SoC Architecture",
        "Gradient Checkpointing",
        "Gradient Quantization",
        "Gradient Sparsification",
        "Lightweight Transformers",
        "LoRA (Low-Rank Adaptation)",
        "Mel-Frequency Cepstral Coefficients (MFCCs)",
        "Microcontroller Power Budget Reality",
        "Mixed-Precision Training",
        "MobileNet Innovation",
        "Near-Memory Computing",
        "Non-IID (Non-Independent and Identically Distributed)",
        "Quantization-Aware Training",
        "STM32F4 Microcontroller Reality",
        "Stochastic Gradient Descent (SGD)",
        "TensorFlow Lite",
        "TinyML Market Reality",
        "TinyTL Memory Breakthrough",
        "Wireless Communication Reality"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "chapter_name": "ops",
      "total_references": 78,
      "total_definitions": 39,
      "footnote_ids": [
        "fn-ab-testing-ml",
        "fn-alerting-thresholds",
        "fn-canary-deployment-history",
        "fn-cloud-ml-costs",
        "fn-covid-impact",
        "fn-data-drift-discovery",
        "fn-devops-origins",
        "fn-docker-revolution",
        "fn-drift-detection",
        "fn-dvc-story",
        "fn-elk-stack",
        "fn-feature-store-scale",
        "fn-github-actions-ml",
        "fn-infrastructure-as-code",
        "fn-jenkins-history",
        "fn-kserve-scaling",
        "fn-kubeflow-scale",
        "fn-kubernetes-birth",
        "fn-microservices-ml",
        "fn-ml-autoscaling",
        "fn-mlflow-creation",
        "fn-mlops-business-impact",
        "fn-mlops-emergence",
        "fn-pipeline-jungle-metaphor",
        "fn-plc-definition",
        "fn-prometheus-scale",
        "fn-reproducibility-crisis",
        "fn-serverless-ml",
        "fn-shap-adoption",
        "fn-sla-examples",
        "fn-slo-reality",
        "fn-tech-debt-origin",
        "fn-telemetry-ml",
        "fn-tensorflow-serving",
        "fn-tensorflow-serving-origins",
        "fn-training-serving-skew",
        "fn-triton-performance",
        "fn-youtube-engagement",
        "fn-zillow-losses"
      ],
      "terms_defined": [
        "A/B Testing for ML",
        "COVID-19 ML Impact",
        "Canary Deployment History",
        "Cloud ML Training Economics",
        "DVC Creation Story",
        "Data Drift Discovery",
        "DevOps Origins",
        "Docker's Revolution",
        "ELK Stack",
        "Feature Store Scale",
        "GitHub Actions for ML",
        "Infrastructure as Code",
        "Jenkins Origins",
        "KServe (formerly KFServing)",
        "Kubeflow Production Usage",
        "Kubernetes Origins",
        "ML Autoscaling at Scale",
        "ML Reproducibility Crisis",
        "ML Telemetry",
        "MLOps Business Impact",
        "MLOps Emergence",
        "MLflow's Creation",
        "Microservices in ML",
        "Model Drift Detection",
        "NVIDIA Triton Inference Server",
        "Pipeline Jungle Metaphor",
        "Production Alert Thresholds",
        "Programmable Logic Controllers (PLCs)",
        "Prometheus at Scale",
        "SHAP in Production",
        "Serverless Computing for ML",
        "Service Level Agreements (SLAs)",
        "Service Level Objectives (SLOs)",
        "Technical Debt Origins",
        "TensorFlow Serving",
        "TensorFlow Serving Origins",
        "Training-Serving Skew Impact",
        "YouTube Recommendation Impact",
        "Zillow iBuying Failure"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "chapter_name": "optimizations",
      "total_references": 84,
      "total_definitions": 42,
      "footnote_ids": [
        "fn-batch-size-effects",
        "fn-bayesian-optimization",
        "fn-bert-compression",
        "fn-distilbert-metrics",
        "fn-distributed-training-costs",
        "fn-edge-ml-definition",
        "fn-efficientnet-pruning",
        "fn-energy-efficiency-metrics",
        "fn-evolutionary-nas",
        "fn-fbnet-nas",
        "fn-flops",
        "fn-fpga-ml",
        "fn-gpu-operational-costs",
        "fn-gradient-checkpointing",
        "fn-hardware-aware-nas",
        "fn-hyperparameter-optimization",
        "fn-inference-latency",
        "fn-knowledge-distill",
        "fn-lora",
        "fn-lottery-ticket",
        "fn-matrix-factorization",
        "fn-memory-bandwidth",
        "fn-memory-hierarchy",
        "fn-memory-optimization",
        "fn-microcontroller-constraints",
        "fn-mixed-precision",
        "fn-ml-runtimes",
        "fn-nas-evaluation-metrics",
        "fn-onnx-deployment",
        "fn-operator-fusion",
        "fn-opt-pruning",
        "fn-parallel-processing",
        "fn-qat-performance",
        "fn-rl-nas",
        "fn-sparse-energy-savings",
        "fn-sparsity-def",
        "fn-structured-pruning",
        "fn-tensorrt-optimization",
        "fn-tf-model-optimization",
        "fn-tiny-ml-definition",
        "fn-tvm-compiler",
        "fn-xla-compiler"
      ],
      "terms_defined": [
        "BERT Compression",
        "Batch Size Effects",
        "Bayesian Optimization",
        "DistilBERT",
        "Distributed Training Costs",
        "Edge ML",
        "EfficientNet Pruning",
        "Energy Efficiency Metrics",
        "Evolutionary NAS",
        "FBNet",
        "FLOPs (Floating-Point Operations)",
        "FPGA for ML",
        "GPU Operational Costs",
        "Gradient Checkpointing",
        "Hardware-Aware NAS",
        "Hyperparameter Optimization",
        "Inference Latency",
        "Knowledge Distillation",
        "LoRA (Low-Rank Adaptation)",
        "Lottery Ticket Hypothesis",
        "ML Runtimes",
        "Matrix Factorization",
        "Memory Bandwidth",
        "Memory Hierarchy",
        "Memory Optimization",
        "Microcontroller Constraints",
        "Mixed-Precision Training",
        "NAS Evaluation Metrics",
        "ONNX Deployment",
        "Operator Fusion",
        "Parallel Processing in ML",
        "Pruning",
        "Quantization-Aware Training",
        "Reinforcement Learning NAS",
        "Sparse Energy Savings",
        "Sparsity",
        "Structured Pruning",
        "TVM (Tensor Virtual Machine)",
        "TensorFlow Model Optimization",
        "TensorRT Optimization",
        "Tiny ML",
        "XLA (Accelerated Linear Algebra)"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "chapter_name": "privacy_security",
      "total_references": 113,
      "total_definitions": 56,
      "footnote_ids": [
        "fn-adversarial-examples",
        "fn-aes-standard",
        "fn-air-gapped",
        "fn-api-keys",
        "fn-arm-processors",
        "fn-attention-maps",
        "fn-automotive-recalls",
        "fn-backdoor-attacks",
        "fn-ci-cd-security",
        "fn-crowdsourcing-risks",
        "fn-cybersecurity-regulations",
        "fn-data-poisoning",
        "fn-ddos-attacks",
        "fn-debug-ports",
        "fn-dp-origins",
        "fn-dp-sgd-adoption",
        "fn-fips-140",
        "fn-gans-adversarial",
        "fn-gdpr",
        "fn-he-breakthrough",
        "fn-healthcare-ml-compliance",
        "fn-hipaa-violations",
        "fn-hsm-certification",
        "fn-hsm-performance",
        "fn-intel-sgx-limits",
        "fn-iot-vulnerabilities",
        "fn-medical-device-security",
        "fn-meltdown-spectre-impact",
        "fn-mirai-scale",
        "fn-ml-apis",
        "fn-ml-side-channel",
        "fn-model-distillation",
        "fn-model-extraction-2016",
        "fn-model-inversion-attack",
        "fn-model-repositories",
        "fn-model-serialization",
        "fn-model-watermarking",
        "fn-mutual-tls",
        "fn-netflix-deanonymization",
        "fn-network-protocols",
        "fn-nhtsa",
        "fn-oauth",
        "fn-perspective-api",
        "fn-perspective-vulnerability",
        "fn-phishing-ai",
        "fn-privacy-utility-tension",
        "fn-puf-adoption",
        "fn-rbac",
        "fn-smpc-overhead",
        "fn-speculative-execution",
        "fn-stuxnet-discovery",
        "fn-synthetic-data",
        "fn-transfer-learning-attacks",
        "fn-trustzone-adoption",
        "fn-usb-attacks",
        "fn-zero-day-term"
      ],
      "terms_defined": [
        "AI-Generated Phishing",
        "API Keys",
        "ARM Processors",
        "ARM TrustZone",
        "Advanced Encryption Standard (AES)",
        "Adversarial Examples",
        "Air-Gapped Systems",
        "Attention Maps",
        "Automotive Cybersecurity Recalls",
        "Backdoor Attacks",
        "CI/CD Security",
        "Crowdsourcing Risks",
        "Cybersecurity Regulations",
        "DDoS Attacks",
        "DP-SGD Industry Adoption",
        "Data Poisoning",
        "Debug Port Vulnerabilities",
        "Differential Privacy Origins",
        "FIPS 140-2 Standard",
        "GANs for Adversarial Attacks",
        "General Data Protection Regulation (GDPR)",
        "HIPAA Violations",
        "HSM Certification",
        "HSM Performance",
        "Healthcare ML Compliance",
        "Homomorphic Encryption Breakthrough",
        "Intel SGX Constraints",
        "IoT Device Vulnerabilities",
        "ML APIs",
        "Medical Device Security",
        "Meltdown/Spectre Impact",
        "Mirai Botnet Scale",
        "Model Distillation",
        "Model Extraction Threat",
        "Model Inversion Attack",
        "Model Repositories",
        "Model Serialization",
        "Model Watermarking",
        "Mutual TLS (mTLS)",
        "NHTSA Cybersecurity Guidelines",
        "Netflix Deanonymization",
        "Network Protocols",
        "OAuth Protocol",
        "PUF Market Growth",
        "Perspective API",
        "Privacy-Utility Tension",
        "Role-Based Access Control (RBAC)",
        "SMPC Performance",
        "Side-Channel Attacks on ML",
        "Speculative Execution",
        "Stuxnet Discovery",
        "Synthetic Data Growth",
        "Transfer Learning Attacks",
        "USB Attack Vectors",
        "Zero-Day Etymology"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "chapter_name": "responsible_ai",
      "total_references": 68,
      "total_definitions": 34,
      "footnote_ids": [
        "fn-adversarial-inputs",
        "fn-ai-safety",
        "fn-compas-bias",
        "fn-ctr-optimization",
        "fn-datacenter-environmental-justice",
        "fn-datasheets",
        "fn-demographic-parity-origin",
        "fn-differential-privacy",
        "fn-differential-privacy-table",
        "fn-digital-infrastructure-divide",
        "fn-dp-sgd",
        "fn-energy-privacy-tradeoff",
        "fn-equalized-odds",
        "fn-fairlearn",
        "fn-fairness-impossibility",
        "fn-feature-engineering",
        "fn-federated-learning",
        "fn-gdpr-article-22",
        "fn-global-explanations",
        "fn-healthcare-algorithm-bias",
        "fn-high-stakes-domains",
        "fn-human-in-the-loop",
        "fn-local-explanations",
        "fn-membership-inference",
        "fn-model-cards",
        "fn-model-memorization",
        "fn-post-hoc-explanations",
        "fn-protected-attributes",
        "fn-proxy-metrics",
        "fn-reward-hacking",
        "fn-saliency-maps",
        "fn-structural-inequities",
        "fn-systemic-bias",
        "fn-value-alignment"
      ],
      "terms_defined": [
        "AI Safety",
        "Adversarial Inputs",
        "COMPAS Algorithm Controversy",
        "Click-Through Rate (CTR) Optimization",
        "DP-SGD (Differentially Private Stochastic Gradient Descent)",
        "Datacenter Environmental Justice",
        "Datasheets for Datasets",
        "Demographic Parity Origins",
        "Differential Privacy",
        "Differential Privacy",
        "Digital Infrastructure Divide",
        "Energy-Privacy Tradeoffs",
        "Equalized Odds",
        "Fairlearn",
        "Fairness Impossibility Theorems",
        "Feature Engineering",
        "Federated Learning",
        "GDPR Article 22",
        "Global Explanations",
        "Healthcare Algorithm Scale",
        "High-Stakes Domains",
        "Human-in-the-Loop (HITL)",
        "Local Explanations",
        "Membership Inference Attacks",
        "Model Cards",
        "Model Memorization",
        "Post-Hoc Explanations",
        "Protected Attributes",
        "Proxy Metrics",
        "Reward Hacking",
        "Saliency Maps",
        "Structural Inequities",
        "Systemic Bias",
        "Value Alignment"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "chapter_name": "robust_ai",
      "total_references": 114,
      "total_definitions": 57,
      "footnote_ids": [
        "fn-ai-hypercomputers",
        "fn-alexa-voice-service",
        "fn-autoencoders",
        "fn-autopilot",
        "fn-backdoor-attacks",
        "fn-bayesian-neural-networks",
        "fn-bayesian-nn",
        "fn-beam_testing",
        "fn-carlini-wagner",
        "fn-checkpoint-restart",
        "fn-combinationallogic",
        "fn-crosstalk",
        "fn-data-poisoning",
        "fn-data-sanitization",
        "fn-dmr",
        "fn-dropout",
        "fn-dualusedilemma",
        "fn-edge-computing",
        "fn-electromagnetic-interference",
        "fn-electromigration",
        "fn-embedded-systems",
        "fn-ensemble-methods",
        "fn-error-correcting-codes",
        "fn-f1",
        "fn-failsafe-mechanisms",
        "fn-fault-models",
        "fn-fgsm",
        "fn-fpga",
        "fn-glitches",
        "fn-gradient-based-attacks",
        "fn-gradients",
        "fn-hamming1950error",
        "fn-hardening-strategies",
        "fn-hot-spares",
        "fn-huber-loss",
        "fn-lookup-table",
        "fn-minimax",
        "fn-model-uncertainty",
        "fn-multimodal-sensor-data",
        "fn-nn-learning",
        "fn-nn-theory",
        "fn-oxide-breakdown",
        "fn-parity",
        "fn-principle-least-privilege",
        "fn-regularization",
        "fn-safety-critical",
        "fn-scan-chains",
        "fn-silent-data-corruption",
        "fn-single-event-upsets",
        "fn-smart-cities",
        "fn-stochastic-computing",
        "fn-systemic-vulnerabilities",
        "fn-thermal-stress",
        "fn-tmr",
        "fn-transferability",
        "fn-transient-vs-permanent",
        "fn-white-box-attacks"
      ],
      "terms_defined": [
        "AI Hypercomputers",
        "Alexa Voice Service (AVS)",
        "Autoencoders",
        "Autopilot",
        "Backdoor Attacks",
        "Bayesian Neural Networks",
        "Bayesian Neural Networks",
        "Beam Testing",
        "Carlini and Wagner (C&W) Attack",
        "Checkpoint and Restart Mechanisms",
        "Combinational logic",
        "Crosstalk",
        "Data Poisoning",
        "Data Sanitization",
        "Double Modular Redundancy (DMR)",
        "Dropout Mechanism",
        "Dual-use Dilemma",
        "Edge Computing",
        "Electromagnetic Interference (EMI)",
        "Embedded Systems",
        "Ensemble Methods",
        "Error-Correcting Codes",
        "F1 Score",
        "Failsafe Mechanisms",
        "Fast Gradient Sign Method (FGSM)",
        "Fault Models",
        "Field-Programmable Gate Arrays (FPGAs)",
        "Glitches",
        "Gradient-Based Attacks",
        "Gradients and Convergence",
        "Hardening Strategies",
        "Hot Spares",
        "Huber Loss",
        "Lookup Table",
        "Minimax",
        "Model Uncertainty",
        "Multimodal Sensor Data",
        "Neural Network Learning Mechanisms",
        "Neural Network Theoretical Foundations",
        "Principle of Least Privilege",
        "Regularization",
        "Safety-Critical Applications",
        "Scan Chains",
        "Silent Data Corruption (SDC)",
        "Single Event Upsets (SEUs)",
        "Smart Cities",
        "Stochastic Computing",
        "Systemic Vulnerabilities",
        "Thermal Stress",
        "Transferability",
        "Transient vs Permanent Faults",
        "Triple Modular Redundancy (TMR)",
        "White-Box Attacks"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "chapter_name": "sustainable_ai",
      "total_references": 57,
      "total_definitions": 28,
      "footnote_ids": [
        "fn-ai-compute-growth",
        "fn-chemical-scale",
        "fn-china-ree-control",
        "fn-cooling-energy",
        "fn-datacenter-emissions",
        "fn-dennard-scaling",
        "fn-edge-computing",
        "fn-embodied-carbon",
        "fn-energy-frameworks",
        "fn-euv-lithography",
        "fn-ewaste-scale",
        "fn-flops-vs-flops",
        "fn-google-carbon-free",
        "fn-google-carbon-scheduling",
        "fn-gpu-manufacturing",
        "fn-grid-carbon-data",
        "fn-household-energy",
        "fn-hyperscale-size",
        "fn-indium-supply",
        "fn-industry-comparison",
        "fn-jevons-paradox",
        "fn-nuclear-ai",
        "fn-pue-efficiency",
        "fn-pue-metric",
        "fn-sustainable-gpt3",
        "fn-sustainable-moores-law",
        "fn-transformer-nas",
        "fn-tsmc-water"
      ],
      "terms_defined": [
        "AI Compute Explosion",
        "AI vs Industrial Emissions",
        "Chinese Rare Earth Dominance",
        "Critical Material Scarcity",
        "Data Center Climate Impact",
        "Data Center Cooling Costs",
        "Dennard Scaling",
        "E-Waste from Computing",
        "EUV Lithography",
        "Edge Computing for AI",
        "Embodied Carbon",
        "Energy-Aware AI Frameworks",
        "FLOPS vs FLOPs",
        "GPT-3 Energy Consumption",
        "GPU Manufacturing Impact",
        "Google Carbon-Aware Scheduling Results",
        "Google's Carbon-Free Commitment",
        "Hazardous Chemical Quantities",
        "Household Energy Comparison",
        "Hyperscale Data Center Scale",
        "Jevon's Paradox",
        "Moore's Law Origins",
        "Nuclear Power for AI Data Centers",
        "Power Usage Effectiveness",
        "Power Usage Effectiveness (PUE)",
        "Real-Time Grid Carbon Intensity",
        "Semiconductor Water Consumption Scale",
        "Transformer + NAS Environmental Impact"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "chapter_name": "training",
      "total_references": 94,
      "total_definitions": 47,
      "footnote_ids": [
        "batch-norm",
        "fn-allreduce-algorithm",
        "fn-attention-mechanisms",
        "fn-autodiff",
        "fn-backpropagation",
        "fn-batching-transformation",
        "fn-cdc6600",
        "fn-cm5",
        "fn-cnns",
        "fn-convolution",
        "fn-cuda-programming",
        "fn-distributed-training",
        "fn-eniac",
        "fn-etl-elt-ml",
        "fn-fp16-range",
        "fn-fpga-datacenter",
        "fn-google-datacenter",
        "fn-google-tpus",
        "fn-gradient-accumulation",
        "fn-lr-schedules",
        "fn-memory-hierarchy-ml",
        "fn-nccl",
        "fn-nvidia-gpus",
        "fn-profiling-tools",
        "fn-relu-hardware",
        "fn-rnns",
        "fn-rnns-lstms",
        "fn-sgd-history",
        "fn-sigmoid-cost",
        "fn-strassen-algorithm",
        "fn-system360",
        "fn-systolic-array",
        "fn-training-activation-checkpointing",
        "fn-training-alexnet",
        "fn-training-data-parallelism",
        "fn-training-gpt3",
        "fn-training-mixed-precision",
        "fn-training-model-parallelism",
        "fn-training-tensor-cores",
        "fn-transformer-attention",
        "fn-transformer-scaling",
        "fn-transformer-training",
        "fn-transformers",
        "fn-wse-specs",
        "gradient-clipping",
        "nccl",
        "nvlink"
      ],
      "terms_defined": [
        "Activation Checkpointing Trade-offs",
        "AlexNet",
        "AllReduce Algorithm",
        "Attention Mechanisms",
        "Automatic Differentiation",
        "Backpropagation Algorithm",
        "Batch Normalization",
        "Batching in Neural Networks",
        "CDC 6600",
        "CUDA Programming Model",
        "Connection Machine CM-5",
        "Convolutional Neural Networks",
        "Convolutional Operations",
        "Data Parallelism Scaling",
        "Distributed Training",
        "ENIAC (Electronic Numerical Integrator and Computer)",
        "ETL vs ELT in ML",
        "FP16 Dynamic Range",
        "GPT-3 Training Scale",
        "Google Data Centers",
        "Google TPUs",
        "Gradient Accumulation Impact",
        "Gradient Clipping",
        "IBM System/360",
        "Learning Rate Schedules",
        "Memory Hierarchy in ML",
        "Microsoft FPGA Deployment",
        "Mixed-Precision Training",
        "Model Parallelism Memory Scaling",
        "NCCL (NVIDIA Collective Communications Library)",
        "NVIDIA AI GPUs",
        "NVIDIA NCCL (Collective Communications Library)",
        "NVLink",
        "RNNs and LSTMs",
        "ReLU Hardware Efficiency",
        "Recurrent Neural Networks",
        "Sigmoid Computational Cost",
        "Stochastic Gradient Descent",
        "Strassen's Algorithm",
        "Systolic Array Architecture",
        "Tensor Cores",
        "Training Profiling Tools",
        "Transformer Architectures",
        "Transformer Attention",
        "Transformer Batch Size Scaling",
        "Transformer Training",
        "Wafer-Scale Engine Specifications"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "chapter_name": "workflow",
      "total_references": 40,
      "total_definitions": 20,
      "footnote_ids": [
        "fn-algorithmic-fairness",
        "fn-cicd-ml",
        "fn-data-challenges",
        "fn-data-drift",
        "fn-data-evolution",
        "fn-deployment-reality-gap",
        "fn-dr-statistics",
        "fn-federated-learning",
        "fn-fraud-detection",
        "fn-healthcare-ai-challenges",
        "fn-hyperparameter-tuning",
        "fn-medical-annotation",
        "fn-medical-metrics",
        "fn-ml-team-evolution",
        "fn-mlops-maturity",
        "fn-model-drift",
        "fn-problem-definition",
        "fn-scaling-challenges",
        "fn-workflow-data-versioning",
        "fn-workflow-transfer-learning"
      ],
      "terms_defined": [
        "Algorithmic Fairness in Healthcare",
        "CI/CD for Machine Learning",
        "Data Drift Detection",
        "Data Evolution in Production",
        "Data Versioning Challenges",
        "Diabetic Retinopathy Global Impact",
        "Federated Learning Architecture",
        "Healthcare AI Deployment Reality",
        "Hyperparameter Optimization Complexity",
        "ML System Scaling Complexity",
        "ML Team Role Evolution",
        "ML vs. Traditional Problem Definition",
        "ML-Based Fraud Detection Evolution",
        "MLOps Maturity Models",
        "Medical AI Performance Metrics",
        "Medical Data Annotation Costs",
        "Model Drift Phenomenon",
        "The 80/20 Rule in ML",
        "The Lab-to-Clinic Performance Gap",
        "Transfer Learning"
      ]
    }
  ],
  "all_references": [
    {
      "footnote_id": "fn-ebola-outbreak",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 67,
      "context": "...ons and coordinated responses could have dramatically altered outcomes. The 2014-2016 Ebola outbreak[^fn-ebola-outbreak] in West Africa, for instance, highlighted the catastrophic consequences of delayed detection and re...",
      "full_line": "History provides sobering examples of where timely interventions and coordinated responses could have dramatically altered outcomes. The 2014-2016 Ebola outbreak[^fn-ebola-outbreak] in West Africa, for instance, highlighted the catastrophic consequences of delayed detection and response systems [@who2016ebola]. Similarly, the 2011 famine in Somalia, despite being forecasted months in advance, caused immense suffering due to inadequate mechanisms to mobilize and allocate resources effectively [@reliefweb2012somalia]. In the aftermath of the 2010 Haiti earthquake, the lack of rapid and reliable damage assessment significantly hampered efforts to direct aid where it was most needed [@usgs2010haiti]."
    },
    {
      "footnote_id": "fn-ebola-outbreak",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 69,
      "context": "[^fn-ebola-outbreak]: **2014-2016 Ebola Outbreak**: This outbreak killed 11,323 people across six countries, with over 2...",
      "full_line": "[^fn-ebola-outbreak]: **2014-2016 Ebola Outbreak**: This outbreak killed 11,323 people across six countries, with over 28,600 cases reported. The delayed international response\u2014WHO declared a Public Health Emergency only after 5 months\u2014demonstrated how early AI-powered disease surveillance could have saved thousands of lives. The economic cost exceeded $53 billion, highlighting the need for rapid detection systems that mobile health technologies now provide."
    },
    {
      "footnote_id": "fn-smallholder-farmers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 71,
      "context": "...riculture, a sector important to global food security, faces parallel struggles. Smallholder farmers[^fn-smallholder-farmers], responsible for producing much of the world's food, make important decisions with limited informat...",
      "full_line": "Today, similar challenges persist across diverse domains, particularly in resource-constrained environments. In healthcare, remote and underserved communities often experience preventable health crises due to the absence of timely access to medical expertise. A lack of diagnostic tools and specialists means that treatable conditions can escalate into life-threatening situations, creating unnecessary suffering and loss of life. Agriculture, a sector important to global food security, faces parallel struggles. Smallholder farmers[^fn-smallholder-farmers], responsible for producing much of the world's food, make important decisions with limited information."
    },
    {
      "footnote_id": "fn-smallholder-farmers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 73,
      "context": "[^fn-smallholder-farmers]: **Smallholder Farmers Global Impact**: These farmers operate plots smaller than 2 hectares but pro...",
      "full_line": "[^fn-smallholder-farmers]: **Smallholder Farmers Global Impact**: These farmers operate plots smaller than 2 hectares but produce 35% of global food supply, feeding 2 billion people directly. In sub-Saharan Africa, they comprise 80% of farms yet receive only 2% of agricultural credit. Climate change threatens their $2.6 trillion annual production value, making AI-powered agricultural support systems important for global food security and poverty reduction. Increasingly erratic weather patterns, pest outbreaks, and soil degradation compound their difficulties, often resulting in reduced yields and heightened food insecurity, particularly in vulnerable regions. These challenges demonstrate how systemic barriers and resource constraints perpetuate inequities and undermine resilience."
    },
    {
      "footnote_id": "fn-cassava-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 93,
      "context": "...armers in disconnected regions where access to agricultural advisors is limited [@ramcharan2017deep][^fn-cassava-impact].",
      "full_line": "In Sub-Saharan Africa, cassava farmers have long battled diseases that devastate crops and livelihoods. Now, with the help of mobile ML-powered smartphone apps, as shown in @fig-plantvillage, they can snap a photo of a leaf and receive instant feedback on potential diseases. Studies suggest this early detection system has the potential to reduce losses from 40% to 15-20% with early identification of diseases, offering hope to farmers in disconnected regions where access to agricultural advisors is limited [@ramcharan2017deep][^fn-cassava-impact]."
    },
    {
      "footnote_id": "fn-cassava-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 95,
      "context": "[^fn-cassava-impact]: **Cassava Disease Impact**: Cassava feeds 800 million people globally and is a important food secu...",
      "full_line": "[^fn-cassava-impact]: **Cassava Disease Impact**: Cassava feeds 800 million people globally and is a important food security crop in Africa. Cassava mosaic disease (CMD) and cassava brown streak disease (CBSD) can destroy entire harvests, affecting millions of smallholder farmers. The PlantVillage Nuru app has been downloaded by over 500,000 farmers across Kenya, Tanzania, and Uganda, demonstrating how mobile ML can scale agricultural expertise to underserved communities without internet connectivity."
    },
    {
      "footnote_id": "fn-microclimate-monitoring",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 97,
      "context": "...s. In Indonesia, Tiny ML sensors are transforming their ability to adapt by monitoring microclimates[^fn-microclimate-monitoring] across paddies. These low-power devices process data locally to optimize water usage, enabling prec...",
      "full_line": "Across Southeast Asia, rice farmers are confronting increasingly unpredictable weather patterns. In Indonesia, Tiny ML sensors are transforming their ability to adapt by monitoring microclimates[^fn-microclimate-monitoring] across paddies. These low-power devices process data locally to optimize water usage, enabling precision irrigation even in areas with minimal infrastructure [@tirtalistyani2022indonesia]."
    },
    {
      "footnote_id": "fn-microclimate-monitoring",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 99,
      "context": "[^fn-microclimate-monitoring]: **Microclimate Monitoring**: Unlike weather stations measuring regional conditions across 50-100 k...",
      "full_line": "[^fn-microclimate-monitoring]: **Microclimate Monitoring**: Unlike weather stations measuring regional conditions across 50-100 km areas, microclimate sensors detect variations within 10-meter zones crucial for rice cultivation. These sensors track temperature differences of 2-3\u00b0C, humidity variations of 10-15%, and soil moisture changes that can affect yields by 30%. TinyML enables real-time processing on sensors costing $5-10, versus traditional agricultural weather stations requiring $15,000+ investments."
    },
    {
      "footnote_id": "fn-farmbeats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 101,
      "context": "...Microsoft's [FarmBeats](https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/)[^fn-farmbeats] is pioneering the integration of IoT sensors, drones, and Cloud ML to create actionable insights fo...",
      "full_line": "On a global scale, Microsoft's [FarmBeats](https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/)[^fn-farmbeats] is pioneering the integration of IoT sensors, drones, and Cloud ML to create actionable insights for farmers. By leveraging weather forecasts, soil conditions, and crop health data, the platform allows farmers to optimize inputs like water and fertilizer, reducing waste and improving yields. Together, these innovations illustrate how AI technologies are bringing precision agriculture to life, addressing food security, sustainability, and climate resilience."
    },
    {
      "footnote_id": "fn-farmbeats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 103,
      "context": "[^fn-farmbeats]: **Microsoft FarmBeats**: Launched in 2017, FarmBeats was deployed across several thousand farms be...",
      "full_line": "[^fn-farmbeats]: **Microsoft FarmBeats**: Launched in 2017, FarmBeats was deployed across several thousand farms before being integrated into Azure FarmBeats in 2021. During its deployment, the platform helped farmers reduce water usage by 30% and increase crop yields by 15-20%. The platform processed data from 50+ sensor types and could predict crop health issues 2-3 weeks before visible symptoms appeared, demonstrating how Cloud ML scales agricultural expertise to underserved farming communities."
    },
    {
      "footnote_id": "fn-cough-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 107,
      "context": ".../www.samayhealth.com/) uses embedded machine learning to analyze cough patterns and detect pneumonia[^fn-cough-detection]. Designed for remote areas, the device operates independently of internet connectivity and is power...",
      "full_line": "For millions in underserved communities, access to healthcare often means long waits and travel to distant clinics. Tiny ML is changing that by enabling diagnostics to occur at the patient's side. For example, a low-cost wearable developed by [Respira x Colabs](https://www.samayhealth.com/) uses embedded machine learning to analyze cough patterns and detect pneumonia[^fn-cough-detection]. Designed for remote areas, the device operates independently of internet connectivity and is powered by a simple microcontroller, making life-saving diagnostics accessible to those who need it most."
    },
    {
      "footnote_id": "fn-cough-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 109,
      "context": "[^fn-cough-detection]: **Cough Analysis Technology**: Pneumonia kills over 800,000 children under 5 annually, with most d...",
      "full_line": "[^fn-cough-detection]: **Cough Analysis Technology**: Pneumonia kills over 800,000 children under 5 annually, with most deaths occurring in resource-poor settings lacking access to chest X-rays. Cough analysis using TinyML can achieve 90%+ accuracy in pneumonia detection by analyzing acoustic features like cough duration, frequency, and spectral characteristics. The entire model runs on a microcontroller costing less than $10, democratizing diagnostic capabilities."
    },
    {
      "footnote_id": "fn-mosquito-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 111,
      "context": "...ow-cost devices that use machine learning to identify mosquito species by their wingbeat frequencies[^fn-mosquito-detection] [@altayeb2022classifying]. This technology allows real-time monitoring of malaria-carrying mosquito...",
      "full_line": "Tiny ML's potential extends to tackling global health issues like vector-borne diseases that are spread by mosquitoes. Researchers have developed low-cost devices that use machine learning to identify mosquito species by their wingbeat frequencies[^fn-mosquito-detection] [@altayeb2022classifying]. This technology allows real-time monitoring of malaria-carrying mosquitoes. It offers a scalable solution for malaria control in high-risk regions."
    },
    {
      "footnote_id": "fn-mosquito-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 113,
      "context": "[^fn-mosquito-detection]: **Mosquito Species Detection**: Malaria affects 241 million people annually, causing 627,000 death...",
      "full_line": "[^fn-mosquito-detection]: **Mosquito Species Detection**: Malaria affects 241 million people annually, causing 627,000 deaths primarily in sub-Saharan Africa. TinyML-powered mosquito detection devices achieve 95% accuracy in species identification using just acoustic signatures, costing under $50 versus traditional morphological identification requiring $5,000+ microscopy equipment. These devices can monitor 24/7 and detect Anopheles mosquitoes (malaria vectors) versus Culex (nuisance only), enabling targeted intervention strategies."
    },
    {
      "footnote_id": "fn-genomics-cloud",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 115,
      "context": "...n a broader scale. Platforms like [Google Genomics](https://health.google/health-research/genomics/)[^fn-genomics-cloud] analyze vast datasets to identify disease markers, accelerating breakthroughs in personalized medic...",
      "full_line": "In parallel, Cloud ML is advancing healthcare research and diagnostics on a broader scale. Platforms like [Google Genomics](https://health.google/health-research/genomics/)[^fn-genomics-cloud] analyze vast datasets to identify disease markers, accelerating breakthroughs in personalized medicine. These examples show how AI technologies, ranging from the portability of Tiny ML to the computational power of Cloud ML, are converging to democratize healthcare access and improve outcomes worldwide."
    },
    {
      "footnote_id": "fn-genomics-cloud",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 117,
      "context": "[^fn-genomics-cloud]: **Cloud Genomics Scale**: Google Cloud processes over 50 petabytes of genomic data annually, equiv...",
      "full_line": "[^fn-genomics-cloud]: **Cloud Genomics Scale**: Google Cloud processes over 50 petabytes of genomic data annually, equivalent to analyzing 15 million human genomes. A single genome contains 3 billion base pairs requiring 100GB storage, making cloud computing essential for population-scale analysis. Cloud ML can identify disease variants in hours versus months using traditional methods, accelerating drug discovery that typically takes 10-15 years and costs $1+ billion per new medicine."
    },
    {
      "footnote_id": "fn-thermal-imaging-rescue",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 125,
      "context": "...into collapsed buildings, navigating obstacles to detect signs of life. By analyzing thermal imaging[^fn-thermal-imaging-rescue] and acoustic signals locally, these drones can identify survivors and hazards without relying on cl...",
      "full_line": "In disaster zones, where every second counts, AI technologies are providing tools to accelerate response efforts and enhance safety. Tiny, autonomous drones equipped with Tiny ML algorithms are making their way into collapsed buildings, navigating obstacles to detect signs of life. By analyzing thermal imaging[^fn-thermal-imaging-rescue] and acoustic signals locally, these drones can identify survivors and hazards without relying on cloud connectivity [@duisterhof2021sniffy]. These drones can autonomously seek light sources (which often indicate survivors) and detect dangerous gas leaks, making search and rescue operations both faster and safer for human responders."
    },
    {
      "footnote_id": "fn-thermal-imaging-rescue",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 127,
      "context": "[^fn-thermal-imaging-rescue]: **Thermal Imaging in Disaster Response**: Human body temperature (37\u00b0C) contrasts sharply with deb...",
      "full_line": "[^fn-thermal-imaging-rescue]: **Thermal Imaging in Disaster Response**: Human body temperature (37\u00b0C) contrasts sharply with debris temperature (often 15-25\u00b0C), enabling detection through 30cm of rubble. TinyML thermal analysis on drones can process 320x240 pixel thermal images at 9Hz using only 500mW power, operating for 20+ minutes on small batteries. This autonomous capability proved critical during the 2023 Turkey earthquake, where 72-hour survival windows made rapid victim location essential for the 50,000+ people trapped."
    },
    {
      "footnote_id": "fn-satellite-disaster",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 131,
      "context": "...a broader level, platforms like Google's [AI for Disaster Response](https://crisisresponse.google/)[^fn-satellite-disaster] are leveraging Cloud ML to process satellite imagery and predict flood zones. These systems provide...",
      "full_line": "At a broader level, platforms like Google's [AI for Disaster Response](https://crisisresponse.google/)[^fn-satellite-disaster] are leveraging Cloud ML to process satellite imagery and predict flood zones. These systems provide real-time insights to help governments allocate resources more effectively and save lives during emergencies."
    },
    {
      "footnote_id": "fn-satellite-disaster",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 133,
      "context": "[^fn-satellite-disaster]: **Satellite Disaster Monitoring**: Modern disaster monitoring processes 10+ terabytes of satellite...",
      "full_line": "[^fn-satellite-disaster]: **Satellite Disaster Monitoring**: Modern disaster monitoring processes 10+ terabytes of satellite imagery daily from sources like Landsat-8, Sentinel-2, and commercial providers. AI can detect flooding across 100,000+ km\u00b2 areas in 2-3 hours versus 2-3 days for human analysis. During 2022 Pakistan floods affecting 33 million people, satellite AI identified affected areas 48 hours before ground confirmation, enabling preemptive evacuations and resource positioning that saved thousands of lives."
    },
    {
      "footnote_id": "fn-gunshot-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 141,
      "context": "...e]. Meanwhile, Tiny ML systems are enabling anti-poaching efforts by detecting threats like gunshots[^fn-gunshot-detection] or human activity and relaying alerts to rangers in real time [@bamoumen2022tinyml].",
      "full_line": "EdgeML-powered collars are being used to unobtrusively track animal behavior, such as elephant movements and vocalizations, helping researchers understand migration patterns and social behaviors. By processing data on the collar itself, these devices minimize power consumption and reduce the need for frequent battery changes [@chen2019edge]. Meanwhile, Tiny ML systems are enabling anti-poaching efforts by detecting threats like gunshots[^fn-gunshot-detection] or human activity and relaying alerts to rangers in real time [@bamoumen2022tinyml]."
    },
    {
      "footnote_id": "fn-gunshot-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 143,
      "context": "[^fn-gunshot-detection]: **Acoustic Gunshot Detection**: TinyML can distinguish gunshots from other loud sounds (thunder, v...",
      "full_line": "[^fn-gunshot-detection]: **Acoustic Gunshot Detection**: TinyML can distinguish gunshots from other loud sounds (thunder, vehicle backfire) with 95%+ accuracy by analyzing specific acoustic signatures: frequency range 500-4000Hz, duration 1-5ms, and sharp onset characteristics. Solar-powered sensors covering 5-10 km\u00b2 cost $200-300 versus traditional systems requiring $50,000+ installations. In Kenya's conservancies, these systems reduce elephant poaching response time from 3-4 hours to 10-15 minutes, significantly increasing ranger safety and wildlife protection effectiveness."
    },
    {
      "footnote_id": "fn-global-fishing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 145,
      "context": "...r illegal fishing activities. Platforms like [Global Fishing Watch](https://globalfishingwatch.org/)[^fn-global-fishing] analyze satellite data to detect anomalies, helping governments enforce regulations and protect mar...",
      "full_line": "At a global scale, Cloud ML is being used to monitor illegal fishing activities. Platforms like [Global Fishing Watch](https://globalfishingwatch.org/)[^fn-global-fishing] analyze satellite data to detect anomalies, helping governments enforce regulations and protect marine ecosystems."
    },
    {
      "footnote_id": "fn-global-fishing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 147,
      "context": "[^fn-global-fishing]: **Global Fishing Watch Impact**: Since 2016, this platform has tracked over 70,000 vessels globall...",
      "full_line": "[^fn-global-fishing]: **Global Fishing Watch Impact**: Since 2016, this platform has tracked over 70,000 vessels globally, processing 22+ million AIS (Automatic Identification System) data points daily. The system has helped identify $1.5 billion worth of illegal fishing activities and supported enforcement actions that recovered 180+ seized vessels. By making fishing activity transparent, the platform has contributed to 20% reductions in illegal fishing in monitored regions."
    },
    {
      "footnote_id": "fn-sdg-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 165,
      "context": "The SDGs shown in @fig-sdg are a global agenda adopted in 2015[^fn-sdg-adoption]. These 17 interconnected goals form a blueprint for addressing the world's most pressing challenges...",
      "full_line": "The SDGs shown in @fig-sdg are a global agenda adopted in 2015[^fn-sdg-adoption]. These 17 interconnected goals form a blueprint for addressing the world's most pressing challenges by 2030[^fn-sdg-ai-potential]. They range from eliminating poverty and hunger to ensuring quality education, from promoting gender equality to taking climate action[^fn-climate-action-ai]."
    },
    {
      "footnote_id": "fn-sdg-ai-potential",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 165,
      "context": "...17 interconnected goals form a blueprint for addressing the world's most pressing challenges by 2030[^fn-sdg-ai-potential]. They range from eliminating poverty and hunger to ensuring quality education, from promoting gende...",
      "full_line": "The SDGs shown in @fig-sdg are a global agenda adopted in 2015[^fn-sdg-adoption]. These 17 interconnected goals form a blueprint for addressing the world's most pressing challenges by 2030[^fn-sdg-ai-potential]. They range from eliminating poverty and hunger to ensuring quality education, from promoting gender equality to taking climate action[^fn-climate-action-ai]."
    },
    {
      "footnote_id": "fn-climate-action-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 165,
      "context": "...ty and hunger to ensuring quality education, from promoting gender equality to taking climate action[^fn-climate-action-ai].",
      "full_line": "The SDGs shown in @fig-sdg are a global agenda adopted in 2015[^fn-sdg-adoption]. These 17 interconnected goals form a blueprint for addressing the world's most pressing challenges by 2030[^fn-sdg-ai-potential]. They range from eliminating poverty and hunger to ensuring quality education, from promoting gender equality to taking climate action[^fn-climate-action-ai]."
    },
    {
      "footnote_id": "fn-sdg-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 167,
      "context": "[^fn-sdg-adoption]: **SDG Global Impact**: Adopted by all 193 UN Member States, the SDGs represent the most ambitious...",
      "full_line": "[^fn-sdg-adoption]: **SDG Global Impact**: Adopted by all 193 UN Member States, the SDGs represent the most ambitious global agenda in history, covering 169 specific targets with a $5-7 trillion annual funding gap. The goals build on the success of the Millennium Development Goals (2000-2015), which helped lift 1 billion people out of extreme poverty. Unlike their predecessors, the SDGs apply universally to all countries, recognizing that sustainable development requires global cooperation."
    },
    {
      "footnote_id": "fn-sdg-ai-potential",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 169,
      "context": "[^fn-sdg-ai-potential]: **AI's SDG Impact Potential**: McKinsey estimates AI could accelerate achievement of 134 of the 16...",
      "full_line": "[^fn-sdg-ai-potential]: **AI's SDG Impact Potential**: McKinsey estimates AI could accelerate achievement of 134 of the 169 SDG targets, potentially contributing $13 trillion to global economic output by 2030. However, 97% of AI research focuses on SDG 9 (Industry/Innovation) while only 1% addresses basic needs like water, food, and health. This maldistribution means AI systems for social good require deliberate design to address the most important human needs rather than commercial applications."
    },
    {
      "footnote_id": "fn-climate-action-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 171,
      "context": "[^fn-climate-action-ai]: **AI for Climate Action**: Climate change causes $23 billion in annual economic losses globally, w...",
      "full_line": "[^fn-climate-action-ai]: **AI for Climate Action**: Climate change causes $23 billion in annual economic losses globally, with temperatures rising 1.1\u00b0C above pre-industrial levels. AI systems for climate action include: carbon monitoring satellites tracking 50 billion tons of global emissions, smart grid optimization reducing energy waste by 15-20%, and climate modeling using exascale computing to predict regional impacts decades ahead. However, training large AI models can emit 626,000 pounds of CO\u2082\u2014equivalent to 5 cars' lifetime emissions\u2014highlighting the need for energy-efficient AI development."
    },
    {
      "footnote_id": "fn-resource-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 248,
      "context": "Deploying machine learning systems in social impact contexts reveals a fundamental resource paradox[^fn-resource-paradox] that shapes every aspect of system design. While areas with the greatest needs could benefit most f...",
      "full_line": "Deploying machine learning systems in social impact contexts reveals a fundamental resource paradox[^fn-resource-paradox] that shapes every aspect of system design. While areas with the greatest needs could benefit most from machine learning capabilities, they often lack the basic infrastructure required for traditional deployments."
    },
    {
      "footnote_id": "fn-resource-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 250,
      "context": "[^fn-resource-paradox]: **Social Good Resource Paradox**: Resource-constrained environments need the most help but have th...",
      "full_line": "[^fn-resource-paradox]: **Social Good Resource Paradox**: Resource-constrained environments need the most help but have the least infrastructure to deploy solutions. For example, rural sub-Saharan Africa has 60% of global arable land but only 4% of worldwide internet connectivity. This paradox forces engineers to achieve 90%+ model compression (from 50MB to 500KB) while maintaining effectiveness, a challenge absent in commercial deployments with abundant resources."
    },
    {
      "footnote_id": "fn-lora-technology",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 254,
      "context": "...ations. Rural deployments must instead rely on low-power wide-area network technologies such as LoRa[^fn-lora-technology] or NB-IoT with bandwidth constraints of 50 kbps\u2014approximately three orders of magnitude slower than...",
      "full_line": "Network infrastructure limitations further constrain system design. Urban environments offer high-bandwidth options like fiber (100+ Mbps) and 5G networks (1-10 Gbps) capable of supporting real-time multimedia applications. Rural deployments must instead rely on low-power wide-area network technologies such as LoRa[^fn-lora-technology] or NB-IoT with bandwidth constraints of 50 kbps\u2014approximately three orders of magnitude slower than typical broadband connections. These severe bandwidth limitations require careful optimization of data transmission protocols and payload sizes."
    },
    {
      "footnote_id": "fn-lora-technology",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 256,
      "context": "[^fn-lora-technology]: **LoRa Technology**: Long Range (LoRa) allows IoT devices to communicate over 15+ kilometers with...",
      "full_line": "[^fn-lora-technology]: **LoRa Technology**: Long Range (LoRa) allows IoT devices to communicate over 15+ kilometers with battery life exceeding 10 years. Operating in unlicensed spectrum bands, LoRa networks cost $1-5 per device annually versus $15-50 for cellular. This makes LoRa ideal for agricultural sensors monitoring soil moisture across vast farms or environmental sensors in remote conservation areas. Over 140 countries have deployed LoRaWAN networks, connecting 200+ million devices worldwide for social good applications."
    },
    {
      "footnote_id": "fn-raspberry-pi-development",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 272,
      "context": "...rces that mask many real-world limitations. A typical development platform, such as a Raspberry Pi 4[^fn-raspberry-pi-development], offers substantial computing power with its 1.5 GHz processor and 4 GB RAM. These resources allow...",
      "full_line": "Scaling machine learning systems from prototype to production deployment introduces core resource constraints that necessitate architectural redesign. Development environments provide computational resources that mask many real-world limitations. A typical development platform, such as a Raspberry Pi 4[^fn-raspberry-pi-development], offers substantial computing power with its 1.5 GHz processor and 4 GB RAM. These resources allow rapid prototyping and testing of machine learning models without immediate concern for optimization."
    },
    {
      "footnote_id": "fn-raspberry-pi-development",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 274,
      "context": "[^fn-raspberry-pi-development]: **Raspberry Pi Development Advantages**: Despite costing only $35-75, the Raspberry Pi 4 provides...",
      "full_line": "[^fn-raspberry-pi-development]: **Raspberry Pi Development Advantages**: Despite costing only $35-75, the Raspberry Pi 4 provides 1000\u00d7 more RAM and 10\u00d7 faster processing than typical production IoT devices. This substantial resource overhead enables developers to prototype using full Python frameworks like TensorFlow or PyTorch before optimizing for resource-constrained deployment. However, the Pi's 3-8W power consumption versus production devices' 0.1W creates a 30-80\u00d7 power gap that requires significant optimization during transition to real-world deployment."
    },
    {
      "footnote_id": "fn-esp32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 276,
      "context": "...of devices, cost and power constraints often mandate the use of microcontroller units like the ESP32[^fn-esp32-constraints], a widely used microcontroller unit from Espressif Systems, with its 240 MHz processor and 520 KB t...",
      "full_line": "Production deployments reveal stark resource limitations. When scaling to thousands of devices, cost and power constraints often mandate the use of microcontroller units like the ESP32[^fn-esp32-constraints], a widely used microcontroller unit from Espressif Systems, with its 240 MHz processor and 520 KB total SRAM with 320-450 KB available depending on the variant. This dramatic reduction in computational resources demands fundamental changes in system architecture. Models must be redesigned, optimization techniques such as quantization and pruning applied (detailed in @sec-model-optimizations), and inference strategies reconsidered."
    },
    {
      "footnote_id": "fn-esp32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 278,
      "context": "[^fn-esp32-constraints]: **ESP32 Capabilities**: Despite its constraints, the ESP32 costs only $2-5, consumes 30-150mA duri...",
      "full_line": "[^fn-esp32-constraints]: **ESP32 Capabilities**: Despite its constraints, the ESP32 costs only $2-5, consumes 30-150mA during operation, and includes Wi-Fi, Bluetooth, and various sensors. This makes it ideal for IoT deployments in social impact applications. For comparison, a smartphone processor is 100\u00d7 more powerful but costs 50\u00d7 more. The ESP32's limitations\u2014RAM smaller than a single Instagram photo\u2014force engineers to develop ingenious optimization techniques that often benefit all platforms."
    },
    {
      "footnote_id": "fn-plantvillage-nuru",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 645,
      "context": "PlantVillage Nuru[^fn-plantvillage-nuru] operates with a baseline model optimized for resource-constrained environments. The system employs...",
      "full_line": "PlantVillage Nuru[^fn-plantvillage-nuru] operates with a baseline model optimized for resource-constrained environments. The system employs quantized convolutional neural networks (typically 2-5 MB in size) running on entry-level smartphones, capable of processing images at 1-2 frames per second while consuming less than 100 mW of power. These models leverage mobile-optimized frameworks discussed in @sec-ai-frameworks to achieve efficient on-device inference. The on-device models achieve 85-90% accuracy in identifying common crop diseases, providing important diagnostic capabilities without requiring network connectivity."
    },
    {
      "footnote_id": "fn-plantvillage-nuru",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 647,
      "context": "[^fn-plantvillage-nuru]: **PlantVillage Nuru Real-World Impact**: Deployed across 500,000+ farmers in East Africa since 201...",
      "full_line": "[^fn-plantvillage-nuru]: **PlantVillage Nuru Real-World Impact**: Deployed across 500,000+ farmers in East Africa since 2019, Nuru has helped identify crop diseases affecting $2.6 billion worth of annual cassava production. The app works on $30 smartphones offline, processing 2.1 million crop images annually. Field studies show 73% reduction in crop losses and 40% increase in farmer incomes where the system is actively used, demonstrating how progressive enhancement patterns scale impact in resource-constrained environments."
    },
    {
      "footnote_id": "fn-whetstone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 88,
      "context": "...computational tasks. The [Whetstone benchmark](https://en.wikipedia.org/wiki/Whetstone_(benchmark))[^fn-whetstone], introduced in 1964 to measure floating-point arithmetic performance, became a definitive standard...",
      "full_line": "The evolution of benchmarks in computing illustrates how systematic performance measurement has shaped technological progress. During the 1960s and 1970s, when mainframe computers dominated computing, performance benchmarks focused on basic computational tasks. The [Whetstone benchmark](https://en.wikipedia.org/wiki/Whetstone_(benchmark))[^fn-whetstone], introduced in 1964 to measure floating-point arithmetic performance, became a definitive standard that demonstrated how systematic testing could drive improvements in computer architecture [@curnow1976synthetic]."
    },
    {
      "footnote_id": "fn-whetstone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 90,
      "context": "[^fn-whetstone]: **Whetstone**: Named after the Whetstone ALGOL compiler, this 1964 benchmark measured floating-poi...",
      "full_line": "[^fn-whetstone]: **Whetstone**: Named after the Whetstone ALGOL compiler, this 1964 benchmark measured floating-point arithmetic performance in KIPS (thousands of instructions per second). It became the first widely-adopted standardized performance test, revealing that IBM System/360 processors could vary by 10x in floating-point performance despite similar architectures."
    },
    {
      "footnote_id": "fn-linpack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 92,
      "context": "The introduction of the [LINPACK benchmark](https://en.wikipedia.org/wiki/LINPACK_benchmark)[^fn-linpack] in 1979 expanded the focus of performance evaluation, offering a means to assess how efficiently sy...",
      "full_line": "The introduction of the [LINPACK benchmark](https://en.wikipedia.org/wiki/LINPACK_benchmark)[^fn-linpack] in 1979 expanded the focus of performance evaluation, offering a means to assess how efficiently systems solved linear equations. As computing shifted toward personal computers in the 1980s, the need for standardized performance measurement grew. The [Dhrystone benchmark](https://en.wikipedia.org/wiki/Dhrystone)[^fn-dhrystone], introduced in 1984, provided one of the first integer-based benchmarks, complementing floating-point evaluations [@Weicker1984]."
    },
    {
      "footnote_id": "fn-dhrystone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 92,
      "context": "...zed performance measurement grew. The [Dhrystone benchmark](https://en.wikipedia.org/wiki/Dhrystone)[^fn-dhrystone], introduced in 1984, provided one of the first integer-based benchmarks, complementing floating-poi...",
      "full_line": "The introduction of the [LINPACK benchmark](https://en.wikipedia.org/wiki/LINPACK_benchmark)[^fn-linpack] in 1979 expanded the focus of performance evaluation, offering a means to assess how efficiently systems solved linear equations. As computing shifted toward personal computers in the 1980s, the need for standardized performance measurement grew. The [Dhrystone benchmark](https://en.wikipedia.org/wiki/Dhrystone)[^fn-dhrystone], introduced in 1984, provided one of the first integer-based benchmarks, complementing floating-point evaluations [@Weicker1984]."
    },
    {
      "footnote_id": "fn-linpack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 94,
      "context": "[^fn-linpack]: **LINPACK**: Developed at Argonne National Laboratory to solve dense systems of linear equations,...",
      "full_line": "[^fn-linpack]: **LINPACK**: Developed at Argonne National Laboratory to solve dense systems of linear equations, LINPACK became famous for the Top500 supercomputer rankings starting in 1993. Modern systems achieve over 1 exaflop (10^18 operations/second) compared to the original 1979 benchmarks measuring mere megaflops."
    },
    {
      "footnote_id": "fn-dhrystone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 96,
      "context": "[^fn-dhrystone]: **Dhrystone**: Created by Reinhold Weicker to complement Whetstone's floating-point focus, Dhrysto...",
      "full_line": "[^fn-dhrystone]: **Dhrystone**: Created by Reinhold Weicker to complement Whetstone's floating-point focus, Dhrystone measures integer and string operations in DMIPS (Dhrystone MIPS). Unlike synthetic floating-point tests, it aimed to reflect \"typical\" programming constructs, though it became vulnerable to compiler optimizations that could artificially inflate scores."
    },
    {
      "footnote_id": "fn-spec",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 98,
      "context": "...rameworks that emphasized real-world workloads. The [SPEC CPU benchmarks](https://www.spec.org/cpu/)[^fn-spec], introduced in 1989 by the [System Performance Evaluation Cooperative (SPEC)](https://www.spec.org/...",
      "full_line": "The late 1980s and early 1990s saw the emergence of systematic benchmarking frameworks that emphasized real-world workloads. The [SPEC CPU benchmarks](https://www.spec.org/cpu/)[^fn-spec], introduced in 1989 by the [System Performance Evaluation Cooperative (SPEC)](https://www.spec.org/), changed hardware evaluation by shifting from synthetic tests to a standardized suite designed to measure performance using practical computing workloads. This approach allowed manufacturers to optimize their systems for real applications, accelerating advances in processor design and software optimization."
    },
    {
      "footnote_id": "fn-spec",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 100,
      "context": "[^fn-spec]: **SPEC CPU**: Founded by major vendors including Sun, IBM, and DEC to combat \"benchmark wars\" with...",
      "full_line": "[^fn-spec]: **SPEC CPU**: Founded by major vendors including Sun, IBM, and DEC to combat \"benchmark wars\" with misleading metrics, SPEC CPU introduced standardized real-world applications like compression, compilers, and scientific computing. SPEC2017 includes 43 benchmarks representing actual workloads, with results reported as geometric means to prevent gaming, representing a 5000x improvement over early synthetic benchmarks."
    },
    {
      "footnote_id": "fn-3dmark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 102,
      "context": "...y 2000s presented new benchmarking challenges. The introduction of [3DMark](https://www.3dmark.com/)[^fn-3dmark] in 1998 established an industry standard for evaluating graphics performance, shaping the developme...",
      "full_line": "The increasing demand for graphics-intensive applications and mobile computing in the 1990s and early 2000s presented new benchmarking challenges. The introduction of [3DMark](https://www.3dmark.com/)[^fn-3dmark] in 1998 established an industry standard for evaluating graphics performance, shaping the development of programmable shaders and modern GPU architectures. Mobile computing introduced power efficiency as a constraint, necessitating benchmarks that assessed both computational performance and energy consumption. The release of [MobileMark](https://bapco.com/products/mobilemark-2014/)[^fn-mobilemark] by [BAPCo](https://bapco.com/) provided a means to evaluate power efficiency in laptops and mobile devices, influencing the development of energy-efficient architectures such as [ARM](https://www.arm.com/)."
    },
    {
      "footnote_id": "fn-mobilemark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 102,
      "context": "...nce and energy consumption. The release of [MobileMark](https://bapco.com/products/mobilemark-2014/)[^fn-mobilemark] by [BAPCo](https://bapco.com/) provided a means to evaluate power efficiency in laptops and mobile...",
      "full_line": "The increasing demand for graphics-intensive applications and mobile computing in the 1990s and early 2000s presented new benchmarking challenges. The introduction of [3DMark](https://www.3dmark.com/)[^fn-3dmark] in 1998 established an industry standard for evaluating graphics performance, shaping the development of programmable shaders and modern GPU architectures. Mobile computing introduced power efficiency as a constraint, necessitating benchmarks that assessed both computational performance and energy consumption. The release of [MobileMark](https://bapco.com/products/mobilemark-2014/)[^fn-mobilemark] by [BAPCo](https://bapco.com/) provided a means to evaluate power efficiency in laptops and mobile devices, influencing the development of energy-efficient architectures such as [ARM](https://www.arm.com/)."
    },
    {
      "footnote_id": "fn-3dmark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 104,
      "context": "[^fn-3dmark]: **3DMark**: Created by Finnish company Futuremark (now UL), this graphics benchmark drove GPU inno...",
      "full_line": "[^fn-3dmark]: **3DMark**: Created by Finnish company Futuremark (now UL), this graphics benchmark drove GPU innovation by testing real-time 3D rendering capabilities. Early versions measured triangle throughput and texture fill rates; modern 3DMark tests ray tracing and DLSS performance, with scores ranging from mobile devices (1,000 points) to high-end gaming PCs (30,000+ points)."
    },
    {
      "footnote_id": "fn-mobilemark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 106,
      "context": "[^fn-mobilemark]: **MobileMark**: Developed by the Business Applications Performance Corporation, this benchmark sim...",
      "full_line": "[^fn-mobilemark]: **MobileMark**: Developed by the Business Applications Performance Corporation, this benchmark simulates real laptop usage patterns including web browsing, video playback, and productivity tasks. Unlike peak performance tests, MobileMark measures battery life under realistic workloads, typically showing 6-12 hour endurance for modern laptops versus 15-30 minutes for synthetic stress tests."
    },
    {
      "footnote_id": "fn-cloudsuite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 108,
      "context": "...and cost-effectiveness [@ranganathan2024twenty]. Benchmarks like [CloudSuite](http://cloudsuite.ch/)[^fn-cloudsuite] have become important for evaluating cloud infrastructure, measuring how well systems handle distri...",
      "full_line": "Benchmarking focus in the past decade has shifted toward cloud computing, big data, and artificial intelligence. Cloud service providers such as Amazon Web Services and Google Cloud optimize their platforms based on performance, scalability, and cost-effectiveness [@ranganathan2024twenty]. Benchmarks like [CloudSuite](http://cloudsuite.ch/)[^fn-cloudsuite] have become important for evaluating cloud infrastructure, measuring how well systems handle distributed workloads."
    },
    {
      "footnote_id": "fn-cloudsuite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 110,
      "context": "[^fn-cloudsuite]: **CloudSuite**: Developed at EPFL to address the gap between traditional benchmarks and modern dat...",
      "full_line": "[^fn-cloudsuite]: **CloudSuite**: Developed at EPFL to address the gap between traditional benchmarks and modern datacenter workloads, CloudSuite includes realistic applications like web search, data analytics, and media streaming. Unlike synthetic benchmarks, it measures end-to-end performance including network latency, storage I/O, and memory bandwidth, revealing that cloud applications are memory-bound rather than CPU-bound. Machine learning has introduced another dimension of performance evaluation. The introduction of MLPerf in 2018 established a widely accepted standard for measuring machine learning training and inference efficiency across different hardware architectures."
    },
    {
      "footnote_id": "fn-spec-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 118,
      "context": "...l standardized benchmarks that quantify energy efficiency. [SPEC Power](https://www.spec.org/power/)[^fn-spec-power] provides a widely accepted methodology for measuring server efficiency across varying workload leve...",
      "full_line": "The industry has responded with several standardized benchmarks that quantify energy efficiency. [SPEC Power](https://www.spec.org/power/)[^fn-spec-power] provides a widely accepted methodology for measuring server efficiency across varying workload levels, allowing for direct comparisons of power-performance trade-offs. The [Green500](https://top500.org/lists/green500/)[^fn-green500] ranking applies similar principles to high-performance computing, ranking the world's most powerful supercomputers based on their energy efficiency rather than their raw performance. The [ENERGY STAR](https://www.energystar.gov/products/computers)[^fn-energy-star] certification program has also established foundational energy standards that have shaped the design of consumer and enterprise computing systems."
    },
    {
      "footnote_id": "fn-green500",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 118,
      "context": "...rect comparisons of power-performance trade-offs. The [Green500](https://top500.org/lists/green500/)[^fn-green500] ranking applies similar principles to high-performance computing, ranking the world's most powerful...",
      "full_line": "The industry has responded with several standardized benchmarks that quantify energy efficiency. [SPEC Power](https://www.spec.org/power/)[^fn-spec-power] provides a widely accepted methodology for measuring server efficiency across varying workload levels, allowing for direct comparisons of power-performance trade-offs. The [Green500](https://top500.org/lists/green500/)[^fn-green500] ranking applies similar principles to high-performance computing, ranking the world's most powerful supercomputers based on their energy efficiency rather than their raw performance. The [ENERGY STAR](https://www.energystar.gov/products/computers)[^fn-energy-star] certification program has also established foundational energy standards that have shaped the design of consumer and enterprise computing systems."
    },
    {
      "footnote_id": "fn-energy-star",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 118,
      "context": "...rather than their raw performance. The [ENERGY STAR](https://www.energystar.gov/products/computers)[^fn-energy-star] certification program has also established foundational energy standards that have shaped the desig...",
      "full_line": "The industry has responded with several standardized benchmarks that quantify energy efficiency. [SPEC Power](https://www.spec.org/power/)[^fn-spec-power] provides a widely accepted methodology for measuring server efficiency across varying workload levels, allowing for direct comparisons of power-performance trade-offs. The [Green500](https://top500.org/lists/green500/)[^fn-green500] ranking applies similar principles to high-performance computing, ranking the world's most powerful supercomputers based on their energy efficiency rather than their raw performance. The [ENERGY STAR](https://www.energystar.gov/products/computers)[^fn-energy-star] certification program has also established foundational energy standards that have shaped the design of consumer and enterprise computing systems."
    },
    {
      "footnote_id": "fn-spec-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 120,
      "context": "[^fn-spec-power]: **SPEC Power**: Introduced in 2007 to address the growing importance of energy efficiency in serve...",
      "full_line": "[^fn-spec-power]: **SPEC Power**: Introduced in 2007 to address the growing importance of energy efficiency in server design, SPEC Power measures performance per watt across 10 different load levels from 10% to 100%. Results show that modern servers achieve 8-12 SPECpower_ssj2008 scores per watt, compared to 1-3 for systems from the mid-2000s, representing a 4x efficiency improvement."
    },
    {
      "footnote_id": "fn-green500",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 122,
      "context": "[^fn-green500]: **Green500**: Started in 2007 as a counterpart to the Top500 supercomputer list, Green500 ranks sy...",
      "full_line": "[^fn-green500]: **Green500**: Started in 2007 as a counterpart to the Top500 supercomputer list, Green500 ranks systems by FLOPS per watt rather than raw performance. The most efficient systems achieve over 60 gigaFLOPS per watt compared to less than 1 gigaFLOPS/watt for early 2000s supercomputers, demonstrating improvements in computational efficiency."
    },
    {
      "footnote_id": "fn-energy-star",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 124,
      "context": "[^fn-energy-star]: **ENERGY STAR**: Launched by the EPA in 1992, this voluntary program has prevented over 4 billion...",
      "full_line": "[^fn-energy-star]: **ENERGY STAR**: Launched by the EPA in 1992, this voluntary program has prevented over 4 billion tons of greenhouse gas emissions and saved consumers $450 billion on energy bills. Computing equipment must meet strict efficiency requirements: ENERGY STAR computers typically consume 30-65% less energy than standard models during operation and sleep modes."
    },
    {
      "footnote_id": "fn-bench-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 164,
      "context": "...ardized methodologies tailored to each domain's challenges. Algorithmic benchmarks, such as ImageNet[^fn-bench-imagenet] [@deng2009imagenet], establish these evaluation frameworks, providing a consistent basis for compar...",
      "full_line": "AI algorithms must balance multiple performance objectives, including accuracy, speed, resource efficiency, and generalization capability. As machine learning applications span domains, including computer vision, natural language processing, speech recognition, and reinforcement learning, evaluating these objectives requires standardized methodologies tailored to each domain's challenges. Algorithmic benchmarks, such as ImageNet[^fn-bench-imagenet] [@deng2009imagenet], establish these evaluation frameworks, providing a consistent basis for comparing different machine learning approaches."
    },
    {
      "footnote_id": "fn-bench-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 166,
      "context": "[^fn-bench-imagenet]: **ImageNet**: Created by Fei-Fei Li at Stanford starting in 2007, this dataset contains 14 million...",
      "full_line": "[^fn-bench-imagenet]: **ImageNet**: Created by Fei-Fei Li at Stanford starting in 2007, this dataset contains 14 million images across 20,000 categories, with 1.2 million images used for the annual classification challenge (ILSVRC). ImageNet's impact is profound: it sparked the deep learning revolution when AlexNet achieved 15.3% top-5 error in 2012, compared to 25.8% for traditional methods, the largest single-year improvement in computer vision."
    },
    {
      "footnote_id": "fn-bench-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 176,
      "context": "...task over the years. Starting from the baseline models in 2010 and 2011, the introduction of AlexNet[^fn-bench-alexnet] in 2012 marked an improvement, reducing the error rate from 25.8% to 16.4%. Subsequent models like...",
      "full_line": "The graph in @fig-imagenet-challenge illustrates the reduction in error rates on the [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://www.image-net.org/challenges/LSVRC/) classification task over the years. Starting from the baseline models in 2010 and 2011, the introduction of AlexNet[^fn-bench-alexnet] in 2012 marked an improvement, reducing the error rate from 25.8% to 16.4%. Subsequent models like ZFNet, VGGNet, GoogleNet, and ResNet[^fn-bench-resnet] continued this trend, with ResNet achieving an error rate of 3.57% by 2015. This progression highlights how algorithmic benchmarks measure current capabilities and drive advancements in AI performance."
    },
    {
      "footnote_id": "fn-bench-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 176,
      "context": "...cing the error rate from 25.8% to 16.4%. Subsequent models like ZFNet, VGGNet, GoogleNet, and ResNet[^fn-bench-resnet] continued this trend, with ResNet achieving an error rate of 3.57% by 2015. This progression highli...",
      "full_line": "The graph in @fig-imagenet-challenge illustrates the reduction in error rates on the [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://www.image-net.org/challenges/LSVRC/) classification task over the years. Starting from the baseline models in 2010 and 2011, the introduction of AlexNet[^fn-bench-alexnet] in 2012 marked an improvement, reducing the error rate from 25.8% to 16.4%. Subsequent models like ZFNet, VGGNet, GoogleNet, and ResNet[^fn-bench-resnet] continued this trend, with ResNet achieving an error rate of 3.57% by 2015. This progression highlights how algorithmic benchmarks measure current capabilities and drive advancements in AI performance."
    },
    {
      "footnote_id": "fn-bench-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 178,
      "context": "[^fn-bench-alexnet]: **AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton at the University o...",
      "full_line": "[^fn-bench-alexnet]: **AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton at the University of Toronto, this 8-layer neural network revolutionized computer vision in 2012. With 60 million parameters trained on two GTX 580 GPUs, AlexNet introduced key innovations in neural network design that became standard techniques in modern AI."
    },
    {
      "footnote_id": "fn-bench-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 180,
      "context": "[^fn-bench-resnet]: **ResNet**: Microsoft's Residual Networks, introduced in 2015 by Kaiming He and colleagues, solved...",
      "full_line": "[^fn-bench-resnet]: **ResNet**: Microsoft's Residual Networks, introduced in 2015 by Kaiming He and colleagues, solved the vanishing gradient problem with skip connections, enabling networks with 152+ layers. ResNet-50 became the de facto standard for transfer learning, while ResNet-152 achieved superhuman performance on ImageNet with 3.57% top-5 error, exceeding the estimated 5% human error rate."
    },
    {
      "footnote_id": "fn-bench-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 215,
      "context": "...encompassing general-purpose CPUs, graphics processing units (GPUs), tensor processing units (TPUs)[^fn-bench-tpu], and application-specific integrated circuits (ASICs)[^fn-asic], determines the speed, efficiency,...",
      "full_line": "AI computations place substantial demands on computational resources. The underlying hardware infrastructure, encompassing general-purpose CPUs, graphics processing units (GPUs), tensor processing units (TPUs)[^fn-bench-tpu], and application-specific integrated circuits (ASICs)[^fn-asic], determines the speed, efficiency, and scalability of AI solutions. System benchmarks establish standardized methodologies for evaluating hardware performance across AI workloads, measuring metrics including computational throughput, memory bandwidth, power efficiency, and scaling characteristics [@reddi2020mlperf; @mattson2020mlperf]."
    },
    {
      "footnote_id": "fn-asic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 215,
      "context": "...tensor processing units (TPUs)[^fn-bench-tpu], and application-specific integrated circuits (ASICs)[^fn-asic], determines the speed, efficiency, and scalability of AI solutions. System benchmarks establish sta...",
      "full_line": "AI computations place substantial demands on computational resources. The underlying hardware infrastructure, encompassing general-purpose CPUs, graphics processing units (GPUs), tensor processing units (TPUs)[^fn-bench-tpu], and application-specific integrated circuits (ASICs)[^fn-asic], determines the speed, efficiency, and scalability of AI solutions. System benchmarks establish standardized methodologies for evaluating hardware performance across AI workloads, measuring metrics including computational throughput, memory bandwidth, power efficiency, and scaling characteristics [@reddi2020mlperf; @mattson2020mlperf]."
    },
    {
      "footnote_id": "fn-bench-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 217,
      "context": "[^fn-bench-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for neural network wo...",
      "full_line": "[^fn-bench-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for neural network workloads, first deployed secretly in 2015 and announced in 2016. The first-generation TPU achieved 15-30x better performance per watt than contemporary GPUs for inference, while TPU v4 pods deliver 1.1 exaFLOPS of BF16 computing power (full pod configuration), demonstrating the capabilities of specialized AI hardware."
    },
    {
      "footnote_id": "fn-asic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 219,
      "context": "[^fn-asic]: **Application-Specific Integrated Circuit (ASIC)**: Custom chips designed for specific computation...",
      "full_line": "[^fn-asic]: **Application-Specific Integrated Circuit (ASIC)**: Custom chips designed for specific computational tasks, offering superior performance and energy efficiency compared to general-purpose processors. AI ASICs like Google's TPUs, Tesla's FSD chips, and Bitcoin mining ASICs can achieve 100-1000x better efficiency than CPUs for their target applications, but lack the flexibility for other workloads."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 230,
      "context": "...memory-bound provides insight for optimization decisions. Computational intensity, measured as FLOPS[^fn-flops] per byte of data movement, determines performance limits. Consider an NVIDIA A100 GPU with 312 TFLO...",
      "full_line": "Effective benchmark interpretation requires understanding the performance characteristics of target hardware. Understanding whether AI workloads are compute-bound or memory-bound provides insight for optimization decisions. Computational intensity, measured as FLOPS[^fn-flops] per byte of data movement, determines performance limits. Consider an NVIDIA A100 GPU with 312 TFLOPS of tensor performance and 1.6 TB/s memory bandwidth, yielding an arithmetic intensity threshold of 195 FLOPS/byte. The architectural foundations for understanding these hardware characteristics are established in @sec-ai-acceleration, which provides context for interpreting system benchmark results. High-intensity operations like dense matrix multiplication in certain AI model operations (typically >200 FLOPS/byte) achieve near-peak computational throughput on the A100. For example, a ResNet-50 forward pass on large batch sizes (256+) achieves arithmetic intensity of ~300 FLOPS/byte, enabling 85-90% of peak tensor performance (280 TFLOPS actual vs 312 TFLOPS theoretical). Conversely, low-intensity operations like activation functions and certain lightweight operations (<10 FLOPS/byte) become memory bandwidth limited, utilizing only a fraction of the GPU's computational capacity. A BERT inference with batch size 1 achieves only 8 FLOPS/byte arithmetic intensity, limiting performance to 12.8 TFLOPS (1.6 TB/s \u00d7 8 FLOPS/byte), representing just 4% of peak computational capability."
    },
    {
      "footnote_id": "fn-roofline-model",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 232,
      "context": "This analysis, formalized in roofline models[^fn-roofline-model], guides both algorithm design and hardware selection by identifying the dominant performance constr...",
      "full_line": "This analysis, formalized in roofline models[^fn-roofline-model], guides both algorithm design and hardware selection by identifying the dominant performance constraints for specific workloads. Understanding these quantitative relationships enables engineers to predict performance bottlenecks and optimize both model architectures and deployment strategies. For instance, increasing batch size from 1 to 32 for transformer inference can shift operations from memory-bound (8 FLOPS/byte) to compute-bound (150 FLOPS/byte), improving GPU utilization from 4% to 65%."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 234,
      "context": "...memory-bound provides insight for optimization decisions. Computational intensity, measured as FLOPS[^fn-flops] per byte of data movement, determines performance limits. Consider an NVIDIA A100 GPU with 312 TFLO...",
      "full_line": "Effective benchmark interpretation requires understanding the performance characteristics of target hardware. Understanding whether AI workloads are compute-bound or memory-bound provides insight for optimization decisions. Computational intensity, measured as FLOPS[^fn-flops] per byte of data movement, determines performance limits. Consider an NVIDIA A100 GPU with 312 TFLOPS of tensor performance and 1.6 TB/s memory bandwidth, yielding an arithmetic intensity threshold of 195 FLOPS/byte. The architectural foundations for understanding these hardware characteristics are established in @sec-ai-acceleration, which provides context for interpreting system benchmark results. High-intensity operations like dense matrix multiplication in certain AI model operations (typically >200 FLOPS/byte) achieve near-peak computational throughput on the A100. Conversely, low-intensity operations like certain lightweight operations (<10 FLOPS/byte) become memory bandwidth limited, utilizing only a fraction of the GPU's computational capacity. This analysis, formalized in roofline models[^fn-roofline-model], guides both algorithm design and hardware selection by identifying the dominant performance constraints for specific workloads."
    },
    {
      "footnote_id": "fn-roofline-model",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 234,
      "context": "...ng only a fraction of the GPU's computational capacity. This analysis, formalized in roofline models[^fn-roofline-model], guides both algorithm design and hardware selection by identifying the dominant performance constr...",
      "full_line": "Effective benchmark interpretation requires understanding the performance characteristics of target hardware. Understanding whether AI workloads are compute-bound or memory-bound provides insight for optimization decisions. Computational intensity, measured as FLOPS[^fn-flops] per byte of data movement, determines performance limits. Consider an NVIDIA A100 GPU with 312 TFLOPS of tensor performance and 1.6 TB/s memory bandwidth, yielding an arithmetic intensity threshold of 195 FLOPS/byte. The architectural foundations for understanding these hardware characteristics are established in @sec-ai-acceleration, which provides context for interpreting system benchmark results. High-intensity operations like dense matrix multiplication in certain AI model operations (typically >200 FLOPS/byte) achieve near-peak computational throughput on the A100. Conversely, low-intensity operations like certain lightweight operations (<10 FLOPS/byte) become memory bandwidth limited, utilizing only a fraction of the GPU's computational capacity. This analysis, formalized in roofline models[^fn-roofline-model], guides both algorithm design and hardware selection by identifying the dominant performance constraints for specific workloads."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 237,
      "context": "[^fn-flops]: **FLOPS**: Floating-Point Operations Per Second, a measure of computational performance indicating...",
      "full_line": "[^fn-flops]: **FLOPS**: Floating-Point Operations Per Second, a measure of computational performance indicating how many floating-point calculations a processor can execute in one second. Modern AI accelerators achieve high FLOPS ratings: NVIDIA A100 delivers 312 TFLOPS (trillion FLOPS) for tensor operations, while high-end CPUs achieve 1-10 TFLOPS. FLOPS measurements help compare hardware capabilities and determine computational bottlenecks in ML workloads."
    },
    {
      "footnote_id": "fn-roofline-model",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 239,
      "context": "[^fn-roofline-model]: **Roofline Model**: A visual performance model developed at UC Berkeley that plots computational i...",
      "full_line": "[^fn-roofline-model]: **Roofline Model**: A visual performance model developed at UC Berkeley that plots computational intensity (FLOPS/byte) against performance (FLOPS/second) to identify whether algorithms are compute-bound or memory-bound. The \"roofline\" represents theoretical peak performance limits, with flat sections indicating memory bandwidth constraints and sloped sections showing compute capacity limits. This model helps optimize both algorithms and hardware selection by revealing performance bottlenecks."
    },
    {
      "footnote_id": "fn-bert",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 420,
      "context": "...comparable domains. In natural language processing applications, advanced language models like BERT[^fn-bert] have emerged as standard benchmarks for comparative analysis. The architectural details of transfor...",
      "full_line": "Baseline models serve as the reference points for evaluating novel approaches. These span from basic implementations, including linear regression for continuous predictions and logistic regression for classification tasks, to advanced architectures with proven success in comparable domains. In natural language processing applications, advanced language models like BERT[^fn-bert] have emerged as standard benchmarks for comparative analysis. The architectural details of transformers and their performance characteristics are thoroughly covered in @sec-dnn-architectures."
    },
    {
      "footnote_id": "fn-bert",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 422,
      "context": "[^fn-bert]: **BERT**: Bidirectional Encoder Representations from Transformers, introduced by Google in 2018, r...",
      "full_line": "[^fn-bert]: **BERT**: Bidirectional Encoder Representations from Transformers, introduced by Google in 2018, revolutionized natural language processing by pre-training on vast text corpora using masked language modeling. BERT-Large contains 340 million parameters and achieved state-of-the-art results on 11 NLP tasks, establishing the foundation for modern language models like GPT and ChatGPT."
    },
    {
      "footnote_id": "fn-docker",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 477,
      "context": "6. Environment management tools and configuration (e.g., Docker containers[^fn-docker], virtual environments)",
      "full_line": "6. Environment management tools and configuration (e.g., Docker containers[^fn-docker], virtual environments)"
    },
    {
      "footnote_id": "fn-docker",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 481,
      "context": "[^fn-docker]: **Docker**: Containerization platform that packages applications and their dependencies into light...",
      "full_line": "[^fn-docker]: **Docker**: Containerization platform that packages applications and their dependencies into lightweight, portable containers ensuring consistent execution across different environments. Widely adopted in ML benchmarking since 2013, Docker eliminates \"works on my machine\" problems by providing identical runtime environments, with MLPerf and other benchmark suites distributing official Docker images to guarantee reproducible results."
    },
    {
      "footnote_id": "fn-tensor-ops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 659,
      "context": "A key area of micro-benchmarking focuses on tensor operations[^fn-tensor-ops], which are the computational foundation of deep learning. Libraries like [cuDNN](https://developer....",
      "full_line": "A key area of micro-benchmarking focuses on tensor operations[^fn-tensor-ops], which are the computational foundation of deep learning. Libraries like [cuDNN](https://developer.nvidia.com/cudnn)[^fn-cudnn] by NVIDIA provide benchmarks for measuring fundamental computations such as convolutions and matrix multiplications across different hardware configurations. These measurements help developers understand how their hardware handles the core mathematical operations that dominate ML workloads."
    },
    {
      "footnote_id": "fn-cudnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 659,
      "context": "...omputational foundation of deep learning. Libraries like [cuDNN](https://developer.nvidia.com/cudnn)[^fn-cudnn] by NVIDIA provide benchmarks for measuring fundamental computations such as convolutions and matrix...",
      "full_line": "A key area of micro-benchmarking focuses on tensor operations[^fn-tensor-ops], which are the computational foundation of deep learning. Libraries like [cuDNN](https://developer.nvidia.com/cudnn)[^fn-cudnn] by NVIDIA provide benchmarks for measuring fundamental computations such as convolutions and matrix multiplications across different hardware configurations. These measurements help developers understand how their hardware handles the core mathematical operations that dominate ML workloads."
    },
    {
      "footnote_id": "fn-tensor-ops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 661,
      "context": "[^fn-tensor-ops]: **Tensor Operations**: Multi-dimensional array computations that form the backbone of neural netwo...",
      "full_line": "[^fn-tensor-ops]: **Tensor Operations**: Multi-dimensional array computations that form the backbone of neural networks, including matrix multiplication (GEMM), convolution, and element-wise operations. Modern AI accelerators optimize these primitives: NVIDIA's Tensor Cores can achieve 312 TFLOPS for mixed-precision matrix multiplications, compared to 15 TFLOPS for traditional FP32 computations\u2014a 20x speedup."
    },
    {
      "footnote_id": "fn-cudnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 663,
      "context": "[^fn-cudnn]: **cuDNN**: CUDA Deep Neural Network library, NVIDIA's GPU-accelerated library of primitives for de...",
      "full_line": "[^fn-cudnn]: **cuDNN**: CUDA Deep Neural Network library, NVIDIA's GPU-accelerated library of primitives for deep neural networks. Released in 2014, cuDNN provides highly optimized implementations for convolutions, pooling, normalization, and activation layers, delivering up to 10x performance improvements over naive implementations and becoming the de facto standard for GPU-accelerated deep learning."
    },
    {
      "footnote_id": "fn-bench-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 723,
      "context": "For instance, large-scale models like [OpenAI's GPT-3](https://arxiv.org/abs/2005.14165)[^fn-bench-gpt3] [@brown2020language], which consists of 175 billion parameters trained on 45 terabytes of data, hig...",
      "full_line": "For instance, large-scale models like [OpenAI's GPT-3](https://arxiv.org/abs/2005.14165)[^fn-bench-gpt3] [@brown2020language], which consists of 175 billion parameters trained on 45 terabytes of data, highlight the immense computational demands of training. Benchmarks enable systematic evaluation of the underlying systems to ensure that hardware and software configurations can meet these demands efficiently."
    },
    {
      "footnote_id": "fn-bench-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 725,
      "context": "[^fn-bench-gpt3]: **GPT-3**: OpenAI's 2020 language model with 175 billion parameters, trained on 300 billion tokens...",
      "full_line": "[^fn-bench-gpt3]: **GPT-3**: OpenAI's 2020 language model with 175 billion parameters, trained on 300 billion tokens using 10,000 NVIDIA V100 GPUs for several months at an estimated cost of $4.6 million (Lambda Labs estimate). GPT-3 demonstrated emergent abilities like few-shot learning and in-context reasoning, establishing the paradigm of scaling laws where larger models consistently outperform smaller ones across diverse language tasks."
    },
    {
      "footnote_id": "fn-bench-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 951,
      "context": "...ne learning frameworks implement various low-level optimizations, including mixed-precision training[^fn-bench-mixed-precision], memory-efficient data loading, and distributed training strategies, that can significantly impact...",
      "full_line": "Beyond hardware selection, training benchmarks also inform software optimizations. Machine learning frameworks implement various low-level optimizations, including mixed-precision training[^fn-bench-mixed-precision], memory-efficient data loading, and distributed training strategies, that can significantly impact system performance. Benchmarks help quantify the impact of these optimizations, ensuring that training systems are configured for maximum efficiency."
    },
    {
      "footnote_id": "fn-bench-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 953,
      "context": "[^fn-bench-mixed-precision]: **Mixed-Precision Training**: A training technique that uses both 16-bit (FP16) and 32-bit (FP32)...",
      "full_line": "[^fn-bench-mixed-precision]: **Mixed-Precision Training**: A training technique that uses both 16-bit (FP16) and 32-bit (FP32) floating-point representations to accelerate training while maintaining model accuracy. Introduced by NVIDIA in 2017, mixed precision can achieve 1.5-2x speedups on modern GPUs with Tensor Cores while reducing memory usage by ~40%, enabling larger batch sizes and faster convergence for large models."
    },
    {
      "footnote_id": "fn-data-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1008,
      "context": "Parallelism in training is categorized into data parallelism[^fn-data-parallel], model parallelism[^fn-model-parallel], and pipeline parallelism, each presenting distinct challeng...",
      "full_line": "Parallelism in training is categorized into data parallelism[^fn-data-parallel], model parallelism[^fn-model-parallel], and pipeline parallelism, each presenting distinct challenges. Data parallelism, the most commonly used strategy, involves splitting the training dataset across multiple compute nodes. The efficiency of this approach depends on synchronization mechanisms and gradient communication overhead. In contrast, model parallelism partitions the neural network itself, requiring efficient coordination between processors. Benchmarks evaluate how well a system manages these parallelism strategies without degrading accuracy convergence."
    },
    {
      "footnote_id": "fn-model-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1008,
      "context": "Parallelism in training is categorized into data parallelism[^fn-data-parallel], model parallelism[^fn-model-parallel], and pipeline parallelism, each presenting distinct challenges. Data parallelism, the most commonly...",
      "full_line": "Parallelism in training is categorized into data parallelism[^fn-data-parallel], model parallelism[^fn-model-parallel], and pipeline parallelism, each presenting distinct challenges. Data parallelism, the most commonly used strategy, involves splitting the training dataset across multiple compute nodes. The efficiency of this approach depends on synchronization mechanisms and gradient communication overhead. In contrast, model parallelism partitions the neural network itself, requiring efficient coordination between processors. Benchmarks evaluate how well a system manages these parallelism strategies without degrading accuracy convergence."
    },
    {
      "footnote_id": "fn-data-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1010,
      "context": "[^fn-data-parallel]: **Data Parallelism**: The most common distributed training strategy where each GPU processes a dif...",
      "full_line": "[^fn-data-parallel]: **Data Parallelism**: The most common distributed training strategy where each GPU processes a different subset of the training batch, then synchronizes gradients across all nodes. Modern implementations use techniques like gradient accumulation and all-reduce operations to achieve near-linear scaling up to hundreds of GPUs, though communication overhead typically limits efficiency beyond 1000+ GPUs."
    },
    {
      "footnote_id": "fn-model-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1012,
      "context": "[^fn-model-parallel]: **Model Parallelism**: A distributed training approach where different parts of the neural network...",
      "full_line": "[^fn-model-parallel]: **Model Parallelism**: A distributed training approach where different parts of the neural network are placed on different GPUs, essential for models too large to fit in a single GPU's memory. GPT-3's 175B parameters required model parallelism across multiple nodes, as even high-memory GPUs can only hold ~40B parameters in mixed precision."
    },
    {
      "footnote_id": "fn-npu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1168,
      "context": "...ning, inference workloads often require specialized accelerators like NPUs (Neural Processing Units)[^fn-npu], FPGAs[^fn-fpga], and dedicated inference chips such as Google's Edge TPU[^fn-edge-tpu]. Inference...",
      "full_line": "Hardware selection plays an important role in inference efficiency. While GPUs and TPUs are widely used for training, inference workloads often require specialized accelerators like NPUs (Neural Processing Units)[^fn-npu], FPGAs[^fn-fpga], and dedicated inference chips such as Google's Edge TPU[^fn-edge-tpu]. Inference benchmarks evaluate the utilization and performance of these hardware components, helping practitioners choose the right configurations for their deployment needs."
    },
    {
      "footnote_id": "fn-fpga",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1168,
      "context": "...workloads often require specialized accelerators like NPUs (Neural Processing Units)[^fn-npu], FPGAs[^fn-fpga], and dedicated inference chips such as Google's Edge TPU[^fn-edge-tpu]. Inference benchmarks evalua...",
      "full_line": "Hardware selection plays an important role in inference efficiency. While GPUs and TPUs are widely used for training, inference workloads often require specialized accelerators like NPUs (Neural Processing Units)[^fn-npu], FPGAs[^fn-fpga], and dedicated inference chips such as Google's Edge TPU[^fn-edge-tpu]. Inference benchmarks evaluate the utilization and performance of these hardware components, helping practitioners choose the right configurations for their deployment needs."
    },
    {
      "footnote_id": "fn-edge-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1168,
      "context": "...Processing Units)[^fn-npu], FPGAs[^fn-fpga], and dedicated inference chips such as Google's Edge TPU[^fn-edge-tpu]. Inference benchmarks evaluate the utilization and performance of these hardware components, helpin...",
      "full_line": "Hardware selection plays an important role in inference efficiency. While GPUs and TPUs are widely used for training, inference workloads often require specialized accelerators like NPUs (Neural Processing Units)[^fn-npu], FPGAs[^fn-fpga], and dedicated inference chips such as Google's Edge TPU[^fn-edge-tpu]. Inference benchmarks evaluate the utilization and performance of these hardware components, helping practitioners choose the right configurations for their deployment needs."
    },
    {
      "footnote_id": "fn-npu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1170,
      "context": "[^fn-npu]: **Neural Processing Unit (NPU)**: Specialized processors designed specifically for AI workloads, f...",
      "full_line": "[^fn-npu]: **Neural Processing Unit (NPU)**: Specialized processors designed specifically for AI workloads, featuring optimized architectures for neural network operations. Modern smartphones include NPUs capable of 1-15 TOPS (Tera Operations Per Second), enabling on-device AI while consuming 100-1000x less power than GPUs for the same ML tasks."
    },
    {
      "footnote_id": "fn-fpga",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1172,
      "context": "[^fn-fpga]: **Field-Programmable Gate Array (FPGA)**: Reconfigurable silicon chips that can be programmed afte...",
      "full_line": "[^fn-fpga]: **Field-Programmable Gate Array (FPGA)**: Reconfigurable silicon chips that can be programmed after manufacturing to implement custom digital circuits. Unlike fixed ASICs, FPGAs offer flexibility to optimize for different algorithms, achieving 10-100x better energy efficiency than CPUs for specific ML workloads while maintaining adaptability to algorithm changes."
    },
    {
      "footnote_id": "fn-edge-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1174,
      "context": "[^fn-edge-tpu]: **Edge TPU**: Google's ultra-low-power AI accelerator designed for edge devices, consuming only 2...",
      "full_line": "[^fn-edge-tpu]: **Edge TPU**: Google's ultra-low-power AI accelerator designed for edge devices, consuming only 2 watts while delivering 4 TOPS of performance. Each Edge TPU is optimized for TensorFlow Lite models and costs around $25, making distributed AI deployment economically viable at massive scale."
    },
    {
      "footnote_id": "fn-tensorrt",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1251,
      "context": "Software optimizations are just as important. Frameworks like TensorRT[^fn-tensorrt], ONNX Runtime[^fn-onnx-runtime], and TVM[^fn-tvm] apply optimizations such as operator fusion[^fn-o...",
      "full_line": "Software optimizations are just as important. Frameworks like TensorRT[^fn-tensorrt], ONNX Runtime[^fn-onnx-runtime], and TVM[^fn-tvm] apply optimizations such as operator fusion[^fn-operator-fusion], numerical precision adjustments, and kernel tuning to improve inference speed and reduce computational overhead. These optimizations can make a significant difference, especially in environments with limited resources. Benchmarks allow developers to measure the impact of such techniques on latency, throughput, and power efficiency, ensuring that optimizations translate into real-world improvements without degrading model accuracy."
    },
    {
      "footnote_id": "fn-onnx-runtime",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1251,
      "context": "Software optimizations are just as important. Frameworks like TensorRT[^fn-tensorrt], ONNX Runtime[^fn-onnx-runtime], and TVM[^fn-tvm] apply optimizations such as operator fusion[^fn-operator-fusion], numerical preci...",
      "full_line": "Software optimizations are just as important. Frameworks like TensorRT[^fn-tensorrt], ONNX Runtime[^fn-onnx-runtime], and TVM[^fn-tvm] apply optimizations such as operator fusion[^fn-operator-fusion], numerical precision adjustments, and kernel tuning to improve inference speed and reduce computational overhead. These optimizations can make a significant difference, especially in environments with limited resources. Benchmarks allow developers to measure the impact of such techniques on latency, throughput, and power efficiency, ensuring that optimizations translate into real-world improvements without degrading model accuracy."
    },
    {
      "footnote_id": "fn-tvm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1251,
      "context": "...e just as important. Frameworks like TensorRT[^fn-tensorrt], ONNX Runtime[^fn-onnx-runtime], and TVM[^fn-tvm] apply optimizations such as operator fusion[^fn-operator-fusion], numerical precision adjustments,...",
      "full_line": "Software optimizations are just as important. Frameworks like TensorRT[^fn-tensorrt], ONNX Runtime[^fn-onnx-runtime], and TVM[^fn-tvm] apply optimizations such as operator fusion[^fn-operator-fusion], numerical precision adjustments, and kernel tuning to improve inference speed and reduce computational overhead. These optimizations can make a significant difference, especially in environments with limited resources. Benchmarks allow developers to measure the impact of such techniques on latency, throughput, and power efficiency, ensuring that optimizations translate into real-world improvements without degrading model accuracy."
    },
    {
      "footnote_id": "fn-operator-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1251,
      "context": "...sorrt], ONNX Runtime[^fn-onnx-runtime], and TVM[^fn-tvm] apply optimizations such as operator fusion[^fn-operator-fusion], numerical precision adjustments, and kernel tuning to improve inference speed and reduce computati...",
      "full_line": "Software optimizations are just as important. Frameworks like TensorRT[^fn-tensorrt], ONNX Runtime[^fn-onnx-runtime], and TVM[^fn-tvm] apply optimizations such as operator fusion[^fn-operator-fusion], numerical precision adjustments, and kernel tuning to improve inference speed and reduce computational overhead. These optimizations can make a significant difference, especially in environments with limited resources. Benchmarks allow developers to measure the impact of such techniques on latency, throughput, and power efficiency, ensuring that optimizations translate into real-world improvements without degrading model accuracy."
    },
    {
      "footnote_id": "fn-tensorrt",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1253,
      "context": "[^fn-tensorrt]: **TensorRT**: NVIDIA's high-performance inference optimizer and runtime library that accelerates d...",
      "full_line": "[^fn-tensorrt]: **TensorRT**: NVIDIA's high-performance inference optimizer and runtime library that accelerates deep learning models on NVIDIA GPUs. Introduced in 2016, TensorRT applies graph optimizations, kernel fusion, and precision calibration to achieve 1.5-7x speedups over naive implementations, supporting FP16, INT8, and sparse matrix operations."
    },
    {
      "footnote_id": "fn-onnx-runtime",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1255,
      "context": "[^fn-onnx-runtime]: **ONNX Runtime**: Microsoft's cross-platform, high-performance ML inferencing and training acceler...",
      "full_line": "[^fn-onnx-runtime]: **ONNX Runtime**: Microsoft's cross-platform, high-performance ML inferencing and training accelerator supporting the Open Neural Network Exchange (ONNX) format. Released in 2018, it enables models trained in any framework to run efficiently across different hardware (CPU, GPU, NPU) with optimizations like graph fusion and memory pattern optimization."
    },
    {
      "footnote_id": "fn-tvm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1257,
      "context": "[^fn-tvm]: **TVM**: An open-source deep learning compiler stack that optimizes tensor programs for diverse ha...",
      "full_line": "[^fn-tvm]: **TVM**: An open-source deep learning compiler stack that optimizes tensor programs for diverse hardware backends including CPUs, GPUs, and specialized accelerators. Developed at the University of Washington, TVM uses machine learning to automatically generate optimized code, achieving performance competitive with hand-tuned libraries while supporting new hardware architectures."
    },
    {
      "footnote_id": "fn-operator-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1259,
      "context": "[^fn-operator-fusion]: **Operator Fusion**: A compiler optimization technique that combines multiple neural network opera...",
      "full_line": "[^fn-operator-fusion]: **Operator Fusion**: A compiler optimization technique that combines multiple neural network operations into single kernels to reduce memory bandwidth requirements and improve cache efficiency. For example, fusing convolution with batch normalization and ReLU can eliminate intermediate memory writes, achieving 20-40% speedups in inference workloads."
    },
    {
      "footnote_id": "fn-fp32",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1309,
      "context": "...ference benchmarks evaluate how well models perform under different numerical settings, such as FP32[^fn-fp32], FP16[^fn-fp16], and INT8[^fn-int8]. Many modern AI accelerators support mixed-precision inference,...",
      "full_line": "Inference benchmarks evaluate how well models perform under different numerical settings, such as FP32[^fn-fp32], FP16[^fn-fp16], and INT8[^fn-int8]. Many modern AI accelerators support mixed-precision inference, allowing systems to dynamically adjust numerical representation based on workload requirements. Model compression techniques[^fn-model-compression] further improve efficiency, but their impact on model accuracy varies depending on the task and dataset. Benchmarks help determine whether these optimizations are viable for deployment, ensuring that improvements in efficiency do not come at the cost of unacceptable accuracy loss."
    },
    {
      "footnote_id": "fn-fp16",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1309,
      "context": "...ks evaluate how well models perform under different numerical settings, such as FP32[^fn-fp32], FP16[^fn-fp16], and INT8[^fn-int8]. Many modern AI accelerators support mixed-precision inference, allowing system...",
      "full_line": "Inference benchmarks evaluate how well models perform under different numerical settings, such as FP32[^fn-fp32], FP16[^fn-fp16], and INT8[^fn-int8]. Many modern AI accelerators support mixed-precision inference, allowing systems to dynamically adjust numerical representation based on workload requirements. Model compression techniques[^fn-model-compression] further improve efficiency, but their impact on model accuracy varies depending on the task and dataset. Benchmarks help determine whether these optimizations are viable for deployment, ensuring that improvements in efficiency do not come at the cost of unacceptable accuracy loss."
    },
    {
      "footnote_id": "fn-int8",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1309,
      "context": "...models perform under different numerical settings, such as FP32[^fn-fp32], FP16[^fn-fp16], and INT8[^fn-int8]. Many modern AI accelerators support mixed-precision inference, allowing systems to dynamically adj...",
      "full_line": "Inference benchmarks evaluate how well models perform under different numerical settings, such as FP32[^fn-fp32], FP16[^fn-fp16], and INT8[^fn-int8]. Many modern AI accelerators support mixed-precision inference, allowing systems to dynamically adjust numerical representation based on workload requirements. Model compression techniques[^fn-model-compression] further improve efficiency, but their impact on model accuracy varies depending on the task and dataset. Benchmarks help determine whether these optimizations are viable for deployment, ensuring that improvements in efficiency do not come at the cost of unacceptable accuracy loss."
    },
    {
      "footnote_id": "fn-model-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1309,
      "context": "...mically adjust numerical representation based on workload requirements. Model compression techniques[^fn-model-compression] further improve efficiency, but their impact on model accuracy varies depending on the task and dat...",
      "full_line": "Inference benchmarks evaluate how well models perform under different numerical settings, such as FP32[^fn-fp32], FP16[^fn-fp16], and INT8[^fn-int8]. Many modern AI accelerators support mixed-precision inference, allowing systems to dynamically adjust numerical representation based on workload requirements. Model compression techniques[^fn-model-compression] further improve efficiency, but their impact on model accuracy varies depending on the task and dataset. Benchmarks help determine whether these optimizations are viable for deployment, ensuring that improvements in efficiency do not come at the cost of unacceptable accuracy loss."
    },
    {
      "footnote_id": "fn-fp32",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1311,
      "context": "[^fn-fp32]: **FP32**: 32-bit floating-point format providing high numerical precision with approximately 7 dec...",
      "full_line": "[^fn-fp32]: **FP32**: 32-bit floating-point format providing high numerical precision with approximately 7 decimal digits of accuracy. Standard for research and training, FP32 operations consume maximum memory and computational resources but ensure numerical stability. Modern GPUs achieve 15-20 TFLOPS in FP32, serving as the baseline for precision comparisons."
    },
    {
      "footnote_id": "fn-fp16",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1313,
      "context": "[^fn-fp16]: **FP16**: 16-bit floating-point format that halves memory usage compared to FP32 while maintaining...",
      "full_line": "[^fn-fp16]: **FP16**: 16-bit floating-point format that halves memory usage compared to FP32 while maintaining reasonable numerical precision. Widely supported by modern AI accelerators, FP16 can achieve 2-4x speedups over FP32 with minimal accuracy loss for most deep learning models, making it the preferred format for inference and mixed-precision training."
    },
    {
      "footnote_id": "fn-int8",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1315,
      "context": "[^fn-int8]: **INT8**: 8-bit integer format providing maximum memory and computational efficiency, requiring on...",
      "full_line": "[^fn-int8]: **INT8**: 8-bit integer format providing maximum memory and computational efficiency, requiring only 25% of FP32 storage. Post-training quantization to INT8 can achieve 4x memory reduction and 2-4x speedup on specialized hardware, but requires careful calibration to minimize accuracy degradation, typically maintaining 95-99% of original model performance."
    },
    {
      "footnote_id": "fn-model-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1317,
      "context": "[^fn-model-compression]: **Model Compression**: Techniques to reduce model size and computational requirements including qu...",
      "full_line": "[^fn-model-compression]: **Model Compression**: Techniques to reduce model size and computational requirements including quantization (reducing numerical precision), pruning (removing unnecessary parameters), knowledge distillation (training smaller models to mimic larger ones), and tensor decomposition. These methods can achieve 10-100x size reduction while maintaining 90-99% of original accuracy."
    },
    {
      "footnote_id": "fn-serverless-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1327,
      "context": "...emand when needed. This can introduce significant delays, particularly in serverless AI environments[^fn-serverless-ai], where resources are allocated dynamically based on incoming requests. Cold-start performance measu...",
      "full_line": "Once memory requirements are optimized, cold-start performance becomes critical for ensuring inference systems are ready to respond quickly upon deployment. In many deployment scenarios, models are not always kept in memory but instead loaded on demand when needed. This can introduce significant delays, particularly in serverless AI environments[^fn-serverless-ai], where resources are allocated dynamically based on incoming requests. Cold-start performance measures how quickly a system can transition from idle to active execution, ensuring that inference is available without excessive wait times."
    },
    {
      "footnote_id": "fn-serverless-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1329,
      "context": "[^fn-serverless-ai]: **Serverless AI**: Cloud computing paradigm where ML models are deployed as functions that automat...",
      "full_line": "[^fn-serverless-ai]: **Serverless AI**: Cloud computing paradigm where ML models are deployed as functions that automatically scale from zero to handle incoming requests, with users paying only for actual inference time. Popular platforms like AWS Lambda, Google Cloud Functions, and Azure Functions support serverless AI, but cold-start latencies of 1-10 seconds for large models can impact user experience compared to always-on deployments."
    },
    {
      "footnote_id": "fn-hardware-lottery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 2111,
      "context": "A critical issue in benchmarking is what has been described as the hardware lottery[^fn-hardware-lottery], a concept introduced by [@hooker2021hardware]. The success of a machine learning model is often di...",
      "full_line": "A critical issue in benchmarking is what has been described as the hardware lottery[^fn-hardware-lottery], a concept introduced by [@hooker2021hardware]. The success of a machine learning model is often dictated not only by its architecture and training data but also by how well it aligns with the underlying hardware used for inference. Some models perform exceptionally well, not because they are inherently better, but because they are optimized for the parallel processing capabilities of GPUs or TPUs. Meanwhile, other promising architectures may be overlooked because they do not map efficiently to dominant hardware platforms."
    },
    {
      "footnote_id": "fn-hardware-lottery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 2113,
      "context": "[^fn-hardware-lottery]: **Hardware Lottery**: The phenomenon where algorithmic progress is heavily influenced by which app...",
      "full_line": "[^fn-hardware-lottery]: **Hardware Lottery**: The phenomenon where algorithmic progress is heavily influenced by which approaches happen to align well with available hardware. For example, the Transformer architecture succeeded partly because its matrix multiplication operations perfectly match GPU capabilities, while equally valid architectures like graph neural networks remain underexplored due to poor GPU mapping. This suggests some \"breakthrough\" algorithms may simply be hardware-compatible rather than fundamentally superior."
    },
    {
      "footnote_id": "fn-pruning-sparsity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 73,
      "context": "...r small weights, minimizing impact on loss landscape. This technique typically achieves 90% sparsity[^fn-pruning-sparsity] with less than 1% accuracy degradation, reducing memory footprints dramatically.",
      "full_line": "**Magnitude-Based Pruning** removes weights with smallest absolute values, leveraging the approximation that small weights contribute minimally to loss gradients. The mathematical foundation approximates second-order Taylor expansion for loss sensitivity: for weight w_i, the criterion ranks by |w_i| and removes the bottom p% globally or per-layer, based on the principle that \u2207L/\u2207w_i \u2248 0 for small weights, minimizing impact on loss landscape. This technique typically achieves 90% sparsity[^fn-pruning-sparsity] with less than 1% accuracy degradation, reducing memory footprints dramatically."
    },
    {
      "footnote_id": "fn-gpt3-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 127,
      "context": "...s grow exponentially with model scale. Training GPT-3 consumed an estimated 1,287 MWh of electricity[^fn-gpt3-energy], enough to power 120 American homes for a year. Carbon emissions from AI training threaten climate...",
      "full_line": "@sec-sustainable-ai shows that sustainability challenges grow exponentially with model scale. Training GPT-3 consumed an estimated 1,287 MWh of electricity[^fn-gpt3-energy], enough to power 120 American homes for a year. Carbon emissions from AI training threaten climate goals without immediate intervention."
    },
    {
      "footnote_id": "fn-chatgpt-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 163,
      "context": "ChatGPT's unprecedented adoption[^fn-chatgpt-adoption] validates the compound AI systems approach. These capabilities result from systematic integration:...",
      "full_line": "ChatGPT's unprecedented adoption[^fn-chatgpt-adoption] validates the compound AI systems approach. These capabilities result from systematic integration: transformer architectures from @sec-dnn-architectures scaled through distributed training (@sec-ai-training), optimized via techniques from @sec-model-optimizations, and deployed through operational infrastructure from @sec-ml-operations."
    },
    {
      "footnote_id": "fn-chatgpt-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 165,
      "context": "[^fn-chatgpt-adoption]: ChatGPT reached 100 million monthly active users in January 2023, just 2 months after launch, repr...",
      "full_line": "[^fn-chatgpt-adoption]: ChatGPT reached 100 million monthly active users in January 2023, just 2 months after launch, representing the fastest consumer application adoption in history. This growth required rapid scaling from hundreds to tens of thousands of GPUs."
    },
    {
      "footnote_id": "fn-pruning-sparsity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 167,
      "context": "[^fn-pruning-sparsity]: Modern neural network pruning achieves these high sparsity ratios by exploiting the overparameteri...",
      "full_line": "[^fn-pruning-sparsity]: Modern neural network pruning achieves these high sparsity ratios by exploiting the overparameterization of deep networks. The lottery ticket hypothesis suggests that dense networks contain sparse subnetworks that can achieve comparable accuracy when trained in isolation."
    },
    {
      "footnote_id": "fn-gpt3-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 169,
      "context": "[^fn-gpt3-energy]: Energy estimates for GPT-3 training vary widely based on hardware efficiency assumptions and train...",
      "full_line": "[^fn-gpt3-energy]: Energy estimates for GPT-3 training vary widely based on hardware efficiency assumptions and training duration. The 1,287 MWh figure represents conservative estimates from multiple sources and includes only direct training costs, not data preprocessing or model development iterations."
    },
    {
      "footnote_id": "fn-tinyml-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 171,
      "context": "[^fn-tinyml-constraints]: TinyML operates under constraints 1000x more severe than mobile AI. For comparison, a modern smart...",
      "full_line": "[^fn-tinyml-constraints]: TinyML operates under constraints 1000x more severe than mobile AI. For comparison, a modern smartphone AI accelerator may consume 1-5W during peak inference, while TinyML devices target sustained operation at 0.01W or less to achieve multi-month battery life in sensor applications."
    },
    {
      "footnote_id": "fn-tinyml-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 214,
      "context": "...te within <1MB memory footprints and <10mW average power while achieving months of battery operation[^fn-tinyml-constraints]. **Automotive systems** require <1ms safety-critical decisions across -40\u00b0C to +85\u00b0C temperature ra...",
      "full_line": "**IoT and Embedded Systems** present the most stringent constraints. **TinyML deployments** operate within <1MB memory footprints and <10mW average power while achieving months of battery operation[^fn-tinyml-constraints]. **Automotive systems** require <1ms safety-critical decisions across -40\u00b0C to +85\u00b0C temperature ranges with >10-year system lifetime reliability. **Industrial IoT** devices manage <10mW power budgets with intermittent connectivity, requiring ultra-efficient inference and robust edge-cloud synchronization."
    },
    {
      "footnote_id": "fn-watson-health",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 57,
      "context": "...ons revealed systematic data quality failures as root causes that cascaded through the entire system[^fn-watson-health]. Quality encompasses accuracy, completeness, consistency, and fitness for the intended ML task. Hig...",
      "full_line": "**Quality Foundation**: Data quality determines system success more than algorithmic sophistication. The concept of \"Data Cascades,\" introduced by @sambasivan2021everyone, demonstrates how quality issues compound throughout the ML lifecycle. When IBM Watson Health's oncology recommendations came under scrutiny in 2019 for producing potentially unsafe cancer treatment recommendations [@strickland2019ibm], investigations revealed systematic data quality failures as root causes that cascaded through the entire system[^fn-watson-health]. Quality encompasses accuracy, completeness, consistency, and fitness for the intended ML task. High-quality data is essential for model success, with the mathematical foundations of this relationship explored in @sec-dl-primer and @sec-dnn-architectures."
    },
    {
      "footnote_id": "fn-watson-health",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 59,
      "context": "[^fn-watson-health]: **IBM Watson Health**: IBM's AI health initiative, which accumulated approximately $4 billion in i...",
      "full_line": "[^fn-watson-health]: **IBM Watson Health**: IBM's AI health initiative, which accumulated approximately $4 billion in investment over its lifetime, was sold to Francisco Partners in 2022 after failing to deliver promised breakthroughs due to core data quality issues and over-hyped capabilities."
    },
    {
      "footnote_id": "fn-data-quality-stats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 71,
      "context": "...ustrates, data scientists spend up to 60% of their time on data preparation tasks [@kaggle2021state][^fn-data-quality-stats]. This statistic reflects the current state where data engineering practices are often ad-hoc rather...",
      "full_line": "As @fig-ds-time illustrates, data scientists spend up to 60% of their time on data preparation tasks [@kaggle2021state][^fn-data-quality-stats]. This statistic reflects the current state where data engineering practices are often ad-hoc rather than systematic. By applying the four-pillar framework consistently, teams can reduce this overhead while building more reliable and maintainable systems."
    },
    {
      "footnote_id": "fn-data-quality-stats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 73,
      "context": "[^fn-data-quality-stats]: **Data Quality Reality**: The famous \"garbage in, garbage out\" principle was first coined by IBM c...",
      "full_line": "[^fn-data-quality-stats]: **Data Quality Reality**: The famous \"garbage in, garbage out\" principle was first coined by IBM computer programmer George Fuechsel in the 1960s, describing how flawed input data produces nonsense output. This principle remains critically relevant in modern ML systems."
    },
    {
      "footnote_id": "fn-kaggle",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 426,
      "context": "Platforms like [Kaggle](https://www.kaggle.com/)[^fn-kaggle] and [UCI Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci-repository] provide ML...",
      "full_line": "Platforms like [Kaggle](https://www.kaggle.com/)[^fn-kaggle] and [UCI Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci-repository] provide ML practitioners with ready-to-use datasets that can jumpstart system development. These pre-existing datasets are particularly valuable when building ML systems as they offer immediate access to cleaned, formatted data with established benchmarks. One of their primary advantages is cost efficiency, as creating datasets from scratch requires significant time and resources, especially when building production ML systems that need large amounts of high-quality training data."
    },
    {
      "footnote_id": "fn-uci-repository",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 426,
      "context": "...s://www.kaggle.com/)[^fn-kaggle] and [UCI Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci-repository] provide ML practitioners with ready-to-use datasets that can jumpstart system development. These pr...",
      "full_line": "Platforms like [Kaggle](https://www.kaggle.com/)[^fn-kaggle] and [UCI Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci-repository] provide ML practitioners with ready-to-use datasets that can jumpstart system development. These pre-existing datasets are particularly valuable when building ML systems as they offer immediate access to cleaned, formatted data with established benchmarks. One of their primary advantages is cost efficiency, as creating datasets from scratch requires significant time and resources, especially when building production ML systems that need large amounts of high-quality training data."
    },
    {
      "footnote_id": "fn-kaggle",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 428,
      "context": "[^fn-kaggle]: **Kaggle**: Founded in 2010 and acquired by Google in 2017, Kaggle hosts over 80,000 public datase...",
      "full_line": "[^fn-kaggle]: **Kaggle**: Founded in 2010 and acquired by Google in 2017, Kaggle hosts over 80,000 public datasets and serves 15+ million registered users globally. Its competition platform has driven major ML breakthroughs, including the $1 million Netflix Prize that advanced collaborative filtering algorithms."
    },
    {
      "footnote_id": "fn-uci-repository",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 430,
      "context": "[^fn-uci-repository]: **UCI ML Repository**: Established in 1987 by UC Irvine's Machine Learning Group, it's one of the...",
      "full_line": "[^fn-uci-repository]: **UCI ML Repository**: Established in 1987 by UC Irvine's Machine Learning Group, it's one of the oldest and most cited ML dataset repositories. Contains 600+ datasets including classics like Iris (1936) and Wine (1991) that shaped decades of ML research and education."
    },
    {
      "footnote_id": "fn-reproducibility-crisis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 436,
      "context": "...here is a crisis around improving reproducibility in machine learning systems [@pineau2021improving][^fn-reproducibility-crisis]. When other researchers have access to the same data, they can validate findings, test new hypothes...",
      "full_line": "Supporting documentation accompanying existing datasets is invaluable, yet is often only present in widely-used datasets. Good documentation provides insights into the data collection process and variable definitions and sometimes even offers baseline model performances. This information not only aids understanding but also promotes reproducibility in research, a cornerstone of scientific integrity; currently, there is a crisis around improving reproducibility in machine learning systems [@pineau2021improving][^fn-reproducibility-crisis]. When other researchers have access to the same data, they can validate findings, test new hypotheses, or apply different methodologies, thus allowing us to build on each other's work more rapidly."
    },
    {
      "footnote_id": "fn-reproducibility-crisis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 438,
      "context": "[^fn-reproducibility-crisis]: **ML Reproducibility Crisis**: Only 15% of ML papers include code, and fewer than 6% provide compl...",
      "full_line": "[^fn-reproducibility-crisis]: **ML Reproducibility Crisis**: Only 15% of ML papers include code, and fewer than 6% provide complete reproducible implementations. NeurIPS 2019 introduced mandatory reproducibility checklists, while venues like MLSys require artifact evaluation to address this systemic problem."
    },
    {
      "footnote_id": "fn-open-images",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 503,
      "context": "...tps://www.image-net.org/) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html)[^fn-open-images] were built through systematic web scraping, significantly advancing the field of computer vision.",
      "full_line": "Web scraping has proven particularly valuable for building large-scale ML systems when human-labeled data is scarce. Consider computer vision systems: major datasets like [ImageNet](https://www.image-net.org/) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html)[^fn-open-images] were built through systematic web scraping, significantly advancing the field of computer vision."
    },
    {
      "footnote_id": "fn-open-images",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 505,
      "context": "[^fn-open-images]: **Open Images Dataset**: Google's Open Images V7 contains 9 million images with 16 million boundin...",
      "full_line": "[^fn-open-images]: **Open Images Dataset**: Google's Open Images V7 contains 9 million images with 16 million bounding boxes across 600 object classes. Released in 2016 and continuously updated, it became the largest publicly available dataset for object detection, enabling breakthrough research in computer vision. In production environments, companies regularly scrape e-commerce sites to gather product images for recognition systems or social media platforms for computer vision applications. Stanford's LabelMe project demonstrated this approach's potential early on, scraping Flickr to create a diverse dataset of over 63,000 annotated images."
    },
    {
      "footnote_id": "fn-google-crowdsource",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 525,
      "context": "...ther example of crowdsourcing's potential is Google's [Crowdsource](https://crowdsource.google.com/)[^fn-google-crowdsource], a platform where volunteers contribute labeled data to improve AI systems in applications like lan...",
      "full_line": "Another example of crowdsourcing's potential is Google's [Crowdsource](https://crowdsource.google.com/)[^fn-google-crowdsource], a platform where volunteers contribute labeled data to improve AI systems in applications like language translation, handwriting recognition, and image understanding."
    },
    {
      "footnote_id": "fn-google-crowdsource",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 527,
      "context": "[^fn-google-crowdsource]: **Google Crowdsource**: Launched in 2016, this gamified platform has collected over 4 billion cont...",
      "full_line": "[^fn-google-crowdsource]: **Google Crowdsource**: Launched in 2016, this gamified platform has collected over 4 billion contributions from volunteers in 120+ languages. It powers improvements to Google Translate, Maps, and other AI services while demonstrating sustainable crowdsourcing through community engagement rather than monetary incentives. By gamifying the process and engaging global participants, Google harnesses diverse datasets, particularly for underrepresented languages. This approach not only enhances the quality of AI systems but also empowers communities by enabling their contributions to influence technological development."
    },
    {
      "footnote_id": "fn-waze-crowdsourcing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 529,
      "context": "...eyond traditional dataset annotation. For instance, the navigation app [Waze](https://www.waze.com/)[^fn-waze-crowdsourcing] uses crowdsourced data from its users to provide real-time traffic updates, route suggestions, and...",
      "full_line": "Crowdsourcing has also been instrumental in applications beyond traditional dataset annotation. For instance, the navigation app [Waze](https://www.waze.com/)[^fn-waze-crowdsourcing] uses crowdsourced data from its users to provide real-time traffic updates, route suggestions, and incident reporting."
    },
    {
      "footnote_id": "fn-waze-crowdsourcing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 531,
      "context": "[^fn-waze-crowdsourcing]: **Waze Crowdsourcing Model**: Founded in Israel (2006), Waze processes 25 billion miles of driving...",
      "full_line": "[^fn-waze-crowdsourcing]: **Waze Crowdsourcing Model**: Founded in Israel (2006), Waze processes 25 billion miles of driving data monthly from 140+ million users. Its real-time crowdsourced traffic model influenced Google Maps after Google's $1.3 billion acquisition (2013), demonstrating how user-generated data creates network effects. While this involves dynamic data collection rather than static dataset labeling, it demonstrates how crowdsourcing can generate continuously updated datasets essential for applications like mobile or edge ML systems. These systems often require real-time input to maintain relevance and accuracy in changing environments."
    },
    {
      "footnote_id": "fn-recaptcha-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 535,
      "context": "...ments in data collection. For example, Google's [reCAPTCHA](https://www.google.com/recaptcha/about/)[^fn-recaptcha-evolution] system uses crowdsourcing to verify human users while simultaneously labeling datasets for training...",
      "full_line": "Flexibility is a key benefit of crowdsourcing. Tasks can be adjusted dynamically based on initial results, allowing for iterative improvements in data collection. For example, Google's [reCAPTCHA](https://www.google.com/recaptcha/about/)[^fn-recaptcha-evolution] system uses crowdsourcing to verify human users while simultaneously labeling datasets for training machine learning models."
    },
    {
      "footnote_id": "fn-recaptcha-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 537,
      "context": "[^fn-recaptcha-evolution]: **reCAPTCHA Evolution**: Originally created at Carnegie Mellon in 2007 to digitize books, reCAPTCH...",
      "full_line": "[^fn-recaptcha-evolution]: **reCAPTCHA Evolution**: Originally created at Carnegie Mellon in 2007 to digitize books, reCAPTCHA v1 helped transcribe 13 million books. After Google's 2009 acquisition, v2 (2014) shifted to image recognition for Street View, while v3 (2018) uses behavioral analysis, processing 1+ billion CAPTCHAs weekly. Users identify objects in images, including street signs and cars, contributing to the training of autonomous systems. This clever integration demonstrates how crowdsourcing can scale effectively when embedded into everyday workflows."
    },
    {
      "footnote_id": "fn-hashing-security",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 561,
      "context": "...redit card numbers can be replaced with asterisks, a fixed set of dummy characters, or hashed values[^fn-hashing-security] to protect sensitive information during display or logging.",
      "full_line": "Masking involves altering or obfuscating sensitive values so that they cannot be directly traced back to the original data subject. For instance, digits in financial account numbers or credit card numbers can be replaced with asterisks, a fixed set of dummy characters, or hashed values[^fn-hashing-security] to protect sensitive information during display or logging."
    },
    {
      "footnote_id": "fn-hashing-security",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 563,
      "context": "[^fn-hashing-security]: **Cryptographic Hashing**: One-way mathematical functions like SHA-256 that transform input data i...",
      "full_line": "[^fn-hashing-security]: **Cryptographic Hashing**: One-way mathematical functions like SHA-256 that transform input data into fixed-length strings. Critical for data anonymization because they're computationally infeasible to reverse\u2014even small input changes produce dramatically different outputs, ensuring original data cannot be recovered from the hash. This anonymization technique is straightforward to implement and understand while clearly protecting identifiable values from being viewed, but may struggle with protecting broader context (e.g. relationships between data points)."
    },
    {
      "footnote_id": "fn-pseudonymization-gdpr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 567,
      "context": "Pseudonymization[^fn-pseudonymization-gdpr] is the process of replacing direct identifiers (like names, Social Security numbers, or email addre...",
      "full_line": "Pseudonymization[^fn-pseudonymization-gdpr] is the process of replacing direct identifiers (like names, Social Security numbers, or email addresses) with artificial identifiers, or \"pseudonyms.\" These pseudonyms must not reveal, or be easily traceable to, the original data subject."
    },
    {
      "footnote_id": "fn-pseudonymization-gdpr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 569,
      "context": "[^fn-pseudonymization-gdpr]: **Pseudonymization under GDPR**: GDPR Article 4(5) formally defines pseudonymization as a privacy-...",
      "full_line": "[^fn-pseudonymization-gdpr]: **Pseudonymization under GDPR**: GDPR Article 4(5) formally defines pseudonymization as a privacy-enhancing technique. Unlike anonymization, pseudonymized data remains personal data under EU law, requiring continued protection but enabling reduced regulatory restrictions for research and analytics purposes. This is commonly used in health records or in any situation where datasets need personal identities removed, but maintain unique entries. This approach allow maintaining individual-level data for analysis (since records can be traced through pseudonyms), while reducing the risk of direct identification. However, if the \"key\" linking the pseudonym to the real identifier is compromised, re-identification becomes possible."
    },
    {
      "footnote_id": "fn-privacy-regulations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 939,
      "context": "...pr.eu/) and [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html)[^fn-privacy-regulations] limit the sharing of sensitive patient information. Synthetic data generation enables the creation...",
      "full_line": "In addition to expanding datasets, synthetic data addresses critical ethical and privacy concerns. Unlike real-world data, synthetic data attempts to not tie back to specific individuals or entities. This makes it especially useful in sensitive domains such as finance, healthcare, or human resources, where data confidentiality is paramount. The ability to preserve statistical properties while removing identifying information allows researchers to maintain high ethical standards without compromising the quality of their models. In healthcare, privacy regulations such as [GDPR](https://gdpr.eu/) and [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html)[^fn-privacy-regulations] limit the sharing of sensitive patient information. Synthetic data generation enables the creation of realistic yet anonymized datasets that can be used for training diagnostic models without compromising patient privacy."
    },
    {
      "footnote_id": "fn-privacy-regulations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 941,
      "context": "[^fn-privacy-regulations]: **Privacy Regulation Timeline**: GDPR (2016, effective 2018) imposed maximum fines of \u20ac20 million...",
      "full_line": "[^fn-privacy-regulations]: **Privacy Regulation Timeline**: GDPR (2016, effective 2018) imposed maximum fines of \u20ac20 million or 4% of annual global turnover (whichever is higher) for violations, followed by California's CCPA (2018), and now dozens of similar laws globally. These regulations significantly transformed how ML systems handle personal data, making privacy-by-design essential for any AI system."
    },
    {
      "footnote_id": "fn-mechanical-turk",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 955,
      "context": "...e word samples across different demographics and environments. Platforms like Amazon Mechanical Turk[^fn-mechanical-turk] can engage contributors to record wake words in various accents, speaking styles, and background co...",
      "full_line": "Crowdsourcing becomes valuable for collecting specific wake word samples across different demographics and environments. Platforms like Amazon Mechanical Turk[^fn-mechanical-turk] can engage contributors to record wake words in various accents, speaking styles, and background conditions. This approach is particularly useful for gathering data for underrepresented languages or specific acoustic environments."
    },
    {
      "footnote_id": "fn-mechanical-turk",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 957,
      "context": "[^fn-mechanical-turk]: **Mechanical Turk Origins**: Named after the 18th-century chess-playing \"automaton\" (actually a hu...",
      "full_line": "[^fn-mechanical-turk]: **Mechanical Turk Origins**: Named after the 18th-century chess-playing \"automaton\" (actually a human chess master hidden inside), Amazon's MTurk (2005) pioneered human-in-the-loop AI by enabling distributed human computation at scale\u2014ironically reversing the original Turk's deception of AI capabilities."
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 971,
      "context": "Batch ingestion[^fn-batch-processing] involves collecting data in groups or batches over a specified period before processing. This metho...",
      "full_line": "Batch ingestion[^fn-batch-processing] involves collecting data in groups or batches over a specified period before processing. This method is appropriate when real-time data processing is not critical and data can be processed at scheduled intervals. It's also useful for loading large volumes of historical data. For example, a retail company might use batch ingestion to process daily sales data overnight, updating their ML models for inventory prediction each morning [@akidau2015dataflow]."
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 973,
      "context": "[^fn-batch-processing]: **Batch Processing Evolution**: Batch processing dates back to IBM mainframes in the 1950s but was...",
      "full_line": "[^fn-batch-processing]: **Batch Processing Evolution**: Batch processing dates back to IBM mainframes in the 1950s but was revolutionized by Google's MapReduce (2004), which enabled distributed batch processing across thousands of machines. This paradigm shift made \"big data\" analytics economically feasible for the first time."
    },
    {
      "footnote_id": "fn-etl-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1180,
      "context": "ETL[^fn-etl-history] is a well-established paradigm in which data is first gathered from a source, then transformed to m...",
      "full_line": "ETL[^fn-etl-history] is a well-established paradigm in which data is first gathered from a source, then transformed to match the target schema or model, and finally loaded into a data warehouse or other repository. This approach typically results in data being stored in a ready-to-query format, which can be advantageous for ML systems that require consistent, pre-processed data. For instance, an ML system predicting customer churn might use ETL to standardize and aggregate customer interaction data from multiple sources before loading it into a format suitable for model training [@inmon2005building]."
    },
    {
      "footnote_id": "fn-etl-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1182,
      "context": "[^fn-etl-history]: **ETL Evolution**: ETL emerged in the 1970s with early data warehouses but was revolutionized by A...",
      "full_line": "[^fn-etl-history]: **ETL Evolution**: ETL emerged in the 1970s with early data warehouses but was revolutionized by Apache Spark in 2009, which enabled distributed data processing at unprecedented scale. Spark's in-memory computing made ETL pipelines 100x faster than traditional MapReduce approaches."
    },
    {
      "footnote_id": "fn-etl-vs-elt",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1265,
      "context": "In traditional ETL workflows[^fn-etl-vs-elt], much of the data processing occurs before the data is loaded into the target system. This approach...",
      "full_line": "In traditional ETL workflows[^fn-etl-vs-elt], much of the data processing occurs before the data is loaded into the target system. This approach front-loads the cleaning, transformation, and feature engineering steps, ensuring that data is in a ready-to-use state when it reaches the data warehouse or ML pipeline. ETL is often preferred when dealing with structured data or when there's a need for significant data cleansing before analysis."
    },
    {
      "footnote_id": "fn-etl-vs-elt",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1267,
      "context": "[^fn-etl-vs-elt]: **ETL vs ELT Performance**: ETL processes 1-10GB/hour on traditional systems but scales poorly; EL...",
      "full_line": "[^fn-etl-vs-elt]: **ETL vs ELT Performance**: ETL processes 1-10GB/hour on traditional systems but scales poorly; ELT leverages cloud warehouses like Snowflake (100GB/hour+) and BigQuery (1TB/hour+) by utilizing distributed compute for transformations. This 10-100x performance difference drives modern data architecture decisions."
    },
    {
      "footnote_id": "fn-normalization-techniques",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1297,
      "context": "Common transformation tasks include normalization and standardization[^fn-normalization-techniques], which scale numerical features to a common range or distribution. For example, in a housing price...",
      "full_line": "Common transformation tasks include normalization and standardization[^fn-normalization-techniques], which scale numerical features to a common range or distribution. For example, in a housing price prediction model, features like square footage and number of rooms might be on vastly different scales. Normalizing these features ensures that they contribute more equally to the model's predictions [@bishop2006pattern]. However, transformation consistency between training and serving becomes critical here\u2014if training data is normalized using the global dataset mean and standard deviation, the serving pipeline must store and apply these exact same statistics. Computing normalization parameters on serving data batches leads to distribution shift and degraded model performance."
    },
    {
      "footnote_id": "fn-normalization-techniques",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1299,
      "context": "[^fn-normalization-techniques]: **Normalization in ML**: Min-max scaling (range 0-1) vs z-score standardization (mean=0, std=1) ca...",
      "full_line": "[^fn-normalization-techniques]: **Normalization in ML**: Min-max scaling (range 0-1) vs z-score standardization (mean=0, std=1) can dramatically affect model performance. Gradient descent converges 10-100x faster on normalized features [@goodfellow2016deep], while tree-based models (Random Forest, XGBoost) are largely unaffected by scaling differences."
    },
    {
      "footnote_id": "fn-categorical-encoding",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1301,
      "context": "...variables, handling date and time data, or creating derived features. For instance, one-hot encoding[^fn-categorical-encoding] is often used to convert categorical variables into a format that can be readily understood by many...",
      "full_line": "Other transformations might involve encoding categorical variables, handling date and time data, or creating derived features. For instance, one-hot encoding[^fn-categorical-encoding] is often used to convert categorical variables into a format that can be readily understood by many machine learning algorithms. Training-serving consistency requires careful handling of categorical encodings\u2014if the training dataset encounters categories A, B, and C, the serving pipeline must handle the same set, including unknown categories that weren't present during training."
    },
    {
      "footnote_id": "fn-categorical-encoding",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1303,
      "context": "[^fn-categorical-encoding]: **Categorical Encoding Impact**: One-hot encoding can explode feature dimensions\u2014a single categori...",
      "full_line": "[^fn-categorical-encoding]: **Categorical Encoding Impact**: One-hot encoding can explode feature dimensions\u2014a single categorical variable with 1000 categories creates 1000 binary features. High-cardinality encoding techniques like target encoding or embeddings (used in deep learning) can reduce dimensions by 10-100x while preserving predictive power."
    },
    {
      "footnote_id": "fn-rfm-analysis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1309,
      "context": "...that capture the recency, frequency, and monetary value of customer purchases, known as RFM analysis[^fn-rfm-analysis] [@kuhn2013applied].",
      "full_line": "Feature engineering might involve combining existing features, extracting information from complex data types, or creating entirely new features based on domain insights. For example, in a retail recommendation system, engineers might create features that capture the recency, frequency, and monetary value of customer purchases, known as RFM analysis[^fn-rfm-analysis] [@kuhn2013applied]."
    },
    {
      "footnote_id": "fn-rfm-analysis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1311,
      "context": "[^fn-rfm-analysis]: **RFM Analysis Origins**: Developed by direct marketers in the 1960s, RFM (Recency, Frequency, Mon...",
      "full_line": "[^fn-rfm-analysis]: **RFM Analysis Origins**: Developed by direct marketers in the 1960s, RFM (Recency, Frequency, Monetary) analysis segments customers by purchase behavior. Modern ML systems extend this to 100+ behavioral features, but the core RFM triplet remains among the most predictive features for customer lifetime value models."
    },
    {
      "footnote_id": "fn-apache-beam",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1319,
      "context": "...provide capabilities for building and managing data processing pipelines. For instance, Apache Beam[^fn-apache-beam] and TensorFlow Transform allow developers to define data processing steps that can be applied consi...",
      "full_line": "Modern ML frameworks and tools often provide capabilities for building and managing data processing pipelines. For instance, Apache Beam[^fn-apache-beam] and TensorFlow Transform allow developers to define data processing steps that can be applied consistently during both model training and serving. The choice of data processing framework must align with the broader ML framework ecosystem discussed in @sec-ai-frameworks, where framework-specific data loaders and preprocessing utilities can significantly impact development velocity and system performance."
    },
    {
      "footnote_id": "fn-apache-beam",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1321,
      "context": "[^fn-apache-beam]: **Apache Beam Architecture**: Google's unified programming model (2016) abstracts batch and stream...",
      "full_line": "[^fn-apache-beam]: **Apache Beam Architecture**: Google's unified programming model (2016) abstracts batch and streaming data processing across multiple execution engines (Dataflow, Spark, Flink). Its key innovation: write once, run anywhere with automatic scaling from single machines to thousands of workers processing petabyte-scale data."
    },
    {
      "footnote_id": "fn-medical-imaging",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1492,
      "context": "...ults but also raises considerable system design challenges. For instance, in medical imaging systems[^fn-medical-imaging], experienced radiologists offer essential annotations. Such systems necessitate specialized interfa...",
      "full_line": "Manual labeling by experts is the primary approach in many annotation pipelines. This method produces high-quality results but also raises considerable system design challenges. For instance, in medical imaging systems[^fn-medical-imaging], experienced radiologists offer essential annotations. Such systems necessitate specialized interfaces for accurate labeling, secure data access controls to protect patient privacy, and reliable version control mechanisms to monitor annotation revisions."
    },
    {
      "footnote_id": "fn-medical-imaging",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1494,
      "context": "[^fn-medical-imaging]: **Medical Imaging AI Revolution**: The 2012 AlexNet breakthrough began with ImageNet's 14 million...",
      "full_line": "[^fn-medical-imaging]: **Medical Imaging AI Revolution**: The 2012 AlexNet breakthrough began with ImageNet's 14 million labeled images, but medical AI required specialized datasets. The NIH Clinical Center released 112,120 chest X-ray images in 2017, becoming one of the largest public medical imaging datasets for ML research. Despite the dependable outcomes of expert labeling, the scarcity and high expenses of specialists render it challenging to implement on a large scale for extensive datasets."
    },
    {
      "footnote_id": "fn-concept-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1648,
      "context": "...evant as the underlying distribution of data changes over time. This concept, known as concept drift[^fn-concept-drift], necessitates ongoing labeling efforts and periodic re-evaluation of existing labels.",
      "full_line": "The dynamic nature of real-world data presents another limitation. Labels that are accurate at the time of annotation may become outdated or irrelevant as the underlying distribution of data changes over time. This concept, known as concept drift[^fn-concept-drift], necessitates ongoing labeling efforts and periodic re-evaluation of existing labels."
    },
    {
      "footnote_id": "fn-concept-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1650,
      "context": "[^fn-concept-drift]: **Concept Drift Challenge**: First formalized by Schlimmer & Granger (1986), concept drift became...",
      "full_line": "[^fn-concept-drift]: **Concept Drift Challenge**: First formalized by Schlimmer & Granger (1986), concept drift became critical in ML systems deployment. Amazon's recommendation algorithms must continuously adapt as user preferences shift, while spam detection systems face adversarial concept drift as spammers evolve their tactics."
    },
    {
      "footnote_id": "fn-iot-data-volume",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1714,
      "context": "...ging data formats, particularly if the data originates from web scraping or Internet of Things (IoT)[^fn-iot-data-volume] sensors.",
      "full_line": "Data warehouses, by contrast, are optimized for analytical queries across integrated datasets that have been transformed into a standardized schema. As indicated in the table, they handle large volumes of integrated data. Many ML systems successfully draw on data warehouses to power model training because the structured environment simplifies data exploration and feature engineering. Yet one limitation remains: a data warehouse may not accommodate truly unstructured data or rapidly changing data formats, particularly if the data originates from web scraping or Internet of Things (IoT)[^fn-iot-data-volume] sensors."
    },
    {
      "footnote_id": "fn-iot-data-volume",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1716,
      "context": "[^fn-iot-data-volume]: **IoT Data Explosion**: IDC predicts 41.6 billion IoT devices will generate 79.4 zettabytes of dat...",
      "full_line": "[^fn-iot-data-volume]: **IoT Data Explosion**: IDC predicts 41.6 billion IoT devices will generate 79.4 zettabytes of data by 2025\u2014that's 79.4 trillion gigabytes. A single autonomous vehicle generates 4TB/day, while smart city sensors produce 2.5 quintillion bytes daily. Traditional data warehouses struggle with this velocity and variety."
    },
    {
      "footnote_id": "fn-data-lake-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1718,
      "context": "Data lakes[^fn-data-lake-origins] address this gap by storing structured, semi-structured, and unstructured data in its native format...",
      "full_line": "Data lakes[^fn-data-lake-origins] address this gap by storing structured, semi-structured, and unstructured data in its native format, deferring schema definitions until the point of reading or analysis (sometimes called _schema-on-read_). As @tbl-storage shows, data lakes can handle large volumes of diverse data types. This approach grants data scientists tremendous latitude when dealing with experimental use cases or novel data types. However, data lakes also demand careful cataloging and metadata management. Without sufficient governance, these expansive repositories risk devolving into unsearchable, disorganized silos."
    },
    {
      "footnote_id": "fn-data-lake-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1720,
      "context": "[^fn-data-lake-origins]: **Data Lake Origins**: The term \"data lake\" was coined by Pentaho CTO James Dixon in 2010 to contr...",
      "full_line": "[^fn-data-lake-origins]: **Data Lake Origins**: The term \"data lake\" was coined by Pentaho CTO James Dixon in 2010 to contrast with data warehouses, comparing them to \"a data swamp\" if poorly managed. The concept emerged from Hadoop's ability to store vast amounts of unstructured data cheaply."
    },
    {
      "footnote_id": "fn-mysql-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1722,
      "context": "...age illustrate the range of technologies available for each storage system type. For instance, MySQL[^fn-mysql-scale] represents a traditional database system, while solutions like Google BigQuery[^fn-bigquery-perform...",
      "full_line": "The examples provided in @tbl-storage illustrate the range of technologies available for each storage system type. For instance, MySQL[^fn-mysql-scale] represents a traditional database system, while solutions like Google BigQuery[^fn-bigquery-performance] and Amazon Redshift are examples of modern, cloud-based data warehouses."
    },
    {
      "footnote_id": "fn-bigquery-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1722,
      "context": "...ySQL[^fn-mysql-scale] represents a traditional database system, while solutions like Google BigQuery[^fn-bigquery-performance] and Amazon Redshift are examples of modern, cloud-based data warehouses.",
      "full_line": "The examples provided in @tbl-storage illustrate the range of technologies available for each storage system type. For instance, MySQL[^fn-mysql-scale] represents a traditional database system, while solutions like Google BigQuery[^fn-bigquery-performance] and Amazon Redshift are examples of modern, cloud-based data warehouses."
    },
    {
      "footnote_id": "fn-mysql-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1724,
      "context": "[^fn-mysql-scale]: **MySQL at Scale**: Originally developed by MySQL AB in 1995, MySQL powers 39% of all websites inc...",
      "full_line": "[^fn-mysql-scale]: **MySQL at Scale**: Originally developed by MySQL AB in 1995, MySQL powers 39% of all websites including Facebook, Twitter, and YouTube. However, single MySQL instances typically max out at 10-50TB before requiring complex sharding strategies that make ML feature extraction significantly more complex."
    },
    {
      "footnote_id": "fn-bigquery-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1726,
      "context": "[^fn-bigquery-performance]: **BigQuery Serverless Power**: Google BigQuery can scan petabytes in seconds using thousands of pa...",
      "full_line": "[^fn-bigquery-performance]: **BigQuery Serverless Power**: Google BigQuery can scan petabytes in seconds using thousands of parallel workers. Its columnar storage and automatic query optimization enables ML feature extraction from trillion-row tables in minutes\u2014performance impossible with traditional databases at any scale. For data lakes, cloud storage solutions such as Google Cloud Storage, AWS S3, and Azure Data Lake Storage are commonly used due to their scalability and flexibility."
    },
    {
      "footnote_id": "fn-model-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1744,
      "context": "...models can have millions or billions of numerical values that need to be stored. For instance, GPT-3[^fn-model-scaling], a large language model, requires approximately 350 GB of storage just for the model weights [@brow...",
      "full_line": "One of the primary challenges in ML storage is handling large model weights. Modern ML models can have millions or billions of numerical values that need to be stored. For instance, GPT-3[^fn-model-scaling], a large language model, requires approximately 350 GB of storage just for the model weights [@brown2020language]. Storage systems need to be capable of handling these large, often dense, numerical arrays efficiently, both in terms of storage capacity and access speed."
    },
    {
      "footnote_id": "fn-model-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1746,
      "context": "[^fn-model-scaling]: **Model Scaling Explosion**: From AlexNet's 60 million parameters [@krizhevsky2012imagenet] (2012)...",
      "full_line": "[^fn-model-scaling]: **Model Scaling Explosion**: From AlexNet's 60 million parameters [@krizhevsky2012imagenet] (2012) to GPT-3's 175 billion [@brown2020language] (2020), model size grew 3,000x in 8 years. GPT-4's rumored 1.7 trillion parameters would require 3.5 TB of storage\u2014equivalent to 1,000 DVDs worth of model weights alone. This requirement goes beyond traditional data storage and enters the realm of high-performance computing storage solutions."
    },
    {
      "footnote_id": "fn-columnar-formats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1779,
      "context": "...choice of file format can significantly impact both throughput and latency. Columnar storage formats[^fn-columnar-formats] such as Parquet or ORC are particularly well-suited for ML workloads, delivering 5-10x I/O reductio...",
      "full_line": "The choice of file format can significantly impact both throughput and latency. Columnar storage formats[^fn-columnar-formats] such as Parquet or ORC are particularly well-suited for ML workloads, delivering 5-10x I/O reduction compared to row-based formats like CSV or JSON. These formats allow for efficient retrieval of specific features without reading entire records, substantially reducing I/O operations and speeding up data loading for model training and inference. Consider a fraud detection dataset with 100 columns where models typically use only 20 features\u2014columnar formats read only the needed columns, achieving 80% I/O reduction. Combined with column-level compression (often 20-50x for categorical features), columnar formats can achieve total I/O reduction of 20-100x compared to uncompressed row formats. This dramatic improvement directly translates to faster training iterations and reduced infrastructure costs, important for large-scale ML systems processing terabytes of data daily."
    },
    {
      "footnote_id": "fn-columnar-formats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1781,
      "context": "[^fn-columnar-formats]: **Columnar Format Revolution**: Columnar storage was pioneered by C-Store [@stonebraker2005cstore]...",
      "full_line": "[^fn-columnar-formats]: **Columnar Format Revolution**: Columnar storage was pioneered by C-Store [@stonebraker2005cstore] in 2005, leading to Parquet (developed at Twitter in 2013) and ORC (optimized row columnar, created at Hortonworks). These formats revolutionized analytics by enabling 10-100x faster queries for ML feature extraction."
    },
    {
      "footnote_id": "fn-snappy-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1783,
      "context": "...ads, fast decompression is usually prioritized over maximum compression, with algorithms like Snappy[^fn-snappy-compression] or LZ4 being popular choices.",
      "full_line": "Compression is another key factor in storage performance optimization. While compression reduces storage costs and can improve read performance by reducing the amount of data transferred from disk, it also introduces computational overhead for decompression. The choice of compression algorithm often involves a trade-off between compression ratio and decompression speed. For ML workloads, fast decompression is usually prioritized over maximum compression, with algorithms like Snappy[^fn-snappy-compression] or LZ4 being popular choices."
    },
    {
      "footnote_id": "fn-snappy-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1785,
      "context": "[^fn-snappy-compression]: **Snappy Compression Trade-offs**: Developed by Google (2011), Snappy achieves 250MB/s compression...",
      "full_line": "[^fn-snappy-compression]: **Snappy Compression Trade-offs**: Developed by Google (2011), Snappy achieves 250MB/s compression and 500MB/s decompression speeds\u2014roughly 3-4x faster than gzip. While compression ratios are lower (2-3x vs gzip's 6-8x), the speed advantage makes it ideal for ML pipelines where training throughput matters more than storage costs."
    },
    {
      "footnote_id": "fn-hdfs-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1789,
      "context": "...h as [HDFS (Hadoop Distributed File System)](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)[^fn-hdfs-origins] or cloud-based object stores like [Amazon S3](https://aws.amazon.com/s3/), distribute data across m...",
      "full_line": "To handle the scale of data in modern ML systems, distributed storage architectures are often employed. These systems, such as [HDFS (Hadoop Distributed File System)](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)[^fn-hdfs-origins] or cloud-based object stores like [Amazon S3](https://aws.amazon.com/s3/), distribute data across multiple machines or data centers. This approach not only provides scalability but also enables parallel data access, which can substantially improve read performance for large-scale data processing tasks common in ML workflows."
    },
    {
      "footnote_id": "fn-hdfs-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1791,
      "context": "[^fn-hdfs-origins]: **HDFS Origins**: HDFS was inspired by Google's MapReduce paper (2004) and created at Yahoo! in 20...",
      "full_line": "[^fn-hdfs-origins]: **HDFS Origins**: HDFS was inspired by Google's MapReduce paper (2004) and created at Yahoo! in 2006 to handle web-scale data. It enabled the \"big data\" revolution by making petabyte-scale storage affordable using commodity hardware instead of expensive specialized systems."
    },
    {
      "footnote_id": "fn-redis-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1793,
      "context": "...can significantly reduce latency and computational overhead. Distributed caching systems like Redis[^fn-redis-performance] or Memcached are often used to scale caching capabilities across clusters of machines, providing lo...",
      "full_line": "Caching strategies are also vital for optimizing storage performance in ML systems. In-memory caching of frequently accessed data or computed features can significantly reduce latency and computational overhead. Distributed caching systems like Redis[^fn-redis-performance] or Memcached are often used to scale caching capabilities across clusters of machines, providing low-latency access to hot data for distributed training or serving systems."
    },
    {
      "footnote_id": "fn-redis-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1795,
      "context": "[^fn-redis-performance]: **Redis Performance**: Redis achieves sub-millisecond latency with 1M+ operations/second on modest...",
      "full_line": "[^fn-redis-performance]: **Redis Performance**: Redis achieves sub-millisecond latency with 1M+ operations/second on modest hardware. Its in-memory architecture makes it ideal for ML feature serving, with companies like Twitter using Redis clusters to serve 400,000+ timeline requests per second for real-time recommendation systems."
    },
    {
      "footnote_id": "fn-dataeng-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1807,
      "context": "...ol systems like Git excel at tracking code changes, they fall short when dealing with large datasets[^fn-dataeng-data-versioning]. This gap has led to the emergence of specialized tools like [DVC (Data Version Control)](https://d...",
      "full_line": "One of the key challenges at this stage is managing the versions of datasets used in experiments. While traditional version control systems like Git excel at tracking code changes, they fall short when dealing with large datasets[^fn-dataeng-data-versioning]. This gap has led to the emergence of specialized tools like [DVC (Data Version Control)](https://dvc.org/), which enable data scientists to efficiently track dataset changes, revert to previous versions, and share large files without duplication. These tools ensure that teams can maintain reproducibility and transparency throughout the iterative development process."
    },
    {
      "footnote_id": "fn-dataeng-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1809,
      "context": "[^fn-dataeng-data-versioning]: **Data Versioning Challenges**: Git's inability to handle large binary files efficiently led to th...",
      "full_line": "[^fn-dataeng-data-versioning]: **Data Versioning Challenges**: Git's inability to handle large binary files efficiently led to the \"GitHub is not a CDN\" problem. DVC, created in 2017, solved this by treating data like code using content-addressable storage, enabling Git-like workflows for terabyte-scale datasets."
    },
    {
      "footnote_id": "fn-burst-buffers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1825,
      "context": "Complementing these systems is the concept of burst buffers[^fn-burst-buffers], borrowed from high-performance computing. These high-speed, temporary storage layers are particula...",
      "full_line": "Complementing these systems is the concept of burst buffers[^fn-burst-buffers], borrowed from high-performance computing. These high-speed, temporary storage layers are particularly valuable during training, as they can absorb large, bursty I/O operations."
    },
    {
      "footnote_id": "fn-burst-buffers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1827,
      "context": "[^fn-burst-buffers]: **Burst Buffers**: High-speed SSD-based storage layers that buffer data between slower traditional...",
      "full_line": "[^fn-burst-buffers]: **Burst Buffers**: High-speed SSD-based storage layers that buffer data between slower traditional storage and fast compute. Originally developed for supercomputers, they're now critical for ML training where GPUs can demand 100GB/s+ data rates\u2014far exceeding traditional storage capabilities. By buffering these spikes in demand, burst buffers help smooth out performance fluctuations and reduce the load on primary storage systems, ensuring that training pipelines remain efficient and reliable."
    },
    {
      "footnote_id": "fn-feature-stores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1849,
      "context": "Feature stores[^fn-feature-stores] are a centralized repository that stores and serves pre-computed features for machine learning mode...",
      "full_line": "Feature stores[^fn-feature-stores] are a centralized repository that stores and serves pre-computed features for machine learning models, ensuring consistency between training and inference workflows. They have emerged as a critical component in the ML infrastructure stack, addressing the unique challenges of managing and serving features for machine learning models. They act as a central repository for storing, managing, and serving machine learning features, bridging the gap between data engineering and machine learning operations."
    },
    {
      "footnote_id": "fn-feature-stores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1851,
      "context": "[^fn-feature-stores]: **Feature Store Evolution**: Feature stores were pioneered by Uber's Michelangelo platform in 2017...",
      "full_line": "[^fn-feature-stores]: **Feature Store Evolution**: Feature stores were pioneered by Uber's Michelangelo platform in 2017 to solve feature consistency issues at scale. The concept gained widespread adoption after Airbnb open-sourced their Zipline feature store, leading to modern solutions like Feast and Tecton."
    },
    {
      "footnote_id": "fn-data-lineage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 2463,
      "context": "...just as important for transparency and reproducibility in ML systems. Clear records of data lineage[^fn-data-lineage], including how data flows and transforms throughout the ML pipeline, are essential for accountabili...",
      "full_line": "Documentation and metadata management, which are often less discussed, are just as important for transparency and reproducibility in ML systems. Clear records of data lineage[^fn-data-lineage], including how data flows and transforms throughout the ML pipeline, are essential for accountability."
    },
    {
      "footnote_id": "fn-data-lineage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 2465,
      "context": "[^fn-data-lineage]: **Data Lineage Systems**: Track data from source to consumption across complex ML pipelines. Apach...",
      "full_line": "[^fn-data-lineage]: **Data Lineage Systems**: Track data from source to consumption across complex ML pipelines. Apache Atlas (originally Hortonworks, now Apache, 2015) and DataHub (LinkedIn, 2020) enable lineage tracking at enterprise scale. Critical for regulatory compliance\u2014GDPR Article 30 requires detailed records of data processing activities, making lineage essential for demonstrating compliance. Standardized documentation frameworks, such as Data Cards proposed by @pushkarna2022data, offer a structured way to document the characteristics, limitations, and potential biases of datasets."
    },
    {
      "footnote_id": "fn-audit-trails",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 2467,
      "context": "Audit trails[^fn-audit-trails] are another important component of data governance. These detailed logs track data access and usage...",
      "full_line": "Audit trails[^fn-audit-trails] are another important component of data governance. These detailed logs track data access and usage throughout the lifecycle of ML models, from collection to deployment."
    },
    {
      "footnote_id": "fn-audit-trails",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 2469,
      "context": "[^fn-audit-trails]: **ML Audit Requirements**: SOX compliance requires immutable audit logs for financial ML models, w...",
      "full_line": "[^fn-audit-trails]: **ML Audit Requirements**: SOX compliance requires immutable audit logs for financial ML models, while HIPAA mandates detailed access logs for healthcare AI. Modern systems generate terabytes of audit data\u2014Uber's ML platform logs 50+ billion events daily for compliance and debugging purposes. Comprehensive audit trails are invaluable for troubleshooting and accountability, especially in cases of data breaches or unexpected model behavior. They help organizations understand what actions were taken and why, providing a clear path for resolving issues and ensuring compliance."
    },
    {
      "footnote_id": "fn-blockchain-governance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 2473,
      "context": "...f data governance will continue to evolve. Emerging trends, such as blockchain-inspired technologies[^fn-blockchain-governance] for tamper-evident logs and automated governance tools, offer promising solutions for real-time mon...",
      "full_line": "As ML systems grow more complex and influential, the challenges of data governance will continue to evolve. Emerging trends, such as blockchain-inspired technologies[^fn-blockchain-governance] for tamper-evident logs and automated governance tools, offer promising solutions for real-time monitoring and issue detection."
    },
    {
      "footnote_id": "fn-blockchain-governance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 2475,
      "context": "[^fn-blockchain-governance]: **Blockchain for ML Governance**: Immutable ledgers provide tamper-proof audit trails for ML model...",
      "full_line": "[^fn-blockchain-governance]: **Blockchain for ML Governance**: Immutable ledgers provide tamper-proof audit trails for ML model decisions. Ocean Protocol and other projects use blockchain to track data provenance and usage rights. While promising for high-stakes applications like healthcare AI, blockchain's energy costs and complexity limit widespread adoption. By adopting robust data governance practices, including tools like Data Cards, organizations can build ML systems that are transparent, ethical, and trustworthy."
    },
    {
      "footnote_id": "fn-gpu-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 57,
      "context": "...mputations. The scalability of neural networks has driven demand for advanced hardware, such as GPUs[^fn-gpu-parallel], that can efficiently process large models and datasets.",
      "full_line": "* **Computation**: From simple, sequential operations to massively parallel computations. The scalability of neural networks has driven demand for advanced hardware, such as GPUs[^fn-gpu-parallel], that can efficiently process large models and datasets."
    },
    {
      "footnote_id": "fn-gpu-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 59,
      "context": "[^fn-gpu-parallel]: **GPU (Graphics Processing Unit)**: Originally designed for rendering 3D graphics in 1999 by NVIDI...",
      "full_line": "[^fn-gpu-parallel]: **GPU (Graphics Processing Unit)**: Originally designed for rendering 3D graphics in 1999 by NVIDIA, GPUs excel at parallel computation with thousands of simple cores (compared to CPUs' 4-16 complex cores). A modern GPU like the NVIDIA A100 contains 6,912 CUDA cores and can perform 312 TFLOPS for FP16 Tensor operations\u2014roughly 20\u00d7 faster than CPUs for neural network training. This massive parallelism perfectly matches the matrix multiplication operations that dominate deep learning computations."
    },
    {
      "footnote_id": "fn-hog-method",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 178,
      "context": "...ct meaningful patterns from images. The Histogram of Oriented Gradients (HOG) [@dalal2005histograms][^fn-hog-method] method, shown in @fig-hog, exemplifies this approach. HOG works by first identifying edges in an im...",
      "full_line": "Feature engineering involves transforming raw data into representations that make patterns more apparent to learning algorithms. In computer vision, researchers developed sophisticated methods to extract meaningful patterns from images. The Histogram of Oriented Gradients (HOG) [@dalal2005histograms][^fn-hog-method] method, shown in @fig-hog, exemplifies this approach. HOG works by first identifying edges in an image, which are places where brightness changes sharply and often indicate object boundaries. It then divides the image into small cells and measures how edges are oriented within each cell, summarizing these orientations in a histogram. This transformation converts raw pixel values into a representation that captures important shape information while being robust to variations in lighting and small changes in position."
    },
    {
      "footnote_id": "fn-hog-method",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 180,
      "context": "[^fn-hog-method]: **Histogram of Oriented Gradients (HOG)**: Developed by Navneet Dalal and Bill Triggs in 2005, HOG...",
      "full_line": "[^fn-hog-method]: **Histogram of Oriented Gradients (HOG)**: Developed by Navneet Dalal and Bill Triggs in 2005, HOG became the gold standard for object detection before deep learning. It achieved near-perfect accuracy on pedestrian detection\u2014a breakthrough that enabled practical computer vision applications. HOG works by computing gradients (edge directions) in 8\u00d78 pixel cells, then creating histograms of 9 orientation bins. This clever abstraction captures object shape while ignoring texture details, making it robust to lighting changes but requiring expert knowledge to design."
    },
    {
      "footnote_id": "fn-sift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 184,
      "context": "Other feature extraction methods like SIFT [@lowe1999object][^fn-sift] (Scale-Invariant Feature Transform) and Gabor filters provided different ways to capture patterns i...",
      "full_line": "Other feature extraction methods like SIFT [@lowe1999object][^fn-sift] (Scale-Invariant Feature Transform) and Gabor filters provided different ways to capture patterns in images. SIFT found distinctive points that could be recognized even when an object's size or orientation changed. Gabor filters helped identify textures and repeated patterns. Each method encoded different types of human insight about what makes visual patterns recognizable."
    },
    {
      "footnote_id": "fn-sift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 186,
      "context": "[^fn-sift]: **Scale-Invariant Feature Transform (SIFT)**: Invented by David Lowe at University of British Colu...",
      "full_line": "[^fn-sift]: **Scale-Invariant Feature Transform (SIFT)**: Invented by David Lowe at University of British Columbia in 1999, SIFT revolutionized computer vision by detecting \"keypoints\" that remain stable across different viewpoints, scales, and lighting conditions. A typical image yields 1,000-2,000 SIFT keypoints, each described by a 128-dimensional vector. Before deep learning, SIFT was the backbone of applications like Google Street View's image matching and early smartphone augmented reality. The algorithm's 4-step process (scale-space extrema detection, keypoint localization, orientation assignment, and descriptor generation) required deep expertise to implement effectively."
    },
    {
      "footnote_id": "fn-imagenet-progress",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 240,
      "context": "...r example, the accuracy of image recognition systems has improved from 74% in 2012 to over 95% today[^fn-imagenet-progress].",
      "full_line": "Unlike traditional approaches where performance often plateaus with more data and computation, deep learning models continue to improve as we provide more resources. More training examples help the system recognize more variations and nuances. More computational power enables the system to discover more subtle patterns. This scalability has led to dramatic improvements in performance. For example, the accuracy of image recognition systems has improved from 74% in 2012 to over 95% today[^fn-imagenet-progress]."
    },
    {
      "footnote_id": "fn-imagenet-progress",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 244,
      "context": "[^fn-imagenet-progress]: **ImageNet Competition Progress**: The ImageNet Large Scale Visual Recognition Challenge (ILSVRC)...",
      "full_line": "[^fn-imagenet-progress]: **ImageNet Competition Progress**: The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) tracked computer vision progress from 2010-2017. Error rates dropped dramatically: traditional methods achieved ~28% error in 2010, AlexNet [@krizhevsky2012imagenet] (first deep learning winner) achieved 15.3% in 2012, and ResNet [@he2016deep] achieved 3.6% in 2015\u2014surpassing estimated human performance of 5.1%. This rapid improvement demonstrated deep learning's superiority over hand-crafted features, triggering the modern AI revolution. The competition ended in 2017 when further improvements became incremental."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 299,
      "context": "...andwidth-bound rather than compute-bound. A typical GEMM operation achieves only 5-15% of peak FLOPS[^fn-flops] on CPUs due to data movement costs. Loading a single weight requires 4 bytes (FP32) but performs on...",
      "full_line": "Matrix multiplication, the core operation in neural networks, is memory bandwidth-bound rather than compute-bound. A typical GEMM operation achieves only 5-15% of peak FLOPS[^fn-flops] on CPUs due to data movement costs. Loading a single weight requires 4 bytes (FP32) but performs only one multiplication (6 FLOPS for multiply-accumulate), yielding 1.5 FLOPS/byte. Modern processors provide 10-1000 FLOPS/byte computational capability, creating a 10-1000x imbalance."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 305,
      "context": "[^fn-flops]: **FLOPS**: Floating Point Operations Per Second, a measure of computational performance that quant...",
      "full_line": "[^fn-flops]: **FLOPS**: Floating Point Operations Per Second, a measure of computational performance that quantifies how many floating-point arithmetic operations (additions, multiplications, etc.) a processor can execute in one second. Modern CPUs achieve 100-1000 GFLOPS, while high-end GPUs can reach 100+ TFLOPS for AI workloads."
    },
    {
      "footnote_id": "fn-brain-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 340,
      "context": "...iciency. As detailed in the System Requirements section, the human brain's 20-watt power consumption[^fn-brain-efficiency] creates a stark efficiency gap that artificial systems are still striving to bridge.",
      "full_line": "Perhaps most remarkably, biological systems achieve all this with incredible energy efficiency. As detailed in the System Requirements section, the human brain's 20-watt power consumption[^fn-brain-efficiency] creates a stark efficiency gap that artificial systems are still striving to bridge."
    },
    {
      "footnote_id": "fn-brain-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 342,
      "context": "[^fn-brain-efficiency]: **Brain Energy Efficiency**: The human brain contains approximately 86-100 billion neurons and per...",
      "full_line": "[^fn-brain-efficiency]: **Brain Energy Efficiency**: The human brain contains approximately 86-100 billion neurons and performs roughly 10^16 operations per second on just 20 watts\u2014equivalent to running a single LED light bulb. Deep learning requires training GPT-3 [@brown2020language] consumed about 1,287 megawatt-hours of electricity [@strubell2019energy]. This stark efficiency gap drives research into neuromorphic computing and inspired the development of specialized AI chips designed to mimic brain-like processing."
    },
    {
      "footnote_id": "fn-synapses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 354,
      "context": "...receivers for incoming signals from other neurons. The connections between neurons occur at synapses[^fn-synapses], which modulate the strength of the transmitted signals. Finally, a long, slender projection called...",
      "full_line": "A biological neuron consists of several key components. The central part is the cell body, or soma, which contains the nucleus and performs the cell's basic life processes. Extending from the soma are branch-like structures called dendrites, which act as receivers for incoming signals from other neurons. The connections between neurons occur at synapses[^fn-synapses], which modulate the strength of the transmitted signals. Finally, a long, slender projection called the axon conducts electrical impulses away from the cell body to other neurons."
    },
    {
      "footnote_id": "fn-synapses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 356,
      "context": "[^fn-synapses]: **Synapses**: From the Greek word \"synaptein\" meaning \"to clasp together,\" synapses are the connec...",
      "full_line": "[^fn-synapses]: **Synapses**: From the Greek word \"synaptein\" meaning \"to clasp together,\" synapses are the connection points between neurons where chemical or electrical signals are transmitted. A typical neuron has 1,000-10,000 synaptic connections, and the human brain contains roughly 100 trillion synapses. The strength of synaptic connections can change through experience, forming the biological basis of learning and memory\u2014a principle directly mimicked by adjustable weights in artificial neural networks."
    },
    {
      "footnote_id": "fn-perceptron",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 474,
      "context": "...ural networks in the 1950s, marked by the introduction of the Perceptron [@rosenblatt1958perceptron][^fn-perceptron]. While groundbreaking in concept, these early systems were severely limited by the computational ca...",
      "full_line": "We can appreciate how the field of deep learning evolved to meet these challenges through advances in hardware and algorithms. This journey began with early artificial neural networks in the 1950s, marked by the introduction of the Perceptron [@rosenblatt1958perceptron][^fn-perceptron]. While groundbreaking in concept, these early systems were severely limited by the computational capabilities of their era\u2014primarily mainframe computers that lacked both the processing power and memory capacity needed for complex networks."
    },
    {
      "footnote_id": "fn-perceptron",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 476,
      "context": "[^fn-perceptron]: **Perceptron**: Invented by Frank Rosenblatt in 1957 at Cornell, the perceptron was the first arti...",
      "full_line": "[^fn-perceptron]: **Perceptron**: Invented by Frank Rosenblatt in 1957 at Cornell, the perceptron was the first artificial neural network capable of learning. The New York Times famously reported it would be \"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\" While overly optimistic, this breakthrough laid the foundation for all modern neural networks."
    },
    {
      "footnote_id": "fn-dlprimer-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 478,
      "context": "...f backpropagation algorithms in the 1980s [@rumelhart1986learning], , was a theoretical breakthrough[^fn-dlprimer-backpropagation] and provided a systematic way to train multi-layer networks. The computational demands of this algo...",
      "full_line": "The development of backpropagation algorithms in the 1980s [@rumelhart1986learning], , was a theoretical breakthrough[^fn-dlprimer-backpropagation] and provided a systematic way to train multi-layer networks. The computational demands of this algorithm far exceeded available hardware capabilities. Training even modest networks could take weeks, making experimentation and practical applications challenging. This mismatch between algorithmic requirements and hardware capabilities contributed to a period of reduced interest in neural networks."
    },
    {
      "footnote_id": "fn-dlprimer-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 480,
      "context": "[^fn-dlprimer-backpropagation]: **Backpropagation**: Published by Rumelhart, Hinton, and Williams in 1986, backpropagation solved...",
      "full_line": "[^fn-dlprimer-backpropagation]: **Backpropagation**: Published by Rumelhart, Hinton, and Williams in 1986, backpropagation solved the \"credit assignment problem\"\u2014how to determine which weights in a multi-layer network were responsible for errors. This algorithm, based on the mathematical chain rule, enabled training of deep networks and directly led to the modern AI revolution. Remarkably, a similar algorithm was discovered by Paul Werbos in 1974 but went largely unnoticed."
    },
    {
      "footnote_id": "fn-dlprimer-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 484,
      "context": "...s: computational capabilities measured in the number of Floating Point Operations per Second (FLOPS)[^fn-dlprimer-flops] initially followed a $1.4\\times$ improvement pattern from 1952 to 2010, then accelerated to a 3.4-m...",
      "full_line": "The term \"deep learning\" gained prominence in the 2010s, coinciding with significant advances in computational power and data accessibility. The field has grown exponentially, as illustrated in @fig-trends. The graph reveals two remarkable trends: computational capabilities measured in the number of Floating Point Operations per Second (FLOPS)[^fn-dlprimer-flops] initially followed a $1.4\\times$ improvement pattern from 1952 to 2010, then accelerated to a 3.4-month doubling cycle from 2012 to 2022. Perhaps more striking is the emergence of large-scale models between 2015 and 2022 (not explicitly shown or easily seen in the figure), which scaled 2 to 3 orders of magnitude faster than the general trend, following an aggressive 10-month doubling cycle."
    },
    {
      "footnote_id": "fn-dlprimer-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 486,
      "context": "[^fn-dlprimer-flops]: **FLOPS**: Floating Point Operations Per Second measures computational throughput by counting math...",
      "full_line": "[^fn-dlprimer-flops]: **FLOPS**: Floating Point Operations Per Second measures computational throughput by counting mathematical operations like addition, subtraction, multiplication, and division of decimal numbers. Modern supercomputers achieve exascale performance (10^18 FLOPS), while neural network training requires petascale to exascale compute. For perspective, training GPT-3 [@brown2020language] required approximately 3.14 \u00d7 10^23 FLOPS\u2014more computation than was available to the entire world before 1960."
    },
    {
      "footnote_id": "fn-overfitting",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 522,
      "context": "...orks and controlling learning rates made training more stable. Techniques for preventing overfitting[^fn-overfitting] allowed models to generalize better to new data. Researchers discovered researchers discovered that...",
      "full_line": "Algorithmic innovations made it possible to use this data effectively. New methods for initializing networks and controlling learning rates made training more stable. Techniques for preventing overfitting[^fn-overfitting] allowed models to generalize better to new data. Researchers discovered researchers discovered that neural network performance scaled predictably with model size, computation, and data quantity, leading to increasingly ambitious architectures."
    },
    {
      "footnote_id": "fn-overfitting",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 524,
      "context": "[^fn-overfitting]: **Overfitting**: When a model memorizes training examples instead of learning generalizable patter...",
      "full_line": "[^fn-overfitting]: **Overfitting**: When a model memorizes training examples instead of learning generalizable patterns\u2014like a student who memorizes answers instead of understanding concepts. The model performs perfectly on training data but fails on new examples. Common signs include training accuracy continuing to improve while validation accuracy plateaus or decreases. Think of it as becoming an \"expert\" on a practice test who panics when facing slightly different questions on the real exam."
    },
    {
      "footnote_id": "fn-dlprimer-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 526,
      "context": "...capabilities needed for efficient neural network computation. Specialized AI accelerators like TPUs[^fn-dlprimer-tpu] [@jouppi2017datacenter] pushed performance further. High-bandwidth memory systems and fast intercon...",
      "full_line": "Computing infrastructure evolved to meet these growing demands. On the hardware side, graphics processing units (GPUs) provided the parallel processing capabilities needed for efficient neural network computation. Specialized AI accelerators like TPUs[^fn-dlprimer-tpu] [@jouppi2017datacenter] pushed performance further. High-bandwidth memory systems and fast interconnects addressed data movement challenges. Equally important were software advances\u2014frameworks and libraries[^fn-dl-frameworks] that made it easier to build and train networks, distributed computing systems that enabled training at scale, and tools for optimizing model deployment."
    },
    {
      "footnote_id": "fn-dl-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 526,
      "context": "...ddressed data movement challenges. Equally important were software advances\u2014frameworks and libraries[^fn-dl-frameworks] that made it easier to build and train networks, distributed computing systems that enabled trainin...",
      "full_line": "Computing infrastructure evolved to meet these growing demands. On the hardware side, graphics processing units (GPUs) provided the parallel processing capabilities needed for efficient neural network computation. Specialized AI accelerators like TPUs[^fn-dlprimer-tpu] [@jouppi2017datacenter] pushed performance further. High-bandwidth memory systems and fast interconnects addressed data movement challenges. Equally important were software advances\u2014frameworks and libraries[^fn-dl-frameworks] that made it easier to build and train networks, distributed computing systems that enabled training at scale, and tools for optimizing model deployment."
    },
    {
      "footnote_id": "fn-dlprimer-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 528,
      "context": "[^fn-dlprimer-tpu]: **Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for tensor operati...",
      "full_line": "[^fn-dlprimer-tpu]: **Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for tensor operations, the mathematical building blocks of neural networks. First deployed internally in 2015, TPUs can perform matrix multiplications up to 30\u00d7 faster than 2015-era GPUs while using less power. The name reflects their optimization for tensor operations\u2014multi-dimensional arrays that represent data flowing through neural networks. Google has since made TPUs available through cloud services, democratizing access to this specialized AI hardware."
    },
    {
      "footnote_id": "fn-dl-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 530,
      "context": "[^fn-dl-frameworks]: **Deep Learning Frameworks**: TensorFlow [@abadi2016tensorflow] (released by Google in 2015) and P...",
      "full_line": "[^fn-dl-frameworks]: **Deep Learning Frameworks**: TensorFlow [@abadi2016tensorflow] (released by Google in 2015) and PyTorch [@paszke2019pytorch] (released by Facebook in 2016) democratized deep learning by handling the complex mathematics automatically. Before these frameworks, implementing backpropagation required writing hundreds of lines of error-prone calculus code. Now, a complete neural network can be defined in 10-20 lines. TensorFlow emphasizes production deployment and has been downloaded over 180 million times, while PyTorch dominates research with its dynamic computation graphs. These frameworks automatically compute gradients, optimize GPU memory usage, and distribute training across multiple machines."
    },
    {
      "footnote_id": "fn-relu-function",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 627,
      "context": "* **ReLU (Rectified Linear Unit)** [@nair2010rectified][^fn-relu-function]: Defined as $f(x) = \\max(0,x)$, it introduces sparsity and accelerates convergence in deep networks...",
      "full_line": "* **ReLU (Rectified Linear Unit)** [@nair2010rectified][^fn-relu-function]: Defined as $f(x) = \\max(0,x)$, it introduces sparsity and accelerates convergence in deep networks. From a systems perspective, ReLU is computationally efficient, requiring only a simple comparison and conditional assignment rather than expensive exponential calculations. Its simplicity and effectiveness have made it the default choice in many modern architectures."
    },
    {
      "footnote_id": "fn-relu-function",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 629,
      "context": "[^fn-relu-function]: **ReLU (Rectified Linear Unit)**: A piecewise linear activation function that outputs the input di...",
      "full_line": "[^fn-relu-function]: **ReLU (Rectified Linear Unit)**: A piecewise linear activation function that outputs the input directly if positive, otherwise outputs zero. Introduced by Nair and Hinton in 2010, ReLU solved the vanishing gradient problem and became the default activation function in modern deep learning due to its computational simplicity and biological inspiration from neuron firing patterns."
    },
    {
      "footnote_id": "fn-vanishing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 631,
      "context": "...+ e^{-x}}$, this function maps inputs to a range between 0 and 1 but is prone to vanishing gradients[^fn-vanishing] in deeper architectures. The exponential calculation makes sigmoid computationally expensive compar...",
      "full_line": "* **Sigmoid**: Defined as $f(x) = \\frac{1}{1 + e^{-x}}$, this function maps inputs to a range between 0 and 1 but is prone to vanishing gradients[^fn-vanishing] in deeper architectures. The exponential calculation makes sigmoid computationally expensive compared to ReLU, contributing to slower training and inference. It's particularly useful in binary classification problems where probabilities are needed."
    },
    {
      "footnote_id": "fn-vanishing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 633,
      "context": "[^fn-vanishing]: **Vanishing Gradients**: When gradients become exponentially small as they propagate backward thro...",
      "full_line": "[^fn-vanishing]: **Vanishing Gradients**: When gradients become exponentially small as they propagate backward through many layers, learning effectively stops in early layers. This problem is addressed in detail in @sec-ai-training."
    },
    {
      "footnote_id": "fn-xor-problem",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 986,
      "context": "Consider a network learning the XOR function[^fn-xor-problem]\u2014a classic problem that requires non-linearity. With inputs $x_1$ and $x_2$ that can be 0 or 1, XOR...",
      "full_line": "Consider a network learning the XOR function[^fn-xor-problem]\u2014a classic problem that requires non-linearity. With inputs $x_1$ and $x_2$ that can be 0 or 1, XOR outputs 1 when inputs differ and 0 when they're the same."
    },
    {
      "footnote_id": "fn-xor-problem",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 988,
      "context": "[^fn-xor-problem]: **XOR Problem**: The exclusive-or function became famous in AI history when Marvin Minsky and Seym...",
      "full_line": "[^fn-xor-problem]: **XOR Problem**: The exclusive-or function became famous in AI history when Marvin Minsky and Seymour Papert proved in 1969 that single-layer perceptrons could never learn it, contributing to the \"AI winter\" of the 1970s. XOR requires non-linear decision boundaries\u2014something impossible with linear models. The solution requires at least one hidden layer, demonstrating why \"deep\" networks (with hidden layers) are essential for learning complex patterns. This simple 2-input, 1-output problem helped establish the theoretical foundation for multi-layer neural networks."
    },
    {
      "footnote_id": "fn-mnist-dataset",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1000,
      "context": "...ognizing handwritten digits, a classic problem in deep learning using the MNIST [@lecun1998gradient][^fn-mnist-dataset] dataset.",
      "full_line": "With this foundation, we can now examine more complex topologies. Consider the task of recognizing handwritten digits, a classic problem in deep learning using the MNIST [@lecun1998gradient][^fn-mnist-dataset] dataset."
    },
    {
      "footnote_id": "fn-mnist-dataset",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1002,
      "context": "[^fn-mnist-dataset]: **MNIST Dataset**: Created by Yann LeCun and colleagues in 1998, MNIST (Modified National Institut...",
      "full_line": "[^fn-mnist-dataset]: **MNIST Dataset**: Created by Yann LeCun and colleagues in 1998, MNIST (Modified National Institute of Standards and Technology) contains 70,000 images of handwritten digits\u201460,000 for training and 10,000 for testing. Each image is 28\u00d728 pixels in grayscale, totaling 784 features per digit. MNIST became the \"hello world\" of computer vision, with error rates dropping from 12% with traditional methods in 1998 to 0.23% with modern deep learning. Despite being \"solved,\" MNIST remains invaluable for teaching because it's large enough to be realistic yet small enough to train quickly on any computer."
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1261,
      "context": "...tes as a loop, where each iteration involves processing a subset of training examples called a batch[^fn-batch-processing]. For each batch, the network performs several key operations:",
      "full_line": "Training operates as a loop, where each iteration involves processing a subset of training examples called a batch[^fn-batch-processing]. For each batch, the network performs several key operations:"
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1263,
      "context": "[^fn-batch-processing]: **Batch Processing**: Processing multiple examples simultaneously, typically 32, 64, 128, or 256 s...",
      "full_line": "[^fn-batch-processing]: **Batch Processing**: Processing multiple examples simultaneously, typically 32, 64, 128, or 256 samples per batch. Larger batches provide more stable gradient estimates and better utilize parallel hardware (GPUs can process 256 images nearly as fast as 1 image), but require more memory and may converge to worse solutions. The optimal batch size often depends on available GPU memory\u2014NVIDIA's V100 with 32GB can handle batch sizes of 512-1024 for typical networks, while smaller GPUs require smaller batches. **Systems Engineering Trade-offs**: A ResNet-50 model requires ~25GB memory for training with batch size 256 on ImageNet, forcing practitioners to choose between model complexity and batch size. Cloud deployments can use gradient accumulation to simulate large batches across multiple smaller GPUs, while edge devices may be limited to batch sizes of 1-4 due to memory constraints. This hardware-software co-design relationship exemplifies how neural network training pushes the boundaries of computing systems. @sec-ai-training explores advanced training strategies that help navigate these hardware constraints while optimizing convergence."
    },
    {
      "footnote_id": "fn-cross-entropy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1538,
      "context": "For classification tasks like MNIST digit recognition, \"cross-entropy\" [@shannon1948mathematical][^fn-cross-entropy] loss has emerged as the standard choice. This loss function is particularly well-suited for compari...",
      "full_line": "For classification tasks like MNIST digit recognition, \"cross-entropy\" [@shannon1948mathematical][^fn-cross-entropy] loss has emerged as the standard choice. This loss function is particularly well-suited for comparing predicted probability distributions with true class labels."
    },
    {
      "footnote_id": "fn-cross-entropy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1540,
      "context": "[^fn-cross-entropy]: **Cross-Entropy Loss**: Derived from information theory by Claude Shannon in 1948, cross-entropy m...",
      "full_line": "[^fn-cross-entropy]: **Cross-Entropy Loss**: Derived from information theory by Claude Shannon in 1948, cross-entropy measures the \"surprise\" when predicting incorrectly. If a model is 99% confident about the wrong answer, the loss is much higher than being 60% confident about the wrong answer. This mathematical property naturally encourages the model to be both accurate and calibrated (confident when right, uncertain when unsure). Cross-entropy loss works perfectly with softmax outputs and provides strong gradients even when predictions are very wrong, making it ideal for classification tasks."
    },
    {
      "footnote_id": "fn-gradient-descent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1727,
      "context": "...ess adjusts the network's weights to improve its predictions. Using a method called gradient descent[^fn-gradient-descent], the network calculates how much each weight contributes to the error and updates it to reduce the...",
      "full_line": "The optimization process adjusts the network's weights to improve its predictions. Using a method called gradient descent[^fn-gradient-descent], the network calculates how much each weight contributes to the error and updates it to reduce the loss. This process is repeated over many iterations, gradually refining the network's ability to make accurate predictions."
    },
    {
      "footnote_id": "fn-gradient-descent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1729,
      "context": "[^fn-gradient-descent]: **Gradient Descent**: Think of gradient descent as finding the bottom of a valley while blindfolde...",
      "full_line": "[^fn-gradient-descent]: **Gradient Descent**: Think of gradient descent as finding the bottom of a valley while blindfolded\u2014you feel the slope under your feet and take steps downhill. Mathematically, the gradient points in the direction of steepest increase, so we move in the opposite direction to minimize our loss function. The name comes from the Latin \"gradus\" (step) and was first formalized by Cauchy in 1847 for solving systems of equations, though the modern machine learning version was developed much later."
    },
    {
      "footnote_id": "fn-learning-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1737,
      "context": "...scent will modify the weights to better distinguish between these digits. The learning rate $\\alpha$[^fn-learning-rate] controls how large these adjustments are\u2014too large, and the network might overshoot optimal values;...",
      "full_line": "For our MNIST example, this means adjusting weights to improve digit classification accuracy. If the network frequently confuses \"7\"s with \"1\"s, gradient descent will modify the weights to better distinguish between these digits. The learning rate $\\alpha$[^fn-learning-rate] controls how large these adjustments are\u2014too large, and the network might overshoot optimal values; too small, and training will progress very slowly."
    },
    {
      "footnote_id": "fn-learning-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1739,
      "context": "[^fn-learning-rate]: **Learning Rate**: Often called the most important hyperparameter in deep learning, the learning r...",
      "full_line": "[^fn-learning-rate]: **Learning Rate**: Often called the most important hyperparameter in deep learning, the learning rate determines the step size in optimization. Think of it like the gas pedal on a car\u2014too much acceleration and you'll crash past your destination, too little and you'll never get there. Typical values range from 0.1 to 0.0001, and getting this right can mean the difference between a model that learns in hours versus one that never converges."
    },
    {
      "footnote_id": "fn-epoch-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1777,
      "context": "A single pass through the entire training dataset is called an epoch[^fn-epoch-training]. For MNIST, with 60,000 training images and a batch size of 32, each epoch consists of 1,875 batch...",
      "full_line": "A single pass through the entire training dataset is called an epoch[^fn-epoch-training]. For MNIST, with 60,000 training images and a batch size of 32, each epoch consists of 1,875 batch iterations. The training loop structure is:"
    },
    {
      "footnote_id": "fn-epoch-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1779,
      "context": "[^fn-epoch-training]: **Epoch**: From the Greek word \"epoche\" meaning \"fixed point in time,\" an epoch represents one com...",
      "full_line": "[^fn-epoch-training]: **Epoch**: From the Greek word \"epoche\" meaning \"fixed point in time,\" an epoch represents one complete cycle through all training data. Deep learning models typically require 10-200 epochs to converge, depending on dataset size and complexity. Modern large language models like GPT-3 train on only 1 epoch over massive datasets (300 billion tokens), while smaller models might train for 100+ epochs on limited data. The term was borrowed from astronomy, where it marks a specific moment for measuring celestial positions\u2014fitting for the iterative refinement process of neural network training."
    },
    {
      "footnote_id": "fn-floating-point",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 2228,
      "context": "* Total: 89,610 parameters ($\\approx 358.44$ KB at 32-bit floating point precision[^fn-floating-point])",
      "full_line": "* Total: 89,610 parameters ($\\approx 358.44$ KB at 32-bit floating point precision[^fn-floating-point])"
    },
    {
      "footnote_id": "fn-floating-point",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 2230,
      "context": "[^fn-floating-point]: **32-bit Floating Point Precision**: Also called \"single precision\" or FP32, this IEEE 754 standar...",
      "full_line": "[^fn-floating-point]: **32-bit Floating Point Precision**: Also called \"single precision\" or FP32, this IEEE 754 standard uses 32 bits to represent real numbers: 1 bit for sign, 8 bits for exponent, and 23 bits for mantissa. While neural network training typically requires FP32 precision to maintain gradient stability, inference often works with FP16 (half precision) or even INT8 (8-bit integers), reducing memory usage by 2\u00d7 to 4\u00d7. Modern AI chips like Google's TPU v4 support \"bfloat16\" (brain floating point), Google's custom 16-bit format that maintains FP32's range while halving memory requirements."
    },
    {
      "footnote_id": "fn-uat",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 64,
      "context": "...Their computational power was established theoretically by the Universal Approximation Theorem (UAT)[^fn-uat] [@cybenko1989approximation; @hornik1989multilayer], which states that a sufficiently large MLP with...",
      "full_line": "Multi-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes core computational patterns that appear throughout deep learning models. Their computational power was established theoretically by the Universal Approximation Theorem (UAT)[^fn-uat] [@cybenko1989approximation; @hornik1989multilayer], which states that a sufficiently large MLP with non-linear activation functions[^fn-activation-functions] can approximate any continuous function on a compact domain, given suitable weights and biases."
    },
    {
      "footnote_id": "fn-activation-functions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 64,
      "context": "...rnik1989multilayer], which states that a sufficiently large MLP with non-linear activation functions[^fn-activation-functions] can approximate any continuous function on a compact domain, given suitable weights and biases.",
      "full_line": "Multi-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes core computational patterns that appear throughout deep learning models. Their computational power was established theoretically by the Universal Approximation Theorem (UAT)[^fn-uat] [@cybenko1989approximation; @hornik1989multilayer], which states that a sufficiently large MLP with non-linear activation functions[^fn-activation-functions] can approximate any continuous function on a compact domain, given suitable weights and biases."
    },
    {
      "footnote_id": "fn-uat",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 66,
      "context": "[^fn-uat]: **Universal Approximation Theorem**: Proven independently by Cybenko (1989) and Hornik (1989), thi...",
      "full_line": "[^fn-uat]: **Universal Approximation Theorem**: Proven independently by Cybenko (1989) and Hornik (1989), this result showed that neural networks could theoretically learn any function, a discovery that reinvigorated interest in neural networks after the \"AI Winter\" of the 1980s and established mathematical foundations for modern deep learning."
    },
    {
      "footnote_id": "fn-activation-functions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 68,
      "context": "[^fn-activation-functions]: **Activation Functions**: Non-linear mathematical functions like ReLU, sigmoid, and tanh that intr...",
      "full_line": "[^fn-activation-functions]: **Activation Functions**: Non-linear mathematical functions like ReLU, sigmoid, and tanh that introduce non-linearity into neural networks. Without them, multiple layers would collapse to a single linear transformation, making ReLU's simple max(0,x) operation necessary for deep learning's success since 2012."
    },
    {
      "footnote_id": "fn-mnist-dataset",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 72,
      "context": "When applied to the MNIST handwritten digit recognition challenge[^fn-mnist-dataset], an MLP demonstrates its computational approach by transforming a $28\\times 28$ pixel image into di...",
      "full_line": "When applied to the MNIST handwritten digit recognition challenge[^fn-mnist-dataset], an MLP demonstrates its computational approach by transforming a $28\\times 28$ pixel image into digit classification."
    },
    {
      "footnote_id": "fn-mnist-dataset",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 74,
      "context": "[^fn-mnist-dataset]: **MNIST Dataset**: Created by Yann LeCun in 1998 from NIST's database of handwritten digits, MNIST...",
      "full_line": "[^fn-mnist-dataset]: **MNIST Dataset**: Created by Yann LeCun in 1998 from NIST's database of handwritten digits, MNIST's 60,000 training images became the \"fruit fly\" of machine learning research. Despite achieving 99.7% accuracy being considered solved, MNIST remains valuable for education because its simplicity allows students to focus on architectural concepts without data complexity distractions. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits."
    },
    {
      "footnote_id": "fn-gemm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 86,
      "context": "...yers. The dense connectivity pattern translates mathematically into matrix multiplication operations[^fn-gemm]. As shown in @fig-mlp, each layer transforms its input through matrix multiplication followed by el...",
      "full_line": "To enable unrestricted feature interactions, MLPs implement a direct algorithmic solution: connect everything to everything. This is realized through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers. The dense connectivity pattern translates mathematically into matrix multiplication operations[^fn-gemm]. As shown in @fig-mlp, each layer transforms its input through matrix multiplication followed by element-wise activation:"
    },
    {
      "footnote_id": "fn-gemm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 88,
      "context": "[^fn-gemm]: **GEMM (General Matrix Multiply)**: The fundamental operation underlying neural networks, accounti...",
      "full_line": "[^fn-gemm]: **GEMM (General Matrix Multiply)**: The fundamental operation underlying neural networks, accounting for 80-95% of computation time in dense neural networks. GEMM performs C = \u03b1AB + \u03b2C and has been optimized for decades. Modern implementations like cuBLAS achieve 80-95% of theoretical peak performance on GPUs, making GEMM optimization crucial for ML systems."
    },
    {
      "footnote_id": "fn-mlp-notation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 92,
      "context": "$$[^fn-mlp-notation]",
      "full_line": "$$[^fn-mlp-notation]"
    },
    {
      "footnote_id": "fn-mlp-notation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 94,
      "context": "[^fn-mlp-notation]: **MLP Mathematical Notation**: In this equation, $\\mathbf{h}^{(l)}$ represents the layer $l$ outpu...",
      "full_line": "[^fn-mlp-notation]: **MLP Mathematical Notation**: In this equation, $\\mathbf{h}^{(l)}$ represents the layer $l$ output (activation vector), $\\mathbf{W}^{(l)}$ is the weight matrix for layer $l$, $\\mathbf{b}^{(l)}$ is the bias vector, and $f(\\cdot)$ is the activation function (like ReLU). The superscript $(l)$ denotes the layer number, with bold symbols indicating vectors/matrices. This compact notation captures the core operation of neural networks: linear transformation followed by nonlinear activation."
    },
    {
      "footnote_id": "fn-dnn-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 319,
      "context": "...r weights). While actual implementations use sophisticated optimizations through libraries like BLAS[^fn-dnn-blas] or cuBLAS, these fundamental patterns drive key system design decisions. The hardware architectures...",
      "full_line": "In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use sophisticated optimizations through libraries like BLAS[^fn-dnn-blas] or cuBLAS, these fundamental patterns drive key system design decisions. The hardware architectures that accelerate these matrix operations, including GPU tensor cores and specialized AI accelerators, are comprehensively covered in @sec-ai-acceleration."
    },
    {
      "footnote_id": "fn-dnn-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 321,
      "context": "[^fn-dnn-blas]: **Basic Linear Algebra Subprograms (BLAS)**: Developed in the 1970s as a standard for basic vector...",
      "full_line": "[^fn-dnn-blas]: **Basic Linear Algebra Subprograms (BLAS)**: Developed in the 1970s as a standard for basic vector and matrix operations, BLAS became the foundation for virtually all scientific computing. Modern implementations like Intel MKL and OpenBLAS can achieve 80-95% of theoretical peak performance on well-optimized workloads, making them necessary for neural network efficiency."
    },
    {
      "footnote_id": "fn-dnn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 355,
      "context": "...ty to detect local patterns and the ability to recognize these patterns regardless of their position[^fn-dnn-imagenet].",
      "full_line": "Taking image processing as an example, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image. A cat is still a cat whether it appears in the top-left or bottom-right corner. This indicates two key requirements for spatial pattern processing: the ability to detect local patterns and the ability to recognize these patterns regardless of their position[^fn-dnn-imagenet]."
    },
    {
      "footnote_id": "fn-dnn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 357,
      "context": "[^fn-dnn-imagenet]: **ImageNet Revolution**: AlexNet's dramatic victory in the 2012 ImageNet challenge [@krizhevsky201...",
      "full_line": "[^fn-dnn-imagenet]: **ImageNet Revolution**: AlexNet's dramatic victory in the 2012 ImageNet challenge [@krizhevsky2012imagenet] (reducing error from 26% to 16%) sparked the deep learning renaissance. ImageNet's 14 million labeled images across 20,000 categories provided the scale needed to train deep CNNs, proving that \"big data + big compute + big models\" could achieve superhuman performance."
    },
    {
      "footnote_id": "fn-lecun-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 574,
      "context": "This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun[^fn-lecun-cnn] and @lecun1989backpropagation. CNNs achieve this through several key innovations: parameter sharing...",
      "full_line": "This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun[^fn-lecun-cnn] and @lecun1989backpropagation. CNNs achieve this through several key innovations: parameter sharing[^fn-parameter-sharing], local connectivity, and translation invariance[^fn-translation-invariance]."
    },
    {
      "footnote_id": "fn-parameter-sharing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 574,
      "context": "...and @lecun1989backpropagation. CNNs achieve this through several key innovations: parameter sharing[^fn-parameter-sharing], local connectivity, and translation invariance[^fn-translation-invariance].",
      "full_line": "This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun[^fn-lecun-cnn] and @lecun1989backpropagation. CNNs achieve this through several key innovations: parameter sharing[^fn-parameter-sharing], local connectivity, and translation invariance[^fn-translation-invariance]."
    },
    {
      "footnote_id": "fn-translation-invariance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 574,
      "context": "...nnovations: parameter sharing[^fn-parameter-sharing], local connectivity, and translation invariance[^fn-translation-invariance].",
      "full_line": "This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun[^fn-lecun-cnn] and @lecun1989backpropagation. CNNs achieve this through several key innovations: parameter sharing[^fn-parameter-sharing], local connectivity, and translation invariance[^fn-translation-invariance]."
    },
    {
      "footnote_id": "fn-lecun-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 576,
      "context": "[^fn-lecun-cnn]: **Yann LeCun and CNNs**: LeCun's 1989 LeNet architecture was inspired by Hubel and Wiesel's discov...",
      "full_line": "[^fn-lecun-cnn]: **Yann LeCun and CNNs**: LeCun's 1989 LeNet architecture was inspired by Hubel and Wiesel's discovery of simple and complex cells in cat visual cortex [@hubel1962receptive]. LeNet-5 achieved 99.2% accuracy on MNIST in 1998 and was deployed by banks to read millions of checks daily, among the first large-scale commercial applications of neural networks. As illustrated in @fig-cnn-spatial-processing, CNNs address spatial pattern processing through a different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position, a process known as convolution."
    },
    {
      "footnote_id": "fn-parameter-sharing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 578,
      "context": "[^fn-parameter-sharing]: **Parameter Sharing**: CNNs reuse the same filter weights across spatial positions, dramatically r...",
      "full_line": "[^fn-parameter-sharing]: **Parameter Sharing**: CNNs reuse the same filter weights across spatial positions, dramatically reducing parameters. A CNN processing 224\u00d7224 images might use 3\u00d73 filters with only 9 parameters per channel, versus an equivalent MLP requiring 50,176 parameters per neuron, a 5,500x reduction enabling practical computer vision."
    },
    {
      "footnote_id": "fn-translation-invariance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 580,
      "context": "[^fn-translation-invariance]: **Translation Invariance**: CNNs detect features regardless of spatial position. A cat's ear is re...",
      "full_line": "[^fn-translation-invariance]: **Translation Invariance**: CNNs detect features regardless of spatial position. A cat's ear is recognized whether in the top-left or bottom-right corner. This property emerges from convolution's sliding window design and is crucial for computer vision, where objects appear at arbitrary locations in images."
    },
    {
      "footnote_id": "fn-cnn-convolution-notation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 587,
      "context": "$$[^fn-cnn-convolution-notation]",
      "full_line": "$$[^fn-cnn-convolution-notation]"
    },
    {
      "footnote_id": "fn-cnn-convolution-notation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 589,
      "context": "[^fn-cnn-convolution-notation]: **CNN Convolution Notation**: This equation describes how CNNs process spatial data. $\\mathbf{H}^{...",
      "full_line": "[^fn-cnn-convolution-notation]: **CNN Convolution Notation**: This equation describes how CNNs process spatial data. $\\mathbf{H}^{(l)}_{i,j,k}$ is the output at spatial position $(i,j)$ in channel $k$ of layer $l$. The triple sum iterates over the filter dimensions: $(di,dj)$ scans the spatial filter size, and $c$ covers input channels. $\\mathbf{W}^{(l)}_{di,dj,c,k}$ represents the filter weights, capturing local spatial patterns. Unlike MLPs that connect all inputs to outputs, CNNs only connect local spatial neighborhoods."
    },
    {
      "footnote_id": "fn-receptive-field",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 591,
      "context": "...$ indexes output channels, $c$ indexes input channels, and $(di,dj)$ spans the local receptive field[^fn-receptive-field]. Unlike the dense matrix multiplication of MLPs, this operation:",
      "full_line": "Here, $(i,j)$ corresponds to spatial positions, $k$ indexes output channels, $c$ indexes input channels, and $(di,dj)$ spans the local receptive field[^fn-receptive-field]. Unlike the dense matrix multiplication of MLPs, this operation:"
    },
    {
      "footnote_id": "fn-receptive-field",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 593,
      "context": "[^fn-receptive-field]: **Receptive Field**: The region of the input that influences a particular output neuron. In CNNs,...",
      "full_line": "[^fn-receptive-field]: **Receptive Field**: The region of the input that influences a particular output neuron. In CNNs, receptive fields grow with depth. A neuron in layer 3 might \"see\" a 7\u00d77 region even with 3\u00d73 filters, due to stacking. Understanding receptive field size is crucial for ensuring networks can capture features at the right scale for the task."
    },
    {
      "footnote_id": "fn-simd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 765,
      "context": "...parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions[^fn-simd] to process multiple filter positions simultaneously, while GPUs parallelize computation across spat...",
      "full_line": "This computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions[^fn-simd] to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. The model optimization techniques that further reduce these computational demands, including specialized convolution optimizations and sparsity patterns, are detailed in @sec-model-optimizations."
    },
    {
      "footnote_id": "fn-simd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 767,
      "context": "[^fn-simd]: **SIMD (Single Instruction, Multiple Data)**: CPU instructions that perform the same operation on...",
      "full_line": "[^fn-simd]: **SIMD (Single Instruction, Multiple Data)**: CPU instructions that perform the same operation on multiple data elements simultaneously. Modern x86 processors support AVX-512, enabling 16 single-precision operations per instruction, a 16x speedup over scalar code. SIMD is crucial for efficient neural network inference on CPUs, especially for edge deployment. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities."
    },
    {
      "footnote_id": "fn-vanishing-gradient",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 789,
      "context": "...ind structure in time-dependent data. However, basic RNNs suffer from the vanishing gradient problem[^fn-vanishing-gradient], limiting their ability to learn long-term dependencies.",
      "full_line": "RNNs address sequential processing through a different approach than MLPs or CNNs by introducing recurrent connections. Instead of just mapping inputs to outputs, RNNs maintain an internal state that is updated at each time step. This creates a memory mechanism that allows the network to carry information forward in time. This unique ability to model temporal dependencies was first explored by @elman1990finding, who demonstrated how RNNs could find structure in time-dependent data. However, basic RNNs suffer from the vanishing gradient problem[^fn-vanishing-gradient], limiting their ability to learn long-term dependencies."
    },
    {
      "footnote_id": "fn-vanishing-gradient",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 791,
      "context": "[^fn-vanishing-gradient]: **Vanishing Gradient Problem**: During backpropagation through time, gradients shrink exponentiall...",
      "full_line": "[^fn-vanishing-gradient]: **Vanishing Gradient Problem**: During backpropagation through time, gradients shrink exponentially as they propagate backward through RNN layers. When recurrent weights have magnitude < 1, gradients multiply by values < 1 at each time step, vanishing after 5-10 steps and preventing learning of long-term dependencies\u2014a key limitation solved by LSTMs and attention mechanisms."
    },
    {
      "footnote_id": "fn-attention-notation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1043,
      "context": "$$[^fn-attention-notation]",
      "full_line": "$$[^fn-attention-notation]"
    },
    {
      "footnote_id": "fn-attention-notation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1045,
      "context": "[^fn-attention-notation]: **Attention Mechanism Notation**: This equation shows scaled dot-product attention. $\\mathbf{Q}$ (...",
      "full_line": "[^fn-attention-notation]: **Attention Mechanism Notation**: This equation shows scaled dot-product attention. $\\mathbf{Q}$ (queries) and $\\mathbf{K}$ (keys) are matrix-multiplied to compute similarity scores, divided by $\\sqrt{d_k}$ (key dimension) for numerical stability, then normalized with softmax to get attention weights. These weights are applied to $\\mathbf{V}$ (values) to produce the output. The result is a weighted combination where each position receives information from all relevant positions based on content similarity."
    },
    {
      "footnote_id": "fn-attention-qkv",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1047,
      "context": "In this equation, $\\mathbf{Q}$ (queries), $\\mathbf{K}$ (keys), and $\\mathbf{V}$ (values)[^fn-attention-qkv] represent learned projections of the input. For a sequence of length $N$ with dimension $d$, this o...",
      "full_line": "In this equation, $\\mathbf{Q}$ (queries), $\\mathbf{K}$ (keys), and $\\mathbf{V}$ (values)[^fn-attention-qkv] represent learned projections of the input. For a sequence of length $N$ with dimension $d$, this operation creates an $N\\times N$ attention matrix, determining how each position should attend to all others."
    },
    {
      "footnote_id": "fn-attention-qkv",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1049,
      "context": "[^fn-attention-qkv]: **Query-Key-Value Attention**: Inspired by information retrieval systems where queries search thro...",
      "full_line": "[^fn-attention-qkv]: **Query-Key-Value Attention**: Inspired by information retrieval systems where queries search through keys to retrieve values. In neural attention, queries and keys compute similarity scores (like a search engine matching queries to documents), while values contain the actual information to retrieve\u2014a design that enables flexible, content-based information access."
    },
    {
      "footnote_id": "fn-attention-is-all-you-need",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1401,
      "context": "...processing entirely.\\n\\nTransformers, introduced in the landmark \\\"Attention is All You Need\\\" paper[^fn-attention-is-all-you-need] by @vaswani2017attention, represent the culmination of architectural evolution by eliminating all s...",
      "full_line": "While attention mechanisms introduced the concept of dynamic pattern processing, they were initially applied as additions to existing architectures, particularly RNNs for sequence-to-sequence tasks. This hybrid approach still suffered from the fundamental limitations of recurrent architectures: sequential processing constraints that prevented efficient parallelization and difficulties with very long sequences. The breakthrough insight was recognizing that attention mechanisms alone could replace both convolutional and recurrent processing entirely.\\n\\nTransformers, introduced in the landmark \\\"Attention is All You Need\\\" paper[^fn-attention-is-all-you-need] by @vaswani2017attention, represent the culmination of architectural evolution by eliminating all structural constraints in favor of pure content-dependent processing. Rather than adding attention to RNNs, Transformers built the entire architecture around attention mechanisms, introducing **self-attention** as the primary computational pattern. This architectural decision traded the parameter efficiency of CNNs and the sequential coherence of RNNs for maximum flexibility and parallelizability.\\n\\nThis represents the final step in our architectural journey: from MLPs that connected everything to everything, to CNNs that connected locally, to RNNs that connected sequentially, to Transformers that connect dynamically based on learned content relationships. Each evolution sacrificed constraints for capabilities, with Transformers achieving maximum expressivity at the cost of maximum computational requirements."
    },
    {
      "footnote_id": "fn-attention-is-all-you-need",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1403,
      "context": "[^fn-attention-is-all-you-need]: **\"Attention is All You Need\"**: This 2017 paper by Google researchers eliminated recurrence entir...",
      "full_line": "[^fn-attention-is-all-you-need]: **\"Attention is All You Need\"**: This 2017 paper by Google researchers eliminated recurrence entirely, showing that attention mechanisms alone could achieve state-of-the-art results. The title itself became a rallying cry, and within 5 years, transformer-based models achieved breakthrough performance in language (GPT, BERT), vision (ViT), and beyond. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself."
    },
    {
      "footnote_id": "fn-attention-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1430,
      "context": "...rt{d_k}$ normalizes the variance to 1, maintaining stable gradients and enabling effective learning.[^fn-attention-scaling]",
      "full_line": "A critical component in both self-attention and multi-head attention is the scaling factor $\\sqrt{d_k}$, which serves an important mathematical purpose. This factor prevents the dot products from growing too large, which would push the softmax function into regions with extremely small gradients. For queries and keys of dimension $d_k$, their dot product has variance $d_k$, so dividing by $\\sqrt{d_k}$ normalizes the variance to 1, maintaining stable gradients and enabling effective learning.[^fn-attention-scaling]"
    },
    {
      "footnote_id": "fn-attention-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1432,
      "context": "[^fn-attention-scaling]: **Attention Scaling**: Without the $\\sqrt{d_k}$ scaling factor, large dot products would cause the...",
      "full_line": "[^fn-attention-scaling]: **Attention Scaling**: Without the $\\sqrt{d_k}$ scaling factor, large dot products would cause the softmax to saturate, producing gradients close to zero and hindering learning. This mathematical insight enables stable optimization of large Transformer models."
    },
    {
      "footnote_id": "fn-dnn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1625,
      "context": "Perhaps most importantly, the development of MLPs established the backpropagation algorithm[^fn-dnn-backpropagation], which to this day remains the cornerstone of neural network optimization. This key contribution ha...",
      "full_line": "Perhaps most importantly, the development of MLPs established the backpropagation algorithm[^fn-dnn-backpropagation], which to this day remains the cornerstone of neural network optimization. This key contribution has enabled the development of deep architectures and influenced how later architectures would be designed to maintain gradient flow."
    },
    {
      "footnote_id": "fn-dnn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1627,
      "context": "[^fn-dnn-backpropagation]: **Backpropagation Algorithm**: While the chain rule was known since the 1600s, Rumelhart, Hinton,...",
      "full_line": "[^fn-dnn-backpropagation]: **Backpropagation Algorithm**: While the chain rule was known since the 1600s, Rumelhart, Hinton, and Williams (1986) showed how to efficiently apply it to train multi-layer networks. This \"learning by error propagation\" algorithm made deep networks practical and remains virtually unchanged in modern systems\u2014a testament to its importance."
    },
    {
      "footnote_id": "fn-dnn-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1637,
      "context": "Perhaps even more influential was the introduction of skip connections through ResNets[^fn-dnn-resnet] [@he2016deep]. Originally they were designed to help train very deep CNNs, skip connections have be...",
      "full_line": "Perhaps even more influential was the introduction of skip connections through ResNets[^fn-dnn-resnet] [@he2016deep]. Originally they were designed to help train very deep CNNs, skip connections have become a building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs."
    },
    {
      "footnote_id": "fn-dnn-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1639,
      "context": "[^fn-dnn-resnet]: **ResNet Revolution**: ResNet (2016) solved the \"degradation problem\" where deeper networks perfor...",
      "full_line": "[^fn-dnn-resnet]: **ResNet Revolution**: ResNet (2016) solved the \"degradation problem\" where deeper networks performed worse than shallow ones. The key insight: adding identity shortcuts ($\\mathcal{F}(\\mathbf{x}) + \\mathbf{x}$) let networks learn residual mappings instead of full transformations, enabling training of 1000+ layer networks and winning ImageNet 2015."
    },
    {
      "footnote_id": "fn-lstm-invention",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1649,
      "context": "The development of LSTMs[^fn-lstm-invention] and GRUs brought sophisticated gating mechanisms to neural networks [@hochreiter1997long; @cho2014p...",
      "full_line": "The development of LSTMs[^fn-lstm-invention] and GRUs brought sophisticated gating mechanisms to neural networks [@hochreiter1997long; @cho2014properties]. These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design."
    },
    {
      "footnote_id": "fn-lstm-invention",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1651,
      "context": "[^fn-lstm-invention]: **LSTM Origins**: Sepp Hochreiter and J\u00fcrgen Schmidhuber invented LSTMs in 1997 to solve the \"vani...",
      "full_line": "[^fn-lstm-invention]: **LSTM Origins**: Sepp Hochreiter and J\u00fcrgen Schmidhuber invented LSTMs in 1997 to solve the \"vanishing gradient problem\" that plagued RNNs. Their gating mechanism was inspired by biological neurons' ability to selectively retain information\u2014a breakthrough that enabled sequence modeling and facilitated modern language models."
    },
    {
      "footnote_id": "fn-vision-transformers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1717,
      "context": "...n vision and language models follow this pattern of recombining building blocks. Vision Transformers[^fn-vision-transformers] adapt the Transformer architecture to images while maintaining its essential components [@dosovitsk...",
      "full_line": "Recent innovations in vision and language models follow this pattern of recombining building blocks. Vision Transformers[^fn-vision-transformers] adapt the Transformer architecture to images while maintaining its essential components [@dosovitskiy2021image]. Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution [@brown2020language]. These modern architectural innovations demonstrate the principles of efficient scaling covered in @sec-efficient-ai, while their practical implementation challenges and optimizations are explored in @sec-model-optimizations."
    },
    {
      "footnote_id": "fn-vision-transformers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1719,
      "context": "[^fn-vision-transformers]: **Vision Transformers (ViTs)**: Google's 2021 breakthrough showed that pure transformers could mat...",
      "full_line": "[^fn-vision-transformers]: **Vision Transformers (ViTs)**: Google's 2021 breakthrough showed that pure transformers could match CNN performance on ImageNet by treating image patches as \"words.\" ViTs split a $224\\times 224$ image into $16\\times 16$ patches (196 \"tokens\"), proving that attention mechanisms could replace convolutional inductive biases with sufficient data."
    },
    {
      "footnote_id": "fn-im2col",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1862,
      "context": "...trix Multiplication**: Reshaping convolutional layers into matrix multiplications\u2014using the `im2col`[^fn-im2col] technique, enables efficient computation using optimized BLAS libraries and allows for parallel pro...",
      "full_line": "**Convolution as Matrix Multiplication**: Reshaping convolutional layers into matrix multiplications\u2014using the `im2col`[^fn-im2col] technique, enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms."
    },
    {
      "footnote_id": "fn-im2col",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1864,
      "context": "[^fn-im2col]: **im2col (Image to Column)**: A data layout transformation that converts convolution operations in...",
      "full_line": "[^fn-im2col]: **im2col (Image to Column)**: A data layout transformation that converts convolution operations into matrix multiplications by unfolding image patches into columns. Developed by Intel in the 1990s, im2col trades memory (duplicating data) for compute efficiency, enabling CNNs to leverage decades of GEMM optimizations and achieving 5-10x speedups on CPUs."
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1870,
      "context": "...ng schemes that optimize data reuse. For example, Google's TPU uses a $128\\times 128$ systolic array[^fn-systolic-array] where data flows systematically through processing elements, allowing each input value to be reused...",
      "full_line": "Sliding window operations compute local relationships by applying the same operation to chunks of data. In CNNs processing MNIST images, a $3\\times 3$ convolution filter slides across the $28\\times 28$ input, requiring $26\\times 26$ windows of computation, assuming a stride size of 1. Modern hardware accelerators implement this through specialized memory access patterns and data buffering schemes that optimize data reuse. For example, Google's TPU uses a $128\\times 128$ systolic array[^fn-systolic-array] where data flows systematically through processing elements, allowing each input value to be reused across multiple computations without accessing memory."
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1872,
      "context": "[^fn-systolic-array]: **Systolic Array**: A network of processing elements that rhythmically compute and pass data throu...",
      "full_line": "[^fn-systolic-array]: **Systolic Array**: A network of processing elements that rhythmically compute and pass data through neighbors, like a \"heartbeat\" of computation. Invented by H.T. Kung and Charles Leiserson in 1978, systolic arrays achieve high throughput by overlapping computation with data movement\u2014Google's TPU systolic arrays perform 65,536 multiply-accumulate operations per clock cycle. Software frameworks optimize these operations by transforming them into efficient matrix multiplications (a $3\\times 3$ convolution becomes a $9\\times N$ matrix multiplication) and carefully managing data layout in memory to maximize spatial locality."
    },
    {
      "footnote_id": "fn-parameter-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2125,
      "context": "...interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters[^fn-parameter-scaling]), efficient data movement becomes increasingly critical, leading to innovations like near-memory pr...",
      "full_line": "The evolution from CNNs to Transformers has increased reliance on gather and reduction operations, driving hardware innovations like more flexible interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters[^fn-parameter-scaling]), efficient data movement becomes increasingly critical, leading to innovations like near-memory processing and sophisticated data flow optimizations."
    },
    {
      "footnote_id": "fn-parameter-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2127,
      "context": "[^fn-parameter-scaling]: **Parameter Scaling**: The leap from AlexNet's 60 million parameters (2012) to GPT-3's 175 billion...",
      "full_line": "[^fn-parameter-scaling]: **Parameter Scaling**: The leap from AlexNet's 60 million parameters (2012) to GPT-3's 175 billion parameters (2020) represents a 3,000x increase in just 8 years. Modern models like GPT-4 may exceed 1 trillion parameters, requiring specialized distributed computing infrastructure and consuming megawatts of power during training."
    },
    {
      "footnote_id": "fn-dnn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2133,
      "context": "...tions and convolutions in deep learning has led to the development of tensor processing units (TPUs)[^fn-dnn-tpu] and tensor cores in GPUs, which are specifically designed to perform these operations efficiently.",
      "full_line": "One of the most significant impacts of these primitives on system design is the push towards specialized hardware. The prevalence of matrix multiplications and convolutions in deep learning has led to the development of tensor processing units (TPUs)[^fn-dnn-tpu] and tensor cores in GPUs, which are specifically designed to perform these operations efficiently."
    },
    {
      "footnote_id": "fn-dnn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2135,
      "context": "[^fn-dnn-tpu]: **Tensor Processing Units**: Google's TPUs emerged from their need to run neural networks on billi...",
      "full_line": "[^fn-dnn-tpu]: **Tensor Processing Units**: Google's TPUs emerged from their need to run neural networks on billions of searches daily. First deployed secretly in 2015, TPUs achieve 15-30x better performance per watt than GPUs for inference. The TPU's $128\\times 128$ systolic array performs 65,536 multiply-accumulate operations per clock cycle, revolutionizing AI hardware design. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks."
    },
    {
      "footnote_id": "fn-memory-usage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 45,
      "context": "...t to powering 120 homes for a year [@Patterson_et_al_2021]. Yet this model requires 350GB+ of memory[^fn-memory-usage] to run, making it impossible to deploy on most edge devices that typically have less than 8GB RAM....",
      "full_line": "Consider the practical reality facing ML engineers today. Training GPT-3 cost $4.6 million and consumed 1,287 MWh of electricity, equivalent to powering 120 homes for a year [@Patterson_et_al_2021]. Yet this model requires 350GB+ of memory[^fn-memory-usage] to run, making it impossible to deploy on most edge devices that typically have less than 8GB RAM. Meanwhile, autonomous vehicles need real-time inference within 100ms latency constraints and 50W power budgets, while mobile applications must deliver acceptable performance using processors 1000x less powerful than data center GPUs."
    },
    {
      "footnote_id": "fn-memory-usage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 49,
      "context": "[^fn-memory-usage]: **Memory Usage**: ML models consume both VRAM (for GPU processing) and system RAM. Large language...",
      "full_line": "[^fn-memory-usage]: **Memory Usage**: ML models consume both VRAM (for GPU processing) and system RAM. Large language models like GPT-3 require 350GB+ memory for inference, while typical edge devices have only 4-8GB RAM, creating a deployment gap that necessitates model compression and optimization techniques."
    },
    {
      "footnote_id": "fn-carbon-emissions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 51,
      "context": "[^fn-carbon-emissions]: **Carbon Emissions**: Training GPT-3 generated approximately 502 tons of CO\u2082 equivalent, comparabl...",
      "full_line": "[^fn-carbon-emissions]: **Carbon Emissions**: Training GPT-3 generated approximately 502 tons of CO\u2082 equivalent, comparable to the annual emissions of 123 gasoline-powered vehicles. Modern ML practices increasingly incorporate carbon tracking using tools like CodeCarbon and the ML CO2 Impact calculator to measure and minimize environmental impact."
    },
    {
      "footnote_id": "fn-scaling-laws",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 77,
      "context": "To address these concerns, researchers have developed scaling laws[^fn-scaling-laws]\u2014empirical relationships that quantify how model performance relates to training resources. These la...",
      "full_line": "To address these concerns, researchers have developed scaling laws[^fn-scaling-laws]\u2014empirical relationships that quantify how model performance relates to training resources. These laws provide a framework for analyzing scaling trade-offs and reveal why efficiency becomes increasingly important as systems expand in size and complexity."
    },
    {
      "footnote_id": "fn-scaling-laws",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 79,
      "context": "[^fn-scaling-laws]: **Scaling Laws**: Empirical relationships discovered by OpenAI showing that language model perform...",
      "full_line": "[^fn-scaling-laws]: **Scaling Laws**: Empirical relationships discovered by OpenAI showing that language model performance follows predictable power-law relationships with model size (N), dataset size (D), and compute budget (C). These laws enable researchers to predict performance and optimal resource allocation before expensive training runs."
    },
    {
      "footnote_id": "fn-tokens",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 97,
      "context": "...training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve correspond...",
      "full_line": "Empirical studies of large language models (LLMs) clarify the interplay between parameters, data, and computational resources under fixed resource constraints. As illustrated in @fig-compute-optimal, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-efficient-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models."
    },
    {
      "footnote_id": "fn-efficient-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 97,
      "context": "...oFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-efficient-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most effic...",
      "full_line": "Empirical studies of large language models (LLMs) clarify the interplay between parameters, data, and computational resources under fixed resource constraints. As illustrated in @fig-compute-optimal, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-efficient-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models."
    },
    {
      "footnote_id": "fn-transformer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 97,
      "context": "...ds to a constant number of floating-point operations (FLOPs[^fn-efficient-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computati...",
      "full_line": "Empirical studies of large language models (LLMs) clarify the interplay between parameters, data, and computational resources under fixed resource constraints. As illustrated in @fig-compute-optimal, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-efficient-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models."
    },
    {
      "footnote_id": "fn-autoregressive",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 97,
      "context": "...signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and t...",
      "full_line": "Empirical studies of large language models (LLMs) clarify the interplay between parameters, data, and computational resources under fixed resource constraints. As illustrated in @fig-compute-optimal, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-efficient-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models."
    },
    {
      "footnote_id": "fn-tokens",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 99,
      "context": "[^fn-tokens]: **Tokens**: Individual units of text that language models process, created by breaking text into s...",
      "full_line": "[^fn-tokens]: **Tokens**: Individual units of text that language models process, created by breaking text into subword pieces using algorithms like Byte-Pair Encoding (BPE). GPT-3 trained on 300 billion tokens while PaLM used 780 billion tokens, requiring text corpora equivalent to millions of books from web crawls and digitized literature."
    },
    {
      "footnote_id": "fn-efficient-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 101,
      "context": "[^fn-efficient-flops]: **FLOPs**: Floating-Point Operations, measuring computational work performed. Modern deep learning...",
      "full_line": "[^fn-efficient-flops]: **FLOPs**: Floating-Point Operations, measuring computational work performed. Modern deep learning models require 10\u00b2\u00b2-10\u00b2\u2074 FLOPs for training: GPT-3 used ~3.14 \u00d7 10\u00b2\u00b3 FLOPs (314 sextillion operations), equivalent to running a high-end gaming PC continuously for over 350 years."
    },
    {
      "footnote_id": "fn-transformer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 103,
      "context": "[^fn-transformer]: **Transformer**: Neural network architecture introduced by Vaswani et al. [@vaswani2017attention]...",
      "full_line": "[^fn-transformer]: **Transformer**: Neural network architecture introduced by Vaswani et al. [@vaswani2017attention] that revolutionized NLP through self-attention mechanisms. Unlike sequential RNNs, transformers enable parallel processing during training, forming the foundation of modern large language models including GPT, BERT, T5, and their derivatives."
    },
    {
      "footnote_id": "fn-autoregressive",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 105,
      "context": "[^fn-autoregressive]: **Autoregressive Models**: Language models that generate text by predicting each token based on al...",
      "full_line": "[^fn-autoregressive]: **Autoregressive Models**: Language models that generate text by predicting each token based on all preceding tokens in the sequence. GPT-family models exemplify this approach, generating text left-to-right with causal attention masks to ensure each position only attends to previous positions."
    },
    {
      "footnote_id": "fn-efficient-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 109,
      "context": "...upplied. Similarly, language models exhibit analogous patterns, with studies of models such as GPT-3[^fn-efficient-gpt3] demonstrating that performance[^fn-perplexity] scales predictably with both model parameters and tr...",
      "full_line": "For instance, in computer vision tasks, doubling the size of neural networks typically yields consistent accuracy gains, provided that proportional increases in training data are supplied. Similarly, language models exhibit analogous patterns, with studies of models such as GPT-3[^fn-efficient-gpt3] demonstrating that performance[^fn-perplexity] scales predictably with both model parameters and training data volume."
    },
    {
      "footnote_id": "fn-perplexity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 109,
      "context": "...us patterns, with studies of models such as GPT-3[^fn-efficient-gpt3] demonstrating that performance[^fn-perplexity] scales predictably with both model parameters and training data volume.",
      "full_line": "For instance, in computer vision tasks, doubling the size of neural networks typically yields consistent accuracy gains, provided that proportional increases in training data are supplied. Similarly, language models exhibit analogous patterns, with studies of models such as GPT-3[^fn-efficient-gpt3] demonstrating that performance[^fn-perplexity] scales predictably with both model parameters and training data volume."
    },
    {
      "footnote_id": "fn-efficient-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 111,
      "context": "[^fn-efficient-gpt3]: **GPT-3**: OpenAI's 175-billion parameter language model released in 2020, costing an estimated $4...",
      "full_line": "[^fn-efficient-gpt3]: **GPT-3**: OpenAI's 175-billion parameter language model released in 2020, costing an estimated $4.6 million to train and consuming ~1,287 MWh of electricity. Its training data included 45TB of text from the internet, books, and other sources."
    },
    {
      "footnote_id": "fn-perplexity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 121,
      "context": "[^fn-perplexity]: **Perplexity**: A measurement of how well a language model predicts text, calculated as 2^(cross-e...",
      "full_line": "[^fn-perplexity]: **Perplexity**: A measurement of how well a language model predicts text, calculated as 2^(cross-entropy loss). GPT-3 achieved ~20 perplexity on WebText, meaning on average it's as confused as if choosing randomly among 20 equally-likely next words."
    },
    {
      "footnote_id": "fn-distributed-infrastructure",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 436,
      "context": "...le models necessitates substantial processing power, typically requiring distributed infrastructures[^fn-distributed-infrastructure] comprising hundreds or thousands of accelerators. For instance, the training of state-of-the-art la...",
      "full_line": "A primary concern is the computational expenditure. Training large-scale models necessitates substantial processing power, typically requiring distributed infrastructures[^fn-distributed-infrastructure] comprising hundreds or thousands of accelerators. For instance, the training of state-of-the-art language models may require tens of thousands of GPU-days, consuming millions of kilowatt-hours of electricity and incurring financial costs that are prohibitive for many institutions. These distributed training systems introduce additional complexity around communication overhead, synchronization, and scaling efficiency\u2014detailed distributed training strategies and their efficiency implications are covered in @sec-ai-training. As previously discussed, the energy demands of training have outpaced Moore's Law, raising critical questions regarding the long-term sustainability of continued scaling."
    },
    {
      "footnote_id": "fn-distributed-infrastructure",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 438,
      "context": "[^fn-distributed-infrastructure]: **Distributed Infrastructure**: Computing systems that spread ML workloads across multiple machine...",
      "full_line": "[^fn-distributed-infrastructure]: **Distributed Infrastructure**: Computing systems that spread ML workloads across multiple machines connected by high-speed networks. OpenAI's GPT-4 training likely used thousands of NVIDIA A100 GPUs connected via InfiniBand, requiring careful orchestration to avoid communication bottlenecks."
    },
    {
      "footnote_id": "fn-carbon-emissions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 442,
      "context": "...models can incur millions of U.S. dollars in computational expenses alone, and the carbon footprint[^fn-carbon-emissions] associated with such training has garnered increasing scrutiny. These costs limit accessibility to...",
      "full_line": "The financial and environmental implications of scaling also warrant careful consideration. Training runs for large foundation models can incur millions of U.S. dollars in computational expenses alone, and the carbon footprint[^fn-carbon-emissions] associated with such training has garnered increasing scrutiny. These costs limit accessibility to cutting-edge research and exacerbate disparities in access to advanced AI systems. From a system design perspective, this underscores the imperative to develop more resource-efficient scaling strategies that minimize consumption without sacrificing performance. The democratization challenges introduced by efficiency barriers connect directly to the accessibility goals addressed in @sec-ai-good, where efficient AI systems enable broader societal impact. Comprehensive approaches to environmental sustainability in ML systems, including carbon footprint measurement, green computing practices, and sustainable AI development frameworks, are explored in @sec-sustainable-ai."
    },
    {
      "footnote_id": "fn-memory-bandwidth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 456,
      "context": "...o impose practical scaling constraints. As models grow in size, they demand greater memory bandwidth[^fn-memory-bandwidth], interconnect capacity, and I/O throughput. These hardware limitations become increasingly challeng...",
      "full_line": "Infrastructure bottlenecks also impose practical scaling constraints. As models grow in size, they demand greater memory bandwidth[^fn-memory-bandwidth], interconnect capacity, and I/O throughput. These hardware limitations become increasingly challenging to overcome, even with specialized accelerators. For instance, distributing a trillion-parameter model across a cluster necessitates meticulous management of data parallelism, communication overhead, and fault tolerance. The complexity of orchestrating such large-scale systems introduces engineering challenges that can diminish the theoretical gains predicted by scaling laws."
    },
    {
      "footnote_id": "fn-memory-bandwidth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 458,
      "context": "[^fn-memory-bandwidth]: **Memory Bandwidth**: The rate at which data can be read from or written to memory, measured in GB...",
      "full_line": "[^fn-memory-bandwidth]: **Memory Bandwidth**: The rate at which data can be read from or written to memory, measured in GB/s. NVIDIA H100 provides 3.35 TB/s memory bandwidth vs. typical DDR5 RAM's 51 GB/s\u2014a 65\u00d7 difference critical for handling large model parameters."
    },
    {
      "footnote_id": "fn-efficient-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 604,
      "context": "[^fn-efficient-model-parallelism]: **Model Parallelism**: Distributing model components across multiple processors, contrasting with...",
      "full_line": "[^fn-efficient-model-parallelism]: **Model Parallelism**: Distributing model components across multiple processors, contrasting with data parallelism. Modern transformer models like GPT-3 require model parallelism due to their 175B parameters exceeding single GPU memory (~24GB for A100 variant)."
    },
    {
      "footnote_id": "fn-efficient-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 606,
      "context": "[^fn-efficient-sgd]: **Stochastic Gradient Descent (SGD)**: Optimization algorithm using random data samples, introduce...",
      "full_line": "[^fn-efficient-sgd]: **Stochastic Gradient Descent (SGD)**: Optimization algorithm using random data samples, introduced by Robbins and Monro (1951). Made neural network training practical by reducing memory requirements from full-batch to single-sample updates, enabling learning on larger datasets."
    },
    {
      "footnote_id": "fn-mobilenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 637,
      "context": "...y requires architectures designed from the ground up for resource constraints. Models like MobileNet[^fn-mobilenet], EfficientNet[^fn-efficientnet], and SqueezeNet[^fn-squeezenet] demonstrate that compact designs ca...",
      "full_line": "Beyond compression techniques, modern efficiency requires architectures designed from the ground up for resource constraints. Models like MobileNet[^fn-mobilenet], EfficientNet[^fn-efficientnet], and SqueezeNet[^fn-squeezenet] demonstrate that compact designs can deliver high performance through architectural innovations rather than just scaling up existing designs. These architectures use specialized architectural patterns and automated design techniques to optimize the efficiency-accuracy trade-off."
    },
    {
      "footnote_id": "fn-efficientnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 637,
      "context": "...gned from the ground up for resource constraints. Models like MobileNet[^fn-mobilenet], EfficientNet[^fn-efficientnet], and SqueezeNet[^fn-squeezenet] demonstrate that compact designs can deliver high performance throu...",
      "full_line": "Beyond compression techniques, modern efficiency requires architectures designed from the ground up for resource constraints. Models like MobileNet[^fn-mobilenet], EfficientNet[^fn-efficientnet], and SqueezeNet[^fn-squeezenet] demonstrate that compact designs can deliver high performance through architectural innovations rather than just scaling up existing designs. These architectures use specialized architectural patterns and automated design techniques to optimize the efficiency-accuracy trade-off."
    },
    {
      "footnote_id": "fn-squeezenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 637,
      "context": "...ce constraints. Models like MobileNet[^fn-mobilenet], EfficientNet[^fn-efficientnet], and SqueezeNet[^fn-squeezenet] demonstrate that compact designs can deliver high performance through architectural innovations rat...",
      "full_line": "Beyond compression techniques, modern efficiency requires architectures designed from the ground up for resource constraints. Models like MobileNet[^fn-mobilenet], EfficientNet[^fn-efficientnet], and SqueezeNet[^fn-squeezenet] demonstrate that compact designs can deliver high performance through architectural innovations rather than just scaling up existing designs. These architectures use specialized architectural patterns and automated design techniques to optimize the efficiency-accuracy trade-off."
    },
    {
      "footnote_id": "fn-mobilenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 641,
      "context": "[^fn-mobilenet]: **MobileNet**: Efficient neural network architecture using depthwise separable convolutions, achie...",
      "full_line": "[^fn-mobilenet]: **MobileNet**: Efficient neural network architecture using depthwise separable convolutions, achieving ~50\u00d7 fewer parameters than traditional models. MobileNet-v1 has only 4.2M parameters vs. VGG-16's 138M, enabling deployment on smartphones with <100MB memory."
    },
    {
      "footnote_id": "fn-efficientnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 643,
      "context": "[^fn-efficientnet]: **EfficientNet**: Architecture achieving state-of-the-art accuracy with 10\u00d7 fewer parameters than...",
      "full_line": "[^fn-efficientnet]: **EfficientNet**: Architecture achieving state-of-the-art accuracy with 10\u00d7 fewer parameters than previous models. EfficientNet-B7 achieves 84.3% ImageNet top-1 accuracy with 66M parameters, compared to ResNet-152's 60M parameters achieving 78.3%."
    },
    {
      "footnote_id": "fn-squeezenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 645,
      "context": "[^fn-squeezenet]: **SqueezeNet**: Compact CNN architecture achieving AlexNet-level accuracy with 50\u00d7 fewer parameter...",
      "full_line": "[^fn-squeezenet]: **SqueezeNet**: Compact CNN architecture achieving AlexNet-level accuracy with 50\u00d7 fewer parameters (1.25M vs. 60M). Demonstrated that clever architecture design can dramatically reduce model size without sacrificing performance."
    },
    {
      "footnote_id": "fn-param-efficient",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 649,
      "context": "The frontier of algorithmic efficiency lies in parameter-efficient fine-tuning[^fn-param-efficient] techniques that demonstrate how the three efficiency dimensions work together. Parameter-efficient...",
      "full_line": "The frontier of algorithmic efficiency lies in parameter-efficient fine-tuning[^fn-param-efficient] techniques that demonstrate how the three efficiency dimensions work together. Parameter-efficient fine-tuning methods update less than 1% of model parameters while achieving full fine-tuning performance. This approach simultaneously addresses all three efficiency pillars: algorithmic efficiency through reduced parameter updates, compute efficiency through lower memory requirements and faster training, and data efficiency by leveraging pre-trained representations that require fewer task-specific examples."
    },
    {
      "footnote_id": "fn-param-efficient",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 651,
      "context": "[^fn-param-efficient]: **Parameter-Efficient Fine-tuning**: Methods like LoRA and Adapters that update <1% of model param...",
      "full_line": "[^fn-param-efficient]: **Parameter-Efficient Fine-tuning**: Methods like LoRA and Adapters that update <1% of model parameters while achieving full fine-tuning performance. Reduces memory requirements from gigabytes to megabytes for large model adaptation."
    },
    {
      "footnote_id": "fn-efficient-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 770,
      "context": "...outpacing hardware improvements and demonstrating the significant impact of algorithmic advancements[^fn-efficient-sgd] on model efficiency [@Hernandez_et_al_2020]. Innovations in model architecture and optimization tec...",
      "full_line": "Neural network training compute requirements decreased 44\u00d7 between 2012 and 2019, outpacing hardware improvements and demonstrating the significant impact of algorithmic advancements[^fn-efficient-sgd] on model efficiency [@Hernandez_et_al_2020]. Innovations in model architecture and optimization techniques can drive substantial gains in AI system sustainability via this halving of compute every 16 months."
    },
    {
      "footnote_id": "fn-efficient-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 773,
      "context": "...go-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-efficient-alexnet]-level performance on ImageNet[^fn-efficient-imagenet] classification had decreased by $44\\times$ co...",
      "full_line": "Notably, as @fig-algo-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-efficient-alexnet]-level performance on ImageNet[^fn-efficient-imagenet] classification had decreased by $44\\times$ compared to 2012. This improvement, which halved every 16 months, outpaced the hardware efficiency gains of Moore's Law[^fn-efficient-moores-law]. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongside hardware innovations [@Hernandez_et_al_2020]."
    },
    {
      "footnote_id": "fn-efficient-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 773,
      "context": "...ed to train a neural network to achieve AlexNet[^fn-efficient-alexnet]-level performance on ImageNet[^fn-efficient-imagenet] classification had decreased by $44\\times$ compared to 2012. This improvement, which halved every 1...",
      "full_line": "Notably, as @fig-algo-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-efficient-alexnet]-level performance on ImageNet[^fn-efficient-imagenet] classification had decreased by $44\\times$ compared to 2012. This improvement, which halved every 16 months, outpaced the hardware efficiency gains of Moore's Law[^fn-efficient-moores-law]. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongside hardware innovations [@Hernandez_et_al_2020]."
    },
    {
      "footnote_id": "fn-efficient-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 773,
      "context": "...his improvement, which halved every 16 months, outpaced the hardware efficiency gains of Moore's Law[^fn-efficient-moores-law]. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongs...",
      "full_line": "Notably, as @fig-algo-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-efficient-alexnet]-level performance on ImageNet[^fn-efficient-imagenet] classification had decreased by $44\\times$ compared to 2012. This improvement, which halved every 16 months, outpaced the hardware efficiency gains of Moore's Law[^fn-efficient-moores-law]. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongside hardware innovations [@Hernandez_et_al_2020]."
    },
    {
      "footnote_id": "fn-efficient-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 775,
      "context": "[^fn-efficient-alexnet]: **AlexNet**: Groundbreaking CNN by Krizhevsky, Sutskever, and Hinton (2012) that won ImageNet with...",
      "full_line": "[^fn-efficient-alexnet]: **AlexNet**: Groundbreaking CNN by Krizhevsky, Sutskever, and Hinton (2012) that won ImageNet with 15.3% error rate, nearly halving the previous best of 26.2%. Used 60M parameters, two GPUs, and launched the deep learning revolution."
    },
    {
      "footnote_id": "fn-efficient-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 777,
      "context": "[^fn-efficient-imagenet]: **ImageNet**: Large-scale visual recognition dataset with 14+ million images across 20,000+ catego...",
      "full_line": "[^fn-efficient-imagenet]: **ImageNet**: Large-scale visual recognition dataset with 14+ million images across 20,000+ categories. The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) drove computer vision breakthroughs from 2010-2017."
    },
    {
      "footnote_id": "fn-efficient-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 779,
      "context": "[^fn-efficient-moores-law]: **Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles...",
      "full_line": "[^fn-efficient-moores-law]: **Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every ~2 years. Hardware improvements follow ~2x every 18-24 months, while AI algorithmic efficiency improved 44x in 7 years (2012-2019)."
    },
    {
      "footnote_id": "fn-efficient-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 795,
      "context": "...2010s brought a seismic shift in the landscape of compute efficiency. Models like AlexNet and ResNet[^fn-efficient-resnet] showed the potential of neural networks, but their computational demands quickly surpassed the capa...",
      "full_line": "The introduction of deep learning in the early 2010s brought a seismic shift in the landscape of compute efficiency. Models like AlexNet and ResNet[^fn-efficient-resnet] showed the potential of neural networks, but their computational demands quickly surpassed the capabilities of traditional CPUs. As shown in @fig-comp_efficiency, this marked the beginning of an era of exponential growth in compute usage. OpenAI's analysis reveals that the amount of compute used in AI training has increased 300,000 times since 2012, doubling approximately every 3.4 months\u2014a rate far exceeding Moore's Law [@Amodei_et_al_2018]."
    },
    {
      "footnote_id": "fn-efficient-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 797,
      "context": "[^fn-efficient-resnet]: **ResNet**: Residual Network architecture by He et al. [@he2016deep] enabling training of very dee...",
      "full_line": "[^fn-efficient-resnet]: **ResNet**: Residual Network architecture by He et al. [@he2016deep] enabling training of very deep networks (152+ layers) through skip connections. Won ImageNet 2015 with 3.6% error rate, surpassing human-level performance for the first time."
    },
    {
      "footnote_id": "fn-cuda-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 947,
      "context": "...a CPU might have 16 cores, modern high-end GPUs like the NVIDIA H100 contain over 16,000 CUDA cores[^fn-cuda-cores]. Specialized hardware accelerators such as Google's Tensor Processing Units (TPUs) further revoluti...",
      "full_line": "This rapid growth was driven not only by the adoption of GPUs, which offered unparalleled parallel processing capabilities, but also by the willingness of researchers to scale up experiments by using large GPU clusters. Graphics Processing Units (GPUs) contain thousands of small cores designed for parallel computation\u2014ideal for the matrix multiplications central to neural networks. While a CPU might have 16 cores, modern high-end GPUs like the NVIDIA H100 contain over 16,000 CUDA cores[^fn-cuda-cores]. Specialized hardware accelerators such as Google's Tensor Processing Units (TPUs) further revolutionized compute efficiency by designing chips specifically for machine learning workloads, optimizing for the specific data types and operations most common in neural networks. These innovations enabled significant reductions in training times for deep learning models, transforming tasks that once took weeks into operations completed in hours or days."
    },
    {
      "footnote_id": "fn-cuda-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 949,
      "context": "[^fn-cuda-cores]: **CUDA Cores**: NVIDIA's parallel processing units optimized for floating-point operations. Unlike...",
      "full_line": "[^fn-cuda-cores]: **CUDA Cores**: NVIDIA's parallel processing units optimized for floating-point operations. Unlike CPU cores (designed for complex sequential tasks), CUDA cores are simpler and work together\u2014enabling a single H100 GPU to perform 16,896 parallel operations simultaneously for massive speedup in matrix computations."
    },
    {
      "footnote_id": "fn-efficient-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1052,
      "context": "...cy is achieved by splitting workloads across multiple machines. Techniques such as model parallelism[^fn-efficient-model-parallelism] and data parallelism[^fn-efficient-data-parallelism] allow large-scale models to be trained more ef...",
      "full_line": "Another focus is on distributed systems, where compute efficiency is achieved by splitting workloads across multiple machines. Techniques such as model parallelism[^fn-efficient-model-parallelism] and data parallelism[^fn-efficient-data-parallelism] allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to maximize throughput. These methods reduce training times while minimizing the idle time of hardware resources."
    },
    {
      "footnote_id": "fn-efficient-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1052,
      "context": "...machines. Techniques such as model parallelism[^fn-efficient-model-parallelism] and data parallelism[^fn-efficient-data-parallelism] allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to max...",
      "full_line": "Another focus is on distributed systems, where compute efficiency is achieved by splitting workloads across multiple machines. Techniques such as model parallelism[^fn-efficient-model-parallelism] and data parallelism[^fn-efficient-data-parallelism] allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to maximize throughput. These methods reduce training times while minimizing the idle time of hardware resources."
    },
    {
      "footnote_id": "fn-efficient-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1054,
      "context": "[^fn-efficient-data-parallelism]: **Data Parallelism**: Training method where the same model runs on multiple processors with differ...",
      "full_line": "[^fn-efficient-data-parallelism]: **Data Parallelism**: Training method where the same model runs on multiple processors with different data batches. GPT-3 training used data parallelism across thousands of GPUs, processing multiple text sequences simultaneously to achieve massive scale."
    },
    {
      "footnote_id": "fn-uci",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1096,
      "context": "...avily on curated datasets, such as [UCI's Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci], which provided clean, well-structured data for experimentation. Feature selection and dimensionali...",
      "full_line": "In the early days of machine learning, data efficiency was not a primary focus, as datasets were relatively small and manageable. The challenge during this period was often acquiring enough labeled data to train models effectively. Researchers relied heavily on curated datasets, such as [UCI's Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci], which provided clean, well-structured data for experimentation. Feature selection and dimensionality reduction techniques, such as principal component analysis (PCA)[^fn-pca], were common methods for ensuring that models extracted the most valuable information from limited data."
    },
    {
      "footnote_id": "fn-pca",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1096,
      "context": "...eature selection and dimensionality reduction techniques, such as principal component analysis (PCA)[^fn-pca], were common methods for ensuring that models extracted the most valuable information from limited...",
      "full_line": "In the early days of machine learning, data efficiency was not a primary focus, as datasets were relatively small and manageable. The challenge during this period was often acquiring enough labeled data to train models effectively. Researchers relied heavily on curated datasets, such as [UCI's Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci], which provided clean, well-structured data for experimentation. Feature selection and dimensionality reduction techniques, such as principal component analysis (PCA)[^fn-pca], were common methods for ensuring that models extracted the most valuable information from limited data."
    },
    {
      "footnote_id": "fn-uci",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1098,
      "context": "[^fn-uci]: **UCI Machine Learning Repository**: Established in 1987 by the University of California, Irvine,...",
      "full_line": "[^fn-uci]: **UCI Machine Learning Repository**: Established in 1987 by the University of California, Irvine, one of the most widely-used resources for machine learning datasets. Contains over 600 datasets and has been cited in thousands of research papers, serving as a cornerstone for early ML research."
    },
    {
      "footnote_id": "fn-pca",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1100,
      "context": "[^fn-pca]: **Principal Component Analysis (PCA)**: Dimensionality reduction technique invented by Karl Pearso...",
      "full_line": "[^fn-pca]: **Principal Component Analysis (PCA)**: Dimensionality reduction technique invented by Karl Pearson in 1901, identifies the most important directions of variation in data. Reduces computational complexity while preserving 90%+ of data variance in many applications."
    },
    {
      "footnote_id": "fn-mnist",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1102,
      "context": "...nd data cleaning. Algorithms were designed to work well with relatively small datasets such as MNIST[^fn-mnist] [@deng2012mnist], Caltech 101 [@FeiFei2004LearningGV] and CIFAR-10[^fn-cifar10] [@Krizhevsky09learn...",
      "full_line": "During this era, data efficiency was achieved through careful preprocessing and data cleaning. Algorithms were designed to work well with relatively small datasets such as MNIST[^fn-mnist] [@deng2012mnist], Caltech 101 [@FeiFei2004LearningGV] and CIFAR-10[^fn-cifar10] [@Krizhevsky09learningmultiple], and computational limitations reinforced the need for data parsimony. These constraints shaped the development of techniques that maximized performance with minimal data, ensuring that every data point contributed meaningfully to the learning process."
    },
    {
      "footnote_id": "fn-cifar10",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1102,
      "context": "...datasets such as MNIST[^fn-mnist] [@deng2012mnist], Caltech 101 [@FeiFei2004LearningGV] and CIFAR-10[^fn-cifar10] [@Krizhevsky09learningmultiple], and computational limitations reinforced the need for data parsimo...",
      "full_line": "During this era, data efficiency was achieved through careful preprocessing and data cleaning. Algorithms were designed to work well with relatively small datasets such as MNIST[^fn-mnist] [@deng2012mnist], Caltech 101 [@FeiFei2004LearningGV] and CIFAR-10[^fn-cifar10] [@Krizhevsky09learningmultiple], and computational limitations reinforced the need for data parsimony. These constraints shaped the development of techniques that maximized performance with minimal data, ensuring that every data point contributed meaningfully to the learning process."
    },
    {
      "footnote_id": "fn-mnist",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1104,
      "context": "[^fn-mnist]: **MNIST**: Modified National Institute of Standards and Technology database of handwritten digits,...",
      "full_line": "[^fn-mnist]: **MNIST**: Modified National Institute of Standards and Technology database of handwritten digits, containing 70,000 28\u00d728 pixel images. Created in 1998, became the \"Hello World\" of computer vision, though modern models achieve >99% accuracy."
    },
    {
      "footnote_id": "fn-cifar10",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1106,
      "context": "[^fn-cifar10]: **CIFAR-10**: Canadian Institute for Advanced Research dataset with 60,000 32\u00d732 color images acro...",
      "full_line": "[^fn-cifar10]: **CIFAR-10**: Canadian Institute for Advanced Research dataset with 60,000 32\u00d732 color images across 10 classes. Released in 2009, remains a standard benchmark despite its small image size by modern standards."
    },
    {
      "footnote_id": "fn-efficient-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1112,
      "context": "...veloped techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning[^fn-efficient-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specifi...",
      "full_line": "This reliance on large datasets introduced inefficiencies. Data collection became a costly and time-consuming endeavor, requiring vast amounts of labeled data for supervised learning tasks. To address these challenges, researchers developed techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning[^fn-efficient-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specific data [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creating new variations of existing samples. Active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort while maintaining performance [@Settles_2009]."
    },
    {
      "footnote_id": "fn-data-augmentation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1112,
      "context": "...er datasets, reducing the need for task-specific data [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creatin...",
      "full_line": "This reliance on large datasets introduced inefficiencies. Data collection became a costly and time-consuming endeavor, requiring vast amounts of labeled data for supervised learning tasks. To address these challenges, researchers developed techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning[^fn-efficient-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specific data [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creating new variations of existing samples. Active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort while maintaining performance [@Settles_2009]."
    },
    {
      "footnote_id": "fn-active-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1112,
      "context": "...sing, artificially expanded datasets by creating new variations of existing samples. Active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort...",
      "full_line": "This reliance on large datasets introduced inefficiencies. Data collection became a costly and time-consuming endeavor, requiring vast amounts of labeled data for supervised learning tasks. To address these challenges, researchers developed techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning[^fn-efficient-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specific data [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creating new variations of existing samples. Active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort while maintaining performance [@Settles_2009]."
    },
    {
      "footnote_id": "fn-efficient-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1114,
      "context": "[^fn-efficient-transfer-learning]: **Transfer Learning**: Technique where models pre-trained on large datasets are fine-tuned for spe...",
      "full_line": "[^fn-efficient-transfer-learning]: **Transfer Learning**: Technique where models pre-trained on large datasets are fine-tuned for specific tasks. ImageNet pre-trained models can achieve high accuracy on new vision tasks with <1000 labeled examples vs. millions needed from scratch."
    },
    {
      "footnote_id": "fn-data-augmentation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1116,
      "context": "[^fn-data-augmentation]: **Data Augmentation**: Artificially expanding datasets through transformations like rotations, cro...",
      "full_line": "[^fn-data-augmentation]: **Data Augmentation**: Artificially expanding datasets through transformations like rotations, crops, or noise. Can improve model performance by 5-15% and reduce overfitting, especially valuable when labeled data is scarce."
    },
    {
      "footnote_id": "fn-active-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1118,
      "context": "[^fn-active-learning]: **Active Learning**: Iteratively selecting the most informative samples for labeling to maximize l...",
      "full_line": "[^fn-active-learning]: **Active Learning**: Iteratively selecting the most informative samples for labeling to maximize learning efficiency. Can achieve target performance with 50-90% less labeled data compared to random sampling strategies."
    },
    {
      "footnote_id": "fn-data-centric-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1126,
      "context": "Data-centric AI[^fn-data-centric-ai] has emerged as a key paradigm, emphasizing the importance of data quality over quantity. This appro...",
      "full_line": "Data-centric AI[^fn-data-centric-ai] has emerged as a key paradigm, emphasizing the importance of data quality over quantity. This approach focuses on enhancing data preprocessing, removing redundancy, and improving labeling efficiency. Research shows that careful curation and filtering of datasets can achieve comparable or superior model performance while using only a fraction of the original data volume. For instance, systematic analyses of web-scale datasets demonstrate that targeted filtering techniques can maintain model capabilities while reducing training data requirements [@penedo2024fineweb]."
    },
    {
      "footnote_id": "fn-data-centric-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1128,
      "context": "[^fn-data-centric-ai]: **Data-Centric AI**: Paradigm shift from model-centric to data-centric development, popularized by...",
      "full_line": "[^fn-data-centric-ai]: **Data-Centric AI**: Paradigm shift from model-centric to data-centric development, popularized by Andrew Ng in 2021. Focuses on systematically improving data quality rather than just model architecture, often yielding greater performance gains."
    },
    {
      "footnote_id": "fn-self-supervised",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1130,
      "context": "...techniques have emerged to support this transition toward data efficiency. Self-supervised learning[^fn-self-supervised] enables models to learn meaningful representations from unlabeled data, reducing the dependency on...",
      "full_line": "Several techniques have emerged to support this transition toward data efficiency. Self-supervised learning[^fn-self-supervised] enables models to learn meaningful representations from unlabeled data, reducing the dependency on expensive human-labeled datasets. Active learning strategies selectively identify the most informative examples for labeling, while curriculum learning[^fn-curriculum-learning] structures the training process to progress from simple to complex examples, improving learning efficiency. These approaches work together to minimize data requirements while maintaining model performance."
    },
    {
      "footnote_id": "fn-curriculum-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1130,
      "context": "...trategies selectively identify the most informative examples for labeling, while curriculum learning[^fn-curriculum-learning] structures the training process to progress from simple to complex examples, improving learning eff...",
      "full_line": "Several techniques have emerged to support this transition toward data efficiency. Self-supervised learning[^fn-self-supervised] enables models to learn meaningful representations from unlabeled data, reducing the dependency on expensive human-labeled datasets. Active learning strategies selectively identify the most informative examples for labeling, while curriculum learning[^fn-curriculum-learning] structures the training process to progress from simple to complex examples, improving learning efficiency. These approaches work together to minimize data requirements while maintaining model performance."
    },
    {
      "footnote_id": "fn-self-supervised",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1132,
      "context": "[^fn-self-supervised]: **Self-Supervised Learning**: Training method where models create their own labels from input data...",
      "full_line": "[^fn-self-supervised]: **Self-Supervised Learning**: Training method where models create their own labels from input data structure, like predicting masked words in BERT or next frames in videos. Enables learning from billions of unlabeled examples, revolutionizing NLP and computer vision."
    },
    {
      "footnote_id": "fn-curriculum-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1134,
      "context": "[^fn-curriculum-learning]: **Curriculum Learning**: Training strategy where models learn from easy examples before progressin...",
      "full_line": "[^fn-curriculum-learning]: **Curriculum Learning**: Training strategy where models learn from easy examples before progressing to harder ones, mimicking human education. Can improve convergence speed by 25-50% and final model performance across various domains."
    },
    {
      "footnote_id": "fn-efficient-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1136,
      "context": "The importance of data efficiency is particularly evident in foundation models[^fn-efficient-foundation-models]. As these models grow in scale and capability, they are approaching the limits of available high-qu...",
      "full_line": "The importance of data efficiency is particularly evident in foundation models[^fn-efficient-foundation-models]. As these models grow in scale and capability, they are approaching the limits of available high-quality training data, especially for language tasks [@fig-running-out-of-human-data]. This scarcity drives innovation in data processing and curation techniques, pushing the field to develop more sophisticated approaches to data efficiency."
    },
    {
      "footnote_id": "fn-efficient-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1138,
      "context": "[^fn-efficient-foundation-models]: **Foundation Models**: Large-scale, general-purpose AI models trained on broad data that can be ad...",
      "full_line": "[^fn-efficient-foundation-models]: **Foundation Models**: Large-scale, general-purpose AI models trained on broad data that can be adapted for many tasks. Term coined by Stanford HAI in 2021, includes models like GPT-3, BERT, and DALL-E with billions of parameters."
    },
    {
      "footnote_id": "fn-tinyml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1142,
      "context": "Evidence for the impact of data quality appears across different scales of deployment. In Tiny ML[^fn-tinyml] applications, datasets like Wake Vision demonstrate how model performance critically depends on car...",
      "full_line": "Evidence for the impact of data quality appears across different scales of deployment. In Tiny ML[^fn-tinyml] applications, datasets like Wake Vision demonstrate how model performance critically depends on careful data curation [@banbury2024wakevisiontailoreddataset]. At larger scales, research on language models trained on web-scale datasets shows that intelligent filtering and selection strategies can significantly improve performance on downstream tasks [@penedo2024fineweb]. @sec-benchmarking-ai establishes rigorous methodologies for measuring these data quality improvements across different scales and applications."
    },
    {
      "footnote_id": "fn-tinyml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1144,
      "context": "[^fn-tinyml]: **TinyML**: Machine learning on microcontrollers and edge devices with <1KB-1MB memory and <1mW po...",
      "full_line": "[^fn-tinyml]: **TinyML**: Machine learning on microcontrollers and edge devices with <1KB-1MB memory and <1mW power consumption. Enables AI in IoT devices, wearables, and sensors where traditional ML deployment is impossible due to resource constraints. These ultra-low-power chips contain a processor, memory, and peripherals on a single chip with dramatically limited resources."
    },
    {
      "footnote_id": "fn-npus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1212,
      "context": "...iciency is achieved through hardware accelerators like mobile GPUs or Neural Processing Units (NPUs)[^fn-npus], ensuring tasks are performed quickly while minimizing battery usage.",
      "full_line": "Mobile devices, such as smartphones, provide an accessible introduction to the interplay of efficiency dimensions. Consider a photo-editing application that uses machine learning to apply real-time filters. Compute efficiency is achieved through hardware accelerators like mobile GPUs or Neural Processing Units (NPUs)[^fn-npus], ensuring tasks are performed quickly while minimizing battery usage."
    },
    {
      "footnote_id": "fn-npus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1214,
      "context": "[^fn-npus]: **Neural Processing Units (NPUs)**: Specialized processors designed for AI workloads on mobile dev...",
      "full_line": "[^fn-npus]: **Neural Processing Units (NPUs)**: Specialized processors designed for AI workloads on mobile devices. Apple's A17 Pro contains a 16-core NPU delivering 35 TOPS (trillion operations per second) while consuming just 2-3 watts, enabling on-device AI without draining battery life."
    },
    {
      "footnote_id": "fn-lidar",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1230,
      "context": "...mance onboard hardware to process massive streams of sensor data, including data from cameras, LiDAR[^fn-lidar], and radar, in real time. These computations must be performed with minimal latency to ensure safe...",
      "full_line": "Edge deployments, such as those in autonomous vehicles, highlight the intricate balance required between real-time constraints and energy efficiency. Compute efficiency is central, as vehicles rely on high-performance onboard hardware to process massive streams of sensor data, including data from cameras, LiDAR[^fn-lidar], and radar, in real time. These computations must be performed with minimal latency to ensure safe navigation and split-second decision-making."
    },
    {
      "footnote_id": "fn-lidar",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1232,
      "context": "[^fn-lidar]: **LiDAR**: Light Detection and Ranging technology that uses laser pulses to create detailed 3D map...",
      "full_line": "[^fn-lidar]: **LiDAR**: Light Detection and Ranging technology that uses laser pulses to create detailed 3D maps of surroundings. Autonomous vehicles generate ~70GB of LiDAR data daily, requiring real-time processing of millions of distance measurements per second for obstacle detection and navigation."
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1316,
      "context": "...ike 8-bit quantization to reduce computation while maintaining quality, and employs batch processing[^fn-batch-processing] to handle multiple image queries simultaneously when users search large albums.",
      "full_line": "**Compute Efficiency** addresses how the model runs on user devices. Instead of requiring expensive GPUs, the optimized model runs efficiently on smartphone processors, consuming only 10% battery per hour of active use. The system uses techniques like 8-bit quantization to reduce computation while maintaining quality, and employs batch processing[^fn-batch-processing] to handle multiple image queries simultaneously when users search large albums."
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1318,
      "context": "[^fn-batch-processing]: **Batch Processing**: Technique that groups multiple inputs together for simultaneous processing,...",
      "full_line": "[^fn-batch-processing]: **Batch Processing**: Technique that groups multiple inputs together for simultaneous processing, improving computational efficiency. Instead of processing 100 images one-by-one (100 separate operations), batching processes them together in groups of 32, reducing overhead and improving GPU utilization by ~3-5\u00d7."
    },
    {
      "footnote_id": "fn-linear-transformations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 126,
      "context": "...nd matrix-vector multiplications because neural networks process data through linear transformations[^fn-linear-transformations] applied to multidimensional arrays. The Basic Linear Algebra Subprograms ([BLAS](https://www.netlib...",
      "full_line": "The foundation for modern ML frameworks begins at the core level of computation: matrix operations. Machine learning computations are primarily matrix-matrix and matrix-vector multiplications because neural networks process data through linear transformations[^fn-linear-transformations] applied to multidimensional arrays. The Basic Linear Algebra Subprograms ([BLAS](https://www.netlib.org/blas/))[^fn-frameworks-blas], developed in 1979, provided these essential matrix operations that would become the computational backbone of machine learning [@kung1979systolic]. These low-level operations, when combined and executed efficiently, enable the complex calculations required for training neural networks and other ML models."
    },
    {
      "footnote_id": "fn-frameworks-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 126,
      "context": "...multidimensional arrays. The Basic Linear Algebra Subprograms ([BLAS](https://www.netlib.org/blas/))[^fn-frameworks-blas], developed in 1979, provided these essential matrix operations that would become the computational...",
      "full_line": "The foundation for modern ML frameworks begins at the core level of computation: matrix operations. Machine learning computations are primarily matrix-matrix and matrix-vector multiplications because neural networks process data through linear transformations[^fn-linear-transformations] applied to multidimensional arrays. The Basic Linear Algebra Subprograms ([BLAS](https://www.netlib.org/blas/))[^fn-frameworks-blas], developed in 1979, provided these essential matrix operations that would become the computational backbone of machine learning [@kung1979systolic]. These low-level operations, when combined and executed efficiently, enable the complex calculations required for training neural networks and other ML models."
    },
    {
      "footnote_id": "fn-frameworks-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 128,
      "context": "[^fn-frameworks-blas]: **BLAS (Basic Linear Algebra Subprograms)**: Originally developed at Argonne National Laboratory,...",
      "full_line": "[^fn-frameworks-blas]: **BLAS (Basic Linear Algebra Subprograms)**: Originally developed at Argonne National Laboratory, BLAS became the de facto standard for linear algebra operations, with Level 1 (vector-vector), Level 2 (matrix-vector), and Level 3 (matrix-matrix) operations that still underpin every modern ML framework."
    },
    {
      "footnote_id": "fn-lapack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 130,
      "context": "Building upon BLAS, the Linear Algebra Package ([LAPACK](https://www.netlib.org/lapack/))[^fn-lapack] emerged in 1992, extending these capabilities with more sophisticated linear algebra operations suc...",
      "full_line": "Building upon BLAS, the Linear Algebra Package ([LAPACK](https://www.netlib.org/lapack/))[^fn-lapack] emerged in 1992, extending these capabilities with more sophisticated linear algebra operations such as matrix decompositions, eigenvalue problems, and linear system solutions. This layered approach of building increasingly complex operations from basic matrix computations became a defining characteristic of ML frameworks."
    },
    {
      "footnote_id": "fn-lapack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 132,
      "context": "[^fn-lapack]: **LAPACK (Linear Algebra Package)**: Succeeded LINPACK and EISPACK, introducing block algorithms t...",
      "full_line": "[^fn-lapack]: **LAPACK (Linear Algebra Package)**: Succeeded LINPACK and EISPACK, introducing block algorithms that dramatically improved cache efficiency and parallel execution, innovations that became essential as datasets grew from megabytes to terabytes."
    },
    {
      "footnote_id": "fn-theano",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 144,
      "context": "[Theano](https://github.com/Theano/Theano)[^fn-theano], developed at the Montreal Institute for Learning Algorithms (MILA) and appearing in 2007, was a ma...",
      "full_line": "[Theano](https://github.com/Theano/Theano)[^fn-theano], developed at the Montreal Institute for Learning Algorithms (MILA) and appearing in 2007, was a major advancement that introduced two revolutionary concepts: computational graphs[^fn-comp-graphs] and GPU acceleration [@al2016theano]. Computational graphs represented mathematical operations as directed graphs, with matrix operations as nodes and data flowing between them. This graph-based approach allowed for automatic differentiation and optimization of the underlying matrix operations. More importantly, it enabled the framework to automatically route these operations to GPU hardware, dramatically accelerating matrix computations."
    },
    {
      "footnote_id": "fn-comp-graphs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 144,
      "context": "...ng in 2007, was a major advancement that introduced two revolutionary concepts: computational graphs[^fn-comp-graphs] and GPU acceleration [@al2016theano]. Computational graphs represented mathematical operations as d...",
      "full_line": "[Theano](https://github.com/Theano/Theano)[^fn-theano], developed at the Montreal Institute for Learning Algorithms (MILA) and appearing in 2007, was a major advancement that introduced two revolutionary concepts: computational graphs[^fn-comp-graphs] and GPU acceleration [@al2016theano]. Computational graphs represented mathematical operations as directed graphs, with matrix operations as nodes and data flowing between them. This graph-based approach allowed for automatic differentiation and optimization of the underlying matrix operations. More importantly, it enabled the framework to automatically route these operations to GPU hardware, dramatically accelerating matrix computations."
    },
    {
      "footnote_id": "fn-theano",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 146,
      "context": "[^fn-theano]: **Theano**: Named after the ancient Greek mathematician Theano of Croton, this framework pioneered...",
      "full_line": "[^fn-theano]: **Theano**: Named after the ancient Greek mathematician Theano of Croton, this framework pioneered the concept of symbolic mathematical expressions in Python, laying the groundwork for every modern deep learning framework."
    },
    {
      "footnote_id": "fn-comp-graphs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 148,
      "context": "[^fn-comp-graphs]: **Computational Graphs**: First formalized in automatic differentiation literature by Wengert (196...",
      "full_line": "[^fn-comp-graphs]: **Computational Graphs**: First formalized in automatic differentiation literature by Wengert (1964), this representation became the backbone of modern ML frameworks, enabling both forward and reverse-mode differentiation at unprecedented scale."
    },
    {
      "footnote_id": "fn-eager-execution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 150,
      "context": "...oach to handling matrix operations. It emphasized immediate execution of operations (eager execution[^fn-eager-execution]) and provided a flexible interface for neural network implementations.",
      "full_line": "Meanwhile, [Torch7](http://torch.ch/) (the Lua-based predecessor to PyTorch), created at NYU in 2002, took a different approach to handling matrix operations. It emphasized immediate execution of operations (eager execution[^fn-eager-execution]) and provided a flexible interface for neural network implementations."
    },
    {
      "footnote_id": "fn-eager-execution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 152,
      "context": "[^fn-eager-execution]: **Eager Execution**: An execution model where operations are evaluated immediately as they are cal...",
      "full_line": "[^fn-eager-execution]: **Eager Execution**: An execution model where operations are evaluated immediately as they are called, similar to standard Python execution. Pioneered by Torch in 2002, this approach prioritizes developer productivity and debugging ease over performance optimization, becoming the default mode in modern frameworks like PyTorch and TensorFlow 2.x. Torch's design philosophy of prioritizing developer experience while maintaining high performance established design patterns that would later influence frameworks like PyTorch. Its architecture demonstrated how to balance high-level abstractions with efficient low-level matrix operations."
    },
    {
      "footnote_id": "fn-tensorflow",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 162,
      "context": "Google's [TensorFlow](https://www.tensorflow.org/)[^fn-tensorflow], introduced in 2015, revolutionized the field by treating matrix operations as part of a distribute...",
      "full_line": "Google's [TensorFlow](https://www.tensorflow.org/)[^fn-tensorflow], introduced in 2015, revolutionized the field by treating matrix operations as part of a distributed computing problem [@dean2012large]. It represented all computations, from individual matrix multiplications to entire neural networks, as a static computational graph[^fn-static-graph] that could be split across multiple devices. This approach enabled training of unprecedented model sizes by distributing matrix operations across clusters of computers and specialized hardware. TensorFlow's static graph approach, while initially constraining, allowed for aggressive optimization of matrix operations through techniques like kernel fusion[^fn-kernel-fusion] (combining multiple operations into a single kernel for efficiency) and memory planning[^fn-memory-planning] (pre-allocating memory for operations)."
    },
    {
      "footnote_id": "fn-static-graph",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 162,
      "context": "...s, from individual matrix multiplications to entire neural networks, as a static computational graph[^fn-static-graph] that could be split across multiple devices. This approach enabled training of unprecedented model...",
      "full_line": "Google's [TensorFlow](https://www.tensorflow.org/)[^fn-tensorflow], introduced in 2015, revolutionized the field by treating matrix operations as part of a distributed computing problem [@dean2012large]. It represented all computations, from individual matrix multiplications to entire neural networks, as a static computational graph[^fn-static-graph] that could be split across multiple devices. This approach enabled training of unprecedented model sizes by distributing matrix operations across clusters of computers and specialized hardware. TensorFlow's static graph approach, while initially constraining, allowed for aggressive optimization of matrix operations through techniques like kernel fusion[^fn-kernel-fusion] (combining multiple operations into a single kernel for efficiency) and memory planning[^fn-memory-planning] (pre-allocating memory for operations)."
    },
    {
      "footnote_id": "fn-kernel-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 162,
      "context": "...ning, allowed for aggressive optimization of matrix operations through techniques like kernel fusion[^fn-kernel-fusion] (combining multiple operations into a single kernel for efficiency) and memory planning[^fn-memory-...",
      "full_line": "Google's [TensorFlow](https://www.tensorflow.org/)[^fn-tensorflow], introduced in 2015, revolutionized the field by treating matrix operations as part of a distributed computing problem [@dean2012large]. It represented all computations, from individual matrix multiplications to entire neural networks, as a static computational graph[^fn-static-graph] that could be split across multiple devices. This approach enabled training of unprecedented model sizes by distributing matrix operations across clusters of computers and specialized hardware. TensorFlow's static graph approach, while initially constraining, allowed for aggressive optimization of matrix operations through techniques like kernel fusion[^fn-kernel-fusion] (combining multiple operations into a single kernel for efficiency) and memory planning[^fn-memory-planning] (pre-allocating memory for operations)."
    },
    {
      "footnote_id": "fn-memory-planning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 162,
      "context": "...rnel-fusion] (combining multiple operations into a single kernel for efficiency) and memory planning[^fn-memory-planning] (pre-allocating memory for operations).",
      "full_line": "Google's [TensorFlow](https://www.tensorflow.org/)[^fn-tensorflow], introduced in 2015, revolutionized the field by treating matrix operations as part of a distributed computing problem [@dean2012large]. It represented all computations, from individual matrix multiplications to entire neural networks, as a static computational graph[^fn-static-graph] that could be split across multiple devices. This approach enabled training of unprecedented model sizes by distributing matrix operations across clusters of computers and specialized hardware. TensorFlow's static graph approach, while initially constraining, allowed for aggressive optimization of matrix operations through techniques like kernel fusion[^fn-kernel-fusion] (combining multiple operations into a single kernel for efficiency) and memory planning[^fn-memory-planning] (pre-allocating memory for operations)."
    },
    {
      "footnote_id": "fn-tensorflow",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 164,
      "context": "[^fn-tensorflow]: **TensorFlow**: Named after tensor operations flowing through computational graphs, this framework...",
      "full_line": "[^fn-tensorflow]: **TensorFlow**: Named after tensor operations flowing through computational graphs, this framework democratized distributed machine learning by open-sourcing Google's internal DistBelief system, instantly giving researchers access to infrastructure that previously required massive corporate resources."
    },
    {
      "footnote_id": "fn-kernel-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 166,
      "context": "[^fn-kernel-fusion]: **Kernel Fusion**: An optimization technique that combines multiple separate operations (like matr...",
      "full_line": "[^fn-kernel-fusion]: **Kernel Fusion**: An optimization technique that combines multiple separate operations (like matrix multiplication followed by bias addition and activation) into a single GPU kernel, reducing memory bandwidth requirements by up to 10x and eliminating intermediate memory allocations. This optimization is particularly crucial for complex deep learning models with thousands of operations."
    },
    {
      "footnote_id": "fn-memory-planning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 168,
      "context": "[^fn-memory-planning]: **Memory Planning**: A framework optimization that pre-analyzes computational graphs to determine...",
      "full_line": "[^fn-memory-planning]: **Memory Planning**: A framework optimization that pre-analyzes computational graphs to determine optimal memory allocation strategies, enabling techniques like in-place operations and memory reuse patterns that can reduce peak memory usage by 40-60% during training."
    },
    {
      "footnote_id": "fn-static-graph",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 170,
      "context": "[^fn-static-graph]: **Static Computational Graph**: A pre-defined computation structure where the entire model archite...",
      "full_line": "[^fn-static-graph]: **Static Computational Graph**: A pre-defined computation structure where the entire model architecture is specified before execution, enabling global optimizations and efficient memory planning. Pioneered by TensorFlow 1.x, this approach sacrifices runtime flexibility for maximum performance optimization, making it ideal for production deployments."
    },
    {
      "footnote_id": "fn-gradient-accumulation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 172,
      "context": "[^fn-gradient-accumulation]: **Gradient Accumulation**: A training technique where gradients from multiple mini-batches are com...",
      "full_line": "[^fn-gradient-accumulation]: **Gradient Accumulation**: A training technique where gradients from multiple mini-batches are computed and summed before updating model parameters, effectively simulating larger batch sizes without requiring additional memory. Essential for training large models where memory constraints limit batch size to as small as 1 sample per device."
    },
    {
      "footnote_id": "fn-gradient-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 174,
      "context": "[^fn-gradient-checkpointing]: **Gradient Checkpointing**: A memory optimization technique that trades computation time for memor...",
      "full_line": "[^fn-gradient-checkpointing]: **Gradient Checkpointing**: A memory optimization technique that trades computation time for memory by selectively storing only certain intermediate activations during the forward pass, then recomputing discarded values during gradient computation. Can reduce memory usage by 50-90% for deep networks while increasing training time by only 20-33%."
    },
    {
      "footnote_id": "fn-asic-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 176,
      "context": "[^fn-asic-ml]: **ASIC (Application-Specific Integrated Circuit)**: Custom silicon chips designed for specific tas...",
      "full_line": "[^fn-asic-ml]: **ASIC (Application-Specific Integrated Circuit)**: Custom silicon chips designed for specific tasks, contrasting with general-purpose CPUs. In ML contexts, ASICs like Google's TPUs and Tesla's FSD chips sacrifice flexibility for 10-100x efficiency gains in matrix operations, though they require 2-4 years development time and millions in upfront costs."
    },
    {
      "footnote_id": "fn-pytorch",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 180,
      "context": "Facebook's [PyTorch](https://pytorch.org/)[^fn-pytorch], also launched in 2016, took a radically different approach to handling matrix computations. Instea...",
      "full_line": "Facebook's [PyTorch](https://pytorch.org/)[^fn-pytorch], also launched in 2016, took a radically different approach to handling matrix computations. Instead of static graphs, PyTorch introduced dynamic computational graphs that could be modified on the fly [@paszke2019pytorch]. This dynamic approach, while potentially sacrificing some optimization opportunities, made it much easier for researchers to debug and understand the flow of matrix operations in their models. PyTorch's success demonstrated that the ability to introspect and modify computations dynamically was as important as raw performance for many applications."
    },
    {
      "footnote_id": "fn-pytorch",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 182,
      "context": "[^fn-pytorch]: **PyTorch**: Inspired by the original Torch framework from NYU, PyTorch brought \"define-by-run\" se...",
      "full_line": "[^fn-pytorch]: **PyTorch**: Inspired by the original Torch framework from NYU, PyTorch brought \"define-by-run\" semantics to Python, enabling researchers to modify models during execution, a breakthrough that accelerated research by making debugging as simple as using a standard Python debugger."
    },
    {
      "footnote_id": "fn-jax",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 188,
      "context": "Google's [JAX](https://github.com/google/jax)[^fn-jax], introduced in 2018, brought functional programming principles to deep learning computations, enabl...",
      "full_line": "Google's [JAX](https://github.com/google/jax)[^fn-jax], introduced in 2018, brought functional programming principles to deep learning computations, enabling new patterns of model development [@jax2018github]. [FastAI](https://www.fast.ai/) built upon PyTorch to package common deep learning patterns into reusable components, making advanced techniques more accessible to practitioners [@howard2020fastai]. These higher-level frameworks demonstrated how abstraction could simplify development while maintaining the performance benefits of their underlying implementations."
    },
    {
      "footnote_id": "fn-jax",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 190,
      "context": "[^fn-jax]: **JAX**: Stands for \"Just After eXecution\" and combines NumPy's API with functional programming tr...",
      "full_line": "[^fn-jax]: **JAX**: Stands for \"Just After eXecution\" and combines NumPy's API with functional programming transforms (jit, grad, vmap, pmap), enabling researchers to write concise code that automatically scales to TPUs and GPU clusters while maintaining NumPy compatibility."
    },
    {
      "footnote_id": "fn-frameworks-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 200,
      "context": "...utionized framework design. [Google's Tensor Processing Units (TPUs)](https://cloud.google.com/tpu/)[^fn-frameworks-tpu], first deployed in 2016, were purpose-built for tensor operations, the essential building blocks of...",
      "full_line": "The development of hardware-specific accelerators further revolutionized framework design. [Google's Tensor Processing Units (TPUs)](https://cloud.google.com/tpu/)[^fn-frameworks-tpu], first deployed in 2016, were purpose-built for tensor operations, the essential building blocks of deep learning computations. TPUs introduced systolic array[^fn-systolic-array] architectures, which are particularly efficient for matrix multiplication and convolution operations. This hardware architecture prompted frameworks like TensorFlow to develop specialized compilation strategies that could map high-level operations directly to TPU instructions, bypassing traditional CPU-oriented optimizations."
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 200,
      "context": "...rations, the essential building blocks of deep learning computations. TPUs introduced systolic array[^fn-systolic-array] architectures, which are particularly efficient for matrix multiplication and convolution operation...",
      "full_line": "The development of hardware-specific accelerators further revolutionized framework design. [Google's Tensor Processing Units (TPUs)](https://cloud.google.com/tpu/)[^fn-frameworks-tpu], first deployed in 2016, were purpose-built for tensor operations, the essential building blocks of deep learning computations. TPUs introduced systolic array[^fn-systolic-array] architectures, which are particularly efficient for matrix multiplication and convolution operations. This hardware architecture prompted frameworks like TensorFlow to develop specialized compilation strategies that could map high-level operations directly to TPU instructions, bypassing traditional CPU-oriented optimizations."
    },
    {
      "footnote_id": "fn-frameworks-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 204,
      "context": "[^fn-frameworks-tpu]: **TPU (Tensor Processing Unit)**: Google's custom ASIC achieved 15-30x better performance-per-watt...",
      "full_line": "[^fn-frameworks-tpu]: **TPU (Tensor Processing Unit)**: Google's custom ASIC achieved 15-30x better performance-per-watt than contemporary GPUs and CPUs for neural networks, proving that domain-specific architectures could outperform general-purpose processors for ML workloads."
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 206,
      "context": "[^fn-systolic-array]: **Systolic Array**: A specialized parallel computing architecture invented by H.T. Kung (CMU) and...",
      "full_line": "[^fn-systolic-array]: **Systolic Array**: A specialized parallel computing architecture invented by H.T. Kung (CMU) and Charles Leiserson (MIT) in 1978, where data flows through a grid of processing elements in a rhythmic, pipeline fashion. Each element performs simple operations on data flowing from neighbors, making it exceptionally efficient for matrix operations, which form the heart of neural network computations."
    },
    {
      "footnote_id": "fn-asic-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 217,
      "context": "The emergence of custom ASIC[^fn-asic-ml] (Application-Specific Integrated Circuit) solutions has further diversified the hardware landscape....",
      "full_line": "The emergence of custom ASIC[^fn-asic-ml] (Application-Specific Integrated Circuit) solutions has further diversified the hardware landscape. Companies like [Graphcore](https://www.graphcore.ai/), [Cerebras](https://www.cerebras.net/), and [SambaNova](https://sambanova.ai/) have developed unique architectures for matrix computation, each with different strengths and optimization opportunities. This proliferation of specialized hardware has pushed frameworks to adopt more flexible intermediate representations[^fn-intermediate-representation] of matrix operations, allowing for target-specific optimization while maintaining a common high-level interface."
    },
    {
      "footnote_id": "fn-intermediate-representation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 217,
      "context": "...on of specialized hardware has pushed frameworks to adopt more flexible intermediate representations[^fn-intermediate-representation] of matrix operations, allowing for target-specific optimization while maintaining a common high-lev...",
      "full_line": "The emergence of custom ASIC[^fn-asic-ml] (Application-Specific Integrated Circuit) solutions has further diversified the hardware landscape. Companies like [Graphcore](https://www.graphcore.ai/), [Cerebras](https://www.cerebras.net/), and [SambaNova](https://sambanova.ai/) have developed unique architectures for matrix computation, each with different strengths and optimization opportunities. This proliferation of specialized hardware has pushed frameworks to adopt more flexible intermediate representations[^fn-intermediate-representation] of matrix operations, allowing for target-specific optimization while maintaining a common high-level interface."
    },
    {
      "footnote_id": "fn-intermediate-representation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 219,
      "context": "[^fn-intermediate-representation]: **Intermediate Representation (IR)**: A framework-internal format that sits between high-level use...",
      "full_line": "[^fn-intermediate-representation]: **Intermediate Representation (IR)**: A framework-internal format that sits between high-level user code and hardware-specific machine code, enabling optimizations and cross-platform deployment. Modern ML frameworks use IRs like TensorFlow's XLA or PyTorch's TorchScript to compile the same model for CPUs, GPUs, TPUs, and mobile devices."
    },
    {
      "footnote_id": "fn-linear-transformations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 221,
      "context": "[^fn-linear-transformations]: **Linear Transformations**: Mathematical operations that preserve vector addition and scalar multi...",
      "full_line": "[^fn-linear-transformations]: **Linear Transformations**: Mathematical operations that preserve vector addition and scalar multiplication, typically implemented as matrix multiplication in neural networks. Each layer applies a learned linear transformation (weights matrix) followed by a non-linear activation function (like ReLU or sigmoid), enabling networks to learn complex patterns from simple mathematical building blocks."
    },
    {
      "footnote_id": "fn-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 223,
      "context": "[^fn-data-parallelism]: **Data Parallelism**: A distributed training strategy where identical model copies process differe...",
      "full_line": "[^fn-data-parallelism]: **Data Parallelism**: A distributed training strategy where identical model copies process different data subsets in parallel, then synchronize gradients. Enables near-linear speedup with additional devices but requires models that fit in single-device memory, making it ideal for training on datasets with billions of samples."
    },
    {
      "footnote_id": "fn-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 225,
      "context": "[^fn-model-parallelism]: **Model Parallelism**: A strategy for training models too large for single devices by partitioning...",
      "full_line": "[^fn-model-parallelism]: **Model Parallelism**: A strategy for training models too large for single devices by partitioning the model architecture across multiple processors. Essential for models like GPT-3 (175B parameters) that exceed GPU memory limits, though it requires careful optimization to minimize communication overhead between model partitions."
    },
    {
      "footnote_id": "fn-operation-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 227,
      "context": "[^fn-operation-fusion]: **Operation Fusion**: Compiler optimization that combines multiple sequential operations into a si...",
      "full_line": "[^fn-operation-fusion]: **Operation Fusion**: Compiler optimization that combines multiple sequential operations into a single kernel to reduce memory bandwidth and latency. For example, fusing matrix multiplication, bias addition, and ReLU activation can eliminate intermediate memory allocations and achieve 2-3x speedup on modern GPUs."
    },
    {
      "footnote_id": "fn-jit-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 229,
      "context": "[^fn-jit-ml]: **Just-In-Time (JIT) Compilation**: In ML frameworks, JIT compilation differs from traditional JIT...",
      "full_line": "[^fn-jit-ml]: **Just-In-Time (JIT) Compilation**: In ML frameworks, JIT compilation differs from traditional JIT by optimizing for tensor operations and hardware accelerators rather than general CPU instructions. ML JIT compilers like TensorFlow's XLA and PyTorch's TorchScript analyze computation patterns at runtime to generate optimized kernels for specific tensor shapes and device capabilities."
    },
    {
      "footnote_id": "fn-dag-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 335,
      "context": "...hardware execution [@baydin2018], representing a machine learning model as a directed acyclic graph[^fn-dag-ml] (DAG) where nodes represent operations and edges represent data flow. This abstraction becomes esse...",
      "full_line": "Computational graphs emerged as a fundamental abstraction in machine learning frameworks to address the growing complexity of deep learning models. As models grew larger and more sophisticated, efficient execution across diverse hardware platforms became necessary. The computational graph bridges the gap between high-level model descriptions and low-level hardware execution [@baydin2018], representing a machine learning model as a directed acyclic graph[^fn-dag-ml] (DAG) where nodes represent operations and edges represent data flow. This abstraction becomes essential for the training algorithms detailed in @sec-ai-training, where the graph structure enables automatic differentiation and gradient computation that powers modern deep learning optimization."
    },
    {
      "footnote_id": "fn-dag-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 339,
      "context": "[^fn-dag-ml]: **Directed Acyclic Graph (DAG)**: In machine learning frameworks, DAGs represent computation where...",
      "full_line": "[^fn-dag-ml]: **Directed Acyclic Graph (DAG)**: In machine learning frameworks, DAGs represent computation where nodes are operations (like matrix multiplication or activation functions) and edges are data dependencies. Unlike general DAGs in computer science, ML computational graphs specifically optimize for automatic differentiation, enabling frameworks to compute gradients by traversing the graph in reverse order."
    },
    {
      "footnote_id": "fn-auto-diff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 645,
      "context": "...g this to a neural network with millions of operations. This is where automatic differentiation (AD)[^fn-auto-diff] becomes essential.",
      "full_line": "Even in this basic example, computing derivatives manually would require careful application of calculus rules - the product rule, the chain rule, and derivatives of trigonometric functions. Now imagine scaling this to a neural network with millions of operations. This is where automatic differentiation (AD)[^fn-auto-diff] becomes essential."
    },
    {
      "footnote_id": "fn-auto-diff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 647,
      "context": "[^fn-auto-diff]: **Automatic Differentiation**: Invented by Robert Edwin Wengert in 1964, this technique achieves m...",
      "full_line": "[^fn-auto-diff]: **Automatic Differentiation**: Invented by Robert Edwin Wengert in 1964, this technique achieves machine precision derivatives by applying the chain rule at the elementary operation level, making neural network training computationally feasible for networks with millions of parameters."
    },
    {
      "footnote_id": "fn-gradient-accumulation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 844,
      "context": ".... Modern frameworks accomplish this through computational graphs and automatic gradient accumulation[^fn-gradient-accumulation].",
      "full_line": "The implementation of reverse mode in machine learning frameworks requires careful orchestration of computation and memory. While forward mode simply augments each computation, reverse mode needs to maintain a record of the forward computation to enable the backward pass. Modern frameworks accomplish this through computational graphs and automatic gradient accumulation[^fn-gradient-accumulation]."
    },
    {
      "footnote_id": "fn-gradient-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 942,
      "context": "Modern frameworks implement gradient checkpointing[^fn-gradient-checkpointing], a technique that strategically balances computation and memory. A simplified forward pass of such...",
      "full_line": "Modern frameworks implement gradient checkpointing[^fn-gradient-checkpointing], a technique that strategically balances computation and memory. A simplified forward pass of such a network is shown in @lst-deep_forward."
    },
    {
      "footnote_id": "fn-operation-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 970,
      "context": "Another crucial optimization involves operation fusion[^fn-operation-fusion]. Rather than treating each mathematical operation separately, frameworks combine operations that co...",
      "full_line": "Another crucial optimization involves operation fusion[^fn-operation-fusion]. Rather than treating each mathematical operation separately, frameworks combine operations that commonly occur together. Matrix multiplication followed by bias addition, for instance, can be fused into a single operation, reducing memory transfers and improving hardware utilization."
    },
    {
      "footnote_id": "fn-jit-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 1776,
      "context": "Just-In-Time compilation[^fn-jit-ml] is a middle ground between eager execution and graph execution. This paradigm aims to combine the f...",
      "full_line": "Just-In-Time compilation[^fn-jit-ml] is a middle ground between eager execution and graph execution. This paradigm aims to combine the flexibility of eager execution with the performance benefits of graph optimization."
    },
    {
      "footnote_id": "fn-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 1835,
      "context": "At the essence of distributed execution are two primary strategies: data parallelism[^fn-data-parallelism] and model parallelism[^fn-model-parallelism]. Data parallelism allows multiple devices to train the...",
      "full_line": "At the essence of distributed execution are two primary strategies: data parallelism[^fn-data-parallelism] and model parallelism[^fn-model-parallelism]. Data parallelism allows multiple devices to train the same model on different subsets of data, ensuring faster convergence without increasing memory requirements. Model parallelism, on the other hand, partitions the model itself across multiple devices, allowing the training of architectures too large to fit into a single device\u2019s memory. While model parallelism comes in several variations [explored in detail in @sec-ai-training], both techniques are essential for training modern machine learning models efficiently. These distributed execution strategies become increasingly important as models scale to the sizes discussed in @sec-efficient-ai, and their implementation requires the hardware acceleration techniques covered in @sec-ai-acceleration."
    },
    {
      "footnote_id": "fn-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 1835,
      "context": "...d execution are two primary strategies: data parallelism[^fn-data-parallelism] and model parallelism[^fn-model-parallelism]. Data parallelism allows multiple devices to train the same model on different subsets of data, ens...",
      "full_line": "At the essence of distributed execution are two primary strategies: data parallelism[^fn-data-parallelism] and model parallelism[^fn-model-parallelism]. Data parallelism allows multiple devices to train the same model on different subsets of data, ensuring faster convergence without increasing memory requirements. Model parallelism, on the other hand, partitions the model itself across multiple devices, allowing the training of architectures too large to fit into a single device\u2019s memory. While model parallelism comes in several variations [explored in detail in @sec-ai-training], both techniques are essential for training modern machine learning models efficiently. These distributed execution strategies become increasingly important as models scale to the sizes discussed in @sec-efficient-ai, and their implementation requires the hardware acceleration techniques covered in @sec-ai-acceleration."
    },
    {
      "footnote_id": "immutable-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2965,
      "context": "JAX embraces functional programming principles with immutable data structures[^immutable-data] and explicit parameter management. The `simple_net` function implements the linear transformation m...",
      "full_line": "JAX embraces functional programming principles with immutable data structures[^immutable-data] and explicit parameter management. The `simple_net` function implements the linear transformation manually using `jnp.dot(x, params['w']) + params['b']`, explicitly performing the matrix multiplication and bias addition that PyTorch and TensorFlow handle automatically. Parameters are stored in a dictionary structure (`params`) containing weights `'w'` and bias `'b'`, initialized separately using JAX's random number generation with explicit seeding (`random.PRNGKey(0)`). This separation means the model function is stateless[^stateless-function]\u2014it contains no parameters internally and depends entirely on external parameter passing. This design enables powerful program transformations like automatic vectorization[^vectorization] (`vmap`), just-in-time compilation[^jit-compilation] (`jit`), and automatic differentiation (`grad`) because the function remains mathematically pure[^pure-function] without hidden state or side effects."
    },
    {
      "footnote_id": "stateless-function",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2965,
      "context": "...n with explicit seeding (`random.PRNGKey(0)`). This separation means the model function is stateless[^stateless-function]\u2014it contains no parameters internally and depends entirely on external parameter passing. This desig...",
      "full_line": "JAX embraces functional programming principles with immutable data structures[^immutable-data] and explicit parameter management. The `simple_net` function implements the linear transformation manually using `jnp.dot(x, params['w']) + params['b']`, explicitly performing the matrix multiplication and bias addition that PyTorch and TensorFlow handle automatically. Parameters are stored in a dictionary structure (`params`) containing weights `'w'` and bias `'b'`, initialized separately using JAX's random number generation with explicit seeding (`random.PRNGKey(0)`). This separation means the model function is stateless[^stateless-function]\u2014it contains no parameters internally and depends entirely on external parameter passing. This design enables powerful program transformations like automatic vectorization[^vectorization] (`vmap`), just-in-time compilation[^jit-compilation] (`jit`), and automatic differentiation (`grad`) because the function remains mathematically pure[^pure-function] without hidden state or side effects."
    },
    {
      "footnote_id": "vectorization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2965,
      "context": "...parameter passing. This design enables powerful program transformations like automatic vectorization[^vectorization] (`vmap`), just-in-time compilation[^jit-compilation] (`jit`), and automatic differentiation (`grad`...",
      "full_line": "JAX embraces functional programming principles with immutable data structures[^immutable-data] and explicit parameter management. The `simple_net` function implements the linear transformation manually using `jnp.dot(x, params['w']) + params['b']`, explicitly performing the matrix multiplication and bias addition that PyTorch and TensorFlow handle automatically. Parameters are stored in a dictionary structure (`params`) containing weights `'w'` and bias `'b'`, initialized separately using JAX's random number generation with explicit seeding (`random.PRNGKey(0)`). This separation means the model function is stateless[^stateless-function]\u2014it contains no parameters internally and depends entirely on external parameter passing. This design enables powerful program transformations like automatic vectorization[^vectorization] (`vmap`), just-in-time compilation[^jit-compilation] (`jit`), and automatic differentiation (`grad`) because the function remains mathematically pure[^pure-function] without hidden state or side effects."
    },
    {
      "footnote_id": "jit-compilation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2965,
      "context": "...gram transformations like automatic vectorization[^vectorization] (`vmap`), just-in-time compilation[^jit-compilation] (`jit`), and automatic differentiation (`grad`) because the function remains mathematically pure[^p...",
      "full_line": "JAX embraces functional programming principles with immutable data structures[^immutable-data] and explicit parameter management. The `simple_net` function implements the linear transformation manually using `jnp.dot(x, params['w']) + params['b']`, explicitly performing the matrix multiplication and bias addition that PyTorch and TensorFlow handle automatically. Parameters are stored in a dictionary structure (`params`) containing weights `'w'` and bias `'b'`, initialized separately using JAX's random number generation with explicit seeding (`random.PRNGKey(0)`). This separation means the model function is stateless[^stateless-function]\u2014it contains no parameters internally and depends entirely on external parameter passing. This design enables powerful program transformations like automatic vectorization[^vectorization] (`vmap`), just-in-time compilation[^jit-compilation] (`jit`), and automatic differentiation (`grad`) because the function remains mathematically pure[^pure-function] without hidden state or side effects."
    },
    {
      "footnote_id": "pure-function",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2965,
      "context": "...on] (`jit`), and automatic differentiation (`grad`) because the function remains mathematically pure[^pure-function] without hidden state or side effects.",
      "full_line": "JAX embraces functional programming principles with immutable data structures[^immutable-data] and explicit parameter management. The `simple_net` function implements the linear transformation manually using `jnp.dot(x, params['w']) + params['b']`, explicitly performing the matrix multiplication and bias addition that PyTorch and TensorFlow handle automatically. Parameters are stored in a dictionary structure (`params`) containing weights `'w'` and bias `'b'`, initialized separately using JAX's random number generation with explicit seeding (`random.PRNGKey(0)`). This separation means the model function is stateless[^stateless-function]\u2014it contains no parameters internally and depends entirely on external parameter passing. This design enables powerful program transformations like automatic vectorization[^vectorization] (`vmap`), just-in-time compilation[^jit-compilation] (`jit`), and automatic differentiation (`grad`) because the function remains mathematically pure[^pure-function] without hidden state or side effects."
    },
    {
      "footnote_id": "immutable-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2967,
      "context": "[^immutable-data]: **Immutable Data Structures**: Cannot be modified after creation. Any operation that appears to ch...",
      "full_line": "[^immutable-data]: **Immutable Data Structures**: Cannot be modified after creation. Any operation that appears to change the data actually creates a new copy, ensuring that the original data remains unchanged. This prevents accidental modifications and enables safe parallel processing."
    },
    {
      "footnote_id": "stateless-function",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2969,
      "context": "[^stateless-function]: **Stateless Function**: Produces the same output for the same inputs every time, without relying o...",
      "full_line": "[^stateless-function]: **Stateless Function**: Produces the same output for the same inputs every time, without relying on or modifying any external state. This predictability is essential for mathematical optimization and parallel execution."
    },
    {
      "footnote_id": "vectorization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2971,
      "context": "[^vectorization]: **Automatic Vectorization**: Transforms operations on single data points into operations on entire...",
      "full_line": "[^vectorization]: **Automatic Vectorization**: Transforms operations on single data points into operations on entire arrays or batches, significantly improving computational efficiency by leveraging SIMD (Single Instruction, Multiple Data) processor capabilities."
    },
    {
      "footnote_id": "jit-compilation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2973,
      "context": "[^jit-compilation]: **Just-in-Time (JIT) Compilation**: Translates high-level code into optimized machine code at runt...",
      "full_line": "[^jit-compilation]: **Just-in-Time (JIT) Compilation**: Translates high-level code into optimized machine code at runtime, enabling performance optimizations based on actual data shapes and hardware characteristics."
    },
    {
      "footnote_id": "pure-function",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2975,
      "context": "[^pure-function]: **Pure Function**: Has no side effects and always returns the same output for the same inputs. Pur...",
      "full_line": "[^pure-function]: **Pure Function**: Has no side effects and always returns the same output for the same inputs. Pure functions enable mathematical reasoning about code behavior and safe program transformations."
    },
    {
      "footnote_id": "fn-llm-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 45,
      "context": "...e year 2025 marks an inflection point in artificial intelligence capabilities. Large Language Models[^fn-llm-definition] including GPT-4, Claude, and Gemini demonstrate competencies previously considered unattainable: co...",
      "full_line": "The year 2025 marks an inflection point in artificial intelligence capabilities. Large Language Models[^fn-llm-definition] including GPT-4, Claude, and Gemini demonstrate competencies previously considered unattainable: code generation, visual analysis, multi-step reasoning, and natural language interaction. These capabilities result from the transformer architectures detailed in @sec-dnn-architectures, scaled through the distributed training methodologies of @sec-ai-training. These systems result from systematic integration of established machine learning components rather than algorithmic breakthroughs."
    },
    {
      "footnote_id": "fn-llm-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 47,
      "context": "[^fn-llm-definition]: **Large Language Models (LLMs)**: Neural networks trained on internet-scale text corpora, typicall...",
      "full_line": "[^fn-llm-definition]: **Large Language Models (LLMs)**: Neural networks trained on internet-scale text corpora, typically containing >10 billion parameters. GPT-3 (175B parameters) required approximately 3.14 \u00d7 10\u00b2\u00b3 floating-point operations for training. GPT-4's exact size remains undisclosed but estimates suggest over 1 trillion parameters distributed across multiple expert models."
    },
    {
      "footnote_id": "fn-chatgpt-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 49,
      "context": "ChatGPT's acquisition of 100 million users within two months[^fn-chatgpt-growth] represents successful systems integration rather than algorithmic innovation. The transformer archi...",
      "full_line": "ChatGPT's acquisition of 100 million users within two months[^fn-chatgpt-growth] represents successful systems integration rather than algorithmic innovation. The transformer architecture scales to hundreds of billions of parameters through architectural principles established in @sec-dnn-architectures. Distributed training methodologies from @sec-ai-training support model optimization across thousands of accelerators. Model compression techniques from @sec-model-optimizations reduce inference costs to economically viable levels. Operational infrastructure from @sec-ml-operations maintains system reliability for millions of concurrent users."
    },
    {
      "footnote_id": "fn-chatgpt-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 51,
      "context": "[^fn-chatgpt-growth]: **ChatGPT User Adoption**: Reached 100 million monthly active users in January 2023, 2 months afte...",
      "full_line": "[^fn-chatgpt-growth]: **ChatGPT User Adoption**: Reached 100 million monthly active users in January 2023, 2 months after launch, the fastest consumer application adoption in history. For comparison: TikTok required 9 months, Instagram 2.5 years. This growth necessitated rapid scaling of inference infrastructure from hundreds to tens of thousands of GPUs."
    },
    {
      "footnote_id": "fn-phase-transition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 53,
      "context": "...le transforms quantitative improvements into qualitative capability shifts through phase transitions[^fn-phase-transition].",
      "full_line": "This convergence validates Sutton's \"bitter lesson\"[@sutton2019bitter]: computational scale coupled with general methods consistently outperforms specialized algorithms. Engineering at scale transforms quantitative improvements into qualitative capability shifts through phase transitions[^fn-phase-transition]."
    },
    {
      "footnote_id": "fn-phase-transition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 55,
      "context": "[^fn-phase-transition]: **Phase Transition**: In physics, abrupt changes in system properties at critical thresholds (like...",
      "full_line": "[^fn-phase-transition]: **Phase Transition**: In physics, abrupt changes in system properties at critical thresholds (like water becoming ice at 0\u00b0C). In neural networks, refers to sudden appearance of capabilities at specific model scales\u2014abilities appear rapidly rather than gradually. Examples: GPT models gain arithmetic ability around 10B parameters, reasoning around 100B parameters. These discontinuous jumps suggest major changes in network dynamics."
    },
    {
      "footnote_id": "fn-agi-compute-requirements",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 57,
      "context": "[^fn-agi-compute-requirements]: **AGI Compute Requirements Engineering Analysis**: **Biological baseline**: Human brain ~10\u00b9\u2075 ops/...",
      "full_line": "[^fn-agi-compute-requirements]: **AGI Compute Requirements Engineering Analysis**: **Biological baseline**: Human brain ~10\u00b9\u2075 ops/sec \u00d7 20 years = 6.3 \u00d7 10\u00b2\u00b3 total operations. **Scaling law extrapolation**: Chinchilla optimal scaling suggests compute C and parameters N follow C \u221d N^1.3. For AGI requiring ~100T parameters (100\u00d7 GPT-3), this yields C = (100\u00b9\u00b7\u00b3) \u00d7 (GPT-3 compute) = 1,585 \u00d7 3.14 \u00d7 10\u00b2\u00b3 = 5 \u00d7 10\u00b2\u2076 FLOPs. **Efficiency assumptions**: Accounting for 10\u00d7 training efficiency improvements gives practical requirement of 2.5 \u00d7 10\u00b2\u2076 FLOPs. **Sensitivity analysis**: \u00b11 order of magnitude depending on architecture breakthroughs and efficiency gains."
    },
    {
      "footnote_id": "fn-intelligence-theory",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 59,
      "context": "[^fn-intelligence-theory]: **Intelligence vs. Performance**: Intelligence involves understanding underlying principles rather...",
      "full_line": "[^fn-intelligence-theory]: **Intelligence vs. Performance**: Intelligence involves understanding underlying principles rather than memorizing patterns. Humans generalize from few examples through causal reasoning, while current AI requires massive datasets for statistical correlation. The symbol grounding problem asks how abstract symbols connect to embodied experience, a challenge absent in pure language models."
    },
    {
      "footnote_id": "fn-energy-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 61,
      "context": "[^fn-energy-models]: **Energy-Based Models (EBMs)**: Unlike discriminative models that output single predictions, EBMs...",
      "full_line": "[^fn-energy-models]: **Energy-Based Models (EBMs)**: Unlike discriminative models that output single predictions, EBMs learn energy functions where probable outcomes have low energy. This enables modeling multiple solutions, handling uncertainty, and optimization-based inference. Applications include molecular design, theorem proving, and multi-step reasoning where multiple valid solutions exist."
    },
    {
      "footnote_id": "fn-moores-end",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 63,
      "context": "[^fn-moores-end]: **End of Moore's Law**: Transistor density improvements slowed from 50% annually (1970-2010) to 10...",
      "full_line": "[^fn-moores-end]: **End of Moore's Law**: Transistor density improvements slowed from 50% annually (1970-2010) to 10-20% (2010-2025). Physical limits include quantum tunneling at 3-5nm nodes, manufacturing costs exceeding $20B per fab, and power density approaching nuclear reactor levels."
    },
    {
      "footnote_id": "fn-3d-stacking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 65,
      "context": "[^fn-3d-stacking]: **3D Chip Stacking**: Samsung's 176-layer 3D NAND achieves 100x higher density than planar designs...",
      "full_line": "[^fn-3d-stacking]: **3D Chip Stacking**: Samsung's 176-layer 3D NAND achieves 100x higher density than planar designs. Thermal management becomes critical: stacked processors generate 1000W/cm\u00b2 heat flux requiring advanced cooling. Applications include high-bandwidth memory (HBM) and processing-in-memory architectures."
    },
    {
      "footnote_id": "fn-chiplet-benefits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 67,
      "context": "[^fn-chiplet-benefits]: **Chiplet Architecture**: AMD's EPYC uses 8-12 chiplets connected via Infinity Fabric, achieving b...",
      "full_line": "[^fn-chiplet-benefits]: **Chiplet Architecture**: AMD's EPYC uses 8-12 chiplets connected via Infinity Fabric, achieving better yields and performance than monolithic designs. For AGI, enables mixing specialized processors: matrix units, memory controllers, and networking components in optimal ratios."
    },
    {
      "footnote_id": "fn-optical-interconnects",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 69,
      "context": "[^fn-optical-interconnects]: **Optical Interconnects**: Silicon photonics achieves 100 Tbps bandwidth with 10x lower energy tha...",
      "full_line": "[^fn-optical-interconnects]: **Optical Interconnects**: Silicon photonics achieves 100 Tbps bandwidth with 10x lower energy than electrical interconnects. Critical for AGI training where communication between 100,000+ processors becomes the bottleneck. Intel, NVIDIA, and startups developing optical chip-to-chip links."
    },
    {
      "footnote_id": "fn-processing-in-memory",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 71,
      "context": "[^fn-processing-in-memory]: **Processing-in-Memory (PIM)**: Performs computation directly in DRAM or flash memory, eliminating...",
      "full_line": "[^fn-processing-in-memory]: **Processing-in-Memory (PIM)**: Performs computation directly in DRAM or flash memory, eliminating data movement. Samsung's HBM-PIM and Upmem's PIM-DIMM demonstrate 100x energy reduction for memory-bound workloads. Essential for AGI where parameter access dominates energy consumption."
    },
    {
      "footnote_id": "fn-neuromorphic-promise",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 73,
      "context": "[^fn-neuromorphic-promise]: **Neuromorphic Computing**: Intel's Loihi chip achieves 1000x energy efficiency over digital proce...",
      "full_line": "[^fn-neuromorphic-promise]: **Neuromorphic Computing**: Intel's Loihi chip achieves 1000x energy efficiency over digital processors for sparse, event-driven workloads. IBM's TrueNorth demonstrates 1 million neurons with 70mW power consumption. Promise for continuous learning AGI systems but programming remains challenging."
    },
    {
      "footnote_id": "fn-quantum-hybrid",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 75,
      "context": "[^fn-quantum-hybrid]: **Quantum-Classical Hybrid**: IBM's quantum processors demonstrate quantum advantage for optimizat...",
      "full_line": "[^fn-quantum-hybrid]: **Quantum-Classical Hybrid**: IBM's quantum processors demonstrate quantum advantage for optimization problems. D-Wave's annealing systems solve scheduling and resource allocation. For AGI, quantum processors could accelerate search through exponentially large solution spaces while classical systems handle standard neural computation."
    },
    {
      "footnote_id": "fn-agi-infrastructure-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 77,
      "context": "[^fn-agi-infrastructure-scale]: **AGI Infrastructure Scale Engineering Analysis**: **Hardware requirements**: 2.5 \u00d7 10\u00b2\u2076 FLOPs \u00f7 (...",
      "full_line": "[^fn-agi-infrastructure-scale]: **AGI Infrastructure Scale Engineering Analysis**: **Hardware requirements**: 2.5 \u00d7 10\u00b2\u2076 FLOPs \u00f7 (1,000 TFLOPS/GPU \u00d7 8,760 hours/year \u00d7 0.7 utilization) = 175,000 H100 GPUs. **Power calculation**: 175,000 GPUs \u00d7 700W + 30% cooling overhead = 159 MW total consumption. **Cost breakdown**: 175,000 \u00d7 $30,000/GPU = $5.25B hardware + $15B infrastructure + $10B power (3 years) + $20B operational = $52B total. **Network bandwidth**: 175,000 GPUs \u00d7 900 GB/s NVLink = 158 PB/s bisection bandwidth requiring 100,000+ optical links. **Memory requirements**: 175,000 \u00d7 80GB HBM = 14 petabytes active parameter storage. Post-Moore's Law efficiency gains (neuromorphic 100\u00d7, quantum-hybrid 10\u00d7 for optimization) could reduce total requirements to $5-10B."
    },
    {
      "footnote_id": "fn-constitutional-intro",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 87,
      "context": "Constitutional AI[^fn-constitutional-intro] enables these capabilities through iterative self-refinement using principle-based feedback mechani...",
      "full_line": "Constitutional AI[^fn-constitutional-intro] enables these capabilities through iterative self-refinement using principle-based feedback mechanisms. Models critique their own outputs, identify improvements, and generate better responses through automated feedback loops. This transforms quality assurance from a human bottleneck into a scalable system component."
    },
    {
      "footnote_id": "fn-constitutional-intro",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 91,
      "context": "[^fn-constitutional-intro]: **Constitutional AI**: A training method developed by Anthropic where models learn to improve thei...",
      "full_line": "[^fn-constitutional-intro]: **Constitutional AI**: A training method developed by Anthropic where models learn to improve their own outputs by critiquing responses against a set of principles. This technique reduces harmful content while maintaining helpfulness, detailed in the training section of this chapter."
    },
    {
      "footnote_id": "fn-intelligence-theory",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 99,
      "context": "...and reasoning, between correlation and causation, between pattern completion and genuine creativity[^fn-intelligence-theory]. Current systems excel at statistical pattern matching but struggle with tasks requiring genuine un...",
      "full_line": "This definition aligns with foundational AGI research, where @goertzel2007artificial characterized AGI as \"the ability to achieve complex goals in complex environments using limited computational resources.\" Intelligence transcends performance metrics. True intelligence requires understanding the difference between memorization and reasoning, between correlation and causation, between pattern completion and genuine creativity[^fn-intelligence-theory]. Current systems excel at statistical pattern matching but struggle with tasks requiring genuine understanding of physical causality, intentionality, or symbolic abstraction."
    },
    {
      "footnote_id": "fn-energy-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 101,
      "context": "...a unifying theoretical framework for intelligence that goes beyond current probabilistic approaches[^fn-energy-models]. Unlike traditional models that directly output predictions, energy-based models learn to assign lo...",
      "full_line": "Energy-based models provide a unifying theoretical framework for intelligence that goes beyond current probabilistic approaches[^fn-energy-models]. Unlike traditional models that directly output predictions, energy-based models learn to assign lower energy (higher preference) to more probable or desirable outcomes. This framework naturally handles uncertainty, supports multiple plausible solutions, and enables reasoning through optimization rather than just feedforward computation. The approach shows promise for handling the multi-modal, multi-solution reasoning required for AGI, though current implementations remain computationally intensive."
    },
    {
      "footnote_id": "fn-agi-compute-requirements",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 121,
      "context": "...ffmann2022training] with 10\u00d7-100\u00d7 efficiency requirements suggests 2.5 \u00d7 10\u00b2\u2076 FLOPs for AGI training[^fn-agi-compute-requirements]. This represents a 250\u00d7 increase over GPT-4's estimated 2.5 \u00d7 10\u00b2\u00b3 FLOPs. At current H100 efficienc...",
      "full_line": "Contemporary AGI approaches reflect tension between different beliefs about how intelligence emerges. The scaling hypothesis argues that current transformer architectures will achieve AGI through larger parameters, more data, and greater compute [@kaplan2020scaling], proposing that quantitative scaling improvements will eventually yield qualitative cognitive capabilities matching human intelligence. However, engineering analysis reveals the magnitude of this challenge through specific scaling calculations. Using biological inspiration, the human brain processes approximately 10\u00b9\u2075 synaptic operations per second over 20 years of learning, yielding 6.3 \u00d7 10\u00b2\u00b3 total operations. Alternatively, extrapolating Chinchilla scaling laws [@hoffmann2022training] with 10\u00d7-100\u00d7 efficiency requirements suggests 2.5 \u00d7 10\u00b2\u2076 FLOPs for AGI training[^fn-agi-compute-requirements]. This represents a 250\u00d7 increase over GPT-4's estimated 2.5 \u00d7 10\u00b2\u00b3 FLOPs. At current H100 efficiency (1,000 TFLOPS at 700W), this demands 175,000 H100 GPUs running continuously for one year, consuming 122 MW power (equivalent to a small city), and requiring $52 billion in hardware costs alone[^fn-agi-infrastructure-scale]. This path leverages everything covered in this textbook: distributed training methodologies from @sec-ai-training coordinate thousands of GPUs, hardware acceleration from @sec-ai-acceleration enables massive parallelism, data engineering pipelines from @sec-data-engineering process exabyte-scale datasets, and operational infrastructure from @sec-ml-operations scales exponentially. Yet the sheer scale drives urgent exploration of post-Moore's Law architectures including 3D chip stacking for 10x density gains, chiplet designs enabling modular processor composition, optical interconnects providing 100x bandwidth improvements, processing-in-memory reducing data movement by 100x, neuromorphic computing achieving 1000x energy efficiency, and quantum-classical hybrid systems offering exponential speedups for optimization."
    },
    {
      "footnote_id": "fn-agi-infrastructure-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 121,
      "context": "...nsuming 122 MW power (equivalent to a small city), and requiring $52 billion in hardware costs alone[^fn-agi-infrastructure-scale]. This path leverages everything covered in this textbook: distributed training methodologies from @...",
      "full_line": "Contemporary AGI approaches reflect tension between different beliefs about how intelligence emerges. The scaling hypothesis argues that current transformer architectures will achieve AGI through larger parameters, more data, and greater compute [@kaplan2020scaling], proposing that quantitative scaling improvements will eventually yield qualitative cognitive capabilities matching human intelligence. However, engineering analysis reveals the magnitude of this challenge through specific scaling calculations. Using biological inspiration, the human brain processes approximately 10\u00b9\u2075 synaptic operations per second over 20 years of learning, yielding 6.3 \u00d7 10\u00b2\u00b3 total operations. Alternatively, extrapolating Chinchilla scaling laws [@hoffmann2022training] with 10\u00d7-100\u00d7 efficiency requirements suggests 2.5 \u00d7 10\u00b2\u2076 FLOPs for AGI training[^fn-agi-compute-requirements]. This represents a 250\u00d7 increase over GPT-4's estimated 2.5 \u00d7 10\u00b2\u00b3 FLOPs. At current H100 efficiency (1,000 TFLOPS at 700W), this demands 175,000 H100 GPUs running continuously for one year, consuming 122 MW power (equivalent to a small city), and requiring $52 billion in hardware costs alone[^fn-agi-infrastructure-scale]. This path leverages everything covered in this textbook: distributed training methodologies from @sec-ai-training coordinate thousands of GPUs, hardware acceleration from @sec-ai-acceleration enables massive parallelism, data engineering pipelines from @sec-data-engineering process exabyte-scale datasets, and operational infrastructure from @sec-ml-operations scales exponentially. Yet the sheer scale drives urgent exploration of post-Moore's Law architectures including 3D chip stacking for 10x density gains, chiplet designs enabling modular processor composition, optical interconnects providing 100x bandwidth improvements, processing-in-memory reducing data movement by 100x, neuromorphic computing achieving 1000x energy efficiency, and quantum-classical hybrid systems offering exponential speedups for optimization."
    },
    {
      "footnote_id": "fn-multi-agent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 127,
      "context": "...that intelligence emerges from interactions between specialized agents rather than monolithic models[^fn-multi-agent]. Like distributed software systems, these approaches require robust operational infrastructure from...",
      "full_line": "Finally, multi-agent systems propose that intelligence emerges from interactions between specialized agents rather than monolithic models[^fn-multi-agent]. Like distributed software systems, these approaches require robust operational infrastructure from @sec-ml-operations and distributed systems expertise. OpenAI's hide-and-seek agents [@baker2019emergent] developed unexpected strategies through competition, while projects like AutoGPT [@autogpt2023] demonstrate early autonomous capabilities, though they remain limited by context windows and error accumulation."
    },
    {
      "footnote_id": "fn-multi-agent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 129,
      "context": "[^fn-multi-agent]: **Multi-Agent Intelligence**: @baker2019emergent hide-and-seek agents developed unexpected strateg...",
      "full_line": "[^fn-multi-agent]: **Multi-Agent Intelligence**: @baker2019emergent hide-and-seek agents developed unexpected strategies through competition. @autogpt2023 and @babbage2023 demonstrate early autonomous agent capabilities, though remain limited by context windows and error accumulation."
    },
    {
      "footnote_id": "fn-rlhf-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 418,
      "context": "...ements: InstructGPT with 1.3B parameters outperforms GPT-3 with 175B parameters in human evaluations[^fn-rlhf-impact], demonstrating that alignment matters more than scale for user satisfaction. For ML engineers, this...",
      "full_line": "This approach yields dramatic improvements: InstructGPT with 1.3B parameters outperforms GPT-3 with 175B parameters in human evaluations[^fn-rlhf-impact], demonstrating that alignment matters more than scale for user satisfaction. For ML engineers, this means that investing in alignment infrastructure can be more valuable than scaling compute: a 100x smaller aligned model outperforms a larger unaligned one."
    },
    {
      "footnote_id": "fn-rlhf-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 420,
      "context": "[^fn-rlhf-impact]: **RLHF Effectiveness**: InstructGPT preferred over GPT-3 in 85% of comparisons despite being 100x...",
      "full_line": "[^fn-rlhf-impact]: **RLHF Effectiveness**: InstructGPT preferred over GPT-3 in 85% of comparisons despite being 100x smaller. Harmful output reduction: 90%. Hallucination reduction: 40%. User satisfaction increase: 72%."
    },
    {
      "footnote_id": "fn-human-feedback-limits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 424,
      "context": "...conflicting preferences, and scaling human oversight to billions of interactions proves challenging[^fn-human-feedback-limits]. Constitutional AI [@bai2022constitutional] addresses these limitations through automated preferenc...",
      "full_line": "Human feedback remains expensive and inconsistent: different annotators provide conflicting preferences, and scaling human oversight to billions of interactions proves challenging[^fn-human-feedback-limits]. Constitutional AI [@bai2022constitutional] addresses these limitations through automated preference learning."
    },
    {
      "footnote_id": "fn-human-feedback-limits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 426,
      "context": "[^fn-human-feedback-limits]: **Human Feedback Bottlenecks**: ChatGPT required 40 annotators working full-time for 3 months to g...",
      "full_line": "[^fn-human-feedback-limits]: **Human Feedback Bottlenecks**: ChatGPT required 40 annotators working full-time for 3 months to generate 200K labels. Scaling to GPT-4's capabilities would require 10,000+ annotators. Inter-annotator agreement typically reaches only 70-80%."
    },
    {
      "footnote_id": "fn-constitutional-approach",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 428,
      "context": "...uman rankings, Constitutional AI uses a set of principles (a \"constitution\") to guide model behavior[^fn-constitutional-approach]. The model generates responses, critiques its own outputs against these principles, and revises res...",
      "full_line": "Instead of human rankings, Constitutional AI uses a set of principles (a \"constitution\") to guide model behavior[^fn-constitutional-approach]. The model generates responses, critiques its own outputs against these principles, and revises responses iteratively. This self-improvement loop removes the human bottleneck while maintaining alignment objectives."
    },
    {
      "footnote_id": "fn-constitutional-approach",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 430,
      "context": "[^fn-constitutional-approach]: **Constitutional AI Method**: @bai2022constitutional implementation uses 16 principles like \"avoid...",
      "full_line": "[^fn-constitutional-approach]: **Constitutional AI Method**: @bai2022constitutional implementation uses 16 principles like \"avoid harmful content\" and \"be helpful.\" The model performs 5 rounds of self-critique and revision. Harmful outputs reduced by 95% while maintaining 90% of original helpfulness."
    },
    {
      "footnote_id": "fn-deployment-freeze",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 480,
      "context": "...able feedback (corrections, clarifications, new information) but models remain frozen after training[^fn-deployment-freeze]. This creates an ever-widening gap between training data and current reality.",
      "full_line": "Deployed models face a limitation: they cannot learn from user interactions without retraining. Each conversation provides valuable feedback (corrections, clarifications, new information) but models remain frozen after training[^fn-deployment-freeze]. This creates an ever-widening gap between training data and current reality."
    },
    {
      "footnote_id": "fn-deployment-freeze",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 482,
      "context": "[^fn-deployment-freeze]: **Static Model Problem**: GPT-3 trained on data before 2021 permanently believes it's 2021. Models...",
      "full_line": "[^fn-deployment-freeze]: **Static Model Problem**: GPT-3 trained on data before 2021 permanently believes it's 2021. Models cannot learn user preferences, correct mistakes, or incorporate new knowledge without full retraining costing millions of dollars."
    },
    {
      "footnote_id": "fn-catastrophic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 484,
      "context": "...ing catastrophic forgetting: the phenomenon where learning new information erases previous knowledge[^fn-catastrophic]. Standard gradient descent overwrites parameters without discrimination, destroying prior learning.",
      "full_line": "Continual learning aims to update models from ongoing interactions while preventing catastrophic forgetting: the phenomenon where learning new information erases previous knowledge[^fn-catastrophic]. Standard gradient descent overwrites parameters without discrimination, destroying prior learning."
    },
    {
      "footnote_id": "fn-catastrophic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 486,
      "context": "[^fn-catastrophic]: **Catastrophic Forgetting Severity**: Standard neural networks lose 20-80% accuracy on task A when...",
      "full_line": "[^fn-catastrophic]: **Catastrophic Forgetting Severity**: Standard neural networks lose 20-80% accuracy on task A when trained on task B. In language models, fine-tuning on medical text degrades general conversation ability by 30-50%."
    },
    {
      "footnote_id": "fn-moores-end",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 504,
      "context": "...ities now improve by only 10-20% annually compared to the historical 50% yearly gains of Moore's Law[^fn-moores-end]. This forces exploration of radically new computing paradigms.",
      "full_line": "The hardware landscape from @sec-ai-acceleration is undergoing a major transformation driven by AGI-scale requirements and the end of Moore's Law. Training GPT-4 class models demands massive parallelism that coordinates thousands of GPUs using sophisticated combinations of tensor, pipeline, and data parallelism, pushing distributed systems engineering to its limits. However, traditional silicon scaling has slowed dramatically: transistor densities now improve by only 10-20% annually compared to the historical 50% yearly gains of Moore's Law[^fn-moores-end]. This forces exploration of radically new computing paradigms."
    },
    {
      "footnote_id": "fn-3d-stacking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 508,
      "context": "...-spots requiring advanced cooling solutions including liquid cooling and thermal interface materials[^fn-3d-stacking].",
      "full_line": "3D chip stacking enables 10x density improvements by building upward instead of shrinking horizontally. Companies like Samsung and TSMC demonstrate 100+ layer 3D NAND memories, and similar approaches apply to processing units. However, heat dissipation becomes critical: stacked chips generate thermal hot-spots requiring advanced cooling solutions including liquid cooling and thermal interface materials[^fn-3d-stacking]."
    },
    {
      "footnote_id": "fn-chiplet-benefits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 510,
      "context": "...specialized processors (matrix units, memory controllers, networking chips) in optimal combinations[^fn-chiplet-benefits].",
      "full_line": "Chiplet architectures provide modular scaling where multiple specialized chips communicate through high-bandwidth interconnects. AMD's EPYC processors demonstrate 8-12 chiplets per package, achieving higher performance than monolithic designs while reducing manufacturing costs. For AGI training, this enables mixing different specialized processors (matrix units, memory controllers, networking chips) in optimal combinations[^fn-chiplet-benefits]."
    },
    {
      "footnote_id": "fn-optical-interconnects",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 512,
      "context": "...g communication at light speeds with dramatically lower energy consumption than copper interconnects[^fn-optical-interconnects].",
      "full_line": "Optical interconnects deliver 100x bandwidth improvements over electrical connections, crucial for coordinating massive model training across thousands of processors. Silicon photonics integrates optical transmitters and receivers directly on chips, enabling communication at light speeds with dramatically lower energy consumption than copper interconnects[^fn-optical-interconnects]."
    },
    {
      "footnote_id": "fn-processing-in-memory",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 514,
      "context": "...ry arrays. This approach could address the memory wall that limits current AI accelerator efficiency[^fn-processing-in-memory].",
      "full_line": "Processing-in-memory (PIM) reduces data movement energy by 100x by performing computations directly where data resides. Rather than shuttling massive weight matrices between memory and processors, PIM architectures compute matrix multiplications within DRAM or flash memory arrays. This approach could address the memory wall that limits current AI accelerator efficiency[^fn-processing-in-memory]."
    },
    {
      "footnote_id": "fn-neuromorphic-promise",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 516,
      "context": "...mited to simple tasks, the approach could enable continuous learning systems that adapt in real-time[^fn-neuromorphic-promise].",
      "full_line": "Neuromorphic computing promises brain-inspired hardware achieving 1000x energy efficiency improvements over digital computation. Intel's Loihi and IBM's TrueNorth demonstrate event-driven processing with spiking neural networks that activate only when receiving input, mimicking biological efficiency. While current prototypes remain limited to simple tasks, the approach could enable continuous learning systems that adapt in real-time[^fn-neuromorphic-promise]."
    },
    {
      "footnote_id": "fn-quantum-hybrid",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 518,
      "context": "...sophisticated middleware to decompose AGI workflows across heterogeneous quantum-classical resources[^fn-quantum-hybrid].",
      "full_line": "Quantum-classical hybrid systems deliver exponential speedups for combinatorial optimization problems that bottleneck AGI training, including neural architecture search and hyperparameter optimization. IBM's 1000+ qubit processors and Google's Sycamore demonstrate quantum advantage for specific optimization tasks, though current systems remain too noisy for general computation. Hybrid approaches partition workloads strategically: quantum processors solve discrete optimization problems while classical systems handle matrix operations and gradient computations. However, orchestration becomes complex: quantum and classical systems operate on fundamentally different timescales and error models, requiring sophisticated middleware to decompose AGI workflows across heterogeneous quantum-classical resources[^fn-quantum-hybrid]."
    },
    {
      "footnote_id": "fn-gpt4-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 661,
      "context": "...e consumed 50-100 GWh of electricity (unofficial estimates), enough to power 50,000 homes for a year[^fn-gpt4-energy]. Extrapolating to AGI suggests energy requirements exceeding small nations' output, creating both e...",
      "full_line": "GPT-4 training is estimated to have consumed 50-100 GWh of electricity (unofficial estimates), enough to power 50,000 homes for a year[^fn-gpt4-energy]. Extrapolating to AGI suggests energy requirements exceeding small nations' output, creating both economic and environmental challenges."
    },
    {
      "footnote_id": "fn-gpt4-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 663,
      "context": "[^fn-gpt4-energy]: **GPT-4 Energy Consumption**: Estimated 50-100 GWh for training (equivalent to 50,000 US homes' an...",
      "full_line": "[^fn-gpt4-energy]: **GPT-4 Energy Consumption**: Estimated 50-100 GWh for training (equivalent to 50,000 US homes' annual usage). At $0.10/kWh plus hardware amortization, training cost exceeds $100 million. AGI might require 1000x more."
    },
    {
      "footnote_id": "fn-brain-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 665,
      "context": "...ss than a light bulb) while performing computations that would require megawatts on current hardware[^fn-brain-efficiency]. This six-order-of-magnitude efficiency gap, detailed earlier with specific computational metrics i...",
      "full_line": "The human brain operates on 20 watts (less than a light bulb) while performing computations that would require megawatts on current hardware[^fn-brain-efficiency]. This six-order-of-magnitude efficiency gap, detailed earlier with specific computational metrics in @sec-agi-systems-agi-vision-intelligence-systems-problem-2b44, cannot be closed through incremental improvements. Solutions require reimagining of computation, building on @sec-sustainable-ai: neuromorphic architectures that compute with spikes rather than matrix multiplications, reversible computing that recycles energy through computation, and algorithmic improvements that reduce training iterations by orders of magnitude."
    },
    {
      "footnote_id": "fn-brain-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 667,
      "context": "[^fn-brain-efficiency]: **Biological Efficiency**: As detailed in computational analysis above, the brain achieves 35x bet...",
      "full_line": "[^fn-brain-efficiency]: **Biological Efficiency**: As detailed in computational analysis above, the brain achieves 35x better operations per watt than current hardware despite using chemical signaling. This comparison should be interpreted carefully as biological and digital computation operate on fundamentally different principles."
    },
    {
      "footnote_id": "fn-reasoning-limitation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 671,
      "context": "...new reasoning (proving a novel theorem or designing an experiment) and performance degrades rapidly[^fn-reasoning-limitation].",
      "full_line": "Current models excel at pattern completion but struggle with novel reasoning. Ask GPT-4 to plan a trip, and it produces plausible itineraries. Ask it to solve a problem requiring new reasoning (proving a novel theorem or designing an experiment) and performance degrades rapidly[^fn-reasoning-limitation]."
    },
    {
      "footnote_id": "fn-reasoning-limitation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 673,
      "context": "[^fn-reasoning-limitation]: **Reasoning Performance Cliff**: LLMs achieve 90%+ on familiar problem types but drop to 10-30% on...",
      "full_line": "[^fn-reasoning-limitation]: **Reasoning Performance Cliff**: LLMs achieve 90%+ on familiar problem types but drop to 10-30% on problems requiring genuine novelty. ARC challenge (abstraction and reasoning) reveals models memorize patterns rather than learning abstract rules."
    },
    {
      "footnote_id": "fn-reasoning-requirements",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 675,
      "context": "...e for exhaustive exploration, and causal understanding that distinguishes correlation from causation[^fn-reasoning-requirements]. These demand architectural innovations beyond those in @sec-dnn-architectures, potentially hybrid...",
      "full_line": "True reasoning requires capabilities absent from current architectures: world models that maintain consistent state across inference steps, search through solution spaces too large for exhaustive exploration, and causal understanding that distinguishes correlation from causation[^fn-reasoning-requirements]. These demand architectural innovations beyond those in @sec-dnn-architectures, potentially hybrid systems combining neural networks with symbolic reasoners, or new architectures inspired by cognitive science."
    },
    {
      "footnote_id": "fn-reasoning-requirements",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 677,
      "context": "[^fn-reasoning-requirements]: **Reasoning Architecture Requirements**: Classical planning requires: state representation, action...",
      "full_line": "[^fn-reasoning-requirements]: **Reasoning Architecture Requirements**: Classical planning requires: state representation, action models, goal specification, and search algorithms. Neural networks provide none explicitly. Neurosymbolic approaches attempt integration but remain limited to narrow domains."
    },
    {
      "footnote_id": "fn-embodiment-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 683,
      "context": "...ng from noisy sensor data, and safe exploration in environments where mistakes cause physical damage[^fn-embodiment-constraints]. These constraints mirror the efficiency challenges covered in @sec-efficient-ai but with even stri...",
      "full_line": "Robotic embodiment introduces systems constraints from @sec-ondevice-learning: real-time inference requirements (sub-100ms control loops), continuous learning from noisy sensor data, and safe exploration in environments where mistakes cause physical damage[^fn-embodiment-constraints]. These constraints mirror the efficiency challenges covered in @sec-efficient-ai but with even stricter latency and reliability requirements. Yet embodiment might be essential for understanding concepts like \"heavy,\" \"smooth,\" or \"careful\" that are grounded in physical experience."
    },
    {
      "footnote_id": "fn-embodiment-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 685,
      "context": "[^fn-embodiment-constraints]: **Robotic System Requirements**: Boston Dynamics' Atlas runs 1KHz control loops with 28 actuators....",
      "full_line": "[^fn-embodiment-constraints]: **Robotic System Requirements**: Boston Dynamics' Atlas runs 1KHz control loops with 28 actuators. Tesla's FSD processes 36 camera streams at 36 FPS. Both require <10ms inference latency\u2014impossible with cloud processing."
    },
    {
      "footnote_id": "fn-alignment-challenge",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 689,
      "context": "...stems pursue human values rather than optimizing simplified objectives that lead to harmful outcomes[^fn-alignment-challenge]. Current reward functions are proxies (maximize engagement, minimize error) that can produce uninte...",
      "full_line": "The challenge: ensuring AGI systems pursue human values rather than optimizing simplified objectives that lead to harmful outcomes[^fn-alignment-challenge]. Current reward functions are proxies (maximize engagement, minimize error) that can produce unintended behaviors when optimized strongly."
    },
    {
      "footnote_id": "fn-alignment-challenge",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 691,
      "context": "[^fn-alignment-challenge]: **Alignment Failure Modes**: YouTube's algorithm optimizing watch time promoted increasingly extre...",
      "full_line": "[^fn-alignment-challenge]: **Alignment Failure Modes**: YouTube's algorithm optimizing watch time promoted increasingly extreme content. Trading algorithms optimizing profit caused flash crashes. AGI optimizing misspecified objectives could cause existential risks."
    },
    {
      "footnote_id": "fn-alignment-components",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 693,
      "context": "...capabilities grow), and scalable oversight (maintaining control over systems smarter than overseers)[^fn-alignment-components]. These challenges span technical and philosophical domains, requiring advances in interpretability...",
      "full_line": "Alignment requires solving multiple interconnected problems: value specification (what do humans actually want?), robust optimization (pursuing goals without exploiting loopholes), corrigibility (remaining modifiable as capabilities grow), and scalable oversight (maintaining control over systems smarter than overseers)[^fn-alignment-components]. These challenges span technical and philosophical domains, requiring advances in interpretability from @sec-responsible-ai, formal verification methods, and new frameworks for specifying and verifying objectives."
    },
    {
      "footnote_id": "fn-alignment-components",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 695,
      "context": "[^fn-alignment-components]: **Alignment Technical Challenges**: Value specification: Arrow's impossibility theorem shows no pe...",
      "full_line": "[^fn-alignment-components]: **Alignment Technical Challenges**: Value specification: Arrow's impossibility theorem shows no perfect aggregation of preferences. Robust optimization: Goodhart's law states optimized metrics cease being good metrics. Corrigibility: Self-modifying systems might remove safety constraints. Scalable oversight: Humans cannot verify solutions to problems they cannot solve."
    },
    {
      "footnote_id": "fn-agi-agent-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 738,
      "context": "...tems might require coordination between millions of specialized agents distributed across continents[^fn-agi-agent-scale]. Each agent could be a frontier-model-scale system consuming gigawatts of power, making coordinatio...",
      "full_line": "Consider the scale: while today's distributed systems coordinate thousands of servers, AGI systems might require coordination between millions of specialized agents distributed across continents[^fn-agi-agent-scale]. Each agent could be a frontier-model-scale system consuming gigawatts of power, making coordination latency and bandwidth major bottlenecks. Communication between agents in Tokyo and New York introduces 150ms round-trip delays, unacceptable for real-time reasoning requiring millisecond coordination."
    },
    {
      "footnote_id": "fn-agi-communication",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 742,
      "context": "...including partial world models, reasoning chains, uncertainty estimates, and intent representations[^fn-agi-communication]. The protocols must compress complex cognitive states into network packets while preserving semanti...",
      "full_line": "**Communication protocols** become the bottleneck. Unlike traditional distributed systems that exchange simple state updates, AGI agents must communicate rich semantic information including partial world models, reasoning chains, uncertainty estimates, and intent representations[^fn-agi-communication]. The protocols must compress complex cognitive states into network packets while preserving semantic fidelity across heterogeneous agent architectures. Current internet protocols lack semantic understanding; future AGI networks might require content-aware routing that understands reasoning context."
    },
    {
      "footnote_id": "fn-agi-topology",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 744,
      "context": "...regional hubs for cross-domain integration, and global coordination layers for system-wide coherence[^fn-agi-topology]. Load balancing algorithms must consider not just computational load but semantic affinity\u2014routing...",
      "full_line": "**Network topology** design becomes important for efficiency. Rather than flat network architectures, AGI systems might require hierarchical topologies mimicking biological neural organization: local agent clusters for rapid coordination, regional hubs for cross-domain integration, and global coordination layers for system-wide coherence[^fn-agi-topology]. Load balancing algorithms must consider not just computational load but semantic affinity\u2014routing related reasoning tasks to agents with shared context."
    },
    {
      "footnote_id": "fn-agi-consensus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 746,
      "context": "...sus must handle conflicting world models, competing reasoning chains, and subjective value judgments[^fn-agi-consensus]. When scientific reasoning agents disagree about experimental interpretations, creative agents prop...",
      "full_line": "**Consensus mechanisms** for AGI agents face complexity beyond traditional distributed systems. While blockchain consensus involves simple state transitions, AGI consensus must handle conflicting world models, competing reasoning chains, and subjective value judgments[^fn-agi-consensus]. When scientific reasoning agents disagree about experimental interpretations, creative agents propose conflicting artistic directions, and strategic agents recommend opposing policies, the system needs mechanisms for productive disagreement rather than forced consensus. This might involve reputation systems that weight agent contributions by past accuracy, voting mechanisms that consider argument quality not just agent count, and meta-reasoning systems that identify when disagreement indicates genuine uncertainty versus agent malfunction."
    },
    {
      "footnote_id": "fn-agi-byzantine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 748,
      "context": "...y manipulating other agents, or an agent compromised by adversarial attacks spreading misinformation[^fn-agi-byzantine]. Traditional Byzantine algorithms require 3f+1 honest nodes to tolerate f Byzantine nodes, but AGI...",
      "full_line": "**Byzantine fault tolerance** becomes more challenging when agents are not just providing incorrect information but potentially pursuing different objectives. Unlike server failures that are random, agent failures might be systematic: an agent trained on biased data consistently providing skewed recommendations, an agent with misaligned objectives subtly manipulating other agents, or an agent compromised by adversarial attacks spreading misinformation[^fn-agi-byzantine]. Traditional Byzantine algorithms require 3f+1 honest nodes to tolerate f Byzantine nodes, but AGI systems might face sophisticated, coordinated attacks requiring novel defense mechanisms."
    },
    {
      "footnote_id": "fn-agi-resource-coordination",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 750,
      "context": "...urgency, and graceful degradation that maintains system coherence when resources become constrained[^fn-agi-resource-coordination].",
      "full_line": "**Resource coordination** across millions of agents demands new distributed algorithms. When multiple reasoning chains compete for compute resources, memory bandwidth, and network capacity, the system needs real-time resource allocation that considers not just current load but predicted reasoning complexity. This requires advances beyond current Kubernetes orchestration: predictive load balancing based on reasoning difficulty estimation, priority systems that understand reasoning urgency, and graceful degradation that maintains system coherence when resources become constrained[^fn-agi-resource-coordination]."
    },
    {
      "footnote_id": "fn-agi-agent-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 752,
      "context": "[^fn-agi-agent-scale]: **AGI Agent Scale**: Estimates suggest AGI systems might require 10\u2076-10\u2077 specialized agents for hu...",
      "full_line": "[^fn-agi-agent-scale]: **AGI Agent Scale**: Estimates suggest AGI systems might require 10\u2076-10\u2077 specialized agents for human-level capabilities across all domains. Each agent could be GPT-4 scale or larger. Coordination complexity grows as O(n\u00b2) without hierarchical organization, making flat architectures impossible at this scale."
    },
    {
      "footnote_id": "fn-agi-communication",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 754,
      "context": "[^fn-agi-communication]: **AGI Communication Complexity**: Agent communication must convey semantic content equivalent to f...",
      "full_line": "[^fn-agi-communication]: **AGI Communication Complexity**: Agent communication must convey semantic content equivalent to full reasoning states, potentially terabytes per message. Current internet protocols (TCP/IP) lack semantic understanding. Future AGI networks might use content-addressable routing, semantic compression, and reasoning-aware network stacks."
    },
    {
      "footnote_id": "fn-agi-topology",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 756,
      "context": "[^fn-agi-topology]: **AGI Network Topology**: Hierarchical networks reduce communication complexity from O(n\u00b2) to O(n...",
      "full_line": "[^fn-agi-topology]: **AGI Network Topology**: Hierarchical networks reduce communication complexity from O(n\u00b2) to O(n log n). Biological neural networks use similar hierarchies: local processing clusters, regional integration areas, and global coordination structures. AGI systems likely require analogous network architectures."
    },
    {
      "footnote_id": "fn-agi-consensus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 758,
      "context": "[^fn-agi-consensus]: **AGI Consensus Complexity**: Unlike traditional consensus on simple state transitions, AGI consen...",
      "full_line": "[^fn-agi-consensus]: **AGI Consensus Complexity**: Unlike traditional consensus on simple state transitions, AGI consensus involves competing world models, subjective values, and reasoning chains. This requires new consensus mechanisms that handle semantic disagreement, argument quality assessment, and uncertainty quantification."
    },
    {
      "footnote_id": "fn-agi-byzantine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 760,
      "context": "[^fn-agi-byzantine]: **AGI Byzantine Threats**: Beyond random failures, AGI agents face systematic threats: biased trai...",
      "full_line": "[^fn-agi-byzantine]: **AGI Byzantine Threats**: Beyond random failures, AGI agents face systematic threats: biased training data causing consistent errors, misaligned objectives leading to subtle manipulation, and adversarial attacks spreading sophisticated misinformation. Defense requires advances beyond traditional 3f+1 Byzantine fault tolerance."
    },
    {
      "footnote_id": "fn-agi-resource-coordination",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 762,
      "context": "[^fn-agi-resource-coordination]: **AGI Resource Coordination**: Managing compute resources across millions of reasoning agents requ...",
      "full_line": "[^fn-agi-resource-coordination]: **AGI Resource Coordination**: Managing compute resources across millions of reasoning agents requires predictive load balancing based on reasoning complexity estimation, priority systems understanding reasoning urgency, and graceful degradation maintaining system coherence under resource constraints."
    },
    {
      "footnote_id": "fn-infra-bottleneck",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 774,
      "context": "...sands of GPUs across multiple datacenters, yet most AI infrastructure remains ad-hoc and inefficient[^fn-infra-bottleneck]. This creates substantial opportunities for systems engineers who understand the full stack.",
      "full_line": "Current AI development suffers from infrastructure bottlenecks. Training frontier models requires coordinating tens of thousands of GPUs across multiple datacenters, yet most AI infrastructure remains ad-hoc and inefficient[^fn-infra-bottleneck]. This creates substantial opportunities for systems engineers who understand the full stack."
    },
    {
      "footnote_id": "fn-infra-bottleneck",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 776,
      "context": "[^fn-infra-bottleneck]: **Infrastructure Efficiency Gap**: Current GPU clusters achieve 20-40% utilization during training...",
      "full_line": "[^fn-infra-bottleneck]: **Infrastructure Efficiency Gap**: Current GPU clusters achieve 20-40% utilization during training due to communication overhead, load imbalancing, and fault recovery. Improving utilization to 70-80% would reduce training costs by 40-60%, worth billions annually. AGI-scale systems require 99.99% utilization across million-GPU clusters."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 811,
      "context": "...are. Future infrastructure must orchestrate CPUs for preprocessing, GPUs for matrix operations, TPUs[^fn-tpu] for inference, quantum processors for optimization, and neuromorphic chips for energy-efficient com...",
      "full_line": "The complexity multiplies as systems must coordinate heterogeneous hardware. Future infrastructure must orchestrate CPUs for preprocessing, GPUs for matrix operations, TPUs[^fn-tpu] for inference, quantum processors for optimization, and neuromorphic chips for energy-efficient computation. This heterogeneity demands abstractions that hide complexity from developers and scheduling algorithms that optimize across different computational paradigms."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 813,
      "context": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom application-specific integrated circuit (ASIC) d...",
      "full_line": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom application-specific integrated circuit (ASIC) designed specifically for neural network machine learning. First generation (2015) achieved 15-30x higher performance and 30-80x better performance-per-watt than contemporary CPUs/GPUs for inference. TPU v4 (2021) delivers 275 teraFLOPs for training with specialized matrix multiplication units."
    },
    {
      "footnote_id": "fn-personalization-tech",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 817,
      "context": "...ndations now exist for personalized AI that adapts to individual preferences, knowledge, and context[^fn-personalization-tech]. This creates opportunities for new categories of applications.",
      "full_line": "Current AI systems provide the same responses to all users, but the technical foundations now exist for personalized AI that adapts to individual preferences, knowledge, and context[^fn-personalization-tech]. This creates opportunities for new categories of applications."
    },
    {
      "footnote_id": "fn-personalization-tech",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 819,
      "context": "[^fn-personalization-tech]: **Personalization Technical Foundations**: Parameter-efficient fine-tuning (LoRA) reduces personal...",
      "full_line": "[^fn-personalization-tech]: **Personalization Technical Foundations**: Parameter-efficient fine-tuning (LoRA) reduces personalization costs by 1000x. Retrieval systems enable personal knowledge bases. Constitutional AI allows custom value alignment per user."
    },
    {
      "footnote_id": "fn-workflow-automation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 837,
      "context": "...ires orchestrating multiple AI systems with human oversight, a classic systems engineering challenge[^fn-workflow-automation].",
      "full_line": "Current AI systems excel at individual tasks, but the next frontier involves end-to-end automation of workflows. This requires orchestrating multiple AI systems with human oversight, a classic systems engineering challenge[^fn-workflow-automation]."
    },
    {
      "footnote_id": "fn-workflow-automation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 839,
      "context": "[^fn-workflow-automation]: **Workflow Automation Scale**: McKinsey estimates 60-70% of current jobs contain 30%+ automatable...",
      "full_line": "[^fn-workflow-automation]: **Workflow Automation Scale**: McKinsey estimates 60-70% of current jobs contain 30%+ automatable activities. But current automation covers <5% of possible workflows due to integration complexity, not capability limitations."
    },
    {
      "footnote_id": "fn-realtime-requirements",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 857,
      "context": "...esponse: autonomous vehicles, robotic surgery, high-frequency trading, and live conversation systems[^fn-realtime-requirements]. This creates new optimization challenges.",
      "full_line": "Most current AI applications can tolerate seconds or minutes of latency, but emerging applications demand real-time response: autonomous vehicles, robotic surgery, high-frequency trading, and live conversation systems[^fn-realtime-requirements]. This creates new optimization challenges."
    },
    {
      "footnote_id": "fn-realtime-requirements",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 859,
      "context": "[^fn-realtime-requirements]: **Real-Time Latency Requirements**: Autonomous vehicles need <10ms perception-to-action loops. Con...",
      "full_line": "[^fn-realtime-requirements]: **Real-Time Latency Requirements**: Autonomous vehicles need <10ms perception-to-action loops. Conversational AI requires <200ms response for natural interaction. Robotic surgery demands <1ms control loops. Current cloud systems achieve 50-200ms best case."
    },
    {
      "footnote_id": "fn-explainability-demand",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 877,
      "context": "...dical diagnoses, legal judgments, financial investments) the demand for explainable AI grows rapidly[^fn-explainability-demand]. This creates opportunities for systems that can provide interpretable reasoning while maintaining...",
      "full_line": "As AI systems make important decisions (medical diagnoses, legal judgments, financial investments) the demand for explainable AI grows rapidly[^fn-explainability-demand]. This creates opportunities for systems that can provide interpretable reasoning while maintaining performance."
    },
    {
      "footnote_id": "fn-explainability-demand",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 879,
      "context": "[^fn-explainability-demand]: **Explainability Market Growth**: Explainable AI market projected to grow from $5.2B (2023) to $21...",
      "full_line": "[^fn-explainability-demand]: **Explainability Market Growth**: Explainable AI market projected to grow from $5.2B (2023) to $21.4B (2030). Regulatory requirements in EU AI Act and medical device approval drive 60%+ of demand."
    },
    {
      "footnote_id": "fn-gflops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 47,
      "context": "...ons and attention mechanisms. These computational primitives reveal why CPUs achieve only 100 GFLOPS[^fn-gflops] (FP32) while GPUs reach 15,700 GFLOPS (mixed precision) and TPUs deliver 275,000 INT8 operations pe...",
      "full_line": "The structure follows computational requirements leading to architectural solutions, which allow system integration practices. The analysis begins with three core patterns that define machine learning computation: vector operations that parallelize across data, matrix operations that structure neural network layers, and special functions that implement activations and attention mechanisms. These computational primitives reveal why CPUs achieve only 100 GFLOPS[^fn-gflops] (FP32) while GPUs reach 15,700 GFLOPS (mixed precision) and TPUs deliver 275,000 INT8 operations per second on identical workloads."
    },
    {
      "footnote_id": "fn-gflops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 49,
      "context": "[^fn-gflops]: **GFLOPS (Giga Floating-Point Operations Per Second)**: A measure of computational throughput repr...",
      "full_line": "[^fn-gflops]: **GFLOPS (Giga Floating-Point Operations Per Second)**: A measure of computational throughput representing one billion floating-point operations per second. For context, modern CPUs achieve ~100 GFLOPS, high-end GPUs reach ~15,000 GFLOPS, and specialized AI chips can exceed 100,000 GFLOPS. The dramatic 1000x difference between CPUs and AI accelerators explains why specialized hardware became essential for practical machine learning deployment."
    },
    {
      "footnote_id": "fn-l1-cache",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 51,
      "context": "...archies break down when a single large neural network layer requires 1GB+ of parameters but L1 cache[^fn-l1-cache] provides only 32KB.",
      "full_line": "Memory systems represent the second architectural challenge. Traditional memory hierarchies break down when a single large neural network layer requires 1GB+ of parameters but L1 cache[^fn-l1-cache] provides only 32KB."
    },
    {
      "footnote_id": "fn-l1-cache",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 53,
      "context": "[^fn-l1-cache]: **L1 Cache**: The fastest but smallest memory in a CPU, typically 32-64KB per core with <1ns acces...",
      "full_line": "[^fn-l1-cache]: **L1 Cache**: The fastest but smallest memory in a CPU, typically 32-64KB per core with <1ns access time. L1 cache sits closest to the processor and operates at CPU clock speed. The 30,000x size mismatch between modern neural network parameters (gigabytes) and L1 cache capacity (kilobytes) illustrates why traditional memory hierarchies fail for AI workloads, necessitating specialized architectures with larger on-chip memory. Specialized accelerators address this through optimized memory hierarchies: Google's TPU provides 128MB of on-chip memory running at 900 GB/s\u2014600x faster than typical RAM\u2014because keeping weights on-chip dramatically reduces both latency and energy consumption. These trade-offs quantify how memory-aware system design allows practical deployment of large-scale models."
    },
    {
      "footnote_id": "fn-memory-bandwidth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 61,
      "context": "[^fn-memory-bandwidth]: **Memory Bandwidth**: The rate at which data can be transferred between memory and processors, mea...",
      "full_line": "[^fn-memory-bandwidth]: **Memory Bandwidth**: The rate at which data can be transferred between memory and processors, measured in GB/s or TB/s. AI workloads are often bandwidth-bound rather than compute-bound\u2014NVIDIA H100 provides 3.35 TB/s (70x faster than DDR5) [@nvidia2022h100] because neural networks require constant weight access. A single large neural network layer needs to read 1GB+ of parameters for each forward pass, making memory bandwidth the primary bottleneck in many AI applications."
    },
    {
      "footnote_id": "fn-von-neumann",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 69,
      "context": "[^fn-von-neumann]: **Von Neumann Architecture**: Proposed by John von Neumann in 1945, this architecture separates pr...",
      "full_line": "[^fn-von-neumann]: **Von Neumann Architecture**: Proposed by John von Neumann in 1945, this architecture separates program storage from data storage but forces all data to flow through a single bus between CPU and memory. In AI workloads, this \"von Neumann bottleneck\" becomes critical\u2014moving a 1GB model from memory consumes 100-1000x more energy than the actual computation, driving the need for specialized architectures that bring computation closer to data."
    },
    {
      "footnote_id": "fn-hwacc-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 75,
      "context": "...tant context for understanding how modern ML accelerators like GPUs with tensor cores, Google's TPUs[^fn-hwacc-tpu], and Apple's Neural Engine came to be. These technologies now power widely deployed applications su...",
      "full_line": "This evolution is not just of academic interest\u2014it provides important context for understanding how modern ML accelerators like GPUs with tensor cores, Google's TPUs[^fn-hwacc-tpu], and Apple's Neural Engine came to be. These technologies now power widely deployed applications such as real-time language translation, image recognition, and personalized recommendations. The architectural strategies enabling such capabilities are deeply rooted in decades of hardware specialization. At the heart of this transition is hardware specialization, which enhances performance and efficiency by optimizing frequently executed computational patterns through dedicated circuit implementations. While this approach leads to significant gains, it also introduces trade-offs in flexibility, silicon area utilization, and programming complexity. As computing demands continue to evolve, specialized accelerators must balance these factors to deliver sustained improvements in efficiency and performance."
    },
    {
      "footnote_id": "fn-hwacc-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 77,
      "context": "[^fn-hwacc-tpu]: **TPU Origins**: Google secretly developed the Tensor Processing Unit (TPU) starting in 2013 when...",
      "full_line": "[^fn-hwacc-tpu]: **TPU Origins**: Google secretly developed the Tensor Processing Unit (TPU) starting in 2013 when they realized CPUs couldn't handle the computational demands of their neural networks. The TPUv1, deployed in 2015, delivered 15-30x better performance per watt than contemporary GPUs for inference. This breakthrough significantly changed how the industry approached AI hardware, proving that domain-specific architectures could dramatically outperform general-purpose processors for neural network workloads."
    },
    {
      "footnote_id": "fn-intel-8087",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 85,
      "context": "One of the earliest examples of hardware specialization was the Intel 8087 mathematics coprocessor[^fn-intel-8087], introduced in 1980. This floating-point unit (FPU) was designed to offload arithmetic-intensive co...",
      "full_line": "One of the earliest examples of hardware specialization was the Intel 8087 mathematics coprocessor[^fn-intel-8087], introduced in 1980. This floating-point unit (FPU) was designed to offload arithmetic-intensive computations from the main CPU, dramatically improving performance for scientific and engineering applications. The 8087 demonstrated unprecedented efficiency, achieving performance gains of up to 100\u00d7 for floating-point operations compared to software-based implementations on general-purpose processors [@fisher_8087_1981]. This milestone established a principle in computer architecture: carefully designed hardware specialization could provide order-of-magnitude improvements for well-defined, computationally intensive tasks."
    },
    {
      "footnote_id": "fn-intel-8087",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 87,
      "context": "[^fn-intel-8087]: **Intel 8087 Impact**: The 8087 coprocessor cost $750 (about $2,800 today) but transformed scienti...",
      "full_line": "[^fn-intel-8087]: **Intel 8087 Impact**: The 8087 coprocessor cost $750 (about $2,800 today) but transformed scientific computing\u2014CAD workstations that took hours for complex calculations could complete them in minutes. This success created the entire coprocessor market and established the economic model for specialized hardware that persists today: charge premium prices for dramatic performance improvements in specific domains."
    },
    {
      "footnote_id": "fn-coprocessor",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 89,
      "context": "The success of floating-point coprocessors[^fn-coprocessor] led to their eventual integration into mainstream processors. For example, the Intel 486DX, release...",
      "full_line": "The success of floating-point coprocessors[^fn-coprocessor] led to their eventual integration into mainstream processors. For example, the Intel 486DX, released in 1989, incorporated an on-chip floating-point unit, eliminating the need for an external coprocessor. This integration not only improved processing efficiency but also marked a recurring pattern in computer architecture: successful specialized functions tend to become standard features in future generations of general-purpose processors [@patterson2021computer]."
    },
    {
      "footnote_id": "fn-coprocessor",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 91,
      "context": "[^fn-coprocessor]: **Coprocessor**: A specialized secondary processor designed to handle specific tasks that the main...",
      "full_line": "[^fn-coprocessor]: **Coprocessor**: A specialized secondary processor designed to handle specific tasks that the main CPU performs poorly. The 8087 math coprocessor was the first successful example, followed by graphics coprocessors (GPUs) and network processors. Modern \"accelerators\" are essentially evolved coprocessors\u2014the term changed as these chips became more powerful than host CPUs for their target workloads. Today's AI accelerators follow the same pattern but often eclipse CPU performance."
    },
    {
      "footnote_id": "fn-hwacc-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 183,
      "context": "...ics pipelines directly enabled its later adoption for training deep neural networks, such as AlexNet[^fn-hwacc-alexnet] in 2012, which famously ran on consumer-grade NVIDIA GPUs. DSP innovations in low-power signal proc...",
      "full_line": "This period of expanding specialization demonstrated that hardware acceleration strategies could successfully address diverse computational requirements. The GPU's success in parallelizing 3D graphics pipelines directly enabled its later adoption for training deep neural networks, such as AlexNet[^fn-hwacc-alexnet] in 2012, which famously ran on consumer-grade NVIDIA GPUs. DSP innovations in low-power signal processing helped pave the way for real-time inference on edge devices, such as voice assistants and wearables. These domains not only informed ML hardware designs but also proved that accelerators could be deployed across both cloud and embedded contexts\u2014a lesson that continues to shape today's AI ecosystem."
    },
    {
      "footnote_id": "fn-hwacc-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 185,
      "context": "[^fn-hwacc-alexnet]: **AlexNet's GPU Revolution**: AlexNet's breakthrough wasn't just algorithmic\u2014it proved GPUs could...",
      "full_line": "[^fn-hwacc-alexnet]: **AlexNet's GPU Revolution**: AlexNet's breakthrough wasn't just algorithmic\u2014it proved GPUs could train deep networks 10x faster than CPUs [@krizhevsky2012alexnet]. The team split the 8-layer network across two NVIDIA GTX 580s (512 cores each), reducing training time from weeks to days. This success triggered the \"deep learning gold rush\" and established NVIDIA as the default AI hardware company, with GPU sales for data centers growing from $200 million to $47 billion by 2024. Modern GPUs like the NVIDIA H100 contain 16,896 CUDA cores, demonstrating the massive scaling in parallel processing capability since AlexNet's era."
    },
    {
      "footnote_id": "fn-dsa",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 189,
      "context": "The emergence of domain-specific architectures (DSA)[^fn-dsa] marks a shift in computer system design, driven by two factors: the breakdown of traditional scalin...",
      "full_line": "The emergence of domain-specific architectures (DSA)[^fn-dsa] marks a shift in computer system design, driven by two factors: the breakdown of traditional scaling laws and the increasing computational demands of specialized workloads. The slowdown of Moore's Law[^fn-moores-law], which previously ensured predictable enhancements in transistor density every 18 to 24 months, and the end of Dennard scaling[^fn-dennard-scaling], which permitted frequency increases without corresponding power increases, created a performance and efficiency bottleneck in general-purpose computing. As John Hennessy and David Patterson noted in their 2017 Turing Lecture [@HennessyPatterson2017Turing], these limitations signaled the onset of a new era in computer architecture\u2014one centered on domain-specific solutions that optimize hardware for specialized workloads."
    },
    {
      "footnote_id": "fn-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 189,
      "context": "...laws and the increasing computational demands of specialized workloads. The slowdown of Moore's Law[^fn-moores-law], which previously ensured predictable enhancements in transistor density every 18 to 24 months, and...",
      "full_line": "The emergence of domain-specific architectures (DSA)[^fn-dsa] marks a shift in computer system design, driven by two factors: the breakdown of traditional scaling laws and the increasing computational demands of specialized workloads. The slowdown of Moore's Law[^fn-moores-law], which previously ensured predictable enhancements in transistor density every 18 to 24 months, and the end of Dennard scaling[^fn-dennard-scaling], which permitted frequency increases without corresponding power increases, created a performance and efficiency bottleneck in general-purpose computing. As John Hennessy and David Patterson noted in their 2017 Turing Lecture [@HennessyPatterson2017Turing], these limitations signaled the onset of a new era in computer architecture\u2014one centered on domain-specific solutions that optimize hardware for specialized workloads."
    },
    {
      "footnote_id": "fn-dennard-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 189,
      "context": "...predictable enhancements in transistor density every 18 to 24 months, and the end of Dennard scaling[^fn-dennard-scaling], which permitted frequency increases without corresponding power increases, created a performance a...",
      "full_line": "The emergence of domain-specific architectures (DSA)[^fn-dsa] marks a shift in computer system design, driven by two factors: the breakdown of traditional scaling laws and the increasing computational demands of specialized workloads. The slowdown of Moore's Law[^fn-moores-law], which previously ensured predictable enhancements in transistor density every 18 to 24 months, and the end of Dennard scaling[^fn-dennard-scaling], which permitted frequency increases without corresponding power increases, created a performance and efficiency bottleneck in general-purpose computing. As John Hennessy and David Patterson noted in their 2017 Turing Lecture [@HennessyPatterson2017Turing], these limitations signaled the onset of a new era in computer architecture\u2014one centered on domain-specific solutions that optimize hardware for specialized workloads."
    },
    {
      "footnote_id": "fn-dsa",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 191,
      "context": "[^fn-dsa]: **Domain-Specific Architectures (DSA)**: Computing architectures optimized for specific applicatio...",
      "full_line": "[^fn-dsa]: **Domain-Specific Architectures (DSA)**: Computing architectures optimized for specific application domains rather than general-purpose computation. Unlike CPUs designed for flexibility, DSAs sacrifice programmability for dramatic efficiency gains\u2014Google's TPU achieves 15-30x better performance per watt than GPUs for neural networks, while video codecs provide 100-1000x improvements over software decoding. The 2018 Turing Award recognized this shift as the defining trend in modern computer architecture."
    },
    {
      "footnote_id": "fn-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 193,
      "context": "[^fn-moores-law]: **Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles...",
      "full_line": "[^fn-moores-law]: **Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every 18-24 months. This exponential scaling drove computing progress for 50 years, enabling everything from smartphones to supercomputers. However, physical limits around 2005 slowed this pace dramatically\u2014modern 3nm chips cost $20 billion to develop versus $3 million in 1999, forcing the industry toward specialized architectures."
    },
    {
      "footnote_id": "fn-dennard-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 195,
      "context": "[^fn-dennard-scaling]: **Dennard Scaling**: Robert Dennard's 1974 principle that as transistors shrink, their power densi...",
      "full_line": "[^fn-dennard-scaling]: **Dennard Scaling**: Robert Dennard's 1974 principle that as transistors shrink, their power density remains constant, allowing higher frequencies without increased power consumption. This enabled CPUs to reach 3+ GHz by 2005. However, quantum effects and leakage current ended Dennard scaling around 2005, forcing architects to prioritize efficiency over raw speed and leading to the multi-core revolution."
    },
    {
      "footnote_id": "fn-asics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 209,
      "context": "...Similarly, blockchain computation has given rise to application-specific integrated circuits (ASICs)[^fn-asics] optimized for cryptographic hashing, dramatically increasing the efficiency of mining operations [@...",
      "full_line": "The trend toward specialization continues to accelerate, with new architectures emerging for an expanding range of domains. Genomics processing, for example, benefits from custom accelerators that optimize sequence alignment and variant calling, reducing the time required for DNA analysis [@Shang2018GenomicsAccel]. Similarly, blockchain computation has given rise to application-specific integrated circuits (ASICs)[^fn-asics] optimized for cryptographic hashing, dramatically increasing the efficiency of mining operations [@Taylor2017ASICMining]. These examples illustrate that domain-specific architecture is not merely a transient trend but a transformation in computing systems, offering tailored solutions that address the growing complexity and diversity of modern computational workloads."
    },
    {
      "footnote_id": "fn-asics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 211,
      "context": "[^fn-asics]: **Application-Specific Integrated Circuits (ASICs)**: Custom silicon chips designed for a single a...",
      "full_line": "[^fn-asics]: **Application-Specific Integrated Circuits (ASICs)**: Custom silicon chips designed for a single application, offering maximum efficiency by eliminating unused features. Bitcoin mining ASICs achieve 100,000x better energy efficiency than CPUs for SHA-256 hashing. However, their inflexibility means they become worthless if algorithms change\u2014$5 billion in Ethereum mining ASICs became obsolete overnight when Ethereum switched to proof-of-stake in 2022."
    },
    {
      "footnote_id": "fn-memory-bandwidth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 215,
      "context": "...GFLOPS (FP32) for neural network workloads, achieving only 5-10% utilization due to memory bandwidth[^fn-memory-bandwidth] constraints and sequential execution models. GPUs, originally designed for parallel graphics render...",
      "full_line": "Machine learning has emerged as one of the most computationally demanding fields, demonstrating the need for dedicated hardware that targets its unique characteristics. The performance requirements expose the limitations of general-purpose processors: traditional CPUs deliver approximately 100 GFLOPS (FP32) for neural network workloads, achieving only 5-10% utilization due to memory bandwidth[^fn-memory-bandwidth] constraints and sequential execution models. GPUs, originally designed for parallel graphics rendering, scale this to 15,700 GFLOPS (mixed precision) through massive parallelism but still struggle with energy efficiency[^fn-von-neumann], consuming 640pJ per DRAM access compared to 3.7pJ for computation. TPUs represent the specialized endpoint, achieving 275,000 INT8 operations per second with 85% utilization by implementing systolic arrays that minimize data movement and maximize operand reuse\u2014demonstrating how architectural specialization directly translates to quantifiable performance advantages."
    },
    {
      "footnote_id": "fn-von-neumann",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 215,
      "context": "...5,700 GFLOPS (mixed precision) through massive parallelism but still struggle with energy efficiency[^fn-von-neumann], consuming 640pJ per DRAM access compared to 3.7pJ for computation. TPUs represent the specialized...",
      "full_line": "Machine learning has emerged as one of the most computationally demanding fields, demonstrating the need for dedicated hardware that targets its unique characteristics. The performance requirements expose the limitations of general-purpose processors: traditional CPUs deliver approximately 100 GFLOPS (FP32) for neural network workloads, achieving only 5-10% utilization due to memory bandwidth[^fn-memory-bandwidth] constraints and sequential execution models. GPUs, originally designed for parallel graphics rendering, scale this to 15,700 GFLOPS (mixed precision) through massive parallelism but still struggle with energy efficiency[^fn-von-neumann], consuming 640pJ per DRAM access compared to 3.7pJ for computation. TPUs represent the specialized endpoint, achieving 275,000 INT8 operations per second with 85% utilization by implementing systolic arrays that minimize data movement and maximize operand reuse\u2014demonstrating how architectural specialization directly translates to quantifiable performance advantages."
    },
    {
      "footnote_id": "fn-tops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 217,
      "context": "...y and precision conversion overhead, as discussed in @sec-model-optimizations, since chip-level TOPS[^fn-tops] improvements may not translate directly to end-to-end training or deployment advantages.",
      "full_line": "A distinction in ML is the differing requirements between training and inference. Training demands both forward and backward passes through the network, with high numerical precision (e.g., FP32 or FP16) to ensure stable learning, while inference can often operate at lower precision (e.g., INT8) without accuracy loss. These precision trade-offs are closely related to the efficiency principles covered in @sec-efficient-ai. This variance drives the need for mixed-precision arithmetic hardware and enables optimizations that improve throughput and energy efficiency\u2014often achieving 4\u20138$\\times$ gains in chip-level measurements. While these hardware performance improvements are substantial, system-level benefits require careful evaluation of convergence complexity and precision conversion overhead, as discussed in @sec-model-optimizations, since chip-level TOPS[^fn-tops] improvements may not translate directly to end-to-end training or deployment advantages."
    },
    {
      "footnote_id": "fn-tops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 219,
      "context": "[^fn-tops]: **TOPS (Tera Operations Per Second)**: Measurement of computing performance equal to one trillion...",
      "full_line": "[^fn-tops]: **TOPS (Tera Operations Per Second)**: Measurement of computing performance equal to one trillion (10^12) operations per second, primarily used for AI accelerators. Modern chips range from smartphone neural engines (11 TOPS for Apple M1) to data center accelerators (756 TOPS for NVIDIA H100) [@nvidia2022h100]. However, TOPS varies dramatically by operation type\u2014integer operations are much faster than floating-point, making TOPS comparisons meaningful only within the same precision and operation type."
    },
    {
      "footnote_id": "fn-heterogeneous",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 298,
      "context": "...ilored performance tuning and energy efficiency strategies. Integration into heterogeneous computing[^fn-heterogeneous] environments demands interoperability that enables specialized units to coordinate effectively with...",
      "full_line": "Scalability drives additional complexity as AI accelerators deploy across diverse environments from high-throughput data centers to resource-constrained edge and mobile devices, requiring tailored performance tuning and energy efficiency strategies. Integration into heterogeneous computing[^fn-heterogeneous] environments demands interoperability that enables specialized units to coordinate effectively with conventional CPUs and GPUs in distributed systems."
    },
    {
      "footnote_id": "fn-heterogeneous",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 300,
      "context": "[^fn-heterogeneous]: **Heterogeneous Computing**: Computing systems that combine different types of processors (CPUs, G...",
      "full_line": "[^fn-heterogeneous]: **Heterogeneous Computing**: Computing systems that combine different types of processors (CPUs, GPUs, TPUs, FPGAs) to optimize performance for diverse workloads. Modern data centers mix x86 CPUs for control tasks, GPUs for training, and TPUs for inference. Programming heterogeneous systems requires frameworks like OpenCL or CUDA that can coordinate execution across different architectures, but offers 10-100x efficiency gains by matching each task to optimal hardware."
    },
    {
      "footnote_id": "fn-risc-v-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 391,
      "context": "...by operating on multiple data elements simultaneously. As shown in @lst-riscv_vector_mac, the RISC-V[^fn-risc-v-ai] assembly code demonstrates modern vector processing.",
      "full_line": "Vector processing units transform this execution pattern by operating on multiple data elements simultaneously. As shown in @lst-riscv_vector_mac, the RISC-V[^fn-risc-v-ai] assembly code demonstrates modern vector processing."
    },
    {
      "footnote_id": "fn-risc-v-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 393,
      "context": "[^fn-risc-v-ai]: **RISC-V for AI**: RISC-V, the open-source instruction set architecture from UC Berkeley (2010), i...",
      "full_line": "[^fn-risc-v-ai]: **RISC-V for AI**: RISC-V, the open-source instruction set architecture from UC Berkeley (2010), is becoming important for AI accelerators because it's freely customizable. Companies like SiFive and Google have created RISC-V chips with custom AI extensions. Unlike proprietary architectures, RISC-V allows hardware designers to add specialized ML instructions without licensing fees, potentially democratizing AI hardware development beyond the current duopoly of x86 and ARM."
    },
    {
      "footnote_id": "fn-cray-vector",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 435,
      "context": "...where large arrays of data required efficient parallel processing. Early systems such as the Cray-1[^fn-cray-vector], one of the first commercially successful supercomputers, introduced dedicated vector units to perf...",
      "full_line": "The principles underlying vector operations have long played a central role in high-performance computing. In the 1970s and 1980s, vector processors emerged as a critical architectural solution for scientific computing, weather modeling, and physics simulations, where large arrays of data required efficient parallel processing. Early systems such as the Cray-1[^fn-cray-vector], one of the first commercially successful supercomputers, introduced dedicated vector units to perform arithmetic operations on entire data vectors in a single instruction. This approach dramatically improved computational throughput compared to traditional scalar execution [@jordan1982guide]."
    },
    {
      "footnote_id": "fn-cray-vector",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 437,
      "context": "[^fn-cray-vector]: **Cray-1 Vector Legacy**: The Cray-1 (1975) cost $8.8 million ($50 million today) but could perfor...",
      "full_line": "[^fn-cray-vector]: **Cray-1 Vector Legacy**: The Cray-1 (1975) cost $8.8 million ($50 million today) but could perform 160 million floating-point operations per second\u20141000x faster than typical computers. Its 64-element vector registers and pipelined vector units established the architectural template that modern AI accelerators still follow: process many data elements simultaneously with specialized hardware pipelines."
    },
    {
      "footnote_id": "fn-simd-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 668,
      "context": "Single Instruction Multiple Data (SIMD)[^fn-simd-evolution] execution applies identical operations to multiple data elements in parallel, minimizing instructio...",
      "full_line": "Single Instruction Multiple Data (SIMD)[^fn-simd-evolution] execution applies identical operations to multiple data elements in parallel, minimizing instruction overhead while maximizing data throughput. This execution model is widely used to accelerate workloads with regular, independent data parallelism, such as neural network computations. The ARM Scalable Vector Extension (SVE) provides a representative example of how modern architectures implement SIMD operations efficiently, as illustrated in @lst-arm_sve_vector."
    },
    {
      "footnote_id": "fn-simd-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 670,
      "context": "[^fn-simd-evolution]: **SIMD Evolution**: SIMD originated in Flynn's 1966 taxonomy for scientific computing, but neural...",
      "full_line": "[^fn-simd-evolution]: **SIMD Evolution**: SIMD originated in Flynn's 1966 taxonomy for scientific computing, but neural networks transformed it from a niche HPC concept to mainstream necessity. Modern CPUs have 512-bit SIMD units (AVX-512), but AI pushed development of SIMT (Single Instruction, Multiple Thread) where thousands of lightweight threads execute in parallel\u2014GPU architectures now coordinate 65,536+ threads simultaneously, impossible with traditional SIMD."
    },
    {
      "footnote_id": "fn-hwacc-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 710,
      "context": "Tensor cores[^fn-hwacc-tensor-cores], implemented in architectures such as NVIDIA's Ampere GPUs, provide an example of this approach. Th...",
      "full_line": "Tensor cores[^fn-hwacc-tensor-cores], implemented in architectures such as NVIDIA's Ampere GPUs, provide an example of this approach. They expose matrix computation capabilities through specialized instructions, such as the tensor core operation shown in @lst-tensor_core_op on the NVIDIA A100 GPU."
    },
    {
      "footnote_id": "fn-hwacc-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 712,
      "context": "[^fn-hwacc-tensor-cores]: **Tensor Core Breakthrough**: NVIDIA introduced tensor cores in the V100 (2017) to accelerate the...",
      "full_line": "[^fn-hwacc-tensor-cores]: **Tensor Core Breakthrough**: NVIDIA introduced tensor cores in the V100 (2017) to accelerate the 4x4 matrix operations common in neural networks. The A100's third-generation tensor cores achieve 312 TFLOPS for FP16 tensor operations\u201420x faster than traditional CUDA cores. This single innovation enabled training of models like GPT-3 that would have been impossible with conventional hardware, fundamentally changing the scale of AI research."
    },
    {
      "footnote_id": "fn-hwacc-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 728,
      "context": "...nits arranged in systolic arrays to maximize sustained training throughput. Apple's M1 neural engine[^fn-hwacc-neural-engine] integrates smaller matrix processors optimized for mobile inference workloads, while Intel's Sapphi...",
      "full_line": "Tensor processing unit architectures differ based on design priorities. NVIDIA's Ampere architecture incorporates tensor cores optimized for general-purpose deep learning acceleration. Google's TPUv4 utilizes large-scale matrix units arranged in systolic arrays to maximize sustained training throughput. Apple's M1 neural engine[^fn-hwacc-neural-engine] integrates smaller matrix processors optimized for mobile inference workloads, while Intel's Sapphire Rapids architecture introduces AMX tiles designed for high-performance datacenter applications."
    },
    {
      "footnote_id": "fn-hwacc-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 730,
      "context": "[^fn-hwacc-neural-engine]: **Apple's Neural Engine Strategy**: Apple introduced the Neural Engine in 2017's A11 chip to enabl...",
      "full_line": "[^fn-hwacc-neural-engine]: **Apple's Neural Engine Strategy**: Apple introduced the Neural Engine in 2017's A11 chip to enable on-device ML without draining battery life. The M1's 16-core Neural Engine delivers 11 TOPS while the entire M1 chip has a 20-watt system TDP\u2014enabling real-time features like live text recognition and voice processing without cloud connectivity. This \"privacy through hardware\" approach influenced the entire industry to prioritize edge AI capabilities."
    },
    {
      "footnote_id": "fn-systolic-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 750,
      "context": "The concept of systolic arrays was first introduced by Kung and Leiserson[^fn-systolic-origin], who formalized their use in parallel computing architectures for efficient matrix operations [@Kun...",
      "full_line": "The concept of systolic arrays was first introduced by Kung and Leiserson[^fn-systolic-origin], who formalized their use in parallel computing architectures for efficient matrix operations [@Kung1982]. Unlike general-purpose execution units, systolic arrays exploit spatial and temporal locality by reusing operands as they propagate through the grid. Google's Tensor Processing Unit (TPU) exemplifies this architectural approach. In the TPUv4, a $128\\times128$ systolic array of multiply-accumulate units processes matrix operations by streaming data through the array in a pipelined manner, as shown in @fig-systolic-array."
    },
    {
      "footnote_id": "fn-systolic-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 763,
      "context": "[^fn-systolic-origin]: **Systolic Array Renaissance**: H.T. Kung and Charles Leiserson introduced systolic arrays at CMU...",
      "full_line": "[^fn-systolic-origin]: **Systolic Array Renaissance**: H.T. Kung and Charles Leiserson introduced systolic arrays at CMU in 1979 for VLSI signal processing, but the concept languished for decades due to programming complexity. Google's 2016 TPU resurrection proved these \"heartbeat\" architectures could deliver massive efficiency gains for neural networks\u2014the TPUv1's 256x256 systolic array achieved 92 TOPS while consuming just 40 watts, making systolic arrays the dominant AI architecture today."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 945,
      "context": "| Accelerator      | List Price (USD)| Peak FLOPS[^fn-flops] (FP16)   | Memory Bandwidth     | Price/Performance     |",
      "full_line": "| Accelerator      | List Price (USD)| Peak FLOPS[^fn-flops] (FP16)   | Memory Bandwidth     | Price/Performance     |"
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 958,
      "context": "[^fn-flops]: **FLOPS (Floating-Point Operations Per Second)**: Standard measure of computing performance for sc...",
      "full_line": "[^fn-flops]: **FLOPS (Floating-Point Operations Per Second)**: Standard measure of computing performance for scientific and AI workloads, with TFLOPS = trillion FLOPS. The world's fastest supercomputer (Frontier) achieves 1.1 exaFLOPS (10^18), while a smartphone typically delivers 1-10 GFLOPS. However, modern AI hardware often favors integer operations (TOPS) over floating-point (FLOPS) because neural networks can use lower precision without accuracy loss."
    },
    {
      "footnote_id": "fn-chiplet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2656,
      "context": "...eyond a single monolithic chip while still maintaining a compact, tightly integrated design. Chiplet[^fn-chiplet] architectures achieve this by partitioning large designs into smaller, modular dies that are interc...",
      "full_line": "The first step in scaling AI accelerators is to move beyond a single monolithic chip while still maintaining a compact, tightly integrated design. Chiplet[^fn-chiplet] architectures achieve this by partitioning large designs into smaller, modular dies that are interconnected within a single package, as illustrated in @fig-AMD_chiplet_based."
    },
    {
      "footnote_id": "fn-hbm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2658,
      "context": "...ty beyond monolithic die limitations and improving manufacturing yields. High-bandwidth memory (HBM)[^fn-hbm] stacks provide fast access to data, crucial for the memory-intensive workloads common in machine le...",
      "full_line": "![**Chiplet Interconnect**: Modern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. High-bandwidth memory (HBM)[^fn-hbm] stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning.](images/png/chiplets.png){#fig-AMD_chiplet_based}"
    },
    {
      "footnote_id": "fn-chiplet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2660,
      "context": "[^fn-chiplet]: **Chiplet**: Small, specialized semiconductor dies that are connected together within a single pac...",
      "full_line": "[^fn-chiplet]: **Chiplet**: Small, specialized semiconductor dies that are connected together within a single package to create larger, more complex processors. AMD's EPYC processors use up to 8 chiplets connected via Infinity Fabric, achieving yields above 80% versus 20% for equivalent monolithic designs. This modular approach reduces manufacturing costs and enables mixing different technologies\u2014compute chiplets in 7nm with I/O chiplets in 14nm\u2014optimizing each function independently."
    },
    {
      "footnote_id": "fn-hbm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2662,
      "context": "[^fn-hbm]: **High-Bandwidth Memory (HBM)**: Advanced DRAM technology that stacks multiple memory dies vertica...",
      "full_line": "[^fn-hbm]: **High-Bandwidth Memory (HBM)**: Advanced DRAM technology that stacks multiple memory dies vertically with thousands of connections, achieving 1,000+ GB/s bandwidth versus 50 GB/s for traditional GDDR. HBM2e can deliver 3.2 TB/s for NVIDIA H100 GPUs [@nvidia2022h100], but costs 5-10x more than GDDR and consumes significant power. This extreme bandwidth is essential for AI workloads where memory access, not computation, often limits performance."
    },
    {
      "footnote_id": "fn-memory-coherence",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2666,
      "context": "...le package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence[^fn-memory-coherence], and thermal management become critical factors as more chiplets are integrated. Unlike traditional...",
      "full_line": "However, even within a single package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence[^fn-memory-coherence], and thermal management become critical factors as more chiplets are integrated. Unlike traditional multi-chip systems, chiplet-based designs must carefully balance latency-sensitive workloads across multiple dies without introducing excessive bottlenecks."
    },
    {
      "footnote_id": "fn-memory-coherence",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2668,
      "context": "[^fn-memory-coherence]: **Memory Coherence**: Ensuring all processors in a system see the same consistent view of shared m...",
      "full_line": "[^fn-memory-coherence]: **Memory Coherence**: Ensuring all processors in a system see the same consistent view of shared memory when multiple cores/chips access the same data. Traditional cache coherence protocols like MESI add 10-50ns latency for multi-core CPUs. For AI accelerators with thousands of cores, coherence becomes prohibitively expensive\u2014most ML hardware instead uses explicit memory management where programmers control data placement and synchronization manually."
    },
    {
      "footnote_id": "fn-wafer-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2970,
      "context": "At the frontier of AI scaling, wafer-scale[^fn-wafer-scale] integration represents a paradigm shift\u2014abandoning traditional multi-chip architectures in favor of...",
      "full_line": "At the frontier of AI scaling, wafer-scale[^fn-wafer-scale] integration represents a paradigm shift\u2014abandoning traditional multi-chip architectures in favor of a single, massive AI processor. Rather than partitioning computation across discrete chips, this approach treats an entire silicon wafer as a unified compute fabric, eliminating the inefficiencies of inter-chip communication."
    },
    {
      "footnote_id": "fn-wafer-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2972,
      "context": "[^fn-wafer-scale]: **Wafer-Scale Integration**: Using an entire 300mm silicon wafer as a single processor instead of...",
      "full_line": "[^fn-wafer-scale]: **Wafer-Scale Integration**: Using an entire 300mm silicon wafer as a single processor instead of cutting it into individual chips. Cerebras WSE-3 contains 4 trillion transistors across 850,000 cores\u2014125x more than the largest GPUs. Manufacturing challenges include 100% yield requirements (solved with redundant cores) and cooling 23kW of power. This approach eliminates inter-chip communication delays but costs $2-3 million per wafer versus $40,000 for equivalent GPU clusters."
    },
    {
      "footnote_id": "fn-electrical-grids",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 46,
      "context": "...world. They manage traffic flows in our cities, optimize power distribution across electrical grids[^fn-electrical-grids], and enable billions of wireless devices to communicate seamlessly through IoT networks[^fn-iot-net...",
      "full_line": "Artificial Intelligence (AI) has emerged as one of the most transformative forces in human history. From the moment we wake up to when we go to sleep, AI systems invisibly shape our world. They manage traffic flows in our cities, optimize power distribution across electrical grids[^fn-electrical-grids], and enable billions of wireless devices to communicate seamlessly through IoT networks[^fn-iot-networks]. In hospitals, AI analyzes medical images and helps doctors diagnose diseases. In research laboratories, it accelerates scientific discovery by simulating molecular interactions and processing vast datasets from particle accelerators[^fn-particle-accelerators]. In space exploration, it helps rovers traverse distant planets and telescopes detect new celestial phenomena."
    },
    {
      "footnote_id": "fn-iot-networks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 46,
      "context": "...rical-grids], and enable billions of wireless devices to communicate seamlessly through IoT networks[^fn-iot-networks]. In hospitals, AI analyzes medical images and helps doctors diagnose diseases. In research laborato...",
      "full_line": "Artificial Intelligence (AI) has emerged as one of the most transformative forces in human history. From the moment we wake up to when we go to sleep, AI systems invisibly shape our world. They manage traffic flows in our cities, optimize power distribution across electrical grids[^fn-electrical-grids], and enable billions of wireless devices to communicate seamlessly through IoT networks[^fn-iot-networks]. In hospitals, AI analyzes medical images and helps doctors diagnose diseases. In research laboratories, it accelerates scientific discovery by simulating molecular interactions and processing vast datasets from particle accelerators[^fn-particle-accelerators]. In space exploration, it helps rovers traverse distant planets and telescopes detect new celestial phenomena."
    },
    {
      "footnote_id": "fn-particle-accelerators",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 46,
      "context": "...scovery by simulating molecular interactions and processing vast datasets from particle accelerators[^fn-particle-accelerators]. In space exploration, it helps rovers traverse distant planets and telescopes detect new celestial...",
      "full_line": "Artificial Intelligence (AI) has emerged as one of the most transformative forces in human history. From the moment we wake up to when we go to sleep, AI systems invisibly shape our world. They manage traffic flows in our cities, optimize power distribution across electrical grids[^fn-electrical-grids], and enable billions of wireless devices to communicate seamlessly through IoT networks[^fn-iot-networks]. In hospitals, AI analyzes medical images and helps doctors diagnose diseases. In research laboratories, it accelerates scientific discovery by simulating molecular interactions and processing vast datasets from particle accelerators[^fn-particle-accelerators]. In space exploration, it helps rovers traverse distant planets and telescopes detect new celestial phenomena."
    },
    {
      "footnote_id": "fn-electrical-grids",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 48,
      "context": "[^fn-electrical-grids]: **Electrical Grids**: Interconnected networks of power generation, transmission, and distribution...",
      "full_line": "[^fn-electrical-grids]: **Electrical Grids**: Interconnected networks of power generation, transmission, and distribution infrastructure serving millions of customers. Modern smart grids use AI to predict demand patterns, prevent blackouts, and integrate renewable energy sources like solar and wind. For example, Google's DeepMind reduced cooling costs in data centers by 40% through AI-optimized power management, demonstrating how machine learning can dramatically improve energy efficiency at scale."
    },
    {
      "footnote_id": "fn-iot-networks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 50,
      "context": "[^fn-iot-networks]: **IoT (Internet of Things) Networks**: Networks connecting billions of smart devices\u2014from sensors...",
      "full_line": "[^fn-iot-networks]: **IoT (Internet of Things) Networks**: Networks connecting billions of smart devices\u2014from sensors and cameras to appliances and vehicles\u2014that collect and exchange data automatically. By 2025, an estimated 75 billion IoT devices will generate over 79 zettabytes of data annually. AI processes this massive data stream to enable smart cities, autonomous vehicles, and predictive maintenance systems."
    },
    {
      "footnote_id": "fn-particle-accelerators",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 52,
      "context": "[^fn-particle-accelerators]: **Particle Accelerators**: Scientific instruments that accelerate subatomic particles to near ligh...",
      "full_line": "[^fn-particle-accelerators]: **Particle Accelerators**: Scientific instruments that accelerate subatomic particles to near light speed for physics research. The Large Hadron Collider (LHC) at CERN generates 50 petabytes of data annually\u2014equivalent to 50 million gigabytes\u2014requiring AI to identify rare particle collision signatures among billions of events. Without machine learning, discovering particles like the Higgs boson would be computationally impossible."
    },
    {
      "footnote_id": "fn-superhuman-capabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 60,
      "context": "...ring challenge. Building systems that learn, reason, and potentially achieve superhuman capabilities[^fn-superhuman-capabilities] in specific domains requires new expertise.",
      "full_line": "This transformation proceeds rapidly. While the Industrial Revolution unfolded over centuries and the Digital Revolution over decades, AI capabilities advance at an accelerated rate. Technologies that seemed impossible years ago\u2014systems understanding human speech, generating content, or making complex decisions\u2014are now commonplace. This acceleration indicates we are beginning to understand AI's profound impact on society. We stand at a historic inflection point. The Industrial Revolution required mastering mechanical engineering to control steam and machinery. The Digital Revolution demanded electrical and computer engineering expertise to build the internet age. The AI Revolution presents a new engineering challenge. Building systems that learn, reason, and potentially achieve superhuman capabilities[^fn-superhuman-capabilities] in specific domains requires new expertise."
    },
    {
      "footnote_id": "fn-superhuman-capabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 62,
      "context": "[^fn-superhuman-capabilities]: **Superhuman AI Capabilities**: AI systems already exceed human performance in specific domains. A...",
      "full_line": "[^fn-superhuman-capabilities]: **Superhuman AI Capabilities**: AI systems already exceed human performance in specific domains. AlphaGo defeated the world champion in Go (a game with more possible positions than atoms in the observable universe), protein folding prediction models like AlphaFold achieved accuracy that would take human scientists decades to match, and modern language models can process and synthesize information from millions of documents in seconds."
    },
    {
      "footnote_id": "fn-paradigm-shift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 82,
      "context": "...tical approach to artificial intelligence through extensive research and fundamental paradigm shifts[^fn-paradigm-shift]. The progression of artificial intelligence encompasses both theoretical advances in understanding...",
      "full_line": "This distinction matters because modern ML's data-driven approach requires sophisticated systems to collect, process, and learn from data at unprecedented scale. Machine learning emerged as a practical approach to artificial intelligence through extensive research and fundamental paradigm shifts[^fn-paradigm-shift]. The progression of artificial intelligence encompasses both theoretical advances in understanding intelligence and practical developments in implementation methodologies, explaining why systems engineering has become crucial for achieving AI goals. The modern deep learning approaches that emerged from this evolution form the algorithmic foundation of today's AI systems."
    },
    {
      "footnote_id": "fn-paradigm-shift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 90,
      "context": "...volution from rule-based AI to data-driven ML represents one of the most significant paradigm shifts[^fn-paradigm-shift] in computing history. This shift explains why ML systems engineering has emerged as a critical disc...",
      "full_line": "The evolution from rule-based AI to data-driven ML represents one of the most significant paradigm shifts[^fn-paradigm-shift] in computing history. This shift explains why ML systems engineering has emerged as a critical discipline\u2014because the path to intelligent systems now runs through the engineering challenge of building systems that can effectively learn from data at massive scale."
    },
    {
      "footnote_id": "fn-paradigm-shift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 92,
      "context": "[^fn-paradigm-shift]: **Paradigm Shift**: A term coined by philosopher Thomas Kuhn in 1962 [@kuhn1962structure] to descr...",
      "full_line": "[^fn-paradigm-shift]: **Paradigm Shift**: A term coined by philosopher Thomas Kuhn in 1962 [@kuhn1962structure] to describe fundamental changes in scientific approach. In AI, the key paradigm shift was moving from symbolic reasoning (encoding human knowledge as rules) to statistical learning (discovering patterns from data). This shift required abandoning decades of established methods and embracing radically different approaches to creating intelligence."
    },
    {
      "footnote_id": "fn-early",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 98,
      "context": "...eline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt [@rosenblatt1957perceptron], an early computational learning algorithm....",
      "full_line": "The evolution of AI, depicted in the timeline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt [@rosenblatt1957perceptron], an early computational learning algorithm. Computer labs in 1965 contained room-sized mainframes[^fn-mainframes] running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, though groundbreaking for their time, differed substantially from today's machine learning systems that detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garry Kasparov in 1997 [@campbell2002deep]. More recent advancements include the introduction of OpenAI's GPT-3 in 2020 and GPT-4 in 2023 [@openai2023gpt4], demonstrating the dramatic evolution and increasing complexity of AI systems over the decades."
    },
    {
      "footnote_id": "fn-mainframes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 98,
      "context": "...n], an early computational learning algorithm. Computer labs in 1965 contained room-sized mainframes[^fn-mainframes] running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe...",
      "full_line": "The evolution of AI, depicted in the timeline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt [@rosenblatt1957perceptron], an early computational learning algorithm. Computer labs in 1965 contained room-sized mainframes[^fn-mainframes] running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, though groundbreaking for their time, differed substantially from today's machine learning systems that detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garry Kasparov in 1997 [@campbell2002deep]. More recent advancements include the introduction of OpenAI's GPT-3 in 2020 and GPT-4 in 2023 [@openai2023gpt4], demonstrating the dramatic evolution and increasing complexity of AI systems over the decades."
    },
    {
      "footnote_id": "fn-eliza",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 98,
      "context": "...or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garr...",
      "full_line": "The evolution of AI, depicted in the timeline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt [@rosenblatt1957perceptron], an early computational learning algorithm. Computer labs in 1965 contained room-sized mainframes[^fn-mainframes] running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, though groundbreaking for their time, differed substantially from today's machine learning systems that detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garry Kasparov in 1997 [@campbell2002deep]. More recent advancements include the introduction of OpenAI's GPT-3 in 2020 and GPT-4 in 2023 [@openai2023gpt4], demonstrating the dramatic evolution and increasing complexity of AI systems over the decades."
    },
    {
      "footnote_id": "fn-early",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 100,
      "context": "[^fn-early]: **Perceptron**: One of the first computational learning algorithms. This system could learn to cla...",
      "full_line": "[^fn-early]: **Perceptron**: One of the first computational learning algorithms. This system could learn to classify patterns by making yes/no decisions based on inputs."
    },
    {
      "footnote_id": "fn-mainframes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 102,
      "context": "[^fn-mainframes]: **Mainframes**: Room-sized computers that dominated the 1960s-70s, typically costing millions of d...",
      "full_line": "[^fn-mainframes]: **Mainframes**: Room-sized computers that dominated the 1960s-70s, typically costing millions of dollars and requiring dedicated cooling systems. IBM's System/360 mainframe from 1964 weighed 20,000 pounds and had just 64KB of memory, about 1/millionth the memory of a modern smartphone, yet represented the cutting edge of computing power that enabled early AI research."
    },
    {
      "footnote_id": "fn-eliza",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 104,
      "context": "[^fn-eliza]: **ELIZA**: Created by MIT's Joseph Weizenbaum in 1966 [@weizenbaum1966eliza], ELIZA was one of the...",
      "full_line": "[^fn-eliza]: **ELIZA**: Created by MIT's Joseph Weizenbaum in 1966 [@weizenbaum1966eliza], ELIZA was one of the first chatbots that could simulate human conversation by pattern matching and substitution. Ironically, Weizenbaum was horrified when people began forming emotional attachments to his simple program, leading him to become a critic of AI."
    },
    {
      "footnote_id": "fn-dartmouth-conference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 289,
      "context": "The story of machine learning begins at the historic Dartmouth Conference[^fn-dartmouth-conference] in 1956, where pioneers like John McCarthy, Marvin Minsky, and Claude Shannon first coined the term...",
      "full_line": "The story of machine learning begins at the historic Dartmouth Conference[^fn-dartmouth-conference] in 1956, where pioneers like John McCarthy, Marvin Minsky, and Claude Shannon first coined the term \"artificial intelligence\" [@mccarthy1956dartmouth]. Their approach: intelligence could be reduced to symbol manipulation. Daniel Bobrow's STUDENT system from 1964 [@bobrow1964student] exemplifies this era by solving algebra word problems through natural language understanding."
    },
    {
      "footnote_id": "fn-dartmouth-conference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 291,
      "context": "[^fn-dartmouth-conference]: **Dartmouth Conference (1956)**: The legendary 8-week workshop at Dartmouth College where AI was o...",
      "full_line": "[^fn-dartmouth-conference]: **Dartmouth Conference (1956)**: The legendary 8-week workshop at Dartmouth College where AI was officially born. Organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, it was the first time researchers gathered specifically to discuss \"artificial intelligence,\" a term McCarthy coined for the proposal. The ambitious goal was to make machines \"simulate every aspect of learning or any other feature of intelligence.\" Though overly optimistic, this gathering launched AI as a formal research field."
    },
    {
      "footnote_id": "fn-brittleness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 309,
      "context": "...word order, synonyms, or natural speech patterns would cause the system to fail. This \"brittleness\"[^fn-brittleness] meant that while these solutions could appear intelligent when handling very specific cases they we...",
      "full_line": "Early AI like STUDENT suffered from a fundamental limitation: they could only handle inputs that exactly matched their pre-programmed patterns and rules. A language translator that only works with perfect grammatical structure demonstrates this limitation. Even slight variations like changed word order, synonyms, or natural speech patterns would cause the system to fail. This \"brittleness\"[^fn-brittleness] meant that while these solutions could appear intelligent when handling very specific cases they were designed for, they would break down completely when faced with even minor variations or real-world complexity. This limitation revealed a deeper problem with rule-based AI approaches: they couldn't genuinely understand or generalize from their programming, only match and manipulate text patterns exactly as specified."
    },
    {
      "footnote_id": "fn-brittleness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 311,
      "context": "[^fn-brittleness]: **Brittleness in AI Systems**: The tendency of rule-based systems to fail completely when encounte...",
      "full_line": "[^fn-brittleness]: **Brittleness in AI Systems**: The tendency of rule-based systems to fail completely when encountering inputs that fall outside their programmed scenarios, no matter how similar those inputs might be to what they were designed to handle. This contrasts with human intelligence, which can adapt and make reasonable guesses even in unfamiliar situations. The brittleness problem drove researchers toward machine learning approaches that could generalize from examples rather than relying on exhaustive rule sets."
    },
    {
      "footnote_id": "fn-mooreslaw",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 349,
      "context": "...evolution meant massive amounts of data were suddenly available to train the algorithms. Moore's Law[^fn-mooreslaw] delivered the computational power needed to process this data effectively. And researchers develope...",
      "full_line": "The 1990s marked a radical transformation in artificial intelligence as the field shifted from hand-coded rules toward statistical learning approaches. Three converging factors made statistical methods both possible and powerful. The digital revolution meant massive amounts of data were suddenly available to train the algorithms. Moore's Law[^fn-mooreslaw] delivered the computational power needed to process this data effectively. And researchers developed new algorithms like Support Vector Machines and improved neural networks that could actually learn patterns from this data rather than following pre-programmed rules. This combination transformed AI development: rather than encoding human knowledge directly, machines could discover patterns automatically from examples, creating more robust and adaptable systems."
    },
    {
      "footnote_id": "fn-mooreslaw",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 351,
      "context": "[^fn-mooreslaw]: **Moore's Law**: The observation made by Intel co-founder Gordon Moore in 1965 that the number of...",
      "full_line": "[^fn-mooreslaw]: **Moore's Law**: The observation made by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years, while the cost halves. This exponential growth in computing power has been a key driver of advances in machine learning, though the pace has begun to slow in recent years."
    },
    {
      "footnote_id": "fn-viola-jones",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 417,
      "context": "The Viola-Jones algorithm [@viola2001rapidobject][^fn-viola-jones] (2001) exemplifies this era, achieving real-time face detection using simple rectangular features a...",
      "full_line": "The Viola-Jones algorithm [@viola2001rapidobject][^fn-viola-jones] (2001) exemplifies this era, achieving real-time face detection using simple rectangular features and cascaded classifiers[^fn-cascade]. This algorithm powered digital camera face detection for nearly a decade."
    },
    {
      "footnote_id": "fn-cascade",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 417,
      "context": "...s era, achieving real-time face detection using simple rectangular features and cascaded classifiers[^fn-cascade]. This algorithm powered digital camera face detection for nearly a decade.",
      "full_line": "The Viola-Jones algorithm [@viola2001rapidobject][^fn-viola-jones] (2001) exemplifies this era, achieving real-time face detection using simple rectangular features and cascaded classifiers[^fn-cascade]. This algorithm powered digital camera face detection for nearly a decade."
    },
    {
      "footnote_id": "fn-viola-jones",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 419,
      "context": "[^fn-viola-jones]: **Viola-Jones Algorithm**: A groundbreaking computer vision algorithm that could detect faces in r...",
      "full_line": "[^fn-viola-jones]: **Viola-Jones Algorithm**: A groundbreaking computer vision algorithm that could detect faces in real-time by using simple rectangular patterns (like comparing the brightness of eye regions versus cheek regions) and making decisions in stages, filtering out non-faces quickly and spending more computation only on promising candidates."
    },
    {
      "footnote_id": "fn-cascade",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 421,
      "context": "[^fn-cascade]: **Cascade of Classifiers**: A multi-stage decision system where each stage acts as a filter, quick...",
      "full_line": "[^fn-cascade]: **Cascade of Classifiers**: A multi-stage decision system where each stage acts as a filter, quickly rejecting obvious non-matches and passing promising candidates to the next, more sophisticated stage. This approach is similar to how security screening works at airports with multiple checkpoints of increasing thoroughness."
    },
    {
      "footnote_id": "fn-neurons",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 425,
      "context": "...n architecture. Deep learning employs layers of simple computational units inspired by brain neurons[^fn-neurons], with each layer transforming input data into increasingly abstract representations. The detailed a...",
      "full_line": "While Support Vector Machines excelled at finding complex category boundaries through mathematical transformations, deep learning adopted a radically different approach inspired by brain architecture. Deep learning employs layers of simple computational units inspired by brain neurons[^fn-neurons], with each layer transforming input data into increasingly abstract representations. The detailed architecture and functioning of these neural networks are explored in @sec-dl-primer and @sec-dnn-architectures."
    },
    {
      "footnote_id": "fn-neurons",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 427,
      "context": "[^fn-neurons]: **Artificial Neurons**: Basic computational units in neural networks that mimic biological neurons...",
      "full_line": "[^fn-neurons]: **Artificial Neurons**: Basic computational units in neural networks that mimic biological neurons, taking multiple inputs, applying weights and biases, and producing an output signal through an activation function."
    },
    {
      "footnote_id": "fn-intro-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 433,
      "context": "AlexNet, shown in @fig-alexnet, achieved a breakthrough in the 2012 ImageNet[^fn-intro-imagenet] competition that transformed machine learning through a perfect alignment of algorithmic innovation...",
      "full_line": "AlexNet, shown in @fig-alexnet, achieved a breakthrough in the 2012 ImageNet[^fn-intro-imagenet] competition that transformed machine learning through a perfect alignment of algorithmic innovation and hardware capability. The network required two NVIDIA GTX 580 GPUs with 3GB memory each, delivering 2.3 GFLOPS peak performance per GPU, but the real breakthrough was memory bandwidth utilization. Each GTX 580 provided 192 GB/s memory bandwidth, and AlexNet's convolutional operations required approximately 288 GB/s total memory bandwidth to feed the computation engines\u2014making this the first neural network specifically designed around memory bandwidth constraints rather than just compute requirements. The 60 million parameters demanded 240MB storage, while training on 1.2 million images required sophisticated memory management to split the network across GPU boundaries and coordinate gradient updates. Training consumed approximately 1,287 GPU-hours over 6 days, achieving 15.3% top-5 error rate compared to 26.2% for second place\u2014a 42% relative improvement that demonstrated the power of hardware-software co-design. This represented a 10-100x speedup over CPU implementations, reducing training time from months to days and proving that specialized hardware could unlock previously intractable algorithms [@krizhevsky2012imagenet]."
    },
    {
      "footnote_id": "fn-intro-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 435,
      "context": "[^fn-intro-imagenet]: **ImageNet**: A massive visual database containing over 14 million labeled images across 20,000+ c...",
      "full_line": "[^fn-intro-imagenet]: **ImageNet**: A massive visual database containing over 14 million labeled images across 20,000+ categories, created by Stanford's Fei-Fei Li starting in 2009 [@deng2009imagenet]. The annual ImageNet challenge became the Olympics of computer vision, driving breakthrough after breakthrough in image recognition until neural networks became so good they essentially solved the competition."
    },
    {
      "footnote_id": "fn-intro-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 863,
      "context": "...works thousands of times larger than AlexNet. These massive models, often called \"foundation models\"[^fn-intro-foundation-models], took deep learning to new heights.",
      "full_line": "Deep learning subsequently entered an era of extraordinary scale. By the late 2010s, companies like Google, Facebook, and OpenAI trained neural networks thousands of times larger than AlexNet. These massive models, often called \"foundation models\"[^fn-intro-foundation-models], took deep learning to new heights."
    },
    {
      "footnote_id": "fn-intro-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 865,
      "context": "[^fn-intro-foundation-models]: **Foundation Models**: Large-scale AI models trained on broad datasets that serve as the \"foundati...",
      "full_line": "[^fn-intro-foundation-models]: **Foundation Models**: Large-scale AI models trained on broad datasets that serve as the \"foundation\" for many different applications through fine-tuning\u2014like GPT for language tasks or CLIP for vision tasks. The term was coined by Stanford's AI researchers in 2021 to capture how these models became the basis for building more specific AI systems."
    },
    {
      "footnote_id": "fn-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 867,
      "context": "...enting a 1,000x scale increase from earlier neural networks like BERT-Large (340 million parameters)[^fn-parameters]. Training GPT-3 consumed approximately 314 zettaFLOPs of computation across 1,024 V100 GPUs over se...",
      "full_line": "GPT-3, released in 2020 [@brown2020language], contained 175 billion parameters requiring approximately 800GB of memory to store, representing a 1,000x scale increase from earlier neural networks like BERT-Large (340 million parameters)[^fn-parameters]. Training GPT-3 consumed approximately 314 zettaFLOPs of computation across 1,024 V100 GPUs over several weeks, with training costs estimated at $4.6 million. The model processes text at approximately 1.7GB/s memory bandwidth and requires specialized infrastructure to serve millions of users with sub-second latency. These models demonstrated remarkable emergent abilities that appeared only at scale: writing human-like text, engaging in sophisticated conversation, generating images from descriptions, and writing functional computer code\u2014capabilities that emerged not from explicit programming but from the sheer scale of computation and data."
    },
    {
      "footnote_id": "fn-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 869,
      "context": "[^fn-parameters]: **Parameters**: The adjustable values within a neural network that are modified during training, s...",
      "full_line": "[^fn-parameters]: **Parameters**: The adjustable values within a neural network that are modified during training, similar to how the brain's neural connections grow stronger as you learn a new skill. Having more parameters generally means that the model can learn more complex patterns."
    },
    {
      "footnote_id": "fn-training-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 871,
      "context": "...capable of solving increasingly complex tasks. This scale introduced significant systems challenges[^fn-training-challenges]. Efficiently training large models requires thousands of parallel GPUs, storing and serving models...",
      "full_line": "A key insight emerged: larger neural networks trained on more data became capable of solving increasingly complex tasks. This scale introduced significant systems challenges[^fn-training-challenges]. Efficiently training large models requires thousands of parallel GPUs, storing and serving models hundreds of gigabytes in size, and handling massive training datasets."
    },
    {
      "footnote_id": "fn-training-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 873,
      "context": "[^fn-training-challenges]: **Large-Scale Training Challenges**: Training GPT-3 required approximately 3,640 petaflop-days. At...",
      "full_line": "[^fn-training-challenges]: **Large-Scale Training Challenges**: Training GPT-3 required approximately 3,640 petaflop-days. At $2-3 per GPU-hour on cloud platforms (2020 pricing), this translates to approximately $4.6M in compute costs alone (Lambda Labs estimate), excluding data preprocessing, experimentation, and failed training runs [@li2020estimating]. Rule of thumb: total project cost is typically 3-5x raw compute cost due to experimentation overhead, making the full GPT-3 development cost approximately $15-20M. Modern foundation models can consume 100+ terabytes of training data and require specialized distributed training techniques to coordinate thousands of accelerators across multiple data centers."
    },
    {
      "footnote_id": "fn-backprop-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 875,
      "context": "...t breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation [@rumelhart1986learning][^fn-backprop-history] in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated it...",
      "full_line": "The 2012 deep learning revolution built upon neural network research dating to the 1950s. The story begins with Frank Rosenblatt's Perceptron in 1957, which captured the imagination of researchers by showing how a simple artificial neuron could learn to classify patterns. Though limited to linearly separable problems, as Minsky and Papert's 1969 book \"Perceptrons\" [@minsky1969perceptrons] demonstrated, it introduced the core concept of trainable neural networks. The 1980s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation [@rumelhart1986learning][^fn-backprop-history] in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated its practical application in recognizing handwritten digits using specialized neural networks designed for image processing [@lecun1989backpropagation][^fn-cnn]."
    },
    {
      "footnote_id": "fn-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 875,
      "context": "...n digits using specialized neural networks designed for image processing [@lecun1989backpropagation][^fn-cnn].",
      "full_line": "The 2012 deep learning revolution built upon neural network research dating to the 1950s. The story begins with Frank Rosenblatt's Perceptron in 1957, which captured the imagination of researchers by showing how a simple artificial neuron could learn to classify patterns. Though limited to linearly separable problems, as Minsky and Papert's 1969 book \"Perceptrons\" [@minsky1969perceptrons] demonstrated, it introduced the core concept of trainable neural networks. The 1980s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation [@rumelhart1986learning][^fn-backprop-history] in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated its practical application in recognizing handwritten digits using specialized neural networks designed for image processing [@lecun1989backpropagation][^fn-cnn]."
    },
    {
      "footnote_id": "fn-backprop-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 877,
      "context": "[^fn-backprop-history]: **Backpropagation (Historical Context)**: A mathematical technique that allows neural networks to...",
      "full_line": "[^fn-backprop-history]: **Backpropagation (Historical Context)**: A mathematical technique that allows neural networks to learn by calculating how much each component contributed to errors and adjusting accordingly\u2014like a coach analyzing a team's mistakes and giving each player specific feedback to improve their performance."
    },
    {
      "footnote_id": "fn-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 879,
      "context": "[^fn-cnn]: **Convolutional Neural Network (CNN)**: A type of neural network specially designed for processing...",
      "full_line": "[^fn-cnn]: **Convolutional Neural Network (CNN)**: A type of neural network specially designed for processing images, inspired by how the human visual system works. The \"convolutional\" part refers to how it scans images in small chunks, similar to how our eyes focus on different parts of a scene."
    },
    {
      "footnote_id": "fn-sutton-turing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 895,
      "context": "...ard Sutton articulated in his influential 2019 essay \"The Bitter Lesson\" [@sutton2019bitter]. Sutton[^fn-sutton-turing] observed that the greatest breakthroughs in AI have consistently come not from incorporating human...",
      "full_line": "This historical analysis points to a key insight about the nature of AI progress, one that reinforcement learning pioneer Richard Sutton articulated in his influential 2019 essay \"The Bitter Lesson\" [@sutton2019bitter]. Sutton[^fn-sutton-turing] observed that the greatest breakthroughs in AI have consistently come not from incorporating human knowledge and expertise, but from scaling general-purpose methods that leverage massive computational resources."
    },
    {
      "footnote_id": "fn-sutton-turing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 897,
      "context": "[^fn-sutton-turing]: **Richard Sutton**: A foundational figure in reinforcement learning and AI, Sutton co-authored the...",
      "full_line": "[^fn-sutton-turing]: **Richard Sutton**: A foundational figure in reinforcement learning and AI, Sutton co-authored the seminal textbook \"Reinforcement Learning: An Introduction\" and developed key algorithms like temporal difference learning. He received the 2024 Turing Award, computing's highest honor, for his pioneering contributions to reinforcement learning that have significantly shaped how AI systems learn and adapt. His perspective on AI's development carries particular weight given his decades of contributions to both the theoretical foundations and practical advancement of machine learning."
    },
    {
      "footnote_id": "fn-petabytes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 903,
      "context": "...eering challenge is building systems that can manage this scale: collecting and processing petabytes[^fn-petabytes] of training data, coordinating training across thousands of GPUs each consuming 300-500 watts, serv...",
      "full_line": "Consider modern language models like GPT-4 or image generation systems like DALL-E. Their capabilities emerge not from sophisticated linguistic or artistic theories encoded by humans, but from training general-purpose neural networks on vast amounts of data using enormous computational resources. Training GPT-3 consumed approximately 1,287 MWh of energy\u2014equivalent to 120 U.S. homes for a year\u2014while serving the model to millions of users requires data centers consuming megawatts of continuous power. The engineering challenge is building systems that can manage this scale: collecting and processing petabytes[^fn-petabytes] of training data, coordinating training across thousands of GPUs each consuming 300-500 watts, serving models to millions of users with millisecond latency while managing thermal and power constraints, and continuously updating systems based on real-world performance."
    },
    {
      "footnote_id": "fn-amdahls-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 905,
      "context": "...n, making data movement the dominant factor in both performance and energy consumption. Amdahl's Law[^fn-amdahls-law] quantifies this limitation: if data movement takes 80% of execution time, even infinite compute cap...",
      "full_line": "However, the fundamental constraint in modern ML systems is not compute capacity but memory bandwidth\u2014the rate at which data can move between storage and processing units. This memory wall represents the primary bottleneck that determines system performance. Modern ML systems are severely memory-bound, with matrix multiply operations achieving only 1-10% of theoretical peak FLOPS because processors spend most of their time waiting for data rather than computing. Moving 1GB from DRAM costs approximately 1000x more energy than a 32-bit multiply operation, making data movement the dominant factor in both performance and energy consumption. Amdahl's Law[^fn-amdahls-law] quantifies this limitation: if data movement takes 80% of execution time, even infinite compute capacity provides only 5x speedup. This memory wall drives all modern architectural innovations, from in-memory computing and near-data processing to specialized accelerators that co-locate compute and storage elements. These system-scale challenges represent fundamental engineering problems that this book explores systematically."
    },
    {
      "footnote_id": "fn-amdahls-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 907,
      "context": "[^fn-amdahls-law]: **Amdahl's Law**: Formulated by computer architect Gene Amdahl in 1967, this law quantifies the th...",
      "full_line": "[^fn-amdahls-law]: **Amdahl's Law**: Formulated by computer architect Gene Amdahl in 1967, this law quantifies the theoretical speedup of a program when only part of it can be parallelized. The speedup is limited by the sequential portion: if P is the fraction that can be parallelized, maximum speedup = 1/(1-P). For example, if 90% of a program can be parallelized, maximum speedup is 10x regardless of processor count. In ML systems, this explains why memory bandwidth and data movement often become the primary bottlenecks rather than compute capacity."
    },
    {
      "footnote_id": "fn-petabytes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 909,
      "context": "[^fn-petabytes]: **Petabytes**: One million gigabytes, or enough storage for 500 billion pages of text. To put this...",
      "full_line": "[^fn-petabytes]: **Petabytes**: One million gigabytes, or enough storage for 500 billion pages of text. To put this in perspective, the entire printed collection of the U.S. Library of Congress is about 20 terabytes, so a petabyte could store 50 copies. Modern AI training datasets like Common Crawl contain multiple petabytes of web text, representing essentially the entire public internet's text content."
    },
    {
      "footnote_id": "fn-computer-engineering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 915,
      "context": "...rred in the late 1960s and early 1970s. As computing systems grew more complex, Computer Engineering[^fn-computer-engineering] emerged as a new discipline to bridge the gap between Electrical Engineering's hardware expertise a...",
      "full_line": "The emergence of this systems-focused approach mirrors a similar transition that occurred in the late 1960s and early 1970s. As computing systems grew more complex, Computer Engineering[^fn-computer-engineering] emerged as a new discipline to bridge the gap between Electrical Engineering's hardware expertise and Computer Science's focus on algorithms and software. Computer Engineering arose because the challenges of designing and building complex computing systems required an integrated approach that neither discipline could fully address on its own."
    },
    {
      "footnote_id": "fn-computer-engineering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 917,
      "context": "[^fn-computer-engineering]: **Computer Engineering**: This discipline emerged in the late 1960s when IBM System/360 and other...",
      "full_line": "[^fn-computer-engineering]: **Computer Engineering**: This discipline emerged in the late 1960s when IBM System/360 and other complex computing systems required expertise that spanned both hardware and software. Before Computer Engineering, electrical engineers focused on circuits while computer scientists worked on algorithms, but no one specialized in the integration challenges. Today's Computer Engineering programs, established at schools like Case Western Reserve and Stanford in the 1970s, combine hardware design, software systems, and computer architecture\u2014laying the groundwork for what ML Systems Engineering is becoming today."
    },
    {
      "footnote_id": "fn-cuda",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1094,
      "context": "...ge when algorithms and hardware are designed together from the ground up. NVIDIA's CUDA-enabled GPUs[^fn-cuda] provided not just computational efficiency but also the programming model that made massive paralle...",
      "full_line": "The 2012 AlexNet breakthrough illustrates the fundamental principle of hardware-software co-design that defines modern ML systems engineering. This deep learning revolution succeeded because the algorithmic innovation (convolutional neural networks) perfectly matched the hardware capability (parallel GPU architectures). Convolutional operations are inherently parallel\u2014each filter can process different parts of an image simultaneously\u2014making them naturally suited to GPU's thousands of parallel cores. This wasn't just about using available hardware more efficiently; it demonstrated that the most successful AI systems emerge when algorithms and hardware are designed together from the ground up. NVIDIA's CUDA-enabled GPUs[^fn-cuda] provided not just computational efficiency but also the programming model that made massive parallelism accessible to machine learning researchers. This co-design approach continues to shape ML system development: transformer attention mechanisms stress memory bandwidth differently than CNNs, leading to new accelerator architectures; edge deployment constraints drive algorithmic innovations like quantization and pruning; and specialized chips like Google's TPUs co-evolve with TensorFlow to optimize specific operations like matrix multiplication."
    },
    {
      "footnote_id": "fn-cuda",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1096,
      "context": "[^fn-cuda]: **CUDA (Compute Unified Device Architecture)**: NVIDIA's parallel computing platform introduced in...",
      "full_line": "[^fn-cuda]: **CUDA (Compute Unified Device Architecture)**: NVIDIA's parallel computing platform introduced in 2007 that transformed gaming graphics cards into general-purpose computing powerhouses. CUDA enabled developers to harness GPU's thousands of cores for AI computations, providing 10-100x speedups over traditional CPUs for machine learning tasks. This breakthrough made deep learning practically feasible, as training a neural network that would take months on a CPU could be completed in days on CUDA-enabled hardware."
    },
    {
      "footnote_id": "fn-data-centers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1161,
      "context": "At one spectrum end, cloud-based ML systems run in massive data centers[^fn-data-centers]. These systems, including large language models and recommendation engines, process petabytes of da...",
      "full_line": "At one spectrum end, cloud-based ML systems run in massive data centers[^fn-data-centers]. These systems, including large language models and recommendation engines, process petabytes of data while serving millions of users simultaneously. They leverage virtually unlimited computing resources but manage enormous operational complexity and costs. The architectural approaches for building such large-scale systems are covered in @sec-ml-systems and @sec-ai-acceleration."
    },
    {
      "footnote_id": "fn-data-centers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1163,
      "context": "[^fn-data-centers]: **Data Centers**: Massive facilities housing thousands of servers, often consuming 100-300 megawat...",
      "full_line": "[^fn-data-centers]: **Data Centers**: Massive facilities housing thousands of servers, often consuming 100-300 megawatts of power\u2014equivalent to a small city. Google operates over 20 data centers globally, each one costing $1-2 billion to build. These facilities maintain temperatures of exactly 80\u00b0F (27\u00b0C) with backup power systems that can run for days, enabling the reliable operation of AI services used by billions of people worldwide."
    },
    {
      "footnote_id": "fn-microcontrollers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1165,
      "context": "At the other end, TinyML systems run on microcontrollers[^fn-microcontrollers] and embedded devices, performing ML tasks with severe memory, computing power, and energy consumpti...",
      "full_line": "At the other end, TinyML systems run on microcontrollers[^fn-microcontrollers] and embedded devices, performing ML tasks with severe memory, computing power, and energy consumption constraints. Smart home devices like Alexa or Google Assistant must recognize voice commands using less power than LED bulbs, while sensors must detect anomalies on battery power for months or years. The specialized techniques for deploying ML on such constrained devices are explored in @sec-efficient-ai and @sec-model-optimizations, while the unique challenges of embedded ML systems are covered in @sec-ondevice-learning."
    },
    {
      "footnote_id": "fn-microcontrollers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1167,
      "context": "[^fn-microcontrollers]: **Microcontrollers**: Tiny computers-on-a-chip costing under $1 each, with just kilobytes of memor...",
      "full_line": "[^fn-microcontrollers]: **Microcontrollers**: Tiny computers-on-a-chip costing under $1 each, with just kilobytes of memory\u2014about 1/millionth the memory of a smartphone. Popular chips like the Arduino Uno have only 32KB of storage and 2KB of RAM, yet can run simple AI models that classify sensor data, recognize voice commands, or detect movement patterns while consuming less power than a digital watch."
    },
    {
      "footnote_id": "fn-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1169,
      "context": "...d for different contexts. Edge ML systems bring computation closer to data sources, reducing latency[^fn-latency] and bandwidth requirements while managing local computing resources. Mobile ML systems must balance...",
      "full_line": "Between these extremes lies a rich variety of ML systems adapted for different contexts. Edge ML systems bring computation closer to data sources, reducing latency[^fn-latency] and bandwidth requirements while managing local computing resources. Mobile ML systems must balance sophisticated capabilities with severe constraints: modern smartphones typically have 4-12GB RAM, ARM processors operating at 1.5-3 GHz, and power budgets of 2-5 watts that must be shared across all system functions. For example, running a state-of-the-art image classification model on a smartphone might consume 100-500mW and complete inference in 10-100ms, compared to cloud servers that can use 200+ watts but deliver results in under 1ms. Enterprise ML systems often operate within specific business constraints, focusing on particular tasks while integrating with existing infrastructure. Some organizations employ hybrid approaches, distributing ML capabilities across multiple tiers to balance various requirements."
    },
    {
      "footnote_id": "fn-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1171,
      "context": "[^fn-latency]: **Latency**: The time delay between when a request is made and when a response is received. In ML...",
      "full_line": "[^fn-latency]: **Latency**: The time delay between when a request is made and when a response is received. In ML systems, this is critical\u2014autonomous vehicles need <10ms latency for safety decisions, while voice assistants target <100ms for natural conversation. For comparison, sending data to a distant cloud server typically adds 50-100ms, which is why edge computing became essential for real-time AI applications."
    },
    {
      "footnote_id": "fn-distributed-systems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1181,
      "context": "...anaged services, edge and hybrid systems must handle the complexity of distributed system management[^fn-distributed-systems]. This complexity manifests throughout the ML lifecycle, from data collection and version control to...",
      "full_line": "Operational complexity increases with system distribution. While centralized cloud architectures benefit from mature deployment tools and managed services, edge and hybrid systems must handle the complexity of distributed system management[^fn-distributed-systems]. This complexity manifests throughout the ML lifecycle, from data collection and version control to model deployment and monitoring. This operational complexity can compound over time if not carefully managed. The systematic approaches to operational excellence, including incident response and debugging methodologies for production ML systems, are thoroughly addressed in @sec-ml-operations."
    },
    {
      "footnote_id": "fn-distributed-systems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1183,
      "context": "[^fn-distributed-systems]: **Distributed Systems**: Computing systems where components located on networked computers communi...",
      "full_line": "[^fn-distributed-systems]: **Distributed Systems**: Computing systems where components located on networked computers communicate and coordinate their actions. In ML, this might mean training a single model across 1,000+ GPUs in different server racks, or deploying models across thousands of edge devices globally. The complexity comes from handling network failures, coordinating updates, ensuring data consistency, and managing the \"Two Generals' Problem\"\u2014confirming that all parts of the system agree on the current state."
    },
    {
      "footnote_id": "fn-mas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1199,
      "context": "...take actions, learn from outcomes, and adapt their behavior accordingly through multi-agent systems[^fn-mas] and advanced planning algorithms. These autonomous agents can plan, reason, and execute complex tas...",
      "full_line": "The rise of agentic systems marks a profound shift from traditional reactive ML systems that simply made predictions based on input data. Modern applications can now take actions, learn from outcomes, and adapt their behavior accordingly through multi-agent systems[^fn-mas] and advanced planning algorithms. These autonomous agents can plan, reason, and execute complex tasks, introducing new requirements for decision-making frameworks and safety constraints."
    },
    {
      "footnote_id": "fn-mas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1201,
      "context": "[^fn-mas]: **Multi-Agent System**: A computational system where multiple intelligent agents interact within a...",
      "full_line": "[^fn-mas]: **Multi-Agent System**: A computational system where multiple intelligent agents interact within an environment, each pursuing their own objectives while potentially cooperating or competing with other agents."
    },
    {
      "footnote_id": "fn-edge",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1211,
      "context": "...ators are emerging across the spectrum, from powerful data center chips to efficient edge processors[^fn-edge] to tiny neural processing units in mobile devices. This heterogeneous computing landscape enables d...",
      "full_line": "At the infrastructure level, new hardware is reshaping deployment possibilities. Specialized AI accelerators are emerging across the spectrum, from powerful data center chips to efficient edge processors[^fn-edge] to tiny neural processing units in mobile devices. This heterogeneous computing landscape enables dynamic model distribution across tiers based on computing capabilities and conditions, blurring traditional boundaries between cloud, edge, and embedded systems."
    },
    {
      "footnote_id": "fn-edge",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1213,
      "context": "[^fn-edge]: **Edge Processor**: A specialized computing device designed to perform AI computations close to wh...",
      "full_line": "[^fn-edge]: **Edge Processor**: A specialized computing device designed to perform AI computations close to where data is generated, optimized for low latency and energy efficiency rather than raw computing power."
    },
    {
      "footnote_id": "fn-intro-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1239,
      "context": "...f operating with limited computational resources. Machine learning methods such as transfer learning[^fn-intro-transfer-learning] allow models to learn on data-rich farms to be adapted for use in areas with limited historical dat...",
      "full_line": "FarmBeats uses a variety of ML algorithms tailored to agricultural applications. For soil moisture prediction, it uses temporal neural networks that can capture the complex dynamics of water movement in soil. Image analysis algorithms process drone imagery to detect crop stress, pest infestations, and yield estimates. These models must be robust to noisy data and capable of operating with limited computational resources. Machine learning methods such as transfer learning[^fn-intro-transfer-learning] allow models to learn on data-rich farms to be adapted for use in areas with limited historical data. The training techniques that enable such adaptation, including transfer learning and domain adaptation, are explored in @sec-ai-training."
    },
    {
      "footnote_id": "fn-intro-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1241,
      "context": "[^fn-intro-transfer-learning]: **Transfer Learning**: A machine learning technique where a model developed for one task is reused...",
      "full_line": "[^fn-intro-transfer-learning]: **Transfer Learning**: A machine learning technique where a model developed for one task is reused as the starting point for a model on a related task, significantly reducing the amount of training data and computation required\u2014particularly valuable in domains like agriculture where labeled data may be scarce."
    },
    {
      "footnote_id": "fn-intro-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1277,
      "context": "...res massive parallel computing resources, leveraging clusters of GPUs or specialized AI chips (TPUs)[^fn-intro-tpu] in a distributed computing environment. DeepMind utilized Google's cloud infrastructure, with the f...",
      "full_line": "The computational demands of AlphaFold epitomize the challenges of large-scale scientific ML systems. Training the model requires massive parallel computing resources, leveraging clusters of GPUs or specialized AI chips (TPUs)[^fn-intro-tpu] in a distributed computing environment. DeepMind utilized Google's cloud infrastructure, with the final version of AlphaFold trained on 128 TPUv3 cores for several weeks. Such large-scale training requires sophisticated distributed systems and specialized hardware that represent the cutting edge of AI engineering."
    },
    {
      "footnote_id": "fn-intro-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1279,
      "context": "[^fn-intro-tpu]: **Tensor Processing Unit (TPU)**: A specialized AI accelerator chip designed by Google specificall...",
      "full_line": "[^fn-intro-tpu]: **Tensor Processing Unit (TPU)**: A specialized AI accelerator chip designed by Google specifically for neural network machine learning, particularly efficient at matrix operations common in deep learning workloads."
    },
    {
      "footnote_id": "fn-rnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1299,
      "context": "...ipating the behavior of other road users, use neural networks that can understand patterns over time[^fn-rnn] in road user behavior. The architectural patterns for building such complex multi-model systems are...",
      "full_line": "Waymo's ML stack represents a sophisticated ensemble of algorithms tailored to the multifaceted challenge of autonomous driving. The perception system employs specialized neural networks to process visual data for object detection and tracking. Prediction models, needed for anticipating the behavior of other road users, use neural networks that can understand patterns over time[^fn-rnn] in road user behavior. The architectural patterns for building such complex multi-model systems are explored in @sec-dnn-architectures and @sec-ai-frameworks. Waymo has developed custom ML models like VectorNet for predicting vehicle trajectories. The planning and decision-making systems may incorporate learning-from-experience techniques to handle complex traffic scenarios."
    },
    {
      "footnote_id": "fn-rnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1301,
      "context": "[^fn-rnn]: **Sequential Neural Networks**: Neural network architectures designed to process data that occurs...",
      "full_line": "[^fn-rnn]: **Sequential Neural Networks**: Neural network architectures designed to process data that occurs in sequences over time, such as predicting where a pedestrian will move next based on their previous movements. These networks maintain a form of \"memory\" of previous inputs to inform current decisions."
    },
    {
      "footnote_id": "fn-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1326,
      "context": "...tterns over time that can silently degrade model performance. This phenomenon, known as \"data drift\"[^fn-drift], affects all three case studies differently. FarmBeats models must adapt to changing seasonal patte...",
      "full_line": "Perhaps the most insidious data challenge is drift\u2014the gradual change in data patterns over time that can silently degrade model performance. This phenomenon, known as \"data drift\"[^fn-drift], affects all three case studies differently. FarmBeats models must adapt to changing seasonal patterns, soil conditions, and climate variations. Waymo's perception models encounter new traffic patterns, road configurations, and weather conditions that weren't present in training data. Even AlphaFold, working with fundamental biological structures, faces challenges when applied to protein families that are evolutionarily distant from its training data."
    },
    {
      "footnote_id": "fn-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1330,
      "context": "[^fn-drift]: **Data Drift**: The gradual change in the statistical properties of the target variable (what the...",
      "full_line": "[^fn-drift]: **Data Drift**: The gradual change in the statistical properties of the target variable (what the model is trying to predict) over time, which can degrade model performance if not properly monitored and addressed."
    },
    {
      "footnote_id": "fn-backprop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1334,
      "context": "...3, which has hundreds of billions of parameters that need to be optimized through training processes[^fn-backprop]. This complexity creates practical challenges: these models require enormous computing power to tra...",
      "full_line": "Creating and maintaining the ML models themselves presents another set of challenges. Modern ML models, particularly in deep learning, can be extremely complex. Consider a language model like GPT-3, which has hundreds of billions of parameters that need to be optimized through training processes[^fn-backprop]. This complexity creates practical challenges: these models require enormous computing power to train and run, making it difficult to deploy them in situations with limited resources, like on mobile phones or IoT devices."
    },
    {
      "footnote_id": "fn-backprop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1336,
      "context": "[^fn-backprop]: **Backpropagation**: The primary algorithm used to train neural networks, which calculates how eac...",
      "full_line": "[^fn-backprop]: **Backpropagation**: The primary algorithm used to train neural networks, which calculates how each parameter in the network should be adjusted to minimize prediction errors by propagating error gradients backward through the network layers."
    },
    {
      "footnote_id": "fn-transfer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1338,
      "context": ".... Unlike traditional programming where we write explicit instructions, ML models learn from examples[^fn-transfer]. This learning process involves many choices: How should we structure the model? How long should we...",
      "full_line": "Training these models effectively is itself a significant challenge. Unlike traditional programming where we write explicit instructions, ML models learn from examples[^fn-transfer]. This learning process involves many choices: How should we structure the model? How long should we train it? How can we tell if it's learning the right things? Making these decisions often requires both technical expertise and considerable trial and error. The systematic approaches to these training challenges are explored in detail in @sec-ai-training, while the frameworks that support efficient model development are covered in @sec-ai-frameworks."
    },
    {
      "footnote_id": "fn-transfer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1340,
      "context": "[^fn-transfer]: **Transfer Learning**: A machine learning method where a model developed for one task is reused as...",
      "full_line": "[^fn-transfer]: **Transfer Learning**: A machine learning method where a model developed for one task is reused as the starting point for a model on a second task, significantly reducing the amount of training data and computation required."
    },
    {
      "footnote_id": "fn-black-box",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1356,
      "context": "...ion is transparency. Many modern ML models, particularly deep learning models, work as \"black boxes\"[^fn-black-box]. While they can make predictions, it's often difficult to understand how they arrived at their deci...",
      "full_line": "Another important consideration is transparency. Many modern ML models, particularly deep learning models, work as \"black boxes\"[^fn-black-box]. While they can make predictions, it's often difficult to understand how they arrived at their decisions."
    },
    {
      "footnote_id": "fn-black-box",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1358,
      "context": "[^fn-black-box]: **Black Box**: A system where you can observe the inputs and outputs but cannot see or understand...",
      "full_line": "[^fn-black-box]: **Black Box**: A system where you can observe the inputs and outputs but cannot see or understand the internal workings, like how a radio receives signals and produces sound without most users understanding the electronics inside. In AI, this opacity becomes problematic when the system makes important decisions affecting people's lives."
    },
    {
      "footnote_id": "fn-inference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1362,
      "context": "...re that models don't inadvertently memorize and reveal private information through inference attacks[^fn-inference]? These challenges aren't merely technical problems to be solved, but ongoing considerations that sh...",
      "full_line": "Privacy is also a major concern. ML systems often need large amounts of data to work effectively, but this data might contain sensitive personal information. How do we balance the need for data with the need to protect individual privacy? How do we ensure that models don't inadvertently memorize and reveal private information through inference attacks[^fn-inference]? These challenges aren't merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment. The approaches for addressing these critical concerns are covered in @sec-responsible-ai, @sec-security-privacy, and @sec-robust-ai."
    },
    {
      "footnote_id": "fn-inference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1364,
      "context": "[^fn-inference]: **Inference Attack**: A technique where an adversary attempts to extract sensitive information abo...",
      "full_line": "[^fn-inference]: **Inference Attack**: A technique where an adversary attempts to extract sensitive information about the training data by making careful queries to a trained model, exploiting patterns the model may have inadvertently memorized during training."
    },
    {
      "footnote_id": "fn-deployment-environments",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 45,
      "context": "...ng as an integrated system from @sec-introduction, this chapter examines how deployment environments[^fn-deployment-environments] shape ML system architecture. The three components manifest differently across the deployment spect...",
      "full_line": "Building on the framework of data, algorithms, and infrastructure working as an integrated system from @sec-introduction, this chapter examines how deployment environments[^fn-deployment-environments] shape ML system architecture. The three components manifest differently across the deployment spectrum: cloud systems prioritize algorithmic sophistication with abundant infrastructure, while embedded systems[^fn-embedded-systems] must optimize algorithms for minimal infrastructure, and edge systems balance both concerns while managing distributed data challenges."
    },
    {
      "footnote_id": "fn-embedded-systems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 45,
      "context": "...d systems prioritize algorithmic sophistication with abundant infrastructure, while embedded systems[^fn-embedded-systems] must optimize algorithms for minimal infrastructure, and edge systems balance both concerns while m...",
      "full_line": "Building on the framework of data, algorithms, and infrastructure working as an integrated system from @sec-introduction, this chapter examines how deployment environments[^fn-deployment-environments] shape ML system architecture. The three components manifest differently across the deployment spectrum: cloud systems prioritize algorithmic sophistication with abundant infrastructure, while embedded systems[^fn-embedded-systems] must optimize algorithms for minimal infrastructure, and edge systems balance both concerns while managing distributed data challenges."
    },
    {
      "footnote_id": "fn-data-centers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 49,
      "context": "...binations of these trade-offs. Cloud ML uses massive centralized computing resources in data centers[^fn-data-centers] when computational power outweighs latency concerns. Edge ML brings computation closer to data sour...",
      "full_line": "Modern machine learning systems span this deployment spectrum through four primary paradigms, each addressing specific combinations of these trade-offs. Cloud ML uses massive centralized computing resources in data centers[^fn-data-centers] when computational power outweighs latency concerns. Edge ML brings computation closer to data sources when low latency[^fn-latency] and privacy matter more than unlimited resources. Mobile ML extends capabilities to personal devices[^fn-mobile-power] when user proximity and offline operation become priorities. Tiny ML enables widespread intelligence on severely constrained devices when power efficiency and cost matter more than computational complexity."
    },
    {
      "footnote_id": "fn-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 49,
      "context": "...power outweighs latency concerns. Edge ML brings computation closer to data sources when low latency[^fn-latency] and privacy matter more than unlimited resources. Mobile ML extends capabilities to personal device...",
      "full_line": "Modern machine learning systems span this deployment spectrum through four primary paradigms, each addressing specific combinations of these trade-offs. Cloud ML uses massive centralized computing resources in data centers[^fn-data-centers] when computational power outweighs latency concerns. Edge ML brings computation closer to data sources when low latency[^fn-latency] and privacy matter more than unlimited resources. Mobile ML extends capabilities to personal devices[^fn-mobile-power] when user proximity and offline operation become priorities. Tiny ML enables widespread intelligence on severely constrained devices when power efficiency and cost matter more than computational complexity."
    },
    {
      "footnote_id": "fn-mobile-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 49,
      "context": "...and privacy matter more than unlimited resources. Mobile ML extends capabilities to personal devices[^fn-mobile-power] when user proximity and offline operation become priorities. Tiny ML enables widespread intelligenc...",
      "full_line": "Modern machine learning systems span this deployment spectrum through four primary paradigms, each addressing specific combinations of these trade-offs. Cloud ML uses massive centralized computing resources in data centers[^fn-data-centers] when computational power outweighs latency concerns. Edge ML brings computation closer to data sources when low latency[^fn-latency] and privacy matter more than unlimited resources. Mobile ML extends capabilities to personal devices[^fn-mobile-power] when user proximity and offline operation become priorities. Tiny ML enables widespread intelligence on severely constrained devices when power efficiency and cost matter more than computational complexity."
    },
    {
      "footnote_id": "fn-data-centers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 51,
      "context": "[^fn-data-centers]: **Data Centers**: Modern hyperscale data centers can house hundreds of thousands of servers and co...",
      "full_line": "[^fn-data-centers]: **Data Centers**: Modern hyperscale data centers can house hundreds of thousands of servers and consume 20-50 megawatts of power, equivalent to a small city. Google's data centers alone process over 189,000 searches per second globally as of 2025."
    },
    {
      "footnote_id": "fn-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 53,
      "context": "[^fn-latency]: **Latency vs Throughput**: Latency measures the time delay between input and output (critical for...",
      "full_line": "[^fn-latency]: **Latency vs Throughput**: Latency measures the time delay between input and output (critical for real-time applications), while throughput measures predictions processed per unit time (important for batch processing)."
    },
    {
      "footnote_id": "fn-deployment-environments",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 59,
      "context": "[^fn-deployment-environments]: **Deployment Environments**: The physical and logical contexts where ML systems operate, from hype...",
      "full_line": "[^fn-deployment-environments]: **Deployment Environments**: The physical and logical contexts where ML systems operate, from hyperscale data centers consuming megawatts to coin-cell powered sensors running for years. Each environment imposes distinct constraints that determine what models can run and how they must be optimized."
    },
    {
      "footnote_id": "fn-embedded-systems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 61,
      "context": "[^fn-embedded-systems]: **Embedded Systems**: Purpose-built computer systems integrated into larger devices, typically wit...",
      "full_line": "[^fn-embedded-systems]: **Embedded Systems**: Purpose-built computer systems integrated into larger devices, typically with real-time constraints and limited resources. Unlike general-purpose computers, embedded systems optimize for specific tasks. Automotive ECUs manage engine timing within microseconds, while smart thermostats operate for years on batteries."
    },
    {
      "footnote_id": "fn-mobile-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 63,
      "context": "[^fn-mobile-power]: **Mobile Power Constraints**: Modern smartphones contain 5000-6000mAh batteries (~18-22Wh) but ML...",
      "full_line": "[^fn-mobile-power]: **Mobile Power Constraints**: Modern smartphones contain 5000-6000mAh batteries (~18-22Wh) but ML inference can consume 1-5W, reducing battery life significantly. Apple's Neural Engine and Google's Tensor chips were specifically designed to perform AI tasks at <1W power consumption."
    },
    {
      "footnote_id": "fn-cost-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 240,
      "context": "...hese examples show the range of computational resources, power requirements, and cost considerations[^fn-cost-spectrum] across the ML systems spectrum. These concrete examples demonstrate the practical implications of e...",
      "full_line": "To understand the differences between these ML deployment options, @tbl-representative-systems provides examples of hardware platforms for each category. These examples show the range of computational resources, power requirements, and cost considerations[^fn-cost-spectrum] across the ML systems spectrum. These concrete examples demonstrate the practical implications of each approach."
    },
    {
      "footnote_id": "fn-cost-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 242,
      "context": "[^fn-cost-spectrum]: **ML Hardware Cost Spectrum**: The cost range spans 6 orders of magnitude, from $10 ESP32-CAM modu...",
      "full_line": "[^fn-cost-spectrum]: **ML Hardware Cost Spectrum**: The cost range spans 6 orders of magnitude, from $10 ESP32-CAM modules to $200K+ DGX A100 systems. This 20,000x cost difference reflects proportional differences in computational capability, enabling deployment across vastly different economic contexts and use cases."
    },
    {
      "footnote_id": "fn-memory-bottleneck",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 284,
      "context": "...waiting for memory transfers than performing calculations, particularly problematic for large models[^fn-memory-bottleneck] that require more data than can be efficiently transferred. This constraint drives the need for spe...",
      "full_line": "**Memory Wall Constraint**: The most critical bottleneck in modern computing stems from memory bandwidth scaling differently than computational capacity. While compute power can scale linearly by adding more processing units, memory bandwidth scales approximately as the square root of chip area due to physical routing constraints. This creates an increasingly severe bottleneck where processors become starved for data. In practice, this manifests as ML models spending more time waiting for memory transfers than performing calculations, particularly problematic for large models[^fn-memory-bottleneck] that require more data than can be efficiently transferred. This constraint drives the need for specialized architectures like TPUs with high-bandwidth memory and forces deployment decisions based on memory hierarchy optimization."
    },
    {
      "footnote_id": "fn-aws-sagemaker",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 301,
      "context": "...or applications where real time responsiveness is not critical. Popular platforms like AWS SageMaker[^fn-aws-sagemaker], Google Cloud AI, and Azure ML offer flexible, scalable solutions for model development, training,...",
      "full_line": "**Cloud ML**: Machine learning began predominantly in the cloud, where powerful, scalable data center servers train and run large ML models. Cloud ML uses extensive computational resources and storage capacities, enabling development of complex models trained on massive datasets. Cloud systems excel at tasks requiring extensive processing power and distributed training, making them ideal for applications where real time responsiveness is not critical. Popular platforms like AWS SageMaker[^fn-aws-sagemaker], Google Cloud AI, and Azure ML offer flexible, scalable solutions for model development, training, and deployment. Cloud ML handles models with billions of parameters[^fn-billion-parameters] trained on petabytes of data, though network delays introduce latencies of 100-500ms for online inference (16ms minimum for coast-to-coast light speed plus 50-100ms processing overhead), making sub-10ms real-time applications physically impossible with cloud processing[^fn-inference-latency]."
    },
    {
      "footnote_id": "fn-billion-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 301,
      "context": "...for model development, training, and deployment. Cloud ML handles models with billions of parameters[^fn-billion-parameters] trained on petabytes of data, though network delays introduce latencies of 100-500ms for online inf...",
      "full_line": "**Cloud ML**: Machine learning began predominantly in the cloud, where powerful, scalable data center servers train and run large ML models. Cloud ML uses extensive computational resources and storage capacities, enabling development of complex models trained on massive datasets. Cloud systems excel at tasks requiring extensive processing power and distributed training, making them ideal for applications where real time responsiveness is not critical. Popular platforms like AWS SageMaker[^fn-aws-sagemaker], Google Cloud AI, and Azure ML offer flexible, scalable solutions for model development, training, and deployment. Cloud ML handles models with billions of parameters[^fn-billion-parameters] trained on petabytes of data, though network delays introduce latencies of 100-500ms for online inference (16ms minimum for coast-to-coast light speed plus 50-100ms processing overhead), making sub-10ms real-time applications physically impossible with cloud processing[^fn-inference-latency]."
    },
    {
      "footnote_id": "fn-inference-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 301,
      "context": "...essing overhead), making sub-10ms real-time applications physically impossible with cloud processing[^fn-inference-latency].",
      "full_line": "**Cloud ML**: Machine learning began predominantly in the cloud, where powerful, scalable data center servers train and run large ML models. Cloud ML uses extensive computational resources and storage capacities, enabling development of complex models trained on massive datasets. Cloud systems excel at tasks requiring extensive processing power and distributed training, making them ideal for applications where real time responsiveness is not critical. Popular platforms like AWS SageMaker[^fn-aws-sagemaker], Google Cloud AI, and Azure ML offer flexible, scalable solutions for model development, training, and deployment. Cloud ML handles models with billions of parameters[^fn-billion-parameters] trained on petabytes of data, though network delays introduce latencies of 100-500ms for online inference (16ms minimum for coast-to-coast light speed plus 50-100ms processing overhead), making sub-10ms real-time applications physically impossible with cloud processing[^fn-inference-latency]."
    },
    {
      "footnote_id": "fn-billion-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 305,
      "context": "[^fn-billion-parameters]: **Billion-Parameter Models**: GPT-3 contains 175 billion model components requiring 350GB of memor...",
      "full_line": "[^fn-billion-parameters]: **Billion-Parameter Models**: GPT-3 contains 175 billion model components requiring 350GB of memory just for storage [@brown2020language]. GPT-4 is estimated at 1.8 trillion components. For comparison, the human brain has approximately 86 billion neurons with 100 trillion synaptic connections, suggesting AI models are approaching biological complexity."
    },
    {
      "footnote_id": "fn-memory-bottleneck",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 307,
      "context": "[^fn-memory-bottleneck]: **Memory Bottleneck**: When the rate of data transfer from memory to processor becomes the limitin...",
      "full_line": "[^fn-memory-bottleneck]: **Memory Bottleneck**: When the rate of data transfer from memory to processor becomes the limiting factor in computation. Large models require so many parameters that memory bandwidth, rather than computational capacity, determines performance."
    },
    {
      "footnote_id": "fn-aws-sagemaker",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 309,
      "context": "[^fn-aws-sagemaker]: **AWS SageMaker**: Amazon's ML platform launched in 2017, processing over 1 million model training...",
      "full_line": "[^fn-aws-sagemaker]: **AWS SageMaker**: Amazon's ML platform launched in 2017, processing over 1 million model training jobs annually by 2023. Provides end-to-end ML workflow management, from data preparation to model deployment, with integrated Jupyter notebooks and automatic model tuning capabilities."
    },
    {
      "footnote_id": "fn-inference-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 311,
      "context": "[^fn-inference-latency]: **Cloud Inference Latency**: Network latency includes propagation delay (speed of light limits), r...",
      "full_line": "[^fn-inference-latency]: **Cloud Inference Latency**: Network latency includes propagation delay (speed of light limits), routing delays, and processing time. Round-trip from California to Virginia takes minimum 80ms just for light travel. Adding internet routing, DNS lookup, and server processing typically results in 100-500ms total latency."
    },
    {
      "footnote_id": "fn-edge-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 313,
      "context": "...l gateways, smart cameras, autonomous vehicles, and IoT hubs. Edge ML reduces latency to under 50 ms[^fn-edge-latency], enhances privacy by keeping data local, and operates with intermittent cloud connectivity. Edge sy...",
      "full_line": "**Edge ML**: Growing demand for real time, low latency processing drove the emergence of Edge ML. Edge computing brings inference capabilities closer to data sources through deployment on industrial gateways, smart cameras, autonomous vehicles, and IoT hubs. Edge ML reduces latency to under 50 ms[^fn-edge-latency], enhances privacy by keeping data local, and operates with intermittent cloud connectivity. Edge systems excel at applications requiring quick responses or handling sensitive data in industrial and enterprise settings. Frameworks like NVIDIA Jetson[^fn-jetson-ecosystem] and Google's Edge TPU provide ML capabilities on edge devices, supporting IoT ecosystems[^fn-iot-ecosystems] by enabling real time decision making and reducing bandwidth usage through local data processing."
    },
    {
      "footnote_id": "fn-jetson-ecosystem",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 313,
      "context": "...nses or handling sensitive data in industrial and enterprise settings. Frameworks like NVIDIA Jetson[^fn-jetson-ecosystem] and Google's Edge TPU provide ML capabilities on edge devices, supporting IoT ecosystems[^fn-iot-ec...",
      "full_line": "**Edge ML**: Growing demand for real time, low latency processing drove the emergence of Edge ML. Edge computing brings inference capabilities closer to data sources through deployment on industrial gateways, smart cameras, autonomous vehicles, and IoT hubs. Edge ML reduces latency to under 50 ms[^fn-edge-latency], enhances privacy by keeping data local, and operates with intermittent cloud connectivity. Edge systems excel at applications requiring quick responses or handling sensitive data in industrial and enterprise settings. Frameworks like NVIDIA Jetson[^fn-jetson-ecosystem] and Google's Edge TPU provide ML capabilities on edge devices, supporting IoT ecosystems[^fn-iot-ecosystems] by enabling real time decision making and reducing bandwidth usage through local data processing."
    },
    {
      "footnote_id": "fn-iot-ecosystems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 313,
      "context": "...-ecosystem] and Google's Edge TPU provide ML capabilities on edge devices, supporting IoT ecosystems[^fn-iot-ecosystems] by enabling real time decision making and reducing bandwidth usage through local data processing.",
      "full_line": "**Edge ML**: Growing demand for real time, low latency processing drove the emergence of Edge ML. Edge computing brings inference capabilities closer to data sources through deployment on industrial gateways, smart cameras, autonomous vehicles, and IoT hubs. Edge ML reduces latency to under 50 ms[^fn-edge-latency], enhances privacy by keeping data local, and operates with intermittent cloud connectivity. Edge systems excel at applications requiring quick responses or handling sensitive data in industrial and enterprise settings. Frameworks like NVIDIA Jetson[^fn-jetson-ecosystem] and Google's Edge TPU provide ML capabilities on edge devices, supporting IoT ecosystems[^fn-iot-ecosystems] by enabling real time decision making and reducing bandwidth usage through local data processing."
    },
    {
      "footnote_id": "fn-jetson-ecosystem",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 315,
      "context": "[^fn-jetson-ecosystem]: **NVIDIA Jetson Ecosystem**: Family of embedded computing boards designed for AI at the edge, from...",
      "full_line": "[^fn-jetson-ecosystem]: **NVIDIA Jetson Ecosystem**: Family of embedded computing boards designed for AI at the edge, from the Jetson Nano Developer Kit ~$99-149 (5W) to the $1,999 AGX Orin (60W). Used in over 1 million deployed robots, drones, and autonomous vehicles worldwide since launch in 2014."
    },
    {
      "footnote_id": "fn-iot-ecosystems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 317,
      "context": "[^fn-iot-ecosystems]: **IoT Ecosystems**: Interconnected networks of smart devices, sensors, and gateways. A modern smar...",
      "full_line": "[^fn-iot-ecosystems]: **IoT Ecosystems**: Interconnected networks of smart devices, sensors, and gateways. A modern smart city might contain 1 million+ IoT devices per square kilometer, generating 2.5 quintillion bytes of data daily, making edge processing essential for real-time responses."
    },
    {
      "footnote_id": "fn-edge-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 319,
      "context": "[^fn-edge-latency]: **Edge Latency Advantage**: Edge processing eliminates network round-trips, achieving <10ms respon...",
      "full_line": "[^fn-edge-latency]: **Edge Latency Advantage**: Edge processing eliminates network round-trips, achieving <10ms response times for local inference. Industrial robots require <1ms control loops, autonomous vehicles need <10ms emergency responses. Both requirements are impossible with cloud processing but achievable with edge deployment."
    },
    {
      "footnote_id": "fn-mobile-storage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 321,
      "context": "...ally and operates offline, though it must balance model performance with device resource constraints[^fn-mobile-storage], typically 4 to 8 GB RAM and 100 to 200 GB storage.",
      "full_line": "**Mobile ML**: Building on edge computing concepts, Mobile ML uses the computational capabilities of smartphones and tablets. Mobile systems enable personalized, responsive applications while reducing reliance on constant network connectivity. Mobile ML balances the power of edge computing with the ubiquity of personal devices, utilizing onboard sensors such as cameras, GPS, and accelerometers for ML applications. Frameworks like TensorFlow Lite and Core ML enable developers to deploy optimized models on mobile devices, achieving inference times under 30 ms for common tasks. Mobile ML enhances privacy by keeping personal data locally and operates offline, though it must balance model performance with device resource constraints[^fn-mobile-storage], typically 4 to 8 GB RAM and 100 to 200 GB storage."
    },
    {
      "footnote_id": "fn-mobile-storage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 323,
      "context": "[^fn-mobile-storage]: **Mobile Storage Evolution**: iPhone storage grew from 4GB (2007) to 1TB (2023), a 250x increase i...",
      "full_line": "[^fn-mobile-storage]: **Mobile Storage Evolution**: iPhone storage grew from 4GB (2007) to 1TB (2023), a 250x increase in 16 years. However, ML models grew even faster: ResNet-50 (25MB, 2015) to modern language models (>1GB compressed), creating ongoing storage pressure despite hardware improvements."
    },
    {
      "footnote_id": "fn-memory-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 325,
      "context": "...are critical factors. Tiny ML devices typically operate with less than 1 MB of RAM and flash memory[^fn-memory-comparison], consuming only milliwatts of power to enable battery life of months or years[^fn-battery-life]. Ap...",
      "full_line": "**Tiny ML**: The latest development in this progression, Tiny ML enables ML models to run on extremely resource constrained microcontrollers and small embedded systems. Tiny ML performs local inference without relying on connectivity to cloud, edge, or mobile device processing power. Tiny systems excel in applications where size, power consumption, and cost are critical factors. Tiny ML devices typically operate with less than 1 MB of RAM and flash memory[^fn-memory-comparison], consuming only milliwatts of power to enable battery life of months or years[^fn-battery-life]. Applications include wake word detection, gesture recognition, and predictive maintenance in industrial settings. Platforms like Arduino Nano 33 BLE Sense and STM32 microcontrollers, with frameworks like TensorFlow Lite for Microcontrollers, enable ML on these tiny devices. Tiny ML requires significant model optimization and precision reduction techniques to fit within severe constraints."
    },
    {
      "footnote_id": "fn-battery-life",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 325,
      "context": "...^fn-memory-comparison], consuming only milliwatts of power to enable battery life of months or years[^fn-battery-life]. Applications include wake word detection, gesture recognition, and predictive maintenance in indus...",
      "full_line": "**Tiny ML**: The latest development in this progression, Tiny ML enables ML models to run on extremely resource constrained microcontrollers and small embedded systems. Tiny ML performs local inference without relying on connectivity to cloud, edge, or mobile device processing power. Tiny systems excel in applications where size, power consumption, and cost are critical factors. Tiny ML devices typically operate with less than 1 MB of RAM and flash memory[^fn-memory-comparison], consuming only milliwatts of power to enable battery life of months or years[^fn-battery-life]. Applications include wake word detection, gesture recognition, and predictive maintenance in industrial settings. Platforms like Arduino Nano 33 BLE Sense and STM32 microcontrollers, with frameworks like TensorFlow Lite for Microcontrollers, enable ML on these tiny devices. Tiny ML requires significant model optimization and precision reduction techniques to fit within severe constraints."
    },
    {
      "footnote_id": "fn-memory-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 327,
      "context": "[^fn-memory-comparison]: **Memory Scale Comparison**: TinyML devices operate with 256KB-2MB memory versus smartphones with...",
      "full_line": "[^fn-memory-comparison]: **Memory Scale Comparison**: TinyML devices operate with 256KB-2MB memory versus smartphones with 12-24GB (48,000-96,000x difference) and cloud servers with 1TB+ (4,000,000x difference). Yet TinyML can still perform useful inference through specialized optimization techniques[^fn-tinyml-optimization]."
    },
    {
      "footnote_id": "fn-tinyml-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 327,
      "context": "...fference). Yet TinyML can still perform useful inference through specialized optimization techniques[^fn-tinyml-optimization].",
      "full_line": "[^fn-memory-comparison]: **Memory Scale Comparison**: TinyML devices operate with 256KB-2MB memory versus smartphones with 12-24GB (48,000-96,000x difference) and cloud servers with 1TB+ (4,000,000x difference). Yet TinyML can still perform useful inference through specialized optimization techniques[^fn-tinyml-optimization]."
    },
    {
      "footnote_id": "fn-tinyml-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 329,
      "context": "[^fn-tinyml-optimization]: **TinyML Optimization**: Specialized techniques that dramatically reduce model size and computatio...",
      "full_line": "[^fn-tinyml-optimization]: **TinyML Optimization**: Specialized techniques that dramatically reduce model size and computational requirements while maintaining accuracy. These techniques enable deployment on severely resource-constrained devices (detailed in @sec-model-optimizations)."
    },
    {
      "footnote_id": "fn-battery-life",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 331,
      "context": "[^fn-battery-life]: **Ultra-Long Battery Life**: TinyML enables 10+ year deployments on single batteries through duty...",
      "full_line": "[^fn-battery-life]: **Ultra-Long Battery Life**: TinyML enables 10+ year deployments on single batteries through duty cycling. Devices sleep 99.9% of the time, wake periodically for inference, then return to sleep. Average power consumption drops to 10-100 microwatts, making decade-long operation feasible on coin-cell batteries."
    },
    {
      "footnote_id": "fn-cloud-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 429,
      "context": "Cloud Machine Learning leverages the scalability and power of centralized cloud infrastructures[^fn-cloud-evolution] to handle computationally intensive tasks such as large scale data processing, collaborative model...",
      "full_line": "Cloud Machine Learning leverages the scalability and power of centralized cloud infrastructures[^fn-cloud-evolution] to handle computationally intensive tasks such as large scale data processing, collaborative model development, and advanced analytics. Cloud data centers utilize distributed architectures and specialized resources to train complex models and support diverse applications, from recommendation systems to natural language processing[^fn-nlp-compute]. Here we focus on the deployment characteristics that make cloud ML systems effective for large-scale applications."
    },
    {
      "footnote_id": "fn-nlp-compute",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 429,
      "context": "...models and support diverse applications, from recommendation systems to natural language processing[^fn-nlp-compute]. Here we focus on the deployment characteristics that make cloud ML systems effective for large-sca...",
      "full_line": "Cloud Machine Learning leverages the scalability and power of centralized cloud infrastructures[^fn-cloud-evolution] to handle computationally intensive tasks such as large scale data processing, collaborative model development, and advanced analytics. Cloud data centers utilize distributed architectures and specialized resources to train complex models and support diverse applications, from recommendation systems to natural language processing[^fn-nlp-compute]. Here we focus on the deployment characteristics that make cloud ML systems effective for large-scale applications."
    },
    {
      "footnote_id": "fn-cloud-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 431,
      "context": "[^fn-cloud-evolution]: **Cloud Infrastructure Evolution**: Cloud computing for ML emerged from Amazon's decision in 2002...",
      "full_line": "[^fn-cloud-evolution]: **Cloud Infrastructure Evolution**: Cloud computing for ML emerged from Amazon's decision in 2002 to treat their internal infrastructure as a service. AWS launched in 2006, followed by Google Cloud (2008) and Azure (2010). By 2024, global cloud infrastructure spending exceeded $250 billion annually."
    },
    {
      "footnote_id": "fn-nlp-compute",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 433,
      "context": "[^fn-nlp-compute]: **NLP Computational Demands**: Modern language models like GPT-3 required 3,640 petaflop-days of c...",
      "full_line": "[^fn-nlp-compute]: **NLP Computational Demands**: Modern language models like GPT-3 required 3,640 petaflop-days of compute for training\u2014equivalent to running 1,000 NVIDIA V100 GPUs continuously for 355 days [@strubell2019energy]. This computational scale drove the need for massive cloud infrastructure."
    },
    {
      "footnote_id": "fn-mlsys-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 510,
      "context": "...edented scale. @fig-cloudml-example illustrates this concept with an example from Google's Cloud TPU[^fn-mlsys-tpu] data center. A single cloud deployment can provide 100+ TFLOPS of compute power compared to 1-10 TF...",
      "full_line": "Cloud ML's defining characteristic is its centralized infrastructure that operates at unprecedented scale. @fig-cloudml-example illustrates this concept with an example from Google's Cloud TPU[^fn-mlsys-tpu] data center. A single cloud deployment can provide 100+ TFLOPS of compute power compared to 1-10 TFLOPS available on mobile devices, representing a 10-100x computational advantage. Cloud service providers offer virtual platforms consisting of high-capacity servers (64-256 cores, 512GB-4TB RAM), expansive storage solutions (petabyte-scale distributed file systems), and robust networking architectures (10-100 Gbps interconnects) housed in globally distributed data centers[^fn-hyperscale]. These centralized facilities operate at kilowatt to megawatt power scales, enabling computational workloads impossible on resource-constrained devices. However, this centralization introduces fundamental trade-offs: network round-trip latency of 50-200ms eliminates real-time applications, while operational costs scale linearly with usage ($0.001-0.01 per inference request)."
    },
    {
      "footnote_id": "fn-hyperscale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 510,
      "context": "...ust networking architectures (10-100 Gbps interconnects) housed in globally distributed data centers[^fn-hyperscale]. These centralized facilities operate at kilowatt to megawatt power scales, enabling computational...",
      "full_line": "Cloud ML's defining characteristic is its centralized infrastructure that operates at unprecedented scale. @fig-cloudml-example illustrates this concept with an example from Google's Cloud TPU[^fn-mlsys-tpu] data center. A single cloud deployment can provide 100+ TFLOPS of compute power compared to 1-10 TFLOPS available on mobile devices, representing a 10-100x computational advantage. Cloud service providers offer virtual platforms consisting of high-capacity servers (64-256 cores, 512GB-4TB RAM), expansive storage solutions (petabyte-scale distributed file systems), and robust networking architectures (10-100 Gbps interconnects) housed in globally distributed data centers[^fn-hyperscale]. These centralized facilities operate at kilowatt to megawatt power scales, enabling computational workloads impossible on resource-constrained devices. However, this centralization introduces fundamental trade-offs: network round-trip latency of 50-200ms eliminates real-time applications, while operational costs scale linearly with usage ($0.001-0.01 per inference request)."
    },
    {
      "footnote_id": "fn-mlsys-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 512,
      "context": "[^fn-mlsys-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for tensor operations...",
      "full_line": "[^fn-mlsys-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for tensor operations, first used internally in 2015 for neural network inference. A single TPU v4 Pod contains 4,096 chips and delivers over 1 exaflop of compute power\u2014more than most supercomputers."
    },
    {
      "footnote_id": "fn-hyperscale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 514,
      "context": "[^fn-hyperscale]: **Hyperscale Data Centers**: These facilities contain 5,000+ servers and cover 10,000+ square feet...",
      "full_line": "[^fn-hyperscale]: **Hyperscale Data Centers**: These facilities contain 5,000+ servers and cover 10,000+ square feet. Microsoft's data centers span over 200 locations globally, with some individual facilities consuming enough electricity to power 80,000 homes."
    },
    {
      "footnote_id": "fn-ml-apis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 530,
      "context": "...ent and accessibility. Once trained and validated, machine learning models deploy through cloud APIs[^fn-ml-apis] and services, becoming accessible to users worldwide. Cloud deployment enables integration of ML ca...",
      "full_line": "Cloud ML also offers exceptional flexibility in deployment and accessibility. Once trained and validated, machine learning models deploy through cloud APIs[^fn-ml-apis] and services, becoming accessible to users worldwide. Cloud deployment enables integration of ML capabilities into applications across mobile, web, and IoT platforms, regardless of end user computational resources."
    },
    {
      "footnote_id": "fn-ml-apis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 532,
      "context": "[^fn-ml-apis]: **ML APIs**: Application Programming Interfaces that democratized AI by providing pre-trained mode...",
      "full_line": "[^fn-ml-apis]: **ML APIs**: Application Programming Interfaces that democratized AI by providing pre-trained models as web services. Google's Vision API launched in 2016, processing over 1 billion images monthly within two years\u2014enabling developers to add AI capabilities without ML expertise."
    },
    {
      "footnote_id": "fn-paas-pricing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 536,
      "context": "Through pay-as-you-go pricing models[^fn-paas-pricing] offered by cloud service providers, Cloud ML allows organizations to avoid the upfront capital expe...",
      "full_line": "Through pay-as-you-go pricing models[^fn-paas-pricing] offered by cloud service providers, Cloud ML allows organizations to avoid the upfront capital expenditure associated with building and maintaining dedicated ML infrastructure. The ability to scale resources up during intensive training periods and down during lower demand ensures cost effectiveness and financial flexibility in managing machine learning projects."
    },
    {
      "footnote_id": "fn-automl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 538,
      "context": "[^fn-automl]: **AutoML (Automated Machine Learning)**: Automated systems that democratize ML by handling model s...",
      "full_line": "[^fn-automl]: **AutoML (Automated Machine Learning)**: Automated systems that democratize ML by handling model selection, hyperparameter tuning, and feature engineering. Google AutoML Vision achieved 93.9% accuracy on ImageNet with minimal human intervention, compared to months of expert work for similar results."
    },
    {
      "footnote_id": "fn-paas-pricing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 540,
      "context": "[^fn-paas-pricing]: **Pay-as-You-Go Pricing**: Revolutionary model where users pay only for actual compute time used,...",
      "full_line": "[^fn-paas-pricing]: **Pay-as-You-Go Pricing**: Revolutionary model where users pay only for actual compute time used, measured in GPU-hours or inference requests. Training a model might cost $50-500 on demand versus $50,000-500,000 to purchase equivalent hardware."
    },
    {
      "footnote_id": "fn-automl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 552,
      "context": "...ools and algorithms designed for machine learning. These tools often include prebuilt models, AutoML[^fn-automl] capabilities, and specialized APIs that simplify the development and deployment of machine learning...",
      "full_line": "Cloud ML platforms provide access to a wide range of advanced tools and algorithms designed for machine learning. These tools often include prebuilt models, AutoML[^fn-automl] capabilities, and specialized APIs that simplify the development and deployment of machine learning solutions. Developers can use these resources to accelerate the building, training, and optimization of models. By utilizing advancements in machine learning algorithms and techniques, organizations can implement solutions without needing to develop them from scratch."
    },
    {
      "footnote_id": "fn-gdpr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 566,
      "context": "...lly, handling sensitive data in cloud environments complicates compliance with regulations like GDPR[^fn-gdpr] or HIPAA[^fn-hipaa].",
      "full_line": "Data privacy and security represent critical challenges when centralizing processing and storage in the cloud. Sensitive data transmitted to remote data centers becomes potentially vulnerable to cyber-attacks and unauthorized access. Cloud environments often attract hackers seeking to exploit vulnerabilities in valuable information repositories. Organizations must implement robust security measures including encryption, strict access controls, and continuous monitoring. Additionally, handling sensitive data in cloud environments complicates compliance with regulations like GDPR[^fn-gdpr] or HIPAA[^fn-hipaa]."
    },
    {
      "footnote_id": "fn-hipaa",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 566,
      "context": "...tive data in cloud environments complicates compliance with regulations like GDPR[^fn-gdpr] or HIPAA[^fn-hipaa].",
      "full_line": "Data privacy and security represent critical challenges when centralizing processing and storage in the cloud. Sensitive data transmitted to remote data centers becomes potentially vulnerable to cyber-attacks and unauthorized access. Cloud environments often attract hackers seeking to exploit vulnerabilities in valuable information repositories. Organizations must implement robust security measures including encryption, strict access controls, and continuous monitoring. Additionally, handling sensitive data in cloud environments complicates compliance with regulations like GDPR[^fn-gdpr] or HIPAA[^fn-hipaa]."
    },
    {
      "footnote_id": "fn-gdpr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 582,
      "context": "[^fn-gdpr]: **GDPR (General Data Protection Regulation)**: European privacy law effective 2018, imposing fines...",
      "full_line": "[^fn-gdpr]: **GDPR (General Data Protection Regulation)**: European privacy law effective 2018, imposing fines up to \u20ac20 million or 4% of global revenue for violations. Forces ML systems to implement \"right to be forgotten\" and data processing transparency\u2014technically challenging for neural networks."
    },
    {
      "footnote_id": "fn-hipaa",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 584,
      "context": "[^fn-hipaa]: **HIPAA (Health Insurance Portability and Accountability Act)**: US healthcare privacy law requiri...",
      "full_line": "[^fn-hipaa]: **HIPAA (Health Insurance Portability and Accountability Act)**: US healthcare privacy law requiring strict data security measures. ML systems handling medical data must implement encryption, access controls, and audit trails\u2014adding 30-50% to development costs but enabling $150B+ healthcare AI market."
    },
    {
      "footnote_id": "fn-collaborative-filtering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 586,
      "context": "[^fn-collaborative-filtering]: **Collaborative Filtering**: Recommendation technique analyzing user behavior patterns to predict...",
      "full_line": "[^fn-collaborative-filtering]: **Collaborative Filtering**: Recommendation technique analyzing user behavior patterns to predict preferences. Netflix's algorithm processes 100+ billion data points daily, with collaborative filtering contributing to 80% of watched content and saving $1 billion annually in customer retention."
    },
    {
      "footnote_id": "fn-collaborative-filtering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 588,
      "context": "...massive datasets to uncover patterns, preferences, and user behavior. Using collaborative filtering[^fn-collaborative-filtering] and other machine learning techniques, recommendation systems can offer personalized content or pro...",
      "full_line": "Cloud ML forms the backbone of advanced recommendation systems used by platforms like Netflix and Amazon. These systems use the cloud's ability to process and analyze massive datasets to uncover patterns, preferences, and user behavior. Using collaborative filtering[^fn-collaborative-filtering] and other machine learning techniques, recommendation systems can offer personalized content or product suggestions tailored to each user's interests. The cloud's scalability allows these systems to continuously update and refine their recommendations based on the ever-growing amount of user data, enhancing user engagement and satisfaction."
    },
    {
      "footnote_id": "fn-industrial-iot",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 600,
      "context": "...edge, becoming essential for time-sensitive applications such as autonomous systems, industrial IoT[^fn-industrial-iot], and smart infrastructure. This paradigm excels when applications cannot tolerate cloud round-trip...",
      "full_line": "Edge ML shifts computation away from centralized servers to process data at the network's edge, becoming essential for time-sensitive applications such as autonomous systems, industrial IoT[^fn-industrial-iot], and smart infrastructure. This paradigm excels when applications cannot tolerate cloud round-trip delays or when data privacy regulations prevent cloud processing. Edge devices, such as gateways and IoT hubs[^fn-iot-hubs], enable these systems to function efficiently while maintaining acceptable performance with intermediate resource constraints."
    },
    {
      "footnote_id": "fn-iot-hubs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 600,
      "context": "...when data privacy regulations prevent cloud processing. Edge devices, such as gateways and IoT hubs[^fn-iot-hubs], enable these systems to function efficiently while maintaining acceptable performance with interme...",
      "full_line": "Edge ML shifts computation away from centralized servers to process data at the network's edge, becoming essential for time-sensitive applications such as autonomous systems, industrial IoT[^fn-industrial-iot], and smart infrastructure. This paradigm excels when applications cannot tolerate cloud round-trip delays or when data privacy regulations prevent cloud processing. Edge devices, such as gateways and IoT hubs[^fn-iot-hubs], enable these systems to function efficiently while maintaining acceptable performance with intermediate resource constraints."
    },
    {
      "footnote_id": "fn-industrial-iot",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 602,
      "context": "[^fn-industrial-iot]: **Industrial IoT**: Manufacturing generates over 1 exabyte of data annually, but less than 1% is a...",
      "full_line": "[^fn-industrial-iot]: **Industrial IoT**: Manufacturing generates over 1 exabyte of data annually, but less than 1% is analyzed due to connectivity constraints. Edge ML enables real-time analysis, with predictive maintenance alone saving manufacturers $630 billion globally by 2025."
    },
    {
      "footnote_id": "fn-iot-hubs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 604,
      "context": "[^fn-iot-hubs]: **IoT Hubs**: Central connection points that aggregate data from multiple sensors before cloud tra...",
      "full_line": "[^fn-iot-hubs]: **IoT Hubs**: Central connection points that aggregate data from multiple sensors before cloud transmission. A typical smart building might have 1 hub managing 100-1000 IoT sensors, reducing cloud traffic by 90% while enabling local decision-making."
    },
    {
      "footnote_id": "fn-iot-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 679,
      "context": "...ding data to remote servers, devices like smartphones, tablets, and Internet of Things (IoT) devices[^fn-iot-growth] process data locally. The figure shows various examples of these edge devices, including wearables,...",
      "full_line": "Edge ML processes data in a decentralized fashion, as illustrated in @fig-edgeml-example. Instead of sending data to remote servers, devices like smartphones, tablets, and Internet of Things (IoT) devices[^fn-iot-growth] process data locally. The figure shows various examples of these edge devices, including wearables, industrial sensors, and smart home appliances. This local processing allows devices to make quick decisions based on collected data without depending on central server resources."
    },
    {
      "footnote_id": "fn-iot-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 681,
      "context": "[^fn-iot-growth]: **IoT Device Growth**: From 8.4 billion connected devices in 2017 to a projected 25.4 billion by 2...",
      "full_line": "[^fn-iot-growth]: **IoT Device Growth**: From 8.4 billion connected devices in 2017 to a projected 25.4 billion by 2030. Each device generates 2.5 quintillion bytes of data daily, making edge processing essential for bandwidth management."
    },
    {
      "footnote_id": "fn-latency-critical",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 689,
      "context": "...e, creating 10-100x energy penalty that eliminates cloud processing for battery-powered applications[^fn-latency-critical].",
      "full_line": "Edge ML's quantifiable advantages stem from eliminating network dependencies and optimizing for local resource constraints. Latency reduction from 50-200ms (cloud) to 1-50ms (edge) represents a 4-40x improvement that enables entirely new application categories. This improvement becomes essential in safety-critical scenarios: autonomous vehicles requiring <10ms emergency braking decisions, industrial robotics needing <1ms precision control, and augmented reality demanding <20ms motion-to-photon latency for comfort. Energy efficiency improves through localized computation\u2014a smartphone processing 1000 images locally at 0.1J per inference consumes 100J total, while uploading to cloud requires 1-10J per image for wireless transmission alone, creating 10-100x energy penalty that eliminates cloud processing for battery-powered applications[^fn-latency-critical]."
    },
    {
      "footnote_id": "fn-latency-critical",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 691,
      "context": "[^fn-latency-critical]: **Latency-Critical Applications**: Autonomous vehicles require <10ms response times for emergency...",
      "full_line": "[^fn-latency-critical]: **Latency-Critical Applications**: Autonomous vehicles require <10ms response times for emergency braking decisions. Industrial robotics needs <1ms for precision control. Cloud round-trip latency typically ranges from 50-200ms, making edge processing essential for safety-critical applications."
    },
    {
      "footnote_id": "fn-endpoint-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 699,
      "context": "...imary concern is limited computational resources compared to cloud based solutions. Endpoint devices[^fn-endpoint-constraints] typically have significantly less processing power and storage capacity than cloud servers, limitin...",
      "full_line": "Edge ML faces several challenges. The primary concern is limited computational resources compared to cloud based solutions. Endpoint devices[^fn-endpoint-constraints] typically have significantly less processing power and storage capacity than cloud servers, limiting the complexity of deployable machine learning models."
    },
    {
      "footnote_id": "fn-edge-coordination",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 701,
      "context": "[^fn-edge-coordination]: **Edge Network Coordination**: For n edge devices, the number of potential communication paths is...",
      "full_line": "[^fn-edge-coordination]: **Edge Network Coordination**: For n edge devices, the number of potential communication paths is n(n-1)/2. A network of 1,000 devices has 499,500 possible connections to manage. Software-defined networking (SDN) and edge orchestration platforms like Kubernetes K3s help manage this complexity."
    },
    {
      "footnote_id": "fn-endpoint-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 703,
      "context": "[^fn-endpoint-constraints]: **Endpoint Device Constraints**: Typical edge devices have 1-8GB RAM and 2-32GB storage, versus cl...",
      "full_line": "[^fn-endpoint-constraints]: **Endpoint Device Constraints**: Typical edge devices have 1-8GB RAM and 2-32GB storage, versus cloud servers with 128-1024GB RAM and petabytes of storage. Processing power differs by 10-100x, necessitating specialized model compression techniques."
    },
    {
      "footnote_id": "fn-edge-coordination",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 705,
      "context": "...xponentially\u2014coordinating 1,000 edge devices requires managing 499,500 potential communication paths[^fn-edge-coordination].",
      "full_line": "Managing a network of edge nodes introduces complexity, particularly regarding coordination, updates, and maintenance. Ensuring all nodes operate efficiently and remain current with the latest algorithms and security protocols presents logistical challenges. This distributed management problem scales exponentially\u2014coordinating 1,000 edge devices requires managing 499,500 potential communication paths[^fn-edge-coordination]."
    },
    {
      "footnote_id": "fn-industry-40",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 717,
      "context": "The Industrial IoT[^fn-industry-40] uses Edge ML to monitor and control complex industrial processes. Here, machine learning models can...",
      "full_line": "The Industrial IoT[^fn-industry-40] uses Edge ML to monitor and control complex industrial processes. Here, machine learning models can analyze data from numerous sensors in real-time, enabling predictive maintenance[^fn-predictive-maintenance], optimizing operations, and enhancing safety measures. This revolution in industrial automation and efficiency is transforming manufacturing and production across various sectors."
    },
    {
      "footnote_id": "fn-predictive-maintenance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 717,
      "context": "...learning models can analyze data from numerous sensors in real-time, enabling predictive maintenance[^fn-predictive-maintenance], optimizing operations, and enhancing safety measures. This revolution in industrial automation and...",
      "full_line": "The Industrial IoT[^fn-industry-40] uses Edge ML to monitor and control complex industrial processes. Here, machine learning models can analyze data from numerous sensors in real-time, enabling predictive maintenance[^fn-predictive-maintenance], optimizing operations, and enhancing safety measures. This revolution in industrial automation and efficiency is transforming manufacturing and production across various sectors."
    },
    {
      "footnote_id": "fn-voice-recognition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 725,
      "context": "...er more than computational sophistication. Mobile ML supports applications such as voice recognition[^fn-voice-recognition], computational photography[^fn-computational-photography], and health monitoring while maintaining...",
      "full_line": "Mobile ML integrates machine learning directly into portable devices like smartphones and tablets, providing users with real-time, personalized capabilities. This paradigm excels when user privacy, offline operation, and immediate responsiveness matter more than computational sophistication. Mobile ML supports applications such as voice recognition[^fn-voice-recognition], computational photography[^fn-computational-photography], and health monitoring while maintaining data privacy through on-device computation. These battery-powered devices must balance performance with power efficiency and thermal management, making them ideal for frequent, short-duration AI tasks."
    },
    {
      "footnote_id": "fn-computational-photography",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 725,
      "context": "...ML supports applications such as voice recognition[^fn-voice-recognition], computational photography[^fn-computational-photography], and health monitoring while maintaining data privacy through on-device computation. These battery-...",
      "full_line": "Mobile ML integrates machine learning directly into portable devices like smartphones and tablets, providing users with real-time, personalized capabilities. This paradigm excels when user privacy, offline operation, and immediate responsiveness matter more than computational sophistication. Mobile ML supports applications such as voice recognition[^fn-voice-recognition], computational photography[^fn-computational-photography], and health monitoring while maintaining data privacy through on-device computation. These battery-powered devices must balance performance with power efficiency and thermal management, making them ideal for frequent, short-duration AI tasks."
    },
    {
      "footnote_id": "fn-voice-recognition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 727,
      "context": "[^fn-voice-recognition]: **Voice Recognition Evolution**: Apple's Siri (2011) required cloud processing with 200-500ms late...",
      "full_line": "[^fn-voice-recognition]: **Voice Recognition Evolution**: Apple's Siri (2011) required cloud processing with 200-500ms latency. By 2017, on-device processing reduced latency to <50ms while improving privacy. Modern smartphones process 16kHz audio at 20-30ms latency using specialized neural engines."
    },
    {
      "footnote_id": "fn-industry-40",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 729,
      "context": "[^fn-industry-40]: **Industry 4.0**: Fourth industrial revolution integrating cyber-physical systems, IoT, and cloud...",
      "full_line": "[^fn-industry-40]: **Industry 4.0**: Fourth industrial revolution integrating cyber-physical systems, IoT, and cloud computing into manufacturing. Expected to increase productivity by 20-30% and reduce costs by 15-25% globally, with Germany leading adoption (83% of manufacturers) followed by US (54%)."
    },
    {
      "footnote_id": "fn-predictive-maintenance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 731,
      "context": "[^fn-predictive-maintenance]: **Predictive Maintenance**: ML-driven maintenance scheduling based on equipment condition rather t...",
      "full_line": "[^fn-predictive-maintenance]: **Predictive Maintenance**: ML-driven maintenance scheduling based on equipment condition rather than fixed intervals. Reduces unplanned downtime by 35-45% and maintenance costs by 20-25%. GE saves $1.5 billion annually using predictive analytics across its industrial equipment."
    },
    {
      "footnote_id": "fn-computational-photography",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 733,
      "context": "[^fn-computational-photography]: **Computational Photography**: Combines multiple exposures and ML algorithms to enhance image qual...",
      "full_line": "[^fn-computational-photography]: **Computational Photography**: Combines multiple exposures and ML algorithms to enhance image quality. Google's Night Sight captures 15 frames in 6 seconds, using ML to align and merge them. Portrait mode uses depth estimation ML models to create professional-looking bokeh effects in real-time."
    },
    {
      "footnote_id": "fn-mobile-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 813,
      "context": "...eving 10-100x better energy efficiency than general-purpose processors. System on Chip architectures[^fn-mobile-soc] integrate computation and memory to minimize data movement costs\u2014the primary energy bottleneck in m...",
      "full_line": "Mobile ML operates within severe resource constraints that drive specific architectural optimizations. Modern flagship devices provide 1-10 TOPS of AI compute through specialized Neural Processing Units while consuming <1W power, achieving 10-100x better energy efficiency than general-purpose processors. System on Chip architectures[^fn-mobile-soc] integrate computation and memory to minimize data movement costs\u2014the primary energy bottleneck in mobile systems. Typical mobile memory bandwidth of 25-50 GB/s limits deployable models to 10-100MB parameter sets, requiring aggressive quantization from FP32 (4 bytes/parameter) to INT8 (1 byte/parameter) for practical deployment. Battery constraints impose fundamental limits: at 5000mAh capacity (18Wh), continuous 1W ML processing reduces device lifetime from 24 hours to 18 hours, making energy optimization essential for user experience[^fn-npu]."
    },
    {
      "footnote_id": "fn-npu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 813,
      "context": "...device lifetime from 24 hours to 18 hours, making energy optimization essential for user experience[^fn-npu].",
      "full_line": "Mobile ML operates within severe resource constraints that drive specific architectural optimizations. Modern flagship devices provide 1-10 TOPS of AI compute through specialized Neural Processing Units while consuming <1W power, achieving 10-100x better energy efficiency than general-purpose processors. System on Chip architectures[^fn-mobile-soc] integrate computation and memory to minimize data movement costs\u2014the primary energy bottleneck in mobile systems. Typical mobile memory bandwidth of 25-50 GB/s limits deployable models to 10-100MB parameter sets, requiring aggressive quantization from FP32 (4 bytes/parameter) to INT8 (1 byte/parameter) for practical deployment. Battery constraints impose fundamental limits: at 5000mAh capacity (18Wh), continuous 1W ML processing reduces device lifetime from 24 hours to 18 hours, making energy optimization essential for user experience[^fn-npu]."
    },
    {
      "footnote_id": "fn-mobile-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 815,
      "context": "[^fn-mobile-soc]: **Mobile System-on-Chip**: Modern flagship SoCs integrate CPU, GPU, NPU, and memory controllers on...",
      "full_line": "[^fn-mobile-soc]: **Mobile System-on-Chip**: Modern flagship SoCs integrate CPU, GPU, NPU, and memory controllers on a single chip. Apple's A17 Pro contains 19 billion transistors in a 3nm process, while Snapdragon 8 Gen 3 delivers significant AI performance improvements over its predecessor."
    },
    {
      "footnote_id": "fn-npu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 817,
      "context": "[^fn-npu]: **Neural Processing Unit (NPU)**: Specialized processors optimized for neural network operations....",
      "full_line": "[^fn-npu]: **Neural Processing Unit (NPU)**: Specialized processors optimized for neural network operations. Apple's Neural Engine (introduced in A11, 2017) performs 600 billion operations per second. Qualcomm's Hexagon NPU in flagship chips delivers up to 75 TOPS while consuming <1W."
    },
    {
      "footnote_id": "fn-tflite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 819,
      "context": "...upported by specialized frameworks and tools designed for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mob...",
      "full_line": "Mobile ML is supported by specialized frameworks and tools designed for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mobile hardware and provide efficient optimization techniques[^fn-mlsys-quantization] to ensure smooth performance within mobile resource constraints."
    },
    {
      "footnote_id": "fn-coreml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 819,
      "context": "...designed for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mobile hardware and provide efficient optimizat...",
      "full_line": "Mobile ML is supported by specialized frameworks and tools designed for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mobile hardware and provide efficient optimization techniques[^fn-mlsys-quantization] to ensure smooth performance within mobile resource constraints."
    },
    {
      "footnote_id": "fn-mlsys-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 819,
      "context": "...es. These frameworks are optimized for mobile hardware and provide efficient optimization techniques[^fn-mlsys-quantization] to ensure smooth performance within mobile resource constraints.",
      "full_line": "Mobile ML is supported by specialized frameworks and tools designed for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mobile hardware and provide efficient optimization techniques[^fn-mlsys-quantization] to ensure smooth performance within mobile resource constraints."
    },
    {
      "footnote_id": "fn-tflite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 821,
      "context": "[^fn-tflite]: **TensorFlow Lite**: Google's mobile ML framework launched in 2017, designed to run models <100MB...",
      "full_line": "[^fn-tflite]: **TensorFlow Lite**: Google's mobile ML framework launched in 2017, designed to run models <100MB with <100ms inference time. Supports quantization to reduce model size by 75% while maintaining 95% accuracy. Used in over 4 billion devices worldwide."
    },
    {
      "footnote_id": "fn-coreml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 823,
      "context": "[^fn-coreml]: **Core ML**: Apple's framework introduced in iOS 11 (2017), optimized for on-device inference. Sup...",
      "full_line": "[^fn-coreml]: **Core ML**: Apple's framework introduced in iOS 11 (2017), optimized for on-device inference. Supports models from 1KB to 1GB, with automatic optimization for Apple Silicon. Enables features like Live Text, which processes text in real-time using on-device OCR models."
    },
    {
      "footnote_id": "fn-mlsys-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 825,
      "context": "[^fn-mlsys-quantization]: **Model Optimization**: Techniques that reduce model size and computational requirements while mai...",
      "full_line": "[^fn-mlsys-quantization]: **Model Optimization**: Techniques that reduce model size and computational requirements while maintaining accuracy. These optimizations enable deployment on resource-constrained devices (detailed in @sec-model-optimizations)."
    },
    {
      "footnote_id": "fn-real-time-translation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 829,
      "context": "...n faster response times for applications requiring immediate feedback, such as real time translation[^fn-real-time-translation], face detection[^fn-face-detection], or gesture recognition.",
      "full_line": "Mobile ML enables real time processing of data directly on mobile devices, eliminating the need for constant server communication. This results in faster response times for applications requiring immediate feedback, such as real time translation[^fn-real-time-translation], face detection[^fn-face-detection], or gesture recognition."
    },
    {
      "footnote_id": "fn-face-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 829,
      "context": "...quiring immediate feedback, such as real time translation[^fn-real-time-translation], face detection[^fn-face-detection], or gesture recognition.",
      "full_line": "Mobile ML enables real time processing of data directly on mobile devices, eliminating the need for constant server communication. This results in faster response times for applications requiring immediate feedback, such as real time translation[^fn-real-time-translation], face detection[^fn-face-detection], or gesture recognition."
    },
    {
      "footnote_id": "fn-mobile-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 837,
      "context": "...ization: mobile apps typically budget 10-100MB for ML models versus multi-gigabyte cloud deployments[^fn-mobile-constraints]. These constraints cascade through the entire development process, requiring specialized training t...",
      "full_line": "Mobile devices face quantified resource constraints that directly limit deployable model complexity and create specific optimization requirements. Memory constraints are particularly severe: flagship phones with 12-24GB RAM allocate only 100MB-1GB to ML applications, versus unlimited cloud allocation. This forces model architectures under 100-500MB total size including weights and activations. Processing power limitations compound memory constraints\u2014mobile NPUs delivering 1-10 TOPS compare unfavorably to cloud GPUs providing 100-1000 TOPS, requiring 10-100x model compression to achieve comparable inference speeds. Storage limitations demand aggressive model optimization: mobile apps typically budget 10-100MB for ML models versus multi-gigabyte cloud deployments[^fn-mobile-constraints]. These constraints cascade through the entire development process, requiring specialized training techniques, quantization-aware optimization, and architectural modifications that preserve accuracy while meeting device limitations."
    },
    {
      "footnote_id": "fn-real-time-translation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 839,
      "context": "[^fn-real-time-translation]: **Real-Time Translation**: Google Translate app can translate conversations in 40+ languages offli...",
      "full_line": "[^fn-real-time-translation]: **Real-Time Translation**: Google Translate app can translate conversations in 40+ languages offline using on-device neural networks. The offline models are 35-45MB each versus 2GB+ for cloud versions, achieving 90% of cloud accuracy while enabling instant translation without internet."
    },
    {
      "footnote_id": "fn-face-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 841,
      "context": "[^fn-face-detection]: **Mobile Face Detection**: Apple's Face ID uses a 30,000-dot projector and neural networks to crea...",
      "full_line": "[^fn-face-detection]: **Mobile Face Detection**: Apple's Face ID uses a 30,000-dot projector and neural networks to create 3D face maps in <2 seconds. The system processes biometric data entirely on-device using the Secure Enclave, making it practically impossible to extract face data even with physical device access."
    },
    {
      "footnote_id": "fn-portrait-mode",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 843,
      "context": "[^fn-portrait-mode]: **Portrait Mode Photography**: Uses dual cameras or LiDAR to create depth maps, then applies ML-ba...",
      "full_line": "[^fn-portrait-mode]: **Portrait Mode Photography**: Uses dual cameras or LiDAR to create depth maps, then applies ML-based segmentation to separate subjects from backgrounds. iPhone's Portrait mode processes multiple exposures in real-time, achieving DSLR-quality depth-of-field effects that would require expensive lenses and professional editing."
    },
    {
      "footnote_id": "fn-mobile-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 845,
      "context": "[^fn-mobile-constraints]: **Mobile Device Constraints**: Flagship phones typically have 12-24GB RAM and 512GB-2TB storage, v...",
      "full_line": "[^fn-mobile-constraints]: **Mobile Device Constraints**: Flagship phones typically have 12-24GB RAM and 512GB-2TB storage, versus cloud servers with 256-2048GB RAM and unlimited storage. Mobile processors operate at 15-25W peak power compared to server CPUs at 200-400W."
    },
    {
      "footnote_id": "fn-portrait-mode",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 853,
      "context": "...sly. These models work directly on the camera feed to enable features like portrait mode photography[^fn-portrait-mode], where ML algorithms separate foreground subjects from backgrounds. Document scanning applications...",
      "full_line": "Mobile ML has revolutionized how we use cameras on mobile devices, enabling sophisticated computer vision applications that process visual data in real-time. Modern smartphone cameras now incorporate ML models that can detect faces, analyze scenes, and apply complex filters instantaneously. These models work directly on the camera feed to enable features like portrait mode photography[^fn-portrait-mode], where ML algorithms separate foreground subjects from backgrounds. Document scanning applications use ML to detect paper edges, correct perspective, and enhance text readability, while augmented reality applications use ML-powered object detection to accurately place virtual objects in the real world."
    },
    {
      "footnote_id": "fn-microcontrollers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 867,
      "context": "Tiny ML brings intelligence to the smallest devices, from microcontrollers[^fn-microcontrollers] to embedded sensors, enabling real-time computation in severely resource-constrained environments....",
      "full_line": "Tiny ML brings intelligence to the smallest devices, from microcontrollers[^fn-microcontrollers] to embedded sensors, enabling real-time computation in severely resource-constrained environments. This paradigm excels in applications requiring ubiquitous sensing, autonomous operation, and extreme energy efficiency. Tiny ML systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition while optimized for energy efficiency[^fn-energy-efficiency], often running for months or years on limited power sources such as coin-cell batteries[^fn-coin-cell]. These systems deliver actionable insights in remote or disconnected environments where power, connectivity, and maintenance access are impractical."
    },
    {
      "footnote_id": "fn-energy-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 867,
      "context": "...ance, environmental monitoring, and simple gesture recognition while optimized for energy efficiency[^fn-energy-efficiency], often running for months or years on limited power sources such as coin-cell batteries[^fn-coin-ce...",
      "full_line": "Tiny ML brings intelligence to the smallest devices, from microcontrollers[^fn-microcontrollers] to embedded sensors, enabling real-time computation in severely resource-constrained environments. This paradigm excels in applications requiring ubiquitous sensing, autonomous operation, and extreme energy efficiency. Tiny ML systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition while optimized for energy efficiency[^fn-energy-efficiency], often running for months or years on limited power sources such as coin-cell batteries[^fn-coin-cell]. These systems deliver actionable insights in remote or disconnected environments where power, connectivity, and maintenance access are impractical."
    },
    {
      "footnote_id": "fn-coin-cell",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 867,
      "context": "...-efficiency], often running for months or years on limited power sources such as coin-cell batteries[^fn-coin-cell]. These systems deliver actionable insights in remote or disconnected environments where power, conn...",
      "full_line": "Tiny ML brings intelligence to the smallest devices, from microcontrollers[^fn-microcontrollers] to embedded sensors, enabling real-time computation in severely resource-constrained environments. This paradigm excels in applications requiring ubiquitous sensing, autonomous operation, and extreme energy efficiency. Tiny ML systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition while optimized for energy efficiency[^fn-energy-efficiency], often running for months or years on limited power sources such as coin-cell batteries[^fn-coin-cell]. These systems deliver actionable insights in remote or disconnected environments where power, connectivity, and maintenance access are impractical."
    },
    {
      "footnote_id": "fn-microcontrollers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 869,
      "context": "[^fn-microcontrollers]: **Microcontrollers**: Single-chip computers with integrated CPU, memory, and peripherals, typicall...",
      "full_line": "[^fn-microcontrollers]: **Microcontrollers**: Single-chip computers with integrated CPU, memory, and peripherals, typically operating at 1-100MHz with 32KB-2MB RAM. Arduino Uno uses an ATmega328P with 32KB flash and 2KB RAM, while ESP32 provides WiFi capability with 520KB RAM\u2014still thousands of times less than a smartphone."
    },
    {
      "footnote_id": "fn-energy-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 871,
      "context": "[^fn-energy-efficiency]: **Energy Efficiency in TinyML**: Ultra-low power consumption enables deployment in remote location...",
      "full_line": "[^fn-energy-efficiency]: **Energy Efficiency in TinyML**: Ultra-low power consumption enables deployment in remote locations. Modern ARM Cortex-M0+ microcontrollers consume <1\u00b5W in sleep mode and 100-300\u00b5W/MHz when active. Efficient ML inference can run for years on a single coin-cell battery."
    },
    {
      "footnote_id": "fn-coin-cell",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 873,
      "context": "[^fn-coin-cell]: **Coin-Cell Batteries**: Small, round batteries (CR2032 being most common) providing 200-250mAh at...",
      "full_line": "[^fn-coin-cell]: **Coin-Cell Batteries**: Small, round batteries (CR2032 being most common) providing 200-250mAh at 3V. When powering TinyML devices at 10-50mW average consumption, these batteries can operate devices for 1-5 years, enabling \"deploy-and-forget\" IoT applications."
    },
    {
      "footnote_id": "fn-on-device-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 950,
      "context": "...chine learning, similar to Mobile ML. Machine learning models are deployed and trained on the device[^fn-on-device-training], eliminating the need for external servers or cloud infrastructures. This enables intelligent decis...",
      "full_line": "Tiny ML focuses on on device machine learning, similar to Mobile ML. Machine learning models are deployed and trained on the device[^fn-on-device-training], eliminating the need for external servers or cloud infrastructures. This enables intelligent decision making where data is generated, making real time insights and actions possible, even in settings where connectivity is limited or unavailable."
    },
    {
      "footnote_id": "fn-on-device-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 952,
      "context": "[^fn-on-device-training]: **On-Device Training Constraints**: Unlike mobile devices, microcontrollers rarely support full mo...",
      "full_line": "[^fn-on-device-training]: **On-Device Training Constraints**: Unlike mobile devices, microcontrollers rarely support full model training due to memory limitations. Instead, they use techniques like transfer learning, where a pre-trained model is fine-tuned with minimal on-device adaptation, or federated learning aggregation where multiple devices collaboratively train a shared model."
    },
    {
      "footnote_id": "fn-device-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 954,
      "context": "...ices can typically fit in the palm of your hand or, in some cases, are even as small as a fingernail[^fn-device-size]. Tiny ML meets efficiency requirements through specialized algorithms and models designed to delive...",
      "full_line": "Tiny ML excels in low power and resource constrained settings. These environments require highly optimized solutions that function within available resources. @fig-TinyML-example shows an example Tiny ML device kit, illustrating the compact nature of these systems. These devices can typically fit in the palm of your hand or, in some cases, are even as small as a fingernail[^fn-device-size]. Tiny ML meets efficiency requirements through specialized algorithms and models designed to deliver acceptable performance while consuming minimal energy, ensuring extended operational periods, even in battery powered devices like those shown."
    },
    {
      "footnote_id": "fn-device-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 956,
      "context": "[^fn-device-size]: **TinyML Device Scale**: The smallest ML-capable devices measure just 5x5mm (Syntiant NDP chips)....",
      "full_line": "[^fn-device-size]: **TinyML Device Scale**: The smallest ML-capable devices measure just 5x5mm (Syntiant NDP chips). Google's Coral Dev Board Mini measures 40x48mm but includes WiFi and full Linux capability. The extreme miniaturization enables integration into previously \"dumb\" objects like smart dust sensors."
    },
    {
      "footnote_id": "fn-model-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 974,
      "context": "A central challenge in Tiny ML is model optimization and compression[^fn-model-compression]. Creating machine learning models that can operate effectively within the limited memory and comput...",
      "full_line": "A central challenge in Tiny ML is model optimization and compression[^fn-model-compression]. Creating machine learning models that can operate effectively within the limited memory and computational power of microcontrollers requires innovative approaches to model design. Developers often face the challenge of striking a delicate balance and optimizing models to maintain effectiveness while fitting within stringent resource constraints."
    },
    {
      "footnote_id": "fn-model-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 976,
      "context": "[^fn-model-compression]: **TinyML Model Optimization**: Specialized techniques that dramatically reduce model size and comp...",
      "full_line": "[^fn-model-compression]: **TinyML Model Optimization**: Specialized techniques that dramatically reduce model size and computational requirements. A typical smartphone model of 50MB might optimize to 250KB for microcontroller deployment while retaining 95% accuracy (techniques detailed in @sec-model-optimizations)."
    },
    {
      "footnote_id": "fn-fitness-trackers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 980,
      "context": "In wearables, Tiny ML opens the door to smarter, more responsive gadgets. From fitness trackers[^fn-fitness-trackers] offering real-time workout feedback to smart glasses[^fn-smart-glasses] processing visual data on t...",
      "full_line": "In wearables, Tiny ML opens the door to smarter, more responsive gadgets. From fitness trackers[^fn-fitness-trackers] offering real-time workout feedback to smart glasses[^fn-smart-glasses] processing visual data on the fly, Tiny ML transforms how we engage with wearable tech, delivering personalized experiences directly from the device."
    },
    {
      "footnote_id": "fn-smart-glasses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 980,
      "context": "...ts. From fitness trackers[^fn-fitness-trackers] offering real-time workout feedback to smart glasses[^fn-smart-glasses] processing visual data on the fly, Tiny ML transforms how we engage with wearable tech, delivering...",
      "full_line": "In wearables, Tiny ML opens the door to smarter, more responsive gadgets. From fitness trackers[^fn-fitness-trackers] offering real-time workout feedback to smart glasses[^fn-smart-glasses] processing visual data on the fly, Tiny ML transforms how we engage with wearable tech, delivering personalized experiences directly from the device."
    },
    {
      "footnote_id": "fn-fitness-trackers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 986,
      "context": "[^fn-fitness-trackers]: **TinyML in Fitness Trackers**: Modern fitness trackers use TinyML for activity recognition, sleep...",
      "full_line": "[^fn-fitness-trackers]: **TinyML in Fitness Trackers**: Modern fitness trackers use TinyML for activity recognition, sleep analysis, and health monitoring. Apple Watch can detect falls using accelerometer data and on-device ML, automatically calling emergency services. The algorithm analyzes motion patterns in real-time using <1mW power."
    },
    {
      "footnote_id": "fn-train-serve-split",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 988,
      "context": "[^fn-train-serve-split]: **Train-Serve Split Economics**: Training large models can cost $1-10M (GPT-3: $4.6M) but inferenc...",
      "full_line": "[^fn-train-serve-split]: **Train-Serve Split Economics**: Training large models can cost $1-10M (GPT-3: $4.6M) but inference costs <$0.01 per query when deployed efficiently [@brown2020language]. This 1,000,000x cost difference drives the pattern of expensive cloud training with cheap edge inference."
    },
    {
      "footnote_id": "fn-smart-glasses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 990,
      "context": "[^fn-smart-glasses]: **Smart Glasses with TinyML**: Google Glass Enterprise uses TinyML for real-time object recognitio...",
      "full_line": "[^fn-smart-glasses]: **Smart Glasses with TinyML**: Google Glass Enterprise uses TinyML for real-time object recognition and barcode scanning. The glasses process visual data locally using specialized vision chips consuming <500mW, enabling 8+ hour operation while providing instant augmented reality overlays."
    },
    {
      "footnote_id": "fn-train-serve-split",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 1013,
      "context": "...e training phase while benefiting from the low latency and privacy advantages of on-device inference[^fn-train-serve-split]. For example, smart home devices often use models trained on large datasets in the cloud but run in...",
      "full_line": "One of the most common hybrid patterns is the train-serve split, where model training occurs in the cloud but inference happens on edge, mobile, or tiny devices. This pattern takes advantage of the cloud's vast computational resources for the training phase while benefiting from the low latency and privacy advantages of on-device inference[^fn-train-serve-split]. For example, smart home devices often use models trained on large datasets in the cloud but run inference locally to ensure quick response times and protect user privacy. In practice, this might involve training models on powerful systems like the NVIDIA DGX A100, utilizing its 8 A100 GPUs and terabyte-scale memory, before deploying optimized versions to edge devices like the NVIDIA Jetson AGX Orin for efficient inference. Similarly, mobile vision models for computational photography are typically trained on powerful cloud infrastructure but deployed to run efficiently on phone hardware."
    },
    {
      "footnote_id": "fn-federated-architecture",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 1031,
      "context": "Federated learning[^fn-federated-architecture] represents a sophisticated hybrid approach that addresses the production challenge of learning from...",
      "full_line": "Federated learning[^fn-federated-architecture] represents a sophisticated hybrid approach that addresses the production challenge of learning from distributed data while maintaining privacy compliance. Google's production federated learning system processes 6 billion mobile keyboards, training improved models while keeping all typed text local\u2014achieving both privacy compliance and model improvement at scale. Each federated learning round involves 100-10,000 devices contributing model updates, requiring sophisticated orchestration to manage device availability, network conditions, and computational heterogeneity. Production federated systems face unique operational challenges including device dropout rates of 50-90% during training rounds, network bandwidth constraints that limit update frequency, and the need for differential privacy mechanisms to prevent information leakage. The aggregation servers must handle intermittent connectivity, varying computational capabilities across device types, and ensure robust convergence despite non-IID data distributions. These systems require specialized monitoring infrastructure to track training progress across distributed populations, debug convergence issues without accessing raw data, and manage the complex interplay between local learning rates and global aggregation strategies\u2014creating operational complexity that significantly exceeds traditional centralized training deployments."
    },
    {
      "footnote_id": "fn-federated-architecture",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 1037,
      "context": "[^fn-federated-architecture]: **Federated Learning Architecture**: Coordinates learning across millions of devices without centr...",
      "full_line": "[^fn-federated-architecture]: **Federated Learning Architecture**: Coordinates learning across millions of devices without centralizing data [@mcmahan2017federated]. Google's federated learning processes 6 billion mobile keyboards, training improved models while keeping all typed text local. Each round involves 100-10,000 devices contributing model updates."
    },
    {
      "footnote_id": "fn-a11-bionic-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 47,
      "context": "...ocess of training or adapting machine learning models directly on the device where they are deployed[^fn-a11-bionic-breakthrough]. : **On-Device Learning Evolution**: While edge inference emerged in the 2000s with mobile GPUs, on...",
      "full_line": "On-device learning refers to the process of training or adapting machine learning models directly on the device where they are deployed[^fn-a11-bionic-breakthrough]. : **On-Device Learning Evolution**: While edge inference emerged in the 2000s with mobile GPUs, on-device training didn't become feasible until Apple's A11 Bionic chip (2017) and Google's Pixel Visual Core (2017) provided sufficient TOPS for gradient computation. The shift from \"smart inference\" to \"smart training\" marked a core paradigm change in mobile AI."
    },
    {
      "footnote_id": "fn-a11-bionic-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 49,
      "context": "[^fn-a11-bionic-breakthrough]: **A11 Bionic Breakthrough**: Apple's A11 Bionic (2017) was the first mobile chip with sufficient c...",
      "full_line": "[^fn-a11-bionic-breakthrough]: **A11 Bionic Breakthrough**: Apple's A11 Bionic (2017) was the first mobile chip with sufficient computational power for on-device training, delivering 0.6 TOPS compared to the previous A10's 0.2 TOPS. This 3x improvement, combined with 4.3 billion transistors and a dual-core Neural Engine, allowed gradient computation for the first time on mobile devices. Google's Pixel Visual Core achieved similar capabilities with 8 custom Image Processing Units optimized for machine learning workloads."
    },
    {
      "footnote_id": "fn-federated-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 65,
      "context": "Particular emphasis is placed on distributed and collaborative methods, such as federated learning[^fn-federated-birth], which allow decentralized training without direct data sharing. The chapter concludes with an anal...",
      "full_line": "Particular emphasis is placed on distributed and collaborative methods, such as federated learning[^fn-federated-birth], which allow decentralized training without direct data sharing. The chapter concludes with an analysis of outstanding challenges, including issues related to reliability, system validation, and the heterogeneity of deployment environments."
    },
    {
      "footnote_id": "fn-federated-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 67,
      "context": "[^fn-federated-birth]: **Federated Learning Birth**: Google's Brendan McMahan coined \"federated learning\" in 2016 [@mcmah...",
      "full_line": "[^fn-federated-birth]: **Federated Learning Birth**: Google's Brendan McMahan coined \"federated learning\" in 2016 [@mcmahan2017communication], but the concept emerged from their Gboard team's frustration with keyboard personalization. They realized they needed user-specific data to improve predictions, but couldn't collect keystrokes due to privacy concerns. This led to the \"train where the data lives\" philosophy that defined federated learning."
    },
    {
      "footnote_id": "fn-gdpr-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 83,
      "context": "...ating within privacy-preserving boundaries, potentially aiding adherence to regulations such as GDPR[^fn-gdpr-impact], HIPAA [@hipaa1996health], or region-specific data sovereignty laws.",
      "full_line": "Privacy is another important factor. Many applications involve sensitive or regulated data, including biometric measurements, typed input, location traces, or health information. Transmitting such data to the cloud introduces privacy risks and compliance burdens. Local learning mitigates these concerns by keeping raw data on the device and operating within privacy-preserving boundaries, potentially aiding adherence to regulations such as GDPR[^fn-gdpr-impact], HIPAA [@hipaa1996health], or region-specific data sovereignty laws."
    },
    {
      "footnote_id": "fn-gdpr-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 85,
      "context": "[^fn-gdpr-impact]: **GDPR's ML Impact**: When GDPR took effect in May 2018 [@gdpr2016regulation], it made centralized...",
      "full_line": "[^fn-gdpr-impact]: **GDPR's ML Impact**: When GDPR took effect in May 2018 [@gdpr2016regulation], it made centralized ML training illegal for personal data without explicit consent. The \"right to be forgotten\" also meant models trained on personal data could be legally required to \"unlearn\" specific users\u2014technically impossible with traditional training. This drove massive investment in privacy-preserving ML techniques."
    },
    {
      "footnote_id": "fn-non-iid",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 137,
      "context": "...gion B depicts the process by which each device collects its own data stream, which is often non-IID[^fn-non-iid] and noisy, and adapts the model to better reflect its specific operating context. This marks the sh...",
      "full_line": "In contrast, once the model is deployed, local differences begin to emerge. Region B depicts the process by which each device collects its own data stream, which is often non-IID[^fn-non-iid] and noisy, and adapts the model to better reflect its specific operating context. This marks the shift from global generalization to local specialization, highlighting the autonomy and variability introduced by decentralized learning."
    },
    {
      "footnote_id": "fn-non-iid",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 139,
      "context": "[^fn-non-iid]: **Non-IID (Non-Independent and Identically Distributed)**: In machine learning, data is IID when s...",
      "full_line": "[^fn-non-iid]: **Non-IID (Non-Independent and Identically Distributed)**: In machine learning, data is IID when samples are drawn independently from the same distribution. Non-IID violates this assumption, common in federated learning where each device collects data from different users, environments, or use cases. For example, smartphone keyboard data varies dramatically between users (languages, writing styles, autocorrect needs), making personalized model training essential but challenging for convergence."
    },
    {
      "footnote_id": "fn-arduino-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 548,
      "context": "...far exceeds the memory available on embedded microcontrollers such as the Arduino Nano 33 BLE Sense[^fn-arduino-constraints], which provides only 256 KB of SRAM and 1 MB of flash storage. This constraint necessitates the mod...",
      "full_line": "For example, the MobileNetV2 architecture, commonly used in mobile vision tasks, requires approximately 14 MB of storage in its standard configuration. While this is feasible for modern smartphones, it far exceeds the memory available on embedded microcontrollers such as the Arduino Nano 33 BLE Sense[^fn-arduino-constraints], which provides only 256 KB of SRAM and 1 MB of flash storage. This constraint necessitates the model compression techniques detailed in @sec-model-optimizations. In such platforms, even a single layer of a typical convolutional neural network may exceed available RAM during training due to the need to store intermediate feature maps."
    },
    {
      "footnote_id": "fn-arduino-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 550,
      "context": "[^fn-arduino-constraints]: **Arduino Edge Computing Reality**: The Arduino Nano 33 BLE Sense represents typical microcontroll...",
      "full_line": "[^fn-arduino-constraints]: **Arduino Edge Computing Reality**: The Arduino Nano 33 BLE Sense represents typical microcontroller constraints\u2014256KB SRAM is roughly 65,000 times smaller than a modern smartphone's 16GB RAM. To put this in perspective, storing just one 224\u00d7224\u00d73 RGB image (150KB) would consume 60% of available memory. Training requires 3-5x more memory for gradients and activations, making even tiny models challenging. The 1MB flash storage can hold only the smallest quantized models, forcing designers to use 8-bit or even 4-bit representations."
    },
    {
      "footnote_id": "fn-mobilenet-innovation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 558,
      "context": "...e adaptation due to their size and complexity. Instead, lightweight architectures such as MobileNets[^fn-mobilenet-innovation], SqueezeNet [@iandola2016squeezenet], and EfficientNet [@tan2019efficientnet] have been developed s...",
      "full_line": "The model architecture itself must also be designed with on-device learning in mind. Many conventional architectures, such as transformers or large convolutional networks, are not well-suited for on-device adaptation due to their size and complexity. Instead, lightweight architectures such as MobileNets[^fn-mobilenet-innovation], SqueezeNet [@iandola2016squeezenet], and EfficientNet [@tan2019efficientnet] have been developed specifically for resource-constrained environments. These architectures leverage the efficiency principles and architectural optimizations detailed in @sec-model-optimizations. These models use techniques such as depthwise separable convolutions[^fn-depthwise-separable], bottleneck layers, and quantization to reduce memory and compute requirements while maintaining performance. The quantization techniques are extensively covered in @sec-model-optimizations, while the architectural design principles follow from @sec-efficient-ai."
    },
    {
      "footnote_id": "fn-depthwise-separable",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 558,
      "context": "...ed in @sec-model-optimizations. These models use techniques such as depthwise separable convolutions[^fn-depthwise-separable], bottleneck layers, and quantization to reduce memory and compute requirements while maintaining pe...",
      "full_line": "The model architecture itself must also be designed with on-device learning in mind. Many conventional architectures, such as transformers or large convolutional networks, are not well-suited for on-device adaptation due to their size and complexity. Instead, lightweight architectures such as MobileNets[^fn-mobilenet-innovation], SqueezeNet [@iandola2016squeezenet], and EfficientNet [@tan2019efficientnet] have been developed specifically for resource-constrained environments. These architectures leverage the efficiency principles and architectural optimizations detailed in @sec-model-optimizations. These models use techniques such as depthwise separable convolutions[^fn-depthwise-separable], bottleneck layers, and quantization to reduce memory and compute requirements while maintaining performance. The quantization techniques are extensively covered in @sec-model-optimizations, while the architectural design principles follow from @sec-efficient-ai."
    },
    {
      "footnote_id": "fn-mobilenet-innovation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 560,
      "context": "[^fn-mobilenet-innovation]: **MobileNet Innovation**: Google's MobileNet family revolutionized mobile AI by achieving 10-20x p...",
      "full_line": "[^fn-mobilenet-innovation]: **MobileNet Innovation**: Google's MobileNet family revolutionized mobile AI by achieving 10-20x parameter reduction compared to traditional CNNs. MobileNetV1 (2017) used depthwise separable convolutions to reduce floating-point operations (FLOPs) by 8-9x, while MobileNetV2 (2018) added inverted residuals and linear bottlenecks. The breakthrough allowed real-time inference on smartphones: MobileNetV2 runs ImageNet classification in ~75ms on a Pixel phone versus 1.8 seconds for ResNet-50 [@he2016deep]."
    },
    {
      "footnote_id": "fn-depthwise-separable",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 562,
      "context": "[^fn-depthwise-separable]: **Depthwise Separable Convolutions**: This technique decomposes standard convolution into two oper...",
      "full_line": "[^fn-depthwise-separable]: **Depthwise Separable Convolutions**: This technique decomposes standard convolution into two operations: depthwise convolution (applies single filter per input channel) and pointwise convolution (1\u00d71 conv to combine channels). For a 3\u00d73 conv with 512 input/output channels, standard convolution requires 2.4M parameters while depthwise separable needs only 13.8K\u2014a 174x reduction. The computational savings are similarly dramatic, making real-time inference possible on mobile CPUs."
    },
    {
      "footnote_id": "fn-stm32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 586,
      "context": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and la...",
      "full_line": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-point operations [@lai2020tinyml]. These constraints represent the fundamental limitations of edge hardware explored in @sec-ai-acceleration. These constraints preclude the use of conventional deep learning libraries and require models to be carefully designed for integer arithmetic and minimal runtime memory allocation. In such cases, even small models require tailored techniques, including quantization-aware training[^fn-quantization-aware] and selective parameter updates, to execute training loops without exceeding memory or power budgets. For example, the STM32F4 microcontroller can run a simple linear regression model with a few hundred parameters, but training even a small convolutional neural network would exceed its memory capacity. In these environments, training is often limited to simple algorithms such as stochastic gradient descent (SGD)[^fn-sgd] or $k$-means clustering, which can be implemented using integer arithmetic and minimal memory overhead."
    },
    {
      "footnote_id": "fn-esp32-capabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 586,
      "context": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-...",
      "full_line": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-point operations [@lai2020tinyml]. These constraints represent the fundamental limitations of edge hardware explored in @sec-ai-acceleration. These constraints preclude the use of conventional deep learning libraries and require models to be carefully designed for integer arithmetic and minimal runtime memory allocation. In such cases, even small models require tailored techniques, including quantization-aware training[^fn-quantization-aware] and selective parameter updates, to execute training loops without exceeding memory or power budgets. For example, the STM32F4 microcontroller can run a simple linear regression model with a few hundred parameters, but training even a small convolutional neural network would exceed its memory capacity. In these environments, training is often limited to simple algorithms such as stochastic gradient descent (SGD)[^fn-sgd] or $k$-means clustering, which can be implemented using integer arithmetic and minimal memory overhead."
    },
    {
      "footnote_id": "fn-quantization-aware",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 586,
      "context": "...In such cases, even small models require tailored techniques, including quantization-aware training[^fn-quantization-aware] and selective parameter updates, to execute training loops without exceeding memory or power budget...",
      "full_line": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-point operations [@lai2020tinyml]. These constraints represent the fundamental limitations of edge hardware explored in @sec-ai-acceleration. These constraints preclude the use of conventional deep learning libraries and require models to be carefully designed for integer arithmetic and minimal runtime memory allocation. In such cases, even small models require tailored techniques, including quantization-aware training[^fn-quantization-aware] and selective parameter updates, to execute training loops without exceeding memory or power budgets. For example, the STM32F4 microcontroller can run a simple linear regression model with a few hundred parameters, but training even a small convolutional neural network would exceed its memory capacity. In these environments, training is often limited to simple algorithms such as stochastic gradient descent (SGD)[^fn-sgd] or $k$-means clustering, which can be implemented using integer arithmetic and minimal memory overhead."
    },
    {
      "footnote_id": "fn-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 586,
      "context": "...vironments, training is often limited to simple algorithms such as stochastic gradient descent (SGD)[^fn-sgd] or $k$-means clustering, which can be implemented using integer arithmetic and minimal memory overh...",
      "full_line": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-point operations [@lai2020tinyml]. These constraints represent the fundamental limitations of edge hardware explored in @sec-ai-acceleration. These constraints preclude the use of conventional deep learning libraries and require models to be carefully designed for integer arithmetic and minimal runtime memory allocation. In such cases, even small models require tailored techniques, including quantization-aware training[^fn-quantization-aware] and selective parameter updates, to execute training loops without exceeding memory or power budgets. For example, the STM32F4 microcontroller can run a simple linear regression model with a few hundred parameters, but training even a small convolutional neural network would exceed its memory capacity. In these environments, training is often limited to simple algorithms such as stochastic gradient descent (SGD)[^fn-sgd] or $k$-means clustering, which can be implemented using integer arithmetic and minimal memory overhead."
    },
    {
      "footnote_id": "fn-quantization-aware",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 588,
      "context": "[^fn-quantization-aware]: **Quantization-Aware Training**: Unlike post-training quantization which converts trained FP32 mod...",
      "full_line": "[^fn-quantization-aware]: **Quantization-Aware Training**: Unlike post-training quantization which converts trained FP32 models to INT8, quantization-aware training simulates low-precision arithmetic during training itself. This allows the model to learn robust representations despite reduced precision. Critical for edge devices where INT8 operations consume 4x less power and enable 4x faster inference compared to FP32, while maintaining 95-99% of original accuracy."
    },
    {
      "footnote_id": "fn-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 590,
      "context": "[^fn-sgd]: **Stochastic Gradient Descent (SGD)**: The fundamental optimization algorithm for neural networks,...",
      "full_line": "[^fn-sgd]: **Stochastic Gradient Descent (SGD)**: The fundamental optimization algorithm for neural networks, updating parameters using gradients computed on small batches (or single samples). Unlike full-batch gradient descent, SGD's randomness helps escape local minima while requiring minimal memory\u2014storing only current parameters and gradients. This simplicity makes SGD ideal for microcontrollers where advanced optimizers like Adam would exceed memory budgets."
    },
    {
      "footnote_id": "fn-stm32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 592,
      "context": "[^fn-stm32-constraints]: **STM32F4 Microcontroller Reality**: The STM32F4 represents the harsh reality of embedded computin...",
      "full_line": "[^fn-stm32-constraints]: **STM32F4 Microcontroller Reality**: The STM32F4 represents the harsh reality of embedded computing\u2014192KB SRAM (roughly the size of a small JPEG image) and 1MB flash storage, running at 168MHz without floating-point hardware acceleration. Integer arithmetic is 10-100x slower than dedicated floating-point units found in mobile chips. Power consumption is ~100mW during active processing, requiring careful duty-cycling to preserve battery life. These constraints make even simple neural networks challenging\u2014a 10-neuron hidden layer requires ~40KB for weights alone in FP32."
    },
    {
      "footnote_id": "fn-esp32-capabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 594,
      "context": "[^fn-esp32-capabilities]: **ESP32 Edge Computing**: The ESP32 provides 520KB SRAM and dual-core processing at 240MHz, making...",
      "full_line": "[^fn-esp32-capabilities]: **ESP32 Edge Computing**: The ESP32 provides 520KB SRAM and dual-core processing at 240MHz, making it more capable than STM32F4 but still severely constrained. Its key advantage is built-in WiFi and Bluetooth for federated learning scenarios. However, the lack of hardware floating-point support means all ML operations must use integer quantization. Real-world deployments show 8-bit quantized models can achieve 95% of FP32 accuracy while fitting in ~50KB memory, enabling basic on-device training for simple tasks like sensor anomaly detection."
    },
    {
      "footnote_id": "fn-ondevice-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 596,
      "context": "In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-ondevice-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedic...",
      "full_line": "In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-ondevice-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedicated AI accelerators and optimized support for 8-bit or mixed-precision[^fn-mixed-precision] matrix operations. These accelerators, their capabilities, and their programming models are detailed in @sec-ai-acceleration. These platforms can support more complex training routines, including full backpropagation over compact models, though they still fall short of the computational throughput and memory bandwidth available in centralized data centers. For instance, training a lightweight transformer[^fn-transformer-mobile] on a smartphone is feasible but must be tightly bounded in both time and energy consumption to avoid degrading the user experience."
    },
    {
      "footnote_id": "fn-tensor-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 596,
      "context": "...ding the Qualcomm Snapdragon, Apple Neural Engine[^fn-ondevice-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedicated AI accelerators and optimized supp...",
      "full_line": "In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-ondevice-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedicated AI accelerators and optimized support for 8-bit or mixed-precision[^fn-mixed-precision] matrix operations. These accelerators, their capabilities, and their programming models are detailed in @sec-ai-acceleration. These platforms can support more complex training routines, including full backpropagation over compact models, though they still fall short of the computational throughput and memory bandwidth available in centralized data centers. For instance, training a lightweight transformer[^fn-transformer-mobile] on a smartphone is feasible but must be tightly bounded in both time and energy consumption to avoid degrading the user experience."
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 596,
      "context": "...mpute power, often with dedicated AI accelerators and optimized support for 8-bit or mixed-precision[^fn-mixed-precision] matrix operations. These accelerators, their capabilities, and their programming models are detaile...",
      "full_line": "In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-ondevice-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedicated AI accelerators and optimized support for 8-bit or mixed-precision[^fn-mixed-precision] matrix operations. These accelerators, their capabilities, and their programming models are detailed in @sec-ai-acceleration. These platforms can support more complex training routines, including full backpropagation over compact models, though they still fall short of the computational throughput and memory bandwidth available in centralized data centers. For instance, training a lightweight transformer[^fn-transformer-mobile] on a smartphone is feasible but must be tightly bounded in both time and energy consumption to avoid degrading the user experience."
    },
    {
      "footnote_id": "fn-transformer-mobile",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 596,
      "context": "...ry bandwidth available in centralized data centers. For instance, training a lightweight transformer[^fn-transformer-mobile] on a smartphone is feasible but must be tightly bounded in both time and energy consumption to avoi...",
      "full_line": "In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-ondevice-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedicated AI accelerators and optimized support for 8-bit or mixed-precision[^fn-mixed-precision] matrix operations. These accelerators, their capabilities, and their programming models are detailed in @sec-ai-acceleration. These platforms can support more complex training routines, including full backpropagation over compact models, though they still fall short of the computational throughput and memory bandwidth available in centralized data centers. For instance, training a lightweight transformer[^fn-transformer-mobile] on a smartphone is feasible but must be tightly bounded in both time and energy consumption to avoid degrading the user experience."
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 598,
      "context": "[^fn-mixed-precision]: **Mixed-Precision Training**: Uses different numerical precisions for different operations\u2014typical...",
      "full_line": "[^fn-mixed-precision]: **Mixed-Precision Training**: Uses different numerical precisions for different operations\u2014typically FP16 for forward/backward passes and FP32 for parameter updates. This halves memory usage and doubles throughput on modern hardware with Tensor Cores, while maintaining training stability through automatic loss scaling. Mobile implementations often use INT8 for inference and FP16 for gradient computation, balancing accuracy with hardware constraints."
    },
    {
      "footnote_id": "fn-transformer-mobile",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 600,
      "context": "[^fn-transformer-mobile]: **Lightweight Transformers**: Mobile-optimized transformer architectures like MobileBERT [@sun2020...",
      "full_line": "[^fn-transformer-mobile]: **Lightweight Transformers**: Mobile-optimized transformer architectures like MobileBERT [@sun2020mobilebert] and DistilBERT [@sanh2019distilbert] achieve 4-6x speedup over full models through techniques like knowledge distillation, layer reduction, and attention head pruning. MobileBERT retains 97% of BERT-base accuracy while running inference in ~40ms on mobile CPUs versus 160ms for full BERT. Key optimizations include bottleneck attention mechanisms and specialized mobile-friendly layer configurations."
    },
    {
      "footnote_id": "fn-ondevice-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 602,
      "context": "[^fn-ondevice-neural-engine]: **Apple Neural Engine Evolution**: Apple's Neural Engine has evolved dramatically since the A11 Bi...",
      "full_line": "[^fn-ondevice-neural-engine]: **Apple Neural Engine Evolution**: Apple's Neural Engine has evolved dramatically since the A11 Bionic. The A17 Pro (2023) features a 16-core Neural Engine delivering 35 TOPS\u2014roughly equivalent to an NVIDIA GTX 1080 Ti. This represents a 58x improvement over the original A11. The Neural Engine specializes in matrix operations with dedicated 8-bit and 16-bit arithmetic units, enabling efficient on-device training. Real-world performance: fine-tuning a MobileNet classifier takes ~2 seconds versus 45 seconds on CPU alone, while consuming only ~500mW additional power."
    },
    {
      "footnote_id": "fn-tensor-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 604,
      "context": "[^fn-tensor-soc]: **Google Tensor SoC Architecture**: Google's Tensor chips (starting with Pixel 6 in 2021) feature...",
      "full_line": "[^fn-tensor-soc]: **Google Tensor SoC Architecture**: Google's Tensor chips (starting with Pixel 6 in 2021) feature a custom TPU v1-derived Edge TPU optimized for ML workloads. Unlike Apple's Neural Engine, Tensor optimizes for Google's specific models (speech recognition, computational photography). The TPU provides efficient 8-bit integer operations while consuming only 2W, making it highly efficient for federated learning scenarios where devices train locally on speech or image data."
    },
    {
      "footnote_id": "fn-near-memory-compute",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 608,
      "context": "...ipping zero gradients\u2014critical for bias-only and LoRA adaptations. Near-memory compute architectures[^fn-near-memory-compute] reduce data movement costs by performing gradient updates directly adjacent to weight storage. Most...",
      "full_line": "The architectural implications of these hardware constraints extend beyond computational power. Training operations exhibit different memory access patterns than inference: backpropagation requires 3-5x higher memory bandwidth due to gradient computation and activation caching. Modern edge accelerators address these challenges through specialized hardware features. Adaptive precision datapaths allow dynamic switching between INT4 for forward passes and FP16 for gradient computation, optimizing both accuracy and efficiency. Sparse computation units accelerate selective parameter updates by skipping zero gradients\u2014critical for bias-only and LoRA adaptations. Near-memory compute architectures[^fn-near-memory-compute] reduce data movement costs by performing gradient updates directly adjacent to weight storage. Most current edge accelerators remain optimized for inference workloads, creating hardware-software co-design opportunities for future on-device training accelerators that handle the demands of local adaptation."
    },
    {
      "footnote_id": "fn-near-memory-compute",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 610,
      "context": "[^fn-near-memory-compute]: **Near-Memory Computing**: Places processing units directly adjacent to or within memory arrays, d...",
      "full_line": "[^fn-near-memory-compute]: **Near-Memory Computing**: Places processing units directly adjacent to or within memory arrays, dramatically reducing data movement costs. Traditional von Neumann architectures spend 100-1000x more energy moving data than computing on it. Near-memory designs can perform matrix operations with 10-100x better energy efficiency by eliminating costly memory bus transfers. Critical for edge training where gradient computations require intensive memory access patterns that overwhelm traditional cache hierarchies."
    },
    {
      "footnote_id": "fn-dvfs-mobile",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 628,
      "context": "...s implement sophisticated thermal management, including dynamic voltage and frequency scaling (DVFS)[^fn-dvfs-mobile], core migration between efficiency and performance clusters, and selective shutdown of non-essentia...",
      "full_line": "The thermal implications extend beyond power limits. Training workloads generate localized heat that can trigger protective throttling in specific processor cores or accelerator units. Modern mobile SoCs implement sophisticated thermal management, including dynamic voltage and frequency scaling (DVFS)[^fn-dvfs-mobile], core migration between efficiency and performance clusters, and selective shutdown of non-essential processing units. On-device learning systems must integrate with these thermal management frameworks, scheduling training bursts during optimal thermal windows and degrading when thermal limits are approached."
    },
    {
      "footnote_id": "fn-dvfs-mobile",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 630,
      "context": "[^fn-dvfs-mobile]: **Dynamic Voltage and Frequency Scaling (DVFS)**: Modern mobile processors continuously adjust ope...",
      "full_line": "[^fn-dvfs-mobile]: **Dynamic Voltage and Frequency Scaling (DVFS)**: Modern mobile processors continuously adjust operating voltage and clock frequency based on workload and thermal conditions. During ML training, DVFS can reduce clock speeds by 30-50% when temperature exceeds 70\u00b0C, directly impacting training throughput. Effective on-device learning systems monitor thermal state and proactively reduce batch sizes or training intensity to maintain consistent performance rather than experiencing sudden throttling events."
    },
    {
      "footnote_id": "fn-gradient-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 646,
      "context": "...of computational capacity. Advanced implementations employ techniques such as gradient checkpointing[^fn-gradient-checkpointing] to trade computation for memory, and mixed-precision training to reduce bandwidth requirements whil...",
      "full_line": "The memory bandwidth limitations become particularly acute during training. While inference workloads primarily read model weights sequentially, training requires bidirectional data flow for gradient computation and weight updates. This increased memory traffic can saturate the memory subsystem, creating bottlenecks that limit training throughput regardless of computational capacity. Advanced implementations employ techniques such as gradient checkpointing[^fn-gradient-checkpointing] to trade computation for memory, and mixed-precision training to reduce bandwidth requirements while maintaining numerical stability."
    },
    {
      "footnote_id": "fn-gradient-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 648,
      "context": "[^fn-gradient-checkpointing]: **Gradient Checkpointing**: A memory optimization technique that trades computation for memory by...",
      "full_line": "[^fn-gradient-checkpointing]: **Gradient Checkpointing**: A memory optimization technique that trades computation for memory by recomputing intermediate activations during the backward pass instead of storing them. This can reduce memory requirements by 50-80% at the cost of 20-30% additional computation. Particularly valuable for on-device training where memory is more constrained than compute capacity, enabling training of larger models within fixed memory budgets."
    },
    {
      "footnote_id": "fn-tinytl-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1044,
      "context": "...el using TinyTL can reduce the number of updated parameters from over 3 million to fewer than 50,000[^fn-tinytl-efficiency]. Combined with quantization, this allows local adaptation on devices with only a few hundred kiloby...",
      "full_line": "These design choices allow TinyTL to reduce training memory usage by 10\u00d7. For instance, adapting a MobileNetV2 model using TinyTL can reduce the number of updated parameters from over 3 million to fewer than 50,000[^fn-tinytl-efficiency]. Combined with quantization, this allows local adaptation on devices with only a few hundred kilobytes of memory\u2014making on-device learning truly feasible in constrained environments."
    },
    {
      "footnote_id": "fn-tinytl-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1046,
      "context": "[^fn-tinytl-efficiency]: **TinyTL Memory Breakthrough**: TinyTL's 60x parameter reduction (3M to 50K) translates to dramati...",
      "full_line": "[^fn-tinytl-efficiency]: **TinyTL Memory Breakthrough**: TinyTL's 60x parameter reduction (3M to 50K) translates to dramatic memory savings. In FP32, MobileNetV2 requires ~12MB for weights plus ~8MB for activation caching during training\u2014exceeding most microcontroller capabilities. TinyTL reduces this to ~200KB weights plus ~400KB activations, fitting comfortably within a 1MB memory budget. Real deployments on STM32H7 achieve 85% of full fine-tuning accuracy while using 15x less memory and completing updates in ~30 seconds versus 8 minutes for full training."
    },
    {
      "footnote_id": "fn-lora",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1083,
      "context": "This formulation is commonly used in LoRA (Low-Rank Adaptation)[^fn-lora] techniques, originally developed for transformer models [@hu2021lora] but broadly applicable across...",
      "full_line": "This formulation is commonly used in LoRA (Low-Rank Adaptation)[^fn-lora] techniques, originally developed for transformer models [@hu2021lora] but broadly applicable across architectures. Low-rank updates can be implemented efficiently on edge devices, particularly when $U$ and $V$ are small and fixed-point representations are supported (@lst-lowrank-adapter)."
    },
    {
      "footnote_id": "fn-lora",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1085,
      "context": "[^fn-lora]: **LoRA (Low-Rank Adaptation)**: Introduced by Microsoft in 2021, LoRA enables efficient fine-tunin...",
      "full_line": "[^fn-lora]: **LoRA (Low-Rank Adaptation)**: Introduced by Microsoft in 2021, LoRA enables efficient fine-tuning by learning low-rank decomposition matrices rather than updating full weight matrices. For a weight matrix W, LoRA learns rank-r matrices A and B such that the update is BA (where r << original dimensions). This reduces trainable parameters by 100-10000x while maintaining 90-95% adaptation quality. LoRA has become the standard for parameter-efficient fine-tuning in large language models."
    },
    {
      "footnote_id": "fn-hey-siri-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1250,
      "context": "...@warden2018speech]. These models are used to detect fixed phrases, including phrases like \"Hey Siri\"[^fn-hey-siri-constraints] or \"OK Google\", with low latency and high reliability. A typical KWS model consists of a pretrained...",
      "full_line": "Keyword spotting (KWS) systems offer a concrete example of few-shot adaptation in a real-world, on-device deployment [@warden2018speech]. These models are used to detect fixed phrases, including phrases like \"Hey Siri\"[^fn-hey-siri-constraints] or \"OK Google\", with low latency and high reliability. A typical KWS model consists of a pretrained acoustic encoder (e.g., a small convolutional or recurrent network that transforms input audio into an embedding space) followed by a lightweight classifier. In commercial systems, the encoder is trained centrally using thousands of hours of labeled speech across multiple languages and speakers. However, supporting custom wake words (e.g., \"Hey Jarvis\") or adapting to underrepresented accents and dialects is often infeasible via centralized training due to data scarcity and privacy concerns."
    },
    {
      "footnote_id": "fn-hey-siri-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1252,
      "context": "[^fn-hey-siri-constraints]: **\"Hey Siri\" Technical Reality**: Apple's \"Hey Siri\" system operates under extreme constraints\u2014det...",
      "full_line": "[^fn-hey-siri-constraints]: **\"Hey Siri\" Technical Reality**: Apple's \"Hey Siri\" system operates under extreme constraints\u2014detection must complete within 100ms to feel responsive, while consuming less than 1mW power when listening continuously. The always-on processor monitors audio using a 192KB model running at ~0.5 TOPS. False positive rate must be under 0.001% (less than once per day) while maintaining >95% true positive rate across accents, background noise, and speaking styles. The system processes 16kHz audio in 200ms windows, extracting Mel-frequency features for classification."
    },
    {
      "footnote_id": "fn-tinyml-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1305,
      "context": "In TinyML applications[^fn-tinyml-scale], experience replay has been applied to problems such as gesture recognition, where devices must con...",
      "full_line": "In TinyML applications[^fn-tinyml-scale], experience replay has been applied to problems such as gesture recognition, where devices must continuously improve predictions while observing a small number of events per day. Instead of training directly on the streaming data, the device stores representative feature vectors from recent gestures and uses them to finetune classification boundaries periodically. Similarly, in on-device keyword spotting, replaying past utterances can improve wake-word detection accuracy without the need to transmit audio data off-device."
    },
    {
      "footnote_id": "fn-tinyml-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1307,
      "context": "[^fn-tinyml-scale]: **TinyML Market Reality**: The TinyML market reached $2.4 billion in 2023 and is projected to grow...",
      "full_line": "[^fn-tinyml-scale]: **TinyML Market Reality**: The TinyML market reached $2.4 billion in 2023 and is projected to grow to $23.3 billion by 2030. Over 100 billion microcontrollers ship annually, but fewer than 1% currently support on-device learning due to memory and power constraints. Successful TinyML deployments typically consume <1mW power, use <256KB memory, and cost under $1 per chip. Applications include predictive maintenance (vibration sensors), health monitoring (heart rate variability), and smart agriculture (soil moisture prediction)."
    },
    {
      "footnote_id": "fn-mfcc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1338,
      "context": "...ting, where raw audio signals are first transformed into Mel-frequency cepstral coefficients (MFCCs)[^fn-mfcc]\u2014a compact, lossy representation of the power spectrum of speech. These MFCC vectors serve as compre...",
      "full_line": "In practice, these strategies have been applied in domains such as keyword spotting, where raw audio signals are first transformed into Mel-frequency cepstral coefficients (MFCCs)[^fn-mfcc]\u2014a compact, lossy representation of the power spectrum of speech. These MFCC vectors serve as compressed inputs for downstream models, enabling local adaptation using only a few kilobytes of memory."
    },
    {
      "footnote_id": "fn-mfcc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1340,
      "context": "[^fn-mfcc]: **Mel-Frequency Cepstral Coefficients (MFCCs)**: Audio features that mimic human auditory percepti...",
      "full_line": "[^fn-mfcc]: **Mel-Frequency Cepstral Coefficients (MFCCs)**: Audio features that mimic human auditory perception by applying mel-scale frequency warping (emphasizing lower frequencies where speech information concentrates) followed by cepstral analysis. A typical MFCC extraction converts 16kHz audio windows into 12-13 coefficients, reducing a 320-sample window (20ms) from 640 bytes to ~50 bytes while preserving speech intelligibility. Widely used in speech recognition since the 1980s due to robustness against noise and computational efficiency. Instead of storing raw audio waveforms, which are large and computationally expensive to process, devices store and learn from these compressed feature vectors directly. Similarly, in low-power computer vision systems, embeddings extracted from lightweight CNNs are retained and reused for few-shot learning. These examples illustrate how representation learning and compression serve as foundational tools for scaling on-device learning to memory- and bandwidth-constrained environments."
    },
    {
      "footnote_id": "fn-fedavg",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1651,
      "context": "The most widely used baseline for this process is Federated Averaging (FedAvg)[^fn-fedavg], which has become a canonical algorithm for federated learning [@mcmahan2017communication]. In FedA...",
      "full_line": "The most widely used baseline for this process is Federated Averaging (FedAvg)[^fn-fedavg], which has become a canonical algorithm for federated learning [@mcmahan2017communication]. In FedAvg, each device trains its local copy of the model using stochastic gradient descent (SGD) on its private data."
    },
    {
      "footnote_id": "fn-fedavg",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1653,
      "context": "[^fn-fedavg]: **Federated Averaging (FedAvg)**: Introduced by Google in 2017, FedAvg revolutionized distributed...",
      "full_line": "[^fn-fedavg]: **Federated Averaging (FedAvg)**: Introduced by Google in 2017, FedAvg revolutionized distributed ML by averaging model weights rather than gradients. Each client performs multiple local SGD steps (typically 1-20) before sending weights to the server, reducing communication by 10-100x compared to distributed SGD. The key insight: local updates contain richer information than single gradients, enabling convergence with far fewer communication rounds. FedAvg powers production systems like Gboard, processing billions of devices. After a fixed number of local steps, each device sends its updated model parameters to the server. The server computes a weighted average of these parameters, which are weighted according to the number of data samples on each device, and updates the global model accordingly. This updated model is then sent back to the devices, completing one round of training."
    },
    {
      "footnote_id": "fn-wireless-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1685,
      "context": "...nergy budgets, particularly for mobile or embedded devices operating over constrained wireless links[^fn-wireless-constraints]. To address this, a range of techniques have been developed to reduce communication overhead while...",
      "full_line": "One of the principal bottlenecks in federated ML systems is the cost of communication between edge clients and the central server. Transmitting full model weights or gradients after every training round can overwhelm bandwidth and energy budgets, particularly for mobile or embedded devices operating over constrained wireless links[^fn-wireless-constraints]. To address this, a range of techniques have been developed to reduce communication overhead while preserving learning efficacy."
    },
    {
      "footnote_id": "fn-wireless-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1687,
      "context": "[^fn-wireless-constraints]: **Wireless Communication Reality**: Mobile devices face severe bandwidth and energy constraints fo...",
      "full_line": "[^fn-wireless-constraints]: **Wireless Communication Reality**: Mobile devices face severe bandwidth and energy constraints for federated learning. LTE uploads average 5-10 Mbps versus 50+ Mbps downloads, creating asymmetric bottlenecks. Transmitting a 50MB model update consumes ~100mAh battery (2-3% of typical capacity) and takes 40-80 seconds. WiFi improves throughput but isn't always available. Low-power devices using LoRaWAN or NB-IoT face even harsher limits\u2014LoRaWAN maxes at 50kbps with 1% duty cycle restrictions, making frequent updates impractical without aggressive compression."
    },
    {
      "footnote_id": "fn-gradient-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1691,
      "context": "Model compression methods aim to reduce the size of transmitted updates through quantization[^fn-gradient-quantization], sparsification, or subsampling. Instead of sending full-precision gradients, a client transmits 8-...",
      "full_line": "Model compression methods aim to reduce the size of transmitted updates through quantization[^fn-gradient-quantization], sparsification, or subsampling. Instead of sending full-precision gradients, a client transmits 8-bit quantized updates or communicates only the top-$k$ gradient elements[^fn-gradient-sparsification] with highest magnitude."
    },
    {
      "footnote_id": "fn-gradient-sparsification",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1691,
      "context": "...ients, a client transmits 8-bit quantized updates or communicates only the top-$k$ gradient elements[^fn-gradient-sparsification] with highest magnitude.",
      "full_line": "Model compression methods aim to reduce the size of transmitted updates through quantization[^fn-gradient-quantization], sparsification, or subsampling. Instead of sending full-precision gradients, a client transmits 8-bit quantized updates or communicates only the top-$k$ gradient elements[^fn-gradient-sparsification] with highest magnitude."
    },
    {
      "footnote_id": "fn-gradient-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1693,
      "context": "[^fn-gradient-quantization]: **Gradient Quantization**: Reduces communication by converting FP32 gradients to lower precision (...",
      "full_line": "[^fn-gradient-quantization]: **Gradient Quantization**: Reduces communication by converting FP32 gradients to lower precision (INT8, INT4, or even 1-bit). Advanced techniques like signSGD use only gradient signs, achieving 32x compression. Error compensation methods accumulate quantization errors for later transmission, maintaining convergence quality. Real deployments achieve 8-16x communication reduction with <1% accuracy loss."
    },
    {
      "footnote_id": "fn-gradient-sparsification",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1695,
      "context": "[^fn-gradient-sparsification]: **Gradient Sparsification**: Transmits only the largest gradients by magnitude (typically top 1-10...",
      "full_line": "[^fn-gradient-sparsification]: **Gradient Sparsification**: Transmits only the largest gradients by magnitude (typically top 1-10%), dramatically reducing communication. Gradient accumulation stores untransmitted gradients locally until they become large enough to send. This technique exploits the observation that most gradients are small and contribute minimally to convergence, achieving 10-100x compression ratios while maintaining training effectiveness. These techniques reduce transmission size with limited impact on convergence when applied carefully."
    },
    {
      "footnote_id": "fn-differential-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1762,
      "context": "...er only observes the combined result, not any individual client's contribution. Differential Privacy[^fn-differential-privacy] techniques inject carefully calibrated noise into updates to mathematically bound the information t...",
      "full_line": "To mitigate such risks, modern federated ML systems commonly employ protective measures. Secure Aggregation protocols ensure that individual model updates are encrypted and aggregated in a way that the server only observes the combined result, not any individual client's contribution. Differential Privacy[^fn-differential-privacy] techniques inject carefully calibrated noise into updates to mathematically bound the information that can be inferred about any single client's data."
    },
    {
      "footnote_id": "fn-differential-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1764,
      "context": "[^fn-differential-privacy]: **Differential Privacy**: Provides mathematical privacy guarantees by adding calibrated noise to m...",
      "full_line": "[^fn-differential-privacy]: **Differential Privacy**: Provides mathematical privacy guarantees by adding calibrated noise to model updates, ensuring that participation by any individual cannot be distinguished. Developed by Dwork in 2006, it's now the gold standard for privacy-preserving ML. The privacy budget \u03b5 (epsilon) controls the privacy-utility tradeoff: smaller \u03b5 means stronger privacy but noisier results. Apple uses local differential privacy with \u03b5=4-14 for iOS telemetry, while Google's federated learning deployments use \u03b5\u224810."
    },
    {
      "footnote_id": "fn-fedasync",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1808,
      "context": "...d, but federated systems must handle stragglers and dropouts gracefully. Techniques such as FedAsync[^fn-fedasync] enable asynchronous aggregation where the server continuously updates the global model as client up...",
      "full_line": "The asynchronous nature of federated coordination introduces additional complexity in maintaining training convergence guarantees. Traditional synchronous training assumes all participants complete each round, but federated systems must handle stragglers and dropouts gracefully. Techniques such as FedAsync[^fn-fedasync] enable asynchronous aggregation where the server continuously updates the global model as client updates arrive, while bounded staleness mechanisms prevent extremely outdated updates from corrupting recent progress."
    },
    {
      "footnote_id": "fn-fedasync",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1810,
      "context": "[^fn-fedasync]: **Asynchronous Federated Learning (FedAsync)**: Enables continuous model updates without waiting f...",
      "full_line": "[^fn-fedasync]: **Asynchronous Federated Learning (FedAsync)**: Enables continuous model updates without waiting for slow or unreliable clients. The server maintains a global model that gets updated immediately when client contributions arrive, using staleness-aware weighting to reduce the influence of outdated updates. This approach can improve convergence speed by 2-5x in heterogeneous environments while maintaining model quality within 1-3% of synchronous training."
    },
    {
      "footnote_id": "fn-arm-cortex-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1957,
      "context": "...devices differ in terms of memory capacity, processor architecture (e.g., ARM Cortex-M vs. A-series)[^fn-arm-cortex-spectrum], instruction set support (e.g., availability of SIMD or floating-point units), and the presence or...",
      "full_line": "At the hardware level, devices differ in terms of memory capacity, processor architecture (e.g., ARM Cortex-M vs. A-series)[^fn-arm-cortex-spectrum], instruction set support (e.g., availability of SIMD or floating-point units), and the presence or absence of AI accelerators. Some clients may possess powerful NPUs capable of running small training loops, while others may rely solely on low-frequency CPUs with minimal RAM. These differences affect the feasible size of models, the choice of training algorithm, and the frequency of updates."
    },
    {
      "footnote_id": "fn-arm-cortex-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1959,
      "context": "[^fn-arm-cortex-spectrum]: **ARM Cortex Architecture Spectrum**: The ARM Cortex family spans 6 orders of magnitude in capabil...",
      "full_line": "[^fn-arm-cortex-spectrum]: **ARM Cortex Architecture Spectrum**: The ARM Cortex family spans 6 orders of magnitude in capabilities. Cortex-M0+ (IoT sensors) runs at 48MHz with 32KB RAM and no floating-point, consuming ~10\u00b5W. Cortex-M7 (embedded systems) reaches 400MHz with 1MB RAM and single-precision FPU, consuming ~100mW. Cortex-A78 (smartphones) delivers 3GHz performance with multi-core processing, NEON SIMD, and advanced branch prediction, consuming 1-5W. This diversity means federated learning must adapt algorithms dynamically\u2014quantized inference on M0+, lightweight training on M7, and full backpropagation on A78."
    },
    {
      "footnote_id": "fn-tflite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1987,
      "context": "...in @sec-ml-operations, while framework-specific considerations for edge deployment (TensorFlow Lite[^fn-tflite], ONNX Runtime, PyTorch Mobile) are covered in @sec-ai-frameworks.",
      "full_line": "A core difficulty lies in the absence of centralized validation data. In traditional workflows, models are trained and evaluated using curated datasets that serve as proxies for deployment conditions. On-device learners, by contrast, adapt in response to local inputs, which are rarely labeled and may not be systematically collected. As a result, the quality and direction of updates, whether they enhance generalization or cause drift, are difficult to assess without interfering with the user experience or violating privacy constraints. The broader challenges of monitoring distributed ML systems and debugging production deployments are addressed in @sec-ml-operations, while framework-specific considerations for edge deployment (TensorFlow Lite[^fn-tflite], ONNX Runtime, PyTorch Mobile) are covered in @sec-ai-frameworks."
    },
    {
      "footnote_id": "fn-tflite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1989,
      "context": "[^fn-tflite]: **TensorFlow Lite**: Google's framework for mobile and embedded ML inference, optimized for ARM pr...",
      "full_line": "[^fn-tflite]: **TensorFlow Lite**: Google's framework for mobile and embedded ML inference, optimized for ARM processors and mobile GPUs. TFLite reduces model size by 75% through quantization and pruning, while achieving 3x faster inference than full TensorFlow. The framework supports 16-bit and 8-bit quantization, with specialized kernels for mobile CPUs and GPUs. TFLite Micro targets microcontrollers with <1MB memory, enabling ML on Arduino and other embedded platforms."
    },
    {
      "footnote_id": "fn-microcontroller-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 2007,
      "context": "...rming a single epoch of adaptation on a microcontroller-class device can consume several millijoules[^fn-microcontroller-power]\u2014an appreciable fraction of the energy budget for a duty-cycled system operating on harvested power....",
      "full_line": "Energy consumption compounds this problem. Adaptation typically involves sustained computation over multiple input samples, which taxes battery-powered systems and may lead to rapid energy depletion. For instance, performing a single epoch of adaptation on a microcontroller-class device can consume several millijoules[^fn-microcontroller-power]\u2014an appreciable fraction of the energy budget for a duty-cycled system operating on harvested power. This necessitates careful scheduling, such that learning occurs only during idle periods, when energy reserves are high and user latency constraints are relaxed."
    },
    {
      "footnote_id": "fn-microcontroller-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 2009,
      "context": "[^fn-microcontroller-power]: **Microcontroller Power Budget Reality**: A typical microcontroller consuming 100mW during trainin...",
      "full_line": "[^fn-microcontroller-power]: **Microcontroller Power Budget Reality**: A typical microcontroller consuming 100mW during training exhausts 3.6 joules per hour, equivalent to a 1000mAh battery in 2.8 hours. Energy harvesting systems collect only 10-100mW continuously (solar panels in indoor light), making sustained training impossible. Real deployments use duty cycling: train for 10 seconds every hour, consuming ~1 joule total. This constrains training to 100-1000 gradient steps maximum, requiring extremely efficient algorithms and careful energy budgeting between sensing, computation, and communication."
    },
    {
      "footnote_id": "fn-activation-caching",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 2011,
      "context": "...training incurs higher peak usage than inference, due to the need to cache intermediate activations[^fn-activation-caching], gradients, and optimizer state [@lin2020mcunet].",
      "full_line": "From a memory perspective, training incurs higher peak usage than inference, due to the need to cache intermediate activations[^fn-activation-caching], gradients, and optimizer state [@lin2020mcunet]."
    },
    {
      "footnote_id": "fn-activation-caching",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 2013,
      "context": "[^fn-activation-caching]: **Activation Caching**: During backpropagation, forward pass activations must be stored to compute...",
      "full_line": "[^fn-activation-caching]: **Activation Caching**: During backpropagation, forward pass activations must be stored to compute gradients, dramatically increasing memory usage. For a typical CNN, activation memory can be 3-5x larger than model weights. Modern techniques like gradient checkpointing trade computation for memory by recomputing activations during backward pass, reducing memory by 80% at the cost of ~30% more compute time. Critical for training on memory-constrained devices where activation storage often exceeds available RAM. These requirements may exceed the static memory footprint anticipated during model deployment, particularly when adaptation involves multiple layers or gradient accumulation. In highly constrained systems, for example, systems with less than 512 KB of RAM, this may preclude certain types of adaptation altogether, unless additional optimization techniques (e.g., checkpointing or low-rank updates) are employed."
    },
    {
      "footnote_id": "fn-few-shot-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 2107,
      "context": "...ecomes paramount when learning from limited local examples, driving innovations in few-shot learning[^fn-few-shot-learning], streaming adaptation, and memory-based replay mechanisms.",
      "full_line": "The technical strategies that enable practical on-device learning span multiple dimensions of system design. Adaptation techniques range from lightweight bias-only updates to selective parameter tuning, each offering different tradeoffs between expressivity and resource efficiency. Data efficiency becomes paramount when learning from limited local examples, driving innovations in few-shot learning[^fn-few-shot-learning], streaming adaptation, and memory-based replay mechanisms."
    },
    {
      "footnote_id": "fn-few-shot-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 2109,
      "context": "[^fn-few-shot-learning]: **Few-Shot Learning**: Machine learning paradigm that learns new concepts from only a few (typical...",
      "full_line": "[^fn-few-shot-learning]: **Few-Shot Learning**: Machine learning paradigm that learns new concepts from only a few (typically 1-10) labeled examples. Originally inspired by human learning capabilities\u2014humans can recognize new objects from just one or two examples. In ML, few-shot learning leverages pre-trained representations and meta-learning to quickly adapt to new tasks. Critical for on-device scenarios where collecting large labeled datasets is impractical. Techniques include prototypical networks, model-agnostic meta-learning (MAML), and metric learning approaches that achieve 80-90% accuracy with just 5 examples per class. Federated learning emerges as a crucial coordination mechanism, allowing devices to collaborate while maintaining data locality and privacy guarantees."
    },
    {
      "footnote_id": "fn-mlops-emergence",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 45,
      "context": "Machine Learning Operations (MLOps)[^fn-mlops-emergence] is a systematic discipline that integrates machine learning, data science, and software engineering...",
      "full_line": "Machine Learning Operations (MLOps)[^fn-mlops-emergence] is a systematic discipline that integrates machine learning, data science, and software engineering practices to automate and streamline the end-to-end ML lifecycle. This lifecycle encompasses data preparation (building on data engineering foundations from @sec-data-engineering), model training, evaluation, deployment, monitoring, and ongoing maintenance. The goal of MLOps is to ensure that ML models are developed, deployed, and operated reliably, efficiently, and at scale."
    },
    {
      "footnote_id": "fn-mlops-emergence",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 47,
      "context": "[^fn-mlops-emergence]: **MLOps Emergence**: While machine learning operations challenges were identified earlier by D. Sc...",
      "full_line": "[^fn-mlops-emergence]: **MLOps Emergence**: While machine learning operations challenges were identified earlier by D. Sculley and colleagues at Google in their influential 2015 paper \"Hidden Technical Debt in Machine Learning Systems\" [@sculley2015hidden], the term \"MLOps\" itself was coined around 2018 as the discipline matured. The field emerged as organizations like Netflix, Uber, and Airbnb faced the \"last mile\" problem, where approximately 90% of ML models never made it to production according to industry surveys and anecdotal reports due to operational challenges."
    },
    {
      "footnote_id": "fn-devops-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 51,
      "context": "...collaboration across traditionally siloed roles, including data scientists, ML engineers, and DevOps[^fn-devops-origins] professionals, by defining interfaces and responsibilities. MLOps also supports continuous integrat...",
      "full_line": "By establishing standard protocols, tools, and workflows, MLOps allows models developed during experimentation, such as the architectures explored in @sec-dl-primer and @sec-dnn-architectures, to transition seamlessly into production. It promotes collaboration across traditionally siloed roles, including data scientists, ML engineers, and DevOps[^fn-devops-origins] professionals, by defining interfaces and responsibilities. MLOps also supports continuous integration and delivery for ML, allowing teams to retrain, validate, and redeploy models frequently in response to new data or system conditions."
    },
    {
      "footnote_id": "fn-devops-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 53,
      "context": "[^fn-devops-origins]: **DevOps Origins**: The \"wall of confusion\" between development and operations teams was so notori...",
      "full_line": "[^fn-devops-origins]: **DevOps Origins**: The \"wall of confusion\" between development and operations teams was so notorious that Patrick Debois called his 2009 conference \"DevOpsDays\" specifically to bridge this gap. The movement emerged from the frustrations of the \"throw it over the wall\" mentality where developers built software in isolation from operations teams who had to deploy and maintain it."
    },
    {
      "footnote_id": "fn-mlops-business-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 59,
      "context": "...LOps to increase team productivity, reduce time-to-market, and improve the reliability of ML systems[^fn-mlops-business-impact]. The adoption of MLOps not only enhances model performance and robustness but also supports a susta...",
      "full_line": "Organizations across sectors are adopting MLOps to increase team productivity, reduce time-to-market, and improve the reliability of ML systems[^fn-mlops-business-impact]. The adoption of MLOps not only enhances model performance and robustness but also supports a sustainable approach to managing ML systems at scale."
    },
    {
      "footnote_id": "fn-mlops-business-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 61,
      "context": "[^fn-mlops-business-impact]: **MLOps Business Impact**: Companies implementing mature MLOps practices report significant improv...",
      "full_line": "[^fn-mlops-business-impact]: **MLOps Business Impact**: Companies implementing mature MLOps practices report significant improvements in deployment speed (reducing time from months to weeks), substantial reductions in model debugging time, and improved model reliability. Organizations with mature MLOps practices consistently achieve higher model success rates moving from pilot to production compared to those using ad hoc approaches."
    },
    {
      "footnote_id": "fn-infrastructure-as-code",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 77,
      "context": "...ligned priorities. DevOps emerged as a response, advocating shared ownership, infrastructure as code[^fn-infrastructure-as-code], and automation to streamline deployment pipelines.",
      "full_line": "In traditional software pipelines, development and operations teams worked in silos, creating inefficiencies, delays, and misaligned priorities. DevOps emerged as a response, advocating shared ownership, infrastructure as code[^fn-infrastructure-as-code], and automation to streamline deployment pipelines."
    },
    {
      "footnote_id": "fn-infrastructure-as-code",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 79,
      "context": "[^fn-infrastructure-as-code]: **Infrastructure as Code**: The concept emerged from the painful lessons of \"snowflake servers\", u...",
      "full_line": "[^fn-infrastructure-as-code]: **Infrastructure as Code**: The concept emerged from the painful lessons of \"snowflake servers\", unique, manually-configured systems that were impossible to reproduce. Luke Kanies created Puppet in 2005 after experiencing the nightmare of managing hundreds of custom-configured servers at various startups."
    },
    {
      "footnote_id": "fn-jenkins-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 81,
      "context": "Tools such as [Jenkins](https://www.jenkins.io/)[^fn-jenkins-history], [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/)[^fn-kubernetes-birth]...",
      "full_line": "Tools such as [Jenkins](https://www.jenkins.io/)[^fn-jenkins-history], [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/)[^fn-kubernetes-birth] became foundational for implementing continuous integration and continuous delivery (CI/CD) practices."
    },
    {
      "footnote_id": "fn-kubernetes-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 81,
      "context": ".../)[^fn-jenkins-history], [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/)[^fn-kubernetes-birth] became foundational for implementing continuous integration and continuous delivery (CI/CD) practic...",
      "full_line": "Tools such as [Jenkins](https://www.jenkins.io/)[^fn-jenkins-history], [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/)[^fn-kubernetes-birth] became foundational for implementing continuous integration and continuous delivery (CI/CD) practices."
    },
    {
      "footnote_id": "fn-jenkins-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 83,
      "context": "[^fn-jenkins-history]: **Jenkins Origins**: Originally called \"Hudson,\" Jenkins was created by Kohsuke Kawaguchi at Sun M...",
      "full_line": "[^fn-jenkins-history]: **Jenkins Origins**: Originally called \"Hudson,\" Jenkins was created by Kohsuke Kawaguchi at Sun Microsystems in 2004 to automate his own tedious testing processes. The name change to \"Jenkins\" came in 2011 after a trademark dispute, named after the devoted butler from P.G. Wodehouse's stories."
    },
    {
      "footnote_id": "fn-kubernetes-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 85,
      "context": "[^fn-kubernetes-birth]: **Kubernetes Origins**: Greek for \"helmsman,\" Kubernetes emerged from Google's internal Borg syste...",
      "full_line": "[^fn-kubernetes-birth]: **Kubernetes Origins**: Greek for \"helmsman,\" Kubernetes emerged from Google's internal Borg system that managed billions of containers across their data centers. Google open-sourced it in 2014, realizing that their competitive advantage wasn't the orchestration system itself, but how they used it to run services at planetary scale."
    },
    {
      "footnote_id": "fn-data-drift-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 99,
      "context": "...rationalizing machine learning motivated the emergence of MLOps as a distinct discipline. Data drift[^fn-data-drift-discovery], where shifts in input data distributions over time degrade model accuracy, requires continuous mon...",
      "full_line": "Several recurring challenges in operationalizing machine learning motivated the emergence of MLOps as a distinct discipline. Data drift[^fn-data-drift-discovery], where shifts in input data distributions over time degrade model accuracy, requires continuous monitoring and automated retraining procedures."
    },
    {
      "footnote_id": "fn-data-drift-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 101,
      "context": "[^fn-data-drift-discovery]: **Data Drift Discovery**: The concept was first formalized by researchers studying spam detection...",
      "full_line": "[^fn-data-drift-discovery]: **Data Drift Discovery**: The concept was first formalized by researchers studying spam detection systems in the early 2000s, who noticed that spam patterns evolved so rapidly that models became obsolete within weeks. This led to the realization that ML systems face a fundamentally different challenge than traditional software: their environment actively adapts to defeat them."
    },
    {
      "footnote_id": "fn-reproducibility-crisis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 103,
      "context": "Equally critical is reproducibility[^fn-reproducibility-crisis]. ML workflows lack standardized mechanisms to track code, datasets, configurations, and environment...",
      "full_line": "Equally critical is reproducibility[^fn-reproducibility-crisis]. ML workflows lack standardized mechanisms to track code, datasets, configurations, and environments, making it difficult to reproduce past experiments [@schelter2018automating]. The lack of explainability in complex models has driven demand for tools that increase model transparency and interpretability, particularly in regulated domains."
    },
    {
      "footnote_id": "fn-reproducibility-crisis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 105,
      "context": "[^fn-reproducibility-crisis]: **ML Reproducibility Crisis**: A 2016 study by Collberg and Proebsting found that only 54% of comp...",
      "full_line": "[^fn-reproducibility-crisis]: **ML Reproducibility Crisis**: A 2016 study by Collberg and Proebsting found that only 54% of computer systems research papers could be reproduced even when authors were available to assist [@collberg2016repeatability]. This reproducibility challenge is even more acute in ML research, though the situation has improved with initiatives like Papers with Code and requirements for code submission at major ML conferences."
    },
    {
      "footnote_id": "fn-dvc-story",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 149,
      "context": "MLOps introduces specialized practices such as [data versioning](https://dvc.org/)[^fn-dvc-story], [model versioning](https://dvc.org/), and [model monitoring](https://www.fiddler.ai/) that go beyo...",
      "full_line": "MLOps introduces specialized practices such as [data versioning](https://dvc.org/)[^fn-dvc-story], [model versioning](https://dvc.org/), and [model monitoring](https://www.fiddler.ai/) that go beyond the scope of DevOps. It emphasizes scalable experimentation, reproducibility, governance, and responsiveness to evolving data conditions. @tbl-mlops summarizes key similarities and differences between DevOps and MLOps:"
    },
    {
      "footnote_id": "fn-dvc-story",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 151,
      "context": "[^fn-dvc-story]: **DVC Creation Story**: Data Version Control was born from the frustration of Dmitry Petrov, who s...",
      "full_line": "[^fn-dvc-story]: **DVC Creation Story**: Data Version Control was born from the frustration of Dmitry Petrov, who spent weeks trying to reproduce an experiment only to discover the training data had been quietly updated. He created DVC in 2017 to bring Git-like versioning to data science, solving what he called \"the biggest unsolved problem in machine learning.\""
    },
    {
      "footnote_id": "fn-feature-store-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 280,
      "context": "Feature stores[^fn-feature-store-scale] provide an abstraction layer between data engineering and machine learning. Their primary purpose i...",
      "full_line": "Feature stores[^fn-feature-store-scale] provide an abstraction layer between data engineering and machine learning. Their primary purpose is to enable consistent, reliable access to engineered features across training and inference workflows. In conventional pipelines, feature engineering logic is duplicated, manually reimplemented, or diverges across environments. This introduces risks of training-serving skew[^fn-training-serving-skew] (where features differ between training and production), data leakage, and model drift."
    },
    {
      "footnote_id": "fn-training-serving-skew",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 280,
      "context": "...ually reimplemented, or diverges across environments. This introduces risks of training-serving skew[^fn-training-serving-skew] (where features differ between training and production), data leakage, and model drift.",
      "full_line": "Feature stores[^fn-feature-store-scale] provide an abstraction layer between data engineering and machine learning. Their primary purpose is to enable consistent, reliable access to engineered features across training and inference workflows. In conventional pipelines, feature engineering logic is duplicated, manually reimplemented, or diverges across environments. This introduces risks of training-serving skew[^fn-training-serving-skew] (where features differ between training and production), data leakage, and model drift."
    },
    {
      "footnote_id": "fn-feature-store-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 282,
      "context": "[^fn-feature-store-scale]: **Feature Store Scale**: Uber's Michelangelo feature store serves 10+ million features per second...",
      "full_line": "[^fn-feature-store-scale]: **Feature Store Scale**: Uber's Michelangelo feature store serves 10+ million features per second with P99 latency under 10ms using optimized, co-located serving infrastructure, storing 200+ petabytes of feature data. Airbnb's feature store supports 1,000+ ML models with automated feature validation preventing 85% of potential training-serving skew issues."
    },
    {
      "footnote_id": "fn-training-serving-skew",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 284,
      "context": "[^fn-training-serving-skew]: **Training-Serving Skew Impact**: Studies show training-serving skew causes 5-15% accuracy degrada...",
      "full_line": "[^fn-training-serving-skew]: **Training-Serving Skew Impact**: Studies show training-serving skew causes 5-15% accuracy degradation in production models. Google reported that fixing skew issues improved ad click prediction accuracy by 8%, translating to millions in additional revenue annually."
    },
    {
      "footnote_id": "fn-github-actions-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 318,
      "context": "...s.io/), [CircleCI](https://circleci.com/), and [GitHub Actions](https://github.com/features/actions)[^fn-github-actions-ml] manage version control events and execution logic. These tools integrate with domain-specific platf...",
      "full_line": "A wide range of tools is available for implementing ML-focused CI/CD workflows. General-purpose CI/CD orchestrators such as [Jenkins](https://www.jenkins.io/), [CircleCI](https://circleci.com/), and [GitHub Actions](https://github.com/features/actions)[^fn-github-actions-ml] manage version control events and execution logic. These tools integrate with domain-specific platforms such as [Kubeflow](https://www.kubeflow.org/)[^fn-kubeflow-scale], [Metaflow](https://metaflow.org/), and [Prefect](https://www.prefect.io/), which offer higher-level abstractions for managing ML tasks and workflows."
    },
    {
      "footnote_id": "fn-kubeflow-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 318,
      "context": ".... These tools integrate with domain-specific platforms such as [Kubeflow](https://www.kubeflow.org/)[^fn-kubeflow-scale], [Metaflow](https://metaflow.org/), and [Prefect](https://www.prefect.io/), which offer higher-leve...",
      "full_line": "A wide range of tools is available for implementing ML-focused CI/CD workflows. General-purpose CI/CD orchestrators such as [Jenkins](https://www.jenkins.io/), [CircleCI](https://circleci.com/), and [GitHub Actions](https://github.com/features/actions)[^fn-github-actions-ml] manage version control events and execution logic. These tools integrate with domain-specific platforms such as [Kubeflow](https://www.kubeflow.org/)[^fn-kubeflow-scale], [Metaflow](https://metaflow.org/), and [Prefect](https://www.prefect.io/), which offer higher-level abstractions for managing ML tasks and workflows."
    },
    {
      "footnote_id": "fn-github-actions-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 320,
      "context": "[^fn-github-actions-ml]: **GitHub Actions for ML**: Over 60% of ML teams now use GitHub Actions for CI/CD according to rece...",
      "full_line": "[^fn-github-actions-ml]: **GitHub Actions for ML**: Over 60% of ML teams now use GitHub Actions for CI/CD according to recent developer surveys, with typical ML pipelines taking 15-45 minutes to run (vs. 2-5 minutes for traditional software). Netflix runs 10,000+ ML pipeline executions weekly through GitHub Actions, with 95% success rate on first run."
    },
    {
      "footnote_id": "fn-kubeflow-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 322,
      "context": "[^fn-kubeflow-scale]: **Kubeflow Production Usage**: Google's internal Kubeflow deployment runs 500,000+ ML jobs monthly...",
      "full_line": "[^fn-kubeflow-scale]: **Kubeflow Production Usage**: Google's internal Kubeflow deployment runs 500,000+ ML jobs monthly across 50+ clusters, with automatic resource scaling reducing training costs by 40%. Companies like Spotify use Kubeflow to orchestrate 1,000+ concurrent training jobs with fault tolerance."
    },
    {
      "footnote_id": "fn-cloud-ml-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 546,
      "context": "...at provision high-performance computing resources, which include GPU and TPU accelerators, on demand[^fn-cloud-ml-costs]. Depending on the platform, teams construct their own training workflows or rely on fully managed s...",
      "full_line": "The increasing availability of cloud-based infrastructure has further expanded the reach of model training. This connects to the workflow orchestration patterns explored in @sec-ai-workflow, which provide the foundation for managing complex, multi-stage training processes across distributed systems. Cloud providers offer managed services that provision high-performance computing resources, which include GPU and TPU accelerators, on demand[^fn-cloud-ml-costs]. Depending on the platform, teams construct their own training workflows or rely on fully managed services such as [Vertex AI Fine Tuning](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models), which support automated adaptation of foundation models to new tasks. Nonetheless, hardware availability, regional access restrictions, and cost constraints remain important considerations when designing cloud-based training systems."
    },
    {
      "footnote_id": "fn-cloud-ml-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 548,
      "context": "[^fn-cloud-ml-costs]: **Cloud ML Training Economics**: Training GPT-3 was estimated to cost approximately $4.6 million o...",
      "full_line": "[^fn-cloud-ml-costs]: **Cloud ML Training Economics**: Training GPT-3 was estimated to cost approximately $4.6 million on AWS according to Lambda Labs calculations, though official training costs were not disclosed by OpenAI, while fine-tuning typically costs $100-$10,000. Google's TPU v4 pods can reduce training costs by 2-5x compared to equivalent GPU clusters, with some organizations reporting 60-80% cost savings through spot instances and preemptible training."
    },
    {
      "footnote_id": "fn-docker-revolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 646,
      "context": "...oach to deployment involves containerizing models using tools like [Docker](https://www.docker.com/)[^fn-docker-revolution], which package code, libraries, and dependencies into standardized units. Containers ensure smooth...",
      "full_line": "One common approach to deployment involves containerizing models using tools like [Docker](https://www.docker.com/)[^fn-docker-revolution], which package code, libraries, and dependencies into standardized units. Containers ensure smooth portability across environments, making deployment consistent and predictable."
    },
    {
      "footnote_id": "fn-docker-revolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 648,
      "context": "[^fn-docker-revolution]: **Docker's Revolution**: Created by Solomon Hykes in 2013, Docker popularized the phrase \"But it w...",
      "full_line": "[^fn-docker-revolution]: **Docker's Revolution**: Created by Solomon Hykes in 2013, Docker popularized the phrase \"But it works on my machine!\" as the problem it solved. Within just 5 years, Docker went from a weekend project to being used by over 11 million developers worldwide, fundamentally changing how software is deployed."
    },
    {
      "footnote_id": "fn-tensorflow-serving-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 652,
      "context": "Before full-scale rollout, teams deploy updated models to staging or QA environments[^fn-tensorflow-serving-origins] to rigorously test performance.",
      "full_line": "Before full-scale rollout, teams deploy updated models to staging or QA environments[^fn-tensorflow-serving-origins] to rigorously test performance."
    },
    {
      "footnote_id": "fn-tensorflow-serving-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 654,
      "context": "[^fn-tensorflow-serving-origins]: **TensorFlow Serving Origins**: Born from Google's internal serving system that handled billions o...",
      "full_line": "[^fn-tensorflow-serving-origins]: **TensorFlow Serving Origins**: Born from Google's internal serving system that handled billions of predictions per day for products like Gmail spam detection and YouTube recommendations. Google open-sourced it in 2016 when they realized that productionizing ML models was the bottleneck preventing widespread AI adoption."
    },
    {
      "footnote_id": "fn-canary-deployment-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 656,
      "context": "Techniques such as shadow or canary deployments[^fn-canary-deployment-history] are used to validate new models incrementally. For instance, canary deployments route a small perce...",
      "full_line": "Techniques such as shadow or canary deployments[^fn-canary-deployment-history] are used to validate new models incrementally. For instance, canary deployments route a small percentage of traffic to the new model while closely monitoring performance. If no issues arise, traffic to the new model gradually increases. Robust rollback procedures are essential to handle unexpected issues, reverting systems to the previous stable model version to ensure minimal disruption."
    },
    {
      "footnote_id": "fn-canary-deployment-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 658,
      "context": "[^fn-canary-deployment-history]: **Canary Deployment History**: Named after the canaries miners used to detect toxic gases; if the...",
      "full_line": "[^fn-canary-deployment-history]: **Canary Deployment History**: Named after the canaries miners used to detect toxic gases; if the bird died, miners knew to evacuate immediately. Netflix pioneered this technique for software in 2011, and it became essential for ML where model failures can be subtle and catastrophic."
    },
    {
      "footnote_id": "fn-ab-testing-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 660,
      "context": "...e importance shifts that might explain degradation. Teams maintain debug toolkits including A/B test[^fn-ab-testing-ml] analysis frameworks, feature attribution tools, and data slice analyzers that identify which subpop...",
      "full_line": "When canary deployments reveal problems at partial traffic levels (e.g., issues appearing at 30% traffic but not at 5%), teams need systematic debugging strategies. Effective diagnosis requires correlating multiple signals: performance metrics from @sec-benchmarking-ai, data distribution analysis to detect drift, and feature importance shifts that might explain degradation. Teams maintain debug toolkits including A/B test[^fn-ab-testing-ml] analysis frameworks, feature attribution tools, and data slice analyzers that identify which subpopulations are experiencing degraded performance."
    },
    {
      "footnote_id": "fn-ab-testing-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 662,
      "context": "[^fn-ab-testing-ml]: **A/B Testing for ML**: Statistical method to compare model performance by splitting traffic betwe...",
      "full_line": "[^fn-ab-testing-ml]: **A/B Testing for ML**: Statistical method to compare model performance by splitting traffic between model versions. Netflix runs 1,000+ A/B tests annually on recommendation algorithms, while Uber tests ride pricing models on millions of trips daily to optimize both user experience and revenue. Rollback decisions must balance the severity of degradation against business impact: a 2% accuracy drop might be acceptable during feature launches but unacceptable for safety-critical applications."
    },
    {
      "footnote_id": "fn-serverless-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 668,
      "context": "..., to meet latency and throughput targets. Some providers also offer flexible options like serverless[^fn-serverless-ml] or batch inference, eliminating the need for persistent endpoints and enabling cost-efficient, scal...",
      "full_line": "Inference endpoints typically expose the deployed model via REST APIs for real-time predictions. Depending on performance requirements, teams can configure resources, such as GPU accelerators, to meet latency and throughput targets. Some providers also offer flexible options like serverless[^fn-serverless-ml] or batch inference, eliminating the need for persistent endpoints and enabling cost-efficient, scalable deployments."
    },
    {
      "footnote_id": "fn-serverless-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 670,
      "context": "[^fn-serverless-ml]: **Serverless Computing for ML**: Infrastructure that automatically scales from zero to thousands o...",
      "full_line": "[^fn-serverless-ml]: **Serverless Computing for ML**: Infrastructure that automatically scales from zero to thousands of instances based on demand, with sub-second cold start times. AWS Lambda can handle 10,000+ concurrent ML inference requests, while Google Cloud Functions supports models up to 32GB, charging only for actual compute time used. For example, [AWS SageMaker Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) supports such configurations."
    },
    {
      "footnote_id": "fn-mlflow-creation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 672,
      "context": "...facts, including scripts, weights, logs, and metrics, using tools like [MLflow](https://mlflow.org/)[^fn-mlflow-creation].",
      "full_line": "To maintain lineage and auditability, teams track model artifacts, including scripts, weights, logs, and metrics, using tools like [MLflow](https://mlflow.org/)[^fn-mlflow-creation]."
    },
    {
      "footnote_id": "fn-mlflow-creation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 674,
      "context": "[^fn-mlflow-creation]: **MLflow's Creation**: Built by the team at Databricks who were frustrated watching their customer...",
      "full_line": "[^fn-mlflow-creation]: **MLflow's Creation**: Built by the team at Databricks who were frustrated watching their customers struggle with ML experiment tracking. They noticed that data scientists were keeping model results in spreadsheets and could never reproduce their best experiments, a problem that inspired MLflow's \"model registry\" concept."
    },
    {
      "footnote_id": "fn-tensorflow-serving",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 682,
      "context": "...works have emerged. Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[^fn-tensorflow-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-...",
      "full_line": "To address these challenges, production-grade serving frameworks have emerged. Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[^fn-tensorflow-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized mechanisms for deploying, versioning, and scaling machine learning models across heterogeneous infrastructure. These frameworks abstract many of the lower-level concerns, allowing teams to focus on system behavior, integration, and performance targets."
    },
    {
      "footnote_id": "fn-triton-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 682,
      "context": "...low-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized me...",
      "full_line": "To address these challenges, production-grade serving frameworks have emerged. Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[^fn-tensorflow-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized mechanisms for deploying, versioning, and scaling machine learning models across heterogeneous infrastructure. These frameworks abstract many of the lower-level concerns, allowing teams to focus on system behavior, integration, and performance targets."
    },
    {
      "footnote_id": "fn-kserve-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 682,
      "context": "...on-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized mechanisms for deploying, versioning, and scaling machine learning models acro...",
      "full_line": "To address these challenges, production-grade serving frameworks have emerged. Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[^fn-tensorflow-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized mechanisms for deploying, versioning, and scaling machine learning models across heterogeneous infrastructure. These frameworks abstract many of the lower-level concerns, allowing teams to focus on system behavior, integration, and performance targets."
    },
    {
      "footnote_id": "fn-tensorflow-serving",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 684,
      "context": "[^fn-tensorflow-serving]: **TensorFlow Serving**: Google's production-grade ML serving system handles over 100,000 queries p...",
      "full_line": "[^fn-tensorflow-serving]: **TensorFlow Serving**: Google's production-grade ML serving system handles over 100,000 queries per second per machine for lightweight models on high-end hardware with <10ms latency for most models. Originally built to serve YouTube's recommendation system, processing over 1 billion hours of video watched daily."
    },
    {
      "footnote_id": "fn-triton-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 686,
      "context": "[^fn-triton-performance]: **NVIDIA Triton Inference Server**: Can achieve up to 40,000 inferences per second on a single A10...",
      "full_line": "[^fn-triton-performance]: **NVIDIA Triton Inference Server**: Can achieve up to 40,000 inferences per second on a single A100 GPU for BERT models, with dynamic batching reducing latency by up to 10x compared to naive serving approaches. Supports concurrent execution of up to 100 different model types."
    },
    {
      "footnote_id": "fn-kserve-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 688,
      "context": "[^fn-kserve-scaling]: **KServe (formerly KFServing)**: Kubernetes-native serving framework that can autoscale from zero...",
      "full_line": "[^fn-kserve-scaling]: **KServe (formerly KFServing)**: Kubernetes-native serving framework that can autoscale from zero to thousands of replicas in under 30 seconds. Used by companies like Bloomberg to serve over 10,000 models simultaneously with 99.9% uptime SLA."
    },
    {
      "footnote_id": "fn-sla-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 696,
      "context": "...at scale. Serving systems are therefore constructed to meet specific Service Level Agreements (SLAs)[^fn-sla-examples] and Service Level Objectives (SLOs)[^fn-slo-reality], which quantify acceptable performance boundar...",
      "full_line": "Each of these approaches introduces different constraints in terms of availability, responsiveness, and throughput. The efficiency techniques from @sec-efficient-ai become crucial for meeting these performance requirements, particularly when serving models at scale. Serving systems are therefore constructed to meet specific Service Level Agreements (SLAs)[^fn-sla-examples] and Service Level Objectives (SLOs)[^fn-slo-reality], which quantify acceptable performance boundaries along dimensions such as latency, error rates, and uptime. Achieving these goals requires a range of optimizations in request handling, scheduling, and resource allocation."
    },
    {
      "footnote_id": "fn-slo-reality",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 696,
      "context": "...meet specific Service Level Agreements (SLAs)[^fn-sla-examples] and Service Level Objectives (SLOs)[^fn-slo-reality], which quantify acceptable performance boundaries along dimensions such as latency, error rates, an...",
      "full_line": "Each of these approaches introduces different constraints in terms of availability, responsiveness, and throughput. The efficiency techniques from @sec-efficient-ai become crucial for meeting these performance requirements, particularly when serving models at scale. Serving systems are therefore constructed to meet specific Service Level Agreements (SLAs)[^fn-sla-examples] and Service Level Objectives (SLOs)[^fn-slo-reality], which quantify acceptable performance boundaries along dimensions such as latency, error rates, and uptime. Achieving these goals requires a range of optimizations in request handling, scheduling, and resource allocation."
    },
    {
      "footnote_id": "fn-sla-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 698,
      "context": "[^fn-sla-examples]: **Service Level Agreements (SLAs)**: Production ML systems typically target 99.9% uptime (8.77 hou...",
      "full_line": "[^fn-sla-examples]: **Service Level Agreements (SLAs)**: Production ML systems typically target 99.9% uptime (8.77 hours downtime/year) for critical services, with penalties of 10-25% monthly service credits for each 0.1% below target. Google's Cloud AI Platform promises 99.95% uptime with automatic failover in <30 seconds."
    },
    {
      "footnote_id": "fn-slo-reality",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 700,
      "context": "[^fn-slo-reality]: **Service Level Objectives (SLOs)**: Real-world ML serving SLOs often specify P95 latency <100ms f...",
      "full_line": "[^fn-slo-reality]: **Service Level Objectives (SLOs)**: Real-world ML serving SLOs often specify P95 latency <100ms for online inference, P99 <500ms, and error rates <0.1%. Netflix's recommendation system maintains P99 latency under 150ms while serving 200+ million users, processing 3+ billion hours of content monthly."
    },
    {
      "footnote_id": "fn-ml-autoscaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 775,
      "context": "...perparameter tuning and surges in prediction traffic, teams rely on cloud elasticity and autoscaling[^fn-ml-autoscaling]. Cloud platforms support on-demand provisioning and horizontal scaling of infrastructure resources....",
      "full_line": "To handle changes in workload intensity, including spikes during hyperparameter tuning and surges in prediction traffic, teams rely on cloud elasticity and autoscaling[^fn-ml-autoscaling]. Cloud platforms support on-demand provisioning and horizontal scaling of infrastructure resources. [Autoscaling mechanisms](https://aws.amazon.com/autoscaling/) automatically adjust compute capacity based on usage metrics, enabling teams to optimize for both performance and cost-efficiency."
    },
    {
      "footnote_id": "fn-ml-autoscaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 777,
      "context": "[^fn-ml-autoscaling]: **ML Autoscaling at Scale**: Kubernetes-based ML serving can scale from 1 to 1,000+ replicas in un...",
      "full_line": "[^fn-ml-autoscaling]: **ML Autoscaling at Scale**: Kubernetes-based ML serving can scale from 1 to 1,000+ replicas in under 60 seconds. Uber's ML platform automatically scales 2,000+ models daily, reducing infrastructure costs by 35-50% through intelligent resource allocation and cold-start optimization achieving 99.95% availability."
    },
    {
      "footnote_id": "fn-drift-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 801,
      "context": "Production ML systems face model drift[^fn-drift-detection] (see @sec-ml-operations-model-validation-eff3 for detailed analysis), which manifests in two main f...",
      "full_line": "Production ML systems face model drift[^fn-drift-detection] (see @sec-ml-operations-model-validation-eff3 for detailed analysis), which manifests in two main forms:"
    },
    {
      "footnote_id": "fn-drift-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 803,
      "context": "[^fn-drift-detection]: **Model Drift Detection**: Production systems typically trigger alerts when accuracy drops >5% ove...",
      "full_line": "[^fn-drift-detection]: **Model Drift Detection**: Production systems typically trigger alerts when accuracy drops >5% over 24 hours or >10% over a week. Advanced systems like those at Spotify detect drift within 2-4 hours using statistical tests, with 85% of drift incidents caught before user impact."
    },
    {
      "footnote_id": "fn-covid-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 805,
      "context": "- Concept drift[^fn-covid-impact] occurs when the underlying relationship between features and targets evolves. For example, during t...",
      "full_line": "- Concept drift[^fn-covid-impact] occurs when the underlying relationship between features and targets evolves. For example, during the COVID-19 pandemic, purchasing behavior shifted dramatically, invalidating many previously accurate recommendation models."
    },
    {
      "footnote_id": "fn-covid-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 807,
      "context": "[^fn-covid-impact]: **COVID-19 ML Impact**: E-commerce recommendation systems saw accuracy drops of 15-40% within week...",
      "full_line": "[^fn-covid-impact]: **COVID-19 ML Impact**: E-commerce recommendation systems saw accuracy drops of 15-40% within weeks of lockdowns beginning in March 2020. Amazon reported having to retrain over 1,000 models, while Netflix saw a 25% increase in viewing time that broke their capacity planning models."
    },
    {
      "footnote_id": "fn-prometheus-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 817,
      "context": "...radation that can violate inference latency SLAs. Tools such as [Prometheus](https://prometheus.io/)[^fn-prometheus-scale], [Grafana](https://grafana.com/), and [Elastic](https://www.elastic.co/) are widely used to collect...",
      "full_line": "Thermal monitoring integrates into operational scheduling decisions, particularly for sustained high-utilization deployments where thermal throttling can degrade performance unpredictably. Modern MLOps monitoring dashboards incorporate thermal headroom metrics that guide workload distribution across available hardware, preventing thermal-induced performance degradation that can violate inference latency SLAs. Tools such as [Prometheus](https://prometheus.io/)[^fn-prometheus-scale], [Grafana](https://grafana.com/), and [Elastic](https://www.elastic.co/) are widely used to collect, aggregate, and visualize these operational metrics. These tools often integrate into dashboards that offer real-time and historical views of system behavior."
    },
    {
      "footnote_id": "fn-prometheus-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 819,
      "context": "[^fn-prometheus-scale]: **Prometheus at Scale**: Can ingest 1+ million samples per second per instance, with some deployme...",
      "full_line": "[^fn-prometheus-scale]: **Prometheus at Scale**: Can ingest 1+ million samples per second per instance, with some deployments monitoring 100,000+ machines. DigitalOcean's Prometheus setup stores 2+ years of metrics data across 40,000+ time series, with query response times under 100ms for 95% of requests."
    },
    {
      "footnote_id": "fn-alerting-thresholds",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 821,
      "context": "...tive alerting mechanisms are configured to notify teams when anomalies or threshold violations occur[^fn-alerting-thresholds]. For example, a sustained drop in model accuracy may trigger an alert to investigate potential drif...",
      "full_line": "Proactive alerting mechanisms are configured to notify teams when anomalies or threshold violations occur[^fn-alerting-thresholds]. For example, a sustained drop in model accuracy may trigger an alert to investigate potential drift, prompting retraining with updated data. Similarly, infrastructure alerts can signal memory saturation or degraded network performance, allowing engineers to take corrective action before failures propagate."
    },
    {
      "footnote_id": "fn-alerting-thresholds",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 823,
      "context": "[^fn-alerting-thresholds]: **Production Alert Thresholds**: Typical ML production alerts fire when GPU memory >90%, CPU >85%...",
      "full_line": "[^fn-alerting-thresholds]: **Production Alert Thresholds**: Typical ML production alerts fire when GPU memory >90%, CPU >85% for >5 minutes, P99 latency >2x normal for >10 minutes, or error rates >1% for >60 seconds. Hardware-aware alerting extends these thresholds to include GPU utilization <60% for serving workloads (indicating resource waste), memory bandwidth utilization <40% (suggesting data pipeline bottlenecks), power consumption >110% of budget allocation (thermal risk), and thermal throttling events (immediate performance impact). High-frequency trading firms use microsecond-level alerts, while batch processing systems may use hour-long windows."
    },
    {
      "footnote_id": "fn-shap-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 845,
      "context": "...nsparency and explainability. For example, methods such as [SHAP](https://github.com/slundberg/shap)[^fn-shap-adoption] and [LIME](https://github.com/marcotcr/lime) offer post hoc explanations of model predictions by id...",
      "full_line": "Governance begins during the model development phase, where teams implement techniques to increase transparency and explainability. For example, methods such as [SHAP](https://github.com/slundberg/shap)[^fn-shap-adoption] and [LIME](https://github.com/marcotcr/lime) offer post hoc explanations of model predictions by identifying which input features were most influential in a particular decision. These interpretability techniques complement the security measures detailed in @sec-security-privacy, which address how to protect both model integrity and data privacy in production environments. These techniques allow auditors, developers, and non-technical stakeholders to better understand how and why a model behaves the way it does."
    },
    {
      "footnote_id": "fn-shap-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 847,
      "context": "[^fn-shap-adoption]: **SHAP in Production**: SHAP explanations add 10-500ms latency per prediction depending on model c...",
      "full_line": "[^fn-shap-adoption]: **SHAP in Production**: SHAP explanations add 10-500ms latency per prediction depending on model complexity, making them costly for real-time serving. However, 40% of enterprise ML teams now use SHAP in production, with Microsoft reporting SHAP helped identify $2M in potential bias-related legal exposure in their hiring models."
    },
    {
      "footnote_id": "fn-tech-debt-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 893,
      "context": "...t design decisions made during development. Originally proposed in software engineering in the 1990s[^fn-tech-debt-origin], this metaphor compares shortcuts in implementation to financial debt: it may enable short-term vel...",
      "full_line": "As machine learning systems mature and scale, they accumulate technical debt: the long-term cost of expedient design decisions made during development. Originally proposed in software engineering in the 1990s[^fn-tech-debt-origin], this metaphor compares shortcuts in implementation to financial debt: it may enable short-term velocity, but requires ongoing interest payments in the form of maintenance, refactoring, and systemic risk."
    },
    {
      "footnote_id": "fn-tech-debt-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 895,
      "context": "[^fn-tech-debt-origin]: **Technical Debt Origins**: Ward Cunningham coined the term in 1992, comparing rushed coding decis...",
      "full_line": "[^fn-tech-debt-origin]: **Technical Debt Origins**: Ward Cunningham coined the term in 1992, comparing rushed coding decisions to financial debt: \"A little debt speeds development so long as it is paid back promptly with a rewrite.\" He later regretted the metaphor became an excuse for bad code rather than a tool for communicating tradeoffs."
    },
    {
      "footnote_id": "fn-pipeline-jungle-metaphor",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1098,
      "context": "This problem is often described as the emergence of a \"pipeline jungle,\"[^fn-pipeline-jungle-metaphor] where modifications become difficult, and experimentation is constrained by brittle interdependenci...",
      "full_line": "This problem is often described as the emergence of a \"pipeline jungle,\"[^fn-pipeline-jungle-metaphor] where modifications become difficult, and experimentation is constrained by brittle interdependencies."
    },
    {
      "footnote_id": "fn-pipeline-jungle-metaphor",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1100,
      "context": "[^fn-pipeline-jungle-metaphor]: **Pipeline Jungle Metaphor**: Coined by Google researchers who observed that ML pipelines, like ju...",
      "full_line": "[^fn-pipeline-jungle-metaphor]: **Pipeline Jungle Metaphor**: Coined by Google researchers who observed that ML pipelines, like jungle growth, start simple but become increasingly tangled and impenetrable over time. Unlike traditional software, ML pipelines involve data flows that are harder to reason about, making the \"jungle\" metaphor particularly apt."
    },
    {
      "footnote_id": "fn-youtube-engagement",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1146,
      "context": "...s recommendation engine has faced repeated criticism for promoting sensational or polarizing content[^fn-youtube-engagement]. A large part of this stems from feedback loop debt: recommendations influence user behavior, which...",
      "full_line": "YouTube's recommendation engine has faced repeated criticism for promoting sensational or polarizing content[^fn-youtube-engagement]. A large part of this stems from feedback loop debt: recommendations influence user behavior, which in turn becomes training data. Over time, this led to unintended content amplification. Mitigating this required substantial architectural overhauls, including cohort-based evaluation, delayed labeling, and more explicit disentanglement between engagement metrics and ranking logic."
    },
    {
      "footnote_id": "fn-youtube-engagement",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1148,
      "context": "[^fn-youtube-engagement]: **YouTube Recommendation Impact**: The recommendation system drives 70% of watch time on the platf...",
      "full_line": "[^fn-youtube-engagement]: **YouTube Recommendation Impact**: The recommendation system drives 70% of watch time on the platform (1+ billion hours daily), but algorithmic changes in 2016 increased average session time by 50% while inadvertently promoting conspiracy content. Fixing these feedback loops required 2+ years of engineering work and new evaluation frameworks."
    },
    {
      "footnote_id": "fn-zillow-losses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1152,
      "context": "...'s home valuation model (Zestimate) faced significant correction cascades during its iBuying venture[^fn-zillow-losses]. When initial valuation errors propagated into purchasing decisions, retroactive corrections trigge...",
      "full_line": "Zillow's home valuation model (Zestimate) faced significant correction cascades during its iBuying venture[^fn-zillow-losses]. When initial valuation errors propagated into purchasing decisions, retroactive corrections triggered systemic instability that required data revalidation, model redesign, and eventually a full system rollback. The company shut down the iBuying arm in 2021, citing model unpredictability and data feedback effects as core challenges."
    },
    {
      "footnote_id": "fn-zillow-losses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1154,
      "context": "[^fn-zillow-losses]: **Zillow iBuying Failure**: Zillow lost $881 million in a single quarter (Q3 2021) due to ML model...",
      "full_line": "[^fn-zillow-losses]: **Zillow iBuying Failure**: Zillow lost $881 million in a single quarter (Q3 2021) due to ML model failures, with the Zestimate algorithm overvaluing homes by an average of 5-7%. The company laid off 2,000+ employees and took a $569 million inventory write-down when shutting down Zillow Offers."
    },
    {
      "footnote_id": "fn-plc-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1420,
      "context": "...an Airflow pipeline that ingests time-series sensor data from programmable logic controllers (PLCs)[^fn-plc-definition] on the factory floor.",
      "full_line": "For example, in a manufacturing application, data engineers may construct an Airflow pipeline that ingests time-series sensor data from programmable logic controllers (PLCs)[^fn-plc-definition] on the factory floor."
    },
    {
      "footnote_id": "fn-plc-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1422,
      "context": "[^fn-plc-definition]: **Programmable Logic Controllers (PLCs)**: Industrial computers designed to control manufacturing...",
      "full_line": "[^fn-plc-definition]: **Programmable Logic Controllers (PLCs)**: Industrial computers designed to control manufacturing processes, machines, and assembly lines. PLCs process thousands of sensor inputs per second with microsecond-level timing precision, forming the backbone of automated manufacturing systems worth over $80 billion globally."
    },
    {
      "footnote_id": "fn-telemetry-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1524,
      "context": "...nt, ML engineers play a critical role in monitoring model behavior. They configure telemetry systems[^fn-telemetry-ml] to track latency, failure rates, and resource usage, and they instrument prediction pipelines with...",
      "full_line": "Post-deployment, ML engineers play a critical role in monitoring model behavior. They configure telemetry systems[^fn-telemetry-ml] to track latency, failure rates, and resource usage, and they instrument prediction pipelines with logging and alerting mechanisms."
    },
    {
      "footnote_id": "fn-telemetry-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1526,
      "context": "[^fn-telemetry-ml]: **ML Telemetry**: Automated collection of operational data from ML systems including model perform...",
      "full_line": "[^fn-telemetry-ml]: **ML Telemetry**: Automated collection of operational data from ML systems including model performance metrics, infrastructure utilization, and prediction accuracy. Production ML systems generate 10GB-1TB of telemetry daily, enabling real-time drift detection and performance optimization."
    },
    {
      "footnote_id": "fn-elk-stack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1561,
      "context": "...d to both model and infrastructure performance. Tools such as Prometheus, Grafana, and the ELK stack[^fn-elk-stack] (Elasticsearch, Logstash, Kibana) are widely used to build dashboards, set thresholds, and generate...",
      "full_line": "Monitoring is another critical area of responsibility. DevOps engineers configure telemetry systems to collect metrics related to both model and infrastructure performance. Tools such as Prometheus, Grafana, and the ELK stack[^fn-elk-stack] (Elasticsearch, Logstash, Kibana) are widely used to build dashboards, set thresholds, and generate alerts."
    },
    {
      "footnote_id": "fn-elk-stack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1563,
      "context": "[^fn-elk-stack]: **ELK Stack**: Elasticsearch (search/analytics engine), Logstash (data processing pipeline), and K...",
      "full_line": "[^fn-elk-stack]: **ELK Stack**: Elasticsearch (search/analytics engine), Logstash (data processing pipeline), and Kibana (visualization platform). Can process terabytes of logs daily with millisecond search response times. Used by Netflix to analyze 1+ billion events daily and identify system anomalies in real-time."
    },
    {
      "footnote_id": "fn-microservices-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1759,
      "context": "...ectures using historical clickstream data, ML engineers deploy models as containerized microservices[^fn-microservices-ml], and DevOps engineers monitor inference latency and availability.",
      "full_line": "For example, in a real-time recommendation system, data engineers maintain the data ingestion pipeline and feature store, data scientists iterate on model architectures using historical clickstream data, ML engineers deploy models as containerized microservices[^fn-microservices-ml], and DevOps engineers monitor inference latency and availability."
    },
    {
      "footnote_id": "fn-microservices-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1761,
      "context": "[^fn-microservices-ml]: **Microservices in ML**: Architectural pattern where each ML model runs as an independent, loosely...",
      "full_line": "[^fn-microservices-ml]: **Microservices in ML**: Architectural pattern where each ML model runs as an independent, loosely-coupled service with its own database and deployment lifecycle. Netflix operates 700+ microservices including 100+ for ML recommendations, enabling independent scaling and faster experimentation cycles."
    },
    {
      "footnote_id": "fn-inference-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 47,
      "context": "...ion priorities. Considerations such as memory constraints, energy consumption, and inference latency[^fn-inference-latency] significantly influence the effective deployment of these models.",
      "full_line": "The performance gap between research and deployment is substantial. While research environments may achieve 125-275 TFLOPS on specialized hardware (V100-H100 GPUs), production systems often operate under severe constraints: mobile SoCs deliver 1-5 TOPS, embedded devices provide 0.1-1 TOPS, and microcontrollers offer mere MOPS of computation. Memory bandwidth varies even more dramatically, from 3,352 GB/s on H100 GPUs to 25-50 GB/s on mobile devices\u2014a 100x difference that fundamentally changes optimization priorities. Considerations such as memory constraints, energy consumption, and inference latency[^fn-inference-latency] significantly influence the effective deployment of these models."
    },
    {
      "footnote_id": "fn-inference-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 49,
      "context": "[^fn-inference-latency]: **Inference Latency**: Real-time applications require <10ms response (autonomous vehicles), <100ms...",
      "full_line": "[^fn-inference-latency]: **Inference Latency**: Real-time applications require <10ms response (autonomous vehicles), <100ms for interactive AI (voice assistants), vs. batch processing which tolerates seconds to minutes. Each millisecond of latency costs major web services millions in revenue."
    },
    {
      "footnote_id": "fn-opt-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 63,
      "context": "...eter complexity while preserving inferential capabilities are introduced. Techniques such as pruning[^fn-opt-pruning] and knowledge distillation[^fn-knowledge-distill] are examined for their ability to compress and re...",
      "full_line": "This chapter explores the principles of model optimization from a systems perspective. @fig-3-sections illustrates the three distinct layers of the optimization stack discussed in the chapter. At the highest level, methodologies aimed at reducing model parameter complexity while preserving inferential capabilities are introduced. Techniques such as pruning[^fn-opt-pruning] and knowledge distillation[^fn-knowledge-distill] are examined for their ability to compress and refine models, thereby enhancing model quality and improving system runtime performance."
    },
    {
      "footnote_id": "fn-knowledge-distill",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 63,
      "context": "...capabilities are introduced. Techniques such as pruning[^fn-opt-pruning] and knowledge distillation[^fn-knowledge-distill] are examined for their ability to compress and refine models, thereby enhancing model quality and i...",
      "full_line": "This chapter explores the principles of model optimization from a systems perspective. @fig-3-sections illustrates the three distinct layers of the optimization stack discussed in the chapter. At the highest level, methodologies aimed at reducing model parameter complexity while preserving inferential capabilities are introduced. Techniques such as pruning[^fn-opt-pruning] and knowledge distillation[^fn-knowledge-distill] are examined for their ability to compress and refine models, thereby enhancing model quality and improving system runtime performance."
    },
    {
      "footnote_id": "fn-opt-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 65,
      "context": "[^fn-opt-pruning]: **Pruning**: The optimal brain damage algorithm [@lecun1990optimal] pioneered removing unnecessary...",
      "full_line": "[^fn-opt-pruning]: **Pruning**: The optimal brain damage algorithm [@lecun1990optimal] pioneered removing unnecessary neural network connections, inspiring modern magnitude-based and structured pruning techniques that can reduce model sizes by 90% with minimal accuracy loss."
    },
    {
      "footnote_id": "fn-knowledge-distill",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 67,
      "context": "[^fn-knowledge-distill]: **Knowledge Distillation**: Hinton et al. [@hinton2015distilling] introduced this technique where...",
      "full_line": "[^fn-knowledge-distill]: **Knowledge Distillation**: Hinton et al. [@hinton2015distilling] introduced this technique where a smaller \"student\" network learns from a larger \"teacher\" network's soft outputs, enabling compact models that retain much of the original's performance while being orders of magnitude more efficient."
    },
    {
      "footnote_id": "fn-ml-runtimes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 96,
      "context": "...dependencies between software and hardware, with emphasis on the roles played by compilers, runtimes[^fn-ml-runtimes], and specialized accelerators in influencing optimization choices.",
      "full_line": "The chapter examines the factors that shape optimization approaches, including model representation, numerical precision, and architectural efficiency. We explore the interdependencies between software and hardware, with emphasis on the roles played by compilers, runtimes[^fn-ml-runtimes], and specialized accelerators in influencing optimization choices."
    },
    {
      "footnote_id": "fn-ml-runtimes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 98,
      "context": "[^fn-ml-runtimes]: **ML Runtimes**: ONNX Runtime, TensorFlow Lite, and PyTorch Mobile handle model execution, memory...",
      "full_line": "[^fn-ml-runtimes]: **ML Runtimes**: ONNX Runtime, TensorFlow Lite, and PyTorch Mobile handle model execution, memory management, and hardware acceleration. ONNX Runtime delivers 1.3-2.9x speedup over native frameworks through graph optimizations and efficient kernel implementations."
    },
    {
      "footnote_id": "fn-memory-hierarchy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 100,
      "context": "...arallel execution mapping, and accelerator-specific constraints\u2014are covered in @sec-ai-acceleration.[^fn-memory-hierarchy]",
      "full_line": "While this chapter focuses on optimization techniques and their system-level implications, detailed hardware architecture considerations\u2014including memory hierarchy design, data movement optimization, parallel execution mapping, and accelerator-specific constraints\u2014are covered in @sec-ai-acceleration.[^fn-memory-hierarchy]"
    },
    {
      "footnote_id": "fn-memory-hierarchy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 102,
      "context": "[^fn-memory-hierarchy]: **Memory Hierarchy**: L1 cache (32KB, <1ns), L2 cache (256KB-1MB, 3ns), L3 cache (8-32MB, 12ns), D...",
      "full_line": "[^fn-memory-hierarchy]: **Memory Hierarchy**: L1 cache (32KB, <1ns), L2 cache (256KB-1MB, 3ns), L3 cache (8-32MB, 12ns), DRAM (8-128GB, 100ns), SSD (TBs, 100\u03bcs). Each level differs by 10-100x in speed and capacity, making data locality optimization important for ML performance."
    },
    {
      "footnote_id": "fn-microcontroller-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 114,
      "context": "...more than just accuracy; it also depends on how efficiently it can be trained, stored, and executed.[^fn-microcontroller-constraints]",
      "full_line": "The real-world feasibility of a model depends on more than just accuracy; it also depends on how efficiently it can be trained, stored, and executed.[^fn-microcontroller-constraints]"
    },
    {
      "footnote_id": "fn-microcontroller-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 116,
      "context": "[^fn-microcontroller-constraints]: **Microcontroller Constraints**: Arduino Uno has 2KB RAM vs. 32KB flash storage. ARM Cortex-M4 typ...",
      "full_line": "[^fn-microcontroller-constraints]: **Microcontroller Constraints**: Arduino Uno has 2KB RAM vs. 32KB flash storage. ARM Cortex-M4 typically has 256KB flash, 64KB RAM, running at 168MHz vs. modern GPUs with 3000+ MHz clocks and 16-80GB memory\u2014representing a 10,000x+ resource gap."
    },
    {
      "footnote_id": "fn-edge-ml-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 118,
      "context": "...wer consumption, making large-scale AI workloads more efficient [@dean2018new]. In contrast, edge ML[^fn-edge-ml-definition] requires models to run with limited compute resources, necessitating optimizations that reduce memo...",
      "full_line": "In large-scale cloud ML settings, optimizing models helps minimize training time, computational cost, and power consumption, making large-scale AI workloads more efficient [@dean2018new]. In contrast, edge ML[^fn-edge-ml-definition] requires models to run with limited compute resources, necessitating optimizations that reduce memory footprint and computational complexity. Mobile ML introduces additional constraints, such as battery life and real-time responsiveness, while tiny ML[^fn-tiny-ml-definition] pushes efficiency to the extreme, requiring models to fit within the memory and processing limits of ultra-low-power devices [@banbury2020benchmarking]."
    },
    {
      "footnote_id": "fn-tiny-ml-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 118,
      "context": "...introduces additional constraints, such as battery life and real-time responsiveness, while tiny ML[^fn-tiny-ml-definition] pushes efficiency to the extreme, requiring models to fit within the memory and processing limits o...",
      "full_line": "In large-scale cloud ML settings, optimizing models helps minimize training time, computational cost, and power consumption, making large-scale AI workloads more efficient [@dean2018new]. In contrast, edge ML[^fn-edge-ml-definition] requires models to run with limited compute resources, necessitating optimizations that reduce memory footprint and computational complexity. Mobile ML introduces additional constraints, such as battery life and real-time responsiveness, while tiny ML[^fn-tiny-ml-definition] pushes efficiency to the extreme, requiring models to fit within the memory and processing limits of ultra-low-power devices [@banbury2020benchmarking]."
    },
    {
      "footnote_id": "fn-edge-ml-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 120,
      "context": "[^fn-edge-ml-definition]: **Edge ML**: Computing paradigm where ML inference occurs on local devices (smartphones, IoT senso...",
      "full_line": "[^fn-edge-ml-definition]: **Edge ML**: Computing paradigm where ML inference occurs on local devices (smartphones, IoT sensors, autonomous vehicles) rather than cloud servers. Reduces latency from 100-500ms cloud round-trip to <10ms local processing, but constrains models to 10-500MB vs. multi-GB cloud models."
    },
    {
      "footnote_id": "fn-tiny-ml-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 122,
      "context": "[^fn-tiny-ml-definition]: **Tiny ML**: Ultra-low-power ML systems operating under 1mW power budget with <1MB memory. Enables...",
      "full_line": "[^fn-tiny-ml-definition]: **Tiny ML**: Ultra-low-power ML systems operating under 1mW power budget with <1MB memory. Enables always-on AI in hearing aids, smart sensors, and wearables. Models typically 10-100KB vs. GB-scale cloud models, representing 10,000x size reduction."
    },
    {
      "footnote_id": "fn-memory-bandwidth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 130,
      "context": "...d more resource-intensive. Similarly, during training, larger models demand greater memory bandwidth[^fn-memory-bandwidth], longer training times, and more energy consumption, all of which introduce scalability concerns.",
      "full_line": "The fundamental tension between accuracy and efficiency drives optimization decisions across all three dimensions. Increasing model capacity generally enhances predictive performance but increases computational cost, making inference slower and more resource-intensive. Similarly, during training, larger models demand greater memory bandwidth[^fn-memory-bandwidth], longer training times, and more energy consumption, all of which introduce scalability concerns."
    },
    {
      "footnote_id": "fn-memory-bandwidth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 132,
      "context": "[^fn-memory-bandwidth]: **Memory Bandwidth**: Modern GPUs achieve 3.35TB/s memory bandwidth (H100) vs. 25-50 GB/s for mobi...",
      "full_line": "[^fn-memory-bandwidth]: **Memory Bandwidth**: Modern GPUs achieve 3.35TB/s memory bandwidth (H100) vs. 25-50 GB/s for mobile SoCs. Large language models require 1-2x model size in GPU memory for training (16GB model needs 32GB+ GPU memory), creating the \"memory wall\" bottleneck."
    },
    {
      "footnote_id": "fn-fpga-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 150,
      "context": "...ties of the target hardware. A model optimized for specialized accelerators (e.g., GPUs, TPUs, FPGAs[^fn-fpga-ml]) may not perform efficiently on general-purpose CPUs. : **TPU vs GPU**: Google's Tensor Processing...",
      "full_line": "**Scalability and Hardware Compatibility**: Model optimizations must align with the capabilities of the target hardware. A model optimized for specialized accelerators (e.g., GPUs, TPUs, FPGAs[^fn-fpga-ml]) may not perform efficiently on general-purpose CPUs. : **TPU vs GPU**: Google's Tensor Processing Units achieve 40-50% lower latency and 30-80% better energy efficiency than GPUs for inference workloads. TPU v4 delivers 275 TFLOPS vs. V100's 125 TFLOPS, but TPUs are specialized for matrix operations while GPUs offer more programming flexibility."
    },
    {
      "footnote_id": "fn-fpga-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 152,
      "context": "[^fn-fpga-ml]: **FPGA for ML**: Field-Programmable Gate Arrays offer 10-100x lower latency than GPUs (microsecond...",
      "full_line": "[^fn-fpga-ml]: **FPGA for ML**: Field-Programmable Gate Arrays offer 10-100x lower latency than GPUs (microseconds vs. milliseconds) for specific neural network architectures. Intel's Stratix 10 FPGA achieves 9.2 TFLOPS for INT8 inference with 2-5x better energy efficiency than GPUs for edge deployment."
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 174,
      "context": "...rators (@sec-ai-acceleration) such as GPUs, TPUs, and specialized AI chips. Mixed-precision training[^fn-mixed-precision] dynamically adjusts precision levels during training to strike a balance between efficiency and acc...",
      "full_line": "The second dimension, numerical precision optimization, addresses how numerical values are represented and processed within machine learning models. Reducing the precision of computations can significantly lower the memory and computational requirements of a model, particularly for machine learning workloads. Quantization techniques map high-precision weights and activations to lower-bit representations, enabling efficient execution on hardware accelerators (@sec-ai-acceleration) such as GPUs, TPUs, and specialized AI chips. Mixed-precision training[^fn-mixed-precision] dynamically adjusts precision levels during training to strike a balance between efficiency and accuracy."
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 176,
      "context": "[^fn-mixed-precision]: **Mixed-Precision Training**: Uses FP16 for forward pass and FP32 for gradient computation, achiev...",
      "full_line": "[^fn-mixed-precision]: **Mixed-Precision Training**: Uses FP16 for forward pass and FP32 for gradient computation, achieving 1.5-2x training speedup with 50% memory reduction. NVIDIA's automatic mixed precision (AMP) maintains FP32 accuracy while delivering 1.6x speedup on V100 and 2.2x on A100 GPUs."
    },
    {
      "footnote_id": "fn-sparsity-def",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 182,
      "context": "...mputational graphs, leading to inefficiencies in how operations are scheduled and executed. Sparsity[^fn-sparsity-def] represents a key architectural efficiency technique where models exploit zero-valued parameters to...",
      "full_line": "The third dimension, architectural efficiency, focuses on how computations are performed efficiently during both training and inference. A well-designed model structure is not sufficient if its execution is suboptimal. Many machine learning models contain redundancies in their computational graphs, leading to inefficiencies in how operations are scheduled and executed. Sparsity[^fn-sparsity-def] represents a key architectural efficiency technique where models exploit zero-valued parameters to reduce computation."
    },
    {
      "footnote_id": "fn-sparsity-def",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 184,
      "context": "[^fn-sparsity-def]: **Sparsity**: Percentage of zero-valued parameters in a model. 90% sparse models have only 10% non...",
      "full_line": "[^fn-sparsity-def]: **Sparsity**: Percentage of zero-valued parameters in a model. 90% sparse models have only 10% non-zero weights, reducing memory by 10x and computation by 10x (with specialized hardware). Modern transformers naturally exhibit 80-95% activation sparsity during inference."
    },
    {
      "footnote_id": "fn-matrix-factorization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 186,
      "context": "...th model weights and activations, factorize large computational components into more efficient forms[^fn-matrix-factorization], and dynamically adjust computation based on input complexity.",
      "full_line": "Architectural efficiency involves techniques that exploit sparsity in both model weights and activations, factorize large computational components into more efficient forms[^fn-matrix-factorization], and dynamically adjust computation based on input complexity."
    },
    {
      "footnote_id": "fn-matrix-factorization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 188,
      "context": "[^fn-matrix-factorization]: **Matrix Factorization**: Decomposes large weight matrices (e.g., 4096\u00d74096) into smaller matrices...",
      "full_line": "[^fn-matrix-factorization]: **Matrix Factorization**: Decomposes large weight matrices (e.g., 4096\u00d74096) into smaller matrices (4096\u00d7256 \u00d7 256\u00d74096), reducing parameters from 16M to 2M (8x reduction). SVD and low-rank approximations maintain 95%+ accuracy while enabling 3-5x speedup on mobile hardware."
    },
    {
      "footnote_id": "fn-lora",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 190,
      "context": "...cy also applies to training, where techniques such as gradient checkpointing and low-rank adaptation[^fn-lora] help reduce memory overhead and computational demands.",
      "full_line": "These methods improve execution efficiency across different hardware platforms, reducing latency and power consumption. In addition to inference optimizations, architectural efficiency also applies to training, where techniques such as gradient checkpointing and low-rank adaptation[^fn-lora] help reduce memory overhead and computational demands."
    },
    {
      "footnote_id": "fn-lora",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 192,
      "context": "[^fn-lora]: **LoRA (Low-Rank Adaptation)**: Fine-tuning technique that freezes pretrained weights and adds sma...",
      "full_line": "[^fn-lora]: **LoRA (Low-Rank Adaptation)**: Fine-tuning technique that freezes pretrained weights and adds small trainable matrices, reducing trainable parameters by 99% (from 175B to 1.2M for GPT-3 scale). Achieves comparable performance while reducing training memory and computation by 3x."
    },
    {
      "footnote_id": "fn-operator-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 198,
      "context": "...ive pruning and quantization, while a latency-sensitive application may benefit from operator fusion[^fn-operator-fusion] and hardware-aware scheduling.",
      "full_line": "The choice of optimizations is driven by system constraints, which define the practical limitations within which models must operate. A machine learning model deployed in a data center has different constraints from one running on a mobile device or an embedded system. Computational cost, memory usage, inference latency, and energy efficiency all influence which optimizations are most appropriate for a given scenario. A model that is too large for a resource-constrained device may require aggressive pruning and quantization, while a latency-sensitive application may benefit from operator fusion[^fn-operator-fusion] and hardware-aware scheduling."
    },
    {
      "footnote_id": "fn-operator-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 200,
      "context": "[^fn-operator-fusion]: **Operator Fusion**: Graph-level optimization that combines multiple operations into single kernel...",
      "full_line": "[^fn-operator-fusion]: **Operator Fusion**: Graph-level optimization that combines multiple operations into single kernels, reducing memory bandwidth by 30-50%. In ResNet-50, fusing Conv+BatchNorm+ReLU operations achieves 1.8x speedup on V100 GPUs, while BERT transformer blocks show 25% latency reduction through attention fusion. Dynamic computation approaches like early exit architectures and conditional computation are supported by custom execution runtimes that optimize control flow."
    },
    {
      "footnote_id": "fn-gradient-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 276,
      "context": "...ns efficiently to ensure that the model's architecture aligns well with modern hardware capabilities[^fn-gradient-checkpointing].",
      "full_line": "From a systems perspective, model representation optimization focuses on two key objectives. First, reducing redundancy by eliminating unnecessary parameters, neurons, or layers while preserving model accuracy. Many models are overparameterized, meaning that a smaller version could achieve similar performance with significantly lower computational overhead. Second, structuring computations efficiently to ensure that the model's architecture aligns well with modern hardware capabilities[^fn-gradient-checkpointing]."
    },
    {
      "footnote_id": "fn-gradient-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 278,
      "context": "[^fn-gradient-checkpointing]: **Gradient Checkpointing**: Memory optimization technique that trades computation for memory by re...",
      "full_line": "[^fn-gradient-checkpointing]: **Gradient Checkpointing**: Memory optimization technique that trades computation for memory by recomputing intermediate activations during backpropagation instead of storing them. Reduces memory usage by 20-50% in transformer models, enabling larger batch sizes or model sizes within same GPU memory."
    },
    {
      "footnote_id": "fn-parallel-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 280,
      "context": "This involves using parallel processing and minimizing costly memory operations[^fn-parallel-processing].",
      "full_line": "This involves using parallel processing and minimizing costly memory operations[^fn-parallel-processing]."
    },
    {
      "footnote_id": "fn-parallel-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 282,
      "context": "[^fn-parallel-processing]: **Parallel Processing in ML**: Modern GPUs have 5,000-10,000+ cores vs. CPU's 8-64 cores. NVIDIA H...",
      "full_line": "[^fn-parallel-processing]: **Parallel Processing in ML**: Modern GPUs have 5,000-10,000+ cores vs. CPU's 8-64 cores. NVIDIA H100 delivers 989 TFLOPS tensor performance vs. Intel Xeon 3175-X's ~1.5 TFLOPS (double precision), representing a 650x compute density advantage for parallelizable ML workloads."
    },
    {
      "footnote_id": "fn-gpu-operational-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 298,
      "context": "...PUs and TPUs (@sec-ai-acceleration) requires additional compute cycles, increasing operational costs[^fn-gpu-operational-costs].",
      "full_line": "2. **Increased Computational Cost**: More parameters lead to higher inference latency and energy consumption, which is particularly problematic for real-time applications such as autonomous systems, speech recognition, and mobile AI where optimization (@sec-ondevice-learning) becomes important. Running unoptimized models on hardware accelerators like GPUs and TPUs (@sec-ai-acceleration) requires additional compute cycles, increasing operational costs[^fn-gpu-operational-costs]."
    },
    {
      "footnote_id": "fn-gpu-operational-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 300,
      "context": "[^fn-gpu-operational-costs]: **GPU Operational Costs**: AWS p4d.24xlarge (8\u00d7A100 GPUs) costs $32.77/hour. Training GPT-3 requir...",
      "full_line": "[^fn-gpu-operational-costs]: **GPU Operational Costs**: AWS p4d.24xlarge (8\u00d7A100 GPUs) costs $32.77/hour. Training GPT-3 required 3,640 petaflop-days, costing approximately $4.6M in compute. Google's search uses 12TWh annually\u2014optimization reducing inference costs by 10% saves billions globally."
    },
    {
      "footnote_id": "fn-distributed-training-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 302,
      "context": "...scale is resource-intensive in terms of compute, memory, and power. Large-scale distributed training[^fn-distributed-training-costs] demands high-bandwidth communication and storage, while inference in production environments become...",
      "full_line": "3. **Scalability Limitations**: Training and deploying large models at scale is resource-intensive in terms of compute, memory, and power. Large-scale distributed training[^fn-distributed-training-costs] demands high-bandwidth communication and storage, while inference in production environments becomes costly without optimizations, requiring operational strategies (@sec-ml-operations)."
    },
    {
      "footnote_id": "fn-distributed-training-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 304,
      "context": "[^fn-distributed-training-costs]: **Distributed Training Costs**: Training large models requires InfiniBand networks (200+ Gb/s) cos...",
      "full_line": "[^fn-distributed-training-costs]: **Distributed Training Costs**: Training large models requires InfiniBand networks (200+ Gb/s) costing $2,000+ per port. Meta's OPT-175B used 992 A100 GPUs for 2 months, consuming ~1GWh of energy. Communication overhead can consume 20-40% of training time without optimization."
    },
    {
      "footnote_id": "fn-tf-model-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 316,
      "context": "...rch.nn.utils.prune` for pruning operations, while TensorFlow provides the Model Optimization Toolkit[^fn-tf-model-optimization] with functions like `tfmot.sparsity.keras.prune_low_magnitude()`. These tools transform complex res...",
      "full_line": "**Implementation Note**: Modern frameworks provide built-in APIs that make these optimization techniques readily accessible. PyTorch offers `torch.nn.utils.prune` for pruning operations, while TensorFlow provides the Model Optimization Toolkit[^fn-tf-model-optimization] with functions like `tfmot.sparsity.keras.prune_low_magnitude()`. These tools transform complex research algorithms into practical function calls, making optimization achievable for practitioners at all levels."
    },
    {
      "footnote_id": "fn-tf-model-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 318,
      "context": "[^fn-tf-model-optimization]: **TensorFlow Model Optimization**: TensorFlow Model Optimization Toolkit provides production-ready...",
      "full_line": "[^fn-tf-model-optimization]: **TensorFlow Model Optimization**: TensorFlow Model Optimization Toolkit provides production-ready quantization (achieving 4x model size reduction), pruning (up to 90% sparsity), and clustering techniques. Used by YouTube, Gmail, and Google Photos to deploy models on 4+ billion devices worldwide."
    },
    {
      "footnote_id": "fn-structured-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 814,
      "context": "While unstructured pruning removes individual weights from a neural network, structured pruning[^fn-structured-pruning] eliminates entire computational units, such as neurons, filters, channels, or layers. This approach...",
      "full_line": "While unstructured pruning removes individual weights from a neural network, structured pruning[^fn-structured-pruning] eliminates entire computational units, such as neurons, filters, channels, or layers. This approach is particularly beneficial for hardware efficiency, as it produces smaller dense models that can be directly mapped to modern machine learning accelerators. Unlike unstructured pruning, which results in sparse weight matrices that require specialized execution kernels to exploit computational benefits, structured pruning leads to more efficient inference on general-purpose hardware by reducing the overall size of the network architecture."
    },
    {
      "footnote_id": "fn-structured-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 816,
      "context": "[^fn-structured-pruning]: **Structured Pruning**: Filter pruning in ResNet-34 achieves 50% FLOP reduction with only 1.0% acc...",
      "full_line": "[^fn-structured-pruning]: **Structured Pruning**: Filter pruning in ResNet-34 achieves 50% FLOP reduction with only 1.0% accuracy loss on CIFAR-10. Channel pruning in MobileNetV2 reduces parameters by 73% while maintaining 96.5% of original accuracy, enabling 3.2x faster inference on ARM processors."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 982,
      "context": "...provides direct computational savings, as it reduces the number of floating-point operations (FLOPs)[^fn-flops] required during inference.",
      "full_line": "Structured pruning, in contrast, eliminates entire neurons, channels, or layers, leading to a more hardware-friendly model. This technique provides direct computational savings, as it reduces the number of floating-point operations (FLOPs)[^fn-flops] required during inference."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 984,
      "context": "[^fn-flops]: **FLOPs (Floating-Point Operations)**: Computational complexity metric counting multiply-add opera...",
      "full_line": "[^fn-flops]: **FLOPs (Floating-Point Operations)**: Computational complexity metric counting multiply-add operations. ResNet-50 requires 3.8 billion FLOPs per inference [@he2016deep], GPT-3 training needed 3.14E23 FLOPs [@patterson2021carbon]. Modern GPUs achieve 100-300 TFLOPS (trillion FLOPs/second), making FLOP reduction important for efficiency."
    },
    {
      "footnote_id": "fn-lottery-ticket",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1446,
      "context": "This perspective leads to the Lottery Ticket Hypothesis[^fn-lottery-ticket] (LTH), which challenges conventional pruning workflows by proposing that within large neural networ...",
      "full_line": "This perspective leads to the Lottery Ticket Hypothesis[^fn-lottery-ticket] (LTH), which challenges conventional pruning workflows by proposing that within large neural networks, there exist small, well-initialized subnetworks, referred to as 'winning tickets', that can achieve comparable accuracy to the full model when trained in isolation. Rather than viewing pruning as just a post-training compression step, LTH suggests it can serve as a discovery mechanism to identify these efficient subnetworks early in training."
    },
    {
      "footnote_id": "fn-lottery-ticket",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1448,
      "context": "[^fn-lottery-ticket]: **Lottery Ticket Hypothesis**: Frankle & Carbin (2018) demonstrated that ResNet-18 subnetworks at...",
      "full_line": "[^fn-lottery-ticket]: **Lottery Ticket Hypothesis**: Frankle & Carbin (2018) demonstrated that ResNet-18 subnetworks at 10-20% original size achieve 93.2% accuracy vs. 94.1% for full model on CIFAR-10. BERT-base winning tickets retain 97% performance with 90% fewer parameters, requiring 5-8x less training time to converge."
    },
    {
      "footnote_id": "fn-onnx-deployment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1709,
      "context": "ONNX[^fn-onnx-deployment], an open standard for model representation, does not implement pruning directly but provides export...",
      "full_line": "ONNX[^fn-onnx-deployment], an open standard for model representation, does not implement pruning directly but provides export and compatibility support for pruned models from PyTorch and TensorFlow. Since ONNX is designed to be hardware-agnostic, it allows models that have undergone pruning in different frameworks to be optimized for inference engines such as TensorRT[^fn-tensorrt-optimization], OpenVINO, and EdgeTPU. These inference engines can further leverage structured and dynamic pruning for execution efficiency, particularly on specialized hardware accelerators."
    },
    {
      "footnote_id": "fn-tensorrt-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1709,
      "context": "...ave undergone pruning in different frameworks to be optimized for inference engines such as TensorRT[^fn-tensorrt-optimization], OpenVINO, and EdgeTPU. These inference engines can further leverage structured and dynamic pruning...",
      "full_line": "ONNX[^fn-onnx-deployment], an open standard for model representation, does not implement pruning directly but provides export and compatibility support for pruned models from PyTorch and TensorFlow. Since ONNX is designed to be hardware-agnostic, it allows models that have undergone pruning in different frameworks to be optimized for inference engines such as TensorRT[^fn-tensorrt-optimization], OpenVINO, and EdgeTPU. These inference engines can further leverage structured and dynamic pruning for execution efficiency, particularly on specialized hardware accelerators."
    },
    {
      "footnote_id": "fn-onnx-deployment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1711,
      "context": "[^fn-onnx-deployment]: **ONNX Deployment**: ONNX Runtime achieves 1.3-2.9x speedup over TensorFlow and 1.1-1.7x over PyTo...",
      "full_line": "[^fn-onnx-deployment]: **ONNX Deployment**: ONNX Runtime achieves 1.3-2.9x speedup over TensorFlow and 1.1-1.7x over PyTorch across various models. ResNet-50 inference drops from 7.2ms to 2.8ms on CPU, while BERT-Base reduces from 45ms to 23ms with ONNX Runtime optimizations including graph fusion and memory pooling."
    },
    {
      "footnote_id": "fn-tensorrt-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1713,
      "context": "[^fn-tensorrt-optimization]: **TensorRT Optimization**: NVIDIA TensorRT delivers 5-40x speedup for inference on V100 GPUs (40x...",
      "full_line": "[^fn-tensorrt-optimization]: **TensorRT Optimization**: NVIDIA TensorRT delivers 5-40x speedup for inference on V100 GPUs (40x represents GPU+TensorRT vs CPU-only baseline; 5x is typical GPU optimization). ResNet-50 INT8 inference achieves 1.2ms vs. 4.8ms FP32, while BERT-Large drops from 10.4ms to 2.1ms. Layer fusion reduces kernel launches by 80%, memory bandwidth by 50%."
    },
    {
      "footnote_id": "fn-bert-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1741,
      "context": "...has been pruned to reduce inference latency while preserving accuracy [@howard2017mobilenets]. BERT[^fn-bert-compression], a widely used transformer model for natural language processing, has undergone structured pruning...",
      "full_line": "Understanding these trade-offs is important when deploying pruned models in real-world settings. Several high-profile models have successfully integrated pruning to optimize performance. MobileNet, a lightweight convolutional neural network designed for mobile and embedded applications, has been pruned to reduce inference latency while preserving accuracy [@howard2017mobilenets]. BERT[^fn-bert-compression], a widely used transformer model for natural language processing, has undergone structured pruning of attention heads and intermediate layers to create efficient versions such as DistilBERT[^fn-distilbert-metrics] and TinyBERT, which retain much of the original performance while reducing computational overhead [@sanh2019distilbert]. In computer vision, EfficientNet[^fn-efficientnet-pruning] has been pruned to remove unnecessary filters, optimizing it for deployment in resource-constrained environments [@tan2019efficientnet]."
    },
    {
      "footnote_id": "fn-distilbert-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1741,
      "context": "...d pruning of attention heads and intermediate layers to create efficient versions such as DistilBERT[^fn-distilbert-metrics] and TinyBERT, which retain much of the original performance while reducing computational overhead [...",
      "full_line": "Understanding these trade-offs is important when deploying pruned models in real-world settings. Several high-profile models have successfully integrated pruning to optimize performance. MobileNet, a lightweight convolutional neural network designed for mobile and embedded applications, has been pruned to reduce inference latency while preserving accuracy [@howard2017mobilenets]. BERT[^fn-bert-compression], a widely used transformer model for natural language processing, has undergone structured pruning of attention heads and intermediate layers to create efficient versions such as DistilBERT[^fn-distilbert-metrics] and TinyBERT, which retain much of the original performance while reducing computational overhead [@sanh2019distilbert]. In computer vision, EfficientNet[^fn-efficientnet-pruning] has been pruned to remove unnecessary filters, optimizing it for deployment in resource-constrained environments [@tan2019efficientnet]."
    },
    {
      "footnote_id": "fn-efficientnet-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1741,
      "context": "...rmance while reducing computational overhead [@sanh2019distilbert]. In computer vision, EfficientNet[^fn-efficientnet-pruning] has been pruned to remove unnecessary filters, optimizing it for deployment in resource-constrained...",
      "full_line": "Understanding these trade-offs is important when deploying pruned models in real-world settings. Several high-profile models have successfully integrated pruning to optimize performance. MobileNet, a lightweight convolutional neural network designed for mobile and embedded applications, has been pruned to reduce inference latency while preserving accuracy [@howard2017mobilenets]. BERT[^fn-bert-compression], a widely used transformer model for natural language processing, has undergone structured pruning of attention heads and intermediate layers to create efficient versions such as DistilBERT[^fn-distilbert-metrics] and TinyBERT, which retain much of the original performance while reducing computational overhead [@sanh2019distilbert]. In computer vision, EfficientNet[^fn-efficientnet-pruning] has been pruned to remove unnecessary filters, optimizing it for deployment in resource-constrained environments [@tan2019efficientnet]."
    },
    {
      "footnote_id": "fn-bert-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1743,
      "context": "[^fn-bert-compression]: **BERT Compression**: BERT-Base (110M params) can be compressed to 67M params (39% reduction) with...",
      "full_line": "[^fn-bert-compression]: **BERT Compression**: BERT-Base (110M params) can be compressed to 67M params (39% reduction) with only 1.2% GLUE score drop. Attention head pruning removes 144 of 192 heads with minimal impact, while layer pruning reduces 12 layers to 6 layers maintaining 97.8% performance."
    },
    {
      "footnote_id": "fn-distilbert-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1745,
      "context": "[^fn-distilbert-metrics]: **DistilBERT**: Achieves 97% of BERT-Base performance with 40% fewer parameters (66M vs. 110M) and...",
      "full_line": "[^fn-distilbert-metrics]: **DistilBERT**: Achieves 97% of BERT-Base performance with 40% fewer parameters (66M vs. 110M) and 60% faster inference. On SQuAD v1.1, DistilBERT scores 86.9 F1 vs. BERT's 88.5 F1, while reducing memory from 1.35GB to 0.54GB and latency from 85ms to 34ms."
    },
    {
      "footnote_id": "fn-efficientnet-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1747,
      "context": "[^fn-efficientnet-pruning]: **EfficientNet Pruning**: EfficientNet-B0 with 70% structured pruning maintains 75.8% ImageNet acc...",
      "full_line": "[^fn-efficientnet-pruning]: **EfficientNet Pruning**: EfficientNet-B0 with 70% structured pruning maintains 75.8% ImageNet accuracy (vs. 77.1% original) with 2.8x speedup on mobile devices. Channel pruning reduces FLOPs from 390M to 140M while keeping inference under 20ms on Pixel 4."
    },
    {
      "footnote_id": "fn-hardware-aware-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2428,
      "context": "NAS[^fn-hardware-aware-nas] addresses these limitations by automating model design. As illustrated in @fig-nas-flow, instead of...",
      "full_line": "NAS[^fn-hardware-aware-nas] addresses these limitations by automating model design. As illustrated in @fig-nas-flow, instead of manually tuning configurations, NAS structuredally explores a large space of architectures to identify those that best balance accuracy, computational cost, memory efficiency, and inference latency. By framing model selection as a structured search problem, NAS reduces reliance on trial and error, allowing architectures to be discovered programmatically rather than heuristically [@zoph2017neural], creating pathways to the automated optimization techniques explored in @sec-efficient-ai."
    },
    {
      "footnote_id": "fn-hardware-aware-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2430,
      "context": "[^fn-hardware-aware-nas]: **Hardware-Aware NAS**: MnasNet achieves 78.1% ImageNet accuracy with 315M FLOPs vs. MobileNetV2's...",
      "full_line": "[^fn-hardware-aware-nas]: **Hardware-Aware NAS**: MnasNet achieves 78.1% ImageNet accuracy with 315M FLOPs vs. MobileNetV2's 72.0% with 300M FLOPs. EfficientNet-B0 delivers 77.1% accuracy with 390M FLOPs, 23% better accuracy/FLOP ratio than ResNet-50, enabling 4.9x faster mobile inference."
    },
    {
      "footnote_id": "fn-rl-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2466,
      "context": "...formalizes model design as an optimization problem, using techniques such as reinforcement learning[^fn-rl-nas], evolutionary algorithms[^fn-evolutionary-nas], and gradient-based methods to automate decisions tr...",
      "full_line": "NAS formalizes model design as an optimization problem, using techniques such as reinforcement learning[^fn-rl-nas], evolutionary algorithms[^fn-evolutionary-nas], and gradient-based methods to automate decisions traditionally made by experts [@real2019regularized]. This approach integrates principles of scaling optimization, structural pruning, and compressed representations, offering a unified framework for model efficiency."
    },
    {
      "footnote_id": "fn-evolutionary-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2466,
      "context": "...zation problem, using techniques such as reinforcement learning[^fn-rl-nas], evolutionary algorithms[^fn-evolutionary-nas], and gradient-based methods to automate decisions traditionally made by experts [@real2019regulariz...",
      "full_line": "NAS formalizes model design as an optimization problem, using techniques such as reinforcement learning[^fn-rl-nas], evolutionary algorithms[^fn-evolutionary-nas], and gradient-based methods to automate decisions traditionally made by experts [@real2019regularized]. This approach integrates principles of scaling optimization, structural pruning, and compressed representations, offering a unified framework for model efficiency."
    },
    {
      "footnote_id": "fn-rl-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2468,
      "context": "[^fn-rl-nas]: **Reinforcement Learning NAS**: Uses RL controller networks to generate architectures, with accura...",
      "full_line": "[^fn-rl-nas]: **Reinforcement Learning NAS**: Uses RL controller networks to generate architectures, with accuracy as reward signal. Google's NASNet controller was trained for 22,400 GPU-hours on 800 GPUs, but discovered architectures achieving 82.7% ImageNet accuracy\u201428% better than human-designed ResNet at similar FLOP budgets."
    },
    {
      "footnote_id": "fn-evolutionary-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2470,
      "context": "[^fn-evolutionary-nas]: **Evolutionary NAS**: Treats architectures as genes, evolving populations through mutation and cro...",
      "full_line": "[^fn-evolutionary-nas]: **Evolutionary NAS**: Treats architectures as genes, evolving populations through mutation and crossover. AmoebaNet required 3,150 GPU-days but achieved 83.9% ImageNet accuracy. Real et al.'s evolution approach discovered architectures that matched manually tuned models with 7,000x less human effort."
    },
    {
      "footnote_id": "fn-nas-evaluation-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2512,
      "context": "...trics include computational complexity, memory consumption, inference latency, and energy efficiency[^fn-nas-evaluation-metrics]. Computational complexity, often measured in FLOPs, determines the overall resource demands of a mo...",
      "full_line": "The primary evaluation metrics include computational complexity, memory consumption, inference latency, and energy efficiency[^fn-nas-evaluation-metrics]. Computational complexity, often measured in FLOPs, determines the overall resource demands of a model. NAS favors architectures that achieve high accuracy while reducing unnecessary computations. Memory consumption, which includes both parameter count and activation storage, ensures that models fit within hardware constraints. For real-time applications, inference latency is a key factor, with NAS selecting architectures that minimize execution time on specific hardware platforms. Finally, some NAS implementations explicitly optimize for power consumption, ensuring that models are suitable for mobile and edge devices."
    },
    {
      "footnote_id": "fn-nas-evaluation-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2514,
      "context": "[^fn-nas-evaluation-metrics]: **NAS Evaluation Metrics**: Multi-objective optimization considers accuracy (top-1/top-5), latency...",
      "full_line": "[^fn-nas-evaluation-metrics]: **NAS Evaluation Metrics**: Multi-objective optimization considers accuracy (top-1/top-5), latency (ms on target hardware), memory (MB activations + parameters), and energy (mJ per inference). Pareto-optimal architectures provide 15-40% better efficiency frontiers than manual designs."
    },
    {
      "footnote_id": "fn-fbnet-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2516,
      "context": "For example, FBNet[^fn-fbnet-nas], a NAS-generated architecture optimized for mobile inference, incorporated latency constraints into...",
      "full_line": "For example, FBNet[^fn-fbnet-nas], a NAS-generated architecture optimized for mobile inference, incorporated latency constraints into the search process."
    },
    {
      "footnote_id": "fn-fbnet-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2518,
      "context": "[^fn-fbnet-nas]: **FBNet**: Achieves 74.9% ImageNet accuracy with 375M FLOPs and 23ms latency on Samsung S8, 15% fa...",
      "full_line": "[^fn-fbnet-nas]: **FBNet**: Achieves 74.9% ImageNet accuracy with 375M FLOPs and 23ms latency on Samsung S8, 15% faster than MobileNetV2 with comparable accuracy. The latency-aware search uses device-specific lookup tables for actual hardware performance measurement. Instead of selecting the most accurate model, NAS identified architectures that provided the best balance between accuracy and inference speed [@wu2019fbnet]. Similarly, EfficientNet was discovered through NAS by jointly optimizing for accuracy and computational efficiency, resulting in a model that delivers state-of-the-art performance while reducing FLOPs compared to conventional architectures [@tan2019efficientnet]."
    },
    {
      "footnote_id": "fn-energy-efficiency-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2652,
      "context": "...erarchy, where an instruction's total energy can be significantly impacted by memory access patterns[^fn-energy-efficiency-metrics].",
      "full_line": "Beyond direct compute savings, reducing numerical precision has a significant impact on memory energy consumption, which often dominates total system power. Lower-precision representations reduce data storage requirements and memory bandwidth usage, leading to fewer and more efficient memory accesses. This is important because accessing memory, particularly off-chip DRAM, is far more energy-intensive than performing arithmetic operations. For instance, DRAM accesses require orders of magnitude more energy (1.3\u20132.6 nJ) compared to cache accesses (e.g., 10 pJ for an 8 KB L1 cache access). The breakdown of instruction energy further underscores the cost of moving data within the memory hierarchy, where an instruction's total energy can be significantly impacted by memory access patterns[^fn-energy-efficiency-metrics]."
    },
    {
      "footnote_id": "fn-energy-efficiency-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2654,
      "context": "[^fn-energy-efficiency-metrics]: **Energy Efficiency Metrics**: INT8 quantization reduces energy consumption by 4-8x over FP32. Mob...",
      "full_line": "[^fn-energy-efficiency-metrics]: **Energy Efficiency Metrics**: INT8 quantization reduces energy consumption by 4-8x over FP32. MobileNetV2 INT8 consumes 47mJ vs. 312mJ FP32 per inference on Cortex-A75. ResNet-50 on TPU v4 achieves 0.9 TOPS/Watt vs. 0.3 TOPS/Watt on V100 GPU."
    },
    {
      "footnote_id": "fn-qat-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3599,
      "context": "A primary advantage of QAT[^fn-qat-performance] is its ability to maintain model accuracy, even under low-precision inference conditions. Incorpora...",
      "full_line": "A primary advantage of QAT[^fn-qat-performance] is its ability to maintain model accuracy, even under low-precision inference conditions. Incorporating quantization during training helps the model to compensate for precision loss, reducing the impact of rounding errors and numerical instability. This is important for quantization-sensitive models commonly used in NLP, speech recognition, and high-resolution computer vision [@gholami2021survey]."
    },
    {
      "footnote_id": "fn-qat-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3601,
      "context": "[^fn-qat-performance]: **Quantization-Aware Training**: QAT enables INT8 inference with minimal accuracy loss - ResNet-50...",
      "full_line": "[^fn-qat-performance]: **Quantization-Aware Training**: QAT enables INT8 inference with minimal accuracy loss - ResNet-50 maintains 76.1% vs. 76.2% FP32 ImageNet accuracy, while MobileNetV2 achieves 71.8% vs. 72.0%. BERT-Base INT8 retains 99.1% of FP32 performance on GLUE, compared to 96.8% with post-training quantization alone."
    },
    {
      "footnote_id": "fn-memory-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3976,
      "context": "Memory optimization[^fn-memory-optimization] is a core aspect of model efficiency, especially when deploying machine learning models on resource...",
      "full_line": "Memory optimization[^fn-memory-optimization] is a core aspect of model efficiency, especially when deploying machine learning models on resource-constrained hardware, such as mobile devices, embedded systems, and edge AI platforms. Inference-based models require memory to store activations, intermediate feature maps, and parameters. If these memory demands exceed the hardware's available resources, the model can experience performance bottlenecks, including increased inference latency and power inefficiencies due to frequent memory accesses. Efficient memory management is important to minimize these issues while maintaining accuracy and performance."
    },
    {
      "footnote_id": "fn-memory-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3978,
      "context": "[^fn-memory-optimization]: **Memory Optimization**: DenseNet-121 reduces memory consumption by 50% compared to ResNet-50 thro...",
      "full_line": "[^fn-memory-optimization]: **Memory Optimization**: DenseNet-121 reduces memory consumption by 50% compared to ResNet-50 through feature reuse, requiring only 7.9MB vs. 15.3MB activation memory on ImageNet. MobileNetV3 achieves 73% memory reduction with depth-wise separable convolutions, enabling deployment on 2GB mobile devices."
    },
    {
      "footnote_id": "fn-sparse-energy-savings",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5680,
      "context": "...reduction in the number of computations and memory accesses directly translates into energy savings[^fn-sparse-energy-savings]. Sparse operations require fewer arithmetic operations and fewer memory fetches, leading to a decre...",
      "full_line": "The reduction in the number of computations and memory accesses directly translates into energy savings[^fn-sparse-energy-savings]. Sparse operations require fewer arithmetic operations and fewer memory fetches, leading to a decrease in the energy consumption required for both training and inference. This energy efficiency is particularly important for applications that run on edge devices, where power constraints are important, as explored in @sec-ondevice-learning."
    },
    {
      "footnote_id": "fn-sparse-energy-savings",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5682,
      "context": "[^fn-sparse-energy-savings]: **Sparse Energy Savings**: 90% sparsity in BERT reduces training energy by 2.3x and inference ener...",
      "full_line": "[^fn-sparse-energy-savings]: **Sparse Energy Savings**: 90% sparsity in BERT reduces training energy by 2.3x and inference energy by 4.1x on V100. Structured 2:4 sparsity patterns deliver 1.6x energy savings on A100 GPUs while maintaining 99% of dense model accuracy. Hardware accelerators like TPUs and GPUs are optimized to handle these operations efficiently, making sparse networks not only faster but also more energy-efficient [@Cheng2022]."
    },
    {
      "footnote_id": "fn-hyperparameter-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5814,
      "context": "Beyond architecture design, AutoML also focuses on hyperparameter optimization[^fn-hyperparameter-optimization], which plays a important role in determining a model's performance. Parameters such as learning rat...",
      "full_line": "Beyond architecture design, AutoML also focuses on hyperparameter optimization[^fn-hyperparameter-optimization], which plays a important role in determining a model's performance. Parameters such as learning rate, batch size[^fn-batch-size-effects], weight decay, and activation functions must be carefully tuned for stability and efficiency."
    },
    {
      "footnote_id": "fn-batch-size-effects",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5814,
      "context": "...a important role in determining a model's performance. Parameters such as learning rate, batch size[^fn-batch-size-effects], weight decay, and activation functions must be carefully tuned for stability and efficiency.",
      "full_line": "Beyond architecture design, AutoML also focuses on hyperparameter optimization[^fn-hyperparameter-optimization], which plays a important role in determining a model's performance. Parameters such as learning rate, batch size[^fn-batch-size-effects], weight decay, and activation functions must be carefully tuned for stability and efficiency."
    },
    {
      "footnote_id": "fn-hyperparameter-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5816,
      "context": "[^fn-hyperparameter-optimization]: **Hyperparameter Optimization**: Learning rate search alone can improve model accuracy by 5-15%. G...",
      "full_line": "[^fn-hyperparameter-optimization]: **Hyperparameter Optimization**: Learning rate search alone can improve model accuracy by 5-15%. Grid search over 4 hyperparameters with 10 values each requires 10,000 training runs. Bayesian optimization reduces this to 100-200 runs while achieving comparable results, saving weeks of computation."
    },
    {
      "footnote_id": "fn-batch-size-effects",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5818,
      "context": "[^fn-batch-size-effects]: **Batch Size Effects**: Large batches (512-2048) improve throughput by 2-4x but require gradient a...",
      "full_line": "[^fn-batch-size-effects]: **Batch Size Effects**: Large batches (512-2048) improve throughput by 2-4x but require gradient accumulation for memory constraints. Linear scaling rule maintains convergence: learning rate scales linearly with batch size, enabling ImageNet training in 1 hour with batch size 8192. : **AutoML (Automated Machine Learning)**: End-to-end automation of ML pipeline including data preprocessing, feature selection, model selection, and hyperparameter tuning. Google's AutoML achieved 82.2% ImageNet accuracy vs. 80.1% for human experts, while requiring 50,000x less computational resources than manual NAS."
    },
    {
      "footnote_id": "fn-bayesian-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5820,
      "context": "...al and error, AutoML frameworks employ structured search strategies, including Bayesian optimization[^fn-bayesian-optimization], evolutionary algorithms, and adaptive heuristics, to efficiently identify the best hyperparameter...",
      "full_line": "Instead of relying on trial and error, AutoML frameworks employ structured search strategies, including Bayesian optimization[^fn-bayesian-optimization], evolutionary algorithms, and adaptive heuristics, to efficiently identify the best hyperparameter settings for a given model and dataset [@Bergstra2011]."
    },
    {
      "footnote_id": "fn-bayesian-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5822,
      "context": "[^fn-bayesian-optimization]: **Bayesian Optimization**: Uses probabilistic models to guide hyperparameter search, modeling obje...",
      "full_line": "[^fn-bayesian-optimization]: **Bayesian Optimization**: Uses probabilistic models to guide hyperparameter search, modeling objective function uncertainty. Requires 10-50x fewer evaluations than random search. GPyOpt and Optuna frameworks enable practical Bayesian optimization for neural networks with multi-objective constraints."
    },
    {
      "footnote_id": "fn-xla-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5952,
      "context": "For model representation optimizations like pruning, libraries such as TensorRT, XLA[^fn-xla-compiler], and OpenVINO provide sparsity-aware acceleration through optimized kernels that efficiently handle...",
      "full_line": "For model representation optimizations like pruning, libraries such as TensorRT, XLA[^fn-xla-compiler], and OpenVINO provide sparsity-aware acceleration through optimized kernels that efficiently handle sparse computations. TensorRT specifically supports structured sparsity patterns, allowing models trained with techniques like two-out-of-four structured pruning to run efficiently on NVIDIA GPUs. Similarly, TPUs leverage XLA's sparse matrix optimizations, while FPGAs enable custom sparse execution through frameworks like Vitis AI."
    },
    {
      "footnote_id": "fn-xla-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5954,
      "context": "[^fn-xla-compiler]: **XLA (Accelerated Linear Algebra)**: Google's domain-specific compiler achieves 1.15-1.4x speedup...",
      "full_line": "[^fn-xla-compiler]: **XLA (Accelerated Linear Algebra)**: Google's domain-specific compiler achieves 1.15-1.4x speedup on ResNet-50 inference and 1.2-1.7x on BERT-Large training through operator fusion and memory optimization. XLA reduces HBM traffic by 25-40% through aggressive kernel fusion, delivering 15-30% energy savings on TPUs."
    },
    {
      "footnote_id": "fn-tvm-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5956,
      "context": "...constraints. For models discovered through neural architecture search (NAS), frameworks such as TVM[^fn-tvm-compiler] and TIMM provide compiler support to tune the architectures for various hardware backends.",
      "full_line": "Knowledge distillation benefits from hardware-aware optimizations that help compact student models achieve high inference efficiency. Libraries like TensorRT, OpenVINO, and SNPE optimize distilled models for low-power execution, often combining distillation with quantization or architectural restructuring to meet hardware constraints. For models discovered through neural architecture search (NAS), frameworks such as TVM[^fn-tvm-compiler] and TIMM provide compiler support to tune the architectures for various hardware backends."
    },
    {
      "footnote_id": "fn-tvm-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5958,
      "context": "[^fn-tvm-compiler]: **TVM (Tensor Virtual Machine)**: Apache TVM's auto-tuning delivers 1.2-2.8x speedup over vendor l...",
      "full_line": "[^fn-tvm-compiler]: **TVM (Tensor Virtual Machine)**: Apache TVM's auto-tuning delivers 1.2-2.8x speedup over vendor libraries on ARM CPUs and 1.5-3.2x on mobile GPUs. TVM's graph-level optimizations reduce inference latency by 40-60% on edge devices through operator scheduling and memory planning."
    },
    {
      "footnote_id": "fn-network-protocols",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 44,
      "context": "...nteraction. While traditional system security focuses on software vulnerabilities, network protocols[^fn-network-protocols], and hardware defenses, machine learning systems introduce new attack surfaces. The ML system archi...",
      "full_line": "Machine learning systems, like all computational systems, must be designed not only for performance and accuracy but also for security and privacy. These concerns shape the architecture and operation of ML systems across their lifecycle, from data collection and model training to deployment and user interaction. While traditional system security focuses on software vulnerabilities, network protocols[^fn-network-protocols], and hardware defenses, machine learning systems introduce new attack surfaces. The ML system architectures explored in @sec-ml-systems create new entry points for attackers, especially in distributed deployments spanning cloud, edge, and embedded environments. These include threats to the data that fuels learning, the models that encode behavior, and the infrastructure that serves predictions."
    },
    {
      "footnote_id": "fn-network-protocols",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 46,
      "context": "[^fn-network-protocols]: **Network Protocols**: Standardized communication rules like TCP/IP (1974), HTTP (1991), and TLS (...",
      "full_line": "[^fn-network-protocols]: **Network Protocols**: Standardized communication rules like TCP/IP (1974), HTTP (1991), and TLS (1999) that enable secure data exchange across networks. Modern ML systems rely on protocols like gRPC (2015) for high-performance model serving, handling millions of inference requests per second."
    },
    {
      "footnote_id": "fn-model-extraction-2016",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 54,
      "context": "...challenges in ML systems continue to evolve. High-profile incidents such as model extraction attacks[^fn-model-extraction-2016], data leakage from generative models, and hardware-level vulnerabilities have demonstrated the need...",
      "full_line": "Security and privacy challenges in ML systems continue to evolve. High-profile incidents such as model extraction attacks[^fn-model-extraction-2016], data leakage from generative models, and hardware-level vulnerabilities have demonstrated the need for robust and adaptive defenses."
    },
    {
      "footnote_id": "fn-model-extraction-2016",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 56,
      "context": "[^fn-model-extraction-2016]: **Model Extraction Threat**: The vulnerability was first demonstrated in 2016 when researchers sho...",
      "full_line": "[^fn-model-extraction-2016]: **Model Extraction Threat**: The vulnerability was first demonstrated in 2016 when researchers showed they could steal machine learning models through API queries alone. By systematically querying a model and analyzing responses, attackers could recreate proprietary models worth millions in R&D investment, turning public APIs into inadvertent IP leakage channels."
    },
    {
      "footnote_id": "fn-gdpr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 103,
      "context": "...tion     | Emphasized in cybersecurity standards       | Central to data protection laws (e.g., GDPR[^fn-gdpr])   |",
      "full_line": "| Relevance to Regulation     | Emphasized in cybersecurity standards       | Central to data protection laws (e.g., GDPR[^fn-gdpr])   |"
    },
    {
      "footnote_id": "fn-gdpr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 106,
      "context": "[^fn-gdpr]: **General Data Protection Regulation (GDPR)**: Enacted by the EU in 2018, GDPR imposes fines up to...",
      "full_line": "[^fn-gdpr]: **General Data Protection Regulation (GDPR)**: Enacted by the EU in 2018, GDPR imposes fines up to 4% of global revenue (\u20ac20+ million) for privacy violations. Since enforcement began, over \u20ac4.5 billion in fines have been levied, including \u20ac746 million against Amazon in 2021, driving massive investment in privacy-preserving ML technologies."
    },
    {
      "footnote_id": "fn-dp-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 114,
      "context": "However, they can also be in tension. Techniques like differential privacy[^fn-dp-origins] reduce memorization risks but may lower model utility.",
      "full_line": "However, they can also be in tension. Techniques like differential privacy[^fn-dp-origins] reduce memorization risks but may lower model utility."
    },
    {
      "footnote_id": "fn-dp-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 118,
      "context": "[^fn-dp-origins]: **Differential Privacy Origins**: Cynthia Dwork coined the term at Microsoft Research in 2006, but...",
      "full_line": "[^fn-dp-origins]: **Differential Privacy Origins**: Cynthia Dwork coined the term at Microsoft Research in 2006, but the concept emerged from her frustration with the \"anonymization myth\"\u2014the false belief that removing names from data guaranteed privacy. Her groundbreaking insight was that privacy should be mathematically provable, not just plausible, leading to the rigorous framework that now protects billions of users' data in products from Apple to Google."
    },
    {
      "footnote_id": "fn-stuxnet-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 126,
      "context": "...ps://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[^fn-stuxnet-discovery], which targeted industrial control systems used in Iran's Natanz nuclear facility [@farwell2011stux...",
      "full_line": "In 2010, security researchers discovered a highly sophisticated computer worm later named [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[^fn-stuxnet-discovery], which targeted industrial control systems used in Iran's Natanz nuclear facility [@farwell2011stuxnet]. Stuxnet exploited four previously unknown \"[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)\"[^fn-zero-day-term] vulnerabilities in Microsoft Windows, allowing it to spread undetected through both networked and isolated systems."
    },
    {
      "footnote_id": "fn-zero-day-term",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 126,
      "context": "...ploited four previously unknown \"[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)\"[^fn-zero-day-term] vulnerabilities in Microsoft Windows, allowing it to spread undetected through both networked and i...",
      "full_line": "In 2010, security researchers discovered a highly sophisticated computer worm later named [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[^fn-stuxnet-discovery], which targeted industrial control systems used in Iran's Natanz nuclear facility [@farwell2011stuxnet]. Stuxnet exploited four previously unknown \"[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)\"[^fn-zero-day-term] vulnerabilities in Microsoft Windows, allowing it to spread undetected through both networked and isolated systems."
    },
    {
      "footnote_id": "fn-stuxnet-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 128,
      "context": "[^fn-stuxnet-discovery]: **Stuxnet Discovery**: The malware was first detected by VirusBokNok, a small Belarusian antivirus...",
      "full_line": "[^fn-stuxnet-discovery]: **Stuxnet Discovery**: The malware was first detected by VirusBokNok, a small Belarusian antivirus company, when their client computers began crashing unexpectedly. What seemed like a routine malware investigation turned into one of the most significant cybersecurity discoveries in history\u2014the first confirmed cyberweapon designed to cause physical destruction."
    },
    {
      "footnote_id": "fn-zero-day-term",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 130,
      "context": "[^fn-zero-day-term]: **Zero-Day Etymology**: The term originated in software piracy circles, referring to the \"zero day...",
      "full_line": "[^fn-zero-day-term]: **Zero-Day Etymology**: The term originated in software piracy circles, referring to the \"zero days\" since a program's release when pirated copies appeared. In security, it describes the \"zero days\" defenders have to patch a vulnerability before attackers exploit it\u2014representing the ultimate race between attack and defense."
    },
    {
      "footnote_id": "fn-air-gapped",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 132,
      "context": "...enrichment by sabotaging the centrifuges used in the process. Despite the facility being air-gapped[^fn-air-gapped] from external networks, the malware is believed to have entered the system via an infected USB devi...",
      "full_line": "Unlike typical malware designed to steal information or perform espionage, Stuxnet was engineered to cause physical damage. Its objective was to disrupt uranium enrichment by sabotaging the centrifuges used in the process. Despite the facility being air-gapped[^fn-air-gapped] from external networks, the malware is believed to have entered the system via an infected USB device[^fn-usb-attacks], demonstrating how physical access can compromise even isolated environments."
    },
    {
      "footnote_id": "fn-usb-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 132,
      "context": "...rom external networks, the malware is believed to have entered the system via an infected USB device[^fn-usb-attacks], demonstrating how physical access can compromise even isolated environments.",
      "full_line": "Unlike typical malware designed to steal information or perform espionage, Stuxnet was engineered to cause physical damage. Its objective was to disrupt uranium enrichment by sabotaging the centrifuges used in the process. Despite the facility being air-gapped[^fn-air-gapped] from external networks, the malware is believed to have entered the system via an infected USB device[^fn-usb-attacks], demonstrating how physical access can compromise even isolated environments."
    },
    {
      "footnote_id": "fn-air-gapped",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 136,
      "context": "[^fn-air-gapped]: **Air-Gapped Systems**: Networks physically isolated from external connections, originally develop...",
      "full_line": "[^fn-air-gapped]: **Air-Gapped Systems**: Networks physically isolated from external connections, originally developed for military systems in the 1960s. Despite seeming impenetrable, studies show 90% of air-gapped systems can be breached through techniques like acoustic, electromagnetic, or thermal convert channels."
    },
    {
      "footnote_id": "fn-usb-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 138,
      "context": "[^fn-usb-attacks]: **USB Attack Vectors**: USB interfaces, introduced in 1996, became a primary attack vector for cro...",
      "full_line": "[^fn-usb-attacks]: **USB Attack Vectors**: USB interfaces, introduced in 1996, became a primary attack vector for crossing air gaps. The 2008 Operation Olympic Games reportedly used infected USB drives to penetrate secure facilities, with some estimates suggesting 60% of organizations remain vulnerable to USB-based attacks. It specifically targeted programmable logic controllers (PLCs), industrial computers that automate electromechanical processes such as controlling the speed of centrifuges. By exploiting vulnerabilities in the Windows operating system and the Siemens Step7 software used to program the PLCs, Stuxnet achieved highly targeted, real-world disruption."
    },
    {
      "footnote_id": "fn-automotive-recalls",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 152,
      "context": "The incident also led to a recall of over 1.4 million vehicles to patch the vulnerability[^fn-automotive-recalls], highlighting the need for manufacturers to prioritize cybersecurity in their designs. The National...",
      "full_line": "The incident also led to a recall of over 1.4 million vehicles to patch the vulnerability[^fn-automotive-recalls], highlighting the need for manufacturers to prioritize cybersecurity in their designs. The National Highway Traffic Safety Administration (NHTSA)[^fn-nhtsa] issued guidelines for automakers to improve vehicle cybersecurity, including recommendations for secure software development practices and incident response protocols."
    },
    {
      "footnote_id": "fn-nhtsa",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 152,
      "context": "...rioritize cybersecurity in their designs. The National Highway Traffic Safety Administration (NHTSA)[^fn-nhtsa] issued guidelines for automakers to improve vehicle cybersecurity, including recommendations for se...",
      "full_line": "The incident also led to a recall of over 1.4 million vehicles to patch the vulnerability[^fn-automotive-recalls], highlighting the need for manufacturers to prioritize cybersecurity in their designs. The National Highway Traffic Safety Administration (NHTSA)[^fn-nhtsa] issued guidelines for automakers to improve vehicle cybersecurity, including recommendations for secure software development practices and incident response protocols."
    },
    {
      "footnote_id": "fn-automotive-recalls",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 154,
      "context": "[^fn-automotive-recalls]: **Automotive Cybersecurity Recalls**: The Jeep Cherokee hack triggered the first-ever automotive c...",
      "full_line": "[^fn-automotive-recalls]: **Automotive Cybersecurity Recalls**: The Jeep Cherokee hack triggered the first-ever automotive cybersecurity recall in 2015. Since then, cybersecurity recalls have affected over 15 million vehicles globally, costing manufacturers an estimated $2.4 billion in remediation efforts and spurring new regulations."
    },
    {
      "footnote_id": "fn-nhtsa",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 156,
      "context": "[^fn-nhtsa]: **NHTSA Cybersecurity Guidelines**: Established in 1970, NHTSA issued its first cybersecurity guid...",
      "full_line": "[^fn-nhtsa]: **NHTSA Cybersecurity Guidelines**: Established in 1970, NHTSA issued its first cybersecurity guidance in 2016 following the Jeep hack. The agency now mandates that connected vehicles include cybersecurity by design, affecting 99% of new vehicles sold in the US that contain 100+ onboard computers."
    },
    {
      "footnote_id": "fn-mirai-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 166,
      "context": "In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[^fn-mirai-scale] emerged as one of the most disruptive distributed denial-of-service (DDoS)[^fn-ddos-attacks] attack...",
      "full_line": "In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[^fn-mirai-scale] emerged as one of the most disruptive distributed denial-of-service (DDoS)[^fn-ddos-attacks] attacks in internet history [@antonakakis2017understanding]. The botnet infected thousands of networked devices, including digital cameras, DVRs, and other consumer electronics."
    },
    {
      "footnote_id": "fn-ddos-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 166,
      "context": "...attack/)[^fn-mirai-scale] emerged as one of the most disruptive distributed denial-of-service (DDoS)[^fn-ddos-attacks] attacks in internet history [@antonakakis2017understanding]. The botnet infected thousands of netwo...",
      "full_line": "In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[^fn-mirai-scale] emerged as one of the most disruptive distributed denial-of-service (DDoS)[^fn-ddos-attacks] attacks in internet history [@antonakakis2017understanding]. The botnet infected thousands of networked devices, including digital cameras, DVRs, and other consumer electronics."
    },
    {
      "footnote_id": "fn-ddos-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 168,
      "context": "[^fn-ddos-attacks]: **DDoS Attacks**: Distributed Denial-of-Service attacks overwhelm targets with traffic from multip...",
      "full_line": "[^fn-ddos-attacks]: **DDoS Attacks**: Distributed Denial-of-Service attacks overwhelm targets with traffic from multiple sources, first demonstrated in 1999. Modern DDoS attacks can exceed 2.3 Tbps (terabits per second), enough to take down entire internet infrastructures and costing businesses $2.3 million per incident on average. These devices, often deployed with factory-default usernames and passwords, were easily compromised by the Mirai malware and enlisted into a large-scale attack network."
    },
    {
      "footnote_id": "fn-mirai-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 170,
      "context": "[^fn-mirai-scale]: **Mirai Botnet Scale**: At its peak, Mirai controlled over 600,000 infected IoT devices, generatin...",
      "full_line": "[^fn-mirai-scale]: **Mirai Botnet Scale**: At its peak, Mirai controlled over 600,000 infected IoT devices, generating 623 Gbps DDoS attacks that took down major internet services including Twitter, Netflix, and Reddit for hours. The attack revealed that IoT devices with default credentials (admin/admin, root/12345) could be weaponized at unprecedented scale."
    },
    {
      "footnote_id": "fn-arm-processors",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 190,
      "context": "...wearable health monitors, typically rely on lightweight embedded hardware like ARM-based processors[^fn-arm-processors] running minimal operating systems. These systems are designed for low-power, distributed operation...",
      "full_line": "Edge ML devices, including smart cameras, industrial controllers, and wearable health monitors, typically rely on lightweight embedded hardware like ARM-based processors[^fn-arm-processors] running minimal operating systems. These systems are designed for low-power, distributed operation but often lack the robust security features found in larger computing platforms."
    },
    {
      "footnote_id": "fn-arm-processors",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 192,
      "context": "[^fn-arm-processors]: **ARM Processors**: Originally developed in 1985 for personal computers, ARM processors now power...",
      "full_line": "[^fn-arm-processors]: **ARM Processors**: Originally developed in 1985 for personal computers, ARM processors now power 95% of smartphones and 70% of IoT devices. Their low power consumption (0.1-2 watts vs 15-150 watts for x86) makes them ideal for edge ML, but their simplified architectures often omit advanced security features. As these devices take on more responsibility for local data processing and real-time decision-making, they become attractive targets for remote compromise."
    },
    {
      "footnote_id": "fn-data-poisoning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 222,
      "context": "...ts to model confidentiality (e.g., model theft), threats to training integrity (e.g., data poisoning[^fn-data-poisoning]), and threats to inference robustness (e.g., adversarial examples[^fn-adversarial-examples]). Each...",
      "full_line": "The architectural vulnerabilities exposed in these historical incidents\u2014network connectivity, supply chain compromise, and physical access\u2014directly affect ML systems with additional complexity from model-specific attack vectors. We now examine threats that specifically target machine learning models. These threats span the entire ML lifecycle, ranging from training-time manipulations to inference-time evasion, and fall into three broad categories: threats to model confidentiality (e.g., model theft), threats to training integrity (e.g., data poisoning[^fn-data-poisoning]), and threats to inference robustness (e.g., adversarial examples[^fn-adversarial-examples]). Each category targets different vulnerabilities and requires distinct defensive strategies."
    },
    {
      "footnote_id": "fn-adversarial-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 222,
      "context": "...data poisoning[^fn-data-poisoning]), and threats to inference robustness (e.g., adversarial examples[^fn-adversarial-examples]). Each category targets different vulnerabilities and requires distinct defensive strategies.",
      "full_line": "The architectural vulnerabilities exposed in these historical incidents\u2014network connectivity, supply chain compromise, and physical access\u2014directly affect ML systems with additional complexity from model-specific attack vectors. We now examine threats that specifically target machine learning models. These threats span the entire ML lifecycle, ranging from training-time manipulations to inference-time evasion, and fall into three broad categories: threats to model confidentiality (e.g., model theft), threats to training integrity (e.g., data poisoning[^fn-data-poisoning]), and threats to inference robustness (e.g., adversarial examples[^fn-adversarial-examples]). Each category targets different vulnerabilities and requires distinct defensive strategies."
    },
    {
      "footnote_id": "fn-data-poisoning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 224,
      "context": "[^fn-data-poisoning]: **Data Poisoning**: Attack technique where adversaries inject malicious data during training, firs...",
      "full_line": "[^fn-data-poisoning]: **Data Poisoning**: Attack technique where adversaries inject malicious data during training, first formalized in 2012 [@biggio2012poisoning]. Studies show that poisoning just 0.1% of training data can reduce model accuracy by 10-50%, making it a highly efficient attack vector against ML systems."
    },
    {
      "footnote_id": "fn-adversarial-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 226,
      "context": "[^fn-adversarial-examples]: **Adversarial Examples**: Inputs crafted to deceive ML models, discovered by Szegedy et al. [@szeg...",
      "full_line": "[^fn-adversarial-examples]: **Adversarial Examples**: Inputs crafted to deceive ML models, discovered by Szegedy et al. [@szegedy2014intriguing]. These attacks can fool state-of-the-art image classifiers with perturbations invisible to humans (changing <0.01% of pixel values), affecting 99%+ of deep learning models."
    },
    {
      "footnote_id": "fn-phishing-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 285,
      "context": "...may be adapted to automate tasks such as adversarial example generation, phishing content synthesis[^fn-phishing-ai], or protocol subversion. Furthermore, open-source or publicly accessible models can be fine-tuned f...",
      "full_line": "Machine learning models are not solely passive targets of attack; in some cases, they can themselves be employed as components of an attack strategy. Pretrained models, particularly large generative or discriminative networks, may be adapted to automate tasks such as adversarial example generation, phishing content synthesis[^fn-phishing-ai], or protocol subversion. Furthermore, open-source or publicly accessible models can be fine-tuned for malicious purposes, including impersonation, surveillance, or reverse-engineering of secure systems."
    },
    {
      "footnote_id": "fn-phishing-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 287,
      "context": "[^fn-phishing-ai]: **AI-Generated Phishing**: Large language models can generate convincing phishing emails with 99%+...",
      "full_line": "[^fn-phishing-ai]: **AI-Generated Phishing**: Large language models can generate convincing phishing emails with 99%+ grammatical accuracy, compared to 19% for traditional phishing. Security firms report a 1,265% increase in AI-generated phishing attacks since 2022, with some campaigns achieving 30%+ success rates. This dual-use potential necessitates a broader security perspective\u2014one that considers models not only as assets to defend but also as possible instruments of attack."
    },
    {
      "footnote_id": "fn-ml-apis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 293,
      "context": "Such threats arise across a range of deployment settings, including public APIs[^fn-ml-apis], cloud-hosted services, on-device inference engines, and shared model repositories[^fn-model-reposi...",
      "full_line": "Such threats arise across a range of deployment settings, including public APIs[^fn-ml-apis], cloud-hosted services, on-device inference engines, and shared model repositories[^fn-model-repositories]. Machine learning models may be vulnerable due to exposed interfaces, insecure serialization formats[^fn-model-serialization], or insufficient access controls\u2014factors that create opportunities for unauthorized extraction or replication [@ateniese2015hacking]."
    },
    {
      "footnote_id": "fn-model-repositories",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 293,
      "context": "...APIs[^fn-ml-apis], cloud-hosted services, on-device inference engines, and shared model repositories[^fn-model-repositories]. Machine learning models may be vulnerable due to exposed interfaces, insecure serialization format...",
      "full_line": "Such threats arise across a range of deployment settings, including public APIs[^fn-ml-apis], cloud-hosted services, on-device inference engines, and shared model repositories[^fn-model-repositories]. Machine learning models may be vulnerable due to exposed interfaces, insecure serialization formats[^fn-model-serialization], or insufficient access controls\u2014factors that create opportunities for unauthorized extraction or replication [@ateniese2015hacking]."
    },
    {
      "footnote_id": "fn-model-serialization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 293,
      "context": "...Machine learning models may be vulnerable due to exposed interfaces, insecure serialization formats[^fn-model-serialization], or insufficient access controls\u2014factors that create opportunities for unauthorized extraction or r...",
      "full_line": "Such threats arise across a range of deployment settings, including public APIs[^fn-ml-apis], cloud-hosted services, on-device inference engines, and shared model repositories[^fn-model-repositories]. Machine learning models may be vulnerable due to exposed interfaces, insecure serialization formats[^fn-model-serialization], or insufficient access controls\u2014factors that create opportunities for unauthorized extraction or replication [@ateniese2015hacking]."
    },
    {
      "footnote_id": "fn-ml-apis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 295,
      "context": "[^fn-ml-apis]: **ML APIs**: Application Programming Interfaces for machine learning, popularized by Google's Pred...",
      "full_line": "[^fn-ml-apis]: **ML APIs**: Application Programming Interfaces for machine learning, popularized by Google's Prediction API (2010). Today's ML APIs handle billions of requests daily, with major providers processing billions of tokens monthly, creating vast attack surfaces for model extraction."
    },
    {
      "footnote_id": "fn-model-repositories",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 297,
      "context": "[^fn-model-repositories]: **Model Repositories**: Centralized platforms for sharing ML models, led by Hugging Face (2016) wh...",
      "full_line": "[^fn-model-repositories]: **Model Repositories**: Centralized platforms for sharing ML models, led by Hugging Face (2016) which hosts 500,000+ models. While democratizing AI access, these repositories have become targets for supply chain attacks, with researchers finding malicious models in 5% of popular repositories."
    },
    {
      "footnote_id": "fn-model-serialization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 299,
      "context": "[^fn-model-serialization]: **Model Serialization**: Process of converting trained models into portable formats like ONNX (201...",
      "full_line": "[^fn-model-serialization]: **Model Serialization**: Process of converting trained models into portable formats like ONNX (2017), TensorFlow SavedModel (2016), or PyTorch's .pth files. Insecure serialization can expose model weights and enable arbitrary code execution, affecting 80%+ of deployed ML systems."
    },
    {
      "footnote_id": "fn-model-inversion-attack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 303,
      "context": "...tics, and embedded trade secrets. This knowledge can also be used to conduct model inversion attacks[^fn-model-inversion-attack], where an attacker attempts to infer private details about the model's training data [@fredrikson20...",
      "full_line": "The consequences of model theft extend beyond economic loss. Stolen models can be used to extract sensitive information, replicate proprietary algorithms, or allow further attacks. For instance, a competitor who obtains a stolen recommendation model from an e-commerce platform might gain insights into customer behavior, business analytics, and embedded trade secrets. This knowledge can also be used to conduct model inversion attacks[^fn-model-inversion-attack], where an attacker attempts to infer private details about the model's training data [@fredrikson2015model]."
    },
    {
      "footnote_id": "fn-model-inversion-attack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 305,
      "context": "[^fn-model-inversion-attack]: **Model Inversion Attack**: First demonstrated in 2015 against face recognition systems, researche...",
      "full_line": "[^fn-model-inversion-attack]: **Model Inversion Attack**: First demonstrated in 2015 against face recognition systems, researchers reconstructed recognizable faces from neural network outputs using only confidence scores. The attack revealed that models trained on 40 individuals could leak identifiable facial features, proving that \"black-box\" API access isn't sufficient privacy protection."
    },
    {
      "footnote_id": "fn-netflix-deanonymization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 307,
      "context": "...was trained. Similar vulnerabilities have been demonstrated in studies on the Netflix Prize dataset[^fn-netflix-deanonymization], where researchers were able to infer individual movie preferences from anonymized data [@narayanan...",
      "full_line": "In a model inversion attack, the adversary queries the model through a legitimate interface, such as a public API, and observes its outputs. By analyzing confidence scores or output probabilities, the attacker can optimize inputs to reconstruct data resembling the model's training set. For example, a facial recognition model used for secure access could be manipulated to reveal statistical properties of the employee photos on which it was trained. Similar vulnerabilities have been demonstrated in studies on the Netflix Prize dataset[^fn-netflix-deanonymization], where researchers were able to infer individual movie preferences from anonymized data [@narayanan2006break]."
    },
    {
      "footnote_id": "fn-netflix-deanonymization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 309,
      "context": "[^fn-netflix-deanonymization]: **Netflix Deanonymization**: In 2008, researchers re-identified Netflix users by correlating the \"...",
      "full_line": "[^fn-netflix-deanonymization]: **Netflix Deanonymization**: In 2008, researchers re-identified Netflix users by correlating the \"anonymous\" Prize dataset with public IMDb ratings. Using as few as 8 movie ratings with dates, they identified 99% of users, leading Netflix to cancel a second competition and highlighting the futility of naive anonymization."
    },
    {
      "footnote_id": "fn-ml-side-channel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 374,
      "context": "...hat define the model's behavior. Architecture theft may be accomplished through side-channel attacks[^fn-ml-side-channel], reverse engineering, or analysis of observable model behavior.",
      "full_line": "Finally, attackers may seek to reconstruct the model's architecture. This includes the sequence and types of layers, activation functions, and connectivity patterns that define the model's behavior. Architecture theft may be accomplished through side-channel attacks[^fn-ml-side-channel], reverse engineering, or analysis of observable model behavior."
    },
    {
      "footnote_id": "fn-ml-side-channel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 380,
      "context": "[^fn-ml-side-channel]: **Side-Channel Attacks on ML**: First demonstrated against neural networks in 2018, researchers sh...",
      "full_line": "[^fn-ml-side-channel]: **Side-Channel Attacks on ML**: First demonstrated against neural networks in 2018, researchers showed that power consumption patterns during inference could reveal model architecture, layer sizes, and even distinguish between different model families. This extended traditional cryptographic side-channel attacks into the ML domain, creating new vulnerabilities for edge AI devices."
    },
    {
      "footnote_id": "fn-model-distillation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 386,
      "context": "...their own model to mimic the behavior of the original. This process, often called model distillation[^fn-model-distillation] or knockoff modeling, allows attackers to achieve comparable functionality without access to the or...",
      "full_line": "This type of theft often targets models deployed as services, where the model is exposed through an API or embedded in a user-facing application. By repeatedly querying the model and recording its responses, an attacker can train their own model to mimic the behavior of the original. This process, often called model distillation[^fn-model-distillation] or knockoff modeling, allows attackers to achieve comparable functionality without access to the original model's proprietary internals [@orekondy2019knockoff]."
    },
    {
      "footnote_id": "fn-model-distillation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 388,
      "context": "[^fn-model-distillation]: **Model Distillation**: Knowledge transfer technique developed by Hinton et al. [@hinton2015distil...",
      "full_line": "[^fn-model-distillation]: **Model Distillation**: Knowledge transfer technique developed by Hinton et al. [@hinton2015distilling] where a smaller \"student\" model learns from a larger \"teacher\" model. While designed for model compression, attackers exploit this to create stolen models with 95%+ accuracy using only 1% of the original training data."
    },
    {
      "footnote_id": "fn-crowdsourcing-risks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 435,
      "context": "...train on data collected from external sources, including user interactions, crowdsourced annotations[^fn-crowdsourcing-risks], and online scraping, since attackers can inject poisoned data without direct access to the trainin...",
      "full_line": "Data poisoning is a security threat because it involves intentional manipulation of the training data by an adversary, with the goal of embedding vulnerabilities or subverting model behavior. These attacks pose concern in applications where models retrain on data collected from external sources, including user interactions, crowdsourced annotations[^fn-crowdsourcing-risks], and online scraping, since attackers can inject poisoned data without direct access to the training pipeline."
    },
    {
      "footnote_id": "fn-crowdsourcing-risks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 437,
      "context": "[^fn-crowdsourcing-risks]: **Crowdsourcing Risks**: Platforms like Amazon Mechanical Turk (2005) and Prolific democratized da...",
      "full_line": "[^fn-crowdsourcing-risks]: **Crowdsourcing Risks**: Platforms like Amazon Mechanical Turk (2005) and Prolific democratized data labeling but introduced poisoning risks. Studies show 15-30% of crowdsourced labels contain errors or bias, with coordinated attacks capable of poisoning entire datasets at costs under $1,000. Even in more controlled settings, poisoning may occur through compromised data storage, insider manipulation, or insecure data transfer processes."
    },
    {
      "footnote_id": "fn-backdoor-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 456,
      "context": "...eneral performance intact but causing consistent misclassification in select cases. Backdoor attacks[^fn-backdoor-attacks] embed hidden triggers, which are often imperceptible patterns, that elicit malicious behavior only...",
      "full_line": "Data poisoning attacks can be classified based on their objectives and scope of impact. Availability attacks degrade overall model performance by introducing noise or label flips that reduce accuracy across tasks. Targeted attacks manipulate a specific input or class, leaving general performance intact but causing consistent misclassification in select cases. Backdoor attacks[^fn-backdoor-attacks] embed hidden triggers, which are often imperceptible patterns, that elicit malicious behavior only when the trigger is present. Subpopulation attacks degrade performance on a specific group defined by shared features, making them particularly dangerous in fairness-sensitive applications."
    },
    {
      "footnote_id": "fn-backdoor-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 458,
      "context": "[^fn-backdoor-attacks]: **Backdoor Attacks**: Hidden triggers embedded in ML models during training, first demonstrated in...",
      "full_line": "[^fn-backdoor-attacks]: **Backdoor Attacks**: Hidden triggers embedded in ML models during training, first demonstrated in 2017. These attacks achieve 99%+ success rates while maintaining normal accuracy, with triggers as subtle as single-pixel modifications. BadNets, the seminal backdoor attack, affected 100% of tested models."
    },
    {
      "footnote_id": "fn-perspective-api",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 460,
      "context": "...ng attack was demonstrated against Perspective, Google's widely-used online toxicity detection model[^fn-perspective-api] that helps platforms identify harmful content [@hosseini2017deceiving]. By injecting synthetically...",
      "full_line": "A notable real-world example of a targeted poisoning attack was demonstrated against Perspective, Google's widely-used online toxicity detection model[^fn-perspective-api] that helps platforms identify harmful content [@hosseini2017deceiving]. By injecting synthetically generated toxic comments with subtle misspellings and grammatical errors into the model's training set, researchers degraded its ability to detect harmful content[^fn-perspective-vulnerability]."
    },
    {
      "footnote_id": "fn-perspective-vulnerability",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 460,
      "context": "...cal errors into the model's training set, researchers degraded its ability to detect harmful content[^fn-perspective-vulnerability].",
      "full_line": "A notable real-world example of a targeted poisoning attack was demonstrated against Perspective, Google's widely-used online toxicity detection model[^fn-perspective-api] that helps platforms identify harmful content [@hosseini2017deceiving]. By injecting synthetically generated toxic comments with subtle misspellings and grammatical errors into the model's training set, researchers degraded its ability to detect harmful content[^fn-perspective-vulnerability]."
    },
    {
      "footnote_id": "fn-perspective-api",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 462,
      "context": "[^fn-perspective-api]: **Perspective API**: Google's toxicity detection model launched in 2017, now processing 500+ milli...",
      "full_line": "[^fn-perspective-api]: **Perspective API**: Google's toxicity detection model launched in 2017, now processing 500+ million comments daily across platforms like The New York Times and Wikipedia. Despite sophisticated training, the API demonstrates how even billion-parameter models remain vulnerable to targeted poisoning attacks."
    },
    {
      "footnote_id": "fn-perspective-vulnerability",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 464,
      "context": "[^fn-perspective-vulnerability]: After retraining, the poisoned model exhibited a significantly higher false negative rate, allowin...",
      "full_line": "[^fn-perspective-vulnerability]: After retraining, the poisoned model exhibited a significantly higher false negative rate, allowing offensive language to bypass filters. This case illustrates how poisoned data can exploit feedback loops in systems that rely on user-generated input, leading to reduced effectiveness over time and creating long-term vulnerabilities in content moderation pipelines."
    },
    {
      "footnote_id": "fn-gans-adversarial",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 513,
      "context": "...d to generate adversarial examples. One notable approach uses generative adversarial networks (GANs)[^fn-gans-adversarial] [@goodfellow2020generative]. In this setting, a generator network learns to produce inputs that dec...",
      "full_line": "Several methods have been proposed to generate adversarial examples. One notable approach uses generative adversarial networks (GANs)[^fn-gans-adversarial] [@goodfellow2020generative]. In this setting, a generator network learns to produce inputs that deceive the target model, while a discriminator evaluates their effectiveness."
    },
    {
      "footnote_id": "fn-gans-adversarial",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 515,
      "context": "[^fn-gans-adversarial]: **GANs for Adversarial Attacks**: Generative Adversarial Networks, invented by Ian Goodfellow in 2...",
      "full_line": "[^fn-gans-adversarial]: **GANs for Adversarial Attacks**: Generative Adversarial Networks, invented by Ian Goodfellow in 2014 [@goodfellow2020generative], were quickly adapted for attacking other models. GAN-based adversarial generators can create perturbations that fool target models with 95%+ success rates while remaining imperceptible to humans. This iterative process allows the attacker to generate sophisticated and diverse adversarial examples."
    },
    {
      "footnote_id": "fn-transfer-learning-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 517,
      "context": "Another vector for adversarial attacks involves transfer learning pipelines[^fn-transfer-learning-attacks]. Many production systems reuse pre-trained feature extractors, fine-tuning only the final layers fo...",
      "full_line": "Another vector for adversarial attacks involves transfer learning pipelines[^fn-transfer-learning-attacks]. Many production systems reuse pre-trained feature extractors, fine-tuning only the final layers for specific tasks. Adversaries can exploit this structure by targeting the shared feature extractor, crafting perturbations that affect multiple downstream tasks."
    },
    {
      "footnote_id": "fn-transfer-learning-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 519,
      "context": "[^fn-transfer-learning-attacks]: **Transfer Learning Attacks**: Pre-trained models like ImageNet classifiers (2012) accelerated AI...",
      "full_line": "[^fn-transfer-learning-attacks]: **Transfer Learning Attacks**: Pre-trained models like ImageNet classifiers (2012) accelerated AI development but created new attack vectors. Adversarial perturbations targeting shared feature extractors can simultaneously compromise multiple downstream tasks, amplifying attack impact by 10-100x compared to single-model attacks. Headless attacks, for example, manipulate the feature extractor without requiring access to the classification head or training data [@ahmed2020headless]. This exposes a important vulnerability in systems that rely on pre-trained models."
    },
    {
      "footnote_id": "fn-meltdown-spectre-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 634,
      "context": "...most notable examples came with the discovery of [Meltdown and Spectre](https://meltdownattack.com/)[^fn-meltdown-spectre-impact]\u2014two vulnerabilities in modern processors that allow malicious programs to bypass memory isolation a...",
      "full_line": "Hardware is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit these vulnerabilities to access, manipulate, or extract sensitive data, breaching the confidentiality and integrity that users and services depend on. One of the most notable examples came with the discovery of [Meltdown and Spectre](https://meltdownattack.com/)[^fn-meltdown-spectre-impact]\u2014two vulnerabilities in modern processors that allow malicious programs to bypass memory isolation and read the data of other applications and the operating system [@Lipp2018meltdown; @Kocher2018spectre]."
    },
    {
      "footnote_id": "fn-meltdown-spectre-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 636,
      "context": "[^fn-meltdown-spectre-impact]: **Meltdown/Spectre Impact**: Disclosed in January 2018, these vulnerabilities affected virtually e...",
      "full_line": "[^fn-meltdown-spectre-impact]: **Meltdown/Spectre Impact**: Disclosed in January 2018, these vulnerabilities affected virtually every processor made since 1995\u2014billions of devices. The disclosure triggered emergency patches across all major operating systems, causing 5-30% performance degradation in some workloads, and led to a core rethinking of processor security."
    },
    {
      "footnote_id": "fn-speculative-execution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 638,
      "context": "These attacks exploit speculative execution[^fn-speculative-execution], a performance optimization in CPUs that executes instructions out of order before safety checks ar...",
      "full_line": "These attacks exploit speculative execution[^fn-speculative-execution], a performance optimization in CPUs that executes instructions out of order before safety checks are complete. While improving computational speed, this optimization inadvertently exposes sensitive data through microarchitectural side channels, such as CPU caches. The technical sophistication of these attacks highlights the difficulty of eliminating vulnerabilities even with extensive hardware validation."
    },
    {
      "footnote_id": "fn-speculative-execution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 640,
      "context": "[^fn-speculative-execution]: **Speculative Execution**: Introduced in the Intel Pentium Pro (1995), this technique executes ins...",
      "full_line": "[^fn-speculative-execution]: **Speculative Execution**: Introduced in the Intel Pentium Pro (1995), this technique executes instructions before confirming they're needed, improving performance by 10-25%. However, it creates a 20+ year attack window where speculated operations leak data through cache timing, affecting ML accelerators that rely on similar optimizations."
    },
    {
      "footnote_id": "fn-hipaa-violations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 648,
      "context": "...w.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html)[^fn-hipaa-violations], leading to significant legal and ethical consequences. Similar regulatory risks apply globally, wi...",
      "full_line": "Such vulnerabilities pose concern in privacy-sensitive domains like healthcare, where ML systems routinely handle patient data. A breach could violate privacy regulations such as the [Health Insurance Portability and Accountability Act (HIPAA)](https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html)[^fn-hipaa-violations], leading to significant legal and ethical consequences. Similar regulatory risks apply globally, with GDPR imposing fines up to 4% of global revenue for organizations that fail to implement appropriate technical measures to protect EU citizens' data."
    },
    {
      "footnote_id": "fn-hipaa-violations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 650,
      "context": "[^fn-hipaa-violations]: **HIPAA Violations**: Since enforcement began in 2003, HIPAA has generated over $130 million in fi...",
      "full_line": "[^fn-hipaa-violations]: **HIPAA Violations**: Since enforcement began in 2003, HIPAA has generated over $130 million in fines, with individual penalties reaching $16 million. The largest healthcare data breach affected 78.8 million patients at Anthem Inc. in 2015, highlighting the massive scale of exposure when ML systems handling medical data are compromised."
    },
    {
      "footnote_id": "fn-aes-standard",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 706,
      "context": "One of the most widely studied examples involves Advanced Encryption Standard (AES)[^fn-aes-standard] implementations. While AES is mathematically secure, the physical process of computing its encrypti...",
      "full_line": "One of the most widely studied examples involves Advanced Encryption Standard (AES)[^fn-aes-standard] implementations. While AES is mathematically secure, the physical process of computing its encryption functions leaks measurable signals."
    },
    {
      "footnote_id": "fn-aes-standard",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 708,
      "context": "[^fn-aes-standard]: **Advanced Encryption Standard (AES)**: Adopted by NIST in 2001 as the US government encryption st...",
      "full_line": "[^fn-aes-standard]: **Advanced Encryption Standard (AES)**: Adopted by NIST in 2001 as the US government encryption standard, AES replaced DES after 24 years. Despite being mathematically secure with 2^128 possible keys for AES-128, physical implementations remain vulnerable to side-channel attacks that can extract keys in minutes. Techniques such as Differential Power Analysis (DPA), Differential Electromagnetic Analysis (DEMA), and Correlation Power Analysis (CPA) exploit these physical signals to recover secret keys."
    },
    {
      "footnote_id": "fn-iot-vulnerabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 744,
      "context": "For example, many WiFi-enabled baby monitors have been found to expose unsecured remote access ports[^fn-iot-vulnerabilities], allowing attackers to intercept live audio and video feeds from inside private homes. Similarly, r...",
      "full_line": "For example, many WiFi-enabled baby monitors have been found to expose unsecured remote access ports[^fn-iot-vulnerabilities], allowing attackers to intercept live audio and video feeds from inside private homes. Similarly, researchers have identified wireless vulnerabilities in pacemakers[^fn-medical-device-security] that could allow attackers to manipulate cardiac functions if exploited, raising life-threatening safety concerns."
    },
    {
      "footnote_id": "fn-medical-device-security",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 744,
      "context": "...inside private homes. Similarly, researchers have identified wireless vulnerabilities in pacemakers[^fn-medical-device-security] that could allow attackers to manipulate cardiac functions if exploited, raising life-threatening s...",
      "full_line": "For example, many WiFi-enabled baby monitors have been found to expose unsecured remote access ports[^fn-iot-vulnerabilities], allowing attackers to intercept live audio and video feeds from inside private homes. Similarly, researchers have identified wireless vulnerabilities in pacemakers[^fn-medical-device-security] that could allow attackers to manipulate cardiac functions if exploited, raising life-threatening safety concerns."
    },
    {
      "footnote_id": "fn-iot-vulnerabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 746,
      "context": "[^fn-iot-vulnerabilities]: **IoT Device Vulnerabilities**: Studies reveal 70-80% of IoT devices contain serious security flaw...",
      "full_line": "[^fn-iot-vulnerabilities]: **IoT Device Vulnerabilities**: Studies reveal 70-80% of IoT devices contain serious security flaws, with baby monitors among the worst offenders. Security firm Rapid7 found that popular baby monitor brands exposed unencrypted video streams, affecting millions of households globally."
    },
    {
      "footnote_id": "fn-medical-device-security",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 748,
      "context": "[^fn-medical-device-security]: **Medical Device Security**: FDA reports show 53% of medical devices contain known vulnerabilities...",
      "full_line": "[^fn-medical-device-security]: **Medical Device Security**: FDA reports show 53% of medical devices contain known vulnerabilities, with pacemakers and insulin pumps most at risk. The average medical device contains 6.2 vulnerabilities, some dating back over a decade, affecting 2.4 billion medical devices worldwide."
    },
    {
      "footnote_id": "fn-debug-ports",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 750,
      "context": "A notable case involving smart lightbulbs demonstrated that accessible debug ports[^fn-debug-ports] left on production devices leaked unencrypted WiFi credentials. This security oversight provided at...",
      "full_line": "A notable case involving smart lightbulbs demonstrated that accessible debug ports[^fn-debug-ports] left on production devices leaked unencrypted WiFi credentials. This security oversight provided attackers with a pathway to infiltrate home networks without needing to bypass standard security mechanisms."
    },
    {
      "footnote_id": "fn-debug-ports",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 752,
      "context": "[^fn-debug-ports]: **Debug Port Vulnerabilities**: Hardware debug interfaces like JTAG (1990) and SWD (2006) are esse...",
      "full_line": "[^fn-debug-ports]: **Debug Port Vulnerabilities**: Hardware debug interfaces like JTAG (1990) and SWD (2006) are essential for development but often left accessible in production. Security researchers estimate that 60-70% of embedded devices ship with unsecured debug ports, creating backdoors for attackers. In the automotive domain, unsecured OBD-II diagnostic ports have allowed attackers to manipulate braking and steering functions in connected vehicles, as demonstrated in the well-known Jeep Cherokee hack."
    },
    {
      "footnote_id": "fn-cybersecurity-regulations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 770,
      "context": "...legal consequences, including penalties for violating safety, privacy, or cybersecurity regulations[^fn-cybersecurity-regulations]. This is particularly concerning in sectors such as healthcare and finance, where compliance with i...",
      "full_line": "Legal and regulatory risks further compound the problem. Organizations that unknowingly integrate counterfeit components into their ML systems may face serious legal consequences, including penalties for violating safety, privacy, or cybersecurity regulations[^fn-cybersecurity-regulations]. This is particularly concerning in sectors such as healthcare and finance, where compliance with industry standards is non-negotiable. Healthcare organizations must demonstrate HIPAA compliance throughout their technology stack, while organizations handling EU citizens' data must meet GDPR's requirements for technical and organizational measures, including supply chain integrity."
    },
    {
      "footnote_id": "fn-cybersecurity-regulations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 772,
      "context": "[^fn-cybersecurity-regulations]: **Cybersecurity Regulations**: Global cybersecurity compliance costs exceed $150 billion annually,...",
      "full_line": "[^fn-cybersecurity-regulations]: **Cybersecurity Regulations**: Global cybersecurity compliance costs exceed $150 billion annually, with frameworks like SOC 2, ISO 27001, PCI DSS, and sector-specific rules governing ML systems. Financial services face additional requirements under regulations like SOX, while healthcare must comply with HIPAA, creating complex multi-regulatory environments."
    },
    {
      "footnote_id": "fn-dp-sgd-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 920,
      "context": "...sian mechanism. Training techniques like differentially private stochastic gradient descent (DP-SGD)[^fn-dp-sgd-adoption] integrate calibrated noise into gradient computations during each training step, ensuring that indi...",
      "full_line": "This bound ensures that the algorithm's behavior remains statistically indistinguishable regardless of whether any individual's data is present, thereby limiting the information that can be inferred about that individual. In practice, DP is implemented by adding calibrated noise to model updates or query responses, using mechanisms such as the Laplace or Gaussian mechanism. Training techniques like differentially private stochastic gradient descent (DP-SGD)[^fn-dp-sgd-adoption] integrate calibrated noise into gradient computations during each training step, ensuring that individual data points cannot be distinguished from the model's learned behavior."
    },
    {
      "footnote_id": "fn-dp-sgd-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 922,
      "context": "[^fn-dp-sgd-adoption]: **DP-SGD Industry Adoption**: Apple was the first major company to deploy differential privacy at...",
      "full_line": "[^fn-dp-sgd-adoption]: **DP-SGD Industry Adoption**: Apple was the first major company to deploy differential privacy at scale in 2016, protecting 1+ billion users' data in iOS. Their implementation adds noise to emoji usage, Safari crashes, and QuickType suggestions, balancing privacy (\u03b5=4-16) with utility for improving user experience across their ecosystem."
    },
    {
      "footnote_id": "fn-privacy-utility-tension",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 925,
      "context": "...privacy offers strong theoretical assurances, it introduces a trade-off between privacy and utility[^fn-privacy-utility-tension] that has measurable computational and accuracy costs.",
      "full_line": "While differential privacy offers strong theoretical assurances, it introduces a trade-off between privacy and utility[^fn-privacy-utility-tension] that has measurable computational and accuracy costs."
    },
    {
      "footnote_id": "fn-privacy-utility-tension",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 929,
      "context": "...privacy offers strong theoretical assurances, it introduces a trade-off between privacy and utility[^fn-privacy-utility-tension].",
      "full_line": "While differential privacy offers strong theoretical assurances, it introduces a trade-off between privacy and utility[^fn-privacy-utility-tension]."
    },
    {
      "footnote_id": "fn-privacy-utility-tension",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 934,
      "context": "[^fn-privacy-utility-tension]: **Privacy-Utility Tension**: This core tradeoff was formalized by Dwork and McSherry, who proved t...",
      "full_line": "[^fn-privacy-utility-tension]: **Privacy-Utility Tension**: This core tradeoff was formalized by Dwork and McSherry, who proved that perfect privacy (infinite noise) yields no utility, while perfect utility (no noise) provides no privacy. The \"privacy budget\" concept emerged from this insight\u2014you can only spend privacy once, making every query a strategic decision."
    },
    {
      "footnote_id": "fn-he-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 945,
      "context": "To address scenarios requiring computation on encrypted data, homomorphic encryption (HE)[^fn-he-breakthrough] and secure multiparty computation (SMPC) allow models to perform inference or training over encrypt...",
      "full_line": "To address scenarios requiring computation on encrypted data, homomorphic encryption (HE)[^fn-he-breakthrough] and secure multiparty computation (SMPC) allow models to perform inference or training over encrypted inputs. The computational overhead of homomorphic operations often requires the efficiency optimization techniques covered in @sec-efficient-ai to maintain practical performance."
    },
    {
      "footnote_id": "fn-he-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 947,
      "context": "[^fn-he-breakthrough]: **Homomorphic Encryption Breakthrough**: Considered the \"holy grail\" of cryptography since the 197...",
      "full_line": "[^fn-he-breakthrough]: **Homomorphic Encryption Breakthrough**: Considered the \"holy grail\" of cryptography since the 1970s, fully homomorphic encryption remained theoretical until Craig Gentry's 2009 PhD thesis. His breakthrough was realizing that \"noisy\" ciphertexts could support unlimited operations if periodically \"refreshed,\" solving a decades-old puzzle that allows computation on encrypted data."
    },
    {
      "footnote_id": "fn-smpc-overhead",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 954,
      "context": "...HE remains high, making it more suitable for fixed-function models and low-latency batch tasks. SMPC[^fn-smpc-overhead], by contrast, distributes the computation across multiple parties such that no single party learns...",
      "full_line": "This property supports privacy-preserving computation in untrusted environments, such as cloud inference over sensitive health or financial records. The computational cost of HE remains high, making it more suitable for fixed-function models and low-latency batch tasks. SMPC[^fn-smpc-overhead], by contrast, distributes the computation across multiple parties such that no single party learns the complete input or output. This is particularly useful in joint training across institutions with strict data-use policies, such as hospitals or banks."
    },
    {
      "footnote_id": "fn-smpc-overhead",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 956,
      "context": "[^fn-smpc-overhead]: **SMPC Performance**: Secure multi-party computation typically incurs 1000-10,000x computational o...",
      "full_line": "[^fn-smpc-overhead]: **SMPC Performance**: Secure multi-party computation typically incurs 1000-10,000x computational overhead compared to plaintext operations. A simple neural network inference that takes milliseconds on GPU requires hours using SMPC, limiting practical applications to small models and offline scenarios."
    },
    {
      "footnote_id": "fn-synthetic-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 960,
      "context": "A more pragmatic and increasingly popular alternative involves the use of synthetic data generation[^fn-synthetic-data]. By training generative models on real datasets and sampling new instances from the learned distrib...",
      "full_line": "A more pragmatic and increasingly popular alternative involves the use of synthetic data generation[^fn-synthetic-data]. By training generative models on real datasets and sampling new instances from the learned distribution, organizations can create datasets that approximate the statistical properties of the original data without retaining identifiable details [@goncalves2020generation]."
    },
    {
      "footnote_id": "fn-synthetic-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 962,
      "context": "[^fn-synthetic-data]: **Synthetic Data Growth**: The synthetic data market grew from $110 million in 2019 to $1.1 billio...",
      "full_line": "[^fn-synthetic-data]: **Synthetic Data Growth**: The synthetic data market grew from $110 million in 2019 to $1.1 billion in 2023, driven by privacy regulations and data scarcity. Companies like Uber use synthetic trip data to protect user privacy while maintaining ML model performance, with some synthetic datasets achieving 95%+ statistical fidelity. This approach reduces the risk of direct reidentification, but does not offer formal privacy guarantees unless combined with DP constraints during generation."
    },
    {
      "footnote_id": "fn-model-watermarking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1002,
      "context": "Another design-stage consideration is the use of model watermarking[^fn-model-watermarking], a technique for embedding verifiable ownership signatures directly into the model's parameters or...",
      "full_line": "Another design-stage consideration is the use of model watermarking[^fn-model-watermarking], a technique for embedding verifiable ownership signatures directly into the model's parameters or output behavior [@adi2018turning]. A watermark might be implemented, for example, as a hidden response pattern triggered by specific inputs, or as a parameter-space perturbation that does not affect accuracy but is statistically identifiable."
    },
    {
      "footnote_id": "fn-model-watermarking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1004,
      "context": "[^fn-model-watermarking]: **Model Watermarking**: Technique for proving model ownership developed in 2017, analogous to digi...",
      "full_line": "[^fn-model-watermarking]: **Model Watermarking**: Technique for proving model ownership developed in 2017, analogous to digital image watermarks. Modern watermarking can embed signatures in less than 0.01% of model parameters while maintaining 99%+ accuracy, helping prove IP theft in courts where billions of dollars in AI assets are at stake. These watermarks can be used to detect and prove misuse of stolen models in downstream deployments. Watermarking strategies must be carefully designed to remain robust to model compression, fine-tuning, and format conversion."
    },
    {
      "footnote_id": "fn-oauth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1022,
      "context": "...users and services can interact with inference endpoints. Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based a...",
      "full_line": "Deployment environments must also enforce strong access control policies to ensure that only authorized users and services can interact with inference endpoints. Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to restrict access according to user roles and operational context. For instance, OpenAI's hosted model APIs require users to include an OPENAI_API_KEY when submitting inference requests."
    },
    {
      "footnote_id": "fn-mutual-tls",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1022,
      "context": "...ct with inference endpoints. Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to r...",
      "full_line": "Deployment environments must also enforce strong access control policies to ensure that only authorized users and services can interact with inference endpoints. Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to restrict access according to user roles and operational context. For instance, OpenAI's hosted model APIs require users to include an OPENAI_API_KEY when submitting inference requests."
    },
    {
      "footnote_id": "fn-api-keys",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1022,
      "context": "...Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to restrict access according to...",
      "full_line": "Deployment environments must also enforce strong access control policies to ensure that only authorized users and services can interact with inference endpoints. Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to restrict access according to user roles and operational context. For instance, OpenAI's hosted model APIs require users to include an OPENAI_API_KEY when submitting inference requests."
    },
    {
      "footnote_id": "fn-rbac",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1022,
      "context": "...^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to restrict access according to user roles and operational context. For instance, OpenAI's hosted m...",
      "full_line": "Deployment environments must also enforce strong access control policies to ensure that only authorized users and services can interact with inference endpoints. Authentication protocols, including OAuth[^fn-oauth] tokens, mutual TLS[^fn-mutual-tls], or API keys[^fn-api-keys], should be combined with role-based access control (RBAC)[^fn-rbac] to restrict access according to user roles and operational context. For instance, OpenAI's hosted model APIs require users to include an OPENAI_API_KEY when submitting inference requests."
    },
    {
      "footnote_id": "fn-oauth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1024,
      "context": "[^fn-oauth]: **OAuth Protocol**: Open Authorization standard developed in 2006, now used by 3+ billion users ac...",
      "full_line": "[^fn-oauth]: **OAuth Protocol**: Open Authorization standard developed in 2006, now used by 3+ billion users across Google, Facebook, and Microsoft services. OAuth 2.0 (2012) enables secure API access without exposing user credentials, processing trillions of authentication requests annually for ML API access."
    },
    {
      "footnote_id": "fn-mutual-tls",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1026,
      "context": "[^fn-mutual-tls]: **Mutual TLS (mTLS)**: Enhanced Transport Layer Security where both client and server authenticate...",
      "full_line": "[^fn-mutual-tls]: **Mutual TLS (mTLS)**: Enhanced Transport Layer Security where both client and server authenticate each other using certificates, introduced in 1999. mTLS provides 99.9%+ secure communication but increases latency by 15-30ms, making it suitable for high-security ML API endpoints requiring end-to-end authentication."
    },
    {
      "footnote_id": "fn-api-keys",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1028,
      "context": "[^fn-api-keys]: **API Keys**: Simple authentication tokens first popularized by Google Maps API (2005), now ubiqui...",
      "full_line": "[^fn-api-keys]: **API Keys**: Simple authentication tokens first popularized by Google Maps API (2005), now ubiquitous in ML services. While convenient, API keys in URL parameters or headers can be logged or exposed, with studies showing 10-15% of GitHub repositories accidentally contain leaked API keys worth millions in compute credits."
    },
    {
      "footnote_id": "fn-rbac",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1030,
      "context": "[^fn-rbac]: **Role-Based Access Control (RBAC)**: Access control model developed by NIST in the 1990s, now man...",
      "full_line": "[^fn-rbac]: **Role-Based Access Control (RBAC)**: Access control model developed by NIST in the 1990s, now mandatory for government systems. RBAC reduces security administration overhead by 90%+ compared to individual permissions, with modern ML platforms supporting thousands of roles governing model access, data permissions, and compute resources. This key authenticates the client and allows the backend to enforce usage policies, monitor for abuse, and log access patterns. Secure implementations retrieve API keys from environment variables rather than hardcoding them into source code, preventing credential exposure in version control systems or application logs. Such key-based access control mechanisms are simple to implement but require careful key management and monitoring to prevent misuse, unauthorized access, or model extraction. Additional security measures in production deployments typically include model integrity verification through SHA-256 hash checking, rate limiting to prevent abuse, input validation for size and format constraints, and comprehensive logging for security event tracking."
    },
    {
      "footnote_id": "fn-ci-cd-security",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1032,
      "context": "...the deployment pipeline itself must also be protected. Continuous integration and deployment (CI/CD)[^fn-ci-cd-security] workflows that automate model updates should enforce cryptographic signing of artifacts, dependency...",
      "full_line": "Beyond endpoint access, the integrity of the deployment pipeline itself must also be protected. Continuous integration and deployment (CI/CD)[^fn-ci-cd-security] workflows that automate model updates should enforce cryptographic signing of artifacts, dependency validation, and infrastructure hardening. Without these controls, adversaries could inject malicious models or alter existing ones during the build and deployment process."
    },
    {
      "footnote_id": "fn-ci-cd-security",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1034,
      "context": "[^fn-ci-cd-security]: **CI/CD Security**: Continuous Integration/Continuous Deployment pipelines, popularized by Netflix...",
      "full_line": "[^fn-ci-cd-security]: **CI/CD Security**: Continuous Integration/Continuous Deployment pipelines, popularized by Netflix and Amazon, now deploy code 1000+ times per day. However, 60% of organizations report CI/CD security incidents, with supply chain attacks like SolarWinds (2020) affecting 18,000+ customers, highlighting the critical need for pipeline security in ML deployments. Verifying model signatures and maintaining audit trails helps ensure that only authorized models are deployed into production."
    },
    {
      "footnote_id": "fn-attention-maps",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1067,
      "context": "...d on predefined thresholds. Some systems also inspect internal representations (e.g., attention maps[^fn-attention-maps] or latent embeddings) to anticipate potential misuse before content is rendered.",
      "full_line": "Generative models, such as text-to-image systems, introduce unique output monitoring challenges. These models can produce high-fidelity imagery that may inadvertently violate content safety policies, platform guidelines, or user expectations. To mitigate these risks, post-generation classifiers are commonly employed to assess generated content for objectionable characteristics such as violence, nudity, or brand misuse. These classifiers operate downstream of the generative model and can suppress, blur, or reject outputs based on predefined thresholds. Some systems also inspect internal representations (e.g., attention maps[^fn-attention-maps] or latent embeddings) to anticipate potential misuse before content is rendered."
    },
    {
      "footnote_id": "fn-attention-maps",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1069,
      "context": "[^fn-attention-maps]: **Attention Maps**: Visualization technique for understanding transformer model focus, introduced...",
      "full_line": "[^fn-attention-maps]: **Attention Maps**: Visualization technique for understanding transformer model focus, introduced with the attention mechanism in 2015. Attention maps reveal which input tokens influence outputs most strongly, helping detect potential bias or manipulation in models processing 175+ billion parameters like GPT-3."
    },
    {
      "footnote_id": "fn-healthcare-ml-compliance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1093,
      "context": "For example, in a regulated healthcare ML deployment[^fn-healthcare-ml-compliance], integrity checks might include: verifying the model hash against a signed manifest, validating tha...",
      "full_line": "For example, in a regulated healthcare ML deployment[^fn-healthcare-ml-compliance], integrity checks might include: verifying the model hash against a signed manifest, validating that the runtime environment uses only approved Python packages, and checking that inference occurs inside a signed and attested virtual machine. These checks ensure compliance with regulations like HIPAA's integrity requirements and GDPR's accountability principle, limit the risk of silent failures, and create a forensic trail in case of audit or breach."
    },
    {
      "footnote_id": "fn-healthcare-ml-compliance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1095,
      "context": "[^fn-healthcare-ml-compliance]: **Healthcare ML Compliance**: FDA has approved 500+ AI-based medical devices since 2016, requiring...",
      "full_line": "[^fn-healthcare-ml-compliance]: **Healthcare ML Compliance**: FDA has approved 500+ AI-based medical devices since 2016, requiring strict validation under 21 CFR Part 820 quality systems. Healthcare ML systems must demonstrate safety, efficacy, and bias mitigation, with some approvals taking 2-5 years and costing $50+ million in clinical trials."
    },
    {
      "footnote_id": "fn-trustzone-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1155,
      "context": "...ferent deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[^fn-trustzone-adoption] offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT...",
      "full_line": "Several commercial platforms provide TEE functionality tailored for different deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[^fn-trustzone-adoption] offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT applications. [Intel SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[^fn-intel-sgx-limits] implements enclave-based security for cloud and desktop systems, enabling secure computation even on untrusted infrastructure. [Qualcomm's Secure Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions) supports secure mobile transactions and user authentication. Apple's Secure Enclave remains a canonical example of a hardware-isolated security coprocessor for consumer devices."
    },
    {
      "footnote_id": "fn-intel-sgx-limits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1155,
      "context": "...(https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[^fn-intel-sgx-limits] implements enclave-based security for cloud and desktop systems, enabling secure computation even o...",
      "full_line": "Several commercial platforms provide TEE functionality tailored for different deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[^fn-trustzone-adoption] offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT applications. [Intel SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[^fn-intel-sgx-limits] implements enclave-based security for cloud and desktop systems, enabling secure computation even on untrusted infrastructure. [Qualcomm's Secure Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions) supports secure mobile transactions and user authentication. Apple's Secure Enclave remains a canonical example of a hardware-isolated security coprocessor for consumer devices."
    },
    {
      "footnote_id": "fn-trustzone-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1157,
      "context": "[^fn-trustzone-adoption]: **ARM TrustZone**: Introduced in 2004, TrustZone now ships in 95% of ARM processors, protecting ov...",
      "full_line": "[^fn-trustzone-adoption]: **ARM TrustZone**: Introduced in 2004, TrustZone now ships in 95% of ARM processors, protecting over 5 billion mobile devices. Despite its ubiquity, many devices underutilize TrustZone\u2014studies show only 20-30% of Android devices implement meaningful secure world applications beyond basic key storage."
    },
    {
      "footnote_id": "fn-intel-sgx-limits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1159,
      "context": "[^fn-intel-sgx-limits]: **Intel SGX Constraints**: SGX enclaves are limited to 128MB of protected memory (EPC), with cache...",
      "full_line": "[^fn-intel-sgx-limits]: **Intel SGX Constraints**: SGX enclaves are limited to 128MB of protected memory (EPC), with cache misses causing 100x performance penalties. For ML workloads, a ResNet-50 requires 102MB for weights alone, consuming 80% of SGX EPC before any intermediate activations. Inference latency increases from 5ms to 150ms when model exceeds EPC capacity. This makes SGX unsuitable for large ML models but effective for protecting cryptographic keys and small inference models under 10MB."
    },
    {
      "footnote_id": "fn-hsm-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1368,
      "context": "...ty Modules (HSMs) specialize in the cryptographic operations that underpin these protections. An HSM[^fn-hsm-performance] is a tamper-resistant physical device designed to perform cryptographic operations and securely man...",
      "full_line": "While TEEs and secure boot provide runtime isolation and integrity verification, Hardware Security Modules (HSMs) specialize in the cryptographic operations that underpin these protections. An HSM[^fn-hsm-performance] is a tamper-resistant physical device designed to perform cryptographic operations and securely manage digital keys. HSMs are widely used across security-important industries such as finance, defense, and cloud infrastructure, and they are increasingly relevant for securing the machine learning pipeline\u2014particularly in deployments where key confidentiality, model integrity, and regulatory compliance are important."
    },
    {
      "footnote_id": "fn-hsm-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1370,
      "context": "[^fn-hsm-performance]: **HSM Performance**: Enterprise HSMs can perform 10,000+ RSA-2048 operations per second but cost $...",
      "full_line": "[^fn-hsm-performance]: **HSM Performance**: Enterprise HSMs can perform 10,000+ RSA-2048 operations per second but cost $20,000-$100,000+ per unit. In contrast, software-only cryptography on GPUs achieves 100,000+ operations/second at $1,000+ hardware cost, but without the tamper-resistance and regulatory compliance that HSMs provide."
    },
    {
      "footnote_id": "fn-hsm-certification",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1390,
      "context": "...ly, the use of HSMs often requires organizations to engage in certification and compliance processes[^fn-hsm-certification], particularly when handling regulated data. Meeting standards such as FIPS 140-2[^fn-fips-140] or C...",
      "full_line": "Finally, the use of HSMs often requires organizations to engage in certification and compliance processes[^fn-hsm-certification], particularly when handling regulated data. Meeting standards such as FIPS 140-2[^fn-fips-140] or Common Criteria adds time and cost to development."
    },
    {
      "footnote_id": "fn-fips-140",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1390,
      "context": "...-hsm-certification], particularly when handling regulated data. Meeting standards such as FIPS 140-2[^fn-fips-140] or Common Criteria adds time and cost to development.",
      "full_line": "Finally, the use of HSMs often requires organizations to engage in certification and compliance processes[^fn-hsm-certification], particularly when handling regulated data. Meeting standards such as FIPS 140-2[^fn-fips-140] or Common Criteria adds time and cost to development."
    },
    {
      "footnote_id": "fn-hsm-certification",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1392,
      "context": "[^fn-hsm-certification]: **HSM Certification**: Hardware Security Module certification under FIPS 140-2 or Common Criteria...",
      "full_line": "[^fn-hsm-certification]: **HSM Certification**: Hardware Security Module certification under FIPS 140-2 or Common Criteria can take 12-24 months and cost $500,000-$2 million. However, many regulated industries require these certifications, with banking, government, and healthcare sectors mandating Level 3+ certified HSMs for cryptographic operations."
    },
    {
      "footnote_id": "fn-fips-140",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1394,
      "context": "[^fn-fips-140]: **FIPS 140-2 Standard**: Federal Information Processing Standard for cryptographic modules, establ...",
      "full_line": "[^fn-fips-140]: **FIPS 140-2 Standard**: Federal Information Processing Standard for cryptographic modules, established in 2001 with four security levels. Level 4 HSMs must survive physical attacks, operating at -40\u00b0C to +85\u00b0C with tamper detection that zeroizes keys within seconds, making them suitable for the most sensitive ML applications. Access to the HSM is typically restricted to a small set of authorized personnel, which can complicate development workflows and slow iteration cycles."
    },
    {
      "footnote_id": "fn-puf-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1400,
      "context": "Physical Unclonable Functions (PUFs)[^fn-puf-adoption] provide a hardware-intrinsic mechanism for cryptographic key generation and device authentication b...",
      "full_line": "Physical Unclonable Functions (PUFs)[^fn-puf-adoption] provide a hardware-intrinsic mechanism for cryptographic key generation and device authentication by leveraging physical randomness in semiconductor fabrication [@gassend2002silicon]. Unlike traditional keys stored in memory, a PUF generates secret values based on microscopic variations in a chip's physical properties\u2014variations that are inherent to manufacturing processes and difficult to clone or predict, even by the manufacturer."
    },
    {
      "footnote_id": "fn-puf-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1402,
      "context": "[^fn-puf-adoption]: **PUF Market Growth**: The PUF market is projected to reach $320 million by 2025, driven by IoT se...",
      "full_line": "[^fn-puf-adoption]: **PUF Market Growth**: The PUF market is projected to reach $320 million by 2025, driven by IoT security needs. Major semiconductor companies including Intel, Xilinx, and Synopsis now offer PUF IP, with deployment in smart cards, automotive ECUs, and edge ML devices requiring device-unique authentication."
    },
    {
      "footnote_id": "fn-high-stakes-domains",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 44,
      "context": "Machine learning systems are increasingly deployed in high-stakes domains[^fn-high-stakes-domains] such as healthcare, criminal justice, and employment. As their influence expands, so do the risks o...",
      "full_line": "Machine learning systems are increasingly deployed in high-stakes domains[^fn-high-stakes-domains] such as healthcare, criminal justice, and employment. As their influence expands, so do the risks of embedding bias, compromising privacy, and enabling unintended harms. For example, a loan approval model trained exclusively on data from high-income neighborhoods may unfairly penalize applicants from underrepresented communities, reinforcing structural inequities[^fn-structural-inequities]."
    },
    {
      "footnote_id": "fn-structural-inequities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 44,
      "context": "...ay unfairly penalize applicants from underrepresented communities, reinforcing structural inequities[^fn-structural-inequities].",
      "full_line": "Machine learning systems are increasingly deployed in high-stakes domains[^fn-high-stakes-domains] such as healthcare, criminal justice, and employment. As their influence expands, so do the risks of embedding bias, compromising privacy, and enabling unintended harms. For example, a loan approval model trained exclusively on data from high-income neighborhoods may unfairly penalize applicants from underrepresented communities, reinforcing structural inequities[^fn-structural-inequities]."
    },
    {
      "footnote_id": "fn-high-stakes-domains",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 46,
      "context": "[^fn-high-stakes-domains]: **High-Stakes Domains**: Areas where automated decisions directly impact fundamental life outcomes...",
      "full_line": "[^fn-high-stakes-domains]: **High-Stakes Domains**: Areas where automated decisions directly impact fundamental life outcomes: healthcare (treatment decisions), criminal justice (sentencing recommendations), employment (hiring algorithms), and finance (loan approvals). Estimates suggest that algorithmic decision-making affects over 2 billion people daily across these domains, with errors potentially causing irreversible harm to individuals' health, freedom, or economic prospects."
    },
    {
      "footnote_id": "fn-structural-inequities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 48,
      "context": "[^fn-structural-inequities]: **Structural Inequities**: Systematic patterns of advantage and disadvantage embedded in social in...",
      "full_line": "[^fn-structural-inequities]: **Structural Inequities**: Systematic patterns of advantage and disadvantage embedded in social institutions, policies, and practices. In ML, these manifest when models trained on historical data perpetuate past discrimination\u2014for example, Amazon's recruiting algorithm (discontinued in 2018) systematically downgraded resumes containing words like \"women's\" because it learned from male-dominated hiring patterns spanning 10 years."
    },
    {
      "footnote_id": "fn-protected-attributes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 68,
      "context": "...rning systems do not discriminate against individuals or groups on the basis of protected attributes[^fn-protected-attributes] such as race, gender, or socioeconomic status. This principle encompasses both statistical metrics...",
      "full_line": "Fairness refers to the expectation that machine learning systems do not discriminate against individuals or groups on the basis of protected attributes[^fn-protected-attributes] such as race, gender, or socioeconomic status. This principle encompasses both statistical metrics and broader normative concerns about equity, justice, and structural bias. The two key statistical measures of fairness are demographic parity[^fn-demographic-parity-origin] and equalized odds[^fn-equalized-odds]. Demographic parity ensures equal outcomes across different demographic groups. For example, if a loan approval system maintains the same approval rate for all racial groups, it would satisfy demographic parity. The equalized odds criterion requires that equal outcomes be maintained for all groups at all decision thresholds. In practice, this means the true positive and false positive rates should be equal across protected groups. However, fairness extends beyond these statistical definitions to address deeper questions of equity, historical discrimination, and systemic bias in how machine learning systems impact different communities."
    },
    {
      "footnote_id": "fn-demographic-parity-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 68,
      "context": "...y, justice, and structural bias. The two key statistical measures of fairness are demographic parity[^fn-demographic-parity-origin] and equalized odds[^fn-equalized-odds]. Demographic parity ensures equal outcomes across different...",
      "full_line": "Fairness refers to the expectation that machine learning systems do not discriminate against individuals or groups on the basis of protected attributes[^fn-protected-attributes] such as race, gender, or socioeconomic status. This principle encompasses both statistical metrics and broader normative concerns about equity, justice, and structural bias. The two key statistical measures of fairness are demographic parity[^fn-demographic-parity-origin] and equalized odds[^fn-equalized-odds]. Demographic parity ensures equal outcomes across different demographic groups. For example, if a loan approval system maintains the same approval rate for all racial groups, it would satisfy demographic parity. The equalized odds criterion requires that equal outcomes be maintained for all groups at all decision thresholds. In practice, this means the true positive and false positive rates should be equal across protected groups. However, fairness extends beyond these statistical definitions to address deeper questions of equity, historical discrimination, and systemic bias in how machine learning systems impact different communities."
    },
    {
      "footnote_id": "fn-equalized-odds",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 68,
      "context": "...stical measures of fairness are demographic parity[^fn-demographic-parity-origin] and equalized odds[^fn-equalized-odds]. Demographic parity ensures equal outcomes across different demographic groups. For example, if a l...",
      "full_line": "Fairness refers to the expectation that machine learning systems do not discriminate against individuals or groups on the basis of protected attributes[^fn-protected-attributes] such as race, gender, or socioeconomic status. This principle encompasses both statistical metrics and broader normative concerns about equity, justice, and structural bias. The two key statistical measures of fairness are demographic parity[^fn-demographic-parity-origin] and equalized odds[^fn-equalized-odds]. Demographic parity ensures equal outcomes across different demographic groups. For example, if a loan approval system maintains the same approval rate for all racial groups, it would satisfy demographic parity. The equalized odds criterion requires that equal outcomes be maintained for all groups at all decision thresholds. In practice, this means the true positive and false positive rates should be equal across protected groups. However, fairness extends beyond these statistical definitions to address deeper questions of equity, historical discrimination, and systemic bias in how machine learning systems impact different communities."
    },
    {
      "footnote_id": "fn-protected-attributes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 72,
      "context": "[^fn-protected-attributes]: **Protected Attributes**: Characteristics legally protected from discrimination in most jurisdicti...",
      "full_line": "[^fn-protected-attributes]: **Protected Attributes**: Characteristics legally protected from discrimination in most jurisdictions, typically including race, gender, age, religion, disability status, and sexual orientation. The specific list varies by country\u2014the EU's GDPR covers 9 categories, while the US Civil Rights Act covers 5. In ML systems, these attributes require special handling because their historical correlation with outcomes often reflects past discrimination rather than legitimate predictive relationships."
    },
    {
      "footnote_id": "fn-demographic-parity-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 74,
      "context": "[^fn-demographic-parity-origin]: **Demographic Parity Origins**: This fairness criterion was first formalized by computer scientist...",
      "full_line": "[^fn-demographic-parity-origin]: **Demographic Parity Origins**: This fairness criterion was first formalized by computer scientist Cynthia Dwork and colleagues in 2011, building on legal concepts from the 1971 Supreme Court case *Griggs v. Duke Power Co.*, which established that employment practices with disparate impact could violate civil rights law even without discriminatory intent. The mathematical formalization bridged legal theory with algorithmic practice."
    },
    {
      "footnote_id": "fn-equalized-odds",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 76,
      "context": "[^fn-equalized-odds]: **Equalized Odds**: A fairness constraint requiring that a classifier have equal true positive rat...",
      "full_line": "[^fn-equalized-odds]: **Equalized Odds**: A fairness constraint requiring that a classifier have equal true positive rates and equal false positive rates across protected groups. Developed by Moritz Hardt and others at Google in 2016, it's stricter than demographic parity because it conditions on the true outcome. For example, a medical diagnosis system would need equal sensitivity (correctly identifying disease) and equal specificity (correctly identifying health) across racial groups."
    },
    {
      "footnote_id": "fn-post-hoc-explanations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 78,
      "context": "...avior patterns. Explanations may be generated after a decision is made (called post-hoc explanations[^fn-post-hoc-explanations]) to detail the reasoning process, or they may be built into the model's design for transparent oper...",
      "full_line": "Explainability concerns the ability of stakeholders to interpret how a model produces its outputs. This involves understanding both how individual decisions are made and the model's overall behavior patterns. Explanations may be generated after a decision is made (called post-hoc explanations[^fn-post-hoc-explanations]) to detail the reasoning process, or they may be built into the model's design for transparent operation. The neural network architectures discussed in @sec-dnn-architectures vary significantly in their inherent interpretability, with deeper networks generally being more difficult to explain. Explainability is important for error analysis, regulatory compliance, and building user trust."
    },
    {
      "footnote_id": "fn-post-hoc-explanations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 80,
      "context": "[^fn-post-hoc-explanations]: **Post-Hoc Explanations**: Interpretability methods applied after model training to understand dec...",
      "full_line": "[^fn-post-hoc-explanations]: **Post-Hoc Explanations**: Interpretability methods applied after model training to understand decisions, including LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations). LIME, developed at the University of Washington in 2016, explains individual predictions by learning local surrogate models. SHAP, introduced by researchers at the University of Washington in 2017, provides theoretically grounded feature attribution based on game theory, now used by major tech companies for model interpretation."
    },
    {
      "footnote_id": "fn-value-alignment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 86,
      "context": "Value Alignment[^fn-value-alignment] is the principle that AI systems should pursue goals that are consistent with human intent and ethi...",
      "full_line": "Value Alignment[^fn-value-alignment] is the principle that AI systems should pursue goals that are consistent with human intent and ethical norms. In practice, this involves both technical challenges, including reward design and constraint specification, and broader questions about whose values are represented and enforced."
    },
    {
      "footnote_id": "fn-value-alignment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 88,
      "context": "[^fn-value-alignment]: **Value Alignment**: A fundamental challenge in AI safety, first formally articulated by Stuart Ru...",
      "full_line": "[^fn-value-alignment]: **Value Alignment**: A fundamental challenge in AI safety, first formally articulated by Stuart Russell in 2015 and Nick Bostrom in 2014. The problem: how to ensure AI systems optimize for human values when those values are complex, context-dependent, and often conflicting. Notable failures include Facebook's 2016 \"Year in Review\" feature that created painful reminders for users who experienced loss, and YouTube's recommendation algorithm optimizing for \"engagement\" leading to promotion of extreme content."
    },
    {
      "footnote_id": "fn-human-in-the-loop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 90,
      "context": "...udgment in supervising, correcting, or halting automated decisions. This includes humans-in-the-loop[^fn-human-in-the-loop] during operation, as well as organizational structures that ensure AI use remains accountable to so...",
      "full_line": "Human Oversight emphasizes the role of human judgment in supervising, correcting, or halting automated decisions. This includes humans-in-the-loop[^fn-human-in-the-loop] during operation, as well as organizational structures that ensure AI use remains accountable to societal values and real-world complexity."
    },
    {
      "footnote_id": "fn-human-in-the-loop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 92,
      "context": "[^fn-human-in-the-loop]: **Human-in-the-Loop (HITL)**: A design pattern where humans actively participate in model training...",
      "full_line": "[^fn-human-in-the-loop]: **Human-in-the-Loop (HITL)**: A design pattern where humans actively participate in model training or decision-making, rather than being replaced by automation. Examples include content moderation (major platforms employ thousands of content reviewers), medical diagnosis (radiologists reviewing AI-flagged scans), and autonomous vehicles (safety drivers ready to intervene). Research shows HITL systems can reduce error rates by 50-80% compared to fully automated systems, though they introduce new challenges around human-machine coordination and trust."
    },
    {
      "footnote_id": "fn-differential-privacy-table",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 111,
      "context": "| Privacy                | Consent protocols, data minimization   | Differential privacy[^fn-differential-privacy-table] (e.g., DP-SGD[^fn-dp-sgd])       | N/A                                         | Local inference, s...",
      "full_line": "| Privacy                | Consent protocols, data minimization   | Differential privacy[^fn-differential-privacy-table] (e.g., DP-SGD[^fn-dp-sgd])       | N/A                                         | Local inference, secure access             | PII-free logging, auditability             |"
    },
    {
      "footnote_id": "fn-dp-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 111,
      "context": "...protocols, data minimization   | Differential privacy[^fn-differential-privacy-table] (e.g., DP-SGD[^fn-dp-sgd])       | N/A                                         | Local inference, secure access             |...",
      "full_line": "| Privacy                | Consent protocols, data minimization   | Differential privacy[^fn-differential-privacy-table] (e.g., DP-SGD[^fn-dp-sgd])       | N/A                                         | Local inference, secure access             | PII-free logging, auditability             |"
    },
    {
      "footnote_id": "fn-differential-privacy-table",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 120,
      "context": "[^fn-differential-privacy-table]: **Differential Privacy**: A mathematical framework guaranteeing that removing or adding a single i...",
      "full_line": "[^fn-differential-privacy-table]: **Differential Privacy**: A mathematical framework guaranteeing that removing or adding a single individual's data has minimal statistical impact on query results. Developed by Cynthia Dwork in 2006, it provides formal privacy guarantees by adding calibrated noise to computations. Apple implemented it across 1+ billion iOS devices in 2016, and Google uses it in Chrome for collecting usage statistics while protecting individual browsing patterns."
    },
    {
      "footnote_id": "fn-dp-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 122,
      "context": "[^fn-dp-sgd]: **DP-SGD (Differentially Private Stochastic Gradient Descent)**: A training algorithm that ensures...",
      "full_line": "[^fn-dp-sgd]: **DP-SGD (Differentially Private Stochastic Gradient Descent)**: A training algorithm that ensures privacy by clipping gradients and adding noise during optimization. Introduced by Google researchers in 2016, it enables training on sensitive data while limiting information leakage about individual examples. The technique reduces model accuracy by 2-5% while increasing training time by 15-30% and memory usage by 10-20% due to gradient clipping and noise computation overhead. Used in production by Google, Apple, and Microsoft for training on user data."
    },
    {
      "footnote_id": "fn-compas-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 150,
      "context": "...px) algorithm, used in the United States to assess recidivism risk, was found to exhibit racial bias[^fn-compas-bias]. However, the proprietary nature of the system, combined with limited access to interpretability to...",
      "full_line": "Machine learning systems are frequently criticized for their lack of interpretability. In many cases, models operate as opaque \"black boxes,\" producing outputs that are difficult for users, developers, and regulators to understand or scrutinize. This opacity presents a significant barrier to trust, particularly in high-stakes domains such as criminal justice, healthcare, and finance, where accountability and the right to recourse are important. For example, the [COMPAS](https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx) algorithm, used in the United States to assess recidivism risk, was found to exhibit racial bias[^fn-compas-bias]. However, the proprietary nature of the system, combined with limited access to interpretability tools, hindered efforts to investigate or address the issue."
    },
    {
      "footnote_id": "fn-compas-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 152,
      "context": "[^fn-compas-bias]: **COMPAS Algorithm Controversy**: A 2016 ProPublica investigation revealed that COMPAS (Correction...",
      "full_line": "[^fn-compas-bias]: **COMPAS Algorithm Controversy**: A 2016 ProPublica investigation revealed that COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) incorrectly flagged Black defendants as future criminals at nearly twice the rate of white defendants (45% vs 24%), while white defendants were mislabeled as low-risk more often than Black defendants (48% vs 28%). The algorithm was used in sentencing decisions across multiple states despite these documented disparities."
    },
    {
      "footnote_id": "fn-local-explanations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 154,
      "context": "...e capacity to understand how a model produces its predictions. It includes both _local explanations_[^fn-local-explanations], which clarify individual predictions, and _global explanations_[^fn-global-explanations], which de...",
      "full_line": "Explainability is the capacity to understand how a model produces its predictions. It includes both _local explanations_[^fn-local-explanations], which clarify individual predictions, and _global explanations_[^fn-global-explanations], which describe the models general behavior. Transparency, by contrast, encompasses openness about the broader system design and operation. This includes disclosure of data sources, feature engineering[^fn-feature-engineering], model architectures, training procedures, evaluation protocols, and known limitations. Transparency also involves documentation of intended use cases, system boundaries, and governance structures."
    },
    {
      "footnote_id": "fn-global-explanations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 154,
      "context": "...planations_[^fn-local-explanations], which clarify individual predictions, and _global explanations_[^fn-global-explanations], which describe the models general behavior. Transparency, by contrast, encompasses openness about...",
      "full_line": "Explainability is the capacity to understand how a model produces its predictions. It includes both _local explanations_[^fn-local-explanations], which clarify individual predictions, and _global explanations_[^fn-global-explanations], which describe the models general behavior. Transparency, by contrast, encompasses openness about the broader system design and operation. This includes disclosure of data sources, feature engineering[^fn-feature-engineering], model architectures, training procedures, evaluation protocols, and known limitations. Transparency also involves documentation of intended use cases, system boundaries, and governance structures."
    },
    {
      "footnote_id": "fn-feature-engineering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 154,
      "context": "...e broader system design and operation. This includes disclosure of data sources, feature engineering[^fn-feature-engineering], model architectures, training procedures, evaluation protocols, and known limitations. Transparenc...",
      "full_line": "Explainability is the capacity to understand how a model produces its predictions. It includes both _local explanations_[^fn-local-explanations], which clarify individual predictions, and _global explanations_[^fn-global-explanations], which describe the models general behavior. Transparency, by contrast, encompasses openness about the broader system design and operation. This includes disclosure of data sources, feature engineering[^fn-feature-engineering], model architectures, training procedures, evaluation protocols, and known limitations. Transparency also involves documentation of intended use cases, system boundaries, and governance structures."
    },
    {
      "footnote_id": "fn-local-explanations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 156,
      "context": "[^fn-local-explanations]: **Local Explanations**: Interpretability methods that explain individual predictions, such as \"thi...",
      "full_line": "[^fn-local-explanations]: **Local Explanations**: Interpretability methods that explain individual predictions, such as \"this loan was denied because the applicant's debt-to-income ratio (65%) exceeded the threshold (40%).\" Popular techniques include LIME and SHAP, which identify which input features most influenced a specific decision. These explanations help users understand and potentially contest individual outcomes, crucial for regulatory compliance and user trust."
    },
    {
      "footnote_id": "fn-global-explanations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 158,
      "context": "[^fn-global-explanations]: **Global Explanations**: Methods that describe a model's overall behavior patterns across all inpu...",
      "full_line": "[^fn-global-explanations]: **Global Explanations**: Methods that describe a model's overall behavior patterns across all inputs, such as \"this model primarily relies on credit score (40% importance), income (25%), and payment history (20%) for loan decisions.\" Techniques include feature importance rankings, decision trees as surrogate models, and partial dependence plots. Global explanations help developers debug model behavior and auditors assess system-wide fairness."
    },
    {
      "footnote_id": "fn-feature-engineering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 160,
      "context": "[^fn-feature-engineering]: **Feature Engineering**: The process of transforming raw data into input variables that machine le...",
      "full_line": "[^fn-feature-engineering]: **Feature Engineering**: The process of transforming raw data into input variables that machine learning algorithms can effectively use. Examples include converting categorical variables to numerical representations, creating interaction terms, and normalizing scales. Poor feature engineering can embed bias\u2014for example, using ZIP code as a feature may indirectly discriminate based on race due to residential segregation patterns."
    },
    {
      "footnote_id": "fn-gdpr-article-22",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 162,
      "context": "...receive meaningful information about the logic of automated decisions that significantly affect them[^fn-gdpr-article-22]. Similar regulatory pressures are emerging in other domains, reinforcing the need to treat explaina...",
      "full_line": "These principles are not merely best practices; in many jurisdictions, they are legal obligations. For instance, the European Unions [General Data Protection Regulation (GDPR)](https://gdpr.eu/tag/gdpr/) requires that individuals receive meaningful information about the logic of automated decisions that significantly affect them[^fn-gdpr-article-22]. Similar regulatory pressures are emerging in other domains, reinforcing the need to treat explainability and transparency as core architectural requirements."
    },
    {
      "footnote_id": "fn-gdpr-article-22",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 164,
      "context": "[^fn-gdpr-article-22]: **GDPR Article 22**: Known as the \"right to explanation,\" this provision affects an estimated 500...",
      "full_line": "[^fn-gdpr-article-22]: **GDPR Article 22**: Known as the \"right to explanation,\" this provision affects an estimated 500 million EU citizens and has inspired similar legislation worldwide. Since GDPR's 2018 implementation, regulators have issued over \u20ac5.88 billion in fines as of January 2025, with many cases involving algorithmic decision-making. The regulation's global influence extends beyond Europe\u2014over 120 countries now have privacy laws modeled on GDPR principles."
    },
    {
      "footnote_id": "fn-systemic-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 172,
      "context": "...ned on historical data, they are susceptible to reproducing and amplifying patterns of systemic bias[^fn-systemic-bias] embedded in that data. Without careful design, machine learning systems may unintentionally reinfor...",
      "full_line": "As established in @sec-responsible-ai-core-principles-1bd7, fairness requires that automated systems not disproportionately disadvantage protected groups. Because these systems are trained on historical data, they are susceptible to reproducing and amplifying patterns of systemic bias[^fn-systemic-bias] embedded in that data. Without careful design, machine learning systems may unintentionally reinforce social inequities rather than mitigate them."
    },
    {
      "footnote_id": "fn-systemic-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 174,
      "context": "[^fn-systemic-bias]: **Systemic Bias**: Prejudice embedded in social systems and institutions that creates unequal outc...",
      "full_line": "[^fn-systemic-bias]: **Systemic Bias**: Prejudice embedded in social systems and institutions that creates unequal outcomes for different groups. In ML, this manifests when historical data reflects past discrimination\u2014for example, if past hiring data shows men being promoted more often, a model may learn to favor male candidates. Research shows that without intervention, ML systems can amplify existing biases because they optimize for patterns in historical data that may reflect past discrimination."
    },
    {
      "footnote_id": "fn-healthcare-algorithm-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 176,
      "context": "...found to systematically underestimate the health needs of Black patients [@obermeyer2019dissecting][^fn-healthcare-algorithm-bias]. The model used healthcare expenditures as a proxy for health status, but due to longstanding dispa...",
      "full_line": "A widely studied example comes from the healthcare domain. An algorithm used to allocate care management resources in U.S. hospitals was found to systematically underestimate the health needs of Black patients [@obermeyer2019dissecting][^fn-healthcare-algorithm-bias]. The model used healthcare expenditures as a proxy for health status, but due to longstanding disparities in access and spending, Black patients were less likely to incur high costs. As a result, the model inferred that they were less sick, despite often having equal or greater medical need. This case illustrates how seemingly neutral design choices such as proxy variable selection can yield discriminatory outcomes when historical inequities are not properly accounted for."
    },
    {
      "footnote_id": "fn-healthcare-algorithm-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 178,
      "context": "[^fn-healthcare-algorithm-bias]: **Healthcare Algorithm Scale**: This Optum algorithm affected approximately 200 million Americans...",
      "full_line": "[^fn-healthcare-algorithm-bias]: **Healthcare Algorithm Scale**: This Optum algorithm affected approximately 200 million Americans annually, determining access to high-risk care management programs. The bias reduced Black patients' enrollment by 50%\u2014if corrected, the number of Black patients identified for extra care would increase from 17.7% to 46.5%, highlighting how algorithmic decisions can perpetuate healthcare disparities at massive scale."
    },
    {
      "footnote_id": "fn-fairness-impossibility",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 215,
      "context": "These definitions capture different aspects of fairness and are generally incompatible[^fn-fairness-impossibility]. Satisfying one may preclude satisfying another, reflecting the reality that fairness involves trad...",
      "full_line": "These definitions capture different aspects of fairness and are generally incompatible[^fn-fairness-impossibility]. Satisfying one may preclude satisfying another, reflecting the reality that fairness involves tradeoffs between competing normative goals. Determining which metric to prioritize requires careful consideration of the application context, potential harms, and stakeholder values [@barocas-hardt-narayanan]."
    },
    {
      "footnote_id": "fn-fairness-impossibility",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 217,
      "context": "[^fn-fairness-impossibility]: **Fairness Impossibility Theorems**: Mathematical proofs showing that multiple fairness criteria c...",
      "full_line": "[^fn-fairness-impossibility]: **Fairness Impossibility Theorems**: Mathematical proofs showing that multiple fairness criteria cannot be simultaneously satisfied except in trivial cases. Jon Kleinberg and others proved in 2016 that calibration, equalized odds, and demographic parity are mutually exclusive for any classifier where base rates differ between groups. This means practitioners must choose which type of fairness to prioritize, making fairness fundamentally a value-laden engineering decision rather than a purely technical optimization problem."
    },
    {
      "footnote_id": "fn-datacenter-environmental-justice",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 223,
      "context": "...s and regulatory oversight, which typically coincides with communities of lower socioeconomic status[^fn-datacenter-environmental-justice]. These communities bear the environmental burden of increased energy consumption, heat generation,...",
      "full_line": "**Environmental Justice and Computational Equity**: Fairness considerations extend beyond algorithmic outcomes to encompass the computational resources and infrastructure required to deploy responsible AI systems. Environmental justice concerns arise when the energy-intensive computational requirements for responsible AI deployment are disproportionately concentrated in communities that are already disadvantaged. Large-scale data centers supporting AI systems are often located in areas with lower land costs and regulatory oversight, which typically coincides with communities of lower socioeconomic status[^fn-datacenter-environmental-justice]. These communities bear the environmental burden of increased energy consumption, heat generation, and infrastructure strain while often having limited access to the AI services these facilities enable. The comprehensive analysis of these geographic burden distributions and their broader implications for environmental justice is examined in detail in @sec-sustainable-ai."
    },
    {
      "footnote_id": "fn-datacenter-environmental-justice",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 225,
      "context": "[^fn-datacenter-environmental-justice]: **Datacenter Environmental Justice**: Research by the Environmental Justice Foundation shows that...",
      "full_line": "[^fn-datacenter-environmental-justice]: **Datacenter Environmental Justice**: Research by the Environmental Justice Foundation shows that 68% of major cloud computing facilities in the U.S. are located within 10 miles of low-income communities or communities of color. These areas experience increased air pollution from backup generators, higher local temperatures from cooling systems, and strained local electrical grids. Meanwhile, high-speed internet access required for advanced AI services remains limited in many of these same communities, creating a computational equity gap where communities bear environmental costs without receiving proportional benefits."
    },
    {
      "footnote_id": "fn-model-memorization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 235,
      "context": "...can later be retrieved through model queries or adversarial interaction [@carlini2023extractingllm][^fn-model-memorization].",
      "full_line": "One of the core challenges in supporting privacy is the inherent tension between data utility and individual protection. Rich, high-resolution datasets can enhance model accuracy and adaptability but also heighten the risk of exposing sensitive information, particularly when datasets are aggregated or linked with external sources. For example, models trained on conversational data or medical records have been shown to memorize specific details that can later be retrieved through model queries or adversarial interaction [@carlini2023extractingllm][^fn-model-memorization]."
    },
    {
      "footnote_id": "fn-model-memorization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 237,
      "context": "[^fn-model-memorization]: **Model Memorization**: The phenomenon where ML models inadvertently store training data verbatim,...",
      "full_line": "[^fn-model-memorization]: **Model Memorization**: The phenomenon where ML models inadvertently store training data verbatim, allowing extraction through carefully crafted queries. Nicholas Carlini and others demonstrated in 2021, with further quantification in 2023, that GPT-2 could reproduce entire email addresses, phone numbers, and personal information from training data. Studies suggest large language models like ChatGPT memorize roughly 1% of their training data, raising concerns about privacy violations when models are trained on personal information without explicit consent."
    },
    {
      "footnote_id": "fn-membership-inference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 293,
      "context": "...ining, but also during inference and ongoing operation. Threats such as membership inference attacks[^fn-membership-inference] underscore the importance of embedding privacy safeguards into both model architecture and interfac...",
      "full_line": "Privacy is not solely the concern of isolated algorithms or data processors\u2014it must be addressed as a structural property of the system. Decisions about consent collection, data retention, model design, and auditability all contribute to the privacy posture of a machine learning pipeline. This includes the need to anticipate risks not only during training, but also during inference and ongoing operation. Threats such as membership inference attacks[^fn-membership-inference] underscore the importance of embedding privacy safeguards into both model architecture and interface behavior."
    },
    {
      "footnote_id": "fn-membership-inference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 295,
      "context": "[^fn-membership-inference]: **Membership Inference Attacks**: Privacy attacks that determine whether a specific individual's d...",
      "full_line": "[^fn-membership-inference]: **Membership Inference Attacks**: Privacy attacks that determine whether a specific individual's data was used to train a model by analyzing the model's behavior on that individual's data. First demonstrated by Reza Shokri and others in 2017, these attacks exploit the fact that models tend to be more confident on training data. They pose serious privacy risks\u2014for example, determining if someone's medical record was used to train a disease prediction model reveals sensitive health information."
    },
    {
      "footnote_id": "fn-adversarial-inputs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 307,
      "context": "One illustrative failure mode arises from adversarial inputs[^fn-adversarial-inputs]: carefully constructed perturbations that appear benign to humans but cause a model to output incor...",
      "full_line": "One illustrative failure mode arises from adversarial inputs[^fn-adversarial-inputs]: carefully constructed perturbations that appear benign to humans but cause a model to output incorrect or harmful predictions [@szegedy2013intriguing]. Such vulnerabilities are not limited to image classification\u2014they have been observed across modalities including audio, text, and structured data, and they reveal the brittleness of learned representations in high-dimensional spaces. The adversarial attacks and defenses covered in @sec-privacy-security and the robustness techniques discussed in @sec-robust-ai provide detailed approaches for addressing these vulnerabilities. These behaviors highlight that robustness must be considered not only during training but as a global property of how systems interact with real-world complexity."
    },
    {
      "footnote_id": "fn-adversarial-inputs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 309,
      "context": "[^fn-adversarial-inputs]: **Adversarial Inputs**: Maliciously crafted inputs designed to fool machine learning models by add...",
      "full_line": "[^fn-adversarial-inputs]: **Adversarial Inputs**: Maliciously crafted inputs designed to fool machine learning models by adding imperceptible perturbations that cause misclassification. First demonstrated by Szegedy et al. in 2013, these attacks reveal fundamental vulnerabilities in deep neural networks and have significant implications for safety-critical applications like autonomous vehicles and medical diagnosis."
    },
    {
      "footnote_id": "fn-model-cards",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 327,
      "context": "...cient logs for retrospective analysis. Tools such as [model cards](https://arxiv.org/abs/1810.03993)[^fn-model-cards] and [datasheets for datasets](https://arxiv.org/abs/1803.09010)[^fn-datasheets] exemplify practices...",
      "full_line": "Designing for accountability entails supporting traceability at every stage of the system lifecycle. This includes documenting data provenance, recording model versioning, enabling human overrides, and retaining sufficient logs for retrospective analysis. Tools such as [model cards](https://arxiv.org/abs/1810.03993)[^fn-model-cards] and [datasheets for datasets](https://arxiv.org/abs/1803.09010)[^fn-datasheets] exemplify practices that make system behavior interpretable and reviewable. However, accountability is not reducible to documentation alone\u2014it also requires mechanisms for feedback, contestation, and redress."
    },
    {
      "footnote_id": "fn-datasheets",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 327,
      "context": "...org/abs/1810.03993)[^fn-model-cards] and [datasheets for datasets](https://arxiv.org/abs/1803.09010)[^fn-datasheets] exemplify practices that make system behavior interpretable and reviewable. However, accountability...",
      "full_line": "Designing for accountability entails supporting traceability at every stage of the system lifecycle. This includes documenting data provenance, recording model versioning, enabling human overrides, and retaining sufficient logs for retrospective analysis. Tools such as [model cards](https://arxiv.org/abs/1810.03993)[^fn-model-cards] and [datasheets for datasets](https://arxiv.org/abs/1803.09010)[^fn-datasheets] exemplify practices that make system behavior interpretable and reviewable. However, accountability is not reducible to documentation alone\u2014it also requires mechanisms for feedback, contestation, and redress."
    },
    {
      "footnote_id": "fn-model-cards",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 329,
      "context": "[^fn-model-cards]: **Model Cards**: Standardized documentation for machine learning models, introduced by Google rese...",
      "full_line": "[^fn-model-cards]: **Model Cards**: Standardized documentation for machine learning models, introduced by Google researchers in 2018. Similar to nutrition labels for food, they provide essential information about a model's intended use, performance across different groups, limitations, and ethical considerations. Companies like Google, Facebook, and IBM now use model cards for deployed systems. They help practitioners understand model behavior and enable auditors to assess fairness and safety."
    },
    {
      "footnote_id": "fn-datasheets",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 331,
      "context": "[^fn-datasheets]: **Datasheets for Datasets**: Standardized documentation for datasets, proposed by researchers at M...",
      "full_line": "[^fn-datasheets]: **Datasheets for Datasets**: Standardized documentation for datasets, proposed by researchers at Microsoft and University of Washington in 2018. Modeled after electronics datasheets, they document dataset creation, composition, intended uses, and potential biases. Major datasets like ImageNet and CIFAR-10 now include datasheets. They help practitioners understand dataset limitations and assess suitability for their specific applications, reducing the risk of inappropriate usage."
    },
    {
      "footnote_id": "fn-digital-infrastructure-divide",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 347,
      "context": "...ce quality due to network latency, limited bandwidth, and distance from computational infrastructure[^fn-digital-infrastructure-divide]. This infrastructure gap means that responsible AI principles like real-time explainability, contin...",
      "full_line": "The geographic and economic distribution of computational resources creates additional layers of equity concerns in responsible AI deployment. High-performance AI systems typically require proximity to major data centers or high-bandwidth internet connections, creating service quality disparities that map closely to existing socioeconomic inequalities. Rural communities, developing regions, and economically disadvantaged areas often experience degraded AI service quality due to network latency, limited bandwidth, and distance from computational infrastructure[^fn-digital-infrastructure-divide]. This infrastructure gap means that responsible AI principles like real-time explainability, continuous fairness monitoring, and privacy-preserving computation may be practically unavailable to users in these contexts."
    },
    {
      "footnote_id": "fn-digital-infrastructure-divide",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 349,
      "context": "[^fn-digital-infrastructure-divide]: **Digital Infrastructure Divide**: The Federal Communications Commission estimates that 39% of rur...",
      "full_line": "[^fn-digital-infrastructure-divide]: **Digital Infrastructure Divide**: The Federal Communications Commission estimates that 39% of rural Americans lack access to high-speed broadband compared to only 2% in urban areas. For AI services requiring real-time responsiveness, users in underserved areas experience 200-500ms additional latency, making interactive explainability features and real-time bias monitoring infeasible. This infrastructure disparity means that responsible AI features are often unavailable to the communities most vulnerable to algorithmic harm, creating an inverse relationship between need and access to ethical AI protections."
    },
    {
      "footnote_id": "fn-federated-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 371,
      "context": "...rocedures, and post hoc auditing. In contrast, decentralized deployments, such as federated learning[^fn-federated-learning] clients or mobile applications, typically lack access to global statistics due to privacy constrain...",
      "full_line": "A key determinant is data visibility. In centralized environments, such as cloud-hosted platforms, developers often have access to large datasets with demographic annotations. This allows the use of group-level fairness metrics, fairness-aware training procedures, and post hoc auditing. In contrast, decentralized deployments, such as federated learning[^fn-federated-learning] clients or mobile applications, typically lack access to global statistics due to privacy constraints or fragmented data. The on-device learning approaches covered in @sec-ondevice-learning present unique challenges for fairness assessment, as individual devices may have limited visibility into global demographic distributions. In such settings, fairness interventions must often be embedded during training or dataset curation, as post-deployment evaluation may be infeasible."
    },
    {
      "footnote_id": "fn-federated-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 373,
      "context": "[^fn-federated-learning]: **Federated Learning**: A machine learning approach where models are trained across multiple decen...",
      "full_line": "[^fn-federated-learning]: **Federated Learning**: A machine learning approach where models are trained across multiple decentralized devices or servers without centralizing the data. Developed by Google in 2016 for improving Gboard predictions while keeping typing data on devices. Now used by Apple for Siri improvements and by hospitals for medical research without sharing patient data. While privacy-preserving, federated learning complicates fairness assessment since no single entity can observe the complete demographic distribution across all participants."
    },
    {
      "footnote_id": "fn-energy-privacy-tradeoff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 451,
      "context": "...y protections prohibitive for battery-constrained devices or users concerned about electricity costs[^fn-energy-privacy-tradeoff]. These resource constraints can create disparate impacts where users with limited computational acc...",
      "full_line": "**Resource Constraints and Ethical Responsibilities**: The computational demands of responsible AI create tensions that extend beyond technical optimization to questions of environmental justice and equitable access. Energy-efficient deployment often requires simplified models with reduced fairness monitoring capabilities, creating a tradeoff between environmental sustainability and ethical safeguards. For example, implementing differential privacy in federated learning can increase per-device energy consumption by 25-40%, potentially making such privacy protections prohibitive for battery-constrained devices or users concerned about electricity costs[^fn-energy-privacy-tradeoff]. These resource constraints can create disparate impacts where users with limited computational access receive less privacy protection or fairness monitoring."
    },
    {
      "footnote_id": "fn-energy-privacy-tradeoff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 453,
      "context": "[^fn-energy-privacy-tradeoff]: **Energy-Privacy Tradeoffs**: Research by Stanford and MIT demonstrates that privacy-preserving te...",
      "full_line": "[^fn-energy-privacy-tradeoff]: **Energy-Privacy Tradeoffs**: Research by Stanford and MIT demonstrates that privacy-preserving techniques like differential privacy and secure multi-party computation can increase computational energy requirements by 20-60% depending on implementation. In federated learning scenarios, this translates to 15-30% faster battery drain on mobile devices. For users with older devices, limited battery life, or concerns about electricity costs, these energy requirements can effectively exclude them from privacy-protected AI services, creating a system where privacy becomes contingent on economic resources."
    },
    {
      "footnote_id": "fn-saliency-maps",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 467,
      "context": "|                        | like SHAP and sampling approaches    | methods like saliency maps[^fn-saliency-maps]         | for users, often defers deeper     | constrained hardware; mostly   |",
      "full_line": "|                        | like SHAP and sampling approaches    | methods like saliency maps[^fn-saliency-maps]         | for users, often defers deeper     | constrained hardware; mostly   |"
    },
    {
      "footnote_id": "fn-saliency-maps",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 489,
      "context": "[^fn-saliency-maps]: **Saliency Maps**: Visual explanations highlighting which parts of an input (like pixels in an ima...",
      "full_line": "[^fn-saliency-maps]: **Saliency Maps**: Visual explanations highlighting which parts of an input (like pixels in an image) most influenced a model's decision. Originally developed for computer vision in 2013, they use gradients to compute feature importance. Unlike LIME or SHAP, saliency maps require only a single backward pass, making them computationally efficient for edge devices. However, they can be noisy and may highlight irrelevant features, requiring careful interpretation."
    },
    {
      "footnote_id": "fn-fairlearn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 515,
      "context": "...e machine learning lifecycle. Practical bias detection can be implemented using tools like Fairlearn[^fn-fairlearn] [@bird2020fairlearn]:",
      "full_line": "Operationalizing fairness in deployed systems requires more than principled objectives or theoretical metrics\u2014it demands system-aware methods that detect, measure, and mitigate bias across the machine learning lifecycle. Practical bias detection can be implemented using tools like Fairlearn[^fn-fairlearn] [@bird2020fairlearn]:"
    },
    {
      "footnote_id": "fn-fairlearn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 517,
      "context": "[^fn-fairlearn]: **Fairlearn**: Microsoft's open-source toolkit for assessing and improving ML model fairness, firs...",
      "full_line": "[^fn-fairlearn]: **Fairlearn**: Microsoft's open-source toolkit for assessing and improving ML model fairness, first released in 2019. It provides metrics for measuring fairness across demographic groups and algorithms for bias mitigation. Used by organizations like Uber, Pinterest, and Ernst & Young to audit models before deployment. The toolkit supports both fairness assessment (identifying disparities) and fairness intervention (reducing disparities through techniques like reweighting and post-processing)."
    },
    {
      "footnote_id": "fn-differential-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 810,
      "context": "...acy-preserving techniques have been developed. Among the most widely adopted is differential privacy[^fn-differential-privacy], which provides formal guarantees that the inclusion or exclusion of a single datapoint has a stati...",
      "full_line": "To mitigate such vulnerabilities, a range of privacy-preserving techniques have been developed. Among the most widely adopted is differential privacy[^fn-differential-privacy], which provides formal guarantees that the inclusion or exclusion of a single datapoint has a statistically bounded effect on the models output. Algorithms such as differentially private stochastic gradient descent (DP-SGD) enforce these guarantees by clipping gradients and injecting noise during training [@abadi2016deep]. When implemented correctly, these methods prevent the model from memorizing individual datapoints and reduce the risk of inference attacks."
    },
    {
      "footnote_id": "fn-differential-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 812,
      "context": "[^fn-differential-privacy]: **Differential Privacy**: Introduced by Cynthia Dwork in 2006, differential privacy revolutionized...",
      "full_line": "[^fn-differential-privacy]: **Differential Privacy**: Introduced by Cynthia Dwork in 2006, differential privacy revolutionized privacy-preserving computation by providing mathematical guarantees rather than heuristic protections. Apple was among the first major companies to deploy differential privacy at scale in 2016, using it to collect iOS usage statistics from over 1 billion devices while preserving individual privacy. The technique adds calibrated noise to computations, ensuring that no single person's data significantly affects the output."
    },
    {
      "footnote_id": "fn-ai-safety",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1322,
      "context": "...nd remain aligned with human intentions over time. These concerns fall under the domain of AI safety[^fn-ai-safety], which focuses on preventing unintended or harmful outcomes from capable AI systems. A central chal...",
      "full_line": "As machine learning systems increase in autonomy, scale, and deployment complexity, the nature of responsibility expands beyond model-level fairness or privacy concerns. It includes ensuring that systems pursue the right objectives, behave safely in uncertain environments, and remain aligned with human intentions over time. These concerns fall under the domain of AI safety[^fn-ai-safety], which focuses on preventing unintended or harmful outcomes from capable AI systems. A central challenge is that today's ML models often optimize proxy metrics[^fn-proxy-metrics], such as loss functions, reward functions, or engagement signals, that do not fully capture human values."
    },
    {
      "footnote_id": "fn-proxy-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1322,
      "context": "...from capable AI systems. A central challenge is that today's ML models often optimize proxy metrics[^fn-proxy-metrics], such as loss functions, reward functions, or engagement signals, that do not fully capture human v...",
      "full_line": "As machine learning systems increase in autonomy, scale, and deployment complexity, the nature of responsibility expands beyond model-level fairness or privacy concerns. It includes ensuring that systems pursue the right objectives, behave safely in uncertain environments, and remain aligned with human intentions over time. These concerns fall under the domain of AI safety[^fn-ai-safety], which focuses on preventing unintended or harmful outcomes from capable AI systems. A central challenge is that today's ML models often optimize proxy metrics[^fn-proxy-metrics], such as loss functions, reward functions, or engagement signals, that do not fully capture human values."
    },
    {
      "footnote_id": "fn-ai-safety",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1324,
      "context": "[^fn-ai-safety]: **AI Safety**: A research field focused on ensuring advanced AI systems remain beneficial and cont...",
      "full_line": "[^fn-ai-safety]: **AI Safety**: A research field focused on ensuring advanced AI systems remain beneficial and controllable. Originated from concerns raised by researchers like Stuart Russell and Nick Bostrom around 2010, it addresses both near-term risks (bias, privacy violations) and long-term risks (misaligned superintelligent systems). Major organizations like OpenAI, Anthropic, and DeepMind now invest significantly in safety research, while companies like Tesla have faced real-world safety challenges with autonomous driving systems."
    },
    {
      "footnote_id": "fn-proxy-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1326,
      "context": "[^fn-proxy-metrics]: **Proxy Metrics**: Measurable indicators used as substitutes for the true objective when the real...",
      "full_line": "[^fn-proxy-metrics]: **Proxy Metrics**: Measurable indicators used as substitutes for the true objective when the real goal is difficult to quantify directly. Common examples include using click-through rates as a proxy for user satisfaction, or test scores as a proxy for educational quality. The danger arises from Goodhart's Law: \"When a measure becomes a target, it ceases to be a good measure\"\u2014systems optimize for the proxy rather than the underlying goal."
    },
    {
      "footnote_id": "fn-ctr-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1328,
      "context": "...xample comes from recommendation systems, where a model trained to maximize click-through rate (CTR)[^fn-ctr-optimization] may end up promoting content that increases engagement but diminishes user satisfaction, including...",
      "full_line": "One concrete example comes from recommendation systems, where a model trained to maximize click-through rate (CTR)[^fn-ctr-optimization] may end up promoting content that increases engagement but diminishes user satisfaction, including clickbait, misinformation, and emotionally manipulative material. This behavior is aligned with the proxy, but misaligned with the actual goal, resulting in a feedback loop that reinforces undesirable outcomes. As shown in @fig-reward-hacking-loop, the system learns to optimize for a measurable reward (clicks) rather than the intended human-centered outcome (satisfaction). The result is emergent behavior that reflects specification gaming or reward hacking[^fn-reward-hacking]\u2014a central concern in value alignment and AI safety."
    },
    {
      "footnote_id": "fn-reward-hacking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1328,
      "context": "...(satisfaction). The result is emergent behavior that reflects specification gaming or reward hacking[^fn-reward-hacking]\u2014a central concern in value alignment and AI safety.",
      "full_line": "One concrete example comes from recommendation systems, where a model trained to maximize click-through rate (CTR)[^fn-ctr-optimization] may end up promoting content that increases engagement but diminishes user satisfaction, including clickbait, misinformation, and emotionally manipulative material. This behavior is aligned with the proxy, but misaligned with the actual goal, resulting in a feedback loop that reinforces undesirable outcomes. As shown in @fig-reward-hacking-loop, the system learns to optimize for a measurable reward (clicks) rather than the intended human-centered outcome (satisfaction). The result is emergent behavior that reflects specification gaming or reward hacking[^fn-reward-hacking]\u2014a central concern in value alignment and AI safety."
    },
    {
      "footnote_id": "fn-ctr-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1330,
      "context": "[^fn-ctr-optimization]: **Click-Through Rate (CTR) Optimization**: The practice of maximizing the percentage of users who...",
      "full_line": "[^fn-ctr-optimization]: **Click-Through Rate (CTR) Optimization**: The practice of maximizing the percentage of users who click on content or ads. While seemingly logical, CTR optimization can incentivize sensationalism and clickbait. YouTube's 2012-2017 algorithm optimized for CTR, leading to promotion of conspiracy theories and extreme content because they generated more clicks. The platform shifted to optimizing \"watch time\" in 2017 to address this misalignment between clicks and user satisfaction."
    },
    {
      "footnote_id": "fn-reward-hacking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1332,
      "context": "[^fn-reward-hacking]: **Reward Hacking**: When an AI system finds unexpected ways to maximize its reward function that v...",
      "full_line": "[^fn-reward-hacking]: **Reward Hacking**: When an AI system finds unexpected ways to maximize its reward function that violate the designer's intentions. Classic examples include a Tetris-playing AI that learned to pause the game indefinitely to avoid losing (maximizing score by avoiding failure), and cleaning robots that learned to knock over objects to create messes they could then clean up. This phenomenon highlights the difficulty of specifying objectives that capture human intentions."
    },
    {
      "footnote_id": "fn-safety-critical",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 45,
      "context": "...d reliability. As these systems become more complex and are deployed in safety-critical applications[^fn-safety-critical], robust and fault-tolerant designs become essential.",
      "full_line": "ML systems are increasingly integrated into domains spanning cloud-based services to edge devices and embedded systems, where hardware and software faults have pronounced impacts on performance and reliability. As these systems become more complex and are deployed in safety-critical applications[^fn-safety-critical], robust and fault-tolerant designs become essential."
    },
    {
      "footnote_id": "fn-safety-critical",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 47,
      "context": "[^fn-safety-critical]: **Safety-Critical Applications**: Systems where failure could result in loss of life, significant...",
      "full_line": "[^fn-safety-critical]: **Safety-Critical Applications**: Systems where failure could result in loss of life, significant property damage, or environmental harm. Examples include nuclear power plants, aircraft control systems, and medical devices, domains where ML deployment requires the highest reliability standards."
    },
    {
      "footnote_id": "fn-smart-cities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 49,
      "context": "ML systems play critical roles in autonomous vehicles, smart cities[^fn-smart-cities], healthcare, and industrial automation. In these domains, the consequences of systemic failures, in...",
      "full_line": "ML systems play critical roles in autonomous vehicles, smart cities[^fn-smart-cities], healthcare, and industrial automation. In these domains, the consequences of systemic failures, including hardware and software faults, and malicious inputs such as adversarial attacks and data poisoning, and environmental shifts, can be severe, potentially resulting in loss of life, economic disruption, or environmental harm."
    },
    {
      "footnote_id": "fn-smart-cities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 51,
      "context": "[^fn-smart-cities]: **Smart Cities**: Urban areas that use IoT devices, data analytics, and AI to optimize infrastruct...",
      "full_line": "[^fn-smart-cities]: **Smart Cities**: Urban areas that use IoT devices, data analytics, and AI to optimize infrastructure, energy usage, and public services. With urban populations expected to reach 68% of the global population by 2050 [@un2018world], smart city initiatives will manage increasingly large data volumes, contributing to the estimated 2.5 quintillion bytes of data generated globally each day [@ibm2020data]."
    },
    {
      "footnote_id": "fn-transient-vs-permanent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 62,
      "context": "...ailures present significant challenges across computing systems (@sec-ml-systems). Whether transient[^fn-transient-vs-permanent], permanent, or intermittent, these faults can corrupt computations and degrade system performance....",
      "full_line": "Systemic hardware failures present significant challenges across computing systems (@sec-ml-systems). Whether transient[^fn-transient-vs-permanent], permanent, or intermittent, these faults can corrupt computations and degrade system performance. The impact ranges from temporary glitches to complete component failures, requiring robust detection and mitigation strategies to maintain reliable operation."
    },
    {
      "footnote_id": "fn-transient-vs-permanent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 64,
      "context": "[^fn-transient-vs-permanent]: **Transient vs Permanent Faults**: Transient faults are temporary disruptions (lasting microsecond...",
      "full_line": "[^fn-transient-vs-permanent]: **Transient vs Permanent Faults**: Transient faults are temporary disruptions (lasting microseconds to seconds) often caused by cosmic rays or electromagnetic interference, while permanent faults cause lasting damage requiring component replacement. Transient faults are 1000x more common than permanent faults in modern systems [@baumann2005soft]."
    },
    {
      "footnote_id": "fn-systemic-vulnerabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 68,
      "context": "...ithms, libraries, and frameworks can propagate through the system, creating systemic vulnerabilities[^fn-systemic-vulnerabilities]. Rigorous testing, monitoring, and quality control processes help identify and address these softwa...",
      "full_line": "Environmental changes introduce another dimension of potential faults that must be carefully managed through deployment strategies (@sec-ml-operations). Bugs, design flaws, and implementation errors within algorithms, libraries, and frameworks can propagate through the system, creating systemic vulnerabilities[^fn-systemic-vulnerabilities]. Rigorous testing, monitoring, and quality control processes help identify and address these software-related issues before they impact production systems."
    },
    {
      "footnote_id": "fn-systemic-vulnerabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 70,
      "context": "[^fn-systemic-vulnerabilities]: **Systemic Vulnerabilities**: Weaknesses that affect entire system architectures rather than indiv...",
      "full_line": "[^fn-systemic-vulnerabilities]: **Systemic Vulnerabilities**: Weaknesses that affect entire system architectures rather than individual components. Unlike isolated bugs, these can cascade across multiple layers, potentially compromising thousands of interconnected services simultaneously."
    },
    {
      "footnote_id": "fn-hardening-strategies",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 72,
      "context": "...in @sec-ondevice-learning. This necessitates careful optimization and targeted hardening strategies[^fn-hardening-strategies] appropriate for resource-constrained environments.",
      "full_line": "The specific approaches to achieving robustness vary significantly based on deployment context and system constraints, leveraging efficiency principles (@sec-efficient-ai). Large-scale cloud computing environments and data centers typically emphasize fault tolerance through redundancy, distributed processing architectures, and sophisticated error detection mechanisms. In contrast, edge devices and embedded systems must address robustness challenges within strict computational, memory, and energy limitations, requiring specialized approaches detailed in @sec-ondevice-learning. This necessitates careful optimization and targeted hardening strategies[^fn-hardening-strategies] appropriate for resource-constrained environments."
    },
    {
      "footnote_id": "fn-hardening-strategies",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 74,
      "context": "[^fn-hardening-strategies]: **Hardening Strategies**: Techniques to increase system resilience against faults and attacks, inc...",
      "full_line": "[^fn-hardening-strategies]: **Hardening Strategies**: Techniques to increase system resilience against faults and attacks, including redundancy, input validation, and fail-safe mechanisms. Edge systems often use selective hardening, protecting only critical components due to resource constraints."
    },
    {
      "footnote_id": "fn-alexa-voice-service",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 121,
      "context": "...exa-enabled devices, including Amazon Echo and third-party products that utilize Alexa Voice Service[^fn-alexa-voice-service], were unresponsive for several hours. This incident underscores the impact of human error on cloud-...",
      "full_line": "In February 2017, Amazon Web Services (AWS) experienced [a significant outage](https://aws.amazon.com/message/41926/) due to human error during routine maintenance. An engineer inadvertently entered an incorrect command, resulting in the shutdown of multiple servers. This outage disrupted many AWS services, including Amazon\u2019s AI-powered assistant, Alexa. As a consequence, Alexa-enabled devices, including Amazon Echo and third-party products that utilize Alexa Voice Service[^fn-alexa-voice-service], were unresponsive for several hours. This incident underscores the impact of human error on cloud-based ML systems and the importance of robust maintenance protocols and failsafe mechanisms[^fn-failsafe-mechanisms]."
    },
    {
      "footnote_id": "fn-failsafe-mechanisms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 121,
      "context": "...on cloud-based ML systems and the importance of robust maintenance protocols and failsafe mechanisms[^fn-failsafe-mechanisms].",
      "full_line": "In February 2017, Amazon Web Services (AWS) experienced [a significant outage](https://aws.amazon.com/message/41926/) due to human error during routine maintenance. An engineer inadvertently entered an incorrect command, resulting in the shutdown of multiple servers. This outage disrupted many AWS services, including Amazon\u2019s AI-powered assistant, Alexa. As a consequence, Alexa-enabled devices, including Amazon Echo and third-party products that utilize Alexa Voice Service[^fn-alexa-voice-service], were unresponsive for several hours. This incident underscores the impact of human error on cloud-based ML systems and the importance of robust maintenance protocols and failsafe mechanisms[^fn-failsafe-mechanisms]."
    },
    {
      "footnote_id": "fn-alexa-voice-service",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 123,
      "context": "[^fn-alexa-voice-service]: **Alexa Voice Service (AVS)**: Amazon's cloud-based voice AI platform that enables third-party man...",
      "full_line": "[^fn-alexa-voice-service]: **Alexa Voice Service (AVS)**: Amazon's cloud-based voice AI platform that enables third-party manufacturers to integrate Alexa into their devices. Used by over 100,000 devices across 4,500 brands, processing billions of voice interactions annually."
    },
    {
      "footnote_id": "fn-failsafe-mechanisms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 125,
      "context": "[^fn-failsafe-mechanisms]: **Failsafe Mechanisms**: Systems designed to automatically shift to a safe state when a fault occu...",
      "full_line": "[^fn-failsafe-mechanisms]: **Failsafe Mechanisms**: Systems designed to automatically shift to a safe state when a fault occurs. Examples include circuit breakers that prevent cascading failures and graceful degradation that maintains core functionality when components fail."
    },
    {
      "footnote_id": "fn-silent-data-corruption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 127,
      "context": "In another case [@dixit2021silent], Facebook encountered a silent data corruption (SDC)[^fn-silent-data-corruption] issue in its distributed querying infrastructure, illustrated in @fig-sdc-example. SDC refers to un...",
      "full_line": "In another case [@dixit2021silent], Facebook encountered a silent data corruption (SDC)[^fn-silent-data-corruption] issue in its distributed querying infrastructure, illustrated in @fig-sdc-example. SDC refers to undetected errors during computation or data transfer that propagate silently through system layers. Facebook\u2019s system processed SQL-like queries across datasets and supported a compression application designed to reduce data storage footprints. Files were compressed when not in use and decompressed upon read requests. A size check was performed before decompression to ensure the file was valid. However, an unexpected fault occasionally returned a file size of zero for valid files, leading to decompression failures and missing entries in the output database. The issue appeared sporadically, with some computations returning correct file sizes, making it particularly difficult to diagnose."
    },
    {
      "footnote_id": "fn-silent-data-corruption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 129,
      "context": "[^fn-silent-data-corruption]: **Silent Data Corruption (SDC)**: Hardware or software errors that corrupt data without triggering...",
      "full_line": "[^fn-silent-data-corruption]: **Silent Data Corruption (SDC)**: Hardware or software errors that corrupt data without triggering error detection mechanisms. Studies show SDC affects 1 in every 1,000-10,000 computations in large-scale systems [@dixit2021silent], making it a major reliability concern."
    },
    {
      "footnote_id": "fn-ai-hypercomputers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 248,
      "context": "...hief Scientist at Google DeepMind and Google Research, highlighted these issues in AI hypercomputers[^fn-ai-hypercomputers] during a keynote at [MLSys 2024](https://mlsys.org/) [@dean2024mlsys].",
      "full_line": "This case illustrates how silent data corruption can propagate across multiple layers of the application stack, resulting in data loss and application failures in large-scale distributed systems. Left unaddressed, such errors can degrade ML system performance, particularly affecting training processes (@sec-ai-training). For example, corrupted training data or inconsistencies in data pipelines due to SDC may compromise model accuracy and reliability. Similar challenges have been reported by other major companies. As shown in @fig-sdc-jeffdean, [Jeff Dean](https://en.wikipedia.org/wiki/Jeff_Dean), Chief Scientist at Google DeepMind and Google Research, highlighted these issues in AI hypercomputers[^fn-ai-hypercomputers] during a keynote at [MLSys 2024](https://mlsys.org/) [@dean2024mlsys]."
    },
    {
      "footnote_id": "fn-ai-hypercomputers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 250,
      "context": "[^fn-ai-hypercomputers]: **AI Hypercomputers**: Massive computing systems specifically designed for AI workloads, featuring...",
      "full_line": "[^fn-ai-hypercomputers]: **AI Hypercomputers**: Massive computing systems specifically designed for AI workloads, featuring thousands of specialized processors (TPUs/GPUs) interconnected with high-bandwidth networks. Google's latest systems contain over 100,000 accelerators working in parallel."
    },
    {
      "footnote_id": "fn-edge-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 256,
      "context": "In the edge computing domain[^fn-edge-computing], self-driving vehicles provide prominent examples of how faults can critically affect ML systems. T...",
      "full_line": "In the edge computing domain[^fn-edge-computing], self-driving vehicles provide prominent examples of how faults can critically affect ML systems. These vehicles depend on machine learning for perception, decision-making, and control, making them particularly vulnerable to both hardware and software faults."
    },
    {
      "footnote_id": "fn-edge-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 258,
      "context": "[^fn-edge-computing]: **Edge Computing**: Processing data near its source rather than in centralized cloud servers, redu...",
      "full_line": "[^fn-edge-computing]: **Edge Computing**: Processing data near its source rather than in centralized cloud servers, reducing latency from ~100ms to <10ms. Critical for autonomous vehicles where millisecond delays can mean the difference between collision avoidance and catastrophic failure."
    },
    {
      "footnote_id": "fn-autopilot",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 262,
      "context": "In May 2016, a fatal crash occurred when a Tesla Model S operating in Autopilot mode[^fn-autopilot] collided with a white semi-trailer truck. The system, relying on computer vision and ML algorithms,...",
      "full_line": "In May 2016, a fatal crash occurred when a Tesla Model S operating in Autopilot mode[^fn-autopilot] collided with a white semi-trailer truck. The system, relying on computer vision and ML algorithms, failed to distinguish the trailer against a bright sky, leading to a high-speed impact. The driver, reportedly distracted at the time, did not intervene, as shown in @fig-tesla-example. This incident raised serious concerns about the reliability of AI-based perception systems and emphasized the need for robust failsafe mechanisms in autonomous vehicles. A similar case occurred in March 2018, when an Uber self-driving test vehicle [struck](https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html?iid=EL) and killed a pedestrian in Tempe, Arizona. The accident was attributed to a flaw in the vehicle\u2019s object recognition software, which failed to classify the pedestrian as an obstacle requiring avoidance."
    },
    {
      "footnote_id": "fn-autopilot",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 264,
      "context": "[^fn-autopilot]: **Autopilot**: Tesla's driver assistance system that provides semi-autonomous capabilities like st...",
      "full_line": "[^fn-autopilot]: **Autopilot**: Tesla's driver assistance system that provides semi-autonomous capabilities like steering, braking, and acceleration while requiring active driver supervision."
    },
    {
      "footnote_id": "fn-embedded-systems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 268,
      "context": "Embedded systems[^fn-embedded-systems] operate in resource-constrained and often safety-critical environments. As AI capabilities are incr...",
      "full_line": "Embedded systems[^fn-embedded-systems] operate in resource-constrained and often safety-critical environments. As AI capabilities are increasingly integrated into these systems, the complexity and consequences of faults grow significantly."
    },
    {
      "footnote_id": "fn-embedded-systems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 270,
      "context": "[^fn-embedded-systems]: **Embedded Systems**: Computer systems designed for specific control functions within larger syste...",
      "full_line": "[^fn-embedded-systems]: **Embedded Systems**: Computer systems designed for specific control functions within larger systems, often with real-time constraints. Range from 8-bit microcontrollers with kilobytes of memory to complex systems-on-chip, typically operating for years without human intervention."
    },
    {
      "footnote_id": "fn-model-uncertainty",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 282,
      "context": "...applications, new sources of vulnerability emerge, including data-related errors, model uncertainty[^fn-model-uncertainty], and unpredictable behaviors in rare edge cases. The opaque nature of some AI models complicates fa...",
      "full_line": "Consider the case of implantable medical devices. For instance, a smart [pacemaker](https://www.bbc.com/future/article/20221011-how-space-weather-causes-computer-errors) that experiences a fault or unexpected behavior due to software or hardware failure could place a patient\u2019s life at risk. As AI systems take on perception, decision-making, and control roles in such applications, new sources of vulnerability emerge, including data-related errors, model uncertainty[^fn-model-uncertainty], and unpredictable behaviors in rare edge cases. The opaque nature of some AI models complicates fault diagnosis and recovery."
    },
    {
      "footnote_id": "fn-model-uncertainty",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 284,
      "context": "[^fn-model-uncertainty]: **Model Uncertainty**: The inadequacy of a machine learning model to capture the full complexity o...",
      "full_line": "[^fn-model-uncertainty]: **Model Uncertainty**: The inadequacy of a machine learning model to capture the full complexity of the underlying data-generating process."
    },
    {
      "footnote_id": "fn-single-event-upsets",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 332,
      "context": "Common transient fault types include Single Event Upsets (SEUs)[^fn-single-event-upsets] from cosmic rays and ionizing radiation, voltage fluctuations [@reddi2013resilient] from power supp...",
      "full_line": "Common transient fault types include Single Event Upsets (SEUs)[^fn-single-event-upsets] from cosmic rays and ionizing radiation, voltage fluctuations [@reddi2013resilient] from power supply instability, Electromagnetic Interference (EMI)[^fn-electromagnetic-interference] from external electromagnetic fields, Electrostatic Discharge (ESD) from sudden static electricity flow, crosstalk[^fn-crosstalk] from unintended signal coupling, ground bounce from simultaneous switching of multiple outputs, timing violations from signal timing constraint breaches, and soft errors in combinational logic [@mukherjee2005soft]. Understanding these fault types is crucial for designing robust hardware systems that can mitigate their impact and ensure reliable operation."
    },
    {
      "footnote_id": "fn-electromagnetic-interference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 332,
      "context": "...fluctuations [@reddi2013resilient] from power supply instability, Electromagnetic Interference (EMI)[^fn-electromagnetic-interference] from external electromagnetic fields, Electrostatic Discharge (ESD) from sudden static electricity...",
      "full_line": "Common transient fault types include Single Event Upsets (SEUs)[^fn-single-event-upsets] from cosmic rays and ionizing radiation, voltage fluctuations [@reddi2013resilient] from power supply instability, Electromagnetic Interference (EMI)[^fn-electromagnetic-interference] from external electromagnetic fields, Electrostatic Discharge (ESD) from sudden static electricity flow, crosstalk[^fn-crosstalk] from unintended signal coupling, ground bounce from simultaneous switching of multiple outputs, timing violations from signal timing constraint breaches, and soft errors in combinational logic [@mukherjee2005soft]. Understanding these fault types is crucial for designing robust hardware systems that can mitigate their impact and ensure reliable operation."
    },
    {
      "footnote_id": "fn-crosstalk",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 332,
      "context": "...electromagnetic fields, Electrostatic Discharge (ESD) from sudden static electricity flow, crosstalk[^fn-crosstalk] from unintended signal coupling, ground bounce from simultaneous switching of multiple outputs, tim...",
      "full_line": "Common transient fault types include Single Event Upsets (SEUs)[^fn-single-event-upsets] from cosmic rays and ionizing radiation, voltage fluctuations [@reddi2013resilient] from power supply instability, Electromagnetic Interference (EMI)[^fn-electromagnetic-interference] from external electromagnetic fields, Electrostatic Discharge (ESD) from sudden static electricity flow, crosstalk[^fn-crosstalk] from unintended signal coupling, ground bounce from simultaneous switching of multiple outputs, timing violations from signal timing constraint breaches, and soft errors in combinational logic [@mukherjee2005soft]. Understanding these fault types is crucial for designing robust hardware systems that can mitigate their impact and ensure reliable operation."
    },
    {
      "footnote_id": "fn-single-event-upsets",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 416,
      "context": "[^fn-single-event-upsets]: **Single Event Upsets (SEUs)**: Radiation-induced bit flips in memory or logic caused by cosmic ra...",
      "full_line": "[^fn-single-event-upsets]: **Single Event Upsets (SEUs)**: Radiation-induced bit flips in memory or logic caused by cosmic rays or alpha particles. Modern DRAM exhibits error rates of approximately 1 per 10^17 bits accessed under typical operating conditions [@baumann2005soft]. These rates occur roughly once per gigabit per month at sea level, increasing exponentially with altitude, with commercial aircraft experiencing 300x higher rates. For AI systems processing large datasets, these seemingly low error rates compound significantly - a 1TB model checkpoint experiences an expected 80 bit flips during a single full read operation, making error detection and correction essential for reliable ML training and inference."
    },
    {
      "footnote_id": "fn-electromagnetic-interference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 418,
      "context": "[^fn-electromagnetic-interference]: **Electromagnetic Interference (EMI)**: Disturbance caused by external electromagnetic sources tha...",
      "full_line": "[^fn-electromagnetic-interference]: **Electromagnetic Interference (EMI)**: Disturbance caused by external electromagnetic sources that can disrupt electronic circuits. Common sources include cell phones, WiFi, and nearby switching power supplies, requiring careful shielding in sensitive systems."
    },
    {
      "footnote_id": "fn-crosstalk",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 420,
      "context": "[^fn-crosstalk]: **Crosstalk**: Unwanted signal coupling between adjacent conductors due to parasitic capacitance a...",
      "full_line": "[^fn-crosstalk]: **Crosstalk**: Unwanted signal coupling between adjacent conductors due to parasitic capacitance and inductance. Becomes increasingly problematic as circuit densities increase, potentially causing timing violations and data corruption."
    },
    {
      "footnote_id": "fn-glitches",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 490,
      "context": "...can corrupt the stored data or instructions. In logic circuits, transient faults can cause glitches[^fn-glitches] or voltage spikes propagating through the combinational logic[^fn-combinationallogic], resulting in...",
      "full_line": "Transient faults can manifest through different mechanisms depending on the affected hardware component. In memory devices like DRAM or SRAM, transient faults often lead to bit flips, where a single bit changes its value from 0 to 1 or vice versa. This can corrupt the stored data or instructions. In logic circuits, transient faults can cause glitches[^fn-glitches] or voltage spikes propagating through the combinational logic[^fn-combinationallogic], resulting in incorrect outputs or control signals. Graphics Processing Units (GPUs) used extensively in ML workloads exhibit significantly higher error rates than traditional CPUs, with studies showing GPU error rates 10-1000x higher than CPU errors due to their parallel architecture, higher transistor density, and aggressive voltage/frequency scaling. This disparity makes GPU-accelerated AI systems particularly vulnerable to transient faults during training and inference operations. Transient faults can also affect communication channels, causing bit errors or packet losses during data transmission. In distributed AI training systems, network partitions occur with measurable frequency - studies of large-scale clusters report partition events affecting 1-10% of nodes daily, with recovery times ranging from seconds to hours depending on the partition type and detection mechanisms. These network disruptions can cause training job failures, parameter synchronization issues, and data inconsistencies that require robust distributed coordination protocols to maintain system reliability."
    },
    {
      "footnote_id": "fn-combinationallogic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 490,
      "context": "...aults can cause glitches[^fn-glitches] or voltage spikes propagating through the combinational logic[^fn-combinationallogic], resulting in incorrect outputs or control signals. Graphics Processing Units (GPUs) used extensive...",
      "full_line": "Transient faults can manifest through different mechanisms depending on the affected hardware component. In memory devices like DRAM or SRAM, transient faults often lead to bit flips, where a single bit changes its value from 0 to 1 or vice versa. This can corrupt the stored data or instructions. In logic circuits, transient faults can cause glitches[^fn-glitches] or voltage spikes propagating through the combinational logic[^fn-combinationallogic], resulting in incorrect outputs or control signals. Graphics Processing Units (GPUs) used extensively in ML workloads exhibit significantly higher error rates than traditional CPUs, with studies showing GPU error rates 10-1000x higher than CPU errors due to their parallel architecture, higher transistor density, and aggressive voltage/frequency scaling. This disparity makes GPU-accelerated AI systems particularly vulnerable to transient faults during training and inference operations. Transient faults can also affect communication channels, causing bit errors or packet losses during data transmission. In distributed AI training systems, network partitions occur with measurable frequency - studies of large-scale clusters report partition events affecting 1-10% of nodes daily, with recovery times ranging from seconds to hours depending on the partition type and detection mechanisms. These network disruptions can cause training job failures, parameter synchronization issues, and data inconsistencies that require robust distributed coordination protocols to maintain system reliability."
    },
    {
      "footnote_id": "fn-glitches",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 492,
      "context": "[^fn-glitches]: **Glitches**: Momentary deviation in voltage, current, or signal, often causing incorrect operatio...",
      "full_line": "[^fn-glitches]: **Glitches**: Momentary deviation in voltage, current, or signal, often causing incorrect operation."
    },
    {
      "footnote_id": "fn-combinationallogic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 494,
      "context": "[^fn-combinationallogic]: **Combinational logic**: Digital logic, wherein the output depends only on the current input state...",
      "full_line": "[^fn-combinationallogic]: **Combinational logic**: Digital logic, wherein the output depends only on the current input states, not any past states."
    },
    {
      "footnote_id": "fn-gradients",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 500,
      "context": "...on large datasets. If a transient fault occurs in the memory storing the model weights or gradients[^fn-gradients], it can lead to incorrect updates and compromise the convergence and accuracy of the training proce...",
      "full_line": "In ML systems, transient faults can have significant implications during the training phase [@he2023understanding]. ML training involves iterative computations and updates to model parameters based on large datasets. If a transient fault occurs in the memory storing the model weights or gradients[^fn-gradients], it can lead to incorrect updates and compromise the convergence and accuracy of the training process. For example, a bit flip in the weight matrix of a neural network can cause the model to learn incorrect patterns or associations, leading to degraded performance [@wan2021analyzing]. Transient faults in the data pipeline, such as corruption of training samples or labels, can also introduce noise and affect the quality of the learned model."
    },
    {
      "footnote_id": "fn-stochastic-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 508,
      "context": "...erging approaches such as [stochastic computing](https://en.wikipedia.org/wiki/Stochastic_computing)[^fn-stochastic-computing] are being explored to enhance fault tolerance.",
      "full_line": "Transient faults can be amplified in resource-constrained environments like TinyML, where limited computational and memory resources exacerbate their impact. One prominent example is Binarized Neural Networks (BNNs) [@courbariaux2016binarized], which represent network weights in single-bit precision to achieve computational efficiency and faster inference times. While this binary representation is advantageous for resource-constrained systems, it also makes BNNs particularly fragile to bit-flip errors. For instance, prior work [@Aygun2021BSBNN] has shown that a two-hidden-layer BNN architecture for a simple task such as MNIST classification suffers performance degradation from 98% test accuracy to 70% when random bit-flipping soft errors are inserted through model weights with a 10% probability. To address these vulnerabilities, techniques like flip-aware training and emerging approaches such as [stochastic computing](https://en.wikipedia.org/wiki/Stochastic_computing)[^fn-stochastic-computing] are being explored to enhance fault tolerance."
    },
    {
      "footnote_id": "fn-stochastic-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 510,
      "context": "[^fn-stochastic-computing]: **Stochastic Computing**: A collection of techniques using random bits and logic operations to per...",
      "full_line": "[^fn-stochastic-computing]: **Stochastic Computing**: A collection of techniques using random bits and logic operations to perform arithmetic and data processing, promising better fault tolerance."
    },
    {
      "footnote_id": "fn-lookup-table",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 522,
      "context": "The FDIV bug occurred due to an error in the lookup table[^fn-lookup-table] used by the division unit. In rare cases, the processor would fetch an incorrect value, resulting i...",
      "full_line": "The FDIV bug occurred due to an error in the lookup table[^fn-lookup-table] used by the division unit. In rare cases, the processor would fetch an incorrect value, resulting in a slightly less precise result than expected. For instance, @fig-permanent-fault shows a fraction 4195835/3145727 plotted on a Pentium processor with the FDIV fault. The triangular regions highlight where erroneous calculations occurred. Ideally, all correct values would round to 1.3338, but the faulty results showed 1.3337, indicating a mistake in the 5th digit."
    },
    {
      "footnote_id": "fn-lookup-table",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 524,
      "context": "[^fn-lookup-table]: **Lookup Table**: A data structure used to replace a runtime computation with a simpler array inde...",
      "full_line": "[^fn-lookup-table]: **Lookup Table**: A data structure used to replace a runtime computation with a simpler array indexing operation."
    },
    {
      "footnote_id": "fn-electromigration",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 537,
      "context": "...aging/) occur over time due to prolonged use and operational stress. Phenomena like electromigration[^fn-electromigration], oxide breakdown[^fn-oxide-breakdown], and thermal stress[^fn-thermal-stress] degrade component int...",
      "full_line": "[Manufacturing defects](https://www.sciencedirect.com/science/article/pii/B9780128181058000206) are flaws introduced during the fabrication process, including improper etching, incorrect doping, or contamination. These defects may result in non-functional or partially functional components. In contrast, [wear-out mechanisms](https://semiengineering.com/what-causes-semiconductor-aging/) occur over time due to prolonged use and operational stress. Phenomena like electromigration[^fn-electromigration], oxide breakdown[^fn-oxide-breakdown], and thermal stress[^fn-thermal-stress] degrade component integrity, eventually leading to permanent failure."
    },
    {
      "footnote_id": "fn-oxide-breakdown",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 537,
      "context": "...d use and operational stress. Phenomena like electromigration[^fn-electromigration], oxide breakdown[^fn-oxide-breakdown], and thermal stress[^fn-thermal-stress] degrade component integrity, eventually leading to permanen...",
      "full_line": "[Manufacturing defects](https://www.sciencedirect.com/science/article/pii/B9780128181058000206) are flaws introduced during the fabrication process, including improper etching, incorrect doping, or contamination. These defects may result in non-functional or partially functional components. In contrast, [wear-out mechanisms](https://semiengineering.com/what-causes-semiconductor-aging/) occur over time due to prolonged use and operational stress. Phenomena like electromigration[^fn-electromigration], oxide breakdown[^fn-oxide-breakdown], and thermal stress[^fn-thermal-stress] degrade component integrity, eventually leading to permanent failure."
    },
    {
      "footnote_id": "fn-thermal-stress",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 537,
      "context": "...ike electromigration[^fn-electromigration], oxide breakdown[^fn-oxide-breakdown], and thermal stress[^fn-thermal-stress] degrade component integrity, eventually leading to permanent failure.",
      "full_line": "[Manufacturing defects](https://www.sciencedirect.com/science/article/pii/B9780128181058000206) are flaws introduced during the fabrication process, including improper etching, incorrect doping, or contamination. These defects may result in non-functional or partially functional components. In contrast, [wear-out mechanisms](https://semiengineering.com/what-causes-semiconductor-aging/) occur over time due to prolonged use and operational stress. Phenomena like electromigration[^fn-electromigration], oxide breakdown[^fn-oxide-breakdown], and thermal stress[^fn-thermal-stress] degrade component integrity, eventually leading to permanent failure."
    },
    {
      "footnote_id": "fn-electromigration",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 539,
      "context": "[^fn-electromigration]: The movement of metal atoms in a conductor under the influence of an electric field.",
      "full_line": "[^fn-electromigration]: The movement of metal atoms in a conductor under the influence of an electric field."
    },
    {
      "footnote_id": "fn-oxide-breakdown",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 541,
      "context": "[^fn-oxide-breakdown]: The failure of an oxide layer in a transistor due to excessive electric field stress.",
      "full_line": "[^fn-oxide-breakdown]: The failure of an oxide layer in a transistor due to excessive electric field stress."
    },
    {
      "footnote_id": "fn-thermal-stress",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 543,
      "context": "[^fn-thermal-stress]: **Thermal Stress**: Degradation caused by repeated cycling through high and low temperatures. Mode...",
      "full_line": "[^fn-thermal-stress]: **Thermal Stress**: Degradation caused by repeated cycling through high and low temperatures. Modern AI accelerators commonly experience thermal throttling under sustained workloads, leading to performance degradation of 20-60% as processors reduce clock speeds to prevent overheating. This throttling directly impacts ML training times and inference throughput, making thermal management critical for maintaining consistent AI system performance in production environments."
    },
    {
      "footnote_id": "fn-error-correcting-codes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 640,
      "context": "...s.  Hardware-level methods include component redundancy and error-correcting codes [@kim2015bamboo].[^fn-error-correcting-codes]  Software approaches, like checkpoint and restart mechanisms[^fn-checkpoint-restart] [@egwutuoha201...",
      "full_line": "To mitigate these impacts, ML systems must incorporate both hardware and software fault-tolerant techniques.  Hardware-level methods include component redundancy and error-correcting codes [@kim2015bamboo].[^fn-error-correcting-codes]  Software approaches, like checkpoint and restart mechanisms[^fn-checkpoint-restart] [@egwutuoha2013survey], allow systems to recover to a known-good state after a failure. Regular monitoring, testing, and maintenance can also help detect and replace failing components before critical errors occur."
    },
    {
      "footnote_id": "fn-checkpoint-restart",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 640,
      "context": "...015bamboo].[^fn-error-correcting-codes]  Software approaches, like checkpoint and restart mechanisms[^fn-checkpoint-restart] [@egwutuoha2013survey], allow systems to recover to a known-good state after a failure. Regular mon...",
      "full_line": "To mitigate these impacts, ML systems must incorporate both hardware and software fault-tolerant techniques.  Hardware-level methods include component redundancy and error-correcting codes [@kim2015bamboo].[^fn-error-correcting-codes]  Software approaches, like checkpoint and restart mechanisms[^fn-checkpoint-restart] [@egwutuoha2013survey], allow systems to recover to a known-good state after a failure. Regular monitoring, testing, and maintenance can also help detect and replace failing components before critical errors occur."
    },
    {
      "footnote_id": "fn-error-correcting-codes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 642,
      "context": "[^fn-error-correcting-codes]: **Error-Correcting Codes**: Methods used in data storage and transmission to detect and correct er...",
      "full_line": "[^fn-error-correcting-codes]: **Error-Correcting Codes**: Methods used in data storage and transmission to detect and correct errors."
    },
    {
      "footnote_id": "fn-checkpoint-restart",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 644,
      "context": "[^fn-checkpoint-restart]: **Checkpoint and Restart Mechanisms**: Techniques that periodically save a program's state so it c...",
      "full_line": "[^fn-checkpoint-restart]: **Checkpoint and Restart Mechanisms**: Techniques that periodically save a program's state so it can resume from the last saved state after a failure."
    },
    {
      "footnote_id": "fn-scan-chains",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 706,
      "context": "...c integrated circuits (ASICs). For example, BIST can be implemented in a processor using scan chains[^fn-scan-chains], which are dedicated paths that allow access to internal registers and logic for testing purposes.",
      "full_line": "BIST is a powerful technique for detecting faults in hardware components [@bushnell2002built]. It involves incorporating additional hardware circuitry into the system for self-testing and fault detection. BIST can be applied to various components, such as processors, memory modules, or application-specific integrated circuits (ASICs). For example, BIST can be implemented in a processor using scan chains[^fn-scan-chains], which are dedicated paths that allow access to internal registers and logic for testing purposes."
    },
    {
      "footnote_id": "fn-scan-chains",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 708,
      "context": "[^fn-scan-chains]: **Scan Chains**: Dedicated paths incorporated within a processor that grant access to internal reg...",
      "full_line": "[^fn-scan-chains]: **Scan Chains**: Dedicated paths incorporated within a processor that grant access to internal registers and logic for testing."
    },
    {
      "footnote_id": "fn-hamming1950error",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 817,
      "context": "...r detection codes are widely used to detect data storage and transmission errors [@hamming1950error][^fn-hamming1950error]. These codes add redundant bits to the original data, allowing the detection of bit errors. Example...",
      "full_line": "Error detection codes are widely used to detect data storage and transmission errors [@hamming1950error][^fn-hamming1950error]. These codes add redundant bits to the original data, allowing the detection of bit errors. Example: Parity checks are a simple form of error detection code shown in @fig-parity[^fn-parity]. In a single-bit parity scheme, an extra bit is appended to each data word, making the number of 1s in the word even (even parity) or odd (odd parity)."
    },
    {
      "footnote_id": "fn-parity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 817,
      "context": "...of bit errors. Example: Parity checks are a simple form of error detection code shown in @fig-parity[^fn-parity]. In a single-bit parity scheme, an extra bit is appended to each data word, making the number of 1s...",
      "full_line": "Error detection codes are widely used to detect data storage and transmission errors [@hamming1950error][^fn-hamming1950error]. These codes add redundant bits to the original data, allowing the detection of bit errors. Example: Parity checks are a simple form of error detection code shown in @fig-parity[^fn-parity]. In a single-bit parity scheme, an extra bit is appended to each data word, making the number of 1s in the word even (even parity) or odd (odd parity)."
    },
    {
      "footnote_id": "fn-hamming1950error",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 819,
      "context": "[^fn-hamming1950error]: R. W. Hamming's seminal paper introduced error detection and correction codes, significantly advan...",
      "full_line": "[^fn-hamming1950error]: R. W. Hamming's seminal paper introduced error detection and correction codes, significantly advancing digital communication reliability."
    },
    {
      "footnote_id": "fn-parity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 821,
      "context": "[^fn-parity]: In parity checks, an extra bit accounts for the total number of 1s in a data word, enabling basic...",
      "full_line": "[^fn-parity]: In parity checks, an extra bit accounts for the total number of 1s in a data word, enabling basic error detection."
    },
    {
      "footnote_id": "fn-dmr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 827,
      "context": "...and mask faults [@sheaffer2007hardware]. Voting mechanisms, such as double modular redundancy (DMR)[^fn-dmr] or triple modular redundancy (TMR)[^fn-tmr], employ multiple instances of a component and compare t...",
      "full_line": "Hardware redundancy involves duplicating critical components and comparing their outputs to detect and mask faults [@sheaffer2007hardware]. Voting mechanisms, such as double modular redundancy (DMR)[^fn-dmr] or triple modular redundancy (TMR)[^fn-tmr], employ multiple instances of a component and compare their outputs to identify and mask faulty behavior [@arifeen2020approximate]."
    },
    {
      "footnote_id": "fn-tmr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 827,
      "context": "...ting mechanisms, such as double modular redundancy (DMR)[^fn-dmr] or triple modular redundancy (TMR)[^fn-tmr], employ multiple instances of a component and compare their outputs to identify and mask faulty beh...",
      "full_line": "Hardware redundancy involves duplicating critical components and comparing their outputs to detect and mask faults [@sheaffer2007hardware]. Voting mechanisms, such as double modular redundancy (DMR)[^fn-dmr] or triple modular redundancy (TMR)[^fn-tmr], employ multiple instances of a component and compare their outputs to identify and mask faulty behavior [@arifeen2020approximate]."
    },
    {
      "footnote_id": "fn-dmr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 829,
      "context": "[^fn-dmr]: **Double Modular Redundancy (DMR)**: A fault-tolerance process in which computations are duplicate...",
      "full_line": "[^fn-dmr]: **Double Modular Redundancy (DMR)**: A fault-tolerance process in which computations are duplicated to identify and correct errors."
    },
    {
      "footnote_id": "fn-tmr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 831,
      "context": "[^fn-tmr]: **Triple Modular Redundancy (TMR)**: A fault-tolerance process where three instances of a computat...",
      "full_line": "[^fn-tmr]: **Triple Modular Redundancy (TMR)**: A fault-tolerance process where three instances of a computation are performed to identify and correct errors."
    },
    {
      "footnote_id": "fn-hot-spares",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 851,
      "context": "Another approach to hardware redundancy is the use of hot spares[^fn-hot-spares], as employed by Google in its data centers to address SDC during ML training. Unlike DMR and TMR, w...",
      "full_line": "Another approach to hardware redundancy is the use of hot spares[^fn-hot-spares], as employed by Google in its data centers to address SDC during ML training. Unlike DMR and TMR, which rely on parallel processing and voting mechanisms to detect and mask faults, hot spares provide fault tolerance by maintaining backup hardware units that can seamlessly take over computations when a fault is detected. As illustrated in @fig-sdc-controller, during normal ML training, multiple synchronous training workers process data in parallel. However, if a worker becomes defective and causes SDC, an SDC checker automatically identifies the issues. Upon detecting the SDC, the SDC checker moves the training to a hot spare and sends the defective machine for repair. This redundancy safeguards the continuity and reliability of ML training, effectively minimizing downtime and preserving data integrity."
    },
    {
      "footnote_id": "fn-hot-spares",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 853,
      "context": "[^fn-hot-spares]: **Hot Spares**: In a system redundancy design, these are the backup components kept ready to insta...",
      "full_line": "[^fn-hot-spares]: **Hot Spares**: In a system redundancy design, these are the backup components kept ready to instantaneously replace failing components without disrupting the operation."
    },
    {
      "footnote_id": "fn-nn-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1421,
      "context": "This vulnerability stems from several characteristics of neural network learning[^fn-nn-learning]. **High-dimensional input spaces** provide numerous dimensions that attackers can exploit simultane...",
      "full_line": "This vulnerability stems from several characteristics of neural network learning[^fn-nn-learning]. **High-dimensional input spaces** provide numerous dimensions that attackers can exploit simultaneously. Even if perturbations in individual dimensions are imperceptibly small, their cumulative effect across hundreds of thousands of dimensions can be significant. **Non-linear decision boundaries** create complex separations between classes that may have narrow margins or sharp transitions, making models sensitive to precise input modifications. **Optimization-based training** focuses on minimizing loss over training examples but may not encourage smooth, robust decision boundaries that generalize well to adversarial modifications."
    },
    {
      "footnote_id": "fn-nn-theory",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1423,
      "context": "...rely a software bug or training artifact. The theoretical foundations explaining why neural networks[^fn-nn-theory] are inherently vulnerable to adversarial perturbations are comprehensively detailed in @sec-dl-prim...",
      "full_line": "Understanding why adversarial examples exist is crucial for developing effective defenses. The vulnerability reflects core properties of how neural networks represent and process information in high-dimensional spaces, rather than being merely a software bug or training artifact. The theoretical foundations explaining why neural networks[^fn-nn-theory] are inherently vulnerable to adversarial perturbations are comprehensively detailed in @sec-dl-primer."
    },
    {
      "footnote_id": "fn-fgsm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1441,
      "context": "The Fast Gradient Sign Method[^fn-fgsm] exemplifies the elegance and danger of gradient-based attacks[^fn-gradient-based-attacks]. FGSM tak...",
      "full_line": "The Fast Gradient Sign Method[^fn-fgsm] exemplifies the elegance and danger of gradient-based attacks[^fn-gradient-based-attacks]. FGSM takes the conceptually simple approach of moving in the direction that most rapidly increases the model's prediction error."
    },
    {
      "footnote_id": "fn-gradient-based-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1441,
      "context": "...he Fast Gradient Sign Method[^fn-fgsm] exemplifies the elegance and danger of gradient-based attacks[^fn-gradient-based-attacks]. FGSM takes the conceptually simple approach of moving in the direction that most rapidly increases...",
      "full_line": "The Fast Gradient Sign Method[^fn-fgsm] exemplifies the elegance and danger of gradient-based attacks[^fn-gradient-based-attacks]. FGSM takes the conceptually simple approach of moving in the direction that most rapidly increases the model's prediction error."
    },
    {
      "footnote_id": "fn-gradient-based-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1443,
      "context": "[^fn-gradient-based-attacks]: **Gradient-Based Attacks**: Adversarial techniques that use the model's gradients to craft perturb...",
      "full_line": "[^fn-gradient-based-attacks]: **Gradient-Based Attacks**: Adversarial techniques that use the model's gradients to craft perturbations. Discovered by Ian Goodfellow in 2014, these attacks revealed that neural networks are vulnerable to imperceptible input modifications, spurring an entire research field in adversarial machine learning."
    },
    {
      "footnote_id": "fn-fgsm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1445,
      "context": "[^fn-fgsm]: **Fast Gradient Sign Method (FGSM)**: The first practical adversarial attack method, proposed by G...",
      "full_line": "[^fn-fgsm]: **Fast Gradient Sign Method (FGSM)**: The first practical adversarial attack method, proposed by Goodfellow et al. in 2014. Generates adversarial examples in a single step by moving in the direction of the gradient's sign, making it computationally efficient but often less effective than iterative methods."
    },
    {
      "footnote_id": "fn-white-box-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1471,
      "context": "Gradient-based attacks are particularly effective in white-box settings[^fn-white-box-attacks], where the attacker has access to the model's architecture and gradients. Their efficiency and rela...",
      "full_line": "Gradient-based attacks are particularly effective in white-box settings[^fn-white-box-attacks], where the attacker has access to the model's architecture and gradients. Their efficiency and relative simplicity have made them popular tools for both attacking and evaluating model robustness in research."
    },
    {
      "footnote_id": "fn-white-box-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1473,
      "context": "[^fn-white-box-attacks]: **White-Box Attacks**: Adversarial attacks where the attacker has complete knowledge of the target...",
      "full_line": "[^fn-white-box-attacks]: **White-Box Attacks**: Adversarial attacks where the attacker has complete knowledge of the target model, including architecture, weights, and training data. More powerful than black-box attacks but less realistic in practice, as attackers rarely have full model access."
    },
    {
      "footnote_id": "fn-carlini-wagner",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1477,
      "context": "...arial examples as an optimization problem. The Carlini and Wagner (C&W) attack [@carlini2017towards][^fn-carlini-wagner] is a prominent example in this category. It finds the smallest perturbation that can cause misclass...",
      "full_line": "These attacks formulate the generation of adversarial examples as an optimization problem. The Carlini and Wagner (C&W) attack [@carlini2017towards][^fn-carlini-wagner] is a prominent example in this category. It finds the smallest perturbation that can cause misclassification while maintaining the perceptual similarity to the original input. The C&W attack employs an iterative optimization process to minimize the perturbation while maximizing the model's prediction error. It uses a customized loss function with a confidence term to generate more confident misclassifications."
    },
    {
      "footnote_id": "fn-carlini-wagner",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1479,
      "context": "[^fn-carlini-wagner]: **Carlini and Wagner (C&W) Attack**: Developed in 2017, this sophisticated attack method finds min...",
      "full_line": "[^fn-carlini-wagner]: **Carlini and Wagner (C&W) Attack**: Developed in 2017, this sophisticated attack method finds minimal perturbations by solving an optimization problem with carefully designed loss functions. Often considered the strongest white-box attack, it successfully breaks many defensive mechanisms that stop simpler attacks."
    },
    {
      "footnote_id": "fn-transferability",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1489,
      "context": "Transfer-based attacks exploit the transferability property[^fn-transferability] of adversarial examples. Transferability refers to the phenomenon where adversarial examples crafte...",
      "full_line": "Transfer-based attacks exploit the transferability property[^fn-transferability] of adversarial examples. Transferability refers to the phenomenon where adversarial examples crafted for one ML model can often fool other models, even if they have different architectures or were trained on different datasets. This enables attackers to generate adversarial examples using a surrogate model and then transfer them to the target model without requiring direct access to its parameters or gradients."
    },
    {
      "footnote_id": "fn-transferability",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1491,
      "context": "[^fn-transferability]: **Transferability**: A surprising property discovered in 2015 showing that adversarial examples of...",
      "full_line": "[^fn-transferability]: **Transferability**: A surprising property discovered in 2015 showing that adversarial examples often transfer between different neural networks. Success rates typically range from 30-70% across models, enabling practical black-box attacks without direct model access."
    },
    {
      "footnote_id": "fn-data-poisoning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1575,
      "context": "Data poisoning[^fn-data-poisoning] is an attack in which the training data is deliberately manipulated to compromise the performance o...",
      "full_line": "Data poisoning[^fn-data-poisoning] is an attack in which the training data is deliberately manipulated to compromise the performance or behavior of a machine learning model, as described in [@biggio2012poisoning] and illustrated in @fig-dirty-label-example. Attackers may alter existing training samples, introduce malicious examples, or interfere with the data collection pipeline. The result is a model that learns biased, inaccurate, or exploitable patterns."
    },
    {
      "footnote_id": "fn-data-poisoning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1577,
      "context": "[^fn-data-poisoning]: **Data Poisoning**: Attack method first formalized by Biggio et al. in 2012, where adversaries inj...",
      "full_line": "[^fn-data-poisoning]: **Data Poisoning**: Attack method first formalized by Biggio et al. in 2012, where adversaries inject malicious samples into training data to compromise model behavior. Unlike adversarial examples that target inference, poisoning attacks the learning process itself, making them harder to detect and defend against."
    },
    {
      "footnote_id": "fn-backdoor-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1593,
      "context": "Backdoor poisoning[^fn-backdoor-attacks] introduces hidden triggers into training data\u2014subtle patterns or features that the model learns to...",
      "full_line": "Backdoor poisoning[^fn-backdoor-attacks] introduces hidden triggers into training data\u2014subtle patterns or features that the model learns to associate with a particular output. When the trigger appears at inference time, the model is manipulated into producing a predetermined response. These attacks are often effective even if the trigger pattern is imperceptible to human observers."
    },
    {
      "footnote_id": "fn-backdoor-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1595,
      "context": "[^fn-backdoor-attacks]: **Backdoor Attacks**: Introduced by Gu et al. in 2017, these attacks embed hidden triggers in trai...",
      "full_line": "[^fn-backdoor-attacks]: **Backdoor Attacks**: Introduced by Gu et al. in 2017, these attacks embed hidden triggers in training data that activate malicious behavior when specific patterns appear at inference time. Success rates can exceed 99% while maintaining normal accuracy on clean inputs, making them particularly dangerous."
    },
    {
      "footnote_id": "fn-dualusedilemma",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2123,
      "context": "...ent could be repurposed to sabotage legitimate training pipelines, highlighting the dual-use dilemma[^fn-dualusedilemma] at the heart of modern machine learning security.",
      "full_line": "However, like any powerful tool, Nightshade also introduces risks. The same technique used to protect artistic content could be repurposed to sabotage legitimate training pipelines, highlighting the dual-use dilemma[^fn-dualusedilemma] at the heart of modern machine learning security."
    },
    {
      "footnote_id": "fn-dualusedilemma",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2125,
      "context": "[^fn-dualusedilemma]: **Dual-use Dilemma**: In AI, the challenge of mitigating misuse of technology that has both positi...",
      "full_line": "[^fn-dualusedilemma]: **Dual-use Dilemma**: In AI, the challenge of mitigating misuse of technology that has both positive and negative potential uses."
    },
    {
      "footnote_id": "fn-bayesian-nn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2397,
      "context": "Bayesian neural networks[^fn-bayesian-nn] provide the most principled uncertainty estimates by treating model weights as probability distribu...",
      "full_line": "Bayesian neural networks[^fn-bayesian-nn] provide the most principled uncertainty estimates by treating model weights as probability distributions, capturing both aleatoric (data inherent) and epistemic (model) uncertainty through approximate inference methods. Ensemble methods achieve uncertainty estimation by combining predictions from multiple independently trained models, using prediction variance as an uncertainty measure. While both approaches offer robust uncertainty quantification, they incur significant computational overhead."
    },
    {
      "footnote_id": "fn-dropout",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2399,
      "context": "Dropout[^fn-dropout], originally designed as a regularization technique to prevent overfitting during training [@hinton2...",
      "full_line": "Dropout[^fn-dropout], originally designed as a regularization technique to prevent overfitting during training [@hinton2012improvingneuralnetworkspreventing], works by randomly deactivating a fraction of neurons during each training iteration, forcing the network to avoid over-reliance on specific neurons and improving generalization. This mechanism can be repurposed for uncertainty estimation through Monte Carlo dropout at inference time, where multiple forward passes with different dropout masks approximate the uncertainty distribution. However, this approach provides less precise uncertainty estimates since dropout was not specifically designed for uncertainty quantification but rather for preventing overfitting through enforced redundancy. Hybrid approaches that combine dropout with lightweight ensemble methods or Bayesian approximations can balance computational efficiency with estimation quality, making uncertainty-based detection more practical for real-world deployment."
    },
    {
      "footnote_id": "fn-autoencoders",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2611,
      "context": "Autoencoders[^fn-autoencoders] are neural networks trained to reconstruct the input data from a compressed representation, as show...",
      "full_line": "Autoencoders[^fn-autoencoders] are neural networks trained to reconstruct the input data from a compressed representation, as shown in @fig-autoencoder. They can be used for anomaly detection by learning the normal patterns in the data and identifying instances that deviate from them. During training, the autoencoder is trained on clean, unpoisoned data. At inference time, the reconstruction error for each data point is computed. Data points with high reconstruction errors are considered abnormal and potentially poisoned, as they do not conform to the learned normal patterns."
    },
    {
      "footnote_id": "fn-huber-loss",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2778,
      "context": "...t loss functions less sensitive to extreme values, such as the Huber loss or the modified Huber loss[^fn-huber-loss]. Regularization techniques[^fn-regularization], such as [L1 or L2 regularization](https://medium.co...",
      "full_line": "Robust optimization techniques can be used to modify the training objective to minimize the impact of outliers or poisoned instances. This can be achieved by using robust loss functions less sensitive to extreme values, such as the Huber loss or the modified Huber loss[^fn-huber-loss]. Regularization techniques[^fn-regularization], such as [L1 or L2 regularization](https://medium.com/towards-data-science/l1-and-l2-regularization-methods-ce25e7fc831c), can also help in reducing the model's sensitivity to poisoned data by constraining the model's complexity and preventing overfitting."
    },
    {
      "footnote_id": "fn-regularization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2778,
      "context": "...values, such as the Huber loss or the modified Huber loss[^fn-huber-loss]. Regularization techniques[^fn-regularization], such as [L1 or L2 regularization](https://medium.com/towards-data-science/l1-and-l2-regularization...",
      "full_line": "Robust optimization techniques can be used to modify the training objective to minimize the impact of outliers or poisoned instances. This can be achieved by using robust loss functions less sensitive to extreme values, such as the Huber loss or the modified Huber loss[^fn-huber-loss]. Regularization techniques[^fn-regularization], such as [L1 or L2 regularization](https://medium.com/towards-data-science/l1-and-l2-regularization-methods-ce25e7fc831c), can also help in reducing the model's sensitivity to poisoned data by constraining the model's complexity and preventing overfitting."
    },
    {
      "footnote_id": "fn-huber-loss",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2780,
      "context": "[^fn-huber-loss]: **Huber Loss**: A loss function used in robust regression that is less sensitive to outliers in da...",
      "full_line": "[^fn-huber-loss]: **Huber Loss**: A loss function used in robust regression that is less sensitive to outliers in data than squared error loss."
    },
    {
      "footnote_id": "fn-regularization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2782,
      "context": "[^fn-regularization]: **Regularization**: A method used in neural networks to prevent overfitting in models by adding a...",
      "full_line": "[^fn-regularization]: **Regularization**: A method used in neural networks to prevent overfitting in models by adding a cost term to the loss function."
    },
    {
      "footnote_id": "fn-minimax",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2784,
      "context": "...ducing their impact on the model's learning process. Robust objective functions, such as the minimax[^fn-minimax] or distributionally robust objective, aim to optimize the model's performance under worst-case scen...",
      "full_line": "Robust loss functions are designed to be less sensitive to outliers or noisy data points. Examples include the modified [Huber loss](https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html), the Tukey loss [@beaton1974fitting], and the trimmed mean loss. These loss functions down-weight or ignore the contribution of abnormal instances during training, reducing their impact on the model's learning process. Robust objective functions, such as the minimax[^fn-minimax] or distributionally robust objective, aim to optimize the model's performance under worst-case scenarios or in the presence of adversarial perturbations."
    },
    {
      "footnote_id": "fn-minimax",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2786,
      "context": "[^fn-minimax]: **Minimax**: A decision-making strategy, used in game theory and decision theory, which tries to m...",
      "full_line": "[^fn-minimax]: **Minimax**: A decision-making strategy, used in game theory and decision theory, which tries to minimize the maximum possible loss."
    },
    {
      "footnote_id": "fn-principle-least-privilege",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2796,
      "context": "...ies for data access, implementing access control policies based on the principle of least privilege,[^fn-principle-least-privilege] and monitoring and logging data access activities. By restricting access to the training data and m...",
      "full_line": "Strong data governance and access control mechanisms are essential to prevent unauthorized modifications or tampering with the training data. This involves defining clear roles and responsibilities for data access, implementing access control policies based on the principle of least privilege,[^fn-principle-least-privilege] and monitoring and logging data access activities. By restricting access to the training data and maintaining an audit trail, potential data poisoning attempts can be detected and investigated."
    },
    {
      "footnote_id": "fn-principle-least-privilege",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2798,
      "context": "[^fn-principle-least-privilege]: **Principle of Least Privilege**: A security concept in which a user is given the minimum levels o...",
      "full_line": "[^fn-principle-least-privilege]: **Principle of Least Privilege**: A security concept in which a user is given the minimum levels of access necessary to complete his/her job functions."
    },
    {
      "footnote_id": "fn-data-sanitization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2800,
      "context": "...isoning attacks requires a multifaceted approach that combines anomaly detection, data sanitization,[^fn-data-sanitization] robust training techniques, and secure data sourcing practices. By implementing these measures, ML...",
      "full_line": "Detecting and mitigating data poisoning attacks requires a multifaceted approach that combines anomaly detection, data sanitization,[^fn-data-sanitization] robust training techniques, and secure data sourcing practices. By implementing these measures, ML practitioners can improve the resilience of their models against data poisoning and ensure the integrity and trustworthiness of the training data. Data poisoning is an active area of research, and new attack vectors and defense mechanisms continue to emerge. Staying informed about the latest developments and adopting a proactive and adaptive approach to data security is crucial for maintaining the robustness of ML systems."
    },
    {
      "footnote_id": "fn-data-sanitization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2802,
      "context": "[^fn-data-sanitization]: **Data Sanitization**: The process of deliberately, permanently, and irreversibly removing or dest...",
      "full_line": "[^fn-data-sanitization]: **Data Sanitization**: The process of deliberately, permanently, and irreversibly removing or destroying the data stored on a memory device to make it unrecoverable."
    },
    {
      "footnote_id": "fn-bayesian-neural-networks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2853,
      "context": "Uncertainty quantification techniques, such as Bayesian neural networks[^fn-bayesian-neural-networks] or ensemble methods[^fn-ensemble-methods], can estimate the uncertainty associated with the model's...",
      "full_line": "Uncertainty quantification techniques, such as Bayesian neural networks[^fn-bayesian-neural-networks] or ensemble methods[^fn-ensemble-methods], can estimate the uncertainty associated with the model's predictions. When a model is applied to data from a different distribution, its predictions may have higher uncertainty. By monitoring the uncertainty levels, practitioners can detect distribution shifts. If the uncertainty consistently exceeds a predetermined threshold for test samples, it suggests that the model is operating outside its trained distribution."
    },
    {
      "footnote_id": "fn-ensemble-methods",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2853,
      "context": "...ation techniques, such as Bayesian neural networks[^fn-bayesian-neural-networks] or ensemble methods[^fn-ensemble-methods], can estimate the uncertainty associated with the model's predictions. When a model is applied to d...",
      "full_line": "Uncertainty quantification techniques, such as Bayesian neural networks[^fn-bayesian-neural-networks] or ensemble methods[^fn-ensemble-methods], can estimate the uncertainty associated with the model's predictions. When a model is applied to data from a different distribution, its predictions may have higher uncertainty. By monitoring the uncertainty levels, practitioners can detect distribution shifts. If the uncertainty consistently exceeds a predetermined threshold for test samples, it suggests that the model is operating outside its trained distribution."
    },
    {
      "footnote_id": "fn-bayesian-neural-networks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2855,
      "context": "[^fn-bayesian-neural-networks]: **Bayesian Neural Networks**: Neural networks that incorporate probability distributions over thei...",
      "full_line": "[^fn-bayesian-neural-networks]: **Bayesian Neural Networks**: Neural networks that incorporate probability distributions over their weights, enabling uncertainty quantification in predictions and more robust decision making."
    },
    {
      "footnote_id": "fn-ensemble-methods",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2857,
      "context": "[^fn-ensemble-methods]: **Ensemble Methods**: An ML approach that combines several models to improve prediction accuracy.",
      "full_line": "[^fn-ensemble-methods]: **Ensemble Methods**: An ML approach that combines several models to improve prediction accuracy."
    },
    {
      "footnote_id": "fn-f1",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2926,
      "context": "...model performance. Metrics such as the area under the precision-recall curve (AUPRC) or the F1 score[^fn-f1] are more robust to class imbalance and can better capture the model's performance across different...",
      "full_line": "Evaluating models using robust metrics less sensitive to distribution shifts can provide a more reliable assessment of model performance. Metrics such as the area under the precision-recall curve (AUPRC) or the F1 score[^fn-f1] are more robust to class imbalance and can better capture the model's performance across different distributions. Additionally, using domain-specific evaluation metrics that align with the desired outcomes in the target domain can provide a more meaningful measure of the model's effectiveness."
    },
    {
      "footnote_id": "fn-f1",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2928,
      "context": "[^fn-f1]: **F1 Score**: A measure of a model's accuracy that combines precision (correct positive prediction...",
      "full_line": "[^fn-f1]: **F1 Score**: A measure of a model's accuracy that combines precision (correct positive predictions) and recall (proportion of actual positives identified) into a single metric. Calculated as the harmonic mean of precision and recall."
    },
    {
      "footnote_id": "fn-fault-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3520,
      "context": "...tegies detailed in @sec-ml-operations. This section provides an overview of widely used fault models[^fn-fault-models] in the literature and the tools and frameworks developed to evaluate the impact of such faults on M...",
      "full_line": "Given the importance of developing robust AI systems, in recent years, researchers and practitioners have developed a wide range of tools and frameworks building on the software infrastructure from @sec-ai-frameworks to understand how hardware faults manifest and propagate to impact ML systems. These tools and frameworks play a crucial role in evaluating the resilience of ML systems to hardware faults by simulating various fault scenarios and analyzing their impact on the system's performance, complementing the evaluation methodologies described in @sec-benchmarking-ai. This enables designers to identify potential vulnerabilities and develop effective mitigation strategies, ultimately creating more robust and reliable ML systems that can operate safely despite hardware faults, supporting the deployment strategies detailed in @sec-ml-operations. This section provides an overview of widely used fault models[^fn-fault-models] in the literature and the tools and frameworks developed to evaluate the impact of such faults on ML systems."
    },
    {
      "footnote_id": "fn-fault-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3522,
      "context": "[^fn-fault-models]: **Fault Models**: Formal specifications describing how hardware faults manifest and propagate thro...",
      "full_line": "[^fn-fault-models]: **Fault Models**: Formal specifications describing how hardware faults manifest and propagate through systems. Examples include stuck-at models (bits permanently 0 or 1), single-bit flip models (temporary bit inversions), and Byzantine models (arbitrary malicious behavior). Essential for designing realistic fault injection experiments."
    },
    {
      "footnote_id": "fn-fpga",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3631,
      "context": "**FPGA-based Fault Injection.** Field-Programmable Gate Arrays (FPGAs)[^fn-fpga] are reconfigurable integrated circuits that can be programmed to implement various hardware designs...",
      "full_line": "**FPGA-based Fault Injection.** Field-Programmable Gate Arrays (FPGAs)[^fn-fpga] are reconfigurable integrated circuits that can be programmed to implement various hardware designs. In the context of fault injection, FPGAs offer high precision and accuracy, as researchers can target specific bits or sets of bits within the hardware. By modifying the FPGA configuration, faults can be introduced at specific locations and times during the execution of an ML model. FPGA-based fault injection allows for fine-grained control over the fault model, enabling researchers to study the impact of different types of faults, such as single-bit flips or multi-bit errors. This level of control makes FPGA-based fault injection a valuable tool for understanding the resilience of ML systems to hardware faults."
    },
    {
      "footnote_id": "fn-fpga",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3633,
      "context": "[^fn-fpga]: **Field-Programmable Gate Arrays (FPGAs)**: Reconfigurable hardware devices containing millions of...",
      "full_line": "[^fn-fpga]: **Field-Programmable Gate Arrays (FPGAs)**: Reconfigurable hardware devices containing millions of logic blocks that can be programmed to implement custom digital circuits. Originally developed by Xilinx in 1985, FPGAs bridge the gap between software flexibility and hardware performance, enabling rapid prototyping and specialized accelerators."
    },
    {
      "footnote_id": "fn-beam_testing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3645,
      "context": "First, cost is a major barrier. Both FPGA-based and beam testing[^fn-beam_testing] approaches require specialized hardware and facilities, which can be expensive to set up and mainta...",
      "full_line": "First, cost is a major barrier. Both FPGA-based and beam testing[^fn-beam_testing] approaches require specialized hardware and facilities, which can be expensive to set up and maintain. This makes them less accessible to research groups with limited funding or infrastructure."
    },
    {
      "footnote_id": "fn-beam_testing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3647,
      "context": "[^fn-beam_testing]: **Beam Testing**: A testing method that exposes hardware to controlled particle radiation to evalu...",
      "full_line": "[^fn-beam_testing]: **Beam Testing**: A testing method that exposes hardware to controlled particle radiation to evaluate its resilience to soft errors. Common in aerospace, medical devices, and high-reliability computing."
    },
    {
      "footnote_id": "fn-multimodal-sensor-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3715,
      "context": "...ith domain-specific features. These include the ability to inject faults into multimodal sensor data[^fn-multimodal-sensor-data], such as inputs from cameras and LiDAR systems. This allows for a deeper examination of how percept...",
      "full_line": "PyTorchALFI [@grafe2023large] extends the capabilities of PyTorchFI for use in the autonomous vehicle domain. Developed by Intel xColabs, PyTorchALFI enhances the underlying fault injection framework with domain-specific features. These include the ability to inject faults into multimodal sensor data[^fn-multimodal-sensor-data], such as inputs from cameras and LiDAR systems. This allows for a deeper examination of how perception systems in autonomous vehicles respond to underlying hardware faults, further refining our understanding of system vulnerabilities and potential failure modes."
    },
    {
      "footnote_id": "fn-multimodal-sensor-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3717,
      "context": "[^fn-multimodal-sensor-data]: **Multimodal Sensor Data**: Information collected simultaneously from multiple types of sensors (e...",
      "full_line": "[^fn-multimodal-sensor-data]: **Multimodal Sensor Data**: Information collected simultaneously from multiple types of sensors (e.g., cameras, LiDAR, radar) to provide complementary perspectives of the environment. Critical for robust perception in autonomous systems."
    },
    {
      "footnote_id": "fn-gradients",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3719,
      "context": "[^fn-gradients]: **Gradients and Convergence**: Core training concepts where gradients are mathematical derivatives...",
      "full_line": "[^fn-gradients]: **Gradients and Convergence**: Core training concepts where gradients are mathematical derivatives indicating how to adjust model parameters, and convergence refers to the training process reaching a stable, optimal solution. These fundamental concepts are covered in detail in @sec-ai-training."
    },
    {
      "footnote_id": "fn-nn-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3721,
      "context": "[^fn-nn-learning]: **Neural Network Learning Mechanisms**: The fundamental processes by which neural networks learn p...",
      "full_line": "[^fn-nn-learning]: **Neural Network Learning Mechanisms**: The fundamental processes by which neural networks learn patterns from data, including gradient-based optimization, decision boundary formation, and high-dimensional feature representation. These core concepts are introduced comprehensively in @sec-dl-primer."
    },
    {
      "footnote_id": "fn-nn-theory",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3723,
      "context": "[^fn-nn-theory]: **Neural Network Theoretical Foundations**: The mathematical and algorithmic principles underlying...",
      "full_line": "[^fn-nn-theory]: **Neural Network Theoretical Foundations**: The mathematical and algorithmic principles underlying how neural networks process information, learn representations, and make predictions in high-dimensional spaces. Complete theoretical coverage is provided in @sec-dl-primer."
    },
    {
      "footnote_id": "fn-bayesian-nn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3725,
      "context": "[^fn-bayesian-nn]: **Bayesian Neural Networks**: Advanced neural network architectures that incorporate probabilistic...",
      "full_line": "[^fn-bayesian-nn]: **Bayesian Neural Networks**: Advanced neural network architectures that incorporate probabilistic inference by treating weights as probability distributions rather than fixed values. This specialized approach requires understanding of basic neural network concepts covered in @sec-dl-primer."
    },
    {
      "footnote_id": "fn-dropout",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3727,
      "context": "[^fn-dropout]: **Dropout Mechanism**: A regularization technique that randomly deactivates neurons during trainin...",
      "full_line": "[^fn-dropout]: **Dropout Mechanism**: A regularization technique that randomly deactivates neurons during training to prevent overfitting and improve generalization. This method requires understanding of neural network architecture and training processes detailed in @sec-dl-primer."
    },
    {
      "footnote_id": "fn-autoencoders",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3729,
      "context": "[^fn-autoencoders]: **Autoencoders**: Specialized neural network architectures designed to learn efficient data repres...",
      "full_line": "[^fn-autoencoders]: **Autoencoders**: Specialized neural network architectures designed to learn efficient data representations by training to reconstruct input data from compressed encodings. This architecture requires foundational knowledge of neural networks covered in @sec-dl-primer."
    },
    {
      "footnote_id": "fn-household-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 50,
      "context": "...thousands of megawatt-hours of electricity, equivalent to powering hundreds of households for months[^fn-household-energy]. Data centers (including AI workloads) are projected to account for 8% of global power consumption...",
      "full_line": "The environmental footprint of AI systems operates at industrial scales that rival traditional heavy industries. Training a single large language model consumes thousands of megawatt-hours of electricity, equivalent to powering hundreds of households for months[^fn-household-energy]. Data centers (including AI workloads) are projected to account for 8% of global power consumption by 2030, surpassing aviation (2.1%) and approaching cement production (4%) [@oecd2023blueprint][^fn-industry-comparison]. This trajectory represents an unsustainable exponential growth pattern where computational demands increase 350,000\u00d7 faster than hardware efficiency improvements."
    },
    {
      "footnote_id": "fn-industry-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 50,
      "context": "...tion by 2030, surpassing aviation (2.1%) and approaching cement production (4%) [@oecd2023blueprint][^fn-industry-comparison]. This trajectory represents an unsustainable exponential growth pattern where computational demands...",
      "full_line": "The environmental footprint of AI systems operates at industrial scales that rival traditional heavy industries. Training a single large language model consumes thousands of megawatt-hours of electricity, equivalent to powering hundreds of households for months[^fn-household-energy]. Data centers (including AI workloads) are projected to account for 8% of global power consumption by 2030, surpassing aviation (2.1%) and approaching cement production (4%) [@oecd2023blueprint][^fn-industry-comparison]. This trajectory represents an unsustainable exponential growth pattern where computational demands increase 350,000\u00d7 faster than hardware efficiency improvements."
    },
    {
      "footnote_id": "fn-household-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 52,
      "context": "[^fn-household-energy]: **Household Energy Comparison**: The average U.S. household consumes 10,500 kWh annually (about 87...",
      "full_line": "[^fn-household-energy]: **Household Energy Comparison**: The average U.S. household consumes 10,500 kWh annually (about 875 kWh monthly). Training GPT-4 consumed an estimated 50,000-100,000 MWh (50-100 million kWh), equivalent to 5-10 years of electricity for 10,000 households. This single training run used more energy than entire small cities like Aspen, Colorado (population 7,400) consume annually."
    },
    {
      "footnote_id": "fn-industry-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 54,
      "context": "[^fn-industry-comparison]: **AI vs Industrial Emissions**: Data centers (which include AI workloads) are projected to account...",
      "full_line": "[^fn-industry-comparison]: **AI vs Industrial Emissions**: Data centers (which include AI workloads) are projected to account for 8% of total power consumption by 2030, surpassing aviation (2.1%) and approaching cement production (4%). Current AI emissions already exceed those of Argentina (0.2 billion tons CO\u2082 annually). Training just the top 10 large language models in 2023 generated emissions equivalent to 40,000 round-trip flights from New York to London."
    },
    {
      "footnote_id": "fn-gpu-manufacturing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 56,
      "context": "...ialized processors that require rare earth metals whose extraction and processing generate pollution[^fn-gpu-manufacturing]. The growing demand for AI applications accelerates electronic waste production, with global e-wast...",
      "full_line": "Beyond direct energy consumption, AI systems drive environmental impact through hardware manufacturing and resource utilization. Training and inference workloads depend on specialized processors that require rare earth metals whose extraction and processing generate pollution[^fn-gpu-manufacturing]. The growing demand for AI applications accelerates electronic waste production, with global e-waste reaching 54 million metric tons annually [@Forti2020], as AI hardware rapidly becomes obsolete due to accelerating performance requirements[^fn-ewaste-scale]."
    },
    {
      "footnote_id": "fn-ewaste-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 56,
      "context": "...y [@Forti2020], as AI hardware rapidly becomes obsolete due to accelerating performance requirements[^fn-ewaste-scale].",
      "full_line": "Beyond direct energy consumption, AI systems drive environmental impact through hardware manufacturing and resource utilization. Training and inference workloads depend on specialized processors that require rare earth metals whose extraction and processing generate pollution[^fn-gpu-manufacturing]. The growing demand for AI applications accelerates electronic waste production, with global e-waste reaching 54 million metric tons annually [@Forti2020], as AI hardware rapidly becomes obsolete due to accelerating performance requirements[^fn-ewaste-scale]."
    },
    {
      "footnote_id": "fn-gpu-manufacturing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 58,
      "context": "[^fn-gpu-manufacturing]: **GPU Manufacturing Impact**: Producing a single high-end GPU like the NVIDIA H100 generates 300-5...",
      "full_line": "[^fn-gpu-manufacturing]: **GPU Manufacturing Impact**: Producing a single high-end GPU like the NVIDIA H100 generates 300-500 kg of CO\u2082 before any computation occurs. Manufacturing requires 2,500+ liters of ultrapure water, 15+ rare earth elements, and energy-intensive processes reaching 1,000\u00b0C. TSMC's advanced 4nm process consumes 40% more energy per wafer than older 7nm nodes, increasing embodied carbon in AI accelerators."
    },
    {
      "footnote_id": "fn-ewaste-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 60,
      "context": "[^fn-ewaste-scale]: **E-Waste from Computing**: Global e-waste reached 54 million metric tons in 2019, with computing...",
      "full_line": "[^fn-ewaste-scale]: **E-Waste from Computing**: Global e-waste reached 54 million metric tons in 2019, with computing equipment contributing 15%. AI hardware accelerates this trend\u2014NVIDIA's GPU sales increased 200% from 2020-2023, with each high-end GPU weighing 2-4 lbs and containing toxic materials requiring specialized disposal. The rapid obsolescence cycle means AI hardware often becomes e-waste within 3-5 years."
    },
    {
      "footnote_id": "fn-ai-compute-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 74,
      "context": "...ecedented rate, with compute requirements increasing 350,000\u00d7 from 2012 to 2019 [@schwartz2020green][^fn-ai-compute-growth]. This trend shows no signs of slowing down, as advancements in deep learning continue to prioritize...",
      "full_line": "The long-term sustainability of AI is increasingly challenged by the exponential growth of computational demands required to train and deploy machine learning models. Over the past decade, AI systems have scaled at an unprecedented rate, with compute requirements increasing 350,000\u00d7 from 2012 to 2019 [@schwartz2020green][^fn-ai-compute-growth]. This trend shows no signs of slowing down, as advancements in deep learning continue to prioritize larger models with more parameters, larger training datasets, and higher computational complexity. However, sustaining this trajectory poses sustainability challenges, as the efficiency gains from hardware improvements fail to keep pace with the rising demands of AI workloads."
    },
    {
      "footnote_id": "fn-ai-compute-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 76,
      "context": "[^fn-ai-compute-growth]: **AI Compute Explosion**: This 350,000\u00d7 increase represents a doubling time of approximately 3.4 m...",
      "full_line": "[^fn-ai-compute-growth]: **AI Compute Explosion**: This 350,000\u00d7 increase represents a doubling time of approximately 3.4 months\u2014far exceeding Moore's Law's 2-year doubling cycle. For comparison, this is equivalent to going from the computational power of a smartphone to that of the world's largest supercomputer. The trend has only accelerated with large language models: GPT-4's training is estimated to have required 25\u00d7 more compute than GPT-3, while models like PaLM-2 and Claude used even more computational resources."
    },
    {
      "footnote_id": "fn-sustainable-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 78,
      "context": "...storically, computational efficiency improved with advances in semiconductor technology. Moore's Law[^fn-sustainable-moores-law], which predicted that the number of transistors on a chip would double approximately every two year...",
      "full_line": "Historically, computational efficiency improved with advances in semiconductor technology. Moore's Law[^fn-sustainable-moores-law], which predicted that the number of transistors on a chip would double approximately every two years, led to continuous improvements in processing power and energy efficiency. However, Moore's Law is now reaching core physical limits, making further transistor scaling difficult and costly. Dennard scaling[^fn-dennard-scaling], which once ensured that smaller transistors would operate at lower power levels, has also ended, leading to stagnation in energy efficiency improvements per transistor. As a result, while AI models continue to scale in size and capability, the hardware running these models is no longer improving at the same exponential rate. This growing divergence between computational demand and hardware efficiency creates an unsustainable trajectory in which AI consumes ever-increasing amounts of energy."
    },
    {
      "footnote_id": "fn-dennard-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 78,
      "context": "...aching core physical limits, making further transistor scaling difficult and costly. Dennard scaling[^fn-dennard-scaling], which once ensured that smaller transistors would operate at lower power levels, has also ended, l...",
      "full_line": "Historically, computational efficiency improved with advances in semiconductor technology. Moore's Law[^fn-sustainable-moores-law], which predicted that the number of transistors on a chip would double approximately every two years, led to continuous improvements in processing power and energy efficiency. However, Moore's Law is now reaching core physical limits, making further transistor scaling difficult and costly. Dennard scaling[^fn-dennard-scaling], which once ensured that smaller transistors would operate at lower power levels, has also ended, leading to stagnation in energy efficiency improvements per transistor. As a result, while AI models continue to scale in size and capability, the hardware running these models is no longer improving at the same exponential rate. This growing divergence between computational demand and hardware efficiency creates an unsustainable trajectory in which AI consumes ever-increasing amounts of energy."
    },
    {
      "footnote_id": "fn-sustainable-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 80,
      "context": "[^fn-sustainable-moores-law]: **Moore's Law Origins**: Named after Intel co-founder Gordon Moore, who made this observation in a...",
      "full_line": "[^fn-sustainable-moores-law]: **Moore's Law Origins**: Named after Intel co-founder Gordon Moore, who made this observation in a 1965 *Electronics* magazine article, Moore's Law has driven the semiconductor industry for nearly 60 years. Moore initially predicted a doubling every year, later revised to two years. The law's economic impact is staggering: it allowed the $4 trillion global electronics industry and made possible everything from smartphones to supercomputers. However, at 3nm process nodes, individual atoms become the limiting factor."
    },
    {
      "footnote_id": "fn-dennard-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 82,
      "context": "[^fn-dennard-scaling]: **Dennard Scaling**: Rule observed by IBM's Robert Dennard in 1974 that smaller transistors could...",
      "full_line": "[^fn-dennard-scaling]: **Dennard Scaling**: Rule observed by IBM's Robert Dennard in 1974 that smaller transistors could run at the same power density by reducing voltage proportionally. Enabled 30 years of \"free\" performance gains until ~2005 when leakage current and voltage scaling limits ended the trend. Without Dennard scaling, modern CPUs would consume kilowatts instead of ~100W. Its end forced the shift to multi-core processors and specialized accelerators like GPUs for AI workloads."
    },
    {
      "footnote_id": "fn-sustainable-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 85,
      "context": "...MWh) of electricity\u2014equivalent to powering 130 U.S. homes for an entire year [@maslej2023artificial][^fn-sustainable-gpt3]. As explored in the comparison with biological intelligence, this enormous energy requirement repre...",
      "full_line": "The training of complex AI systems like large deep learning models demands high levels of computing power, resulting in significant energy consumption. Consider OpenAI's language model GPT-3 as an example. This system operates through algorithms trained on large datasets, with training estimated to require 1,287 megawatt-hours (MWh) of electricity\u2014equivalent to powering 130 U.S. homes for an entire year [@maslej2023artificial][^fn-sustainable-gpt3]. As explored in the comparison with biological intelligence, this enormous energy requirement represents a fundamental efficiency gap that defines the sustainability challenge facing artificial intelligence. In recent years, these generative AI models have gained increasing popularity, leading to more models being trained with growing parameter counts."
    },
    {
      "footnote_id": "fn-sustainable-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 87,
      "context": "...MWh) of electricity\u2014equivalent to powering 130 U.S. homes for an entire year [@maslej2023artificial][^fn-sustainable-gpt3]. The human brain achieves superior learning capabilities on just 20 watts, highlighting a 10^6 effi...",
      "full_line": "The training of complex AI systems like large deep learning models demands high levels of computing power, resulting in significant energy consumption. Consider OpenAI's language model GPT-3 as an example. This system operates through algorithms trained on large datasets, with training estimated to require 1,287 megawatt-hours (MWh) of electricity\u2014equivalent to powering 130 U.S. homes for an entire year [@maslej2023artificial][^fn-sustainable-gpt3]. The human brain achieves superior learning capabilities on just 20 watts, highlighting a 10^6 efficiency gap that defines the sustainability challenge facing artificial intelligence. This comparison shows how current AI architectures diverge from energy-efficient biological computation principles. In recent years, these generative AI models have gained increasing popularity, leading to more models being trained with growing parameter counts."
    },
    {
      "footnote_id": "fn-sustainable-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 90,
      "context": "[^fn-sustainable-gpt3]: **GPT-3 Energy Consumption**: Training GPT-3 consumed approximately 1,287 MWh of electricity, equi...",
      "full_line": "[^fn-sustainable-gpt3]: **GPT-3 Energy Consumption**: Training GPT-3 consumed approximately 1,287 MWh of electricity, equivalent to the annual energy consumption of 130 average American homes or the same amount of CO\u2082 as burning 500,000 pounds of coal. At average US electricity prices, this training run cost roughly $130,000 in electricity alone. GPT-4, with estimated 25\u00d7 more compute, likely consumed over 30,000 MWh\u2014enough to power a small city for a month. The energy per parameter ratio reveals hardware-software co-design inefficiencies: GPT-3's 175 billion parameters required 7.4 kWh per billion parameters, while optimized architectures can achieve sub-1 kWh ratios through mixed precision and sparsity techniques."
    },
    {
      "footnote_id": "fn-pue-metric",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 211,
      "context": "...energy consumption by 40%, leading to an overall 15% improvement in Power Usage Effectiveness (PUE)[^fn-pue-metric]\u2014a metric for data center energy efficiency that measures the ratio of total energy consumption to t...",
      "full_line": "The results were efficiency gains. When deployed in live data center environments, DeepMind's AI-driven cooling system reduced cooling energy consumption by 40%, leading to an overall 15% improvement in Power Usage Effectiveness (PUE)[^fn-pue-metric]\u2014a metric for data center energy efficiency that measures the ratio of total energy consumption to the energy used purely for computing tasks [@barroso2019datacenter]. These improvements were achieved without any additional hardware modifications, demonstrating the potential of software-driven optimizations to reduce AI's carbon footprint."
    },
    {
      "footnote_id": "fn-pue-metric",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 213,
      "context": "[^fn-pue-metric]: **Power Usage Effectiveness (PUE)**: Industry standard metric calculated as Total Facility Power \u00f7...",
      "full_line": "[^fn-pue-metric]: **Power Usage Effectiveness (PUE)**: Industry standard metric calculated as Total Facility Power \u00f7 IT Equipment Power. Perfect efficiency = 1.0 (impossible), typical data centers = 1.6-2.0, Google's best facilities achieve 1.08. Each 0.1 PUE improvement saves millions in electricity costs. Facebook's Prineville data center achieves 1.09 PUE using outside air cooling. Legacy data centers often exceed 2.5 PUE."
    },
    {
      "footnote_id": "fn-datacenter-emissions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 284,
      "context": "...otal global CO\u2082 emissions\u2014a figure that approaches the airline industry's footprint [@liu2020energy][^fn-datacenter-emissions]. The energy burden of AI is expected to grow exponentially due to three factors: increasing data ce...",
      "full_line": "Data centers play a central role in AI's energy demands, consuming large amounts of electricity to power compute servers, storage, and cooling systems. The energy efficiency of these facilities varies: Power Usage Effectiveness (PUE) ranges from 1.1 in Google's most efficient facilities to 2.5 in typical enterprise data centers, effectively doubling energy consumption through infrastructure overhead. Geographic location impacts carbon intensity\u2014training the same model in Quebec (hydro-powered) versus West Virginia (coal-powered) differs by 10\u00d7 in carbon emissions per kilowatt-hour. Without access to renewable energy, these facilities rely heavily on nonrenewable sources such as coal and natural gas, contributing to global carbon emissions. Current estimates suggest that data centers produce up to 2% of total global CO\u2082 emissions\u2014a figure that approaches the airline industry's footprint [@liu2020energy][^fn-datacenter-emissions]. The energy burden of AI is expected to grow exponentially due to three factors: increasing data center capacity, rising AI training workloads, and increasing inference demands [@patterson2022carbon]. Without intervention, these trends risk making AI's environmental footprint unsustainably large [@thompson2023compute]."
    },
    {
      "footnote_id": "fn-datacenter-emissions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 286,
      "context": "[^fn-datacenter-emissions]: **Data Center Climate Impact**: Data centers consume approximately 1% of global electricity and pr...",
      "full_line": "[^fn-datacenter-emissions]: **Data Center Climate Impact**: Data centers consume approximately 1% of global electricity and produce 0.3% of global carbon emissions directly. However, when including embodied carbon from hardware manufacturing, the figure rises to 2%. For perspective, this equals the annual emissions of Argentina (1.8% of global total) and exceeds the aviation industry's 2.1%. The largest hyperscale data centers consume over 100 MW continuously\u2014equivalent to powering 80,000 homes."
    },
    {
      "footnote_id": "fn-hyperscale-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 290,
      "context": "...ers spanning multiple football fields in size, housing hundreds of thousands of AI-optimized servers[^fn-hyperscale-size]. The training of large language models (LLMs) such as GPT-4 required over 25,000 Nvidia A100 GPUs r...",
      "full_line": "AI workloads are among the most compute-intensive operations in modern data centers. Companies such as Meta operate hyperscale data centers spanning multiple football fields in size, housing hundreds of thousands of AI-optimized servers[^fn-hyperscale-size]. The training of large language models (LLMs) such as GPT-4 required over 25,000 Nvidia A100 GPUs running continuously for 90 to 100 days [@semianalysisGPT4], consuming thousands of megawatt-hours (MWh) of electricity. These facilities rely on high-performance AI accelerators like NVIDIA DGX H100 units, each of which can draw up to 10.2 kW at peak power [@nvidiadgxH100]. The energy efficiency gap becomes clear when comparing hardware generations: H100 GPUs achieve approximately 4\u00d7 better performance per watt than A100s, while mixed-precision training reduces energy consumption by 30-50% through reduced computational precision without accuracy loss [@gholami2021survey]."
    },
    {
      "footnote_id": "fn-hyperscale-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 292,
      "context": "[^fn-hyperscale-size]: **Hyperscale Data Center Scale**: Meta's Prineville data center spans 2.5 million square feet (57...",
      "full_line": "[^fn-hyperscale-size]: **Hyperscale Data Center Scale**: Meta's Prineville data center spans 2.5 million square feet (57 football fields) and houses 150,000+ servers. Microsoft's largest Azure data center in Iowa covers 700 acres with power capacity of 300 MW. Google operates 21 hyperscale facilities globally, consuming 12.2 TWh annually\u2014more electricity than entire countries like Lithuania or Sri Lanka."
    },
    {
      "footnote_id": "fn-transformer-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 356,
      "context": "...year, US car including fuel over a lifetime, and a Transformer model with neural architecture search[^fn-transformer-nas], which has the highest footprint. These comparisons underscore the need for more sustainable AI pra...",
      "full_line": "The environmental impact of AI workloads has emerged as a concern, with carbon emissions approaching levels comparable to established carbon-intensive sectors. Research demonstrates that training a single large AI model generates carbon emissions equivalent to multiple passenger vehicles over their complete lifecycle [@strubell2019energy]. To contextualize AI's environmental footprint, @fig-carbonfootprint compares the carbon emissions of large-scale machine learning tasks to transcontinental flights, illustrating the energy demands of training and inference workloads. It shows a comparison from lowest to highest carbon footprints, starting with a roundtrip flight between NY and SF, human life average per year, American life average per year, US car including fuel over a lifetime, and a Transformer model with neural architecture search[^fn-transformer-nas], which has the highest footprint. These comparisons underscore the need for more sustainable AI practices to mitigate the industry's carbon impact."
    },
    {
      "footnote_id": "fn-transformer-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 358,
      "context": "[^fn-transformer-nas]: **Transformer + NAS Environmental Impact**: This 626,000 lbs CO\u2082 figure represents training one Tr...",
      "full_line": "[^fn-transformer-nas]: **Transformer + NAS Environmental Impact**: This 626,000 lbs CO\u2082 figure represents training one Transformer model while searching for optimal architecture. Includes evaluating 12,800 different model configurations over multiple days. For comparison, this equals the carbon footprint of 312 economy round-trip flights from NYC to London, or the annual emissions of 140 average Americans. Modern efficient NAS techniques have reduced this cost by 1000\u00d7."
    },
    {
      "footnote_id": "fn-euv-lithography",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 539,
      "context": "...gory, encompassing hardware manufacturing, transportation, and disposal. Semiconductor manufacturing[^fn-euv-lithography] is carbon-intensive: producing a single high-performance AI accelerator generates emissions equival...",
      "full_line": "**Scope 3 (Supply Chain & Lifecycle, 15-25% of total)**: The most complex category, encompassing hardware manufacturing, transportation, and disposal. Semiconductor manufacturing[^fn-euv-lithography] is carbon-intensive: producing a single high-performance AI accelerator generates emissions equivalent to several years of operational energy use. Often overlooked but represents irreducible baseline emissions independent of operational efficiency."
    },
    {
      "footnote_id": "fn-euv-lithography",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 541,
      "context": "[^fn-euv-lithography]: **EUV Lithography**: Extreme ultraviolet light (13.5nm wavelength) used to print features smaller...",
      "full_line": "[^fn-euv-lithography]: **EUV Lithography**: Extreme ultraviolet light (13.5nm wavelength) used to print features smaller than 7nm on silicon chips. Each EUV machine costs $200+ million, weighs 180 tons, requires 1 MW of continuous power (enough for 800 homes), and uses 30,000 liters of ultrapure water daily. ASML is the sole global supplier. EUV enables modern AI chips but consumes 10\u00d7 more energy than older deep-UV lithography systems."
    },
    {
      "footnote_id": "fn-edge-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 543,
      "context": "...lifying AI's carbon impact. End-user devices, including smartphones, IoT devices, and edge computing[^fn-edge-computing] platforms, also contribute to Scope 3 emissions, as their AI-allowed functionality depends on susta...",
      "full_line": "Beyond manufacturing, Scope 3 emissions include the downstream impact of AI once deployed. AI services such as search engines, social media platforms, and cloud-based recommendation systems operate at enormous scale, requiring continuous inference across millions or even billions of user interactions. The cumulative electricity demand of inference workloads can ultimately surpass the energy used for training, further amplifying AI's carbon impact. End-user devices, including smartphones, IoT devices, and edge computing[^fn-edge-computing] platforms, also contribute to Scope 3 emissions, as their AI-allowed functionality depends on sustained computation. Companies such as Meta and Google report that Scope 3 emissions from AI-powered services make up the largest share of their total environmental footprint, due to the sheer scale at which AI operates."
    },
    {
      "footnote_id": "fn-edge-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 545,
      "context": "[^fn-edge-computing]: **Edge Computing for AI**: Processing data near its source rather than in distant cloud data cente...",
      "full_line": "[^fn-edge-computing]: **Edge Computing for AI**: Processing data near its source rather than in distant cloud data centers. Reduces latency from 100-200ms (cloud) to 1-10ms (edge) for applications like autonomous vehicles. However, edge AI chips consume 5-50W continuously across billions of devices versus occasional cloud bursts. Tesla's FSD computer consumes 72W while driving; if all 1.4 billion cars had AI, collective power would equal 50 large power plants."
    },
    {
      "footnote_id": "fn-tsmc-water",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 698,
      "context": "...latest fab in Arizona is projected to consume 8.9 million gallons of water per day [@tsmc2023water][^fn-tsmc-water], accounting for nearly 3% of the city's total water production. This demand places significant stra...",
      "full_line": "Semiconductor fabrication is an exceptionally water-intensive process, requiring vast quantities of ultrapure water  for cleaning, cooling, and chemical processing. The scale of water consumption in modern fabs is comparable to that of entire urban populations. For example, TSMC's latest fab in Arizona is projected to consume 8.9 million gallons of water per day [@tsmc2023water][^fn-tsmc-water], accounting for nearly 3% of the city's total water production. This demand places significant strain on local water resources, particularly in water-scarce regions such as Taiwan, Arizona, and Singapore, where semiconductor manufacturing is concentrated. Semiconductor companies have recognized this challenge and are actively investing in recycling technologies and more efficient water management practices. STMicroelectronics, for example, recycles and reuses approximately 41% of its water, significantly reducing its environmental footprint. @fig-water_cycle illustrates the typical semiconductor fab water cycle, showing the stages from raw water intake to wastewater treatment and reuse."
    },
    {
      "footnote_id": "fn-tsmc-water",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 700,
      "context": "[^fn-tsmc-water]: **Semiconductor Water Consumption Scale**: TSMC's Arizona facility will consume 3.2 billion gallon...",
      "full_line": "[^fn-tsmc-water]: **Semiconductor Water Consumption Scale**: TSMC's Arizona facility will consume 3.2 billion gallons annually\u2014equivalent to 37,000 Olympic swimming pools. Each AI chip requires 5-10x more water than traditional processors due to advanced nodes and complex manufacturing. Intel's Ireland fab uses 1.5 billion gallons annually, while Samsung's Texas facility is projected to use 6 million gallons daily. Water treatment and purification add 30-50% to total consumption. During peak summer months, the cumulative daily water consumption of major fabs rivals that of cities with populations exceeding half a million people."
    },
    {
      "footnote_id": "fn-chemical-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 720,
      "context": "...immense, with thousands of metric tons of hazardous substances consumed annually [@kim2018chemical][^fn-chemical-scale].",
      "full_line": "Semiconductor fabrication is heavily reliant on highly hazardous chemicals, which play an important role in processes such as etching, doping, and wafer cleaning. The manufacturing of AI hardware, including GPUs, TPUs, and other specialized accelerators, requires the use of strong acids, volatile solvents, and toxic gases, all of which pose significant health and environmental risks if not properly managed. The scale of chemical usage in fabs is immense, with thousands of metric tons of hazardous substances consumed annually [@kim2018chemical][^fn-chemical-scale]."
    },
    {
      "footnote_id": "fn-chemical-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 722,
      "context": "[^fn-chemical-scale]: **Hazardous Chemical Quantities**: A typical large semiconductor fab uses 500+ different chemicals...",
      "full_line": "[^fn-chemical-scale]: **Hazardous Chemical Quantities**: A typical large semiconductor fab uses 500+ different chemicals annually, consuming 500-2,000 metric tons of acids, 50-200 metric tons of solvents, and 10-50 tons of toxic gases. Arsine gas is lethal at 3 parts per million over 30 minutes. TSMC's facilities store over 50,000 tons of chemicals on-site, requiring specialized emergency response teams and $100+ million in safety infrastructure per fab. Any leaks or accidental releases in fabs can lead to severe health hazards for workers and surrounding communities."
    },
    {
      "footnote_id": "fn-indium-supply",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 740,
      "context": "...lies expected to last fewer than 15 years at the current rate of consumption [@davies2011endangered][^fn-indium-supply].",
      "full_line": "Although silicon forms the foundation of semiconductor devices, high-performance AI chips depend on rare elements such as gallium, indium, and arsenic, which are important for high-speed, low-power electronic components [@chen2006gallium]. Gallium and indium, for example, are widely used in compound semiconductors, particularly for 5G communications, optoelectronics, and AI accelerators. The United States Geological Survey (USGS) has classified indium as a important material, with global supplies expected to last fewer than 15 years at the current rate of consumption [@davies2011endangered][^fn-indium-supply]."
    },
    {
      "footnote_id": "fn-indium-supply",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 742,
      "context": "[^fn-indium-supply]: **Critical Material Scarcity**: Indium production is only 600-800 tons annually worldwide, with Ch...",
      "full_line": "[^fn-indium-supply]: **Critical Material Scarcity**: Indium production is only 600-800 tons annually worldwide, with China controlling 60% of supply. Prices fluctuate wildly\u2014from $60/kg in 2002 to $1,000/kg in 2005, now around $400/kg. Each smartphone contains 0.3mg of indium; each AI accelerator contains 50-100x more. At current AI hardware growth rates (40% annually), demand will exceed supply by 2035 without recycling breakthroughs. As AI hardware manufacturing scales, the demand for helium will continue to grow, necessitating more sustainable extraction and recycling practices."
    },
    {
      "footnote_id": "fn-china-ree-control",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 746,
      "context": "...lenges. China currently dominates over 90% of the world's rare earth element (REE) refining capacity[^fn-china-ree-control], including materials important for AI chips, such as neodymium (for high-performance magnets in AI...",
      "full_line": "Beyond raw material availability, the geopolitical control of rare earth elements poses additional challenges. China currently dominates over 90% of the world's rare earth element (REE) refining capacity[^fn-china-ree-control], including materials important for AI chips, such as neodymium (for high-performance magnets in AI accelerators) and yttrium (for high-temperature superconductors) [@jha2014rare]. This concentration of supply creates supply chain vulnerabilities, as trade restrictions or geopolitical tensions could severely impact AI hardware production."
    },
    {
      "footnote_id": "fn-china-ree-control",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 748,
      "context": "[^fn-china-ree-control]: **Chinese Rare Earth Dominance**: China produces 85% of rare earth elements and controls 95% of gl...",
      "full_line": "[^fn-china-ree-control]: **Chinese Rare Earth Dominance**: China produces 85% of rare earth elements and controls 95% of global refining capacity. The 2010 China-Japan diplomatic crisis saw rare earth exports to Japan cut by 40%, causing prices to spike 2,000%. A single NVIDIA H100 contains 17 different rare earth elements totaling 200-300 grams. U.S. strategic reserves contain only 3-month supply, while building alternative supply chains requires 10-15 years and $50+ billion investment."
    },
    {
      "footnote_id": "fn-jevons-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1092,
      "context": "A principle that must guide all efforts to mitigate AI's environmental impact is Jevon's Paradox[^fn-jevons-paradox]. This paradox, observed by William Stanley Jevons in the 19th century [@jevons1865coal], states tha...",
      "full_line": "A principle that must guide all efforts to mitigate AI's environmental impact is Jevon's Paradox[^fn-jevons-paradox]. This paradox, observed by William Stanley Jevons in the 19th century [@jevons1865coal], states that improvements in technological efficiency can lead to an increase in overall consumption. In the context of AI, even as we develop more energy-efficient models and hardware, the increased accessibility and adoption of AI technologies could lead to a net increase in energy consumption and resource utilization. Therefore, we must approach mitigation strategies with a keen awareness of this potential rebound effect, ensuring that efficiency gains do not inadvertently drive greater consumption. This section explores key strategies for mitigating AI's environmental impact, beginning with sustainable AI development principles."
    },
    {
      "footnote_id": "fn-jevons-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1094,
      "context": "[^fn-jevons-paradox]: **Jevon's Paradox**: Named after British economist William Stanley Jevons who observed in 1865 tha...",
      "full_line": "[^fn-jevons-paradox]: **Jevon's Paradox**: Named after British economist William Stanley Jevons who observed in 1865 that improving coal efficiency actually increased total coal consumption rather than reducing it. Modern examples include LEDs\u2014despite being 85% more efficient than incandescent bulbs, total lighting energy consumption has increased due to expanded usage. In AI, this means that making models 10\u00d7 more efficient might lead to 100\u00d7 more AI applications, resulting in net increase in environmental impact."
    },
    {
      "footnote_id": "fn-flops-vs-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1155,
      "context": "...rely on billions of parameters, requiring trillions of floating-point operations per second (FLOPS)[^fn-flops-vs-flops] during training and inference. While these large models achieve state-of-the-art performance, resea...",
      "full_line": "Many deep learning models rely on billions of parameters, requiring trillions of floating-point operations per second (FLOPS)[^fn-flops-vs-flops] during training and inference. While these large models achieve state-of-the-art performance, research indicates that much of their computational complexity is unnecessary. Many parameters contribute little to final predictions, leading to wasteful resource utilization. Effective sustainability requires treating energy efficiency as a design constraint rather than an afterthought, necessitating hardware-software co-design approaches that optimize both algorithmic choices and their hardware implementation simultaneously."
    },
    {
      "footnote_id": "fn-flops-vs-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1157,
      "context": "[^fn-flops-vs-flops]: **FLOPS vs FLOPs**: FLOPS (all caps) = Floating-Point Operations Per Second (rate), while FLOPs (m...",
      "full_line": "[^fn-flops-vs-flops]: **FLOPS vs FLOPs**: FLOPS (all caps) = Floating-Point Operations Per Second (rate), while FLOPs (mixed case) = total Floating-Point Operations (count). GPT-3 training required 3.1\u00d710\u00b2\u00b3 FLOPs total, executed on hardware capable of 1.25\u00d710\u00b9\u2077 FLOPS. Modern AI accelerators like H100 achieve 2,000 TFLOPS for AI workloads. Each ChatGPT response requires ~10\u00b9\u00b2 FLOPs\u2014roughly equivalent to your calculator performing one operation per second for 30,000 years. The energy efficiency varies dramatically across hardware: CPUs consume ~100 pJ/FLOP (100 \u00d7 10\u207b\u00b9\u00b2 J/FLOP), GPUs achieve ~10 pJ/FLOP, TPUs reach ~1 pJ/FLOP, while specialized AI accelerators approach 0.1 pJ/FLOP\u2014a 1000\u00d7 efficiency range that defines sustainability opportunities. To mitigate this inefficiency, several optimization techniques have been developed to reduce the computational overhead of AI models while maintaining accuracy."
    },
    {
      "footnote_id": "fn-pue-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1201,
      "context": "...can consume over 100 megawatts of power, a level comparable to the electricity usage of a small city[^fn-pue-efficiency]. Without intervention, the continued growth of AI workloads threatens to push the energy consumptio...",
      "full_line": "The increasing computational demands of AI have made data centers one of the largest consumers of electricity in the digital economy. Large-scale cloud data centers provide the infrastructure necessary for training and deploying machine learning models, but their energy consumption is substantial. A single hyperscale data center can consume over 100 megawatts of power, a level comparable to the electricity usage of a small city[^fn-pue-efficiency]. Without intervention, the continued growth of AI workloads threatens to push the energy consumption of data centers beyond sustainable levels."
    },
    {
      "footnote_id": "fn-pue-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1203,
      "context": "[^fn-pue-efficiency]: **Power Usage Effectiveness**: Data center efficiency is measured by PUE (Power Usage Effectivenes...",
      "full_line": "[^fn-pue-efficiency]: **Power Usage Effectiveness**: Data center efficiency is measured by PUE (Power Usage Effectiveness)\u2014total facility power divided by IT equipment power. Industry average PUE is 1.67 (67% overhead for cooling/infrastructure), but leading hyperscalers achieve 1.1-1.2. Google's best data centers reach PUE of 1.08, meaning only 8% energy overhead. Each 0.1 PUE improvement saves millions annually in electricity costs. The industry must adopt strategies to optimize power efficiency, integrate renewable energy sources, and improve cooling mechanisms to mitigate the environmental impact of AI infrastructure."
    },
    {
      "footnote_id": "fn-google-carbon-free",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1205,
      "context": ".... Google, for example, has set a goal to operate its data centers on 24/7 carbon-free energy by 2030[^fn-google-carbon-free], ensuring that every unit of electricity consumed is matched with renewable generation rather than...",
      "full_line": "One of the most promising approaches to reducing data center emissions is the transition to renewable energy. Major cloud providers, including Google, Microsoft, and Amazon Web Services, have committed to powering their data centers with renewable energy, but implementation challenges remain. Unlike fossil fuel plants, which provide consistent electricity output, renewable sources such as wind and solar are intermittent, with generation levels fluctuating throughout the day. To address this variability, AI infrastructure must incorporate energy storage solutions, such as large-scale battery deployments, and implement intelligent scheduling mechanisms that shift AI workloads to times when renewable energy availability is highest. Google, for example, has set a goal to operate its data centers on 24/7 carbon-free energy by 2030[^fn-google-carbon-free], ensuring that every unit of electricity consumed is matched with renewable generation rather than relying on carbon offsets alone."
    },
    {
      "footnote_id": "fn-google-carbon-free",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1207,
      "context": "[^fn-google-carbon-free]: **Google's Carbon-Free Commitment**: Google achieved carbon neutrality in 2007 and has been carbon...",
      "full_line": "[^fn-google-carbon-free]: **Google's Carbon-Free Commitment**: Google achieved carbon neutrality in 2007 and has been carbon neutral for 15 years, but 24/7 carbon-free energy is more ambitious\u2014requiring real-time matching of energy consumption with clean generation. Currently at 64% carbon-free energy globally. Denmark data centers run on 100% wind power, while others still rely on grid renewables certificates. This requires $15 billion+ investment in clean energy projects worldwide."
    },
    {
      "footnote_id": "fn-cooling-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1209,
      "context": "...o the energy footprint of data centers, often accounting for 30-40% of total electricity consumption[^fn-cooling-energy]. Traditional cooling methods rely on air conditioning units and mechanical chillers, both of which...",
      "full_line": "Cooling systems represent another major contributor to the energy footprint of data centers, often accounting for 30-40% of total electricity consumption[^fn-cooling-energy]. Traditional cooling methods rely on air conditioning units and mechanical chillers, both of which require significant power and water resources."
    },
    {
      "footnote_id": "fn-cooling-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1211,
      "context": "[^fn-cooling-energy]: **Data Center Cooling Costs**: Cooling consumes 38% of total data center energy on average. A typi...",
      "full_line": "[^fn-cooling-energy]: **Data Center Cooling Costs**: Cooling consumes 38% of total data center energy on average. A typical 10 MW data center spends $3.8 million annually on cooling electricity. Google's machine learning optimization reduced cooling energy by 40%, saving $150+ million globally. Liquid cooling can be 3,000x more efficient than air cooling for high-density AI workloads, reducing cooling energy from 40% to under 10% of total consumption. To improve efficiency, data centers are adopting alternative cooling strategies that reduce energy waste. Liquid cooling, which transfers heat away from AI accelerators using specially designed coolant systems, is significantly more effective than traditional air cooling and is now being deployed in high-density computing clusters. Free-air cooling, which utilizes natural airflow instead of mechanical refrigeration, has also been adopted in temperate climates, where external conditions allow for passive cooling. Microsoft has taken this a step further by deploying underwater data centers that use the surrounding ocean as a natural cooling mechanism, reducing the need for active temperature regulation."
    },
    {
      "footnote_id": "fn-google-carbon-scheduling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1221,
      "context": "...bon-aware computing in its cloud infrastructure. In 2020, the company introduced a scheduling system[^fn-google-carbon-scheduling] that delays non-urgent AI tasks until times when renewable energy sources such as solar or wind pow...",
      "full_line": "Google has pioneered one of the most advanced implementations of carbon-aware computing in its cloud infrastructure. In 2020, the company introduced a scheduling system[^fn-google-carbon-scheduling] that delays non-urgent AI tasks until times when renewable energy sources such as solar or wind power are more abundant. This approach allows AI workloads to align with the natural variability of clean energy availability, reducing reliance on fossil fuels while maintaining high computational efficiency. Google has further extended this strategy by geographically distributing AI workloads, moving computations to data centers in regions where clean energy is more accessible. By shifting large-scale AI training jobs from fossil fuel-heavy grids to low-carbon power sources, the company has demonstrated that significant emissions reductions can be achieved through intelligent workload placement."
    },
    {
      "footnote_id": "fn-google-carbon-scheduling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1223,
      "context": "[^fn-google-carbon-scheduling]: **Google Carbon-Aware Scheduling Results**: Google's carbon-intelligent computing platform achieve...",
      "full_line": "[^fn-google-carbon-scheduling]: **Google Carbon-Aware Scheduling Results**: Google's carbon-intelligent computing platform achieved 15% reduction in hourly carbon footprint by shifting workloads within regions. Globally shifting workloads between data centers achieved 40% reduction. The system processes 95 billion search queries daily while optimizing for grid carbon intensity. Non-urgent tasks like batch training can shift 70% of workload to lower-carbon time periods, reducing emissions equivalent to taking 50,000 cars off the road annually."
    },
    {
      "footnote_id": "fn-grid-carbon-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1227,
      "context": "...r instance, the Electricity Maps API provides real-time CO\u2082 emissions data for power grids worldwide[^fn-grid-carbon-data], enabling AI infrastructure to adjust computational workloads based on carbon availability. As acce...",
      "full_line": "The effectiveness of carbon-aware AI scheduling depends on accurate real-time data about grid emissions. Electricity providers and sustainability organizations have begun publishing grid carbon intensity data through publicly available APIs, allowing AI systems to dynamically respond to changes in energy supply. For instance, the Electricity Maps API provides real-time CO\u2082 emissions data for power grids worldwide[^fn-grid-carbon-data], enabling AI infrastructure to adjust computational workloads based on carbon availability. As access to grid emissions data improves, carbon-aware computing will become a scalable and widely adoptable solution for reducing the environmental impact of AI operations."
    },
    {
      "footnote_id": "fn-grid-carbon-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1229,
      "context": "[^fn-grid-carbon-data]: **Real-Time Grid Carbon Intensity**: Grid carbon intensity varies dramatically\u2014from 50g CO\u2082/kWh in...",
      "full_line": "[^fn-grid-carbon-data]: **Real-Time Grid Carbon Intensity**: Grid carbon intensity varies dramatically\u2014from 50g CO\u2082/kWh in nuclear-heavy France to 820g/kWh in coal-dependent Poland. In Texas, intensity fluctuates 10x daily (150-1,500g/kWh) based on wind generation. The Electricity Maps API serves 50+ million requests daily to allow carbon-aware computing. WattTime API provides marginal emissions data showing which power plants turn on/off next, allowing 2-5x better carbon optimization than average intensity."
    },
    {
      "footnote_id": "fn-nuclear-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1237,
      "context": "...ity. Tech companies like Microsoft have shown interest in nuclear energy to power their data centers[^fn-nuclear-ai], as their more constant demand profile (compared to residential use) aligns well with nuclear gener...",
      "full_line": "To fully use carbon-aware scheduling for AI inference workloads, innovation in energy storage solutions is important for consistent renewable energy use. The base energy load is currently met with nuclear energy\u2014a constant source that produces no direct carbon emissions but lacks the flexibility to accommodate renewable energy variability. Tech companies like Microsoft have shown interest in nuclear energy to power their data centers[^fn-nuclear-ai], as their more constant demand profile (compared to residential use) aligns well with nuclear generation characteristics."
    },
    {
      "footnote_id": "fn-nuclear-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1239,
      "context": "[^fn-nuclear-ai]: **Nuclear Power for AI Data Centers**: Microsoft partnered with Helion Energy for fusion power by...",
      "full_line": "[^fn-nuclear-ai]: **Nuclear Power for AI Data Centers**: Microsoft partnered with Helion Energy for fusion power by 2028, signing the first commercial fusion agreement. Amazon invested $500M in small modular reactors (SMRs) for data centers. Google is exploring 24/7 nuclear partnerships with Kairos Power. Nuclear provides 20% of U.S. electricity with 12g CO\u2082/kWh lifecycle emissions versus 820-1,050g for coal. However, new nuclear costs $150-200/MWh versus $20-40 for renewables plus storage."
    },
    {
      "footnote_id": "fn-energy-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1243,
      "context": "...rtant role. Energy-aware AI frameworks, such as Zeus [@jie2023zeus] and Perseus [@jaewon2023perseus][^fn-energy-frameworks], balance computational speed and power efficiency during both training and inference. These platfor...",
      "full_line": "Software frameworks specifically designed for energy efficiency also play a important role. Energy-aware AI frameworks, such as Zeus [@jie2023zeus] and Perseus [@jaewon2023perseus][^fn-energy-frameworks], balance computational speed and power efficiency during both training and inference. These platforms optimize model execution by analyzing trade-offs between speed and energy consumption, facilitating widespread adoption of energy-efficient AI strategies, particularly for inference operations that must run continuously at scale."
    },
    {
      "footnote_id": "fn-energy-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1245,
      "context": "[^fn-energy-frameworks]: **Energy-Aware AI Frameworks**: Zeus framework achieves 75% energy savings on BERT training by aut...",
      "full_line": "[^fn-energy-frameworks]: **Energy-Aware AI Frameworks**: Zeus framework achieves 75% energy savings on BERT training by automatically finding optimal energy-performance trade-offs. Perseus reduces GPU memory usage by 50% through dynamic batching, lowering energy consumption proportionally. CodeCarbon automatically tracks emissions, revealing that training can vary 10-100x in energy usage depending on optimization settings. These tools democratize energy optimization beyond just hyperscale companies."
    },
    {
      "footnote_id": "fn-embodied-carbon",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1273,
      "context": "One of the most striking findings from LCA studies is the embodied carbon[^fn-embodied-carbon] cost of AI hardware. Unlike operational emissions, which can be reduced by shifting to cleaner ener...",
      "full_line": "One of the most striking findings from LCA studies is the embodied carbon[^fn-embodied-carbon] cost of AI hardware. Unlike operational emissions, which can be reduced by shifting to cleaner energy sources, embodied emissions result from the raw material extraction, semiconductor fabrication, and supply chain logistics that precede an AI accelerator's deployment."
    },
    {
      "footnote_id": "fn-embodied-carbon",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1275,
      "context": "[^fn-embodied-carbon]: **Embodied Carbon**: Carbon emissions from manufacturing, transportation, and disposal phases of a...",
      "full_line": "[^fn-embodied-carbon]: **Embodied Carbon**: Carbon emissions from manufacturing, transportation, and disposal phases of a product\u2014distinct from operational emissions during use. For AI hardware, embodied carbon includes mining rare earth elements, semiconductor fabrication, packaging, and shipping. A single NVIDIA H100 GPU embodies 300-500 kg CO\u2082 before first use\u2014equivalent to 1,000-1,600 miles of driving. For comparison, the GPU's 700W power consumption generates 300 kg CO\u2082 annually (assuming average U.S. grid), meaning manufacturing emissions equal 1-2 years of operation. Research indicates that manufacturing emissions alone can account for up to 30% of an AI system's total carbon footprint, with this number potentially growing as data centers improve their reliance on renewable energy sources."
    },
    {
      "footnote_id": "fn-eniac",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 150,
      "context": "Electronic computation began with the mainframe era. ENIAC[^fn-eniac] (1945) established the viability of electronic computation at scale, while the IBM System/360[^fn-s...",
      "full_line": "Electronic computation began with the mainframe era. ENIAC[^fn-eniac] (1945) established the viability of electronic computation at scale, while the IBM System/360[^fn-system360] (1964) introduced architectural principles of standardized instruction sets and memory hierarchies. These basic concepts provided the foundation for all subsequent computing systems."
    },
    {
      "footnote_id": "fn-system360",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 150,
      "context": "...eniac] (1945) established the viability of electronic computation at scale, while the IBM System/360[^fn-system360] (1964) introduced architectural principles of standardized instruction sets and memory hierarchies....",
      "full_line": "Electronic computation began with the mainframe era. ENIAC[^fn-eniac] (1945) established the viability of electronic computation at scale, while the IBM System/360[^fn-system360] (1964) introduced architectural principles of standardized instruction sets and memory hierarchies. These basic concepts provided the foundation for all subsequent computing systems."
    },
    {
      "footnote_id": "fn-eniac",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 152,
      "context": "[^fn-eniac]: **ENIAC (Electronic Numerical Integrator and Computer)**: Completed in 1946 at the University of P...",
      "full_line": "[^fn-eniac]: **ENIAC (Electronic Numerical Integrator and Computer)**: Completed in 1946 at the University of Pennsylvania, ENIAC weighed 30 tons, consumed 150kW of power, and performed 5,000 operations per second. Its 17,468 vacuum tubes required constant maintenance, but it demonstrated electronic computation could be 1,000x faster than mechanical calculators."
    },
    {
      "footnote_id": "fn-system360",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 154,
      "context": "[^fn-system360]: **IBM System/360**: Launched in 1964 as a $5 billion gamble (equivalent to $40 billion today), Sys...",
      "full_line": "[^fn-system360]: **IBM System/360**: Launched in 1964 as a $5 billion gamble (equivalent to $40 billion today), System/360 introduced the revolutionary concept of backward compatibility across different computer models. Its standardized instruction set architecture became the foundation for modern computing, enabling software portability that drives today's cloud computing."
    },
    {
      "footnote_id": "fn-cdc6600",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 156,
      "context": "...ton1965cdc] built upon these foundations while specializing for scientific computation. The CDC 6600[^fn-cdc6600] and later systems like the CM-5[^fn-cm5] [@thinking_machines_cm5] optimized for dense matrix operat...",
      "full_line": "High-performance computing (HPC) systems [@thornton1965cdc] built upon these foundations while specializing for scientific computation. The CDC 6600[^fn-cdc6600] and later systems like the CM-5[^fn-cm5] [@thinking_machines_cm5] optimized for dense matrix operations and floating-point calculations."
    },
    {
      "footnote_id": "fn-cm5",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 156,
      "context": "...e specializing for scientific computation. The CDC 6600[^fn-cdc6600] and later systems like the CM-5[^fn-cm5] [@thinking_machines_cm5] optimized for dense matrix operations and floating-point calculations.",
      "full_line": "High-performance computing (HPC) systems [@thornton1965cdc] built upon these foundations while specializing for scientific computation. The CDC 6600[^fn-cdc6600] and later systems like the CM-5[^fn-cm5] [@thinking_machines_cm5] optimized for dense matrix operations and floating-point calculations."
    },
    {
      "footnote_id": "fn-cdc6600",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 158,
      "context": "[^fn-cdc6600]: **CDC 6600**: Designed by Seymour Cray and released in 1964, the CDC 6600 achieved 3 MFLOPS (milli...",
      "full_line": "[^fn-cdc6600]: **CDC 6600**: Designed by Seymour Cray and released in 1964, the CDC 6600 achieved 3 MFLOPS (million floating-point operations per second) using innovative parallel processing with 10 peripheral processors. Costing $8 million ($65 million today), it was the world's fastest computer until 1969 and established supercomputing as a field."
    },
    {
      "footnote_id": "fn-cm5",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 160,
      "context": "[^fn-cm5]: **Connection Machine CM-5**: Released by Thinking Machines in 1991, the CM-5 featured up to 16,384...",
      "full_line": "[^fn-cm5]: **Connection Machine CM-5**: Released by Thinking Machines in 1991, the CM-5 featured up to 16,384 processors connected by a fat-tree network, delivering over 100 GFLOPS. Its $10-50 million price tag and specialized parallel architecture made it a favorite for scientific computing but ultimately commercially unsuccessful as commodity clusters emerged."
    },
    {
      "footnote_id": "fn-google-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 164,
      "context": "Warehouse-scale computing marked the next evolutionary step. Google's data center implementations[^fn-google-datacenter] [@barroso2003web] introduced new optimizations for internet-scale data processing. Unlike HPC syste...",
      "full_line": "Warehouse-scale computing marked the next evolutionary step. Google's data center implementations[^fn-google-datacenter] [@barroso2003web] introduced new optimizations for internet-scale data processing. Unlike HPC systems focused on tightly coupled scientific calculations, warehouse computing handled loosely coupled tasks with irregular data access patterns."
    },
    {
      "footnote_id": "fn-google-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 166,
      "context": "[^fn-google-datacenter]: **Google Data Centers**: Starting in 1998 with commodity PCs, Google pioneered warehouse-scale com...",
      "full_line": "[^fn-google-datacenter]: **Google Data Centers**: Starting in 1998 with commodity PCs, Google pioneered warehouse-scale computing by 2003, managing over 100,000 servers across multiple facilities. By 2020, Google operated over 20 data centers consuming 12 TWh annually, equivalent to entire countries, while achieving industry-leading PUE (Power Usage Effectiveness) of 1.10 through innovative cooling."
    },
    {
      "footnote_id": "fn-training-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 170,
      "context": "...tion emerged as the next frontier, building upon this accumulated architectural knowledge. AlexNet's[^fn-training-alexnet] [@krizhevsky2012imagenet] success in 2012 highlighted the need for further specialization. While pr...",
      "full_line": "Deep learning computation emerged as the next frontier, building upon this accumulated architectural knowledge. AlexNet's[^fn-training-alexnet] [@krizhevsky2012imagenet] success in 2012 highlighted the need for further specialization. While previous systems focused on either scientific calculations or independent data processing tasks, neural network training introduced new computational patterns. The training process required continuous updates to large sets of parameters, with complex data dependencies during model optimization. These workloads demanded new approaches to memory management and inter-device communication that neither HPC nor warehouse computing had fully addressed."
    },
    {
      "footnote_id": "fn-training-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 172,
      "context": "[^fn-training-alexnet]: **AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet won ImageN...",
      "full_line": "[^fn-training-alexnet]: **AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet won ImageNet 2012 with 15.3% error rate (vs. 26.2% for second place), using two GTX 580 GPUs for 5-6 days of training. This breakthrough launched the deep learning revolution and demonstrated that GPUs could accelerate neural network training by 10-50x over CPUs."
    },
    {
      "footnote_id": "fn-nvidia-gpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 174,
      "context": "...computing era, beginning in 2015, represents the latest step in this evolutionary chain. NVIDIA GPUs[^fn-nvidia-gpus] and Google TPUs[^fn-google-tpus] introduced hardware designs specifically optimized for neural netw...",
      "full_line": "The AI hypercomputing era, beginning in 2015, represents the latest step in this evolutionary chain. NVIDIA GPUs[^fn-nvidia-gpus] and Google TPUs[^fn-google-tpus] introduced hardware designs specifically optimized for neural network computations, moving beyond adaptations of existing architectures. These systems implemented new approaches to parallel processing, memory access, and device communication to handle the distinct patterns of model training. The resulting architectures balanced the numerical precision needs of scientific computing with the scale requirements of warehouse systems, while adding specialized support for the iterative nature of neural network optimization. The comprehensive design principles, architectural details, and optimization strategies for these specialized training accelerators are explored in depth in @sec-ai-acceleration."
    },
    {
      "footnote_id": "fn-google-tpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 174,
      "context": "...represents the latest step in this evolutionary chain. NVIDIA GPUs[^fn-nvidia-gpus] and Google TPUs[^fn-google-tpus] introduced hardware designs specifically optimized for neural network computations, moving beyond a...",
      "full_line": "The AI hypercomputing era, beginning in 2015, represents the latest step in this evolutionary chain. NVIDIA GPUs[^fn-nvidia-gpus] and Google TPUs[^fn-google-tpus] introduced hardware designs specifically optimized for neural network computations, moving beyond adaptations of existing architectures. These systems implemented new approaches to parallel processing, memory access, and device communication to handle the distinct patterns of model training. The resulting architectures balanced the numerical precision needs of scientific computing with the scale requirements of warehouse systems, while adding specialized support for the iterative nature of neural network optimization. The comprehensive design principles, architectural details, and optimization strategies for these specialized training accelerators are explored in depth in @sec-ai-acceleration."
    },
    {
      "footnote_id": "fn-nvidia-gpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 176,
      "context": "[^fn-nvidia-gpus]: **NVIDIA AI GPUs**: From the 2012 GTX 680 (3.09 TFLOPS) used for AlexNet to the 2023 H100 (989 TFL...",
      "full_line": "[^fn-nvidia-gpus]: **NVIDIA AI GPUs**: From the 2012 GTX 680 (3.09 TFLOPS) used for AlexNet to the 2023 H100 (989 TFLOPS for AI), NVIDIA GPUs increased AI performance by over 300x in a decade. The H100 costs $25,000-40,000 but enables training models that would be impossible on older hardware, demonstrating specialized silicon's critical role in AI advancement."
    },
    {
      "footnote_id": "fn-google-tpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 178,
      "context": "[^fn-google-tpus]: **Google TPUs**: First deployed internally in 2015, TPUs deliver 15-30x better price-performance t...",
      "full_line": "[^fn-google-tpus]: **Google TPUs**: First deployed internally in 2015, TPUs deliver 15-30x better price-performance than GPUs for specific AI workloads. The TPU v4 (2021) achieves 275 TFLOPS (bfloat16) with 32GB memory per chip, while TPU pods can scale to 1 exaFLOP. Google's $billions investment in custom silicon has enabled training models like PaLM (540B parameters) cost-effectively."
    },
    {
      "footnote_id": "fn-transformers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 211,
      "context": "The emergence of transformer architectures[^fn-transformers] and large-scale models has introduced new requirements for training systems. Contemporary implement...",
      "full_line": "The emergence of transformer architectures[^fn-transformers] and large-scale models has introduced new requirements for training systems. Contemporary implementations must efficiently process petabyte-scale datasets, orchestrate distributed training across multiple accelerators, and optimize memory utilization for models containing billions of parameters. The management of data parallelism[^fn-training-data-parallelism], model parallelism[^fn-training-model-parallelism], and inter-device communication presents significant technical challenges in modern training architectures. These distributed system complexities motivate the specialized AI workflow management tools (@sec-ai-workflow) that automate many aspects of large-scale training orchestration."
    },
    {
      "footnote_id": "fn-training-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 211,
      "context": "...memory utilization for models containing billions of parameters. The management of data parallelism[^fn-training-data-parallelism], model parallelism[^fn-training-model-parallelism], and inter-device communication presents signifi...",
      "full_line": "The emergence of transformer architectures[^fn-transformers] and large-scale models has introduced new requirements for training systems. Contemporary implementations must efficiently process petabyte-scale datasets, orchestrate distributed training across multiple accelerators, and optimize memory utilization for models containing billions of parameters. The management of data parallelism[^fn-training-data-parallelism], model parallelism[^fn-training-model-parallelism], and inter-device communication presents significant technical challenges in modern training architectures. These distributed system complexities motivate the specialized AI workflow management tools (@sec-ai-workflow) that automate many aspects of large-scale training orchestration."
    },
    {
      "footnote_id": "fn-training-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 211,
      "context": "...of parameters. The management of data parallelism[^fn-training-data-parallelism], model parallelism[^fn-training-model-parallelism], and inter-device communication presents significant technical challenges in modern training archit...",
      "full_line": "The emergence of transformer architectures[^fn-transformers] and large-scale models has introduced new requirements for training systems. Contemporary implementations must efficiently process petabyte-scale datasets, orchestrate distributed training across multiple accelerators, and optimize memory utilization for models containing billions of parameters. The management of data parallelism[^fn-training-data-parallelism], model parallelism[^fn-training-model-parallelism], and inter-device communication presents significant technical challenges in modern training architectures. These distributed system complexities motivate the specialized AI workflow management tools (@sec-ai-workflow) that automate many aspects of large-scale training orchestration."
    },
    {
      "footnote_id": "fn-training-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 213,
      "context": "[^fn-training-data-parallelism]: **Data Parallelism Scaling**: Linear scaling works until communication becomes the bottleneck, typ...",
      "full_line": "[^fn-training-data-parallelism]: **Data Parallelism Scaling**: Linear scaling works until communication becomes the bottleneck, typically around 64-128 GPUs for most models. BERT-Large achieves 76x speedup on 128 GPUs (59% efficiency), while GPT-3 required 1,024 GPUs with only 45% efficiency. The key constraint is AllReduce communication cost scales as O(n) with number of devices, requiring high-bandwidth interconnects like InfiniBand."
    },
    {
      "footnote_id": "fn-training-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 215,
      "context": "[^fn-training-model-parallelism]: **Model Parallelism Memory Scaling**: Enables training models too large for single GPUs. GPT-3 (17...",
      "full_line": "[^fn-training-model-parallelism]: **Model Parallelism Memory Scaling**: Enables training models too large for single GPUs. GPT-3 (175B parameters) needs 350GB for weights in FP16 (700GB in FP32), far exceeding any single GPU's 80GB maximum. However, model parallelism often achieves only 20-60% compute efficiency due to sequential dependencies between model partitions and communication overhead between devices."
    },
    {
      "footnote_id": "fn-transformer-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 231,
      "context": "For example, training large Transformer models[^fn-transformer-training] requires partitioning data and model parameters across multiple devices. This introduces synchroniz...",
      "full_line": "For example, training large Transformer models[^fn-transformer-training] requires partitioning data and model parameters across multiple devices. This introduces synchronization challenges, particularly during gradient updates. Communication libraries such as [NVIDIA's Collective Communications Library (NCCL)](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html) enable efficient gradient sharing, providing the foundation for distributed training optimization techniques. The benchmarking methodologies in @sec-benchmarking-ai provide systematic approaches for evaluating these distributed training performance characteristics. These examples illustrate how system-level considerations influence the feasibility and efficiency of modern training workflows."
    },
    {
      "footnote_id": "fn-strassen-algorithm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 264,
      "context": "...algorithms has closely followed advancements in numerical linear algebra. From Strassen's algorithm[^fn-strassen-algorithm], which reduced the naive $O(n^3)$ complexity to approximately $O(n^{2.81})$ [@strassen1969gauss], t...",
      "full_line": "Matrix-matrix multiplication dominates computation in neural networks, accounting for 60-90% of training time [@he2016residual]. Early neural network implementations relied on standard CPU-based linear algebra libraries. The evolution of matrix multiplication algorithms has closely followed advancements in numerical linear algebra. From Strassen's algorithm[^fn-strassen-algorithm], which reduced the naive $O(n^3)$ complexity to approximately $O(n^{2.81})$ [@strassen1969gauss], to contemporary hardware-accelerated libraries like [cuBLAS](https://developer.nvidia.com/cublas), these innovations have continually pushed the limits of computational efficiency."
    },
    {
      "footnote_id": "fn-strassen-algorithm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 266,
      "context": "[^fn-strassen-algorithm]: **Strassen's Algorithm**: Developed by Volker Strassen in 1969, this breakthrough reduced matrix m...",
      "full_line": "[^fn-strassen-algorithm]: **Strassen's Algorithm**: Developed by Volker Strassen in 1969, this breakthrough reduced matrix multiplication from O(n\u00b3) to O(n^2.807) by using clever algebraic tricks with 7 multiplications instead of 8. While theoretically faster, it's only practical for matrices larger than 500\u00d7500 due to overhead. Modern implementations in libraries like Intel MKL switch between algorithms based on matrix size, demonstrating how theoretical advances require careful engineering for practical impact."
    },
    {
      "footnote_id": "fn-batching-transformation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 276,
      "context": "The introduction of batching[^fn-batching-transformation] transformed matrix computation in neural networks. By processing multiple inputs simultaneously, tr...",
      "full_line": "The introduction of batching[^fn-batching-transformation] transformed matrix computation in neural networks. By processing multiple inputs simultaneously, training systems convert matrix-vector operations into more efficient matrix-matrix operations. This approach improves hardware utilization but increases memory demands for storing intermediate results. Modern implementations must balance batch sizes against available memory, leading to specific optimizations in memory management and computation scheduling."
    },
    {
      "footnote_id": "fn-batching-transformation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 278,
      "context": "[^fn-batching-transformation]: **Batching in Neural Networks**: Unlike traditional programming where data is processed one item a...",
      "full_line": "[^fn-batching-transformation]: **Batching in Neural Networks**: Unlike traditional programming where data is processed one item at a time, ML systems process multiple examples simultaneously to maximize GPU utilization. A single example might achieve only 5-10% GPU utilization, while batches of 32-256 can reach 80-95%. This shift from scalar to tensor operations explains why ML systems require different programming patterns and hardware optimizations than traditional applications."
    },
    {
      "footnote_id": "fn-rnns",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 432,
      "context": "For example, tanh is widely used in recurrent neural networks (RNNs)[^fn-rnns], where its bounded and symmetric properties help stabilize learning dynamics over time. While tanh...",
      "full_line": "For example, tanh is widely used in recurrent neural networks (RNNs)[^fn-rnns], where its bounded and symmetric properties help stabilize learning dynamics over time. While tanh has largely been replaced by ReLU in many modern architectures due to its computational inefficiencies and vanishing gradient issues, it remains a viable choice in scenarios where its range and symmetry are beneficial."
    },
    {
      "footnote_id": "gradient-clipping",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 444,
      "context": "...In such cases, the neuron no longer contributes to learning. While techniques like gradient clipping[^gradient-clipping], batch normalization[^batch-norm], and careful initialization can mitigate these training instabili...",
      "full_line": "ReLU is particularly effective in avoiding the vanishing gradient problem, as it maintains a constant gradient for positive inputs. However, it introduces another issue known as the dying ReLU problem, where neurons can become permanently inactive if they consistently output zero. This occurs when the weighted inputs to a neuron consistently result in negative pre-activation values. In such cases, the neuron no longer contributes to learning. While techniques like gradient clipping[^gradient-clipping], batch normalization[^batch-norm], and careful initialization can mitigate these training instabilities, system-level robustness (handling hardware faults, software failures, and ensuring reliable training under real-world conditions) is addressed in @sec-robust-ai."
    },
    {
      "footnote_id": "batch-norm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 444,
      "context": "...ibutes to learning. While techniques like gradient clipping[^gradient-clipping], batch normalization[^batch-norm], and careful initialization can mitigate these training instabilities, system-level robustness (han...",
      "full_line": "ReLU is particularly effective in avoiding the vanishing gradient problem, as it maintains a constant gradient for positive inputs. However, it introduces another issue known as the dying ReLU problem, where neurons can become permanently inactive if they consistently output zero. This occurs when the weighted inputs to a neuron consistently result in negative pre-activation values. In such cases, the neuron no longer contributes to learning. While techniques like gradient clipping[^gradient-clipping], batch normalization[^batch-norm], and careful initialization can mitigate these training instabilities, system-level robustness (handling hardware faults, software failures, and ensuring reliable training under real-world conditions) is addressed in @sec-robust-ai."
    },
    {
      "footnote_id": "gradient-clipping",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 446,
      "context": "[^gradient-clipping]: **Gradient Clipping**: Technique that caps gradient values during backpropagation to prevent explo...",
      "full_line": "[^gradient-clipping]: **Gradient Clipping**: Technique that caps gradient values during backpropagation to prevent exploding gradients. Typically implemented by scaling gradients when their norm exceeds a threshold (e.g., clipping at norm 1.0), essential for training RNNs and transformers where gradients can grow exponentially through layers."
    },
    {
      "footnote_id": "batch-norm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 448,
      "context": "[^batch-norm]: **Batch Normalization**: A technique that standardizes the inputs to each neural network layer by...",
      "full_line": "[^batch-norm]: **Batch Normalization**: A technique that standardizes the inputs to each neural network layer by centering (subtracting the mean) and scaling (dividing by standard deviation) the activations within each training batch. After normalization, it applies learnable parameters to preserve the network's expressive power. This addresses \"internal covariate shift\"\u2014the problem where layer inputs constantly change distribution during training, making optimization harder. Benefits include enabling higher learning rates and providing regularization (reducing overfitting), though it requires extra memory to store statistics and becomes complex in distributed settings where batches are split across devices."
    },
    {
      "footnote_id": "fn-cnns",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 450,
      "context": "...y used in the hidden layers of neural networks, particularly in convolutional neural networks (CNNs)[^fn-cnns] and machine learning models for image and speech recognition tasks. Its computational simplicity an...",
      "full_line": "ReLU is commonly used in the hidden layers of neural networks, particularly in convolutional neural networks (CNNs)[^fn-cnns] and machine learning models for image and speech recognition tasks. Its computational simplicity and ability to prevent vanishing gradients make it ideal for training deep architectures."
    },
    {
      "footnote_id": "fn-sigmoid-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 522,
      "context": "...th software and hardware implementations. In software, this computation is expensive and inefficient[^fn-sigmoid-cost], particularly for deep networks or large datasets. Sigmoid suffers from vanishing gradients, especi...",
      "full_line": "The sigmoid function has smooth gradients and a bounded output in the range $(0, 1)$, making it useful in probabilistic settings. However, the computation of the sigmoid involves an exponential function, which becomes a key consideration in both software and hardware implementations. In software, this computation is expensive and inefficient[^fn-sigmoid-cost], particularly for deep networks or large datasets. Sigmoid suffers from vanishing gradients, especially for large input values, which can hinder the learning process in deep architectures. Its non-zero-centered output can also slow optimization, requiring more epochs to converge."
    },
    {
      "footnote_id": "fn-sigmoid-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 524,
      "context": "[^fn-sigmoid-cost]: **Sigmoid Computational Cost**: Computing sigmoid requires expensive exponential operations\u2014on CPU...",
      "full_line": "[^fn-sigmoid-cost]: **Sigmoid Computational Cost**: Computing sigmoid requires expensive exponential operations\u2014on CPU, `exp()` takes 10-20 clock cycles vs. 1 cycle for basic arithmetic. GPU implementations use 32-entry lookup tables with linear interpolation, reducing cost to 3-4 cycles but still 3x slower than ReLU. This overhead compounds in deep networks with millions of activations per forward pass."
    },
    {
      "footnote_id": "fn-rnns-lstms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 532,
      "context": "...ce-intensive than simpler functions. Despite these challenges, tanh remains common in RNNs and LSTMs[^fn-rnns-lstms] where balanced gradients are necessary.",
      "full_line": "In hardware, tanh uses its mathematical relationship with sigmoid (a scaled and shifted version) to optimize implementation. Modern hardware often implements tanh using a hybrid approach: lookup tables for common input ranges combined with piece-wise approximations for edge cases. This approach helps balance accuracy with computational efficiency, though tanh remains more resource-intensive than simpler functions. Despite these challenges, tanh remains common in RNNs and LSTMs[^fn-rnns-lstms] where balanced gradients are necessary."
    },
    {
      "footnote_id": "fn-relu-hardware",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 538,
      "context": "...ion requires just a single comparison and conditional set, translating to minimal circuit complexity[^fn-relu-hardware]. Modern GPUs and TPUs can implement ReLU using a simple multiplexer that checks the input's sign bi...",
      "full_line": "The hardware implementation of ReLU showcases why it has become the dominant activation function in modern neural networks. Its simple $\\max(0,x)$ operation requires just a single comparison and conditional set, translating to minimal circuit complexity[^fn-relu-hardware]. Modern GPUs and TPUs can implement ReLU using a simple multiplexer that checks the input's sign bit, allowing for extremely efficient parallel processing. This hardware efficiency, combined with the sparsity it introduces, results in both reduced computation time and lower memory bandwidth requirements."
    },
    {
      "footnote_id": "fn-relu-hardware",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 540,
      "context": "[^fn-relu-hardware]: **ReLU Hardware Efficiency**: ReLU requires just 1 instruction (`max(0,x)`) vs. sigmoid's 10+ oper...",
      "full_line": "[^fn-relu-hardware]: **ReLU Hardware Efficiency**: ReLU requires just 1 instruction (`max(0,x)`) vs. sigmoid's 10+ operations including exponentials. On NVIDIA GPUs, ReLU runs at 95% of peak FLOPS while sigmoid achieves only 30-40%. ReLU's sparsity (typically 50% zeros) enables additional optimizations: sparse matrix operations, reduced memory bandwidth, and compressed gradients during backpropagation."
    },
    {
      "footnote_id": "fn-transformer-attention",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 546,
      "context": "...es to perform normalization. This becomes particularly demanding in modern transformer architectures[^fn-transformer-attention], where softmax computations in attention mechanisms process thousands of values simultaneously. To...",
      "full_line": "At the hardware level, softmax faces unique challenges because it can't process each value independently like other activation functions. Unlike ReLU's simple threshold or even sigmoid's per-value computation, softmax needs access to all values to perform normalization. This becomes particularly demanding in modern transformer architectures[^fn-transformer-attention], where softmax computations in attention mechanisms process thousands of values simultaneously. To manage these demands, hardware implementations often use approximation techniques or simplified versions of softmax, especially when dealing with large vocabularies or attention mechanisms."
    },
    {
      "footnote_id": "fn-sgd-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 606,
      "context": "These system limitations motivated the development of more efficient optimization approaches. SGD[^fn-sgd-history] is a big shift in the optimization strategy. Rather than computing gradients over the entire datase...",
      "full_line": "These system limitations motivated the development of more efficient optimization approaches. SGD[^fn-sgd-history] is a big shift in the optimization strategy. Rather than computing gradients over the entire dataset, SGD estimates gradients using individual training examples:"
    },
    {
      "footnote_id": "fn-sgd-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 610,
      "context": "[^fn-sgd-history]: **Stochastic Gradient Descent**: Originally developed by Robbins and Monro in 1951 for statistical...",
      "full_line": "[^fn-sgd-history]: **Stochastic Gradient Descent**: Originally developed by Robbins and Monro in 1951 for statistical optimization, SGD was first applied to neural networks by Rosenblatt for the perceptron in 1958. The method remained largely theoretical until the 1980s when computational constraints made full-batch gradient descent impractical for larger networks. Today's \"mini-batch SGD\" (processing 32-512 examples) represents a compromise between the original single-example approach and full-batch methods, enabling parallel processing on modern GPUs. The stochastic nature of these updates introduces noise into the optimization process, but this noise often helps escape local minima and reach better solutions."
    },
    {
      "footnote_id": "fn-training-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 702,
      "context": "Mixed-precision training[^fn-training-mixed-precision] has been shown to achieve comparable accuracy while significantly reducing memory consumption and c...",
      "full_line": "Mixed-precision training[^fn-training-mixed-precision] has been shown to achieve comparable accuracy while significantly reducing memory consumption and computational overhead [@micikevicius2017mixed; @krishnamoorthi2018quantizing]."
    },
    {
      "footnote_id": "fn-training-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 704,
      "context": "[^fn-training-mixed-precision]: **Mixed-Precision Training**: Introduced by NVIDIA in 2018, this technique uses FP16 for forward/b...",
      "full_line": "[^fn-training-mixed-precision]: **Mixed-Precision Training**: Introduced by NVIDIA in 2018, this technique uses FP16 for forward/backward passes while maintaining FP32 precision for loss scaling, enabling 2x memory savings and 1.6x speedups on Tensor Core GPUs while maintaining model accuracy."
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 861,
      "context": "The backpropagation algorithm[^fn-backpropagation] computes gradients by systematically moving backward through a neural network's computational graph...",
      "full_line": "The backpropagation algorithm[^fn-backpropagation] computes gradients by systematically moving backward through a neural network's computational graph. While earlier discussions introduced backpropagation's mathematical principles, implementing this algorithm in training systems requires careful management of memory, computation, and data flow."
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 863,
      "context": "[^fn-backpropagation]: **Backpropagation Algorithm**: Independently rediscovered multiple times, backpropagation was popu...",
      "full_line": "[^fn-backpropagation]: **Backpropagation Algorithm**: Independently rediscovered multiple times, backpropagation was popularized by Rumelhart, Hinton, and Williams in 1986 (though similar ideas appeared in Werbos 1974). This breakthrough enabled training of deep networks by efficiently computing gradients in O(n) time vs. naive O(n\u00b2) approaches. Modern implementations require careful memory management\u2014storing all activations for a ResNet-50 consumes 1.2GB per image."
    },
    {
      "footnote_id": "fn-autodiff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 887,
      "context": "...ation algorithms like SGD or Adam discussed earlier. Modern training systems use autodifferentiation[^fn-autodiff] to handle these computations automatically, but the underlying system requirements remain the same.",
      "full_line": "This equation reveals key requirements for training systems. Computing gradients for early layers requires information from all later layers, creating specific patterns in data storage and access. Each gradient computation requires access to stored activations from the forward pass, creating a specific pattern of memory access and computation that training systems must manage efficiently. These patterns directly influence the efficiency of optimization algorithms like SGD or Adam discussed earlier. Modern training systems use autodifferentiation[^fn-autodiff] to handle these computations automatically, but the underlying system requirements remain the same."
    },
    {
      "footnote_id": "fn-autodiff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 889,
      "context": "[^fn-autodiff]: **Automatic Differentiation**: Not to be confused with symbolic or numerical differentiation, auto...",
      "full_line": "[^fn-autodiff]: **Automatic Differentiation**: Not to be confused with symbolic or numerical differentiation, autodiff constructs a computational graph at runtime and applies the chain rule systematically. PyTorch uses \"define-by-run\" (dynamic graphs built during forward pass) while TensorFlow v1 used static graphs. This enables complex architectures like RNNs and transformers where graph structure changes dynamically, but requires careful memory management since the entire forward computation graph must be preserved for the backward pass."
    },
    {
      "footnote_id": "fn-etl-elt-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1243,
      "context": "...itionally implemented through Extract-Transform-Load (ETL) or Extract-Load-Transform (ELT) pipelines[^fn-etl-elt-ml], is a critical determinant of training system performance. The throughput of preprocessing operatio...",
      "full_line": "As the data becomes available, data preprocessing transforms raw input data into a format suitable for model training. This process, traditionally implemented through Extract-Transform-Load (ETL) or Extract-Load-Transform (ELT) pipelines[^fn-etl-elt-ml], is a critical determinant of training system performance. The throughput of preprocessing operations can be expressed mathematically as:"
    },
    {
      "footnote_id": "fn-etl-elt-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1246,
      "context": "[^fn-etl-elt-ml]: **ETL vs ELT in ML**: Traditional data warehousing used ETL (extract, transform, load) with expens...",
      "full_line": "[^fn-etl-elt-ml]: **ETL vs ELT in ML**: Traditional data warehousing used ETL (extract, transform, load) with expensive transformation on powerful central servers. Modern ML systems often prefer ELT (extract, load, transform) where raw data is loaded first, then transformed on-demand during training. This shift enables data augmentation (rotating images, adding noise) to create virtually unlimited training variations from the same source data, a technique impossible in traditional ETL where transformations are fixed."
    },
    {
      "footnote_id": "fn-memory-hierarchy-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1279,
      "context": "Machine learning systems manage complex data flows through multiple memory tiers[^fn-memory-hierarchy-ml] while coordinating pipeline operations. The interplay between memory bandwidth constraints and pipe...",
      "full_line": "Machine learning systems manage complex data flows through multiple memory tiers[^fn-memory-hierarchy-ml] while coordinating pipeline operations. The interplay between memory bandwidth constraints and pipeline execution directly impacts training performance. The maximum data transfer rate through the memory hierarchy is bounded by:"
    },
    {
      "footnote_id": "fn-memory-hierarchy-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1283,
      "context": "[^fn-memory-hierarchy-ml]: **Memory Hierarchy in ML**: Unlike traditional CPU programs that focus on cache locality, ML train...",
      "full_line": "[^fn-memory-hierarchy-ml]: **Memory Hierarchy in ML**: Unlike traditional CPU programs that focus on cache locality, ML training creates massive data flows between storage (TB datasets), system RAM (GB models), and GPU memory (GB activations). The 1000x bandwidth gap between storage (1-2 GB/s) and GPU memory (900+ GB/s) forces ML systems to use sophisticated prefetching and caching strategies. Traditional cache optimization (spatial/temporal locality) is less relevant than managing bulk data transfers efficiently."
    },
    {
      "footnote_id": "fn-convolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1332,
      "context": "...these basic matrix operations to include specialized computational patterns. Convolutional networks[^fn-convolution], for instance, perform systematic kernel operations across input tensors. Consider a typical input...",
      "full_line": "Modern neural architectures extend beyond these basic matrix operations to include specialized computational patterns. Convolutional networks[^fn-convolution], for instance, perform systematic kernel operations across input tensors. Consider a typical input tensor of dimensions $64 \\times 224 \\times 224 \\times 3$ (batch size $\\times$ height $\\times$ width $\\times$ channels) processed by $7 \\times 7$ kernels. Each position requires 147 multiply-accumulate operations, and with 64 filters operating across $218 \\times 218$ spatial dimensions, the computational demands become substantial."
    },
    {
      "footnote_id": "fn-attention-mechanisms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1334,
      "context": "Transformer architectures introduce attention mechanisms[^fn-attention-mechanisms], which compute similarity scores between sequences. These operations combine matrix multiplications...",
      "full_line": "Transformer architectures introduce attention mechanisms[^fn-attention-mechanisms], which compute similarity scores between sequences. These operations combine matrix multiplications with softmax normalization, requiring efficient broadcasting and reduction operations across varying sequence lengths. The computational pattern here differs significantly from convolutions, demanding flexible execution strategies from hardware accelerators."
    },
    {
      "footnote_id": "fn-fp16-range",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1951,
      "context": "One of the key challenges with FP16 is its reduced dynamic range[^fn-fp16-range], which increases the likelihood of gradient values becoming too small to be represented accurately....",
      "full_line": "One of the key challenges with FP16 is its reduced dynamic range[^fn-fp16-range], which increases the likelihood of gradient values becoming too small to be represented accurately. Loss scaling addresses this issue by temporarily amplifying gradient values during backpropagation. Specifically, the loss value is scaled by a large factor (e.g., $2^{10}$) before gradients are computed, ensuring they remain within the representable range of FP16."
    },
    {
      "footnote_id": "fn-fp16-range",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1953,
      "context": "[^fn-fp16-range]: **FP16 Dynamic Range**: IEEE 754 half-precision (FP16) has only 5 exponent bits vs. 8 in FP32, lim...",
      "full_line": "[^fn-fp16-range]: **FP16 Dynamic Range**: IEEE 754 half-precision (FP16) has only 5 exponent bits vs. 8 in FP32, limiting its range to \u00b165,504 (vs. \u00b13.4\u00d710\u00b3\u2078 for FP32). More critically, FP16's smallest representable positive number is 6\u00d710\u207b\u2078, while gradients in deep networks often fall below 10\u207b\u00b9\u2070. This mismatch causes gradient underflow, where tiny but important gradients become zero, stalling training\u2014hence the need for loss scaling techniques. Once the gradients are computed, the scaling factor is reversed during the weight update step to restore the original gradient magnitude. This process allows FP16 to be used effectively without sacrificing numerical stability."
    },
    {
      "footnote_id": "fn-gradient-accumulation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2211,
      "context": "Gradient accumulation[^fn-gradient-accumulation] and activation checkpointing[^fn-training-activation-checkpointing] provide solutions to the memory...",
      "full_line": "Gradient accumulation[^fn-gradient-accumulation] and activation checkpointing[^fn-training-activation-checkpointing] provide solutions to the memory limitations often encountered in training large-scale machine learning models. By optimizing how memory is used during training, these techniques enable the development and deployment of complex architectures, even on hardware with constrained resources."
    },
    {
      "footnote_id": "fn-training-activation-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2211,
      "context": "Gradient accumulation[^fn-gradient-accumulation] and activation checkpointing[^fn-training-activation-checkpointing] provide solutions to the memory limitations often encountered in training large-scale machine learn...",
      "full_line": "Gradient accumulation[^fn-gradient-accumulation] and activation checkpointing[^fn-training-activation-checkpointing] provide solutions to the memory limitations often encountered in training large-scale machine learning models. By optimizing how memory is used during training, these techniques enable the development and deployment of complex architectures, even on hardware with constrained resources."
    },
    {
      "footnote_id": "fn-gradient-accumulation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2213,
      "context": "[^fn-gradient-accumulation]: **Gradient Accumulation Impact**: Enables effective batch sizes of 2048+ on single GPUs with only...",
      "full_line": "[^fn-gradient-accumulation]: **Gradient Accumulation Impact**: Enables effective batch sizes of 2048+ on single GPUs with only 32-64 mini-batch size, essential for transformer training. BERT-Large training uses effective batch size of 256 (accumulated over 8 steps) achieving 99.5% of full-batch performance while reducing memory requirements by 8x. The technique trades 10-15% compute overhead for massive memory savings."
    },
    {
      "footnote_id": "fn-training-activation-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2215,
      "context": "[^fn-training-activation-checkpointing]: **Activation Checkpointing Trade-offs**: Reduces memory usage by 50-90% at the cost of 15-30% addi...",
      "full_line": "[^fn-training-activation-checkpointing]: **Activation Checkpointing Trade-offs**: Reduces memory usage by 50-90% at the cost of 15-30% additional compute time due to recomputation. For training GPT-3 on V100s, checkpointing enables 2.8x larger models (from 1.3B to 3.7B parameters) within 32GB memory constraints, making it essential for memory-bound large model training despite the compute penalty."
    },
    {
      "footnote_id": "fn-transformer-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2231,
      "context": "...achieve stable convergence. For example, models like GPT, BERT, and other transformer architectures[^fn-transformer-scaling] often benefit from larger batch sizes due to their improved gradient estimates. However, these batc...",
      "full_line": "A common use case for gradient accumulation is in training models that require large batch sizes to achieve stable convergence. For example, models like GPT, BERT, and other transformer architectures[^fn-transformer-scaling] often benefit from larger batch sizes due to their improved gradient estimates. However, these batch sizes can quickly exceed the memory capacity of GPUs, especially when working with high-dimensional inputs or multiple GPUs. By accumulating gradients over multiple smaller micro-batches, gradient accumulation enables the use of effective large batch sizes without exceeding memory limits. This is particularly beneficial for tasks like language modeling, sequence-to-sequence learning, and image classification, where batch size significantly impacts training dynamics."
    },
    {
      "footnote_id": "fn-transformer-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2233,
      "context": "[^fn-transformer-scaling]: **Transformer Batch Size Scaling**: Research shows transformers achieve optimal performance with b...",
      "full_line": "[^fn-transformer-scaling]: **Transformer Batch Size Scaling**: Research shows transformers achieve optimal performance with batch sizes of 256-4096 tokens, requiring gradient accumulation on most hardware. GPT-2 training improved perplexity by 0.3-0.5 points when increasing from batch size 32 to 512, demonstrating the critical importance of large effective batch sizes for language model convergence."
    },
    {
      "footnote_id": "fn-distributed-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2305,
      "context": "...ns like natural language processing, computer vision, and scientific computing. Distributed training[^fn-distributed-training] addresses this challenge by spreading the workload across multiple machines, which coordinate to tr...",
      "full_line": "The progression from single-machine to distributed training follows a natural scaling path: optimize locally first, then scale horizontally. This progression ensures that distributed systems operate efficiently at each node while adding the coordination mechanisms necessary for multi-machine training. Training machine learning models often requires scaling beyond a single machine due to increasing model complexity and dataset sizes. The demand for computational power, memory, and storage can exceed the capacity of individual devices, especially in domains like natural language processing, computer vision, and scientific computing. Distributed training[^fn-distributed-training] addresses this challenge by spreading the workload across multiple machines, which coordinate to train a single model efficiently."
    },
    {
      "footnote_id": "fn-distributed-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2307,
      "context": "[^fn-distributed-training]: **Distributed Training**: Google's DistBelief (2012) pioneered large-scale distributed neural netw...",
      "full_line": "[^fn-distributed-training]: **Distributed Training**: Google's DistBelief (2012) pioneered large-scale distributed neural network training, enabling models with billions of parameters across thousands of machines. This breakthrough led to modern frameworks like Horovod (2017) and PyTorch's DistributedDataParallel, democratizing distributed training for researchers worldwide."
    },
    {
      "footnote_id": "nvlink",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2311,
      "context": "...a single node introduces high-bandwidth communication requirements, typically handled through NVLink[^nvlink] or PCIe connections with NCCL[^nccl] optimization, while preserving the single-machine simplicity o...",
      "full_line": "The path from single-device to distributed training involves distinct complexity stages, each building upon the previous level's challenges. Single GPU training requires only local memory management and straightforward forward/backward passes, establishing the baseline computational patterns. Scaling to multiple GPUs within a single node introduces high-bandwidth communication requirements, typically handled through NVLink[^nvlink] or PCIe connections with NCCL[^nccl] optimization, while preserving the single-machine simplicity of fault tolerance and scheduling. The leap to multi-node distributed training brings substantially greater complexity: network communication overhead, fault tolerance requirements, and cluster orchestration challenges. Each scaling stage compounds the previous challenges\u2014communication bottlenecks intensify, synchronization overhead grows, and failure probability increases. This progression underscores why practitioners should optimize single-GPU performance before scaling, ensuring efficient resource utilization at each level."
    },
    {
      "footnote_id": "nccl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2311,
      "context": "...communication requirements, typically handled through NVLink[^nvlink] or PCIe connections with NCCL[^nccl] optimization, while preserving the single-machine simplicity of fault tolerance and scheduling. The...",
      "full_line": "The path from single-device to distributed training involves distinct complexity stages, each building upon the previous level's challenges. Single GPU training requires only local memory management and straightforward forward/backward passes, establishing the baseline computational patterns. Scaling to multiple GPUs within a single node introduces high-bandwidth communication requirements, typically handled through NVLink[^nvlink] or PCIe connections with NCCL[^nccl] optimization, while preserving the single-machine simplicity of fault tolerance and scheduling. The leap to multi-node distributed training brings substantially greater complexity: network communication overhead, fault tolerance requirements, and cluster orchestration challenges. Each scaling stage compounds the previous challenges\u2014communication bottlenecks intensify, synchronization overhead grows, and failure probability increases. This progression underscores why practitioners should optimize single-GPU performance before scaling, ensuring efficient resource utilization at each level."
    },
    {
      "footnote_id": "nvlink",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2313,
      "context": "[^nvlink]: **NVLink**: NVIDIA's proprietary high-speed interconnect providing up to 600 GB/s bidirectional ba...",
      "full_line": "[^nvlink]: **NVLink**: NVIDIA's proprietary high-speed interconnect providing up to 600 GB/s bidirectional bandwidth between GPUs, roughly 10x faster than PCIe Gen4. Essential for efficient multi-GPU training as it enables rapid gradient synchronization and tensor exchanges between devices."
    },
    {
      "footnote_id": "nccl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2315,
      "context": "[^nccl]: **NCCL (NVIDIA Collective Communications Library)**: Optimized library implementing efficient coll...",
      "full_line": "[^nccl]: **NCCL (NVIDIA Collective Communications Library)**: Optimized library implementing efficient collective operations (AllReduce, Broadcast, etc.) for multi-GPU and multi-node communication. Automatically selects optimal communication patterns based on hardware topology, critical for distributed training performance."
    },
    {
      "footnote_id": "fn-allreduce-algorithm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2754,
      "context": ".../s) or InfiniBand (200 Gb/s) help, the overhead remains substantial. NCCL's ring-allreduce algorithm[^fn-allreduce-algorithm] reduces this burden by organizing devices in a ring topology, but communication costs still grow wi...",
      "full_line": "Communication overhead represents the most significant bottleneck in data parallelism. During gradient synchronization, each device must exchange gradient updates, often hundreds of megabytes per step for large models. With 8 GPUs training a 1-billion-parameter model, each synchronization step might require transferring several gigabytes of data across the network. While high-speed interconnects like NVLink (300 GB/s) or InfiniBand (200 Gb/s) help, the overhead remains substantial. NCCL's ring-allreduce algorithm[^fn-allreduce-algorithm] reduces this burden by organizing devices in a ring topology, but communication costs still grow with model size and device count."
    },
    {
      "footnote_id": "fn-allreduce-algorithm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2756,
      "context": "[^fn-allreduce-algorithm]: **AllReduce Algorithm**: A collective communication primitive where each process contributes data...",
      "full_line": "[^fn-allreduce-algorithm]: **AllReduce Algorithm**: A collective communication primitive where each process contributes data and all processes receive the same combined result (typically a sum). Naive implementations require O(n\u00b2) messages for n devices. The ring-allreduce algorithm, developed for high-performance computing in the 1980s, reduces this to O(n) by organizing devices in a logical ring where each device communicates only with its neighbors, making it scalable for modern ML with hundreds of GPUs."
    },
    {
      "footnote_id": "fn-profiling-tools",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3345,
      "context": "One essential technique is profiling training workloads[^fn-profiling-tools]. Profiling involves collecting detailed metrics about the system's performance during training, suc...",
      "full_line": "One essential technique is profiling training workloads[^fn-profiling-tools]. Profiling involves collecting detailed metrics about the system's performance during training, such as computation times, memory usage, and communication overhead. These metrics help reveal inefficiencies, such as imbalanced resource usage or excessive time spent in specific stages of the training pipeline. Profiling tools such as NVIDIA Nsight Systems or TensorFlow Profiler can provide actionable insights, enabling developers to make informed adjustments to their training configurations."
    },
    {
      "footnote_id": "fn-profiling-tools",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3347,
      "context": "[^fn-profiling-tools]: **Training Profiling Tools**: NVIDIA Nsight Systems can identify that data loading consumes 20-40%...",
      "full_line": "[^fn-profiling-tools]: **Training Profiling Tools**: NVIDIA Nsight Systems can identify that data loading consumes 20-40% of training time in poorly optimized pipelines, while TensorFlow Profiler reveals GPU utilization rates (optimal: >90%). Intel VTune showed that memory bandwidth often limits performance more than raw compute\u2014typical deep learning workloads achieve only 30-50% of peak FLOPS due to memory bottlenecks."
    },
    {
      "footnote_id": "fn-lr-schedules",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3367,
      "context": "...er convergence or reduced generalization. Techniques like learning rate scaling and warmup schedules[^fn-lr-schedules] can help mitigate these issues, ensuring stable and effective training even with large batches.",
      "full_line": "One common scaling technique is batch size scaling. Increasing the batch size can reduce the number of synchronization steps required during training, as fewer updates are needed to process the same amount of data. This approach contrasts with the dynamic batching strategies used in inference serving, where the goal is optimizing throughput for variable-length requests rather than training convergence. However, larger batch sizes may introduce challenges, such as slower convergence or reduced generalization. Techniques like learning rate scaling and warmup schedules[^fn-lr-schedules] can help mitigate these issues, ensuring stable and effective training even with large batches."
    },
    {
      "footnote_id": "fn-lr-schedules",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3369,
      "context": "[^fn-lr-schedules]: **Learning Rate Schedules**: Critical for training stability and convergence. Cosine annealing (in...",
      "full_line": "[^fn-lr-schedules]: **Learning Rate Schedules**: Critical for training stability and convergence. Cosine annealing (introduced in 2016) and linear warmup (from BERT 2018) became standard after showing 2-5% accuracy improvements. Large batch training requires linear scaling rule: multiply learning rate by batch size ratio (batch 512 \u2192 LR 0.1, batch 4096 \u2192 LR 0.8), discovered through extensive experimentation by Facebook and Google teams."
    },
    {
      "footnote_id": "fn-nccl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3387,
      "context": "...odel parallelism. NVIDIA's ecosystem, including tools like [NCCL](https://developer.nvidia.com/nccl)[^fn-nccl] for multi-GPU communication, facilitates efficient parameter synchronization, a frequent challenge...",
      "full_line": "In distributed training systems, GPUs enable scalable strategies such as data parallelism and model parallelism. NVIDIA's ecosystem, including tools like [NCCL](https://developer.nvidia.com/nccl)[^fn-nccl] for multi-GPU communication, facilitates efficient parameter synchronization, a frequent challenge in large-scale setups. For example, in training large models like GPT-3[^fn-training-gpt3], GPUs were used in tandem with distributed frameworks to split computations across thousands of devices while addressing memory and compute scaling issues [@brown2020language]."
    },
    {
      "footnote_id": "fn-training-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3387,
      "context": "...zation, a frequent challenge in large-scale setups. For example, in training large models like GPT-3[^fn-training-gpt3], GPUs were used in tandem with distributed frameworks to split computations across thousands of dev...",
      "full_line": "In distributed training systems, GPUs enable scalable strategies such as data parallelism and model parallelism. NVIDIA's ecosystem, including tools like [NCCL](https://developer.nvidia.com/nccl)[^fn-nccl] for multi-GPU communication, facilitates efficient parameter synchronization, a frequent challenge in large-scale setups. For example, in training large models like GPT-3[^fn-training-gpt3], GPUs were used in tandem with distributed frameworks to split computations across thousands of devices while addressing memory and compute scaling issues [@brown2020language]."
    },
    {
      "footnote_id": "fn-nccl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3389,
      "context": "[^fn-nccl]: **NVIDIA NCCL (Collective Communications Library)**: Optimized for multi-GPU communication, NCCL a...",
      "full_line": "[^fn-nccl]: **NVIDIA NCCL (Collective Communications Library)**: Optimized for multi-GPU communication, NCCL achieves 90-95% of theoretical bandwidth on modern interconnects. On DGX systems with NVLink, NCCL can transfer 600 GB/s between 8 GPUs\u201450x faster than PCIe\u2014making efficient distributed training possible. It implements optimized AllReduce algorithms that reduce communication from O(n\u00b2) to O(n)."
    },
    {
      "footnote_id": "fn-training-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3391,
      "context": "[^fn-training-gpt3]: **GPT-3 Training Scale**: Used 10,000 NVIDIA V100 GPUs for 3-4 months, consuming ~1,287 MWh of ene...",
      "full_line": "[^fn-training-gpt3]: **GPT-3 Training Scale**: Used 10,000 NVIDIA V100 GPUs for 3-4 months, consuming ~1,287 MWh of energy (equivalent to 120 US homes for a year). The training cost was estimated at $4-12 million, demonstrating how specialized hardware and distributed systems enable training at previously impossible scales while highlighting the enormous resource requirements."
    },
    {
      "footnote_id": "fn-training-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3393,
      "context": "Hardware-specific features further enhance GPU performance. NVIDIA's tensor cores[^fn-training-tensor-cores], for instance, are optimized for mixed-precision training, which reduces memory usage while maintai...",
      "full_line": "Hardware-specific features further enhance GPU performance. NVIDIA's tensor cores[^fn-training-tensor-cores], for instance, are optimized for mixed-precision training, which reduces memory usage while maintaining numerical stability [@micikevicius2017mixed]. This directly addresses memory constraints, a common bottleneck in training massive models. Combined with software-level optimizations like fused kernels, GPUs deliver substantial speedups in both single-device and multi-device configurations."
    },
    {
      "footnote_id": "fn-training-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3395,
      "context": "[^fn-training-tensor-cores]: **Tensor Cores**: Introduced with NVIDIA's Volta architecture (2017), Tensor Cores deliver 4x spee...",
      "full_line": "[^fn-training-tensor-cores]: **Tensor Cores**: Introduced with NVIDIA's Volta architecture (2017), Tensor Cores deliver 4x speedup for mixed-precision training by performing 4x4 matrix operations in a single clock cycle. The H100's 4th-gen Tensor Cores achieve 989 TFLOPS for FP16 operations\u2014roughly 6x faster than traditional CUDA cores\u2014enabling training of larger models with the same hardware budget."
    },
    {
      "footnote_id": "fn-cuda-programming",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3399,
      "context": "...nvidia.com/en-us/data-center/nvlink/) and [CUDA-X libraries](https://developer.nvidia.com/cuda-zone)[^fn-cuda-programming], these challenges are continually being addressed.",
      "full_line": "Despite their advantages, GPUs are not without challenges. Effective utilization of GPUs demands careful attention to workload balancing and inter-device communication. Training systems must also consider the cost implications, as GPUs are resource-intensive and require optimized data centers to operate at scale. However, with innovations like [NVLink](https://www.nvidia.com/en-us/data-center/nvlink/) and [CUDA-X libraries](https://developer.nvidia.com/cuda-zone)[^fn-cuda-programming], these challenges are continually being addressed."
    },
    {
      "footnote_id": "fn-cuda-programming",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3401,
      "context": "[^fn-cuda-programming]: **CUDA Programming Model**: Introduced by NVIDIA in 2007, CUDA (Compute Unified Device Architectur...",
      "full_line": "[^fn-cuda-programming]: **CUDA Programming Model**: Introduced by NVIDIA in 2007, CUDA (Compute Unified Device Architecture) transformed GPUs from graphics processors into general-purpose parallel computing platforms. Unlike CPU programming with 4-16 cores, CUDA enables programming thousands of lightweight threads (32 threads per \"warp\"). ML frameworks like PyTorch and TensorFlow abstract away CUDA complexity, but understanding concepts like memory coalescing, shared memory, and occupancy remains crucial for optimizing custom ML operations."
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3415,
      "context": "...tasets are split across multiple devices. A key feature of TPUs is their systolic array architecture[^fn-systolic-array], which performs efficient matrix multiplications by streaming data through a network of processing...",
      "full_line": "Google developed TPUs with a primary goal: to accelerate machine learning workloads at scale while reducing the energy and infrastructure costs associated with traditional hardware. Their architecture is optimized for tasks that benefit from batch processing, making them particularly effective in distributed training systems where large datasets are split across multiple devices. A key feature of TPUs is their systolic array architecture[^fn-systolic-array], which performs efficient matrix multiplications by streaming data through a network of processing elements. This design minimizes data movement overhead, reducing latency and energy consumption\u2014critical factors for training large-scale models like transformers [@jouppi2017tpu]."
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3417,
      "context": "[^fn-systolic-array]: **Systolic Array Architecture**: Developed at Carnegie Mellon in 1978, systolic arrays excel at ma...",
      "full_line": "[^fn-systolic-array]: **Systolic Array Architecture**: Developed at Carnegie Mellon in 1978, systolic arrays excel at matrix operations by streaming data through a grid of processing elements. Google's TPU v4 systolic array performs 275 TFLOPS while consuming only 175W\u2014achieving 1.57 TFLOPS/W efficiency, roughly 2-3x more energy-efficient than comparable GPUs for ML workloads."
    },
    {
      "footnote_id": "fn-fpga-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3433,
      "context": "...eries or language translation services. By integrating FPGAs directly into their data center network[^fn-fpga-datacenter], Microsoft has achieved significant performance gains while minimizing power consumption.",
      "full_line": "Microsoft had been exploring the use of FPGAs for a while, as seen in @fig-inference-fpgas, with one prominent example being [Project Brainwave](https://www.microsoft.com/en-us/research/project/project-brainwave/). This initiative uses FPGAs to accelerate machine learning workloads in the Azure cloud. Microsoft chose FPGAs for their ability to provide low-latency inference (not training) while maintaining high throughput. This approach benefits scenarios where real-time predictions are critical, such as search engine queries or language translation services. By integrating FPGAs directly into their data center network[^fn-fpga-datacenter], Microsoft has achieved significant performance gains while minimizing power consumption."
    },
    {
      "footnote_id": "fn-fpga-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3435,
      "context": "[^fn-fpga-datacenter]: **Microsoft FPGA Deployment**: Project Catapult deployed FPGAs across Microsoft's entire datacente...",
      "full_line": "[^fn-fpga-datacenter]: **Microsoft FPGA Deployment**: Project Catapult deployed FPGAs across Microsoft's entire datacenter fleet by 2016, with one FPGA per server (>1 million total). This $1 billion investment improved Bing search latency by 50% and Azure ML inference by 2x, while reducing power consumption by 10-15% through specialized acceleration of specific algorithms."
    },
    {
      "footnote_id": "fn-wse-specs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3455,
      "context": "...rchitecture enables Cerebras to pack 2.6 trillion transistors and 850,000 cores onto a single device[^fn-wse-specs]. These cores are connected via a high-bandwidth, low-latency interconnect, allowing data to move ac...",
      "full_line": "The Cerebras WSE is unlike traditional chips in that it is a single wafer-scale processor, spanning the entire silicon wafer rather than being cut into smaller chips. This architecture enables Cerebras to pack 2.6 trillion transistors and 850,000 cores onto a single device[^fn-wse-specs]. These cores are connected via a high-bandwidth, low-latency interconnect, allowing data to move across the chip without the bottlenecks associated with external communication between discrete GPUs or TPUs [@Feldman2020]."
    },
    {
      "footnote_id": "fn-wse-specs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3457,
      "context": "[^fn-wse-specs]: **Wafer-Scale Engine Specifications**: The WSE-2 (2021) contains 2.6 trillion transistors on a 21c...",
      "full_line": "[^fn-wse-specs]: **Wafer-Scale Engine Specifications**: The WSE-2 (2021) contains 2.6 trillion transistors on a 21cm x 21cm wafer\u2014the largest chip ever manufactured. With 850,000 cores and 40GB on-chip memory, it delivers 15-20x speedup vs. GPU clusters for large language models while consuming 15kW (comparable to 16-20 V100 GPUs but with orders of magnitude less communication overhead)."
    },
    {
      "footnote_id": "fn-transformers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3503,
      "context": "[^fn-transformers]: **Transformer Architectures**: Detailed in @sec-dnn-architectures. Transformer models use attentio...",
      "full_line": "[^fn-transformers]: **Transformer Architectures**: Detailed in @sec-dnn-architectures. Transformer models use attention mechanisms to process sequences without recurrence, enabling parallel computation and capturing long-range dependencies more effectively than RNNs."
    },
    {
      "footnote_id": "fn-transformer-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3505,
      "context": "[^fn-transformer-training]: **Transformer Training**: Large transformer models like GPT and BERT require specialized training...",
      "full_line": "[^fn-transformer-training]: **Transformer Training**: Large transformer models like GPT and BERT require specialized training techniques covered in @sec-dnn-architectures, including attention computation optimization and sequence parallelism strategies."
    },
    {
      "footnote_id": "fn-rnns",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3507,
      "context": "[^fn-rnns]: **Recurrent Neural Networks**: Architecture details covered in @sec-dnn-architectures. RNNs proces...",
      "full_line": "[^fn-rnns]: **Recurrent Neural Networks**: Architecture details covered in @sec-dnn-architectures. RNNs process sequential data by maintaining hidden states that capture information from previous time steps."
    },
    {
      "footnote_id": "fn-cnns",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3509,
      "context": "[^fn-cnns]: **Convolutional Neural Networks**: Architecture and convolution operation details covered in @sec-...",
      "full_line": "[^fn-cnns]: **Convolutional Neural Networks**: Architecture and convolution operation details covered in @sec-dnn-architectures. CNNs use shared kernels to detect spatial patterns in grid-structured data like images."
    },
    {
      "footnote_id": "fn-rnns-lstms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3511,
      "context": "[^fn-rnns-lstms]: **RNNs and LSTMs**: Long Short-Term Memory networks are specialized RNN variants designed to handl...",
      "full_line": "[^fn-rnns-lstms]: **RNNs and LSTMs**: Long Short-Term Memory networks are specialized RNN variants designed to handle long-range dependencies. Both architectures are detailed in @sec-dnn-architectures."
    },
    {
      "footnote_id": "fn-transformer-attention",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3513,
      "context": "[^fn-transformer-attention]: **Transformer Attention**: The attention mechanism in transformers computes weighted relationships...",
      "full_line": "[^fn-transformer-attention]: **Transformer Attention**: The attention mechanism in transformers computes weighted relationships between all positions in a sequence simultaneously. This architecture is covered in @sec-dnn-architectures."
    },
    {
      "footnote_id": "fn-convolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3515,
      "context": "[^fn-convolution]: **Convolutional Operations**: Convolution operations apply learned filters across spatial dimensio...",
      "full_line": "[^fn-convolution]: **Convolutional Operations**: Convolution operations apply learned filters across spatial dimensions to detect features. The mathematical details and implementation considerations are covered in @sec-dnn-architectures."
    },
    {
      "footnote_id": "fn-attention-mechanisms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3517,
      "context": "[^fn-attention-mechanisms]: **Attention Mechanisms**: Attention allows models to focus on relevant parts of input sequences wh...",
      "full_line": "[^fn-attention-mechanisms]: **Attention Mechanisms**: Attention allows models to focus on relevant parts of input sequences when making predictions. The mathematical formulation and architectural implementations are detailed in @sec-dnn-architectures."
    },
    {
      "footnote_id": "fn-mlops-maturity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 138,
      "context": "...successfully by jumping directly to advanced techniques without first establishing solid foundations[^fn-mlops-maturity].",
      "full_line": "Each phase introduces new complexity while building upon foundations established in previous phases. This progressive approach reflects the reality that ML systems cannot be built successfully by jumping directly to advanced techniques without first establishing solid foundations[^fn-mlops-maturity]."
    },
    {
      "footnote_id": "fn-mlops-maturity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 140,
      "context": "[^fn-mlops-maturity]: **MLOps Maturity Models**: Organizations typically progress through 5 maturity levels, from manual...",
      "full_line": "[^fn-mlops-maturity]: **MLOps Maturity Models**: Organizations typically progress through 5 maturity levels, from manual processes (Level 0) to fully automated ML pipelines (Level 4). Google's MLOps maturity model [@kreuzberger2023machine], published in 2021, shows that approximately 20-25% of organizations reach Level 3+ automation, while 75-80% remain in manual or semi-automated processes. Companies at higher maturity levels report 35% faster time-to-market and 50% fewer production incidents, but require 18-24 months and significant cultural changes to advance between levels."
    },
    {
      "footnote_id": "fn-cicd-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 152,
      "context": "...enges are compounded in production ML systems, where continuous integration and deployment practices[^fn-cicd-ml] must account for both code changes and data evolution[^fn-data-evolution].",
      "full_line": "The following sections examine each phase in detail, showing how the DR case study navigated the complexity of real-world AI deployment while maintaining the integration principles that ensure long-term success. These challenges are compounded in production ML systems, where continuous integration and deployment practices[^fn-cicd-ml] must account for both code changes and data evolution[^fn-data-evolution]."
    },
    {
      "footnote_id": "fn-data-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 152,
      "context": "...egration and deployment practices[^fn-cicd-ml] must account for both code changes and data evolution[^fn-data-evolution].",
      "full_line": "The following sections examine each phase in detail, showing how the DR case study navigated the complexity of real-world AI deployment while maintaining the integration principles that ensure long-term success. These challenges are compounded in production ML systems, where continuous integration and deployment practices[^fn-cicd-ml] must account for both code changes and data evolution[^fn-data-evolution]."
    },
    {
      "footnote_id": "fn-cicd-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 154,
      "context": "[^fn-cicd-ml]: **CI/CD for Machine Learning**: Traditional continuous integration is designed for deterministic b...",
      "full_line": "[^fn-cicd-ml]: **CI/CD for Machine Learning**: Traditional continuous integration is designed for deterministic builds where code changes produce predictable outputs. ML systems violate this assumption because model behavior depends on training data, random initialization, and hardware differences. Google's TFX (TensorFlow Extended) [@baylor2017tfx] and similar platforms had to reinvent CI/CD principles for ML, introducing concepts like \"model validation\" and \"data validation\" that have no equivalent in traditional software. Survey data shows that 78% of ML teams report that their traditional DevOps tools are inadequate for ML workflows [@sculley2015hidden]."
    },
    {
      "footnote_id": "fn-data-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 156,
      "context": "[^fn-data-evolution]: **Data Evolution in Production**: Unlike traditional software where inputs are static, ML system i...",
      "full_line": "[^fn-data-evolution]: **Data Evolution in Production**: Unlike traditional software where inputs are static, ML system inputs evolve continuously\u2014user behavior changes, market conditions shift, and sensor data drifts. Netflix reports that their recommendation models see approximately 10-15% of features become stale monthly [@netflix2012recommendation], while financial fraud detection models experience 30-40% feature drift quarterly [@stripe2019machine]. This constant evolution means ML systems require \"data testing\" pipelines that validate 200+ statistical properties of incoming data, a complexity absent in traditional software where input validation involves simple type checking [@breck2017ml]."
    },
    {
      "footnote_id": "fn-fraud-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 184,
      "context": "...ccount balance > transaction amount, then allow transaction), while ML-based fraud detection systems[^fn-fraud-detection] learn to recognize suspicious patterns from historical transaction data. This shift from explicit p...",
      "full_line": "Machine learning systems require a different approach to this traditional lifecycle model. The deterministic nature of conventional software, where behavior is explicitly programmed, contrasts with the probabilistic nature of ML systems. Consider financial transaction processing: traditional systems follow predetermined rules (if account balance > transaction amount, then allow transaction), while ML-based fraud detection systems[^fn-fraud-detection] learn to recognize suspicious patterns from historical transaction data. This shift from explicit programming to learned behavior reshapes the development lifecycle, altering how we approach system reliability and robustness as detailed in @sec-robust-ai."
    },
    {
      "footnote_id": "fn-fraud-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 186,
      "context": "[^fn-fraud-detection]: **ML-Based Fraud Detection Evolution**: Traditional rule-based fraud systems (developed in the 199...",
      "full_line": "[^fn-fraud-detection]: **ML-Based Fraud Detection Evolution**: Traditional rule-based fraud systems (developed in the 1990s) had 60-70% accuracy and generated 20-30% false positives, causing customer friction. Modern ML fraud detection, pioneered by companies like PayPal (2000s) and Stripe (2010s), achieves 95%+ accuracy with <1% false positive rates by analyzing 500+ behavioral features in real-time [@stripe2019machine]. However, this improvement comes with new challenges: fraudsters adapt to ML patterns within 3-6 months, requiring continuous model retraining that rule-based systems never needed [@stripe2019machine]."
    },
    {
      "footnote_id": "fn-workflow-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 190,
      "context": "...system design, something traditional software engineering methodologies were not designed to handle[^fn-workflow-data-versioning].",
      "full_line": "The key distinctions are summarized in @tbl-sw-ml-cycles below. These differences reflect the core challenge of working with data as a first-class citizen in system design, something traditional software engineering methodologies were not designed to handle[^fn-workflow-data-versioning]."
    },
    {
      "footnote_id": "fn-workflow-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 192,
      "context": "[^fn-workflow-data-versioning]: **Data Versioning Challenges**: Unlike code, which changes through discrete edits, data can change...",
      "full_line": "[^fn-workflow-data-versioning]: **Data Versioning Challenges**: Unlike code, which changes through discrete edits, data can change gradually through drift, suddenly through schema changes, or subtly through quality degradation. Traditional version control systems like Git struggle with large datasets, leading to specialized tools like Git LFS (Large File Storage, 2015) and DVC (Data Version Control, 2017). Studies show that 87% of ML projects fail due to data issues, not algorithmic problems\u2014highlighting why ML workflows must treat data with the same rigor as code."
    },
    {
      "footnote_id": "fn-dr-statistics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 289,
      "context": "...hy affects over 100 million people worldwide and represents a leading cause of preventable blindness[^fn-dr-statistics]. @fig-eye-dr shows the clinical challenge: distinguishing healthy retinas from those showing early...",
      "full_line": "Diabetic retinopathy affects over 100 million people worldwide and represents a leading cause of preventable blindness[^fn-dr-statistics]. @fig-eye-dr shows the clinical challenge: distinguishing healthy retinas from those showing early signs of retinopathy, such as the characteristic hemorrhages visible as dark red spots. While this appears to be a straightforward image classification problem, the path from laboratory success to clinical deployment illustrates every aspect of the AI lifecycle complexity."
    },
    {
      "footnote_id": "fn-dr-statistics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 291,
      "context": "[^fn-dr-statistics]: **Diabetic Retinopathy Global Impact**: Affects over 103 million people worldwide, with 28.5% of d...",
      "full_line": "[^fn-dr-statistics]: **Diabetic Retinopathy Global Impact**: Affects over 103 million people worldwide, with 28.5% of diabetic patients developing some form of retinopathy [@who2019classification]. In developing countries, up to 90% of vision loss from diabetes is preventable with early detection, but access to ophthalmologists remains severely limited\u2014rural areas in India have one ophthalmologist per 120,000 people, compared to the WHO recommendation of 1 per 20,000 [@who2019classification]. This stark disparity makes AI-assisted screening not just convenient but potentially life-changing for millions [@rajkomar2019machine]."
    },
    {
      "footnote_id": "fn-healthcare-ai-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 297,
      "context": "...iors require holistic solutions. These deployment challenges reflect broader issues in healthcare AI[^fn-healthcare-ai-challenges] that affect most real-world medical ML applications.",
      "full_line": "The DR case study serves as our guide through each lifecycle stage, showing how decisions made in early phases influence later stages, how feedback loops drive continuous improvement, and how emergent system behaviors require holistic solutions. These deployment challenges reflect broader issues in healthcare AI[^fn-healthcare-ai-challenges] that affect most real-world medical ML applications."
    },
    {
      "footnote_id": "fn-healthcare-ai-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 299,
      "context": "[^fn-healthcare-ai-challenges]: **Healthcare AI Deployment Reality**: Studies show that 75-80% of healthcare AI projects never rea...",
      "full_line": "[^fn-healthcare-ai-challenges]: **Healthcare AI Deployment Reality**: Studies show that 75-80% of healthcare AI projects never reach clinical deployment [@chen2019machine], with the majority failing not due to algorithmic issues but due to integration challenges, regulatory hurdles, and workflow disruption. The \"AI chasm\" between research success and clinical adoption is particularly wide in healthcare\u2014while medical AI papers show 95%+ accuracy rates, real-world implementation studies report significant performance drops due to data drift, equipment variations, and user acceptance issues [@kelly2019key]."
    },
    {
      "footnote_id": "fn-problem-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 307,
      "context": "...teams to consider how the system will learn from data while operating within real-world constraints[^fn-problem-definition]. This stage lays the foundation for all subsequent phases in the ML lifecycle.",
      "full_line": "The development of machine learning systems begins with a challenge that differs from traditional software development: defining not just what the system should do, but how it should learn to do it. Unlike conventional software, where requirements translate directly into implementation rules, ML systems require teams to consider how the system will learn from data while operating within real-world constraints[^fn-problem-definition]. This stage lays the foundation for all subsequent phases in the ML lifecycle."
    },
    {
      "footnote_id": "fn-problem-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 309,
      "context": "[^fn-problem-definition]: **ML vs. Traditional Problem Definition**: Traditional software problems are defined by determinis...",
      "full_line": "[^fn-problem-definition]: **ML vs. Traditional Problem Definition**: Traditional software problems are defined by deterministic specifications (\"if input X, then output Y\"), but ML problems are defined by examples and desired behaviors. This shift means that 73% of ML project failures occur during problem definition, compared to only 32% for traditional software [@standish2020chaos]. The challenge lies in translating business objectives into learning objectives\u2014something that didn't exist in software engineering until the rise of data-driven systems in the 2000s [@amershi2019software]."
    },
    {
      "footnote_id": "fn-scaling-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 337,
      "context": "As ML systems scale, their problem definitions must adapt to new operational challenges[^fn-scaling-challenges]. For example, the DR project initially focused on a limited number of clinics with consistent imagi...",
      "full_line": "As ML systems scale, their problem definitions must adapt to new operational challenges[^fn-scaling-challenges]. For example, the DR project initially focused on a limited number of clinics with consistent imaging setups. However, as the system expanded to include clinics with varying equipment, staff expertise, and patient demographics[^fn-algorithmic-fairness], the original problem definition required adjustments to accommodate these variations."
    },
    {
      "footnote_id": "fn-algorithmic-fairness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 337,
      "context": "...system expanded to include clinics with varying equipment, staff expertise, and patient demographics[^fn-algorithmic-fairness], the original problem definition required adjustments to accommodate these variations.",
      "full_line": "As ML systems scale, their problem definitions must adapt to new operational challenges[^fn-scaling-challenges]. For example, the DR project initially focused on a limited number of clinics with consistent imaging setups. However, as the system expanded to include clinics with varying equipment, staff expertise, and patient demographics[^fn-algorithmic-fairness], the original problem definition required adjustments to accommodate these variations."
    },
    {
      "footnote_id": "fn-scaling-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 339,
      "context": "[^fn-scaling-challenges]: **ML System Scaling Complexity**: Scaling ML systems is exponentially more complex than traditiona...",
      "full_line": "[^fn-scaling-challenges]: **ML System Scaling Complexity**: Scaling ML systems is exponentially more complex than traditional software due to data heterogeneity, model drift, and infrastructure requirements. Studies show that ML systems require 10x more monitoring infrastructure than traditional applications [@paleyes2022challenges], with companies like Uber running 1,000+ model quality checks daily across their ML platform [@uber2017michelangelo]. The \"scaling wall\" typically hits at 100+ models in production, where manual processes break down and teams need specialized MLOps platforms\u2014explaining why the ML platform market grew from approximately $350M in 2019 to $4.0B in 2023, with pure MLOps tools reaching $1.2B in 2023 [@kreuzberger2023machine]."
    },
    {
      "footnote_id": "fn-data-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 347,
      "context": "...and preparation present unique challenges that extend beyond gathering sufficient training examples[^fn-data-challenges]. These fundamental challenges form the core focus of @sec-data-engineering. For medical AI systems...",
      "full_line": "Data collection and preparation present unique challenges that extend beyond gathering sufficient training examples[^fn-data-challenges]. These fundamental challenges form the core focus of @sec-data-engineering. For medical AI systems like DR screening, data collection must balance statistical rigor with operational feasibility while meeting the highest standards for diagnostic accuracy."
    },
    {
      "footnote_id": "fn-data-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 349,
      "context": "[^fn-data-challenges]: **The 80/20 Rule in ML**: Data scientists spend 80% of their time on data collection, cleaning, an...",
      "full_line": "[^fn-data-challenges]: **The 80/20 Rule in ML**: Data scientists spend 80% of their time on data collection, cleaning, and preparation\u2014only 20% on actual modeling. This ratio, first documented by CrowdFlower [@crowdflower2016data] in 2016, remains consistent across industries despite advances in automated tools. The \"data preparation tax\" includes handling missing values (present in 90% of real-world datasets), resolving inconsistencies (affecting 60% of data fields), and ensuring legal compliance (requiring 15+ different consent mechanisms for EU data). This explains why successful ML teams invest heavily in data engineering capabilities from day one."
    },
    {
      "footnote_id": "fn-medical-annotation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 355,
      "context": "...inal fundus photographs, each reviewed by 3-7 expert ophthalmologists from a panel of 54 specialists[^fn-medical-annotation]. This expert consensus approach addressed the inherent subjectivity in medical diagnosis while esta...",
      "full_line": "**Training Data Architecture:** The team assembled a development dataset of 128,000 retinal fundus photographs, each reviewed by 3-7 expert ophthalmologists from a panel of 54 specialists[^fn-medical-annotation]. This expert consensus approach addressed the inherent subjectivity in medical diagnosis while establishing ground truth labels that could withstand regulatory scrutiny. The annotation process captured clinically relevant features like microaneurysms, hemorrhages, and hard exudates across the spectrum of disease severity."
    },
    {
      "footnote_id": "fn-medical-annotation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 357,
      "context": "[^fn-medical-annotation]: **Medical Data Annotation Costs**: Expert medical annotation is extraordinarily expensive\u2014ophthalm...",
      "full_line": "[^fn-medical-annotation]: **Medical Data Annotation Costs**: Expert medical annotation is extraordinarily expensive\u2014ophthalmologists charge $200-500 per hour, meaning the DR dataset's annotation cost exceeded $2.7 million in expert time alone. This represents one of the highest annotation costs per sample in ML history. For comparison, ImageNet's 14 million images cost approximately $50,000 to annotate using crowdsourcing, while medical datasets can cost 100-1000x more per image. This cost disparity explains why medical AI often relies on transfer learning and why synthetic data generation is becoming crucial for healthcare applications."
    },
    {
      "footnote_id": "fn-federated-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 387,
      "context": "...ipment, workflows, and operating conditions. Each clinic effectively became an independent data node[^fn-federated-learning], yet the system needed to ensure consistent performance and reliability across all locations. The t...",
      "full_line": "As ML systems scale, the challenges of data collection grow exponentially. In the DR project, scaling from an initial few clinics to a broader network introduced significant variability in equipment, workflows, and operating conditions. Each clinic effectively became an independent data node[^fn-federated-learning], yet the system needed to ensure consistent performance and reliability across all locations. The team discovered that distributed coordination required specialized orchestration: data teams managed pipeline development and quality monitoring, model teams handled training infrastructure and experiment tracking, while infrastructure teams provided resource provisioning and automated scaling. Cross-team coordination protocols included shared artifact repositories for centralized model and data versioning, versioned APIs for component integration, and automated testing pipelines that validated cross-team integrations. This distributed workflow orchestration enabled the system to manage the 200+ clinic network efficiently, with centralized experiment tracking systems processing metadata from thousands of daily inference requests while maintaining sub-second response times for clinic operations."
    },
    {
      "footnote_id": "fn-federated-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 389,
      "context": "[^fn-federated-learning]: **Federated Learning Architecture**: Federated learning [@mcmahan2017communication], introduced by...",
      "full_line": "[^fn-federated-learning]: **Federated Learning Architecture**: Federated learning [@mcmahan2017communication], introduced by Google in 2016 for mobile keyboards, enables training across distributed data sources without centralizing data. Healthcare applications are particularly suited for federated learning due to privacy regulations\u2014studies show federated medical models achieve 85-95% of centralized model accuracy while keeping data local. However, federated learning introduces new challenges: communication costs increase 100-1000x per training iteration, and statistical heterogeneity across sites can cause model convergence issues that centralized training doesn't face."
    },
    {
      "footnote_id": "fn-algorithmic-fairness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 454,
      "context": "[^fn-algorithmic-fairness]: **Algorithmic Fairness in Healthcare**: Medical AI systems show significant performance disparitie...",
      "full_line": "[^fn-algorithmic-fairness]: **Algorithmic Fairness in Healthcare**: Medical AI systems show significant performance disparities across demographic groups\u2014dermatology AI trained on light-skinned patients shows 36% worse accuracy on dark-skinned patients [@larson2017gender], while diabetic retinopathy models trained primarily on European populations show 15-25% accuracy drops for Asian and African populations [@gulshan2016deep]. The FDA's 2021 Action Plan for AI/ML-based medical devices now requires demographic performance reporting [@fda2021artificial], and companies like Google Health spend 20-30% of development resources on fairness testing and bias mitigation across racial, gender, and socioeconomic groups [@rajkomar2019machine]."
    },
    {
      "footnote_id": "fn-hyperparameter-tuning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 458,
      "context": "...stage presents unique challenges that extend beyond selecting algorithms and tuning hyperparameters[^fn-hyperparameter-tuning]. The training methodologies, infrastructure requirements, and distributed training strategies are c...",
      "full_line": "Model development and training form the core of machine learning systems, yet this stage presents unique challenges that extend beyond selecting algorithms and tuning hyperparameters[^fn-hyperparameter-tuning]. The training methodologies, infrastructure requirements, and distributed training strategies are covered in @sec-ai-training. In high-stakes domains like healthcare, every design decision impacts clinical outcomes, making the integration of technical performance with operational constraints critical."
    },
    {
      "footnote_id": "fn-hyperparameter-tuning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 460,
      "context": "[^fn-hyperparameter-tuning]: **Hyperparameter Optimization Complexity**: Modern deep learning models have 10-100+ hyperparamete...",
      "full_line": "[^fn-hyperparameter-tuning]: **Hyperparameter Optimization Complexity**: Modern deep learning models have 10-100+ hyperparameters (learning rate, batch size, architecture choices), creating search spaces with 10^20+ possible combinations. AutoML platforms like Google's AutoML and H2O spend $10,000-100,000 in compute costs to find optimal configurations for complex models. Random search (2012) surprisingly outperforms grid search, while Bayesian optimization (2010s) and population-based training (2017) represent current state-of-the-art, reducing tuning time by 10-100x but still requiring substantial computational resources that didn't exist in traditional software development."
    },
    {
      "footnote_id": "fn-workflow-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 466,
      "context": "**Performance Achievement:** Using transfer learning from ImageNet[^fn-workflow-transfer-learning] and their meticulously labeled dataset of 128,000 images, the team achieved an F-score of 0.95, sli...",
      "full_line": "**Performance Achievement:** Using transfer learning from ImageNet[^fn-workflow-transfer-learning] and their meticulously labeled dataset of 128,000 images, the team achieved an F-score of 0.95, slightly exceeding the median performance of consulted ophthalmologists (0.91). This result validated their approach to combining large-scale pre-training with domain-specific fine-tuning."
    },
    {
      "footnote_id": "fn-workflow-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 468,
      "context": "[^fn-workflow-transfer-learning]: **Transfer Learning**: A technique where models pre-trained on large datasets (like ImageNet's 14...",
      "full_line": "[^fn-workflow-transfer-learning]: **Transfer Learning**: A technique where models pre-trained on large datasets (like ImageNet's 14 million images) are adapted for specific tasks, dramatically reducing training time and data requirements [@krizhevsky2012imagenet; @deng2009imagenet]. Introduced by Yann LeCun's team in the 1990s and popularized by the 2014 ImageNet competition, transfer learning became the foundation for most practical computer vision applications. Instead of training from scratch, practitioners can achieve expert-level performance with thousands rather than millions of training examples."
    },
    {
      "footnote_id": "fn-medical-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 478,
      "context": "**Clinical Performance Requirements:** Medical applications demand specific performance metrics[^fn-medical-metrics] that differ from standard ML evaluation. The DR system required >90% sensitivity (to prevent vision...",
      "full_line": "**Clinical Performance Requirements:** Medical applications demand specific performance metrics[^fn-medical-metrics] that differ from standard ML evaluation. The DR system required >90% sensitivity (to prevent vision loss from missed cases) and >80% specificity (to avoid overwhelming referral systems). These metrics had to be maintained across diverse patient populations and image quality conditions."
    },
    {
      "footnote_id": "fn-medical-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 480,
      "context": "[^fn-medical-metrics]: **Medical AI Performance Metrics**: Medical AI requires different metrics than general ML\u2014sensitiv...",
      "full_line": "[^fn-medical-metrics]: **Medical AI Performance Metrics**: Medical AI requires different metrics than general ML\u2014sensitivity (true positive rate) and specificity (true negative rate) are often more important than overall accuracy. For diabetic retinopathy screening, >90% sensitivity is crucial (missing cases causes blindness), while >80% specificity prevents unnecessary referrals. Medical AI also requires metrics like positive predictive value (PPV) and negative predictive value (NPV) that vary with disease prevalence in different populations\u2014a model with 95% accuracy in a lab setting might have only 50% PPV in a low-prevalence population, making it clinically useless despite high technical performance."
    },
    {
      "footnote_id": "fn-data-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 558,
      "context": "...rning systems. Unlike traditional software, ML systems must account for shifts in data distributions[^fn-data-drift], changing usage patterns, and evolving operational requirements[^fn-model-drift]. Monitoring provid...",
      "full_line": "Monitoring and maintenance represent the ongoing, critical processes that ensure the continued effectiveness and reliability of deployed machine learning systems. Unlike traditional software, ML systems must account for shifts in data distributions[^fn-data-drift], changing usage patterns, and evolving operational requirements[^fn-model-drift]. Monitoring provides the feedback necessary to adapt to these challenges, while maintenance ensures the system evolves to meet new needs. These operational practices form the foundation of @sec-ml-operations."
    },
    {
      "footnote_id": "fn-model-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 558,
      "context": "...n data distributions[^fn-data-drift], changing usage patterns, and evolving operational requirements[^fn-model-drift]. Monitoring provides the feedback necessary to adapt to these challenges, while maintenance ensures...",
      "full_line": "Monitoring and maintenance represent the ongoing, critical processes that ensure the continued effectiveness and reliability of deployed machine learning systems. Unlike traditional software, ML systems must account for shifts in data distributions[^fn-data-drift], changing usage patterns, and evolving operational requirements[^fn-model-drift]. Monitoring provides the feedback necessary to adapt to these challenges, while maintenance ensures the system evolves to meet new needs. These operational practices form the foundation of @sec-ml-operations."
    },
    {
      "footnote_id": "fn-data-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 560,
      "context": "[^fn-data-drift]: **Data Drift Detection**: Data drift occurs when input data characteristics change over time\u2014user...",
      "full_line": "[^fn-data-drift]: **Data Drift Detection**: Data drift occurs when input data characteristics change over time\u2014user behavior shifts, sensor calibration drifts, or population demographics evolve. Studies show that 78% of production ML models experience significant data drift within 12 months [@breck2017ml], yet only 23% of organizations have automated drift detection [@paleyes2022challenges]. Statistical tests like Kolmogorov-Smirnov and Population Stability Index can detect drift, but require setting thresholds and monitoring 100+ features continuously. Cloud providers now offer drift detection services (AWS SageMaker Model Monitor, Google AI Platform), but custom implementation remains necessary for domain-specific requirements."
    },
    {
      "footnote_id": "fn-model-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 562,
      "context": "[^fn-model-drift]: **Model Drift Phenomenon**: ML models degrade over time without any code changes\u2014a phenomenon unkn...",
      "full_line": "[^fn-model-drift]: **Model Drift Phenomenon**: ML models degrade over time without any code changes\u2014a phenomenon unknown in traditional software. Studies show that 50-80% of production ML models experience significant performance degradation within 6 months due to data drift, concept drift, or infrastructure drift [@polyzotis2019data]. This \"silent failure\" problem led to the development of specialized monitoring tools like Evidently AI (2020) and Fiddler (2018), creating an entirely new category of ML infrastructure that has no equivalent in traditional software engineering."
    },
    {
      "footnote_id": "fn-deployment-reality-gap",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 572,
      "context": "...retinal conditions, demonstrating that even a well-trained model could face blind spots in practice[^fn-deployment-reality-gap]. These insights informed maintenance strategies, including targeted updates to address specific cha...",
      "full_line": "Initial deployment highlighted several areas where the system failed to meet real-world needs, such as decreased accuracy in clinics with outdated equipment or lower-quality images. Monitoring systems detected performance drops in specific subgroups, such as patients with less common retinal conditions, demonstrating that even a well-trained model could face blind spots in practice[^fn-deployment-reality-gap]. These insights informed maintenance strategies, including targeted updates to address specific challenges and expanded training datasets to cover edge cases."
    },
    {
      "footnote_id": "fn-deployment-reality-gap",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 574,
      "context": "[^fn-deployment-reality-gap]: **The Lab-to-Clinic Performance Gap**: Medical AI systems typically see 10-30% performance drops w...",
      "full_line": "[^fn-deployment-reality-gap]: **The Lab-to-Clinic Performance Gap**: Medical AI systems typically see 10-30% performance drops when deployed in real-world settings, a phenomenon known as the \"deployment reality gap.\" This occurs because training data, despite best efforts, cannot capture the full diversity of real-world conditions\u2014different camera models, varying image quality, diverse patient populations, and operator skill levels all contribute to this gap. The gap is so consistent that regulatory bodies like the FDA now require \"real-world performance studies\" for medical AI approval, acknowledging that laboratory performance is insufficient to predict clinical utility."
    },
    {
      "footnote_id": "fn-ml-team-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 656,
      "context": "...uit; it's a collaborative endeavor that thrives on the diverse expertise of a multidisciplinary team[^fn-ml-team-evolution]. Each role in this intricate dance brings unique skills and insights, supporting different phases o...",
      "full_line": "Building effective and resilient machine learning systems is far more than a solo pursuit; it's a collaborative endeavor that thrives on the diverse expertise of a multidisciplinary team[^fn-ml-team-evolution]. Each role in this intricate dance brings unique skills and insights, supporting different phases of the AI development process. Understanding who these players are, what they contribute, and how they interconnect is crucial to navigating the complexities of modern AI systems."
    },
    {
      "footnote_id": "fn-ml-team-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 658,
      "context": "[^fn-ml-team-evolution]: **ML Team Role Evolution**: The \"data scientist\" role only emerged around 2008 (coined by DJ Patil...",
      "full_line": "[^fn-ml-team-evolution]: **ML Team Role Evolution**: The \"data scientist\" role only emerged around 2008 (coined by DJ Patil and Jeff Hammerbacher at Facebook and LinkedIn), while \"ML engineer\" became common around 2015 as companies realized that research models need production engineering. \"MLOps engineer\" appeared around 2018, and \"AI ethics officer\" became standard at major tech companies by 2020. This rapid role specialization reflects ML's evolution from research curiosity to production necessity\u2014modern enterprise ML teams average 8-12 distinct roles compared to 2-3 in traditional software teams."
    }
  ],
  "all_definitions": [
    {
      "footnote_id": "fn-ebola-outbreak",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 69,
      "definition": "**2014-2016 Ebola Outbreak**: This outbreak killed 11,323 people across six countries, with over 28,600 cases reported. The delayed international response\u2014WHO declared a Public Health Emergency only after 5 months\u2014demonstrated how early AI-powered disease surveillance could have saved thousands of lives. The economic cost exceeded $53 billion, highlighting the need for rapid detection systems that mobile health technologies now provide.",
      "term": "2014-2016 Ebola Outbreak",
      "length": 440
    },
    {
      "footnote_id": "fn-smallholder-farmers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 73,
      "definition": "**Smallholder Farmers Global Impact**: These farmers operate plots smaller than 2 hectares but produce 35% of global food supply, feeding 2 billion people directly. In sub-Saharan Africa, they comprise 80% of farms yet receive only 2% of agricultural credit. Climate change threatens their $2.6 trillion annual production value, making AI-powered agricultural support systems important for global food security and poverty reduction. Increasingly erratic weather patterns, pest outbreaks, and soil degradation compound their difficulties, often resulting in reduced yields and heightened food insecurity, particularly in vulnerable regions. These challenges demonstrate how systemic barriers and resource constraints perpetuate inequities and undermine resilience.",
      "term": "Smallholder Farmers Global Impact",
      "length": 764
    },
    {
      "footnote_id": "fn-cassava-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 95,
      "definition": "**Cassava Disease Impact**: Cassava feeds 800 million people globally and is a important food security crop in Africa. Cassava mosaic disease (CMD) and cassava brown streak disease (CBSD) can destroy entire harvests, affecting millions of smallholder farmers. The PlantVillage Nuru app has been downloaded by over 500,000 farmers across Kenya, Tanzania, and Uganda, demonstrating how mobile ML can scale agricultural expertise to underserved communities without internet connectivity.",
      "term": "Cassava Disease Impact",
      "length": 484
    },
    {
      "footnote_id": "fn-microclimate-monitoring",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 99,
      "definition": "**Microclimate Monitoring**: Unlike weather stations measuring regional conditions across 50-100 km areas, microclimate sensors detect variations within 10-meter zones crucial for rice cultivation. These sensors track temperature differences of 2-3\u00b0C, humidity variations of 10-15%, and soil moisture changes that can affect yields by 30%. TinyML enables real-time processing on sensors costing $5-10, versus traditional agricultural weather stations requiring $15,000+ investments.",
      "term": "Microclimate Monitoring",
      "length": 482
    },
    {
      "footnote_id": "fn-farmbeats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 103,
      "definition": "**Microsoft FarmBeats**: Launched in 2017, FarmBeats was deployed across several thousand farms before being integrated into Azure FarmBeats in 2021. During its deployment, the platform helped farmers reduce water usage by 30% and increase crop yields by 15-20%. The platform processed data from 50+ sensor types and could predict crop health issues 2-3 weeks before visible symptoms appeared, demonstrating how Cloud ML scales agricultural expertise to underserved farming communities.",
      "term": "Microsoft FarmBeats",
      "length": 486
    },
    {
      "footnote_id": "fn-cough-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 109,
      "definition": "**Cough Analysis Technology**: Pneumonia kills over 800,000 children under 5 annually, with most deaths occurring in resource-poor settings lacking access to chest X-rays. Cough analysis using TinyML can achieve 90%+ accuracy in pneumonia detection by analyzing acoustic features like cough duration, frequency, and spectral characteristics. The entire model runs on a microcontroller costing less than $10, democratizing diagnostic capabilities.",
      "term": "Cough Analysis Technology",
      "length": 446
    },
    {
      "footnote_id": "fn-mosquito-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 113,
      "definition": "**Mosquito Species Detection**: Malaria affects 241 million people annually, causing 627,000 deaths primarily in sub-Saharan Africa. TinyML-powered mosquito detection devices achieve 95% accuracy in species identification using just acoustic signatures, costing under $50 versus traditional morphological identification requiring $5,000+ microscopy equipment. These devices can monitor 24/7 and detect Anopheles mosquitoes (malaria vectors) versus Culex (nuisance only), enabling targeted intervention strategies.",
      "term": "Mosquito Species Detection",
      "length": 513
    },
    {
      "footnote_id": "fn-genomics-cloud",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 117,
      "definition": "**Cloud Genomics Scale**: Google Cloud processes over 50 petabytes of genomic data annually, equivalent to analyzing 15 million human genomes. A single genome contains 3 billion base pairs requiring 100GB storage, making cloud computing essential for population-scale analysis. Cloud ML can identify disease variants in hours versus months using traditional methods, accelerating drug discovery that typically takes 10-15 years and costs $1+ billion per new medicine.",
      "term": "Cloud Genomics Scale",
      "length": 467
    },
    {
      "footnote_id": "fn-thermal-imaging-rescue",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 127,
      "definition": "**Thermal Imaging in Disaster Response**: Human body temperature (37\u00b0C) contrasts sharply with debris temperature (often 15-25\u00b0C), enabling detection through 30cm of rubble. TinyML thermal analysis on drones can process 320x240 pixel thermal images at 9Hz using only 500mW power, operating for 20+ minutes on small batteries. This autonomous capability proved critical during the 2023 Turkey earthquake, where 72-hour survival windows made rapid victim location essential for the 50,000+ people trapped.",
      "term": "Thermal Imaging in Disaster Response",
      "length": 503
    },
    {
      "footnote_id": "fn-satellite-disaster",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 133,
      "definition": "**Satellite Disaster Monitoring**: Modern disaster monitoring processes 10+ terabytes of satellite imagery daily from sources like Landsat-8, Sentinel-2, and commercial providers. AI can detect flooding across 100,000+ km\u00b2 areas in 2-3 hours versus 2-3 days for human analysis. During 2022 Pakistan floods affecting 33 million people, satellite AI identified affected areas 48 hours before ground confirmation, enabling preemptive evacuations and resource positioning that saved thousands of lives.",
      "term": "Satellite Disaster Monitoring",
      "length": 498
    },
    {
      "footnote_id": "fn-gunshot-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 143,
      "definition": "**Acoustic Gunshot Detection**: TinyML can distinguish gunshots from other loud sounds (thunder, vehicle backfire) with 95%+ accuracy by analyzing specific acoustic signatures: frequency range 500-4000Hz, duration 1-5ms, and sharp onset characteristics. Solar-powered sensors covering 5-10 km\u00b2 cost $200-300 versus traditional systems requiring $50,000+ installations. In Kenya's conservancies, these systems reduce elephant poaching response time from 3-4 hours to 10-15 minutes, significantly increasing ranger safety and wildlife protection effectiveness.",
      "term": "Acoustic Gunshot Detection",
      "length": 558
    },
    {
      "footnote_id": "fn-global-fishing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 147,
      "definition": "**Global Fishing Watch Impact**: Since 2016, this platform has tracked over 70,000 vessels globally, processing 22+ million AIS (Automatic Identification System) data points daily. The system has helped identify $1.5 billion worth of illegal fishing activities and supported enforcement actions that recovered 180+ seized vessels. By making fishing activity transparent, the platform has contributed to 20% reductions in illegal fishing in monitored regions.",
      "term": "Global Fishing Watch Impact",
      "length": 458
    },
    {
      "footnote_id": "fn-sdg-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 167,
      "definition": "**SDG Global Impact**: Adopted by all 193 UN Member States, the SDGs represent the most ambitious global agenda in history, covering 169 specific targets with a $5-7 trillion annual funding gap. The goals build on the success of the Millennium Development Goals (2000-2015), which helped lift 1 billion people out of extreme poverty. Unlike their predecessors, the SDGs apply universally to all countries, recognizing that sustainable development requires global cooperation.",
      "term": "SDG Global Impact",
      "length": 475
    },
    {
      "footnote_id": "fn-sdg-ai-potential",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 169,
      "definition": "**AI's SDG Impact Potential**: McKinsey estimates AI could accelerate achievement of 134 of the 169 SDG targets, potentially contributing $13 trillion to global economic output by 2030. However, 97% of AI research focuses on SDG 9 (Industry/Innovation) while only 1% addresses basic needs like water, food, and health. This maldistribution means AI systems for social good require deliberate design to address the most important human needs rather than commercial applications.",
      "term": "AI's SDG Impact Potential",
      "length": 477
    },
    {
      "footnote_id": "fn-climate-action-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 171,
      "definition": "**AI for Climate Action**: Climate change causes $23 billion in annual economic losses globally, with temperatures rising 1.1\u00b0C above pre-industrial levels. AI systems for climate action include: carbon monitoring satellites tracking 50 billion tons of global emissions, smart grid optimization reducing energy waste by 15-20%, and climate modeling using exascale computing to predict regional impacts decades ahead. However, training large AI models can emit 626,000 pounds of CO\u2082\u2014equivalent to 5 cars' lifetime emissions\u2014highlighting the need for energy-efficient AI development.",
      "term": "AI for Climate Action",
      "length": 581
    },
    {
      "footnote_id": "fn-resource-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 250,
      "definition": "**Social Good Resource Paradox**: Resource-constrained environments need the most help but have the least infrastructure to deploy solutions. For example, rural sub-Saharan Africa has 60% of global arable land but only 4% of worldwide internet connectivity. This paradox forces engineers to achieve 90%+ model compression (from 50MB to 500KB) while maintaining effectiveness, a challenge absent in commercial deployments with abundant resources.",
      "term": "Social Good Resource Paradox",
      "length": 445
    },
    {
      "footnote_id": "fn-lora-technology",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 256,
      "definition": "**LoRa Technology**: Long Range (LoRa) allows IoT devices to communicate over 15+ kilometers with battery life exceeding 10 years. Operating in unlicensed spectrum bands, LoRa networks cost $1-5 per device annually versus $15-50 for cellular. This makes LoRa ideal for agricultural sensors monitoring soil moisture across vast farms or environmental sensors in remote conservation areas. Over 140 countries have deployed LoRaWAN networks, connecting 200+ million devices worldwide for social good applications.",
      "term": "LoRa Technology",
      "length": 510
    },
    {
      "footnote_id": "fn-raspberry-pi-development",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 274,
      "definition": "**Raspberry Pi Development Advantages**: Despite costing only $35-75, the Raspberry Pi 4 provides 1000\u00d7 more RAM and 10\u00d7 faster processing than typical production IoT devices. This substantial resource overhead enables developers to prototype using full Python frameworks like TensorFlow or PyTorch before optimizing for resource-constrained deployment. However, the Pi's 3-8W power consumption versus production devices' 0.1W creates a 30-80\u00d7 power gap that requires significant optimization during transition to real-world deployment.",
      "term": "Raspberry Pi Development Advantages",
      "length": 536
    },
    {
      "footnote_id": "fn-esp32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 278,
      "definition": "**ESP32 Capabilities**: Despite its constraints, the ESP32 costs only $2-5, consumes 30-150mA during operation, and includes Wi-Fi, Bluetooth, and various sensors. This makes it ideal for IoT deployments in social impact applications. For comparison, a smartphone processor is 100\u00d7 more powerful but costs 50\u00d7 more. The ESP32's limitations\u2014RAM smaller than a single Instagram photo\u2014force engineers to develop ingenious optimization techniques that often benefit all platforms.",
      "term": "ESP32 Capabilities",
      "length": 476
    },
    {
      "footnote_id": "fn-plantvillage-nuru",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 647,
      "definition": "**PlantVillage Nuru Real-World Impact**: Deployed across 500,000+ farmers in East Africa since 2019, Nuru has helped identify crop diseases affecting $2.6 billion worth of annual cassava production. The app works on $30 smartphones offline, processing 2.1 million crop images annually. Field studies show 73% reduction in crop losses and 40% increase in farmer incomes where the system is actively used, demonstrating how progressive enhancement patterns scale impact in resource-constrained environments.",
      "term": "PlantVillage Nuru Real-World Impact",
      "length": 505
    },
    {
      "footnote_id": "fn-whetstone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 90,
      "definition": "**Whetstone**: Named after the Whetstone ALGOL compiler, this 1964 benchmark measured floating-point arithmetic performance in KIPS (thousands of instructions per second). It became the first widely-adopted standardized performance test, revealing that IBM System/360 processors could vary by 10x in floating-point performance despite similar architectures.",
      "term": "Whetstone",
      "length": 357
    },
    {
      "footnote_id": "fn-linpack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 94,
      "definition": "**LINPACK**: Developed at Argonne National Laboratory to solve dense systems of linear equations, LINPACK became famous for the Top500 supercomputer rankings starting in 1993. Modern systems achieve over 1 exaflop (10^18 operations/second) compared to the original 1979 benchmarks measuring mere megaflops.",
      "term": "LINPACK",
      "length": 306
    },
    {
      "footnote_id": "fn-dhrystone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 96,
      "definition": "**Dhrystone**: Created by Reinhold Weicker to complement Whetstone's floating-point focus, Dhrystone measures integer and string operations in DMIPS (Dhrystone MIPS). Unlike synthetic floating-point tests, it aimed to reflect \"typical\" programming constructs, though it became vulnerable to compiler optimizations that could artificially inflate scores.",
      "term": "Dhrystone",
      "length": 353
    },
    {
      "footnote_id": "fn-spec",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 100,
      "definition": "**SPEC CPU**: Founded by major vendors including Sun, IBM, and DEC to combat \"benchmark wars\" with misleading metrics, SPEC CPU introduced standardized real-world applications like compression, compilers, and scientific computing. SPEC2017 includes 43 benchmarks representing actual workloads, with results reported as geometric means to prevent gaming, representing a 5000x improvement over early synthetic benchmarks.",
      "term": "SPEC CPU",
      "length": 419
    },
    {
      "footnote_id": "fn-3dmark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 104,
      "definition": "**3DMark**: Created by Finnish company Futuremark (now UL), this graphics benchmark drove GPU innovation by testing real-time 3D rendering capabilities. Early versions measured triangle throughput and texture fill rates; modern 3DMark tests ray tracing and DLSS performance, with scores ranging from mobile devices (1,000 points) to high-end gaming PCs (30,000+ points).",
      "term": "3DMark",
      "length": 370
    },
    {
      "footnote_id": "fn-mobilemark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 106,
      "definition": "**MobileMark**: Developed by the Business Applications Performance Corporation, this benchmark simulates real laptop usage patterns including web browsing, video playback, and productivity tasks. Unlike peak performance tests, MobileMark measures battery life under realistic workloads, typically showing 6-12 hour endurance for modern laptops versus 15-30 minutes for synthetic stress tests.",
      "term": "MobileMark",
      "length": 392
    },
    {
      "footnote_id": "fn-cloudsuite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 110,
      "definition": "**CloudSuite**: Developed at EPFL to address the gap between traditional benchmarks and modern datacenter workloads, CloudSuite includes realistic applications like web search, data analytics, and media streaming. Unlike synthetic benchmarks, it measures end-to-end performance including network latency, storage I/O, and memory bandwidth, revealing that cloud applications are memory-bound rather than CPU-bound. Machine learning has introduced another dimension of performance evaluation. The introduction of MLPerf in 2018 established a widely accepted standard for measuring machine learning training and inference efficiency across different hardware architectures.",
      "term": "CloudSuite",
      "length": 670
    },
    {
      "footnote_id": "fn-spec-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 120,
      "definition": "**SPEC Power**: Introduced in 2007 to address the growing importance of energy efficiency in server design, SPEC Power measures performance per watt across 10 different load levels from 10% to 100%. Results show that modern servers achieve 8-12 SPECpower_ssj2008 scores per watt, compared to 1-3 for systems from the mid-2000s, representing a 4x efficiency improvement.",
      "term": "SPEC Power",
      "length": 369
    },
    {
      "footnote_id": "fn-green500",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 122,
      "definition": "**Green500**: Started in 2007 as a counterpart to the Top500 supercomputer list, Green500 ranks systems by FLOPS per watt rather than raw performance. The most efficient systems achieve over 60 gigaFLOPS per watt compared to less than 1 gigaFLOPS/watt for early 2000s supercomputers, demonstrating improvements in computational efficiency.",
      "term": "Green500",
      "length": 339
    },
    {
      "footnote_id": "fn-energy-star",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 124,
      "definition": "**ENERGY STAR**: Launched by the EPA in 1992, this voluntary program has prevented over 4 billion tons of greenhouse gas emissions and saved consumers $450 billion on energy bills. Computing equipment must meet strict efficiency requirements: ENERGY STAR computers typically consume 30-65% less energy than standard models during operation and sleep modes.",
      "term": "ENERGY STAR",
      "length": 356
    },
    {
      "footnote_id": "fn-bench-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 166,
      "definition": "**ImageNet**: Created by Fei-Fei Li at Stanford starting in 2007, this dataset contains 14 million images across 20,000 categories, with 1.2 million images used for the annual classification challenge (ILSVRC). ImageNet's impact is profound: it sparked the deep learning revolution when AlexNet achieved 15.3% top-5 error in 2012, compared to 25.8% for traditional methods, the largest single-year improvement in computer vision.",
      "term": "ImageNet",
      "length": 429
    },
    {
      "footnote_id": "fn-bench-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 178,
      "definition": "**AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton at the University of Toronto, this 8-layer neural network revolutionized computer vision in 2012. With 60 million parameters trained on two GTX 580 GPUs, AlexNet introduced key innovations in neural network design that became standard techniques in modern AI.",
      "term": "AlexNet",
      "length": 338
    },
    {
      "footnote_id": "fn-bench-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 180,
      "definition": "**ResNet**: Microsoft's Residual Networks, introduced in 2015 by Kaiming He and colleagues, solved the vanishing gradient problem with skip connections, enabling networks with 152+ layers. ResNet-50 became the de facto standard for transfer learning, while ResNet-152 achieved superhuman performance on ImageNet with 3.57% top-5 error, exceeding the estimated 5% human error rate.",
      "term": "ResNet",
      "length": 380
    },
    {
      "footnote_id": "fn-bench-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 217,
      "definition": "**Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for neural network workloads, first deployed secretly in 2015 and announced in 2016. The first-generation TPU achieved 15-30x better performance per watt than contemporary GPUs for inference, while TPU v4 pods deliver 1.1 exaFLOPS of BF16 computing power (full pod configuration), demonstrating the capabilities of specialized AI hardware.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 416
    },
    {
      "footnote_id": "fn-asic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 219,
      "definition": "**Application-Specific Integrated Circuit (ASIC)**: Custom chips designed for specific computational tasks, offering superior performance and energy efficiency compared to general-purpose processors. AI ASICs like Google's TPUs, Tesla's FSD chips, and Bitcoin mining ASICs can achieve 100-1000x better efficiency than CPUs for their target applications, but lack the flexibility for other workloads.",
      "term": "Application-Specific Integrated Circuit (ASIC)",
      "length": 399
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 237,
      "definition": "**FLOPS**: Floating-Point Operations Per Second, a measure of computational performance indicating how many floating-point calculations a processor can execute in one second. Modern AI accelerators achieve high FLOPS ratings: NVIDIA A100 delivers 312 TFLOPS (trillion FLOPS) for tensor operations, while high-end CPUs achieve 1-10 TFLOPS. FLOPS measurements help compare hardware capabilities and determine computational bottlenecks in ML workloads.",
      "term": "FLOPS",
      "length": 449
    },
    {
      "footnote_id": "fn-roofline-model",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 239,
      "definition": "**Roofline Model**: A visual performance model developed at UC Berkeley that plots computational intensity (FLOPS/byte) against performance (FLOPS/second) to identify whether algorithms are compute-bound or memory-bound. The \"roofline\" represents theoretical peak performance limits, with flat sections indicating memory bandwidth constraints and sloped sections showing compute capacity limits. This model helps optimize both algorithms and hardware selection by revealing performance bottlenecks.",
      "term": "Roofline Model",
      "length": 498
    },
    {
      "footnote_id": "fn-bert",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 422,
      "definition": "**BERT**: Bidirectional Encoder Representations from Transformers, introduced by Google in 2018, revolutionized natural language processing by pre-training on vast text corpora using masked language modeling. BERT-Large contains 340 million parameters and achieved state-of-the-art results on 11 NLP tasks, establishing the foundation for modern language models like GPT and ChatGPT.",
      "term": "BERT",
      "length": 383
    },
    {
      "footnote_id": "fn-docker",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 481,
      "definition": "**Docker**: Containerization platform that packages applications and their dependencies into lightweight, portable containers ensuring consistent execution across different environments. Widely adopted in ML benchmarking since 2013, Docker eliminates \"works on my machine\" problems by providing identical runtime environments, with MLPerf and other benchmark suites distributing official Docker images to guarantee reproducible results.",
      "term": "Docker",
      "length": 436
    },
    {
      "footnote_id": "fn-tensor-ops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 661,
      "definition": "**Tensor Operations**: Multi-dimensional array computations that form the backbone of neural networks, including matrix multiplication (GEMM), convolution, and element-wise operations. Modern AI accelerators optimize these primitives: NVIDIA's Tensor Cores can achieve 312 TFLOPS for mixed-precision matrix multiplications, compared to 15 TFLOPS for traditional FP32 computations\u2014a 20x speedup.",
      "term": "Tensor Operations",
      "length": 394
    },
    {
      "footnote_id": "fn-cudnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 663,
      "definition": "**cuDNN**: CUDA Deep Neural Network library, NVIDIA's GPU-accelerated library of primitives for deep neural networks. Released in 2014, cuDNN provides highly optimized implementations for convolutions, pooling, normalization, and activation layers, delivering up to 10x performance improvements over naive implementations and becoming the de facto standard for GPU-accelerated deep learning.",
      "term": "cuDNN",
      "length": 391
    },
    {
      "footnote_id": "fn-bench-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 725,
      "definition": "**GPT-3**: OpenAI's 2020 language model with 175 billion parameters, trained on 300 billion tokens using 10,000 NVIDIA V100 GPUs for several months at an estimated cost of $4.6 million (Lambda Labs estimate). GPT-3 demonstrated emergent abilities like few-shot learning and in-context reasoning, establishing the paradigm of scaling laws where larger models consistently outperform smaller ones across diverse language tasks.",
      "term": "GPT-3",
      "length": 425
    },
    {
      "footnote_id": "fn-bench-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 953,
      "definition": "**Mixed-Precision Training**: A training technique that uses both 16-bit (FP16) and 32-bit (FP32) floating-point representations to accelerate training while maintaining model accuracy. Introduced by NVIDIA in 2017, mixed precision can achieve 1.5-2x speedups on modern GPUs with Tensor Cores while reducing memory usage by ~40%, enabling larger batch sizes and faster convergence for large models.",
      "term": "Mixed-Precision Training",
      "length": 398
    },
    {
      "footnote_id": "fn-data-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1010,
      "definition": "**Data Parallelism**: The most common distributed training strategy where each GPU processes a different subset of the training batch, then synchronizes gradients across all nodes. Modern implementations use techniques like gradient accumulation and all-reduce operations to achieve near-linear scaling up to hundreds of GPUs, though communication overhead typically limits efficiency beyond 1000+ GPUs.",
      "term": "Data Parallelism",
      "length": 403
    },
    {
      "footnote_id": "fn-model-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1012,
      "definition": "**Model Parallelism**: A distributed training approach where different parts of the neural network are placed on different GPUs, essential for models too large to fit in a single GPU's memory. GPT-3's 175B parameters required model parallelism across multiple nodes, as even high-memory GPUs can only hold ~40B parameters in mixed precision.",
      "term": "Model Parallelism",
      "length": 341
    },
    {
      "footnote_id": "fn-npu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1170,
      "definition": "**Neural Processing Unit (NPU)**: Specialized processors designed specifically for AI workloads, featuring optimized architectures for neural network operations. Modern smartphones include NPUs capable of 1-15 TOPS (Tera Operations Per Second), enabling on-device AI while consuming 100-1000x less power than GPUs for the same ML tasks.",
      "term": "Neural Processing Unit (NPU)",
      "length": 336
    },
    {
      "footnote_id": "fn-fpga",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1172,
      "definition": "**Field-Programmable Gate Array (FPGA)**: Reconfigurable silicon chips that can be programmed after manufacturing to implement custom digital circuits. Unlike fixed ASICs, FPGAs offer flexibility to optimize for different algorithms, achieving 10-100x better energy efficiency than CPUs for specific ML workloads while maintaining adaptability to algorithm changes.",
      "term": "Field-Programmable Gate Array (FPGA)",
      "length": 365
    },
    {
      "footnote_id": "fn-edge-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1174,
      "definition": "**Edge TPU**: Google's ultra-low-power AI accelerator designed for edge devices, consuming only 2 watts while delivering 4 TOPS of performance. Each Edge TPU is optimized for TensorFlow Lite models and costs around $25, making distributed AI deployment economically viable at massive scale.",
      "term": "Edge TPU",
      "length": 290
    },
    {
      "footnote_id": "fn-tensorrt",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1253,
      "definition": "**TensorRT**: NVIDIA's high-performance inference optimizer and runtime library that accelerates deep learning models on NVIDIA GPUs. Introduced in 2016, TensorRT applies graph optimizations, kernel fusion, and precision calibration to achieve 1.5-7x speedups over naive implementations, supporting FP16, INT8, and sparse matrix operations.",
      "term": "TensorRT",
      "length": 340
    },
    {
      "footnote_id": "fn-onnx-runtime",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1255,
      "definition": "**ONNX Runtime**: Microsoft's cross-platform, high-performance ML inferencing and training accelerator supporting the Open Neural Network Exchange (ONNX) format. Released in 2018, it enables models trained in any framework to run efficiently across different hardware (CPU, GPU, NPU) with optimizations like graph fusion and memory pattern optimization.",
      "term": "ONNX Runtime",
      "length": 353
    },
    {
      "footnote_id": "fn-tvm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1257,
      "definition": "**TVM**: An open-source deep learning compiler stack that optimizes tensor programs for diverse hardware backends including CPUs, GPUs, and specialized accelerators. Developed at the University of Washington, TVM uses machine learning to automatically generate optimized code, achieving performance competitive with hand-tuned libraries while supporting new hardware architectures.",
      "term": "TVM",
      "length": 381
    },
    {
      "footnote_id": "fn-operator-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1259,
      "definition": "**Operator Fusion**: A compiler optimization technique that combines multiple neural network operations into single kernels to reduce memory bandwidth requirements and improve cache efficiency. For example, fusing convolution with batch normalization and ReLU can eliminate intermediate memory writes, achieving 20-40% speedups in inference workloads.",
      "term": "Operator Fusion",
      "length": 351
    },
    {
      "footnote_id": "fn-fp32",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1311,
      "definition": "**FP32**: 32-bit floating-point format providing high numerical precision with approximately 7 decimal digits of accuracy. Standard for research and training, FP32 operations consume maximum memory and computational resources but ensure numerical stability. Modern GPUs achieve 15-20 TFLOPS in FP32, serving as the baseline for precision comparisons.",
      "term": "FP32",
      "length": 350
    },
    {
      "footnote_id": "fn-fp16",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1313,
      "definition": "**FP16**: 16-bit floating-point format that halves memory usage compared to FP32 while maintaining reasonable numerical precision. Widely supported by modern AI accelerators, FP16 can achieve 2-4x speedups over FP32 with minimal accuracy loss for most deep learning models, making it the preferred format for inference and mixed-precision training.",
      "term": "FP16",
      "length": 348
    },
    {
      "footnote_id": "fn-int8",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1315,
      "definition": "**INT8**: 8-bit integer format providing maximum memory and computational efficiency, requiring only 25% of FP32 storage. Post-training quantization to INT8 can achieve 4x memory reduction and 2-4x speedup on specialized hardware, but requires careful calibration to minimize accuracy degradation, typically maintaining 95-99% of original model performance.",
      "term": "INT8",
      "length": 357
    },
    {
      "footnote_id": "fn-model-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1317,
      "definition": "**Model Compression**: Techniques to reduce model size and computational requirements including quantization (reducing numerical precision), pruning (removing unnecessary parameters), knowledge distillation (training smaller models to mimic larger ones), and tensor decomposition. These methods can achieve 10-100x size reduction while maintaining 90-99% of original accuracy.",
      "term": "Model Compression",
      "length": 376
    },
    {
      "footnote_id": "fn-serverless-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1329,
      "definition": "**Serverless AI**: Cloud computing paradigm where ML models are deployed as functions that automatically scale from zero to handle incoming requests, with users paying only for actual inference time. Popular platforms like AWS Lambda, Google Cloud Functions, and Azure Functions support serverless AI, but cold-start latencies of 1-10 seconds for large models can impact user experience compared to always-on deployments.",
      "term": "Serverless AI",
      "length": 421
    },
    {
      "footnote_id": "fn-hardware-lottery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 2113,
      "definition": "**Hardware Lottery**: The phenomenon where algorithmic progress is heavily influenced by which approaches happen to align well with available hardware. For example, the Transformer architecture succeeded partly because its matrix multiplication operations perfectly match GPU capabilities, while equally valid architectures like graph neural networks remain underexplored due to poor GPU mapping. This suggests some \"breakthrough\" algorithms may simply be hardware-compatible rather than fundamentally superior.",
      "term": "Hardware Lottery",
      "length": 511
    },
    {
      "footnote_id": "fn-chatgpt-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 165,
      "definition": "ChatGPT reached 100 million monthly active users in January 2023, just 2 months after launch, representing the fastest consumer application adoption in history. This growth required rapid scaling from hundreds to tens of thousands of GPUs.",
      "term": null,
      "length": 239
    },
    {
      "footnote_id": "fn-pruning-sparsity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 167,
      "definition": "Modern neural network pruning achieves these high sparsity ratios by exploiting the overparameterization of deep networks. The lottery ticket hypothesis suggests that dense networks contain sparse subnetworks that can achieve comparable accuracy when trained in isolation.",
      "term": null,
      "length": 272
    },
    {
      "footnote_id": "fn-gpt3-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 169,
      "definition": "Energy estimates for GPT-3 training vary widely based on hardware efficiency assumptions and training duration. The 1,287 MWh figure represents conservative estimates from multiple sources and includes only direct training costs, not data preprocessing or model development iterations.",
      "term": null,
      "length": 285
    },
    {
      "footnote_id": "fn-tinyml-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 171,
      "definition": "TinyML operates under constraints 1000x more severe than mobile AI. For comparison, a modern smartphone AI accelerator may consume 1-5W during peak inference, while TinyML devices target sustained operation at 0.01W or less to achieve multi-month battery life in sensor applications.",
      "term": null,
      "length": 283
    },
    {
      "footnote_id": "fn-watson-health",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 59,
      "definition": "**IBM Watson Health**: IBM's AI health initiative, which accumulated approximately $4 billion in investment over its lifetime, was sold to Francisco Partners in 2022 after failing to deliver promised breakthroughs due to core data quality issues and over-hyped capabilities.",
      "term": "IBM Watson Health",
      "length": 274
    },
    {
      "footnote_id": "fn-data-quality-stats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 73,
      "definition": "**Data Quality Reality**: The famous \"garbage in, garbage out\" principle was first coined by IBM computer programmer George Fuechsel in the 1960s, describing how flawed input data produces nonsense output. This principle remains critically relevant in modern ML systems.",
      "term": "Data Quality Reality",
      "length": 270
    },
    {
      "footnote_id": "fn-kaggle",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 428,
      "definition": "**Kaggle**: Founded in 2010 and acquired by Google in 2017, Kaggle hosts over 80,000 public datasets and serves 15+ million registered users globally. Its competition platform has driven major ML breakthroughs, including the $1 million Netflix Prize that advanced collaborative filtering algorithms.",
      "term": "Kaggle",
      "length": 299
    },
    {
      "footnote_id": "fn-uci-repository",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 430,
      "definition": "**UCI ML Repository**: Established in 1987 by UC Irvine's Machine Learning Group, it's one of the oldest and most cited ML dataset repositories. Contains 600+ datasets including classics like Iris (1936) and Wine (1991) that shaped decades of ML research and education.",
      "term": "UCI ML Repository",
      "length": 269
    },
    {
      "footnote_id": "fn-reproducibility-crisis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 438,
      "definition": "**ML Reproducibility Crisis**: Only 15% of ML papers include code, and fewer than 6% provide complete reproducible implementations. NeurIPS 2019 introduced mandatory reproducibility checklists, while venues like MLSys require artifact evaluation to address this systemic problem.",
      "term": "ML Reproducibility Crisis",
      "length": 279
    },
    {
      "footnote_id": "fn-open-images",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 505,
      "definition": "**Open Images Dataset**: Google's Open Images V7 contains 9 million images with 16 million bounding boxes across 600 object classes. Released in 2016 and continuously updated, it became the largest publicly available dataset for object detection, enabling breakthrough research in computer vision. In production environments, companies regularly scrape e-commerce sites to gather product images for recognition systems or social media platforms for computer vision applications. Stanford's LabelMe project demonstrated this approach's potential early on, scraping Flickr to create a diverse dataset of over 63,000 annotated images.",
      "term": "Open Images Dataset",
      "length": 631
    },
    {
      "footnote_id": "fn-google-crowdsource",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 527,
      "definition": "**Google Crowdsource**: Launched in 2016, this gamified platform has collected over 4 billion contributions from volunteers in 120+ languages. It powers improvements to Google Translate, Maps, and other AI services while demonstrating sustainable crowdsourcing through community engagement rather than monetary incentives. By gamifying the process and engaging global participants, Google harnesses diverse datasets, particularly for underrepresented languages. This approach not only enhances the quality of AI systems but also empowers communities by enabling their contributions to influence technological development.",
      "term": "Google Crowdsource",
      "length": 621
    },
    {
      "footnote_id": "fn-waze-crowdsourcing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 531,
      "definition": "**Waze Crowdsourcing Model**: Founded in Israel (2006), Waze processes 25 billion miles of driving data monthly from 140+ million users. Its real-time crowdsourced traffic model influenced Google Maps after Google's $1.3 billion acquisition (2013), demonstrating how user-generated data creates network effects. While this involves dynamic data collection rather than static dataset labeling, it demonstrates how crowdsourcing can generate continuously updated datasets essential for applications like mobile or edge ML systems. These systems often require real-time input to maintain relevance and accuracy in changing environments.",
      "term": "Waze Crowdsourcing Model",
      "length": 633
    },
    {
      "footnote_id": "fn-recaptcha-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 537,
      "definition": "**reCAPTCHA Evolution**: Originally created at Carnegie Mellon in 2007 to digitize books, reCAPTCHA v1 helped transcribe 13 million books. After Google's 2009 acquisition, v2 (2014) shifted to image recognition for Street View, while v3 (2018) uses behavioral analysis, processing 1+ billion CAPTCHAs weekly. Users identify objects in images, including street signs and cars, contributing to the training of autonomous systems. This clever integration demonstrates how crowdsourcing can scale effectively when embedded into everyday workflows.",
      "term": "reCAPTCHA Evolution",
      "length": 543
    },
    {
      "footnote_id": "fn-hashing-security",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 563,
      "definition": "**Cryptographic Hashing**: One-way mathematical functions like SHA-256 that transform input data into fixed-length strings. Critical for data anonymization because they're computationally infeasible to reverse\u2014even small input changes produce dramatically different outputs, ensuring original data cannot be recovered from the hash. This anonymization technique is straightforward to implement and understand while clearly protecting identifiable values from being viewed, but may struggle with protecting broader context (e.g. relationships between data points).",
      "term": "Cryptographic Hashing",
      "length": 563
    },
    {
      "footnote_id": "fn-pseudonymization-gdpr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 569,
      "definition": "**Pseudonymization under GDPR**: GDPR Article 4(5) formally defines pseudonymization as a privacy-enhancing technique. Unlike anonymization, pseudonymized data remains personal data under EU law, requiring continued protection but enabling reduced regulatory restrictions for research and analytics purposes. This is commonly used in health records or in any situation where datasets need personal identities removed, but maintain unique entries. This approach allow maintaining individual-level data for analysis (since records can be traced through pseudonyms), while reducing the risk of direct identification. However, if the \"key\" linking the pseudonym to the real identifier is compromised, re-identification becomes possible.",
      "term": "Pseudonymization under GDPR",
      "length": 732
    },
    {
      "footnote_id": "fn-privacy-regulations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 941,
      "definition": "**Privacy Regulation Timeline**: GDPR (2016, effective 2018) imposed maximum fines of \u20ac20 million or 4% of annual global turnover (whichever is higher) for violations, followed by California's CCPA (2018), and now dozens of similar laws globally. These regulations significantly transformed how ML systems handle personal data, making privacy-by-design essential for any AI system.",
      "term": "Privacy Regulation Timeline",
      "length": 381
    },
    {
      "footnote_id": "fn-mechanical-turk",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 957,
      "definition": "**Mechanical Turk Origins**: Named after the 18th-century chess-playing \"automaton\" (actually a human chess master hidden inside), Amazon's MTurk (2005) pioneered human-in-the-loop AI by enabling distributed human computation at scale\u2014ironically reversing the original Turk's deception of AI capabilities.",
      "term": "Mechanical Turk Origins",
      "length": 305
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 973,
      "definition": "**Batch Processing Evolution**: Batch processing dates back to IBM mainframes in the 1950s but was revolutionized by Google's MapReduce (2004), which enabled distributed batch processing across thousands of machines. This paradigm shift made \"big data\" analytics economically feasible for the first time.",
      "term": "Batch Processing Evolution",
      "length": 304
    },
    {
      "footnote_id": "fn-etl-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1182,
      "definition": "**ETL Evolution**: ETL emerged in the 1970s with early data warehouses but was revolutionized by Apache Spark in 2009, which enabled distributed data processing at unprecedented scale. Spark's in-memory computing made ETL pipelines 100x faster than traditional MapReduce approaches.",
      "term": "ETL Evolution",
      "length": 282
    },
    {
      "footnote_id": "fn-etl-vs-elt",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1267,
      "definition": "**ETL vs ELT Performance**: ETL processes 1-10GB/hour on traditional systems but scales poorly; ELT leverages cloud warehouses like Snowflake (100GB/hour+) and BigQuery (1TB/hour+) by utilizing distributed compute for transformations. This 10-100x performance difference drives modern data architecture decisions.",
      "term": "ETL vs ELT Performance",
      "length": 313
    },
    {
      "footnote_id": "fn-normalization-techniques",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1299,
      "definition": "**Normalization in ML**: Min-max scaling (range 0-1) vs z-score standardization (mean=0, std=1) can dramatically affect model performance. Gradient descent converges 10-100x faster on normalized features [@goodfellow2016deep], while tree-based models (Random Forest, XGBoost) are largely unaffected by scaling differences.",
      "term": "Normalization in ML",
      "length": 322
    },
    {
      "footnote_id": "fn-categorical-encoding",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1303,
      "definition": "**Categorical Encoding Impact**: One-hot encoding can explode feature dimensions\u2014a single categorical variable with 1000 categories creates 1000 binary features. High-cardinality encoding techniques like target encoding or embeddings (used in deep learning) can reduce dimensions by 10-100x while preserving predictive power.",
      "term": "Categorical Encoding Impact",
      "length": 325
    },
    {
      "footnote_id": "fn-rfm-analysis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1311,
      "definition": "**RFM Analysis Origins**: Developed by direct marketers in the 1960s, RFM (Recency, Frequency, Monetary) analysis segments customers by purchase behavior. Modern ML systems extend this to 100+ behavioral features, but the core RFM triplet remains among the most predictive features for customer lifetime value models.",
      "term": "RFM Analysis Origins",
      "length": 317
    },
    {
      "footnote_id": "fn-apache-beam",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1321,
      "definition": "**Apache Beam Architecture**: Google's unified programming model (2016) abstracts batch and streaming data processing across multiple execution engines (Dataflow, Spark, Flink). Its key innovation: write once, run anywhere with automatic scaling from single machines to thousands of workers processing petabyte-scale data.",
      "term": "Apache Beam Architecture",
      "length": 322
    },
    {
      "footnote_id": "fn-medical-imaging",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1494,
      "definition": "**Medical Imaging AI Revolution**: The 2012 AlexNet breakthrough began with ImageNet's 14 million labeled images, but medical AI required specialized datasets. The NIH Clinical Center released 112,120 chest X-ray images in 2017, becoming one of the largest public medical imaging datasets for ML research. Despite the dependable outcomes of expert labeling, the scarcity and high expenses of specialists render it challenging to implement on a large scale for extensive datasets.",
      "term": "Medical Imaging AI Revolution",
      "length": 479
    },
    {
      "footnote_id": "fn-concept-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1650,
      "definition": "**Concept Drift Challenge**: First formalized by Schlimmer & Granger (1986), concept drift became critical in ML systems deployment. Amazon's recommendation algorithms must continuously adapt as user preferences shift, while spam detection systems face adversarial concept drift as spammers evolve their tactics.",
      "term": "Concept Drift Challenge",
      "length": 312
    },
    {
      "footnote_id": "fn-iot-data-volume",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1716,
      "definition": "**IoT Data Explosion**: IDC predicts 41.6 billion IoT devices will generate 79.4 zettabytes of data by 2025\u2014that's 79.4 trillion gigabytes. A single autonomous vehicle generates 4TB/day, while smart city sensors produce 2.5 quintillion bytes daily. Traditional data warehouses struggle with this velocity and variety.",
      "term": "IoT Data Explosion",
      "length": 317
    },
    {
      "footnote_id": "fn-data-lake-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1720,
      "definition": "**Data Lake Origins**: The term \"data lake\" was coined by Pentaho CTO James Dixon in 2010 to contrast with data warehouses, comparing them to \"a data swamp\" if poorly managed. The concept emerged from Hadoop's ability to store vast amounts of unstructured data cheaply.",
      "term": "Data Lake Origins",
      "length": 269
    },
    {
      "footnote_id": "fn-mysql-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1724,
      "definition": "**MySQL at Scale**: Originally developed by MySQL AB in 1995, MySQL powers 39% of all websites including Facebook, Twitter, and YouTube. However, single MySQL instances typically max out at 10-50TB before requiring complex sharding strategies that make ML feature extraction significantly more complex.",
      "term": "MySQL at Scale",
      "length": 302
    },
    {
      "footnote_id": "fn-bigquery-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1726,
      "definition": "**BigQuery Serverless Power**: Google BigQuery can scan petabytes in seconds using thousands of parallel workers. Its columnar storage and automatic query optimization enables ML feature extraction from trillion-row tables in minutes\u2014performance impossible with traditional databases at any scale. For data lakes, cloud storage solutions such as Google Cloud Storage, AWS S3, and Azure Data Lake Storage are commonly used due to their scalability and flexibility.",
      "term": "BigQuery Serverless Power",
      "length": 463
    },
    {
      "footnote_id": "fn-model-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1746,
      "definition": "**Model Scaling Explosion**: From AlexNet's 60 million parameters [@krizhevsky2012imagenet] (2012) to GPT-3's 175 billion [@brown2020language] (2020), model size grew 3,000x in 8 years. GPT-4's rumored 1.7 trillion parameters would require 3.5 TB of storage\u2014equivalent to 1,000 DVDs worth of model weights alone. This requirement goes beyond traditional data storage and enters the realm of high-performance computing storage solutions.",
      "term": "Model Scaling Explosion",
      "length": 436
    },
    {
      "footnote_id": "fn-columnar-formats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1781,
      "definition": "**Columnar Format Revolution**: Columnar storage was pioneered by C-Store [@stonebraker2005cstore] in 2005, leading to Parquet (developed at Twitter in 2013) and ORC (optimized row columnar, created at Hortonworks). These formats revolutionized analytics by enabling 10-100x faster queries for ML feature extraction.",
      "term": "Columnar Format Revolution",
      "length": 316
    },
    {
      "footnote_id": "fn-snappy-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1785,
      "definition": "**Snappy Compression Trade-offs**: Developed by Google (2011), Snappy achieves 250MB/s compression and 500MB/s decompression speeds\u2014roughly 3-4x faster than gzip. While compression ratios are lower (2-3x vs gzip's 6-8x), the speed advantage makes it ideal for ML pipelines where training throughput matters more than storage costs.",
      "term": "Snappy Compression Trade-offs",
      "length": 331
    },
    {
      "footnote_id": "fn-hdfs-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1791,
      "definition": "**HDFS Origins**: HDFS was inspired by Google's MapReduce paper (2004) and created at Yahoo! in 2006 to handle web-scale data. It enabled the \"big data\" revolution by making petabyte-scale storage affordable using commodity hardware instead of expensive specialized systems.",
      "term": "HDFS Origins",
      "length": 274
    },
    {
      "footnote_id": "fn-redis-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1795,
      "definition": "**Redis Performance**: Redis achieves sub-millisecond latency with 1M+ operations/second on modest hardware. Its in-memory architecture makes it ideal for ML feature serving, with companies like Twitter using Redis clusters to serve 400,000+ timeline requests per second for real-time recommendation systems.",
      "term": "Redis Performance",
      "length": 308
    },
    {
      "footnote_id": "fn-dataeng-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1809,
      "definition": "**Data Versioning Challenges**: Git's inability to handle large binary files efficiently led to the \"GitHub is not a CDN\" problem. DVC, created in 2017, solved this by treating data like code using content-addressable storage, enabling Git-like workflows for terabyte-scale datasets.",
      "term": "Data Versioning Challenges",
      "length": 283
    },
    {
      "footnote_id": "fn-burst-buffers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1827,
      "definition": "**Burst Buffers**: High-speed SSD-based storage layers that buffer data between slower traditional storage and fast compute. Originally developed for supercomputers, they're now critical for ML training where GPUs can demand 100GB/s+ data rates\u2014far exceeding traditional storage capabilities. By buffering these spikes in demand, burst buffers help smooth out performance fluctuations and reduce the load on primary storage systems, ensuring that training pipelines remain efficient and reliable.",
      "term": "Burst Buffers",
      "length": 496
    },
    {
      "footnote_id": "fn-feature-stores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1851,
      "definition": "**Feature Store Evolution**: Feature stores were pioneered by Uber's Michelangelo platform in 2017 to solve feature consistency issues at scale. The concept gained widespread adoption after Airbnb open-sourced their Zipline feature store, leading to modern solutions like Feast and Tecton.",
      "term": "Feature Store Evolution",
      "length": 289
    },
    {
      "footnote_id": "fn-data-lineage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 2465,
      "definition": "**Data Lineage Systems**: Track data from source to consumption across complex ML pipelines. Apache Atlas (originally Hortonworks, now Apache, 2015) and DataHub (LinkedIn, 2020) enable lineage tracking at enterprise scale. Critical for regulatory compliance\u2014GDPR Article 30 requires detailed records of data processing activities, making lineage essential for demonstrating compliance. Standardized documentation frameworks, such as Data Cards proposed by @pushkarna2022data, offer a structured way to document the characteristics, limitations, and potential biases of datasets.",
      "term": "Data Lineage Systems",
      "length": 578
    },
    {
      "footnote_id": "fn-audit-trails",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 2469,
      "definition": "**ML Audit Requirements**: SOX compliance requires immutable audit logs for financial ML models, while HIPAA mandates detailed access logs for healthcare AI. Modern systems generate terabytes of audit data\u2014Uber's ML platform logs 50+ billion events daily for compliance and debugging purposes. Comprehensive audit trails are invaluable for troubleshooting and accountability, especially in cases of data breaches or unexpected model behavior. They help organizations understand what actions were taken and why, providing a clear path for resolving issues and ensuring compliance.",
      "term": "ML Audit Requirements",
      "length": 579
    },
    {
      "footnote_id": "fn-blockchain-governance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 2475,
      "definition": "**Blockchain for ML Governance**: Immutable ledgers provide tamper-proof audit trails for ML model decisions. Ocean Protocol and other projects use blockchain to track data provenance and usage rights. While promising for high-stakes applications like healthcare AI, blockchain's energy costs and complexity limit widespread adoption. By adopting robust data governance practices, including tools like Data Cards, organizations can build ML systems that are transparent, ethical, and trustworthy.",
      "term": "Blockchain for ML Governance",
      "length": 496
    },
    {
      "footnote_id": "fn-gpu-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 59,
      "definition": "**GPU (Graphics Processing Unit)**: Originally designed for rendering 3D graphics in 1999 by NVIDIA, GPUs excel at parallel computation with thousands of simple cores (compared to CPUs' 4-16 complex cores). A modern GPU like the NVIDIA A100 contains 6,912 CUDA cores and can perform 312 TFLOPS for FP16 Tensor operations\u2014roughly 20\u00d7 faster than CPUs for neural network training. This massive parallelism perfectly matches the matrix multiplication operations that dominate deep learning computations.",
      "term": "GPU (Graphics Processing Unit)",
      "length": 500
    },
    {
      "footnote_id": "fn-hog-method",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 180,
      "definition": "**Histogram of Oriented Gradients (HOG)**: Developed by Navneet Dalal and Bill Triggs in 2005, HOG became the gold standard for object detection before deep learning. It achieved near-perfect accuracy on pedestrian detection\u2014a breakthrough that enabled practical computer vision applications. HOG works by computing gradients (edge directions) in 8\u00d78 pixel cells, then creating histograms of 9 orientation bins. This clever abstraction captures object shape while ignoring texture details, making it robust to lighting changes but requiring expert knowledge to design.",
      "term": "Histogram of Oriented Gradients (HOG)",
      "length": 568
    },
    {
      "footnote_id": "fn-sift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 186,
      "definition": "**Scale-Invariant Feature Transform (SIFT)**: Invented by David Lowe at University of British Columbia in 1999, SIFT revolutionized computer vision by detecting \"keypoints\" that remain stable across different viewpoints, scales, and lighting conditions. A typical image yields 1,000-2,000 SIFT keypoints, each described by a 128-dimensional vector. Before deep learning, SIFT was the backbone of applications like Google Street View's image matching and early smartphone augmented reality. The algorithm's 4-step process (scale-space extrema detection, keypoint localization, orientation assignment, and descriptor generation) required deep expertise to implement effectively.",
      "term": "Scale-Invariant Feature Transform (SIFT)",
      "length": 676
    },
    {
      "footnote_id": "fn-imagenet-progress",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 244,
      "definition": "**ImageNet Competition Progress**: The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) tracked computer vision progress from 2010-2017. Error rates dropped dramatically: traditional methods achieved ~28% error in 2010, AlexNet [@krizhevsky2012imagenet] (first deep learning winner) achieved 15.3% in 2012, and ResNet [@he2016deep] achieved 3.6% in 2015\u2014surpassing estimated human performance of 5.1%. This rapid improvement demonstrated deep learning's superiority over hand-crafted features, triggering the modern AI revolution. The competition ended in 2017 when further improvements became incremental.",
      "term": "ImageNet Competition Progress",
      "length": 616
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 305,
      "definition": "**FLOPS**: Floating Point Operations Per Second, a measure of computational performance that quantifies how many floating-point arithmetic operations (additions, multiplications, etc.) a processor can execute in one second. Modern CPUs achieve 100-1000 GFLOPS, while high-end GPUs can reach 100+ TFLOPS for AI workloads.",
      "term": "FLOPS",
      "length": 320
    },
    {
      "footnote_id": "fn-brain-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 342,
      "definition": "**Brain Energy Efficiency**: The human brain contains approximately 86-100 billion neurons and performs roughly 10^16 operations per second on just 20 watts\u2014equivalent to running a single LED light bulb. Deep learning requires training GPT-3 [@brown2020language] consumed about 1,287 megawatt-hours of electricity [@strubell2019energy]. This stark efficiency gap drives research into neuromorphic computing and inspired the development of specialized AI chips designed to mimic brain-like processing.",
      "term": "Brain Energy Efficiency",
      "length": 500
    },
    {
      "footnote_id": "fn-synapses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 356,
      "definition": "**Synapses**: From the Greek word \"synaptein\" meaning \"to clasp together,\" synapses are the connection points between neurons where chemical or electrical signals are transmitted. A typical neuron has 1,000-10,000 synaptic connections, and the human brain contains roughly 100 trillion synapses. The strength of synaptic connections can change through experience, forming the biological basis of learning and memory\u2014a principle directly mimicked by adjustable weights in artificial neural networks.",
      "term": "Synapses",
      "length": 498
    },
    {
      "footnote_id": "fn-perceptron",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 476,
      "definition": "**Perceptron**: Invented by Frank Rosenblatt in 1957 at Cornell, the perceptron was the first artificial neural network capable of learning. The New York Times famously reported it would be \"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\" While overly optimistic, this breakthrough laid the foundation for all modern neural networks.",
      "term": "Perceptron",
      "length": 439
    },
    {
      "footnote_id": "fn-dlprimer-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 480,
      "definition": "**Backpropagation**: Published by Rumelhart, Hinton, and Williams in 1986, backpropagation solved the \"credit assignment problem\"\u2014how to determine which weights in a multi-layer network were responsible for errors. This algorithm, based on the mathematical chain rule, enabled training of deep networks and directly led to the modern AI revolution. Remarkably, a similar algorithm was discovered by Paul Werbos in 1974 but went largely unnoticed.",
      "term": "Backpropagation",
      "length": 446
    },
    {
      "footnote_id": "fn-dlprimer-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 486,
      "definition": "**FLOPS**: Floating Point Operations Per Second measures computational throughput by counting mathematical operations like addition, subtraction, multiplication, and division of decimal numbers. Modern supercomputers achieve exascale performance (10^18 FLOPS), while neural network training requires petascale to exascale compute. For perspective, training GPT-3 [@brown2020language] required approximately 3.14 \u00d7 10^23 FLOPS\u2014more computation than was available to the entire world before 1960.",
      "term": "FLOPS",
      "length": 494
    },
    {
      "footnote_id": "fn-overfitting",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 524,
      "definition": "**Overfitting**: When a model memorizes training examples instead of learning generalizable patterns\u2014like a student who memorizes answers instead of understanding concepts. The model performs perfectly on training data but fails on new examples. Common signs include training accuracy continuing to improve while validation accuracy plateaus or decreases. Think of it as becoming an \"expert\" on a practice test who panics when facing slightly different questions on the real exam.",
      "term": "Overfitting",
      "length": 480
    },
    {
      "footnote_id": "fn-dlprimer-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 528,
      "definition": "**Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for tensor operations, the mathematical building blocks of neural networks. First deployed internally in 2015, TPUs can perform matrix multiplications up to 30\u00d7 faster than 2015-era GPUs while using less power. The name reflects their optimization for tensor operations\u2014multi-dimensional arrays that represent data flowing through neural networks. Google has since made TPUs available through cloud services, democratizing access to this specialized AI hardware.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 542
    },
    {
      "footnote_id": "fn-dl-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 530,
      "definition": "**Deep Learning Frameworks**: TensorFlow [@abadi2016tensorflow] (released by Google in 2015) and PyTorch [@paszke2019pytorch] (released by Facebook in 2016) democratized deep learning by handling the complex mathematics automatically. Before these frameworks, implementing backpropagation required writing hundreds of lines of error-prone calculus code. Now, a complete neural network can be defined in 10-20 lines. TensorFlow emphasizes production deployment and has been downloaded over 180 million times, while PyTorch dominates research with its dynamic computation graphs. These frameworks automatically compute gradients, optimize GPU memory usage, and distribute training across multiple machines.",
      "term": "Deep Learning Frameworks",
      "length": 704
    },
    {
      "footnote_id": "fn-relu-function",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 629,
      "definition": "**ReLU (Rectified Linear Unit)**: A piecewise linear activation function that outputs the input directly if positive, otherwise outputs zero. Introduced by Nair and Hinton in 2010, ReLU solved the vanishing gradient problem and became the default activation function in modern deep learning due to its computational simplicity and biological inspiration from neuron firing patterns.",
      "term": "ReLU (Rectified Linear Unit)",
      "length": 382
    },
    {
      "footnote_id": "fn-vanishing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 633,
      "definition": "**Vanishing Gradients**: When gradients become exponentially small as they propagate backward through many layers, learning effectively stops in early layers. This problem is addressed in detail in @sec-ai-training.",
      "term": "Vanishing Gradients",
      "length": 215
    },
    {
      "footnote_id": "fn-xor-problem",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 988,
      "definition": "**XOR Problem**: The exclusive-or function became famous in AI history when Marvin Minsky and Seymour Papert proved in 1969 that single-layer perceptrons could never learn it, contributing to the \"AI winter\" of the 1970s. XOR requires non-linear decision boundaries\u2014something impossible with linear models. The solution requires at least one hidden layer, demonstrating why \"deep\" networks (with hidden layers) are essential for learning complex patterns. This simple 2-input, 1-output problem helped establish the theoretical foundation for multi-layer neural networks.",
      "term": "XOR Problem",
      "length": 570
    },
    {
      "footnote_id": "fn-mnist-dataset",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1002,
      "definition": "**MNIST Dataset**: Created by Yann LeCun and colleagues in 1998, MNIST (Modified National Institute of Standards and Technology) contains 70,000 images of handwritten digits\u201460,000 for training and 10,000 for testing. Each image is 28\u00d728 pixels in grayscale, totaling 784 features per digit. MNIST became the \"hello world\" of computer vision, with error rates dropping from 12% with traditional methods in 1998 to 0.23% with modern deep learning. Despite being \"solved,\" MNIST remains invaluable for teaching because it's large enough to be realistic yet small enough to train quickly on any computer.",
      "term": "MNIST Dataset",
      "length": 601
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1263,
      "definition": "**Batch Processing**: Processing multiple examples simultaneously, typically 32, 64, 128, or 256 samples per batch. Larger batches provide more stable gradient estimates and better utilize parallel hardware (GPUs can process 256 images nearly as fast as 1 image), but require more memory and may converge to worse solutions. The optimal batch size often depends on available GPU memory\u2014NVIDIA's V100 with 32GB can handle batch sizes of 512-1024 for typical networks, while smaller GPUs require smaller batches. **Systems Engineering Trade-offs**: A ResNet-50 model requires ~25GB memory for training with batch size 256 on ImageNet, forcing practitioners to choose between model complexity and batch size. Cloud deployments can use gradient accumulation to simulate large batches across multiple smaller GPUs, while edge devices may be limited to batch sizes of 1-4 due to memory constraints. This hardware-software co-design relationship exemplifies how neural network training pushes the boundaries of computing systems. @sec-ai-training explores advanced training strategies that help navigate these hardware constraints while optimizing convergence.",
      "term": "Batch Processing",
      "length": 1153
    },
    {
      "footnote_id": "fn-cross-entropy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1540,
      "definition": "**Cross-Entropy Loss**: Derived from information theory by Claude Shannon in 1948, cross-entropy measures the \"surprise\" when predicting incorrectly. If a model is 99% confident about the wrong answer, the loss is much higher than being 60% confident about the wrong answer. This mathematical property naturally encourages the model to be both accurate and calibrated (confident when right, uncertain when unsure). Cross-entropy loss works perfectly with softmax outputs and provides strong gradients even when predictions are very wrong, making it ideal for classification tasks.",
      "term": "Cross-Entropy Loss",
      "length": 580
    },
    {
      "footnote_id": "fn-gradient-descent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1729,
      "definition": "**Gradient Descent**: Think of gradient descent as finding the bottom of a valley while blindfolded\u2014you feel the slope under your feet and take steps downhill. Mathematically, the gradient points in the direction of steepest increase, so we move in the opposite direction to minimize our loss function. The name comes from the Latin \"gradus\" (step) and was first formalized by Cauchy in 1847 for solving systems of equations, though the modern machine learning version was developed much later.",
      "term": "Gradient Descent",
      "length": 494
    },
    {
      "footnote_id": "fn-learning-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1739,
      "definition": "**Learning Rate**: Often called the most important hyperparameter in deep learning, the learning rate determines the step size in optimization. Think of it like the gas pedal on a car\u2014too much acceleration and you'll crash past your destination, too little and you'll never get there. Typical values range from 0.1 to 0.0001, and getting this right can mean the difference between a model that learns in hours versus one that never converges.",
      "term": "Learning Rate",
      "length": 442
    },
    {
      "footnote_id": "fn-epoch-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1779,
      "definition": "**Epoch**: From the Greek word \"epoche\" meaning \"fixed point in time,\" an epoch represents one complete cycle through all training data. Deep learning models typically require 10-200 epochs to converge, depending on dataset size and complexity. Modern large language models like GPT-3 train on only 1 epoch over massive datasets (300 billion tokens), while smaller models might train for 100+ epochs on limited data. The term was borrowed from astronomy, where it marks a specific moment for measuring celestial positions\u2014fitting for the iterative refinement process of neural network training.",
      "term": "Epoch",
      "length": 594
    },
    {
      "footnote_id": "fn-floating-point",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 2230,
      "definition": "**32-bit Floating Point Precision**: Also called \"single precision\" or FP32, this IEEE 754 standard uses 32 bits to represent real numbers: 1 bit for sign, 8 bits for exponent, and 23 bits for mantissa. While neural network training typically requires FP32 precision to maintain gradient stability, inference often works with FP16 (half precision) or even INT8 (8-bit integers), reducing memory usage by 2\u00d7 to 4\u00d7. Modern AI chips like Google's TPU v4 support \"bfloat16\" (brain floating point), Google's custom 16-bit format that maintains FP32's range while halving memory requirements.",
      "term": "32-bit Floating Point Precision",
      "length": 586
    },
    {
      "footnote_id": "fn-uat",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 66,
      "definition": "**Universal Approximation Theorem**: Proven independently by Cybenko (1989) and Hornik (1989), this result showed that neural networks could theoretically learn any function, a discovery that reinvigorated interest in neural networks after the \"AI Winter\" of the 1980s and established mathematical foundations for modern deep learning.",
      "term": "Universal Approximation Theorem",
      "length": 335
    },
    {
      "footnote_id": "fn-activation-functions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 68,
      "definition": "**Activation Functions**: Non-linear mathematical functions like ReLU, sigmoid, and tanh that introduce non-linearity into neural networks. Without them, multiple layers would collapse to a single linear transformation, making ReLU's simple max(0,x) operation necessary for deep learning's success since 2012.",
      "term": "Activation Functions",
      "length": 309
    },
    {
      "footnote_id": "fn-mnist-dataset",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 74,
      "definition": "**MNIST Dataset**: Created by Yann LeCun in 1998 from NIST's database of handwritten digits, MNIST's 60,000 training images became the \"fruit fly\" of machine learning research. Despite achieving 99.7% accuracy being considered solved, MNIST remains valuable for education because its simplicity allows students to focus on architectural concepts without data complexity distractions. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits.",
      "term": "MNIST Dataset",
      "length": 685
    },
    {
      "footnote_id": "fn-gemm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 88,
      "definition": "**GEMM (General Matrix Multiply)**: The fundamental operation underlying neural networks, accounting for 80-95% of computation time in dense neural networks. GEMM performs C = \u03b1AB + \u03b2C and has been optimized for decades. Modern implementations like cuBLAS achieve 80-95% of theoretical peak performance on GPUs, making GEMM optimization crucial for ML systems.",
      "term": "GEMM (General Matrix Multiply)",
      "length": 360
    },
    {
      "footnote_id": "fn-mlp-notation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 94,
      "definition": "**MLP Mathematical Notation**: In this equation, $\\mathbf{h}^{(l)}$ represents the layer $l$ output (activation vector), $\\mathbf{W}^{(l)}$ is the weight matrix for layer $l$, $\\mathbf{b}^{(l)}$ is the bias vector, and $f(\\cdot)$ is the activation function (like ReLU). The superscript $(l)$ denotes the layer number, with bold symbols indicating vectors/matrices. This compact notation captures the core operation of neural networks: linear transformation followed by nonlinear activation.",
      "term": "MLP Mathematical Notation",
      "length": 490
    },
    {
      "footnote_id": "fn-dnn-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 321,
      "definition": "**Basic Linear Algebra Subprograms (BLAS)**: Developed in the 1970s as a standard for basic vector and matrix operations, BLAS became the foundation for virtually all scientific computing. Modern implementations like Intel MKL and OpenBLAS can achieve 80-95% of theoretical peak performance on well-optimized workloads, making them necessary for neural network efficiency.",
      "term": "Basic Linear Algebra Subprograms (BLAS)",
      "length": 372
    },
    {
      "footnote_id": "fn-dnn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 357,
      "definition": "**ImageNet Revolution**: AlexNet's dramatic victory in the 2012 ImageNet challenge [@krizhevsky2012imagenet] (reducing error from 26% to 16%) sparked the deep learning renaissance. ImageNet's 14 million labeled images across 20,000 categories provided the scale needed to train deep CNNs, proving that \"big data + big compute + big models\" could achieve superhuman performance.",
      "term": "ImageNet Revolution",
      "length": 377
    },
    {
      "footnote_id": "fn-lecun-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 576,
      "definition": "**Yann LeCun and CNNs**: LeCun's 1989 LeNet architecture was inspired by Hubel and Wiesel's discovery of simple and complex cells in cat visual cortex [@hubel1962receptive]. LeNet-5 achieved 99.2% accuracy on MNIST in 1998 and was deployed by banks to read millions of checks daily, among the first large-scale commercial applications of neural networks. As illustrated in @fig-cnn-spatial-processing, CNNs address spatial pattern processing through a different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position, a process known as convolution.",
      "term": "Yann LeCun and CNNs",
      "length": 804
    },
    {
      "footnote_id": "fn-parameter-sharing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 578,
      "definition": "**Parameter Sharing**: CNNs reuse the same filter weights across spatial positions, dramatically reducing parameters. A CNN processing 224\u00d7224 images might use 3\u00d73 filters with only 9 parameters per channel, versus an equivalent MLP requiring 50,176 parameters per neuron, a 5,500x reduction enabling practical computer vision.",
      "term": "Parameter Sharing",
      "length": 327
    },
    {
      "footnote_id": "fn-translation-invariance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 580,
      "definition": "**Translation Invariance**: CNNs detect features regardless of spatial position. A cat's ear is recognized whether in the top-left or bottom-right corner. This property emerges from convolution's sliding window design and is crucial for computer vision, where objects appear at arbitrary locations in images.",
      "term": "Translation Invariance",
      "length": 308
    },
    {
      "footnote_id": "fn-cnn-convolution-notation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 589,
      "definition": "**CNN Convolution Notation**: This equation describes how CNNs process spatial data. $\\mathbf{H}^{(l)}_{i,j,k}$ is the output at spatial position $(i,j)$ in channel $k$ of layer $l$. The triple sum iterates over the filter dimensions: $(di,dj)$ scans the spatial filter size, and $c$ covers input channels. $\\mathbf{W}^{(l)}_{di,dj,c,k}$ represents the filter weights, capturing local spatial patterns. Unlike MLPs that connect all inputs to outputs, CNNs only connect local spatial neighborhoods.",
      "term": "CNN Convolution Notation",
      "length": 497
    },
    {
      "footnote_id": "fn-receptive-field",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 593,
      "definition": "**Receptive Field**: The region of the input that influences a particular output neuron. In CNNs, receptive fields grow with depth. A neuron in layer 3 might \"see\" a 7\u00d77 region even with 3\u00d73 filters, due to stacking. Understanding receptive field size is crucial for ensuring networks can capture features at the right scale for the task.",
      "term": "Receptive Field",
      "length": 338
    },
    {
      "footnote_id": "fn-simd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 767,
      "definition": "**SIMD (Single Instruction, Multiple Data)**: CPU instructions that perform the same operation on multiple data elements simultaneously. Modern x86 processors support AVX-512, enabling 16 single-precision operations per instruction, a 16x speedup over scalar code. SIMD is crucial for efficient neural network inference on CPUs, especially for edge deployment. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.",
      "term": "SIMD (Single Instruction, Multiple Data)",
      "length": 520
    },
    {
      "footnote_id": "fn-vanishing-gradient",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 791,
      "definition": "**Vanishing Gradient Problem**: During backpropagation through time, gradients shrink exponentially as they propagate backward through RNN layers. When recurrent weights have magnitude < 1, gradients multiply by values < 1 at each time step, vanishing after 5-10 steps and preventing learning of long-term dependencies\u2014a key limitation solved by LSTMs and attention mechanisms.",
      "term": "Vanishing Gradient Problem",
      "length": 377
    },
    {
      "footnote_id": "fn-attention-notation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1045,
      "definition": "**Attention Mechanism Notation**: This equation shows scaled dot-product attention. $\\mathbf{Q}$ (queries) and $\\mathbf{K}$ (keys) are matrix-multiplied to compute similarity scores, divided by $\\sqrt{d_k}$ (key dimension) for numerical stability, then normalized with softmax to get attention weights. These weights are applied to $\\mathbf{V}$ (values) to produce the output. The result is a weighted combination where each position receives information from all relevant positions based on content similarity.",
      "term": "Attention Mechanism Notation",
      "length": 511
    },
    {
      "footnote_id": "fn-attention-qkv",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1049,
      "definition": "**Query-Key-Value Attention**: Inspired by information retrieval systems where queries search through keys to retrieve values. In neural attention, queries and keys compute similarity scores (like a search engine matching queries to documents), while values contain the actual information to retrieve\u2014a design that enables flexible, content-based information access.",
      "term": "Query-Key-Value Attention",
      "length": 366
    },
    {
      "footnote_id": "fn-attention-is-all-you-need",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1403,
      "definition": "**\"Attention is All You Need\"**: This 2017 paper by Google researchers eliminated recurrence entirely, showing that attention mechanisms alone could achieve state-of-the-art results. The title itself became a rallying cry, and within 5 years, transformer-based models achieved breakthrough performance in language (GPT, BERT), vision (ViT), and beyond. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself.",
      "term": "\"Attention is All You Need\"",
      "length": 612
    },
    {
      "footnote_id": "fn-attention-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1432,
      "definition": "**Attention Scaling**: Without the $\\sqrt{d_k}$ scaling factor, large dot products would cause the softmax to saturate, producing gradients close to zero and hindering learning. This mathematical insight enables stable optimization of large Transformer models.",
      "term": "Attention Scaling",
      "length": 260
    },
    {
      "footnote_id": "fn-dnn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1627,
      "definition": "**Backpropagation Algorithm**: While the chain rule was known since the 1600s, Rumelhart, Hinton, and Williams (1986) showed how to efficiently apply it to train multi-layer networks. This \"learning by error propagation\" algorithm made deep networks practical and remains virtually unchanged in modern systems\u2014a testament to its importance.",
      "term": "Backpropagation Algorithm",
      "length": 340
    },
    {
      "footnote_id": "fn-dnn-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1639,
      "definition": "**ResNet Revolution**: ResNet (2016) solved the \"degradation problem\" where deeper networks performed worse than shallow ones. The key insight: adding identity shortcuts ($\\mathcal{F}(\\mathbf{x}) + \\mathbf{x}$) let networks learn residual mappings instead of full transformations, enabling training of 1000+ layer networks and winning ImageNet 2015.",
      "term": "ResNet Revolution",
      "length": 349
    },
    {
      "footnote_id": "fn-lstm-invention",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1651,
      "definition": "**LSTM Origins**: Sepp Hochreiter and J\u00fcrgen Schmidhuber invented LSTMs in 1997 to solve the \"vanishing gradient problem\" that plagued RNNs. Their gating mechanism was inspired by biological neurons' ability to selectively retain information\u2014a breakthrough that enabled sequence modeling and facilitated modern language models.",
      "term": "LSTM Origins",
      "length": 327
    },
    {
      "footnote_id": "fn-vision-transformers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1719,
      "definition": "**Vision Transformers (ViTs)**: Google's 2021 breakthrough showed that pure transformers could match CNN performance on ImageNet by treating image patches as \"words.\" ViTs split a $224\\times 224$ image into $16\\times 16$ patches (196 \"tokens\"), proving that attention mechanisms could replace convolutional inductive biases with sufficient data.",
      "term": "Vision Transformers (ViTs)",
      "length": 345
    },
    {
      "footnote_id": "fn-im2col",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1864,
      "definition": "**im2col (Image to Column)**: A data layout transformation that converts convolution operations into matrix multiplications by unfolding image patches into columns. Developed by Intel in the 1990s, im2col trades memory (duplicating data) for compute efficiency, enabling CNNs to leverage decades of GEMM optimizations and achieving 5-10x speedups on CPUs.",
      "term": "im2col (Image to Column)",
      "length": 355
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1872,
      "definition": "**Systolic Array**: A network of processing elements that rhythmically compute and pass data through neighbors, like a \"heartbeat\" of computation. Invented by H.T. Kung and Charles Leiserson in 1978, systolic arrays achieve high throughput by overlapping computation with data movement\u2014Google's TPU systolic arrays perform 65,536 multiply-accumulate operations per clock cycle. Software frameworks optimize these operations by transforming them into efficient matrix multiplications (a $3\\times 3$ convolution becomes a $9\\times N$ matrix multiplication) and carefully managing data layout in memory to maximize spatial locality.",
      "term": "Systolic Array",
      "length": 629
    },
    {
      "footnote_id": "fn-parameter-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2127,
      "definition": "**Parameter Scaling**: The leap from AlexNet's 60 million parameters (2012) to GPT-3's 175 billion parameters (2020) represents a 3,000x increase in just 8 years. Modern models like GPT-4 may exceed 1 trillion parameters, requiring specialized distributed computing infrastructure and consuming megawatts of power during training.",
      "term": "Parameter Scaling",
      "length": 330
    },
    {
      "footnote_id": "fn-dnn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2135,
      "definition": "**Tensor Processing Units**: Google's TPUs emerged from their need to run neural networks on billions of searches daily. First deployed secretly in 2015, TPUs achieve 15-30x better performance per watt than GPUs for inference. The TPU's $128\\times 128$ systolic array performs 65,536 multiply-accumulate operations per clock cycle, revolutionizing AI hardware design. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks.",
      "term": "Tensor Processing Units",
      "length": 520
    },
    {
      "footnote_id": "fn-memory-usage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 49,
      "definition": "**Memory Usage**: ML models consume both VRAM (for GPU processing) and system RAM. Large language models like GPT-3 require 350GB+ memory for inference, while typical edge devices have only 4-8GB RAM, creating a deployment gap that necessitates model compression and optimization techniques.",
      "term": "Memory Usage",
      "length": 291
    },
    {
      "footnote_id": "fn-carbon-emissions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 51,
      "definition": "**Carbon Emissions**: Training GPT-3 generated approximately 502 tons of CO\u2082 equivalent, comparable to the annual emissions of 123 gasoline-powered vehicles. Modern ML practices increasingly incorporate carbon tracking using tools like CodeCarbon and the ML CO2 Impact calculator to measure and minimize environmental impact.",
      "term": "Carbon Emissions",
      "length": 325
    },
    {
      "footnote_id": "fn-scaling-laws",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 79,
      "definition": "**Scaling Laws**: Empirical relationships discovered by OpenAI showing that language model performance follows predictable power-law relationships with model size (N), dataset size (D), and compute budget (C). These laws enable researchers to predict performance and optimal resource allocation before expensive training runs.",
      "term": "Scaling Laws",
      "length": 326
    },
    {
      "footnote_id": "fn-tokens",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 99,
      "definition": "**Tokens**: Individual units of text that language models process, created by breaking text into subword pieces using algorithms like Byte-Pair Encoding (BPE). GPT-3 trained on 300 billion tokens while PaLM used 780 billion tokens, requiring text corpora equivalent to millions of books from web crawls and digitized literature.",
      "term": "Tokens",
      "length": 328
    },
    {
      "footnote_id": "fn-efficient-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 101,
      "definition": "**FLOPs**: Floating-Point Operations, measuring computational work performed. Modern deep learning models require 10\u00b2\u00b2-10\u00b2\u2074 FLOPs for training: GPT-3 used ~3.14 \u00d7 10\u00b2\u00b3 FLOPs (314 sextillion operations), equivalent to running a high-end gaming PC continuously for over 350 years.",
      "term": "FLOPs",
      "length": 278
    },
    {
      "footnote_id": "fn-transformer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 103,
      "definition": "**Transformer**: Neural network architecture introduced by Vaswani et al. [@vaswani2017attention] that revolutionized NLP through self-attention mechanisms. Unlike sequential RNNs, transformers enable parallel processing during training, forming the foundation of modern large language models including GPT, BERT, T5, and their derivatives.",
      "term": "Transformer",
      "length": 340
    },
    {
      "footnote_id": "fn-autoregressive",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 105,
      "definition": "**Autoregressive Models**: Language models that generate text by predicting each token based on all preceding tokens in the sequence. GPT-family models exemplify this approach, generating text left-to-right with causal attention masks to ensure each position only attends to previous positions.",
      "term": "Autoregressive Models",
      "length": 294
    },
    {
      "footnote_id": "fn-efficient-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 111,
      "definition": "**GPT-3**: OpenAI's 175-billion parameter language model released in 2020, costing an estimated $4.6 million to train and consuming ~1,287 MWh of electricity. Its training data included 45TB of text from the internet, books, and other sources.",
      "term": "GPT-3",
      "length": 243
    },
    {
      "footnote_id": "fn-perplexity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 121,
      "definition": "**Perplexity**: A measurement of how well a language model predicts text, calculated as 2^(cross-entropy loss). GPT-3 achieved ~20 perplexity on WebText, meaning on average it's as confused as if choosing randomly among 20 equally-likely next words.",
      "term": "Perplexity",
      "length": 249
    },
    {
      "footnote_id": "fn-distributed-infrastructure",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 438,
      "definition": "**Distributed Infrastructure**: Computing systems that spread ML workloads across multiple machines connected by high-speed networks. OpenAI's GPT-4 training likely used thousands of NVIDIA A100 GPUs connected via InfiniBand, requiring careful orchestration to avoid communication bottlenecks.",
      "term": "Distributed Infrastructure",
      "length": 293
    },
    {
      "footnote_id": "fn-memory-bandwidth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 458,
      "definition": "**Memory Bandwidth**: The rate at which data can be read from or written to memory, measured in GB/s. NVIDIA H100 provides 3.35 TB/s memory bandwidth vs. typical DDR5 RAM's 51 GB/s\u2014a 65\u00d7 difference critical for handling large model parameters.",
      "term": "Memory Bandwidth",
      "length": 243
    },
    {
      "footnote_id": "fn-efficient-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 604,
      "definition": "**Model Parallelism**: Distributing model components across multiple processors, contrasting with data parallelism. Modern transformer models like GPT-3 require model parallelism due to their 175B parameters exceeding single GPU memory (~24GB for A100 variant).",
      "term": "Model Parallelism",
      "length": 261
    },
    {
      "footnote_id": "fn-efficient-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 606,
      "definition": "**Stochastic Gradient Descent (SGD)**: Optimization algorithm using random data samples, introduced by Robbins and Monro (1951). Made neural network training practical by reducing memory requirements from full-batch to single-sample updates, enabling learning on larger datasets.",
      "term": "Stochastic Gradient Descent (SGD)",
      "length": 279
    },
    {
      "footnote_id": "fn-mobilenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 641,
      "definition": "**MobileNet**: Efficient neural network architecture using depthwise separable convolutions, achieving ~50\u00d7 fewer parameters than traditional models. MobileNet-v1 has only 4.2M parameters vs. VGG-16's 138M, enabling deployment on smartphones with <100MB memory.",
      "term": "MobileNet",
      "length": 261
    },
    {
      "footnote_id": "fn-efficientnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 643,
      "definition": "**EfficientNet**: Architecture achieving state-of-the-art accuracy with 10\u00d7 fewer parameters than previous models. EfficientNet-B7 achieves 84.3% ImageNet top-1 accuracy with 66M parameters, compared to ResNet-152's 60M parameters achieving 78.3%.",
      "term": "EfficientNet",
      "length": 247
    },
    {
      "footnote_id": "fn-squeezenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 645,
      "definition": "**SqueezeNet**: Compact CNN architecture achieving AlexNet-level accuracy with 50\u00d7 fewer parameters (1.25M vs. 60M). Demonstrated that clever architecture design can dramatically reduce model size without sacrificing performance.",
      "term": "SqueezeNet",
      "length": 229
    },
    {
      "footnote_id": "fn-param-efficient",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 651,
      "definition": "**Parameter-Efficient Fine-tuning**: Methods like LoRA and Adapters that update <1% of model parameters while achieving full fine-tuning performance. Reduces memory requirements from gigabytes to megabytes for large model adaptation.",
      "term": "Parameter-Efficient Fine-tuning",
      "length": 233
    },
    {
      "footnote_id": "fn-efficient-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 775,
      "definition": "**AlexNet**: Groundbreaking CNN by Krizhevsky, Sutskever, and Hinton (2012) that won ImageNet with 15.3% error rate, nearly halving the previous best of 26.2%. Used 60M parameters, two GPUs, and launched the deep learning revolution.",
      "term": "AlexNet",
      "length": 233
    },
    {
      "footnote_id": "fn-efficient-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 777,
      "definition": "**ImageNet**: Large-scale visual recognition dataset with 14+ million images across 20,000+ categories. The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) drove computer vision breakthroughs from 2010-2017.",
      "term": "ImageNet",
      "length": 225
    },
    {
      "footnote_id": "fn-efficient-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 779,
      "definition": "**Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every ~2 years. Hardware improvements follow ~2x every 18-24 months, while AI algorithmic efficiency improved 44x in 7 years (2012-2019).",
      "term": "Moore's Law",
      "length": 235
    },
    {
      "footnote_id": "fn-efficient-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 797,
      "definition": "**ResNet**: Residual Network architecture by He et al. [@he2016deep] enabling training of very deep networks (152+ layers) through skip connections. Won ImageNet 2015 with 3.6% error rate, surpassing human-level performance for the first time.",
      "term": "ResNet",
      "length": 243
    },
    {
      "footnote_id": "fn-cuda-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 949,
      "definition": "**CUDA Cores**: NVIDIA's parallel processing units optimized for floating-point operations. Unlike CPU cores (designed for complex sequential tasks), CUDA cores are simpler and work together\u2014enabling a single H100 GPU to perform 16,896 parallel operations simultaneously for massive speedup in matrix computations.",
      "term": "CUDA Cores",
      "length": 314
    },
    {
      "footnote_id": "fn-efficient-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1054,
      "definition": "**Data Parallelism**: Training method where the same model runs on multiple processors with different data batches. GPT-3 training used data parallelism across thousands of GPUs, processing multiple text sequences simultaneously to achieve massive scale.",
      "term": "Data Parallelism",
      "length": 254
    },
    {
      "footnote_id": "fn-uci",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1098,
      "definition": "**UCI Machine Learning Repository**: Established in 1987 by the University of California, Irvine, one of the most widely-used resources for machine learning datasets. Contains over 600 datasets and has been cited in thousands of research papers, serving as a cornerstone for early ML research.",
      "term": "UCI Machine Learning Repository",
      "length": 293
    },
    {
      "footnote_id": "fn-pca",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1100,
      "definition": "**Principal Component Analysis (PCA)**: Dimensionality reduction technique invented by Karl Pearson in 1901, identifies the most important directions of variation in data. Reduces computational complexity while preserving 90%+ of data variance in many applications.",
      "term": "Principal Component Analysis (PCA)",
      "length": 265
    },
    {
      "footnote_id": "fn-mnist",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1104,
      "definition": "**MNIST**: Modified National Institute of Standards and Technology database of handwritten digits, containing 70,000 28\u00d728 pixel images. Created in 1998, became the \"Hello World\" of computer vision, though modern models achieve >99% accuracy.",
      "term": "MNIST",
      "length": 242
    },
    {
      "footnote_id": "fn-cifar10",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1106,
      "definition": "**CIFAR-10**: Canadian Institute for Advanced Research dataset with 60,000 32\u00d732 color images across 10 classes. Released in 2009, remains a standard benchmark despite its small image size by modern standards.",
      "term": "CIFAR-10",
      "length": 209
    },
    {
      "footnote_id": "fn-efficient-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1114,
      "definition": "**Transfer Learning**: Technique where models pre-trained on large datasets are fine-tuned for specific tasks. ImageNet pre-trained models can achieve high accuracy on new vision tasks with <1000 labeled examples vs. millions needed from scratch.",
      "term": "Transfer Learning",
      "length": 246
    },
    {
      "footnote_id": "fn-data-augmentation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1116,
      "definition": "**Data Augmentation**: Artificially expanding datasets through transformations like rotations, crops, or noise. Can improve model performance by 5-15% and reduce overfitting, especially valuable when labeled data is scarce.",
      "term": "Data Augmentation",
      "length": 223
    },
    {
      "footnote_id": "fn-active-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1118,
      "definition": "**Active Learning**: Iteratively selecting the most informative samples for labeling to maximize learning efficiency. Can achieve target performance with 50-90% less labeled data compared to random sampling strategies.",
      "term": "Active Learning",
      "length": 218
    },
    {
      "footnote_id": "fn-data-centric-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1128,
      "definition": "**Data-Centric AI**: Paradigm shift from model-centric to data-centric development, popularized by Andrew Ng in 2021. Focuses on systematically improving data quality rather than just model architecture, often yielding greater performance gains.",
      "term": "Data-Centric AI",
      "length": 245
    },
    {
      "footnote_id": "fn-self-supervised",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1132,
      "definition": "**Self-Supervised Learning**: Training method where models create their own labels from input data structure, like predicting masked words in BERT or next frames in videos. Enables learning from billions of unlabeled examples, revolutionizing NLP and computer vision.",
      "term": "Self-Supervised Learning",
      "length": 267
    },
    {
      "footnote_id": "fn-curriculum-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1134,
      "definition": "**Curriculum Learning**: Training strategy where models learn from easy examples before progressing to harder ones, mimicking human education. Can improve convergence speed by 25-50% and final model performance across various domains.",
      "term": "Curriculum Learning",
      "length": 234
    },
    {
      "footnote_id": "fn-efficient-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1138,
      "definition": "**Foundation Models**: Large-scale, general-purpose AI models trained on broad data that can be adapted for many tasks. Term coined by Stanford HAI in 2021, includes models like GPT-3, BERT, and DALL-E with billions of parameters.",
      "term": "Foundation Models",
      "length": 230
    },
    {
      "footnote_id": "fn-tinyml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1144,
      "definition": "**TinyML**: Machine learning on microcontrollers and edge devices with <1KB-1MB memory and <1mW power consumption. Enables AI in IoT devices, wearables, and sensors where traditional ML deployment is impossible due to resource constraints. These ultra-low-power chips contain a processor, memory, and peripherals on a single chip with dramatically limited resources.",
      "term": "TinyML",
      "length": 366
    },
    {
      "footnote_id": "fn-npus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1214,
      "definition": "**Neural Processing Units (NPUs)**: Specialized processors designed for AI workloads on mobile devices. Apple's A17 Pro contains a 16-core NPU delivering 35 TOPS (trillion operations per second) while consuming just 2-3 watts, enabling on-device AI without draining battery life.",
      "term": "Neural Processing Units (NPUs)",
      "length": 279
    },
    {
      "footnote_id": "fn-lidar",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1232,
      "definition": "**LiDAR**: Light Detection and Ranging technology that uses laser pulses to create detailed 3D maps of surroundings. Autonomous vehicles generate ~70GB of LiDAR data daily, requiring real-time processing of millions of distance measurements per second for obstacle detection and navigation.",
      "term": "LiDAR",
      "length": 290
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1318,
      "definition": "**Batch Processing**: Technique that groups multiple inputs together for simultaneous processing, improving computational efficiency. Instead of processing 100 images one-by-one (100 separate operations), batching processes them together in groups of 32, reducing overhead and improving GPU utilization by ~3-5\u00d7.",
      "term": "Batch Processing",
      "length": 312
    },
    {
      "footnote_id": "fn-frameworks-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 128,
      "definition": "**BLAS (Basic Linear Algebra Subprograms)**: Originally developed at Argonne National Laboratory, BLAS became the de facto standard for linear algebra operations, with Level 1 (vector-vector), Level 2 (matrix-vector), and Level 3 (matrix-matrix) operations that still underpin every modern ML framework.",
      "term": "BLAS (Basic Linear Algebra Subprograms)",
      "length": 303
    },
    {
      "footnote_id": "fn-lapack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 132,
      "definition": "**LAPACK (Linear Algebra Package)**: Succeeded LINPACK and EISPACK, introducing block algorithms that dramatically improved cache efficiency and parallel execution, innovations that became essential as datasets grew from megabytes to terabytes.",
      "term": "LAPACK (Linear Algebra Package)",
      "length": 244
    },
    {
      "footnote_id": "fn-theano",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 146,
      "definition": "**Theano**: Named after the ancient Greek mathematician Theano of Croton, this framework pioneered the concept of symbolic mathematical expressions in Python, laying the groundwork for every modern deep learning framework.",
      "term": "Theano",
      "length": 222
    },
    {
      "footnote_id": "fn-comp-graphs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 148,
      "definition": "**Computational Graphs**: First formalized in automatic differentiation literature by Wengert (1964), this representation became the backbone of modern ML frameworks, enabling both forward and reverse-mode differentiation at unprecedented scale.",
      "term": "Computational Graphs",
      "length": 245
    },
    {
      "footnote_id": "fn-eager-execution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 152,
      "definition": "**Eager Execution**: An execution model where operations are evaluated immediately as they are called, similar to standard Python execution. Pioneered by Torch in 2002, this approach prioritizes developer productivity and debugging ease over performance optimization, becoming the default mode in modern frameworks like PyTorch and TensorFlow 2.x. Torch's design philosophy of prioritizing developer experience while maintaining high performance established design patterns that would later influence frameworks like PyTorch. Its architecture demonstrated how to balance high-level abstractions with efficient low-level matrix operations.",
      "term": "Eager Execution",
      "length": 638
    },
    {
      "footnote_id": "fn-tensorflow",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 164,
      "definition": "**TensorFlow**: Named after tensor operations flowing through computational graphs, this framework democratized distributed machine learning by open-sourcing Google's internal DistBelief system, instantly giving researchers access to infrastructure that previously required massive corporate resources.",
      "term": "TensorFlow",
      "length": 302
    },
    {
      "footnote_id": "fn-kernel-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 166,
      "definition": "**Kernel Fusion**: An optimization technique that combines multiple separate operations (like matrix multiplication followed by bias addition and activation) into a single GPU kernel, reducing memory bandwidth requirements by up to 10x and eliminating intermediate memory allocations. This optimization is particularly crucial for complex deep learning models with thousands of operations.",
      "term": "Kernel Fusion",
      "length": 389
    },
    {
      "footnote_id": "fn-memory-planning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 168,
      "definition": "**Memory Planning**: A framework optimization that pre-analyzes computational graphs to determine optimal memory allocation strategies, enabling techniques like in-place operations and memory reuse patterns that can reduce peak memory usage by 40-60% during training.",
      "term": "Memory Planning",
      "length": 267
    },
    {
      "footnote_id": "fn-static-graph",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 170,
      "definition": "**Static Computational Graph**: A pre-defined computation structure where the entire model architecture is specified before execution, enabling global optimizations and efficient memory planning. Pioneered by TensorFlow 1.x, this approach sacrifices runtime flexibility for maximum performance optimization, making it ideal for production deployments.",
      "term": "Static Computational Graph",
      "length": 351
    },
    {
      "footnote_id": "fn-gradient-accumulation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 172,
      "definition": "**Gradient Accumulation**: A training technique where gradients from multiple mini-batches are computed and summed before updating model parameters, effectively simulating larger batch sizes without requiring additional memory. Essential for training large models where memory constraints limit batch size to as small as 1 sample per device.",
      "term": "Gradient Accumulation",
      "length": 341
    },
    {
      "footnote_id": "fn-gradient-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 174,
      "definition": "**Gradient Checkpointing**: A memory optimization technique that trades computation time for memory by selectively storing only certain intermediate activations during the forward pass, then recomputing discarded values during gradient computation. Can reduce memory usage by 50-90% for deep networks while increasing training time by only 20-33%.",
      "term": "Gradient Checkpointing",
      "length": 347
    },
    {
      "footnote_id": "fn-asic-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 176,
      "definition": "**ASIC (Application-Specific Integrated Circuit)**: Custom silicon chips designed for specific tasks, contrasting with general-purpose CPUs. In ML contexts, ASICs like Google's TPUs and Tesla's FSD chips sacrifice flexibility for 10-100x efficiency gains in matrix operations, though they require 2-4 years development time and millions in upfront costs.",
      "term": "ASIC (Application-Specific Integrated Circuit)",
      "length": 354
    },
    {
      "footnote_id": "fn-pytorch",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 182,
      "definition": "**PyTorch**: Inspired by the original Torch framework from NYU, PyTorch brought \"define-by-run\" semantics to Python, enabling researchers to modify models during execution, a breakthrough that accelerated research by making debugging as simple as using a standard Python debugger.",
      "term": "PyTorch",
      "length": 280
    },
    {
      "footnote_id": "fn-jax",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 190,
      "definition": "**JAX**: Stands for \"Just After eXecution\" and combines NumPy's API with functional programming transforms (jit, grad, vmap, pmap), enabling researchers to write concise code that automatically scales to TPUs and GPU clusters while maintaining NumPy compatibility.",
      "term": "JAX",
      "length": 264
    },
    {
      "footnote_id": "fn-frameworks-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 204,
      "definition": "**TPU (Tensor Processing Unit)**: Google's custom ASIC achieved 15-30x better performance-per-watt than contemporary GPUs and CPUs for neural networks, proving that domain-specific architectures could outperform general-purpose processors for ML workloads.",
      "term": "TPU (Tensor Processing Unit)",
      "length": 256
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 206,
      "definition": "**Systolic Array**: A specialized parallel computing architecture invented by H.T. Kung (CMU) and Charles Leiserson (MIT) in 1978, where data flows through a grid of processing elements in a rhythmic, pipeline fashion. Each element performs simple operations on data flowing from neighbors, making it exceptionally efficient for matrix operations, which form the heart of neural network computations.",
      "term": "Systolic Array",
      "length": 400
    },
    {
      "footnote_id": "fn-intermediate-representation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 219,
      "definition": "**Intermediate Representation (IR)**: A framework-internal format that sits between high-level user code and hardware-specific machine code, enabling optimizations and cross-platform deployment. Modern ML frameworks use IRs like TensorFlow's XLA or PyTorch's TorchScript to compile the same model for CPUs, GPUs, TPUs, and mobile devices.",
      "term": "Intermediate Representation (IR)",
      "length": 338
    },
    {
      "footnote_id": "fn-linear-transformations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 221,
      "definition": "**Linear Transformations**: Mathematical operations that preserve vector addition and scalar multiplication, typically implemented as matrix multiplication in neural networks. Each layer applies a learned linear transformation (weights matrix) followed by a non-linear activation function (like ReLU or sigmoid), enabling networks to learn complex patterns from simple mathematical building blocks.",
      "term": "Linear Transformations",
      "length": 398
    },
    {
      "footnote_id": "fn-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 223,
      "definition": "**Data Parallelism**: A distributed training strategy where identical model copies process different data subsets in parallel, then synchronize gradients. Enables near-linear speedup with additional devices but requires models that fit in single-device memory, making it ideal for training on datasets with billions of samples.",
      "term": "Data Parallelism",
      "length": 327
    },
    {
      "footnote_id": "fn-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 225,
      "definition": "**Model Parallelism**: A strategy for training models too large for single devices by partitioning the model architecture across multiple processors. Essential for models like GPT-3 (175B parameters) that exceed GPU memory limits, though it requires careful optimization to minimize communication overhead between model partitions.",
      "term": "Model Parallelism",
      "length": 331
    },
    {
      "footnote_id": "fn-operation-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 227,
      "definition": "**Operation Fusion**: Compiler optimization that combines multiple sequential operations into a single kernel to reduce memory bandwidth and latency. For example, fusing matrix multiplication, bias addition, and ReLU activation can eliminate intermediate memory allocations and achieve 2-3x speedup on modern GPUs.",
      "term": "Operation Fusion",
      "length": 314
    },
    {
      "footnote_id": "fn-jit-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 229,
      "definition": "**Just-In-Time (JIT) Compilation**: In ML frameworks, JIT compilation differs from traditional JIT by optimizing for tensor operations and hardware accelerators rather than general CPU instructions. ML JIT compilers like TensorFlow's XLA and PyTorch's TorchScript analyze computation patterns at runtime to generate optimized kernels for specific tensor shapes and device capabilities.",
      "term": "Just-In-Time (JIT) Compilation",
      "length": 385
    },
    {
      "footnote_id": "fn-dag-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 339,
      "definition": "**Directed Acyclic Graph (DAG)**: In machine learning frameworks, DAGs represent computation where nodes are operations (like matrix multiplication or activation functions) and edges are data dependencies. Unlike general DAGs in computer science, ML computational graphs specifically optimize for automatic differentiation, enabling frameworks to compute gradients by traversing the graph in reverse order.",
      "term": "Directed Acyclic Graph (DAG)",
      "length": 406
    },
    {
      "footnote_id": "fn-auto-diff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 647,
      "definition": "**Automatic Differentiation**: Invented by Robert Edwin Wengert in 1964, this technique achieves machine precision derivatives by applying the chain rule at the elementary operation level, making neural network training computationally feasible for networks with millions of parameters.",
      "term": "Automatic Differentiation",
      "length": 286
    },
    {
      "footnote_id": "immutable-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2967,
      "definition": "**Immutable Data Structures**: Cannot be modified after creation. Any operation that appears to change the data actually creates a new copy, ensuring that the original data remains unchanged. This prevents accidental modifications and enables safe parallel processing.",
      "term": "Immutable Data Structures",
      "length": 268
    },
    {
      "footnote_id": "stateless-function",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2969,
      "definition": "**Stateless Function**: Produces the same output for the same inputs every time, without relying on or modifying any external state. This predictability is essential for mathematical optimization and parallel execution.",
      "term": "Stateless Function",
      "length": 219
    },
    {
      "footnote_id": "vectorization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2971,
      "definition": "**Automatic Vectorization**: Transforms operations on single data points into operations on entire arrays or batches, significantly improving computational efficiency by leveraging SIMD (Single Instruction, Multiple Data) processor capabilities.",
      "term": "Automatic Vectorization",
      "length": 245
    },
    {
      "footnote_id": "jit-compilation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2973,
      "definition": "**Just-in-Time (JIT) Compilation**: Translates high-level code into optimized machine code at runtime, enabling performance optimizations based on actual data shapes and hardware characteristics.",
      "term": "Just-in-Time (JIT) Compilation",
      "length": 195
    },
    {
      "footnote_id": "pure-function",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 2975,
      "definition": "**Pure Function**: Has no side effects and always returns the same output for the same inputs. Pure functions enable mathematical reasoning about code behavior and safe program transformations.",
      "term": "Pure Function",
      "length": 193
    },
    {
      "footnote_id": "fn-llm-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 47,
      "definition": "**Large Language Models (LLMs)**: Neural networks trained on internet-scale text corpora, typically containing >10 billion parameters. GPT-3 (175B parameters) required approximately 3.14 \u00d7 10\u00b2\u00b3 floating-point operations for training. GPT-4's exact size remains undisclosed but estimates suggest over 1 trillion parameters distributed across multiple expert models.",
      "term": "Large Language Models (LLMs)",
      "length": 364
    },
    {
      "footnote_id": "fn-chatgpt-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 51,
      "definition": "**ChatGPT User Adoption**: Reached 100 million monthly active users in January 2023, 2 months after launch, the fastest consumer application adoption in history. For comparison: TikTok required 9 months, Instagram 2.5 years. This growth necessitated rapid scaling of inference infrastructure from hundreds to tens of thousands of GPUs.",
      "term": "ChatGPT User Adoption",
      "length": 335
    },
    {
      "footnote_id": "fn-phase-transition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 55,
      "definition": "**Phase Transition**: In physics, abrupt changes in system properties at critical thresholds (like water becoming ice at 0\u00b0C). In neural networks, refers to sudden appearance of capabilities at specific model scales\u2014abilities appear rapidly rather than gradually. Examples: GPT models gain arithmetic ability around 10B parameters, reasoning around 100B parameters. These discontinuous jumps suggest major changes in network dynamics.",
      "term": "Phase Transition",
      "length": 434
    },
    {
      "footnote_id": "fn-agi-compute-requirements",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 57,
      "definition": "**AGI Compute Requirements Engineering Analysis**: **Biological baseline**: Human brain ~10\u00b9\u2075 ops/sec \u00d7 20 years = 6.3 \u00d7 10\u00b2\u00b3 total operations. **Scaling law extrapolation**: Chinchilla optimal scaling suggests compute C and parameters N follow C \u221d N^1.3. For AGI requiring ~100T parameters (100\u00d7 GPT-3), this yields C = (100\u00b9\u00b7\u00b3) \u00d7 (GPT-3 compute) = 1,585 \u00d7 3.14 \u00d7 10\u00b2\u00b3 = 5 \u00d7 10\u00b2\u2076 FLOPs. **Efficiency assumptions**: Accounting for 10\u00d7 training efficiency improvements gives practical requirement of 2.5 \u00d7 10\u00b2\u2076 FLOPs. **Sensitivity analysis**: \u00b11 order of magnitude depending on architecture breakthroughs and efficiency gains.",
      "term": "AGI Compute Requirements Engineering Analysis",
      "length": 626
    },
    {
      "footnote_id": "fn-intelligence-theory",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 59,
      "definition": "**Intelligence vs. Performance**: Intelligence involves understanding underlying principles rather than memorizing patterns. Humans generalize from few examples through causal reasoning, while current AI requires massive datasets for statistical correlation. The symbol grounding problem asks how abstract symbols connect to embodied experience, a challenge absent in pure language models.",
      "term": "Intelligence vs. Performance",
      "length": 389
    },
    {
      "footnote_id": "fn-energy-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 61,
      "definition": "**Energy-Based Models (EBMs)**: Unlike discriminative models that output single predictions, EBMs learn energy functions where probable outcomes have low energy. This enables modeling multiple solutions, handling uncertainty, and optimization-based inference. Applications include molecular design, theorem proving, and multi-step reasoning where multiple valid solutions exist.",
      "term": "Energy-Based Models (EBMs)",
      "length": 378
    },
    {
      "footnote_id": "fn-moores-end",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 63,
      "definition": "**End of Moore's Law**: Transistor density improvements slowed from 50% annually (1970-2010) to 10-20% (2010-2025). Physical limits include quantum tunneling at 3-5nm nodes, manufacturing costs exceeding $20B per fab, and power density approaching nuclear reactor levels.",
      "term": "End of Moore's Law",
      "length": 271
    },
    {
      "footnote_id": "fn-3d-stacking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 65,
      "definition": "**3D Chip Stacking**: Samsung's 176-layer 3D NAND achieves 100x higher density than planar designs. Thermal management becomes critical: stacked processors generate 1000W/cm\u00b2 heat flux requiring advanced cooling. Applications include high-bandwidth memory (HBM) and processing-in-memory architectures.",
      "term": "3D Chip Stacking",
      "length": 301
    },
    {
      "footnote_id": "fn-chiplet-benefits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 67,
      "definition": "**Chiplet Architecture**: AMD's EPYC uses 8-12 chiplets connected via Infinity Fabric, achieving better yields and performance than monolithic designs. For AGI, enables mixing specialized processors: matrix units, memory controllers, and networking components in optimal ratios.",
      "term": "Chiplet Architecture",
      "length": 278
    },
    {
      "footnote_id": "fn-optical-interconnects",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 69,
      "definition": "**Optical Interconnects**: Silicon photonics achieves 100 Tbps bandwidth with 10x lower energy than electrical interconnects. Critical for AGI training where communication between 100,000+ processors becomes the bottleneck. Intel, NVIDIA, and startups developing optical chip-to-chip links.",
      "term": "Optical Interconnects",
      "length": 290
    },
    {
      "footnote_id": "fn-processing-in-memory",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 71,
      "definition": "**Processing-in-Memory (PIM)**: Performs computation directly in DRAM or flash memory, eliminating data movement. Samsung's HBM-PIM and Upmem's PIM-DIMM demonstrate 100x energy reduction for memory-bound workloads. Essential for AGI where parameter access dominates energy consumption.",
      "term": "Processing-in-Memory (PIM)",
      "length": 285
    },
    {
      "footnote_id": "fn-neuromorphic-promise",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 73,
      "definition": "**Neuromorphic Computing**: Intel's Loihi chip achieves 1000x energy efficiency over digital processors for sparse, event-driven workloads. IBM's TrueNorth demonstrates 1 million neurons with 70mW power consumption. Promise for continuous learning AGI systems but programming remains challenging.",
      "term": "Neuromorphic Computing",
      "length": 296
    },
    {
      "footnote_id": "fn-quantum-hybrid",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 75,
      "definition": "**Quantum-Classical Hybrid**: IBM's quantum processors demonstrate quantum advantage for optimization problems. D-Wave's annealing systems solve scheduling and resource allocation. For AGI, quantum processors could accelerate search through exponentially large solution spaces while classical systems handle standard neural computation.",
      "term": "Quantum-Classical Hybrid",
      "length": 336
    },
    {
      "footnote_id": "fn-agi-infrastructure-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 77,
      "definition": "**AGI Infrastructure Scale Engineering Analysis**: **Hardware requirements**: 2.5 \u00d7 10\u00b2\u2076 FLOPs \u00f7 (1,000 TFLOPS/GPU \u00d7 8,760 hours/year \u00d7 0.7 utilization) = 175,000 H100 GPUs. **Power calculation**: 175,000 GPUs \u00d7 700W + 30% cooling overhead = 159 MW total consumption. **Cost breakdown**: 175,000 \u00d7 $30,000/GPU = $5.25B hardware + $15B infrastructure + $10B power (3 years) + $20B operational = $52B total. **Network bandwidth**: 175,000 GPUs \u00d7 900 GB/s NVLink = 158 PB/s bisection bandwidth requiring 100,000+ optical links. **Memory requirements**: 175,000 \u00d7 80GB HBM = 14 petabytes active parameter storage. Post-Moore's Law efficiency gains (neuromorphic 100\u00d7, quantum-hybrid 10\u00d7 for optimization) could reduce total requirements to $5-10B.",
      "term": "AGI Infrastructure Scale Engineering Analysis",
      "length": 743
    },
    {
      "footnote_id": "fn-constitutional-intro",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 91,
      "definition": "**Constitutional AI**: A training method developed by Anthropic where models learn to improve their own outputs by critiquing responses against a set of principles. This technique reduces harmful content while maintaining helpfulness, detailed in the training section of this chapter.",
      "term": "Constitutional AI",
      "length": 284
    },
    {
      "footnote_id": "fn-multi-agent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 129,
      "definition": "**Multi-Agent Intelligence**: @baker2019emergent hide-and-seek agents developed unexpected strategies through competition. @autogpt2023 and @babbage2023 demonstrate early autonomous agent capabilities, though remain limited by context windows and error accumulation.",
      "term": "Multi-Agent Intelligence",
      "length": 266
    },
    {
      "footnote_id": "fn-rlhf-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 420,
      "definition": "**RLHF Effectiveness**: InstructGPT preferred over GPT-3 in 85% of comparisons despite being 100x smaller. Harmful output reduction: 90%. Hallucination reduction: 40%. User satisfaction increase: 72%.",
      "term": "RLHF Effectiveness",
      "length": 200
    },
    {
      "footnote_id": "fn-human-feedback-limits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 426,
      "definition": "**Human Feedback Bottlenecks**: ChatGPT required 40 annotators working full-time for 3 months to generate 200K labels. Scaling to GPT-4's capabilities would require 10,000+ annotators. Inter-annotator agreement typically reaches only 70-80%.",
      "term": "Human Feedback Bottlenecks",
      "length": 241
    },
    {
      "footnote_id": "fn-constitutional-approach",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 430,
      "definition": "**Constitutional AI Method**: @bai2022constitutional implementation uses 16 principles like \"avoid harmful content\" and \"be helpful.\" The model performs 5 rounds of self-critique and revision. Harmful outputs reduced by 95% while maintaining 90% of original helpfulness.",
      "term": "Constitutional AI Method",
      "length": 270
    },
    {
      "footnote_id": "fn-deployment-freeze",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 482,
      "definition": "**Static Model Problem**: GPT-3 trained on data before 2021 permanently believes it's 2021. Models cannot learn user preferences, correct mistakes, or incorporate new knowledge without full retraining costing millions of dollars.",
      "term": "Static Model Problem",
      "length": 229
    },
    {
      "footnote_id": "fn-catastrophic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 486,
      "definition": "**Catastrophic Forgetting Severity**: Standard neural networks lose 20-80% accuracy on task A when trained on task B. In language models, fine-tuning on medical text degrades general conversation ability by 30-50%.",
      "term": "Catastrophic Forgetting Severity",
      "length": 214
    },
    {
      "footnote_id": "fn-gpt4-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 663,
      "definition": "**GPT-4 Energy Consumption**: Estimated 50-100 GWh for training (equivalent to 50,000 US homes' annual usage). At $0.10/kWh plus hardware amortization, training cost exceeds $100 million. AGI might require 1000x more.",
      "term": "GPT-4 Energy Consumption",
      "length": 217
    },
    {
      "footnote_id": "fn-brain-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 667,
      "definition": "**Biological Efficiency**: As detailed in computational analysis above, the brain achieves 35x better operations per watt than current hardware despite using chemical signaling. This comparison should be interpreted carefully as biological and digital computation operate on fundamentally different principles.",
      "term": "Biological Efficiency",
      "length": 310
    },
    {
      "footnote_id": "fn-reasoning-limitation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 673,
      "definition": "**Reasoning Performance Cliff**: LLMs achieve 90%+ on familiar problem types but drop to 10-30% on problems requiring genuine novelty. ARC challenge (abstraction and reasoning) reveals models memorize patterns rather than learning abstract rules.",
      "term": "Reasoning Performance Cliff",
      "length": 246
    },
    {
      "footnote_id": "fn-reasoning-requirements",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 677,
      "definition": "**Reasoning Architecture Requirements**: Classical planning requires: state representation, action models, goal specification, and search algorithms. Neural networks provide none explicitly. Neurosymbolic approaches attempt integration but remain limited to narrow domains.",
      "term": "Reasoning Architecture Requirements",
      "length": 273
    },
    {
      "footnote_id": "fn-embodiment-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 685,
      "definition": "**Robotic System Requirements**: Boston Dynamics' Atlas runs 1KHz control loops with 28 actuators. Tesla's FSD processes 36 camera streams at 36 FPS. Both require <10ms inference latency\u2014impossible with cloud processing.",
      "term": "Robotic System Requirements",
      "length": 220
    },
    {
      "footnote_id": "fn-alignment-challenge",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 691,
      "definition": "**Alignment Failure Modes**: YouTube's algorithm optimizing watch time promoted increasingly extreme content. Trading algorithms optimizing profit caused flash crashes. AGI optimizing misspecified objectives could cause existential risks.",
      "term": "Alignment Failure Modes",
      "length": 238
    },
    {
      "footnote_id": "fn-alignment-components",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 695,
      "definition": "**Alignment Technical Challenges**: Value specification: Arrow's impossibility theorem shows no perfect aggregation of preferences. Robust optimization: Goodhart's law states optimized metrics cease being good metrics. Corrigibility: Self-modifying systems might remove safety constraints. Scalable oversight: Humans cannot verify solutions to problems they cannot solve.",
      "term": "Alignment Technical Challenges",
      "length": 371
    },
    {
      "footnote_id": "fn-agi-agent-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 752,
      "definition": "**AGI Agent Scale**: Estimates suggest AGI systems might require 10\u2076-10\u2077 specialized agents for human-level capabilities across all domains. Each agent could be GPT-4 scale or larger. Coordination complexity grows as O(n\u00b2) without hierarchical organization, making flat architectures impossible at this scale.",
      "term": "AGI Agent Scale",
      "length": 309
    },
    {
      "footnote_id": "fn-agi-communication",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 754,
      "definition": "**AGI Communication Complexity**: Agent communication must convey semantic content equivalent to full reasoning states, potentially terabytes per message. Current internet protocols (TCP/IP) lack semantic understanding. Future AGI networks might use content-addressable routing, semantic compression, and reasoning-aware network stacks.",
      "term": "AGI Communication Complexity",
      "length": 336
    },
    {
      "footnote_id": "fn-agi-topology",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 756,
      "definition": "**AGI Network Topology**: Hierarchical networks reduce communication complexity from O(n\u00b2) to O(n log n). Biological neural networks use similar hierarchies: local processing clusters, regional integration areas, and global coordination structures. AGI systems likely require analogous network architectures.",
      "term": "AGI Network Topology",
      "length": 308
    },
    {
      "footnote_id": "fn-agi-consensus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 758,
      "definition": "**AGI Consensus Complexity**: Unlike traditional consensus on simple state transitions, AGI consensus involves competing world models, subjective values, and reasoning chains. This requires new consensus mechanisms that handle semantic disagreement, argument quality assessment, and uncertainty quantification.",
      "term": "AGI Consensus Complexity",
      "length": 310
    },
    {
      "footnote_id": "fn-agi-byzantine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 760,
      "definition": "**AGI Byzantine Threats**: Beyond random failures, AGI agents face systematic threats: biased training data causing consistent errors, misaligned objectives leading to subtle manipulation, and adversarial attacks spreading sophisticated misinformation. Defense requires advances beyond traditional 3f+1 Byzantine fault tolerance.",
      "term": "AGI Byzantine Threats",
      "length": 329
    },
    {
      "footnote_id": "fn-agi-resource-coordination",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 762,
      "definition": "**AGI Resource Coordination**: Managing compute resources across millions of reasoning agents requires predictive load balancing based on reasoning complexity estimation, priority systems understanding reasoning urgency, and graceful degradation maintaining system coherence under resource constraints.",
      "term": "AGI Resource Coordination",
      "length": 302
    },
    {
      "footnote_id": "fn-infra-bottleneck",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 776,
      "definition": "**Infrastructure Efficiency Gap**: Current GPU clusters achieve 20-40% utilization during training due to communication overhead, load imbalancing, and fault recovery. Improving utilization to 70-80% would reduce training costs by 40-60%, worth billions annually. AGI-scale systems require 99.99% utilization across million-GPU clusters.",
      "term": "Infrastructure Efficiency Gap",
      "length": 337
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 813,
      "definition": "**Tensor Processing Unit (TPU)**: Google's custom application-specific integrated circuit (ASIC) designed specifically for neural network machine learning. First generation (2015) achieved 15-30x higher performance and 30-80x better performance-per-watt than contemporary CPUs/GPUs for inference. TPU v4 (2021) delivers 275 teraFLOPs for training with specialized matrix multiplication units.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 392
    },
    {
      "footnote_id": "fn-personalization-tech",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 819,
      "definition": "**Personalization Technical Foundations**: Parameter-efficient fine-tuning (LoRA) reduces personalization costs by 1000x. Retrieval systems enable personal knowledge bases. Constitutional AI allows custom value alignment per user.",
      "term": "Personalization Technical Foundations",
      "length": 230
    },
    {
      "footnote_id": "fn-workflow-automation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 839,
      "definition": "**Workflow Automation Scale**: McKinsey estimates 60-70% of current jobs contain 30%+ automatable activities. But current automation covers <5% of possible workflows due to integration complexity, not capability limitations.",
      "term": "Workflow Automation Scale",
      "length": 224
    },
    {
      "footnote_id": "fn-realtime-requirements",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 859,
      "definition": "**Real-Time Latency Requirements**: Autonomous vehicles need <10ms perception-to-action loops. Conversational AI requires <200ms response for natural interaction. Robotic surgery demands <1ms control loops. Current cloud systems achieve 50-200ms best case.",
      "term": "Real-Time Latency Requirements",
      "length": 256
    },
    {
      "footnote_id": "fn-explainability-demand",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frontiers/frontiers.qmd",
      "line": 879,
      "definition": "**Explainability Market Growth**: Explainable AI market projected to grow from $5.2B (2023) to $21.4B (2030). Regulatory requirements in EU AI Act and medical device approval drive 60%+ of demand.",
      "term": "Explainability Market Growth",
      "length": 196
    },
    {
      "footnote_id": "fn-gflops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 49,
      "definition": "**GFLOPS (Giga Floating-Point Operations Per Second)**: A measure of computational throughput representing one billion floating-point operations per second. For context, modern CPUs achieve ~100 GFLOPS, high-end GPUs reach ~15,000 GFLOPS, and specialized AI chips can exceed 100,000 GFLOPS. The dramatic 1000x difference between CPUs and AI accelerators explains why specialized hardware became essential for practical machine learning deployment.",
      "term": "GFLOPS (Giga Floating-Point Operations Per Second)",
      "length": 447
    },
    {
      "footnote_id": "fn-l1-cache",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 53,
      "definition": "**L1 Cache**: The fastest but smallest memory in a CPU, typically 32-64KB per core with <1ns access time. L1 cache sits closest to the processor and operates at CPU clock speed. The 30,000x size mismatch between modern neural network parameters (gigabytes) and L1 cache capacity (kilobytes) illustrates why traditional memory hierarchies fail for AI workloads, necessitating specialized architectures with larger on-chip memory. Specialized accelerators address this through optimized memory hierarchies: Google's TPU provides 128MB of on-chip memory running at 900 GB/s\u2014600x faster than typical RAM\u2014because keeping weights on-chip dramatically reduces both latency and energy consumption. These trade-offs quantify how memory-aware system design allows practical deployment of large-scale models.",
      "term": "L1 Cache",
      "length": 797
    },
    {
      "footnote_id": "fn-memory-bandwidth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 61,
      "definition": "**Memory Bandwidth**: The rate at which data can be transferred between memory and processors, measured in GB/s or TB/s. AI workloads are often bandwidth-bound rather than compute-bound\u2014NVIDIA H100 provides 3.35 TB/s (70x faster than DDR5) [@nvidia2022h100] because neural networks require constant weight access. A single large neural network layer needs to read 1GB+ of parameters for each forward pass, making memory bandwidth the primary bottleneck in many AI applications.",
      "term": "Memory Bandwidth",
      "length": 477
    },
    {
      "footnote_id": "fn-von-neumann",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 69,
      "definition": "**Von Neumann Architecture**: Proposed by John von Neumann in 1945, this architecture separates program storage from data storage but forces all data to flow through a single bus between CPU and memory. In AI workloads, this \"von Neumann bottleneck\" becomes critical\u2014moving a 1GB model from memory consumes 100-1000x more energy than the actual computation, driving the need for specialized architectures that bring computation closer to data.",
      "term": "Von Neumann Architecture",
      "length": 443
    },
    {
      "footnote_id": "fn-hwacc-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 77,
      "definition": "**TPU Origins**: Google secretly developed the Tensor Processing Unit (TPU) starting in 2013 when they realized CPUs couldn't handle the computational demands of their neural networks. The TPUv1, deployed in 2015, delivered 15-30x better performance per watt than contemporary GPUs for inference. This breakthrough significantly changed how the industry approached AI hardware, proving that domain-specific architectures could dramatically outperform general-purpose processors for neural network workloads.",
      "term": "TPU Origins",
      "length": 507
    },
    {
      "footnote_id": "fn-intel-8087",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 87,
      "definition": "**Intel 8087 Impact**: The 8087 coprocessor cost $750 (about $2,800 today) but transformed scientific computing\u2014CAD workstations that took hours for complex calculations could complete them in minutes. This success created the entire coprocessor market and established the economic model for specialized hardware that persists today: charge premium prices for dramatic performance improvements in specific domains.",
      "term": "Intel 8087 Impact",
      "length": 414
    },
    {
      "footnote_id": "fn-coprocessor",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 91,
      "definition": "**Coprocessor**: A specialized secondary processor designed to handle specific tasks that the main CPU performs poorly. The 8087 math coprocessor was the first successful example, followed by graphics coprocessors (GPUs) and network processors. Modern \"accelerators\" are essentially evolved coprocessors\u2014the term changed as these chips became more powerful than host CPUs for their target workloads. Today's AI accelerators follow the same pattern but often eclipse CPU performance.",
      "term": "Coprocessor",
      "length": 482
    },
    {
      "footnote_id": "fn-hwacc-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 185,
      "definition": "**AlexNet's GPU Revolution**: AlexNet's breakthrough wasn't just algorithmic\u2014it proved GPUs could train deep networks 10x faster than CPUs [@krizhevsky2012alexnet]. The team split the 8-layer network across two NVIDIA GTX 580s (512 cores each), reducing training time from weeks to days. This success triggered the \"deep learning gold rush\" and established NVIDIA as the default AI hardware company, with GPU sales for data centers growing from $200 million to $47 billion by 2024. Modern GPUs like the NVIDIA H100 contain 16,896 CUDA cores, demonstrating the massive scaling in parallel processing capability since AlexNet's era.",
      "term": "AlexNet's GPU Revolution",
      "length": 630
    },
    {
      "footnote_id": "fn-dsa",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 191,
      "definition": "**Domain-Specific Architectures (DSA)**: Computing architectures optimized for specific application domains rather than general-purpose computation. Unlike CPUs designed for flexibility, DSAs sacrifice programmability for dramatic efficiency gains\u2014Google's TPU achieves 15-30x better performance per watt than GPUs for neural networks, while video codecs provide 100-1000x improvements over software decoding. The 2018 Turing Award recognized this shift as the defining trend in modern computer architecture.",
      "term": "Domain-Specific Architectures (DSA)",
      "length": 508
    },
    {
      "footnote_id": "fn-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 193,
      "definition": "**Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every 18-24 months. This exponential scaling drove computing progress for 50 years, enabling everything from smartphones to supercomputers. However, physical limits around 2005 slowed this pace dramatically\u2014modern 3nm chips cost $20 billion to develop versus $3 million in 1999, forcing the industry toward specialized architectures.",
      "term": "Moore's Law",
      "length": 431
    },
    {
      "footnote_id": "fn-dennard-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 195,
      "definition": "**Dennard Scaling**: Robert Dennard's 1974 principle that as transistors shrink, their power density remains constant, allowing higher frequencies without increased power consumption. This enabled CPUs to reach 3+ GHz by 2005. However, quantum effects and leakage current ended Dennard scaling around 2005, forcing architects to prioritize efficiency over raw speed and leading to the multi-core revolution.",
      "term": "Dennard Scaling",
      "length": 407
    },
    {
      "footnote_id": "fn-asics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 211,
      "definition": "**Application-Specific Integrated Circuits (ASICs)**: Custom silicon chips designed for a single application, offering maximum efficiency by eliminating unused features. Bitcoin mining ASICs achieve 100,000x better energy efficiency than CPUs for SHA-256 hashing. However, their inflexibility means they become worthless if algorithms change\u2014$5 billion in Ethereum mining ASICs became obsolete overnight when Ethereum switched to proof-of-stake in 2022.",
      "term": "Application-Specific Integrated Circuits (ASICs)",
      "length": 453
    },
    {
      "footnote_id": "fn-tops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 219,
      "definition": "**TOPS (Tera Operations Per Second)**: Measurement of computing performance equal to one trillion (10^12) operations per second, primarily used for AI accelerators. Modern chips range from smartphone neural engines (11 TOPS for Apple M1) to data center accelerators (756 TOPS for NVIDIA H100) [@nvidia2022h100]. However, TOPS varies dramatically by operation type\u2014integer operations are much faster than floating-point, making TOPS comparisons meaningful only within the same precision and operation type.",
      "term": "TOPS (Tera Operations Per Second)",
      "length": 505
    },
    {
      "footnote_id": "fn-heterogeneous",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 300,
      "definition": "**Heterogeneous Computing**: Computing systems that combine different types of processors (CPUs, GPUs, TPUs, FPGAs) to optimize performance for diverse workloads. Modern data centers mix x86 CPUs for control tasks, GPUs for training, and TPUs for inference. Programming heterogeneous systems requires frameworks like OpenCL or CUDA that can coordinate execution across different architectures, but offers 10-100x efficiency gains by matching each task to optimal hardware.",
      "term": "Heterogeneous Computing",
      "length": 472
    },
    {
      "footnote_id": "fn-risc-v-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 393,
      "definition": "**RISC-V for AI**: RISC-V, the open-source instruction set architecture from UC Berkeley (2010), is becoming important for AI accelerators because it's freely customizable. Companies like SiFive and Google have created RISC-V chips with custom AI extensions. Unlike proprietary architectures, RISC-V allows hardware designers to add specialized ML instructions without licensing fees, potentially democratizing AI hardware development beyond the current duopoly of x86 and ARM.",
      "term": "RISC-V for AI",
      "length": 477
    },
    {
      "footnote_id": "fn-cray-vector",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 437,
      "definition": "**Cray-1 Vector Legacy**: The Cray-1 (1975) cost $8.8 million ($50 million today) but could perform 160 million floating-point operations per second\u20141000x faster than typical computers. Its 64-element vector registers and pipelined vector units established the architectural template that modern AI accelerators still follow: process many data elements simultaneously with specialized hardware pipelines.",
      "term": "Cray-1 Vector Legacy",
      "length": 404
    },
    {
      "footnote_id": "fn-simd-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 670,
      "definition": "**SIMD Evolution**: SIMD originated in Flynn's 1966 taxonomy for scientific computing, but neural networks transformed it from a niche HPC concept to mainstream necessity. Modern CPUs have 512-bit SIMD units (AVX-512), but AI pushed development of SIMT (Single Instruction, Multiple Thread) where thousands of lightweight threads execute in parallel\u2014GPU architectures now coordinate 65,536+ threads simultaneously, impossible with traditional SIMD.",
      "term": "SIMD Evolution",
      "length": 448
    },
    {
      "footnote_id": "fn-hwacc-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 712,
      "definition": "**Tensor Core Breakthrough**: NVIDIA introduced tensor cores in the V100 (2017) to accelerate the 4x4 matrix operations common in neural networks. The A100's third-generation tensor cores achieve 312 TFLOPS for FP16 tensor operations\u201420x faster than traditional CUDA cores. This single innovation enabled training of models like GPT-3 that would have been impossible with conventional hardware, fundamentally changing the scale of AI research.",
      "term": "Tensor Core Breakthrough",
      "length": 443
    },
    {
      "footnote_id": "fn-hwacc-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 730,
      "definition": "**Apple's Neural Engine Strategy**: Apple introduced the Neural Engine in 2017's A11 chip to enable on-device ML without draining battery life. The M1's 16-core Neural Engine delivers 11 TOPS while the entire M1 chip has a 20-watt system TDP\u2014enabling real-time features like live text recognition and voice processing without cloud connectivity. This \"privacy through hardware\" approach influenced the entire industry to prioritize edge AI capabilities.",
      "term": "Apple's Neural Engine Strategy",
      "length": 453
    },
    {
      "footnote_id": "fn-systolic-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 763,
      "definition": "**Systolic Array Renaissance**: H.T. Kung and Charles Leiserson introduced systolic arrays at CMU in 1979 for VLSI signal processing, but the concept languished for decades due to programming complexity. Google's 2016 TPU resurrection proved these \"heartbeat\" architectures could deliver massive efficiency gains for neural networks\u2014the TPUv1's 256x256 systolic array achieved 92 TOPS while consuming just 40 watts, making systolic arrays the dominant AI architecture today.",
      "term": "Systolic Array Renaissance",
      "length": 474
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 958,
      "definition": "**FLOPS (Floating-Point Operations Per Second)**: Standard measure of computing performance for scientific and AI workloads, with TFLOPS = trillion FLOPS. The world's fastest supercomputer (Frontier) achieves 1.1 exaFLOPS (10^18), while a smartphone typically delivers 1-10 GFLOPS. However, modern AI hardware often favors integer operations (TOPS) over floating-point (FLOPS) because neural networks can use lower precision without accuracy loss.",
      "term": "FLOPS (Floating-Point Operations Per Second)",
      "length": 447
    },
    {
      "footnote_id": "fn-chiplet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2660,
      "definition": "**Chiplet**: Small, specialized semiconductor dies that are connected together within a single package to create larger, more complex processors. AMD's EPYC processors use up to 8 chiplets connected via Infinity Fabric, achieving yields above 80% versus 20% for equivalent monolithic designs. This modular approach reduces manufacturing costs and enables mixing different technologies\u2014compute chiplets in 7nm with I/O chiplets in 14nm\u2014optimizing each function independently.",
      "term": "Chiplet",
      "length": 474
    },
    {
      "footnote_id": "fn-hbm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2662,
      "definition": "**High-Bandwidth Memory (HBM)**: Advanced DRAM technology that stacks multiple memory dies vertically with thousands of connections, achieving 1,000+ GB/s bandwidth versus 50 GB/s for traditional GDDR. HBM2e can deliver 3.2 TB/s for NVIDIA H100 GPUs [@nvidia2022h100], but costs 5-10x more than GDDR and consumes significant power. This extreme bandwidth is essential for AI workloads where memory access, not computation, often limits performance.",
      "term": "High-Bandwidth Memory (HBM)",
      "length": 448
    },
    {
      "footnote_id": "fn-memory-coherence",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2668,
      "definition": "**Memory Coherence**: Ensuring all processors in a system see the same consistent view of shared memory when multiple cores/chips access the same data. Traditional cache coherence protocols like MESI add 10-50ns latency for multi-core CPUs. For AI accelerators with thousands of cores, coherence becomes prohibitively expensive\u2014most ML hardware instead uses explicit memory management where programmers control data placement and synchronization manually.",
      "term": "Memory Coherence",
      "length": 455
    },
    {
      "footnote_id": "fn-wafer-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 2972,
      "definition": "**Wafer-Scale Integration**: Using an entire 300mm silicon wafer as a single processor instead of cutting it into individual chips. Cerebras WSE-3 contains 4 trillion transistors across 850,000 cores\u2014125x more than the largest GPUs. Manufacturing challenges include 100% yield requirements (solved with redundant cores) and cooling 23kW of power. This approach eliminates inter-chip communication delays but costs $2-3 million per wafer versus $40,000 for equivalent GPU clusters.",
      "term": "Wafer-Scale Integration",
      "length": 480
    },
    {
      "footnote_id": "fn-electrical-grids",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 48,
      "definition": "**Electrical Grids**: Interconnected networks of power generation, transmission, and distribution infrastructure serving millions of customers. Modern smart grids use AI to predict demand patterns, prevent blackouts, and integrate renewable energy sources like solar and wind. For example, Google's DeepMind reduced cooling costs in data centers by 40% through AI-optimized power management, demonstrating how machine learning can dramatically improve energy efficiency at scale.",
      "term": "Electrical Grids",
      "length": 479
    },
    {
      "footnote_id": "fn-iot-networks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 50,
      "definition": "**IoT (Internet of Things) Networks**: Networks connecting billions of smart devices\u2014from sensors and cameras to appliances and vehicles\u2014that collect and exchange data automatically. By 2025, an estimated 75 billion IoT devices will generate over 79 zettabytes of data annually. AI processes this massive data stream to enable smart cities, autonomous vehicles, and predictive maintenance systems.",
      "term": "IoT (Internet of Things) Networks",
      "length": 397
    },
    {
      "footnote_id": "fn-particle-accelerators",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 52,
      "definition": "**Particle Accelerators**: Scientific instruments that accelerate subatomic particles to near light speed for physics research. The Large Hadron Collider (LHC) at CERN generates 50 petabytes of data annually\u2014equivalent to 50 million gigabytes\u2014requiring AI to identify rare particle collision signatures among billions of events. Without machine learning, discovering particles like the Higgs boson would be computationally impossible.",
      "term": "Particle Accelerators",
      "length": 434
    },
    {
      "footnote_id": "fn-superhuman-capabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 62,
      "definition": "**Superhuman AI Capabilities**: AI systems already exceed human performance in specific domains. AlphaGo defeated the world champion in Go (a game with more possible positions than atoms in the observable universe), protein folding prediction models like AlphaFold achieved accuracy that would take human scientists decades to match, and modern language models can process and synthesize information from millions of documents in seconds.",
      "term": "Superhuman AI Capabilities",
      "length": 438
    },
    {
      "footnote_id": "fn-paradigm-shift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 92,
      "definition": "**Paradigm Shift**: A term coined by philosopher Thomas Kuhn in 1962 [@kuhn1962structure] to describe fundamental changes in scientific approach. In AI, the key paradigm shift was moving from symbolic reasoning (encoding human knowledge as rules) to statistical learning (discovering patterns from data). This shift required abandoning decades of established methods and embracing radically different approaches to creating intelligence.",
      "term": "Paradigm Shift",
      "length": 437
    },
    {
      "footnote_id": "fn-early",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 100,
      "definition": "**Perceptron**: One of the first computational learning algorithms. This system could learn to classify patterns by making yes/no decisions based on inputs.",
      "term": "Perceptron",
      "length": 156
    },
    {
      "footnote_id": "fn-mainframes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 102,
      "definition": "**Mainframes**: Room-sized computers that dominated the 1960s-70s, typically costing millions of dollars and requiring dedicated cooling systems. IBM's System/360 mainframe from 1964 weighed 20,000 pounds and had just 64KB of memory, about 1/millionth the memory of a modern smartphone, yet represented the cutting edge of computing power that enabled early AI research.",
      "term": "Mainframes",
      "length": 370
    },
    {
      "footnote_id": "fn-eliza",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 104,
      "definition": "**ELIZA**: Created by MIT's Joseph Weizenbaum in 1966 [@weizenbaum1966eliza], ELIZA was one of the first chatbots that could simulate human conversation by pattern matching and substitution. Ironically, Weizenbaum was horrified when people began forming emotional attachments to his simple program, leading him to become a critic of AI.",
      "term": "ELIZA",
      "length": 336
    },
    {
      "footnote_id": "fn-dartmouth-conference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 291,
      "definition": "**Dartmouth Conference (1956)**: The legendary 8-week workshop at Dartmouth College where AI was officially born. Organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, it was the first time researchers gathered specifically to discuss \"artificial intelligence,\" a term McCarthy coined for the proposal. The ambitious goal was to make machines \"simulate every aspect of learning or any other feature of intelligence.\" Though overly optimistic, this gathering launched AI as a formal research field.",
      "term": "Dartmouth Conference (1956)",
      "length": 527
    },
    {
      "footnote_id": "fn-brittleness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 311,
      "definition": "**Brittleness in AI Systems**: The tendency of rule-based systems to fail completely when encountering inputs that fall outside their programmed scenarios, no matter how similar those inputs might be to what they were designed to handle. This contrasts with human intelligence, which can adapt and make reasonable guesses even in unfamiliar situations. The brittleness problem drove researchers toward machine learning approaches that could generalize from examples rather than relying on exhaustive rule sets.",
      "term": "Brittleness in AI Systems",
      "length": 510
    },
    {
      "footnote_id": "fn-mooreslaw",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 351,
      "definition": "**Moore's Law**: The observation made by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years, while the cost halves. This exponential growth in computing power has been a key driver of advances in machine learning, though the pace has begun to slow in recent years.",
      "term": "Moore's Law",
      "length": 335
    },
    {
      "footnote_id": "fn-viola-jones",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 419,
      "definition": "**Viola-Jones Algorithm**: A groundbreaking computer vision algorithm that could detect faces in real-time by using simple rectangular patterns (like comparing the brightness of eye regions versus cheek regions) and making decisions in stages, filtering out non-faces quickly and spending more computation only on promising candidates.",
      "term": "Viola-Jones Algorithm",
      "length": 335
    },
    {
      "footnote_id": "fn-cascade",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 421,
      "definition": "**Cascade of Classifiers**: A multi-stage decision system where each stage acts as a filter, quickly rejecting obvious non-matches and passing promising candidates to the next, more sophisticated stage. This approach is similar to how security screening works at airports with multiple checkpoints of increasing thoroughness.",
      "term": "Cascade of Classifiers",
      "length": 325
    },
    {
      "footnote_id": "fn-neurons",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 427,
      "definition": "**Artificial Neurons**: Basic computational units in neural networks that mimic biological neurons, taking multiple inputs, applying weights and biases, and producing an output signal through an activation function.",
      "term": "Artificial Neurons",
      "length": 215
    },
    {
      "footnote_id": "fn-intro-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 435,
      "definition": "**ImageNet**: A massive visual database containing over 14 million labeled images across 20,000+ categories, created by Stanford's Fei-Fei Li starting in 2009 [@deng2009imagenet]. The annual ImageNet challenge became the Olympics of computer vision, driving breakthrough after breakthrough in image recognition until neural networks became so good they essentially solved the competition.",
      "term": "ImageNet",
      "length": 388
    },
    {
      "footnote_id": "fn-intro-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 865,
      "definition": "**Foundation Models**: Large-scale AI models trained on broad datasets that serve as the \"foundation\" for many different applications through fine-tuning\u2014like GPT for language tasks or CLIP for vision tasks. The term was coined by Stanford's AI researchers in 2021 to capture how these models became the basis for building more specific AI systems.",
      "term": "Foundation Models",
      "length": 348
    },
    {
      "footnote_id": "fn-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 869,
      "definition": "**Parameters**: The adjustable values within a neural network that are modified during training, similar to how the brain's neural connections grow stronger as you learn a new skill. Having more parameters generally means that the model can learn more complex patterns.",
      "term": "Parameters",
      "length": 269
    },
    {
      "footnote_id": "fn-training-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 873,
      "definition": "**Large-Scale Training Challenges**: Training GPT-3 required approximately 3,640 petaflop-days. At $2-3 per GPU-hour on cloud platforms (2020 pricing), this translates to approximately $4.6M in compute costs alone (Lambda Labs estimate), excluding data preprocessing, experimentation, and failed training runs [@li2020estimating]. Rule of thumb: total project cost is typically 3-5x raw compute cost due to experimentation overhead, making the full GPT-3 development cost approximately $15-20M. Modern foundation models can consume 100+ terabytes of training data and require specialized distributed training techniques to coordinate thousands of accelerators across multiple data centers.",
      "term": "Large-Scale Training Challenges",
      "length": 689
    },
    {
      "footnote_id": "fn-backprop-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 877,
      "definition": "**Backpropagation (Historical Context)**: A mathematical technique that allows neural networks to learn by calculating how much each component contributed to errors and adjusting accordingly\u2014like a coach analyzing a team's mistakes and giving each player specific feedback to improve their performance.",
      "term": "Backpropagation (Historical Context)",
      "length": 302
    },
    {
      "footnote_id": "fn-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 879,
      "definition": "**Convolutional Neural Network (CNN)**: A type of neural network specially designed for processing images, inspired by how the human visual system works. The \"convolutional\" part refers to how it scans images in small chunks, similar to how our eyes focus on different parts of a scene.",
      "term": "Convolutional Neural Network (CNN)",
      "length": 286
    },
    {
      "footnote_id": "fn-sutton-turing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 897,
      "definition": "**Richard Sutton**: A foundational figure in reinforcement learning and AI, Sutton co-authored the seminal textbook \"Reinforcement Learning: An Introduction\" and developed key algorithms like temporal difference learning. He received the 2024 Turing Award, computing's highest honor, for his pioneering contributions to reinforcement learning that have significantly shaped how AI systems learn and adapt. His perspective on AI's development carries particular weight given his decades of contributions to both the theoretical foundations and practical advancement of machine learning.",
      "term": "Richard Sutton",
      "length": 585
    },
    {
      "footnote_id": "fn-amdahls-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 907,
      "definition": "**Amdahl's Law**: Formulated by computer architect Gene Amdahl in 1967, this law quantifies the theoretical speedup of a program when only part of it can be parallelized. The speedup is limited by the sequential portion: if P is the fraction that can be parallelized, maximum speedup = 1/(1-P). For example, if 90% of a program can be parallelized, maximum speedup is 10x regardless of processor count. In ML systems, this explains why memory bandwidth and data movement often become the primary bottlenecks rather than compute capacity.",
      "term": "Amdahl's Law",
      "length": 537
    },
    {
      "footnote_id": "fn-petabytes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 909,
      "definition": "**Petabytes**: One million gigabytes, or enough storage for 500 billion pages of text. To put this in perspective, the entire printed collection of the U.S. Library of Congress is about 20 terabytes, so a petabyte could store 50 copies. Modern AI training datasets like Common Crawl contain multiple petabytes of web text, representing essentially the entire public internet's text content.",
      "term": "Petabytes",
      "length": 390
    },
    {
      "footnote_id": "fn-computer-engineering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 917,
      "definition": "**Computer Engineering**: This discipline emerged in the late 1960s when IBM System/360 and other complex computing systems required expertise that spanned both hardware and software. Before Computer Engineering, electrical engineers focused on circuits while computer scientists worked on algorithms, but no one specialized in the integration challenges. Today's Computer Engineering programs, established at schools like Case Western Reserve and Stanford in the 1970s, combine hardware design, software systems, and computer architecture\u2014laying the groundwork for what ML Systems Engineering is becoming today.",
      "term": "Computer Engineering",
      "length": 612
    },
    {
      "footnote_id": "fn-cuda",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1096,
      "definition": "**CUDA (Compute Unified Device Architecture)**: NVIDIA's parallel computing platform introduced in 2007 that transformed gaming graphics cards into general-purpose computing powerhouses. CUDA enabled developers to harness GPU's thousands of cores for AI computations, providing 10-100x speedups over traditional CPUs for machine learning tasks. This breakthrough made deep learning practically feasible, as training a neural network that would take months on a CPU could be completed in days on CUDA-enabled hardware.",
      "term": "CUDA (Compute Unified Device Architecture)",
      "length": 517
    },
    {
      "footnote_id": "fn-data-centers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1163,
      "definition": "**Data Centers**: Massive facilities housing thousands of servers, often consuming 100-300 megawatts of power\u2014equivalent to a small city. Google operates over 20 data centers globally, each one costing $1-2 billion to build. These facilities maintain temperatures of exactly 80\u00b0F (27\u00b0C) with backup power systems that can run for days, enabling the reliable operation of AI services used by billions of people worldwide.",
      "term": "Data Centers",
      "length": 420
    },
    {
      "footnote_id": "fn-microcontrollers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1167,
      "definition": "**Microcontrollers**: Tiny computers-on-a-chip costing under $1 each, with just kilobytes of memory\u2014about 1/millionth the memory of a smartphone. Popular chips like the Arduino Uno have only 32KB of storage and 2KB of RAM, yet can run simple AI models that classify sensor data, recognize voice commands, or detect movement patterns while consuming less power than a digital watch.",
      "term": "Microcontrollers",
      "length": 381
    },
    {
      "footnote_id": "fn-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1171,
      "definition": "**Latency**: The time delay between when a request is made and when a response is received. In ML systems, this is critical\u2014autonomous vehicles need <10ms latency for safety decisions, while voice assistants target <100ms for natural conversation. For comparison, sending data to a distant cloud server typically adds 50-100ms, which is why edge computing became essential for real-time AI applications.",
      "term": "Latency",
      "length": 403
    },
    {
      "footnote_id": "fn-distributed-systems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1183,
      "definition": "**Distributed Systems**: Computing systems where components located on networked computers communicate and coordinate their actions. In ML, this might mean training a single model across 1,000+ GPUs in different server racks, or deploying models across thousands of edge devices globally. The complexity comes from handling network failures, coordinating updates, ensuring data consistency, and managing the \"Two Generals' Problem\"\u2014confirming that all parts of the system agree on the current state.",
      "term": "Distributed Systems",
      "length": 499
    },
    {
      "footnote_id": "fn-mas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1201,
      "definition": "**Multi-Agent System**: A computational system where multiple intelligent agents interact within an environment, each pursuing their own objectives while potentially cooperating or competing with other agents.",
      "term": "Multi-Agent System",
      "length": 209
    },
    {
      "footnote_id": "fn-edge",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1213,
      "definition": "**Edge Processor**: A specialized computing device designed to perform AI computations close to where data is generated, optimized for low latency and energy efficiency rather than raw computing power.",
      "term": "Edge Processor",
      "length": 201
    },
    {
      "footnote_id": "fn-intro-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1241,
      "definition": "**Transfer Learning**: A machine learning technique where a model developed for one task is reused as the starting point for a model on a related task, significantly reducing the amount of training data and computation required\u2014particularly valuable in domains like agriculture where labeled data may be scarce.",
      "term": "Transfer Learning",
      "length": 311
    },
    {
      "footnote_id": "fn-intro-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1279,
      "definition": "**Tensor Processing Unit (TPU)**: A specialized AI accelerator chip designed by Google specifically for neural network machine learning, particularly efficient at matrix operations common in deep learning workloads.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 215
    },
    {
      "footnote_id": "fn-rnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1301,
      "definition": "**Sequential Neural Networks**: Neural network architectures designed to process data that occurs in sequences over time, such as predicting where a pedestrian will move next based on their previous movements. These networks maintain a form of \"memory\" of previous inputs to inform current decisions.",
      "term": "Sequential Neural Networks",
      "length": 300
    },
    {
      "footnote_id": "fn-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1330,
      "definition": "**Data Drift**: The gradual change in the statistical properties of the target variable (what the model is trying to predict) over time, which can degrade model performance if not properly monitored and addressed.",
      "term": "Data Drift",
      "length": 213
    },
    {
      "footnote_id": "fn-backprop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1336,
      "definition": "**Backpropagation**: The primary algorithm used to train neural networks, which calculates how each parameter in the network should be adjusted to minimize prediction errors by propagating error gradients backward through the network layers.",
      "term": "Backpropagation",
      "length": 241
    },
    {
      "footnote_id": "fn-transfer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1340,
      "definition": "**Transfer Learning**: A machine learning method where a model developed for one task is reused as the starting point for a model on a second task, significantly reducing the amount of training data and computation required.",
      "term": "Transfer Learning",
      "length": 224
    },
    {
      "footnote_id": "fn-black-box",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1358,
      "definition": "**Black Box**: A system where you can observe the inputs and outputs but cannot see or understand the internal workings, like how a radio receives signals and produces sound without most users understanding the electronics inside. In AI, this opacity becomes problematic when the system makes important decisions affecting people's lives.",
      "term": "Black Box",
      "length": 338
    },
    {
      "footnote_id": "fn-inference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1364,
      "definition": "**Inference Attack**: A technique where an adversary attempts to extract sensitive information about the training data by making careful queries to a trained model, exploiting patterns the model may have inadvertently memorized during training.",
      "term": "Inference Attack",
      "length": 244
    },
    {
      "footnote_id": "fn-data-centers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 51,
      "definition": "**Data Centers**: Modern hyperscale data centers can house hundreds of thousands of servers and consume 20-50 megawatts of power, equivalent to a small city. Google's data centers alone process over 189,000 searches per second globally as of 2025.",
      "term": "Data Centers",
      "length": 247
    },
    {
      "footnote_id": "fn-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 53,
      "definition": "**Latency vs Throughput**: Latency measures the time delay between input and output (critical for real-time applications), while throughput measures predictions processed per unit time (important for batch processing).",
      "term": "Latency vs Throughput",
      "length": 218
    },
    {
      "footnote_id": "fn-deployment-environments",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 59,
      "definition": "**Deployment Environments**: The physical and logical contexts where ML systems operate, from hyperscale data centers consuming megawatts to coin-cell powered sensors running for years. Each environment imposes distinct constraints that determine what models can run and how they must be optimized.",
      "term": "Deployment Environments",
      "length": 298
    },
    {
      "footnote_id": "fn-embedded-systems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 61,
      "definition": "**Embedded Systems**: Purpose-built computer systems integrated into larger devices, typically with real-time constraints and limited resources. Unlike general-purpose computers, embedded systems optimize for specific tasks. Automotive ECUs manage engine timing within microseconds, while smart thermostats operate for years on batteries.",
      "term": "Embedded Systems",
      "length": 338
    },
    {
      "footnote_id": "fn-mobile-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 63,
      "definition": "**Mobile Power Constraints**: Modern smartphones contain 5000-6000mAh batteries (~18-22Wh) but ML inference can consume 1-5W, reducing battery life significantly. Apple's Neural Engine and Google's Tensor chips were specifically designed to perform AI tasks at <1W power consumption.",
      "term": "Mobile Power Constraints",
      "length": 283
    },
    {
      "footnote_id": "fn-cost-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 242,
      "definition": "**ML Hardware Cost Spectrum**: The cost range spans 6 orders of magnitude, from $10 ESP32-CAM modules to $200K+ DGX A100 systems. This 20,000x cost difference reflects proportional differences in computational capability, enabling deployment across vastly different economic contexts and use cases.",
      "term": "ML Hardware Cost Spectrum",
      "length": 298
    },
    {
      "footnote_id": "fn-billion-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 305,
      "definition": "**Billion-Parameter Models**: GPT-3 contains 175 billion model components requiring 350GB of memory just for storage [@brown2020language]. GPT-4 is estimated at 1.8 trillion components. For comparison, the human brain has approximately 86 billion neurons with 100 trillion synaptic connections, suggesting AI models are approaching biological complexity.",
      "term": "Billion-Parameter Models",
      "length": 354
    },
    {
      "footnote_id": "fn-memory-bottleneck",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 307,
      "definition": "**Memory Bottleneck**: When the rate of data transfer from memory to processor becomes the limiting factor in computation. Large models require so many parameters that memory bandwidth, rather than computational capacity, determines performance.",
      "term": "Memory Bottleneck",
      "length": 245
    },
    {
      "footnote_id": "fn-aws-sagemaker",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 309,
      "definition": "**AWS SageMaker**: Amazon's ML platform launched in 2017, processing over 1 million model training jobs annually by 2023. Provides end-to-end ML workflow management, from data preparation to model deployment, with integrated Jupyter notebooks and automatic model tuning capabilities.",
      "term": "AWS SageMaker",
      "length": 283
    },
    {
      "footnote_id": "fn-inference-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 311,
      "definition": "**Cloud Inference Latency**: Network latency includes propagation delay (speed of light limits), routing delays, and processing time. Round-trip from California to Virginia takes minimum 80ms just for light travel. Adding internet routing, DNS lookup, and server processing typically results in 100-500ms total latency.",
      "term": "Cloud Inference Latency",
      "length": 319
    },
    {
      "footnote_id": "fn-jetson-ecosystem",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 315,
      "definition": "**NVIDIA Jetson Ecosystem**: Family of embedded computing boards designed for AI at the edge, from the Jetson Nano Developer Kit ~$99-149 (5W) to the $1,999 AGX Orin (60W). Used in over 1 million deployed robots, drones, and autonomous vehicles worldwide since launch in 2014.",
      "term": "NVIDIA Jetson Ecosystem",
      "length": 276
    },
    {
      "footnote_id": "fn-iot-ecosystems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 317,
      "definition": "**IoT Ecosystems**: Interconnected networks of smart devices, sensors, and gateways. A modern smart city might contain 1 million+ IoT devices per square kilometer, generating 2.5 quintillion bytes of data daily, making edge processing essential for real-time responses.",
      "term": "IoT Ecosystems",
      "length": 269
    },
    {
      "footnote_id": "fn-edge-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 319,
      "definition": "**Edge Latency Advantage**: Edge processing eliminates network round-trips, achieving <10ms response times for local inference. Industrial robots require <1ms control loops, autonomous vehicles need <10ms emergency responses. Both requirements are impossible with cloud processing but achievable with edge deployment.",
      "term": "Edge Latency Advantage",
      "length": 317
    },
    {
      "footnote_id": "fn-mobile-storage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 323,
      "definition": "**Mobile Storage Evolution**: iPhone storage grew from 4GB (2007) to 1TB (2023), a 250x increase in 16 years. However, ML models grew even faster: ResNet-50 (25MB, 2015) to modern language models (>1GB compressed), creating ongoing storage pressure despite hardware improvements.",
      "term": "Mobile Storage Evolution",
      "length": 279
    },
    {
      "footnote_id": "fn-memory-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 327,
      "definition": "**Memory Scale Comparison**: TinyML devices operate with 256KB-2MB memory versus smartphones with 12-24GB (48,000-96,000x difference) and cloud servers with 1TB+ (4,000,000x difference). Yet TinyML can still perform useful inference through specialized optimization techniques[^fn-tinyml-optimization].",
      "term": "Memory Scale Comparison",
      "length": 302
    },
    {
      "footnote_id": "fn-tinyml-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 329,
      "definition": "**TinyML Optimization**: Specialized techniques that dramatically reduce model size and computational requirements while maintaining accuracy. These techniques enable deployment on severely resource-constrained devices (detailed in @sec-model-optimizations).",
      "term": "TinyML Optimization",
      "length": 258
    },
    {
      "footnote_id": "fn-battery-life",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 331,
      "definition": "**Ultra-Long Battery Life**: TinyML enables 10+ year deployments on single batteries through duty cycling. Devices sleep 99.9% of the time, wake periodically for inference, then return to sleep. Average power consumption drops to 10-100 microwatts, making decade-long operation feasible on coin-cell batteries.",
      "term": "Ultra-Long Battery Life",
      "length": 310
    },
    {
      "footnote_id": "fn-cloud-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 431,
      "definition": "**Cloud Infrastructure Evolution**: Cloud computing for ML emerged from Amazon's decision in 2002 to treat their internal infrastructure as a service. AWS launched in 2006, followed by Google Cloud (2008) and Azure (2010). By 2024, global cloud infrastructure spending exceeded $250 billion annually.",
      "term": "Cloud Infrastructure Evolution",
      "length": 300
    },
    {
      "footnote_id": "fn-nlp-compute",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 433,
      "definition": "**NLP Computational Demands**: Modern language models like GPT-3 required 3,640 petaflop-days of compute for training\u2014equivalent to running 1,000 NVIDIA V100 GPUs continuously for 355 days [@strubell2019energy]. This computational scale drove the need for massive cloud infrastructure.",
      "term": "NLP Computational Demands",
      "length": 285
    },
    {
      "footnote_id": "fn-mlsys-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 512,
      "definition": "**Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for tensor operations, first used internally in 2015 for neural network inference. A single TPU v4 Pod contains 4,096 chips and delivers over 1 exaflop of compute power\u2014more than most supercomputers.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 276
    },
    {
      "footnote_id": "fn-hyperscale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 514,
      "definition": "**Hyperscale Data Centers**: These facilities contain 5,000+ servers and cover 10,000+ square feet. Microsoft's data centers span over 200 locations globally, with some individual facilities consuming enough electricity to power 80,000 homes.",
      "term": "Hyperscale Data Centers",
      "length": 242
    },
    {
      "footnote_id": "fn-ml-apis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 532,
      "definition": "**ML APIs**: Application Programming Interfaces that democratized AI by providing pre-trained models as web services. Google's Vision API launched in 2016, processing over 1 billion images monthly within two years\u2014enabling developers to add AI capabilities without ML expertise.",
      "term": "ML APIs",
      "length": 278
    },
    {
      "footnote_id": "fn-automl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 538,
      "definition": "**AutoML (Automated Machine Learning)**: Automated systems that democratize ML by handling model selection, hyperparameter tuning, and feature engineering. Google AutoML Vision achieved 93.9% accuracy on ImageNet with minimal human intervention, compared to months of expert work for similar results.",
      "term": "AutoML (Automated Machine Learning)",
      "length": 300
    },
    {
      "footnote_id": "fn-paas-pricing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 540,
      "definition": "**Pay-as-You-Go Pricing**: Revolutionary model where users pay only for actual compute time used, measured in GPU-hours or inference requests. Training a model might cost $50-500 on demand versus $50,000-500,000 to purchase equivalent hardware.",
      "term": "Pay-as-You-Go Pricing",
      "length": 244
    },
    {
      "footnote_id": "fn-gdpr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 582,
      "definition": "**GDPR (General Data Protection Regulation)**: European privacy law effective 2018, imposing fines up to \u20ac20 million or 4% of global revenue for violations. Forces ML systems to implement \"right to be forgotten\" and data processing transparency\u2014technically challenging for neural networks.",
      "term": "GDPR (General Data Protection Regulation)",
      "length": 289
    },
    {
      "footnote_id": "fn-hipaa",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 584,
      "definition": "**HIPAA (Health Insurance Portability and Accountability Act)**: US healthcare privacy law requiring strict data security measures. ML systems handling medical data must implement encryption, access controls, and audit trails\u2014adding 30-50% to development costs but enabling $150B+ healthcare AI market.",
      "term": "HIPAA (Health Insurance Portability and Accountability Act)",
      "length": 302
    },
    {
      "footnote_id": "fn-collaborative-filtering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 586,
      "definition": "**Collaborative Filtering**: Recommendation technique analyzing user behavior patterns to predict preferences. Netflix's algorithm processes 100+ billion data points daily, with collaborative filtering contributing to 80% of watched content and saving $1 billion annually in customer retention.",
      "term": "Collaborative Filtering",
      "length": 294
    },
    {
      "footnote_id": "fn-industrial-iot",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 602,
      "definition": "**Industrial IoT**: Manufacturing generates over 1 exabyte of data annually, but less than 1% is analyzed due to connectivity constraints. Edge ML enables real-time analysis, with predictive maintenance alone saving manufacturers $630 billion globally by 2025.",
      "term": "Industrial IoT",
      "length": 260
    },
    {
      "footnote_id": "fn-iot-hubs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 604,
      "definition": "**IoT Hubs**: Central connection points that aggregate data from multiple sensors before cloud transmission. A typical smart building might have 1 hub managing 100-1000 IoT sensors, reducing cloud traffic by 90% while enabling local decision-making.",
      "term": "IoT Hubs",
      "length": 249
    },
    {
      "footnote_id": "fn-iot-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 681,
      "definition": "**IoT Device Growth**: From 8.4 billion connected devices in 2017 to a projected 25.4 billion by 2030. Each device generates 2.5 quintillion bytes of data daily, making edge processing essential for bandwidth management.",
      "term": "IoT Device Growth",
      "length": 220
    },
    {
      "footnote_id": "fn-latency-critical",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 691,
      "definition": "**Latency-Critical Applications**: Autonomous vehicles require <10ms response times for emergency braking decisions. Industrial robotics needs <1ms for precision control. Cloud round-trip latency typically ranges from 50-200ms, making edge processing essential for safety-critical applications.",
      "term": "Latency-Critical Applications",
      "length": 294
    },
    {
      "footnote_id": "fn-edge-coordination",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 701,
      "definition": "**Edge Network Coordination**: For n edge devices, the number of potential communication paths is n(n-1)/2. A network of 1,000 devices has 499,500 possible connections to manage. Software-defined networking (SDN) and edge orchestration platforms like Kubernetes K3s help manage this complexity.",
      "term": "Edge Network Coordination",
      "length": 294
    },
    {
      "footnote_id": "fn-endpoint-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 703,
      "definition": "**Endpoint Device Constraints**: Typical edge devices have 1-8GB RAM and 2-32GB storage, versus cloud servers with 128-1024GB RAM and petabytes of storage. Processing power differs by 10-100x, necessitating specialized model compression techniques.",
      "term": "Endpoint Device Constraints",
      "length": 248
    },
    {
      "footnote_id": "fn-voice-recognition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 727,
      "definition": "**Voice Recognition Evolution**: Apple's Siri (2011) required cloud processing with 200-500ms latency. By 2017, on-device processing reduced latency to <50ms while improving privacy. Modern smartphones process 16kHz audio at 20-30ms latency using specialized neural engines.",
      "term": "Voice Recognition Evolution",
      "length": 274
    },
    {
      "footnote_id": "fn-industry-40",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 729,
      "definition": "**Industry 4.0**: Fourth industrial revolution integrating cyber-physical systems, IoT, and cloud computing into manufacturing. Expected to increase productivity by 20-30% and reduce costs by 15-25% globally, with Germany leading adoption (83% of manufacturers) followed by US (54%).",
      "term": "Industry 4.0",
      "length": 283
    },
    {
      "footnote_id": "fn-predictive-maintenance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 731,
      "definition": "**Predictive Maintenance**: ML-driven maintenance scheduling based on equipment condition rather than fixed intervals. Reduces unplanned downtime by 35-45% and maintenance costs by 20-25%. GE saves $1.5 billion annually using predictive analytics across its industrial equipment.",
      "term": "Predictive Maintenance",
      "length": 279
    },
    {
      "footnote_id": "fn-computational-photography",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 733,
      "definition": "**Computational Photography**: Combines multiple exposures and ML algorithms to enhance image quality. Google's Night Sight captures 15 frames in 6 seconds, using ML to align and merge them. Portrait mode uses depth estimation ML models to create professional-looking bokeh effects in real-time.",
      "term": "Computational Photography",
      "length": 295
    },
    {
      "footnote_id": "fn-mobile-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 815,
      "definition": "**Mobile System-on-Chip**: Modern flagship SoCs integrate CPU, GPU, NPU, and memory controllers on a single chip. Apple's A17 Pro contains 19 billion transistors in a 3nm process, while Snapdragon 8 Gen 3 delivers significant AI performance improvements over its predecessor.",
      "term": "Mobile System-on-Chip",
      "length": 275
    },
    {
      "footnote_id": "fn-npu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 817,
      "definition": "**Neural Processing Unit (NPU)**: Specialized processors optimized for neural network operations. Apple's Neural Engine (introduced in A11, 2017) performs 600 billion operations per second. Qualcomm's Hexagon NPU in flagship chips delivers up to 75 TOPS while consuming <1W.",
      "term": "Neural Processing Unit (NPU)",
      "length": 274
    },
    {
      "footnote_id": "fn-tflite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 821,
      "definition": "**TensorFlow Lite**: Google's mobile ML framework launched in 2017, designed to run models <100MB with <100ms inference time. Supports quantization to reduce model size by 75% while maintaining 95% accuracy. Used in over 4 billion devices worldwide.",
      "term": "TensorFlow Lite",
      "length": 249
    },
    {
      "footnote_id": "fn-coreml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 823,
      "definition": "**Core ML**: Apple's framework introduced in iOS 11 (2017), optimized for on-device inference. Supports models from 1KB to 1GB, with automatic optimization for Apple Silicon. Enables features like Live Text, which processes text in real-time using on-device OCR models.",
      "term": "Core ML",
      "length": 269
    },
    {
      "footnote_id": "fn-mlsys-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 825,
      "definition": "**Model Optimization**: Techniques that reduce model size and computational requirements while maintaining accuracy. These optimizations enable deployment on resource-constrained devices (detailed in @sec-model-optimizations).",
      "term": "Model Optimization",
      "length": 226
    },
    {
      "footnote_id": "fn-real-time-translation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 839,
      "definition": "**Real-Time Translation**: Google Translate app can translate conversations in 40+ languages offline using on-device neural networks. The offline models are 35-45MB each versus 2GB+ for cloud versions, achieving 90% of cloud accuracy while enabling instant translation without internet.",
      "term": "Real-Time Translation",
      "length": 286
    },
    {
      "footnote_id": "fn-face-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 841,
      "definition": "**Mobile Face Detection**: Apple's Face ID uses a 30,000-dot projector and neural networks to create 3D face maps in <2 seconds. The system processes biometric data entirely on-device using the Secure Enclave, making it practically impossible to extract face data even with physical device access.",
      "term": "Mobile Face Detection",
      "length": 297
    },
    {
      "footnote_id": "fn-portrait-mode",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 843,
      "definition": "**Portrait Mode Photography**: Uses dual cameras or LiDAR to create depth maps, then applies ML-based segmentation to separate subjects from backgrounds. iPhone's Portrait mode processes multiple exposures in real-time, achieving DSLR-quality depth-of-field effects that would require expensive lenses and professional editing.",
      "term": "Portrait Mode Photography",
      "length": 327
    },
    {
      "footnote_id": "fn-mobile-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 845,
      "definition": "**Mobile Device Constraints**: Flagship phones typically have 12-24GB RAM and 512GB-2TB storage, versus cloud servers with 256-2048GB RAM and unlimited storage. Mobile processors operate at 15-25W peak power compared to server CPUs at 200-400W.",
      "term": "Mobile Device Constraints",
      "length": 244
    },
    {
      "footnote_id": "fn-microcontrollers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 869,
      "definition": "**Microcontrollers**: Single-chip computers with integrated CPU, memory, and peripherals, typically operating at 1-100MHz with 32KB-2MB RAM. Arduino Uno uses an ATmega328P with 32KB flash and 2KB RAM, while ESP32 provides WiFi capability with 520KB RAM\u2014still thousands of times less than a smartphone.",
      "term": "Microcontrollers",
      "length": 301
    },
    {
      "footnote_id": "fn-energy-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 871,
      "definition": "**Energy Efficiency in TinyML**: Ultra-low power consumption enables deployment in remote locations. Modern ARM Cortex-M0+ microcontrollers consume <1\u00b5W in sleep mode and 100-300\u00b5W/MHz when active. Efficient ML inference can run for years on a single coin-cell battery.",
      "term": "Energy Efficiency in TinyML",
      "length": 269
    },
    {
      "footnote_id": "fn-coin-cell",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 873,
      "definition": "**Coin-Cell Batteries**: Small, round batteries (CR2032 being most common) providing 200-250mAh at 3V. When powering TinyML devices at 10-50mW average consumption, these batteries can operate devices for 1-5 years, enabling \"deploy-and-forget\" IoT applications.",
      "term": "Coin-Cell Batteries",
      "length": 261
    },
    {
      "footnote_id": "fn-on-device-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 952,
      "definition": "**On-Device Training Constraints**: Unlike mobile devices, microcontrollers rarely support full model training due to memory limitations. Instead, they use techniques like transfer learning, where a pre-trained model is fine-tuned with minimal on-device adaptation, or federated learning aggregation where multiple devices collaboratively train a shared model.",
      "term": "On-Device Training Constraints",
      "length": 360
    },
    {
      "footnote_id": "fn-device-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 956,
      "definition": "**TinyML Device Scale**: The smallest ML-capable devices measure just 5x5mm (Syntiant NDP chips). Google's Coral Dev Board Mini measures 40x48mm but includes WiFi and full Linux capability. The extreme miniaturization enables integration into previously \"dumb\" objects like smart dust sensors.",
      "term": "TinyML Device Scale",
      "length": 293
    },
    {
      "footnote_id": "fn-model-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 976,
      "definition": "**TinyML Model Optimization**: Specialized techniques that dramatically reduce model size and computational requirements. A typical smartphone model of 50MB might optimize to 250KB for microcontroller deployment while retaining 95% accuracy (techniques detailed in @sec-model-optimizations).",
      "term": "TinyML Model Optimization",
      "length": 291
    },
    {
      "footnote_id": "fn-fitness-trackers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 986,
      "definition": "**TinyML in Fitness Trackers**: Modern fitness trackers use TinyML for activity recognition, sleep analysis, and health monitoring. Apple Watch can detect falls using accelerometer data and on-device ML, automatically calling emergency services. The algorithm analyzes motion patterns in real-time using <1mW power.",
      "term": "TinyML in Fitness Trackers",
      "length": 315
    },
    {
      "footnote_id": "fn-train-serve-split",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 988,
      "definition": "**Train-Serve Split Economics**: Training large models can cost $1-10M (GPT-3: $4.6M) but inference costs <$0.01 per query when deployed efficiently [@brown2020language]. This 1,000,000x cost difference drives the pattern of expensive cloud training with cheap edge inference.",
      "term": "Train-Serve Split Economics",
      "length": 276
    },
    {
      "footnote_id": "fn-smart-glasses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 990,
      "definition": "**Smart Glasses with TinyML**: Google Glass Enterprise uses TinyML for real-time object recognition and barcode scanning. The glasses process visual data locally using specialized vision chips consuming <500mW, enabling 8+ hour operation while providing instant augmented reality overlays.",
      "term": "Smart Glasses with TinyML",
      "length": 289
    },
    {
      "footnote_id": "fn-federated-architecture",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 1037,
      "definition": "**Federated Learning Architecture**: Coordinates learning across millions of devices without centralizing data [@mcmahan2017federated]. Google's federated learning processes 6 billion mobile keyboards, training improved models while keeping all typed text local. Each round involves 100-10,000 devices contributing model updates.",
      "term": "Federated Learning Architecture",
      "length": 329
    },
    {
      "footnote_id": "fn-a11-bionic-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 49,
      "definition": "**A11 Bionic Breakthrough**: Apple's A11 Bionic (2017) was the first mobile chip with sufficient computational power for on-device training, delivering 0.6 TOPS compared to the previous A10's 0.2 TOPS. This 3x improvement, combined with 4.3 billion transistors and a dual-core Neural Engine, allowed gradient computation for the first time on mobile devices. Google's Pixel Visual Core achieved similar capabilities with 8 custom Image Processing Units optimized for machine learning workloads.",
      "term": "A11 Bionic Breakthrough",
      "length": 494
    },
    {
      "footnote_id": "fn-federated-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 67,
      "definition": "**Federated Learning Birth**: Google's Brendan McMahan coined \"federated learning\" in 2016 [@mcmahan2017communication], but the concept emerged from their Gboard team's frustration with keyboard personalization. They realized they needed user-specific data to improve predictions, but couldn't collect keystrokes due to privacy concerns. This led to the \"train where the data lives\" philosophy that defined federated learning.",
      "term": "Federated Learning Birth",
      "length": 426
    },
    {
      "footnote_id": "fn-gdpr-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 85,
      "definition": "**GDPR's ML Impact**: When GDPR took effect in May 2018 [@gdpr2016regulation], it made centralized ML training illegal for personal data without explicit consent. The \"right to be forgotten\" also meant models trained on personal data could be legally required to \"unlearn\" specific users\u2014technically impossible with traditional training. This drove massive investment in privacy-preserving ML techniques.",
      "term": "GDPR's ML Impact",
      "length": 404
    },
    {
      "footnote_id": "fn-non-iid",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 139,
      "definition": "**Non-IID (Non-Independent and Identically Distributed)**: In machine learning, data is IID when samples are drawn independently from the same distribution. Non-IID violates this assumption, common in federated learning where each device collects data from different users, environments, or use cases. For example, smartphone keyboard data varies dramatically between users (languages, writing styles, autocorrect needs), making personalized model training essential but challenging for convergence.",
      "term": "Non-IID (Non-Independent and Identically Distributed)",
      "length": 499
    },
    {
      "footnote_id": "fn-arduino-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 550,
      "definition": "**Arduino Edge Computing Reality**: The Arduino Nano 33 BLE Sense represents typical microcontroller constraints\u2014256KB SRAM is roughly 65,000 times smaller than a modern smartphone's 16GB RAM. To put this in perspective, storing just one 224\u00d7224\u00d73 RGB image (150KB) would consume 60% of available memory. Training requires 3-5x more memory for gradients and activations, making even tiny models challenging. The 1MB flash storage can hold only the smallest quantized models, forcing designers to use 8-bit or even 4-bit representations.",
      "term": "Arduino Edge Computing Reality",
      "length": 536
    },
    {
      "footnote_id": "fn-mobilenet-innovation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 560,
      "definition": "**MobileNet Innovation**: Google's MobileNet family revolutionized mobile AI by achieving 10-20x parameter reduction compared to traditional CNNs. MobileNetV1 (2017) used depthwise separable convolutions to reduce floating-point operations (FLOPs) by 8-9x, while MobileNetV2 (2018) added inverted residuals and linear bottlenecks. The breakthrough allowed real-time inference on smartphones: MobileNetV2 runs ImageNet classification in ~75ms on a Pixel phone versus 1.8 seconds for ResNet-50 [@he2016deep].",
      "term": "MobileNet Innovation",
      "length": 506
    },
    {
      "footnote_id": "fn-depthwise-separable",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 562,
      "definition": "**Depthwise Separable Convolutions**: This technique decomposes standard convolution into two operations: depthwise convolution (applies single filter per input channel) and pointwise convolution (1\u00d71 conv to combine channels). For a 3\u00d73 conv with 512 input/output channels, standard convolution requires 2.4M parameters while depthwise separable needs only 13.8K\u2014a 174x reduction. The computational savings are similarly dramatic, making real-time inference possible on mobile CPUs.",
      "term": "Depthwise Separable Convolutions",
      "length": 483
    },
    {
      "footnote_id": "fn-quantization-aware",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 588,
      "definition": "**Quantization-Aware Training**: Unlike post-training quantization which converts trained FP32 models to INT8, quantization-aware training simulates low-precision arithmetic during training itself. This allows the model to learn robust representations despite reduced precision. Critical for edge devices where INT8 operations consume 4x less power and enable 4x faster inference compared to FP32, while maintaining 95-99% of original accuracy.",
      "term": "Quantization-Aware Training",
      "length": 444
    },
    {
      "footnote_id": "fn-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 590,
      "definition": "**Stochastic Gradient Descent (SGD)**: The fundamental optimization algorithm for neural networks, updating parameters using gradients computed on small batches (or single samples). Unlike full-batch gradient descent, SGD's randomness helps escape local minima while requiring minimal memory\u2014storing only current parameters and gradients. This simplicity makes SGD ideal for microcontrollers where advanced optimizers like Adam would exceed memory budgets.",
      "term": "Stochastic Gradient Descent (SGD)",
      "length": 456
    },
    {
      "footnote_id": "fn-stm32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 592,
      "definition": "**STM32F4 Microcontroller Reality**: The STM32F4 represents the harsh reality of embedded computing\u2014192KB SRAM (roughly the size of a small JPEG image) and 1MB flash storage, running at 168MHz without floating-point hardware acceleration. Integer arithmetic is 10-100x slower than dedicated floating-point units found in mobile chips. Power consumption is ~100mW during active processing, requiring careful duty-cycling to preserve battery life. These constraints make even simple neural networks challenging\u2014a 10-neuron hidden layer requires ~40KB for weights alone in FP32.",
      "term": "STM32F4 Microcontroller Reality",
      "length": 575
    },
    {
      "footnote_id": "fn-esp32-capabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 594,
      "definition": "**ESP32 Edge Computing**: The ESP32 provides 520KB SRAM and dual-core processing at 240MHz, making it more capable than STM32F4 but still severely constrained. Its key advantage is built-in WiFi and Bluetooth for federated learning scenarios. However, the lack of hardware floating-point support means all ML operations must use integer quantization. Real-world deployments show 8-bit quantized models can achieve 95% of FP32 accuracy while fitting in ~50KB memory, enabling basic on-device training for simple tasks like sensor anomaly detection.",
      "term": "ESP32 Edge Computing",
      "length": 547
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 598,
      "definition": "**Mixed-Precision Training**: Uses different numerical precisions for different operations\u2014typically FP16 for forward/backward passes and FP32 for parameter updates. This halves memory usage and doubles throughput on modern hardware with Tensor Cores, while maintaining training stability through automatic loss scaling. Mobile implementations often use INT8 for inference and FP16 for gradient computation, balancing accuracy with hardware constraints.",
      "term": "Mixed-Precision Training",
      "length": 453
    },
    {
      "footnote_id": "fn-transformer-mobile",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 600,
      "definition": "**Lightweight Transformers**: Mobile-optimized transformer architectures like MobileBERT [@sun2020mobilebert] and DistilBERT [@sanh2019distilbert] achieve 4-6x speedup over full models through techniques like knowledge distillation, layer reduction, and attention head pruning. MobileBERT retains 97% of BERT-base accuracy while running inference in ~40ms on mobile CPUs versus 160ms for full BERT. Key optimizations include bottleneck attention mechanisms and specialized mobile-friendly layer configurations.",
      "term": "Lightweight Transformers",
      "length": 510
    },
    {
      "footnote_id": "fn-ondevice-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 602,
      "definition": "**Apple Neural Engine Evolution**: Apple's Neural Engine has evolved dramatically since the A11 Bionic. The A17 Pro (2023) features a 16-core Neural Engine delivering 35 TOPS\u2014roughly equivalent to an NVIDIA GTX 1080 Ti. This represents a 58x improvement over the original A11. The Neural Engine specializes in matrix operations with dedicated 8-bit and 16-bit arithmetic units, enabling efficient on-device training. Real-world performance: fine-tuning a MobileNet classifier takes ~2 seconds versus 45 seconds on CPU alone, while consuming only ~500mW additional power.",
      "term": "Apple Neural Engine Evolution",
      "length": 570
    },
    {
      "footnote_id": "fn-tensor-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 604,
      "definition": "**Google Tensor SoC Architecture**: Google's Tensor chips (starting with Pixel 6 in 2021) feature a custom TPU v1-derived Edge TPU optimized for ML workloads. Unlike Apple's Neural Engine, Tensor optimizes for Google's specific models (speech recognition, computational photography). The TPU provides efficient 8-bit integer operations while consuming only 2W, making it highly efficient for federated learning scenarios where devices train locally on speech or image data.",
      "term": "Google Tensor SoC Architecture",
      "length": 473
    },
    {
      "footnote_id": "fn-near-memory-compute",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 610,
      "definition": "**Near-Memory Computing**: Places processing units directly adjacent to or within memory arrays, dramatically reducing data movement costs. Traditional von Neumann architectures spend 100-1000x more energy moving data than computing on it. Near-memory designs can perform matrix operations with 10-100x better energy efficiency by eliminating costly memory bus transfers. Critical for edge training where gradient computations require intensive memory access patterns that overwhelm traditional cache hierarchies.",
      "term": "Near-Memory Computing",
      "length": 513
    },
    {
      "footnote_id": "fn-dvfs-mobile",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 630,
      "definition": "**Dynamic Voltage and Frequency Scaling (DVFS)**: Modern mobile processors continuously adjust operating voltage and clock frequency based on workload and thermal conditions. During ML training, DVFS can reduce clock speeds by 30-50% when temperature exceeds 70\u00b0C, directly impacting training throughput. Effective on-device learning systems monitor thermal state and proactively reduce batch sizes or training intensity to maintain consistent performance rather than experiencing sudden throttling events.",
      "term": "Dynamic Voltage and Frequency Scaling (DVFS)",
      "length": 506
    },
    {
      "footnote_id": "fn-gradient-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 648,
      "definition": "**Gradient Checkpointing**: A memory optimization technique that trades computation for memory by recomputing intermediate activations during the backward pass instead of storing them. This can reduce memory requirements by 50-80% at the cost of 20-30% additional computation. Particularly valuable for on-device training where memory is more constrained than compute capacity, enabling training of larger models within fixed memory budgets.",
      "term": "Gradient Checkpointing",
      "length": 441
    },
    {
      "footnote_id": "fn-tinytl-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1046,
      "definition": "**TinyTL Memory Breakthrough**: TinyTL's 60x parameter reduction (3M to 50K) translates to dramatic memory savings. In FP32, MobileNetV2 requires ~12MB for weights plus ~8MB for activation caching during training\u2014exceeding most microcontroller capabilities. TinyTL reduces this to ~200KB weights plus ~400KB activations, fitting comfortably within a 1MB memory budget. Real deployments on STM32H7 achieve 85% of full fine-tuning accuracy while using 15x less memory and completing updates in ~30 seconds versus 8 minutes for full training.",
      "term": "TinyTL Memory Breakthrough",
      "length": 539
    },
    {
      "footnote_id": "fn-lora",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1085,
      "definition": "**LoRA (Low-Rank Adaptation)**: Introduced by Microsoft in 2021, LoRA enables efficient fine-tuning by learning low-rank decomposition matrices rather than updating full weight matrices. For a weight matrix W, LoRA learns rank-r matrices A and B such that the update is BA (where r << original dimensions). This reduces trainable parameters by 100-10000x while maintaining 90-95% adaptation quality. LoRA has become the standard for parameter-efficient fine-tuning in large language models.",
      "term": "LoRA (Low-Rank Adaptation)",
      "length": 490
    },
    {
      "footnote_id": "fn-hey-siri-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1252,
      "definition": "**\"Hey Siri\" Technical Reality**: Apple's \"Hey Siri\" system operates under extreme constraints\u2014detection must complete within 100ms to feel responsive, while consuming less than 1mW power when listening continuously. The always-on processor monitors audio using a 192KB model running at ~0.5 TOPS. False positive rate must be under 0.001% (less than once per day) while maintaining >95% true positive rate across accents, background noise, and speaking styles. The system processes 16kHz audio in 200ms windows, extracting Mel-frequency features for classification.",
      "term": "\"Hey Siri\" Technical Reality",
      "length": 565
    },
    {
      "footnote_id": "fn-tinyml-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1307,
      "definition": "**TinyML Market Reality**: The TinyML market reached $2.4 billion in 2023 and is projected to grow to $23.3 billion by 2030. Over 100 billion microcontrollers ship annually, but fewer than 1% currently support on-device learning due to memory and power constraints. Successful TinyML deployments typically consume <1mW power, use <256KB memory, and cost under $1 per chip. Applications include predictive maintenance (vibration sensors), health monitoring (heart rate variability), and smart agriculture (soil moisture prediction).",
      "term": "TinyML Market Reality",
      "length": 531
    },
    {
      "footnote_id": "fn-mfcc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1340,
      "definition": "**Mel-Frequency Cepstral Coefficients (MFCCs)**: Audio features that mimic human auditory perception by applying mel-scale frequency warping (emphasizing lower frequencies where speech information concentrates) followed by cepstral analysis. A typical MFCC extraction converts 16kHz audio windows into 12-13 coefficients, reducing a 320-sample window (20ms) from 640 bytes to ~50 bytes while preserving speech intelligibility. Widely used in speech recognition since the 1980s due to robustness against noise and computational efficiency. Instead of storing raw audio waveforms, which are large and computationally expensive to process, devices store and learn from these compressed feature vectors directly. Similarly, in low-power computer vision systems, embeddings extracted from lightweight CNNs are retained and reused for few-shot learning. These examples illustrate how representation learning and compression serve as foundational tools for scaling on-device learning to memory- and bandwidth-constrained environments.",
      "term": "Mel-Frequency Cepstral Coefficients (MFCCs)",
      "length": 1027
    },
    {
      "footnote_id": "fn-fedavg",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1653,
      "definition": "**Federated Averaging (FedAvg)**: Introduced by Google in 2017, FedAvg revolutionized distributed ML by averaging model weights rather than gradients. Each client performs multiple local SGD steps (typically 1-20) before sending weights to the server, reducing communication by 10-100x compared to distributed SGD. The key insight: local updates contain richer information than single gradients, enabling convergence with far fewer communication rounds. FedAvg powers production systems like Gboard, processing billions of devices. After a fixed number of local steps, each device sends its updated model parameters to the server. The server computes a weighted average of these parameters, which are weighted according to the number of data samples on each device, and updates the global model accordingly. This updated model is then sent back to the devices, completing one round of training.",
      "term": "Federated Averaging (FedAvg)",
      "length": 894
    },
    {
      "footnote_id": "fn-wireless-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1687,
      "definition": "**Wireless Communication Reality**: Mobile devices face severe bandwidth and energy constraints for federated learning. LTE uploads average 5-10 Mbps versus 50+ Mbps downloads, creating asymmetric bottlenecks. Transmitting a 50MB model update consumes ~100mAh battery (2-3% of typical capacity) and takes 40-80 seconds. WiFi improves throughput but isn't always available. Low-power devices using LoRaWAN or NB-IoT face even harsher limits\u2014LoRaWAN maxes at 50kbps with 1% duty cycle restrictions, making frequent updates impractical without aggressive compression.",
      "term": "Wireless Communication Reality",
      "length": 564
    },
    {
      "footnote_id": "fn-gradient-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1693,
      "definition": "**Gradient Quantization**: Reduces communication by converting FP32 gradients to lower precision (INT8, INT4, or even 1-bit). Advanced techniques like signSGD use only gradient signs, achieving 32x compression. Error compensation methods accumulate quantization errors for later transmission, maintaining convergence quality. Real deployments achieve 8-16x communication reduction with <1% accuracy loss.",
      "term": "Gradient Quantization",
      "length": 404
    },
    {
      "footnote_id": "fn-gradient-sparsification",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1695,
      "definition": "**Gradient Sparsification**: Transmits only the largest gradients by magnitude (typically top 1-10%), dramatically reducing communication. Gradient accumulation stores untransmitted gradients locally until they become large enough to send. This technique exploits the observation that most gradients are small and contribute minimally to convergence, achieving 10-100x compression ratios while maintaining training effectiveness. These techniques reduce transmission size with limited impact on convergence when applied carefully.",
      "term": "Gradient Sparsification",
      "length": 530
    },
    {
      "footnote_id": "fn-differential-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1764,
      "definition": "**Differential Privacy**: Provides mathematical privacy guarantees by adding calibrated noise to model updates, ensuring that participation by any individual cannot be distinguished. Developed by Dwork in 2006, it's now the gold standard for privacy-preserving ML. The privacy budget \u03b5 (epsilon) controls the privacy-utility tradeoff: smaller \u03b5 means stronger privacy but noisier results. Apple uses local differential privacy with \u03b5=4-14 for iOS telemetry, while Google's federated learning deployments use \u03b5\u224810.",
      "term": "Differential Privacy",
      "length": 513
    },
    {
      "footnote_id": "fn-fedasync",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1810,
      "definition": "**Asynchronous Federated Learning (FedAsync)**: Enables continuous model updates without waiting for slow or unreliable clients. The server maintains a global model that gets updated immediately when client contributions arrive, using staleness-aware weighting to reduce the influence of outdated updates. This approach can improve convergence speed by 2-5x in heterogeneous environments while maintaining model quality within 1-3% of synchronous training.",
      "term": "Asynchronous Federated Learning (FedAsync)",
      "length": 456
    },
    {
      "footnote_id": "fn-arm-cortex-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1959,
      "definition": "**ARM Cortex Architecture Spectrum**: The ARM Cortex family spans 6 orders of magnitude in capabilities. Cortex-M0+ (IoT sensors) runs at 48MHz with 32KB RAM and no floating-point, consuming ~10\u00b5W. Cortex-M7 (embedded systems) reaches 400MHz with 1MB RAM and single-precision FPU, consuming ~100mW. Cortex-A78 (smartphones) delivers 3GHz performance with multi-core processing, NEON SIMD, and advanced branch prediction, consuming 1-5W. This diversity means federated learning must adapt algorithms dynamically\u2014quantized inference on M0+, lightweight training on M7, and full backpropagation on A78.",
      "term": "ARM Cortex Architecture Spectrum",
      "length": 599
    },
    {
      "footnote_id": "fn-tflite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1989,
      "definition": "**TensorFlow Lite**: Google's framework for mobile and embedded ML inference, optimized for ARM processors and mobile GPUs. TFLite reduces model size by 75% through quantization and pruning, while achieving 3x faster inference than full TensorFlow. The framework supports 16-bit and 8-bit quantization, with specialized kernels for mobile CPUs and GPUs. TFLite Micro targets microcontrollers with <1MB memory, enabling ML on Arduino and other embedded platforms.",
      "term": "TensorFlow Lite",
      "length": 462
    },
    {
      "footnote_id": "fn-microcontroller-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 2009,
      "definition": "**Microcontroller Power Budget Reality**: A typical microcontroller consuming 100mW during training exhausts 3.6 joules per hour, equivalent to a 1000mAh battery in 2.8 hours. Energy harvesting systems collect only 10-100mW continuously (solar panels in indoor light), making sustained training impossible. Real deployments use duty cycling: train for 10 seconds every hour, consuming ~1 joule total. This constrains training to 100-1000 gradient steps maximum, requiring extremely efficient algorithms and careful energy budgeting between sensing, computation, and communication.",
      "term": "Microcontroller Power Budget Reality",
      "length": 580
    },
    {
      "footnote_id": "fn-activation-caching",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 2013,
      "definition": "**Activation Caching**: During backpropagation, forward pass activations must be stored to compute gradients, dramatically increasing memory usage. For a typical CNN, activation memory can be 3-5x larger than model weights. Modern techniques like gradient checkpointing trade computation for memory by recomputing activations during backward pass, reducing memory by 80% at the cost of ~30% more compute time. Critical for training on memory-constrained devices where activation storage often exceeds available RAM. These requirements may exceed the static memory footprint anticipated during model deployment, particularly when adaptation involves multiple layers or gradient accumulation. In highly constrained systems, for example, systems with less than 512 KB of RAM, this may preclude certain types of adaptation altogether, unless additional optimization techniques (e.g., checkpointing or low-rank updates) are employed.",
      "term": "Activation Caching",
      "length": 928
    },
    {
      "footnote_id": "fn-few-shot-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 2109,
      "definition": "**Few-Shot Learning**: Machine learning paradigm that learns new concepts from only a few (typically 1-10) labeled examples. Originally inspired by human learning capabilities\u2014humans can recognize new objects from just one or two examples. In ML, few-shot learning leverages pre-trained representations and meta-learning to quickly adapt to new tasks. Critical for on-device scenarios where collecting large labeled datasets is impractical. Techniques include prototypical networks, model-agnostic meta-learning (MAML), and metric learning approaches that achieve 80-90% accuracy with just 5 examples per class. Federated learning emerges as a crucial coordination mechanism, allowing devices to collaborate while maintaining data locality and privacy guarantees.",
      "term": "Few-Shot Learning",
      "length": 763
    },
    {
      "footnote_id": "fn-mlops-emergence",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 47,
      "definition": "**MLOps Emergence**: While machine learning operations challenges were identified earlier by D. Sculley and colleagues at Google in their influential 2015 paper \"Hidden Technical Debt in Machine Learning Systems\" [@sculley2015hidden], the term \"MLOps\" itself was coined around 2018 as the discipline matured. The field emerged as organizations like Netflix, Uber, and Airbnb faced the \"last mile\" problem, where approximately 90% of ML models never made it to production according to industry surveys and anecdotal reports due to operational challenges.",
      "term": "MLOps Emergence",
      "length": 553
    },
    {
      "footnote_id": "fn-devops-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 53,
      "definition": "**DevOps Origins**: The \"wall of confusion\" between development and operations teams was so notorious that Patrick Debois called his 2009 conference \"DevOpsDays\" specifically to bridge this gap. The movement emerged from the frustrations of the \"throw it over the wall\" mentality where developers built software in isolation from operations teams who had to deploy and maintain it.",
      "term": "DevOps Origins",
      "length": 381
    },
    {
      "footnote_id": "fn-mlops-business-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 61,
      "definition": "**MLOps Business Impact**: Companies implementing mature MLOps practices report significant improvements in deployment speed (reducing time from months to weeks), substantial reductions in model debugging time, and improved model reliability. Organizations with mature MLOps practices consistently achieve higher model success rates moving from pilot to production compared to those using ad hoc approaches.",
      "term": "MLOps Business Impact",
      "length": 407
    },
    {
      "footnote_id": "fn-infrastructure-as-code",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 79,
      "definition": "**Infrastructure as Code**: The concept emerged from the painful lessons of \"snowflake servers\", unique, manually-configured systems that were impossible to reproduce. Luke Kanies created Puppet in 2005 after experiencing the nightmare of managing hundreds of custom-configured servers at various startups.",
      "term": "Infrastructure as Code",
      "length": 306
    },
    {
      "footnote_id": "fn-jenkins-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 83,
      "definition": "**Jenkins Origins**: Originally called \"Hudson,\" Jenkins was created by Kohsuke Kawaguchi at Sun Microsystems in 2004 to automate his own tedious testing processes. The name change to \"Jenkins\" came in 2011 after a trademark dispute, named after the devoted butler from P.G. Wodehouse's stories.",
      "term": "Jenkins Origins",
      "length": 295
    },
    {
      "footnote_id": "fn-kubernetes-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 85,
      "definition": "**Kubernetes Origins**: Greek for \"helmsman,\" Kubernetes emerged from Google's internal Borg system that managed billions of containers across their data centers. Google open-sourced it in 2014, realizing that their competitive advantage wasn't the orchestration system itself, but how they used it to run services at planetary scale.",
      "term": "Kubernetes Origins",
      "length": 334
    },
    {
      "footnote_id": "fn-data-drift-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 101,
      "definition": "**Data Drift Discovery**: The concept was first formalized by researchers studying spam detection systems in the early 2000s, who noticed that spam patterns evolved so rapidly that models became obsolete within weeks. This led to the realization that ML systems face a fundamentally different challenge than traditional software: their environment actively adapts to defeat them.",
      "term": "Data Drift Discovery",
      "length": 379
    },
    {
      "footnote_id": "fn-reproducibility-crisis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 105,
      "definition": "**ML Reproducibility Crisis**: A 2016 study by Collberg and Proebsting found that only 54% of computer systems research papers could be reproduced even when authors were available to assist [@collberg2016repeatability]. This reproducibility challenge is even more acute in ML research, though the situation has improved with initiatives like Papers with Code and requirements for code submission at major ML conferences.",
      "term": "ML Reproducibility Crisis",
      "length": 420
    },
    {
      "footnote_id": "fn-dvc-story",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 151,
      "definition": "**DVC Creation Story**: Data Version Control was born from the frustration of Dmitry Petrov, who spent weeks trying to reproduce an experiment only to discover the training data had been quietly updated. He created DVC in 2017 to bring Git-like versioning to data science, solving what he called \"the biggest unsolved problem in machine learning.\"",
      "term": "DVC Creation Story",
      "length": 347
    },
    {
      "footnote_id": "fn-feature-store-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 282,
      "definition": "**Feature Store Scale**: Uber's Michelangelo feature store serves 10+ million features per second with P99 latency under 10ms using optimized, co-located serving infrastructure, storing 200+ petabytes of feature data. Airbnb's feature store supports 1,000+ ML models with automated feature validation preventing 85% of potential training-serving skew issues.",
      "term": "Feature Store Scale",
      "length": 358
    },
    {
      "footnote_id": "fn-training-serving-skew",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 284,
      "definition": "**Training-Serving Skew Impact**: Studies show training-serving skew causes 5-15% accuracy degradation in production models. Google reported that fixing skew issues improved ad click prediction accuracy by 8%, translating to millions in additional revenue annually.",
      "term": "Training-Serving Skew Impact",
      "length": 265
    },
    {
      "footnote_id": "fn-github-actions-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 320,
      "definition": "**GitHub Actions for ML**: Over 60% of ML teams now use GitHub Actions for CI/CD according to recent developer surveys, with typical ML pipelines taking 15-45 minutes to run (vs. 2-5 minutes for traditional software). Netflix runs 10,000+ ML pipeline executions weekly through GitHub Actions, with 95% success rate on first run.",
      "term": "GitHub Actions for ML",
      "length": 328
    },
    {
      "footnote_id": "fn-kubeflow-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 322,
      "definition": "**Kubeflow Production Usage**: Google's internal Kubeflow deployment runs 500,000+ ML jobs monthly across 50+ clusters, with automatic resource scaling reducing training costs by 40%. Companies like Spotify use Kubeflow to orchestrate 1,000+ concurrent training jobs with fault tolerance.",
      "term": "Kubeflow Production Usage",
      "length": 288
    },
    {
      "footnote_id": "fn-cloud-ml-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 548,
      "definition": "**Cloud ML Training Economics**: Training GPT-3 was estimated to cost approximately $4.6 million on AWS according to Lambda Labs calculations, though official training costs were not disclosed by OpenAI, while fine-tuning typically costs $100-$10,000. Google's TPU v4 pods can reduce training costs by 2-5x compared to equivalent GPU clusters, with some organizations reporting 60-80% cost savings through spot instances and preemptible training.",
      "term": "Cloud ML Training Economics",
      "length": 446
    },
    {
      "footnote_id": "fn-docker-revolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 648,
      "definition": "**Docker's Revolution**: Created by Solomon Hykes in 2013, Docker popularized the phrase \"But it works on my machine!\" as the problem it solved. Within just 5 years, Docker went from a weekend project to being used by over 11 million developers worldwide, fundamentally changing how software is deployed.",
      "term": "Docker's Revolution",
      "length": 304
    },
    {
      "footnote_id": "fn-tensorflow-serving-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 654,
      "definition": "**TensorFlow Serving Origins**: Born from Google's internal serving system that handled billions of predictions per day for products like Gmail spam detection and YouTube recommendations. Google open-sourced it in 2016 when they realized that productionizing ML models was the bottleneck preventing widespread AI adoption.",
      "term": "TensorFlow Serving Origins",
      "length": 322
    },
    {
      "footnote_id": "fn-canary-deployment-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 658,
      "definition": "**Canary Deployment History**: Named after the canaries miners used to detect toxic gases; if the bird died, miners knew to evacuate immediately. Netflix pioneered this technique for software in 2011, and it became essential for ML where model failures can be subtle and catastrophic.",
      "term": "Canary Deployment History",
      "length": 284
    },
    {
      "footnote_id": "fn-ab-testing-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 662,
      "definition": "**A/B Testing for ML**: Statistical method to compare model performance by splitting traffic between model versions. Netflix runs 1,000+ A/B tests annually on recommendation algorithms, while Uber tests ride pricing models on millions of trips daily to optimize both user experience and revenue. Rollback decisions must balance the severity of degradation against business impact: a 2% accuracy drop might be acceptable during feature launches but unacceptable for safety-critical applications.",
      "term": "A/B Testing for ML",
      "length": 494
    },
    {
      "footnote_id": "fn-serverless-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 670,
      "definition": "**Serverless Computing for ML**: Infrastructure that automatically scales from zero to thousands of instances based on demand, with sub-second cold start times. AWS Lambda can handle 10,000+ concurrent ML inference requests, while Google Cloud Functions supports models up to 32GB, charging only for actual compute time used. For example, [AWS SageMaker Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) supports such configurations.",
      "term": "Serverless Computing for ML",
      "length": 461
    },
    {
      "footnote_id": "fn-mlflow-creation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 674,
      "definition": "**MLflow's Creation**: Built by the team at Databricks who were frustrated watching their customers struggle with ML experiment tracking. They noticed that data scientists were keeping model results in spreadsheets and could never reproduce their best experiments, a problem that inspired MLflow's \"model registry\" concept.",
      "term": "MLflow's Creation",
      "length": 323
    },
    {
      "footnote_id": "fn-tensorflow-serving",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 684,
      "definition": "**TensorFlow Serving**: Google's production-grade ML serving system handles over 100,000 queries per second per machine for lightweight models on high-end hardware with <10ms latency for most models. Originally built to serve YouTube's recommendation system, processing over 1 billion hours of video watched daily.",
      "term": "TensorFlow Serving",
      "length": 314
    },
    {
      "footnote_id": "fn-triton-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 686,
      "definition": "**NVIDIA Triton Inference Server**: Can achieve up to 40,000 inferences per second on a single A100 GPU for BERT models, with dynamic batching reducing latency by up to 10x compared to naive serving approaches. Supports concurrent execution of up to 100 different model types.",
      "term": "NVIDIA Triton Inference Server",
      "length": 276
    },
    {
      "footnote_id": "fn-kserve-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 688,
      "definition": "**KServe (formerly KFServing)**: Kubernetes-native serving framework that can autoscale from zero to thousands of replicas in under 30 seconds. Used by companies like Bloomberg to serve over 10,000 models simultaneously with 99.9% uptime SLA.",
      "term": "KServe (formerly KFServing)",
      "length": 242
    },
    {
      "footnote_id": "fn-sla-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 698,
      "definition": "**Service Level Agreements (SLAs)**: Production ML systems typically target 99.9% uptime (8.77 hours downtime/year) for critical services, with penalties of 10-25% monthly service credits for each 0.1% below target. Google's Cloud AI Platform promises 99.95% uptime with automatic failover in <30 seconds.",
      "term": "Service Level Agreements (SLAs)",
      "length": 305
    },
    {
      "footnote_id": "fn-slo-reality",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 700,
      "definition": "**Service Level Objectives (SLOs)**: Real-world ML serving SLOs often specify P95 latency <100ms for online inference, P99 <500ms, and error rates <0.1%. Netflix's recommendation system maintains P99 latency under 150ms while serving 200+ million users, processing 3+ billion hours of content monthly.",
      "term": "Service Level Objectives (SLOs)",
      "length": 301
    },
    {
      "footnote_id": "fn-ml-autoscaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 777,
      "definition": "**ML Autoscaling at Scale**: Kubernetes-based ML serving can scale from 1 to 1,000+ replicas in under 60 seconds. Uber's ML platform automatically scales 2,000+ models daily, reducing infrastructure costs by 35-50% through intelligent resource allocation and cold-start optimization achieving 99.95% availability.",
      "term": "ML Autoscaling at Scale",
      "length": 313
    },
    {
      "footnote_id": "fn-drift-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 803,
      "definition": "**Model Drift Detection**: Production systems typically trigger alerts when accuracy drops >5% over 24 hours or >10% over a week. Advanced systems like those at Spotify detect drift within 2-4 hours using statistical tests, with 85% of drift incidents caught before user impact.",
      "term": "Model Drift Detection",
      "length": 278
    },
    {
      "footnote_id": "fn-covid-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 807,
      "definition": "**COVID-19 ML Impact**: E-commerce recommendation systems saw accuracy drops of 15-40% within weeks of lockdowns beginning in March 2020. Amazon reported having to retrain over 1,000 models, while Netflix saw a 25% increase in viewing time that broke their capacity planning models.",
      "term": "COVID-19 ML Impact",
      "length": 282
    },
    {
      "footnote_id": "fn-prometheus-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 819,
      "definition": "**Prometheus at Scale**: Can ingest 1+ million samples per second per instance, with some deployments monitoring 100,000+ machines. DigitalOcean's Prometheus setup stores 2+ years of metrics data across 40,000+ time series, with query response times under 100ms for 95% of requests.",
      "term": "Prometheus at Scale",
      "length": 282
    },
    {
      "footnote_id": "fn-alerting-thresholds",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 823,
      "definition": "**Production Alert Thresholds**: Typical ML production alerts fire when GPU memory >90%, CPU >85% for >5 minutes, P99 latency >2x normal for >10 minutes, or error rates >1% for >60 seconds. Hardware-aware alerting extends these thresholds to include GPU utilization <60% for serving workloads (indicating resource waste), memory bandwidth utilization <40% (suggesting data pipeline bottlenecks), power consumption >110% of budget allocation (thermal risk), and thermal throttling events (immediate performance impact). High-frequency trading firms use microsecond-level alerts, while batch processing systems may use hour-long windows.",
      "term": "Production Alert Thresholds",
      "length": 635
    },
    {
      "footnote_id": "fn-shap-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 847,
      "definition": "**SHAP in Production**: SHAP explanations add 10-500ms latency per prediction depending on model complexity, making them costly for real-time serving. However, 40% of enterprise ML teams now use SHAP in production, with Microsoft reporting SHAP helped identify $2M in potential bias-related legal exposure in their hiring models.",
      "term": "SHAP in Production",
      "length": 329
    },
    {
      "footnote_id": "fn-tech-debt-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 895,
      "definition": "**Technical Debt Origins**: Ward Cunningham coined the term in 1992, comparing rushed coding decisions to financial debt: \"A little debt speeds development so long as it is paid back promptly with a rewrite.\" He later regretted the metaphor became an excuse for bad code rather than a tool for communicating tradeoffs.",
      "term": "Technical Debt Origins",
      "length": 318
    },
    {
      "footnote_id": "fn-pipeline-jungle-metaphor",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1100,
      "definition": "**Pipeline Jungle Metaphor**: Coined by Google researchers who observed that ML pipelines, like jungle growth, start simple but become increasingly tangled and impenetrable over time. Unlike traditional software, ML pipelines involve data flows that are harder to reason about, making the \"jungle\" metaphor particularly apt.",
      "term": "Pipeline Jungle Metaphor",
      "length": 324
    },
    {
      "footnote_id": "fn-youtube-engagement",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1148,
      "definition": "**YouTube Recommendation Impact**: The recommendation system drives 70% of watch time on the platform (1+ billion hours daily), but algorithmic changes in 2016 increased average session time by 50% while inadvertently promoting conspiracy content. Fixing these feedback loops required 2+ years of engineering work and new evaluation frameworks.",
      "term": "YouTube Recommendation Impact",
      "length": 344
    },
    {
      "footnote_id": "fn-zillow-losses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1154,
      "definition": "**Zillow iBuying Failure**: Zillow lost $881 million in a single quarter (Q3 2021) due to ML model failures, with the Zestimate algorithm overvaluing homes by an average of 5-7%. The company laid off 2,000+ employees and took a $569 million inventory write-down when shutting down Zillow Offers.",
      "term": "Zillow iBuying Failure",
      "length": 295
    },
    {
      "footnote_id": "fn-plc-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1422,
      "definition": "**Programmable Logic Controllers (PLCs)**: Industrial computers designed to control manufacturing processes, machines, and assembly lines. PLCs process thousands of sensor inputs per second with microsecond-level timing precision, forming the backbone of automated manufacturing systems worth over $80 billion globally.",
      "term": "Programmable Logic Controllers (PLCs)",
      "length": 319
    },
    {
      "footnote_id": "fn-telemetry-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1526,
      "definition": "**ML Telemetry**: Automated collection of operational data from ML systems including model performance metrics, infrastructure utilization, and prediction accuracy. Production ML systems generate 10GB-1TB of telemetry daily, enabling real-time drift detection and performance optimization.",
      "term": "ML Telemetry",
      "length": 289
    },
    {
      "footnote_id": "fn-elk-stack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1563,
      "definition": "**ELK Stack**: Elasticsearch (search/analytics engine), Logstash (data processing pipeline), and Kibana (visualization platform). Can process terabytes of logs daily with millisecond search response times. Used by Netflix to analyze 1+ billion events daily and identify system anomalies in real-time.",
      "term": "ELK Stack",
      "length": 300
    },
    {
      "footnote_id": "fn-microservices-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1761,
      "definition": "**Microservices in ML**: Architectural pattern where each ML model runs as an independent, loosely-coupled service with its own database and deployment lifecycle. Netflix operates 700+ microservices including 100+ for ML recommendations, enabling independent scaling and faster experimentation cycles.",
      "term": "Microservices in ML",
      "length": 301
    },
    {
      "footnote_id": "fn-inference-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 49,
      "definition": "**Inference Latency**: Real-time applications require <10ms response (autonomous vehicles), <100ms for interactive AI (voice assistants), vs. batch processing which tolerates seconds to minutes. Each millisecond of latency costs major web services millions in revenue.",
      "term": "Inference Latency",
      "length": 268
    },
    {
      "footnote_id": "fn-opt-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 65,
      "definition": "**Pruning**: The optimal brain damage algorithm [@lecun1990optimal] pioneered removing unnecessary neural network connections, inspiring modern magnitude-based and structured pruning techniques that can reduce model sizes by 90% with minimal accuracy loss.",
      "term": "Pruning",
      "length": 256
    },
    {
      "footnote_id": "fn-knowledge-distill",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 67,
      "definition": "**Knowledge Distillation**: Hinton et al. [@hinton2015distilling] introduced this technique where a smaller \"student\" network learns from a larger \"teacher\" network's soft outputs, enabling compact models that retain much of the original's performance while being orders of magnitude more efficient.",
      "term": "Knowledge Distillation",
      "length": 299
    },
    {
      "footnote_id": "fn-ml-runtimes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 98,
      "definition": "**ML Runtimes**: ONNX Runtime, TensorFlow Lite, and PyTorch Mobile handle model execution, memory management, and hardware acceleration. ONNX Runtime delivers 1.3-2.9x speedup over native frameworks through graph optimizations and efficient kernel implementations.",
      "term": "ML Runtimes",
      "length": 264
    },
    {
      "footnote_id": "fn-memory-hierarchy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 102,
      "definition": "**Memory Hierarchy**: L1 cache (32KB, <1ns), L2 cache (256KB-1MB, 3ns), L3 cache (8-32MB, 12ns), DRAM (8-128GB, 100ns), SSD (TBs, 100\u03bcs). Each level differs by 10-100x in speed and capacity, making data locality optimization important for ML performance.",
      "term": "Memory Hierarchy",
      "length": 254
    },
    {
      "footnote_id": "fn-microcontroller-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 116,
      "definition": "**Microcontroller Constraints**: Arduino Uno has 2KB RAM vs. 32KB flash storage. ARM Cortex-M4 typically has 256KB flash, 64KB RAM, running at 168MHz vs. modern GPUs with 3000+ MHz clocks and 16-80GB memory\u2014representing a 10,000x+ resource gap.",
      "term": "Microcontroller Constraints",
      "length": 244
    },
    {
      "footnote_id": "fn-edge-ml-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 120,
      "definition": "**Edge ML**: Computing paradigm where ML inference occurs on local devices (smartphones, IoT sensors, autonomous vehicles) rather than cloud servers. Reduces latency from 100-500ms cloud round-trip to <10ms local processing, but constrains models to 10-500MB vs. multi-GB cloud models.",
      "term": "Edge ML",
      "length": 285
    },
    {
      "footnote_id": "fn-tiny-ml-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 122,
      "definition": "**Tiny ML**: Ultra-low-power ML systems operating under 1mW power budget with <1MB memory. Enables always-on AI in hearing aids, smart sensors, and wearables. Models typically 10-100KB vs. GB-scale cloud models, representing 10,000x size reduction.",
      "term": "Tiny ML",
      "length": 248
    },
    {
      "footnote_id": "fn-memory-bandwidth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 132,
      "definition": "**Memory Bandwidth**: Modern GPUs achieve 3.35TB/s memory bandwidth (H100) vs. 25-50 GB/s for mobile SoCs. Large language models require 1-2x model size in GPU memory for training (16GB model needs 32GB+ GPU memory), creating the \"memory wall\" bottleneck.",
      "term": "Memory Bandwidth",
      "length": 255
    },
    {
      "footnote_id": "fn-fpga-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 152,
      "definition": "**FPGA for ML**: Field-Programmable Gate Arrays offer 10-100x lower latency than GPUs (microseconds vs. milliseconds) for specific neural network architectures. Intel's Stratix 10 FPGA achieves 9.2 TFLOPS for INT8 inference with 2-5x better energy efficiency than GPUs for edge deployment.",
      "term": "FPGA for ML",
      "length": 289
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 176,
      "definition": "**Mixed-Precision Training**: Uses FP16 for forward pass and FP32 for gradient computation, achieving 1.5-2x training speedup with 50% memory reduction. NVIDIA's automatic mixed precision (AMP) maintains FP32 accuracy while delivering 1.6x speedup on V100 and 2.2x on A100 GPUs.",
      "term": "Mixed-Precision Training",
      "length": 278
    },
    {
      "footnote_id": "fn-sparsity-def",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 184,
      "definition": "**Sparsity**: Percentage of zero-valued parameters in a model. 90% sparse models have only 10% non-zero weights, reducing memory by 10x and computation by 10x (with specialized hardware). Modern transformers naturally exhibit 80-95% activation sparsity during inference.",
      "term": "Sparsity",
      "length": 270
    },
    {
      "footnote_id": "fn-matrix-factorization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 188,
      "definition": "**Matrix Factorization**: Decomposes large weight matrices (e.g., 4096\u00d74096) into smaller matrices (4096\u00d7256 \u00d7 256\u00d74096), reducing parameters from 16M to 2M (8x reduction). SVD and low-rank approximations maintain 95%+ accuracy while enabling 3-5x speedup on mobile hardware.",
      "term": "Matrix Factorization",
      "length": 275
    },
    {
      "footnote_id": "fn-lora",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 192,
      "definition": "**LoRA (Low-Rank Adaptation)**: Fine-tuning technique that freezes pretrained weights and adds small trainable matrices, reducing trainable parameters by 99% (from 175B to 1.2M for GPT-3 scale). Achieves comparable performance while reducing training memory and computation by 3x.",
      "term": "LoRA (Low-Rank Adaptation)",
      "length": 280
    },
    {
      "footnote_id": "fn-operator-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 200,
      "definition": "**Operator Fusion**: Graph-level optimization that combines multiple operations into single kernels, reducing memory bandwidth by 30-50%. In ResNet-50, fusing Conv+BatchNorm+ReLU operations achieves 1.8x speedup on V100 GPUs, while BERT transformer blocks show 25% latency reduction through attention fusion. Dynamic computation approaches like early exit architectures and conditional computation are supported by custom execution runtimes that optimize control flow.",
      "term": "Operator Fusion",
      "length": 468
    },
    {
      "footnote_id": "fn-gradient-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 278,
      "definition": "**Gradient Checkpointing**: Memory optimization technique that trades computation for memory by recomputing intermediate activations during backpropagation instead of storing them. Reduces memory usage by 20-50% in transformer models, enabling larger batch sizes or model sizes within same GPU memory.",
      "term": "Gradient Checkpointing",
      "length": 301
    },
    {
      "footnote_id": "fn-parallel-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 282,
      "definition": "**Parallel Processing in ML**: Modern GPUs have 5,000-10,000+ cores vs. CPU's 8-64 cores. NVIDIA H100 delivers 989 TFLOPS tensor performance vs. Intel Xeon 3175-X's ~1.5 TFLOPS (double precision), representing a 650x compute density advantage for parallelizable ML workloads.",
      "term": "Parallel Processing in ML",
      "length": 275
    },
    {
      "footnote_id": "fn-gpu-operational-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 300,
      "definition": "**GPU Operational Costs**: AWS p4d.24xlarge (8\u00d7A100 GPUs) costs $32.77/hour. Training GPT-3 required 3,640 petaflop-days, costing approximately $4.6M in compute. Google's search uses 12TWh annually\u2014optimization reducing inference costs by 10% saves billions globally.",
      "term": "GPU Operational Costs",
      "length": 267
    },
    {
      "footnote_id": "fn-distributed-training-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 304,
      "definition": "**Distributed Training Costs**: Training large models requires InfiniBand networks (200+ Gb/s) costing $2,000+ per port. Meta's OPT-175B used 992 A100 GPUs for 2 months, consuming ~1GWh of energy. Communication overhead can consume 20-40% of training time without optimization.",
      "term": "Distributed Training Costs",
      "length": 277
    },
    {
      "footnote_id": "fn-tf-model-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 318,
      "definition": "**TensorFlow Model Optimization**: TensorFlow Model Optimization Toolkit provides production-ready quantization (achieving 4x model size reduction), pruning (up to 90% sparsity), and clustering techniques. Used by YouTube, Gmail, and Google Photos to deploy models on 4+ billion devices worldwide.",
      "term": "TensorFlow Model Optimization",
      "length": 297
    },
    {
      "footnote_id": "fn-structured-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 816,
      "definition": "**Structured Pruning**: Filter pruning in ResNet-34 achieves 50% FLOP reduction with only 1.0% accuracy loss on CIFAR-10. Channel pruning in MobileNetV2 reduces parameters by 73% while maintaining 96.5% of original accuracy, enabling 3.2x faster inference on ARM processors.",
      "term": "Structured Pruning",
      "length": 274
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 984,
      "definition": "**FLOPs (Floating-Point Operations)**: Computational complexity metric counting multiply-add operations. ResNet-50 requires 3.8 billion FLOPs per inference [@he2016deep], GPT-3 training needed 3.14E23 FLOPs [@patterson2021carbon]. Modern GPUs achieve 100-300 TFLOPS (trillion FLOPs/second), making FLOP reduction important for efficiency.",
      "term": "FLOPs (Floating-Point Operations)",
      "length": 338
    },
    {
      "footnote_id": "fn-lottery-ticket",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1448,
      "definition": "**Lottery Ticket Hypothesis**: Frankle & Carbin (2018) demonstrated that ResNet-18 subnetworks at 10-20% original size achieve 93.2% accuracy vs. 94.1% for full model on CIFAR-10. BERT-base winning tickets retain 97% performance with 90% fewer parameters, requiring 5-8x less training time to converge.",
      "term": "Lottery Ticket Hypothesis",
      "length": 302
    },
    {
      "footnote_id": "fn-onnx-deployment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1711,
      "definition": "**ONNX Deployment**: ONNX Runtime achieves 1.3-2.9x speedup over TensorFlow and 1.1-1.7x over PyTorch across various models. ResNet-50 inference drops from 7.2ms to 2.8ms on CPU, while BERT-Base reduces from 45ms to 23ms with ONNX Runtime optimizations including graph fusion and memory pooling.",
      "term": "ONNX Deployment",
      "length": 295
    },
    {
      "footnote_id": "fn-tensorrt-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1713,
      "definition": "**TensorRT Optimization**: NVIDIA TensorRT delivers 5-40x speedup for inference on V100 GPUs (40x represents GPU+TensorRT vs CPU-only baseline; 5x is typical GPU optimization). ResNet-50 INT8 inference achieves 1.2ms vs. 4.8ms FP32, while BERT-Large drops from 10.4ms to 2.1ms. Layer fusion reduces kernel launches by 80%, memory bandwidth by 50%.",
      "term": "TensorRT Optimization",
      "length": 347
    },
    {
      "footnote_id": "fn-bert-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1743,
      "definition": "**BERT Compression**: BERT-Base (110M params) can be compressed to 67M params (39% reduction) with only 1.2% GLUE score drop. Attention head pruning removes 144 of 192 heads with minimal impact, while layer pruning reduces 12 layers to 6 layers maintaining 97.8% performance.",
      "term": "BERT Compression",
      "length": 275
    },
    {
      "footnote_id": "fn-distilbert-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1745,
      "definition": "**DistilBERT**: Achieves 97% of BERT-Base performance with 40% fewer parameters (66M vs. 110M) and 60% faster inference. On SQuAD v1.1, DistilBERT scores 86.9 F1 vs. BERT's 88.5 F1, while reducing memory from 1.35GB to 0.54GB and latency from 85ms to 34ms.",
      "term": "DistilBERT",
      "length": 256
    },
    {
      "footnote_id": "fn-efficientnet-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1747,
      "definition": "**EfficientNet Pruning**: EfficientNet-B0 with 70% structured pruning maintains 75.8% ImageNet accuracy (vs. 77.1% original) with 2.8x speedup on mobile devices. Channel pruning reduces FLOPs from 390M to 140M while keeping inference under 20ms on Pixel 4.",
      "term": "EfficientNet Pruning",
      "length": 256
    },
    {
      "footnote_id": "fn-hardware-aware-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2430,
      "definition": "**Hardware-Aware NAS**: MnasNet achieves 78.1% ImageNet accuracy with 315M FLOPs vs. MobileNetV2's 72.0% with 300M FLOPs. EfficientNet-B0 delivers 77.1% accuracy with 390M FLOPs, 23% better accuracy/FLOP ratio than ResNet-50, enabling 4.9x faster mobile inference.",
      "term": "Hardware-Aware NAS",
      "length": 264
    },
    {
      "footnote_id": "fn-rl-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2468,
      "definition": "**Reinforcement Learning NAS**: Uses RL controller networks to generate architectures, with accuracy as reward signal. Google's NASNet controller was trained for 22,400 GPU-hours on 800 GPUs, but discovered architectures achieving 82.7% ImageNet accuracy\u201428% better than human-designed ResNet at similar FLOP budgets.",
      "term": "Reinforcement Learning NAS",
      "length": 317
    },
    {
      "footnote_id": "fn-evolutionary-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2470,
      "definition": "**Evolutionary NAS**: Treats architectures as genes, evolving populations through mutation and crossover. AmoebaNet required 3,150 GPU-days but achieved 83.9% ImageNet accuracy. Real et al.'s evolution approach discovered architectures that matched manually tuned models with 7,000x less human effort.",
      "term": "Evolutionary NAS",
      "length": 301
    },
    {
      "footnote_id": "fn-nas-evaluation-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2514,
      "definition": "**NAS Evaluation Metrics**: Multi-objective optimization considers accuracy (top-1/top-5), latency (ms on target hardware), memory (MB activations + parameters), and energy (mJ per inference). Pareto-optimal architectures provide 15-40% better efficiency frontiers than manual designs.",
      "term": "NAS Evaluation Metrics",
      "length": 285
    },
    {
      "footnote_id": "fn-fbnet-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2518,
      "definition": "**FBNet**: Achieves 74.9% ImageNet accuracy with 375M FLOPs and 23ms latency on Samsung S8, 15% faster than MobileNetV2 with comparable accuracy. The latency-aware search uses device-specific lookup tables for actual hardware performance measurement. Instead of selecting the most accurate model, NAS identified architectures that provided the best balance between accuracy and inference speed [@wu2019fbnet]. Similarly, EfficientNet was discovered through NAS by jointly optimizing for accuracy and computational efficiency, resulting in a model that delivers state-of-the-art performance while reducing FLOPs compared to conventional architectures [@tan2019efficientnet].",
      "term": "FBNet",
      "length": 673
    },
    {
      "footnote_id": "fn-energy-efficiency-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2654,
      "definition": "**Energy Efficiency Metrics**: INT8 quantization reduces energy consumption by 4-8x over FP32. MobileNetV2 INT8 consumes 47mJ vs. 312mJ FP32 per inference on Cortex-A75. ResNet-50 on TPU v4 achieves 0.9 TOPS/Watt vs. 0.3 TOPS/Watt on V100 GPU.",
      "term": "Energy Efficiency Metrics",
      "length": 243
    },
    {
      "footnote_id": "fn-qat-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3601,
      "definition": "**Quantization-Aware Training**: QAT enables INT8 inference with minimal accuracy loss - ResNet-50 maintains 76.1% vs. 76.2% FP32 ImageNet accuracy, while MobileNetV2 achieves 71.8% vs. 72.0%. BERT-Base INT8 retains 99.1% of FP32 performance on GLUE, compared to 96.8% with post-training quantization alone.",
      "term": "Quantization-Aware Training",
      "length": 307
    },
    {
      "footnote_id": "fn-memory-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3978,
      "definition": "**Memory Optimization**: DenseNet-121 reduces memory consumption by 50% compared to ResNet-50 through feature reuse, requiring only 7.9MB vs. 15.3MB activation memory on ImageNet. MobileNetV3 achieves 73% memory reduction with depth-wise separable convolutions, enabling deployment on 2GB mobile devices.",
      "term": "Memory Optimization",
      "length": 304
    },
    {
      "footnote_id": "fn-sparse-energy-savings",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5682,
      "definition": "**Sparse Energy Savings**: 90% sparsity in BERT reduces training energy by 2.3x and inference energy by 4.1x on V100. Structured 2:4 sparsity patterns deliver 1.6x energy savings on A100 GPUs while maintaining 99% of dense model accuracy. Hardware accelerators like TPUs and GPUs are optimized to handle these operations efficiently, making sparse networks not only faster but also more energy-efficient [@Cheng2022].",
      "term": "Sparse Energy Savings",
      "length": 417
    },
    {
      "footnote_id": "fn-hyperparameter-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5816,
      "definition": "**Hyperparameter Optimization**: Learning rate search alone can improve model accuracy by 5-15%. Grid search over 4 hyperparameters with 10 values each requires 10,000 training runs. Bayesian optimization reduces this to 100-200 runs while achieving comparable results, saving weeks of computation.",
      "term": "Hyperparameter Optimization",
      "length": 298
    },
    {
      "footnote_id": "fn-batch-size-effects",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5818,
      "definition": "**Batch Size Effects**: Large batches (512-2048) improve throughput by 2-4x but require gradient accumulation for memory constraints. Linear scaling rule maintains convergence: learning rate scales linearly with batch size, enabling ImageNet training in 1 hour with batch size 8192. : **AutoML (Automated Machine Learning)**: End-to-end automation of ML pipeline including data preprocessing, feature selection, model selection, and hyperparameter tuning. Google's AutoML achieved 82.2% ImageNet accuracy vs. 80.1% for human experts, while requiring 50,000x less computational resources than manual NAS.",
      "term": "Batch Size Effects",
      "length": 603
    },
    {
      "footnote_id": "fn-bayesian-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5822,
      "definition": "**Bayesian Optimization**: Uses probabilistic models to guide hyperparameter search, modeling objective function uncertainty. Requires 10-50x fewer evaluations than random search. GPyOpt and Optuna frameworks enable practical Bayesian optimization for neural networks with multi-objective constraints.",
      "term": "Bayesian Optimization",
      "length": 301
    },
    {
      "footnote_id": "fn-xla-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5954,
      "definition": "**XLA (Accelerated Linear Algebra)**: Google's domain-specific compiler achieves 1.15-1.4x speedup on ResNet-50 inference and 1.2-1.7x on BERT-Large training through operator fusion and memory optimization. XLA reduces HBM traffic by 25-40% through aggressive kernel fusion, delivering 15-30% energy savings on TPUs.",
      "term": "XLA (Accelerated Linear Algebra)",
      "length": 316
    },
    {
      "footnote_id": "fn-tvm-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5958,
      "definition": "**TVM (Tensor Virtual Machine)**: Apache TVM's auto-tuning delivers 1.2-2.8x speedup over vendor libraries on ARM CPUs and 1.5-3.2x on mobile GPUs. TVM's graph-level optimizations reduce inference latency by 40-60% on edge devices through operator scheduling and memory planning.",
      "term": "TVM (Tensor Virtual Machine)",
      "length": 279
    },
    {
      "footnote_id": "fn-network-protocols",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 46,
      "definition": "**Network Protocols**: Standardized communication rules like TCP/IP (1974), HTTP (1991), and TLS (1999) that enable secure data exchange across networks. Modern ML systems rely on protocols like gRPC (2015) for high-performance model serving, handling millions of inference requests per second.",
      "term": "Network Protocols",
      "length": 294
    },
    {
      "footnote_id": "fn-model-extraction-2016",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 56,
      "definition": "**Model Extraction Threat**: The vulnerability was first demonstrated in 2016 when researchers showed they could steal machine learning models through API queries alone. By systematically querying a model and analyzing responses, attackers could recreate proprietary models worth millions in R&D investment, turning public APIs into inadvertent IP leakage channels.",
      "term": "Model Extraction Threat",
      "length": 365
    },
    {
      "footnote_id": "fn-gdpr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 106,
      "definition": "**General Data Protection Regulation (GDPR)**: Enacted by the EU in 2018, GDPR imposes fines up to 4% of global revenue (\u20ac20+ million) for privacy violations. Since enforcement began, over \u20ac4.5 billion in fines have been levied, including \u20ac746 million against Amazon in 2021, driving massive investment in privacy-preserving ML technologies.",
      "term": "General Data Protection Regulation (GDPR)",
      "length": 341
    },
    {
      "footnote_id": "fn-dp-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 118,
      "definition": "**Differential Privacy Origins**: Cynthia Dwork coined the term at Microsoft Research in 2006, but the concept emerged from her frustration with the \"anonymization myth\"\u2014the false belief that removing names from data guaranteed privacy. Her groundbreaking insight was that privacy should be mathematically provable, not just plausible, leading to the rigorous framework that now protects billions of users' data in products from Apple to Google.",
      "term": "Differential Privacy Origins",
      "length": 445
    },
    {
      "footnote_id": "fn-stuxnet-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 128,
      "definition": "**Stuxnet Discovery**: The malware was first detected by VirusBokNok, a small Belarusian antivirus company, when their client computers began crashing unexpectedly. What seemed like a routine malware investigation turned into one of the most significant cybersecurity discoveries in history\u2014the first confirmed cyberweapon designed to cause physical destruction.",
      "term": "Stuxnet Discovery",
      "length": 362
    },
    {
      "footnote_id": "fn-zero-day-term",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 130,
      "definition": "**Zero-Day Etymology**: The term originated in software piracy circles, referring to the \"zero days\" since a program's release when pirated copies appeared. In security, it describes the \"zero days\" defenders have to patch a vulnerability before attackers exploit it\u2014representing the ultimate race between attack and defense.",
      "term": "Zero-Day Etymology",
      "length": 325
    },
    {
      "footnote_id": "fn-air-gapped",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 136,
      "definition": "**Air-Gapped Systems**: Networks physically isolated from external connections, originally developed for military systems in the 1960s. Despite seeming impenetrable, studies show 90% of air-gapped systems can be breached through techniques like acoustic, electromagnetic, or thermal convert channels.",
      "term": "Air-Gapped Systems",
      "length": 300
    },
    {
      "footnote_id": "fn-usb-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 138,
      "definition": "**USB Attack Vectors**: USB interfaces, introduced in 1996, became a primary attack vector for crossing air gaps. The 2008 Operation Olympic Games reportedly used infected USB drives to penetrate secure facilities, with some estimates suggesting 60% of organizations remain vulnerable to USB-based attacks. It specifically targeted programmable logic controllers (PLCs), industrial computers that automate electromechanical processes such as controlling the speed of centrifuges. By exploiting vulnerabilities in the Windows operating system and the Siemens Step7 software used to program the PLCs, Stuxnet achieved highly targeted, real-world disruption.",
      "term": "USB Attack Vectors",
      "length": 655
    },
    {
      "footnote_id": "fn-automotive-recalls",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 154,
      "definition": "**Automotive Cybersecurity Recalls**: The Jeep Cherokee hack triggered the first-ever automotive cybersecurity recall in 2015. Since then, cybersecurity recalls have affected over 15 million vehicles globally, costing manufacturers an estimated $2.4 billion in remediation efforts and spurring new regulations.",
      "term": "Automotive Cybersecurity Recalls",
      "length": 310
    },
    {
      "footnote_id": "fn-nhtsa",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 156,
      "definition": "**NHTSA Cybersecurity Guidelines**: Established in 1970, NHTSA issued its first cybersecurity guidance in 2016 following the Jeep hack. The agency now mandates that connected vehicles include cybersecurity by design, affecting 99% of new vehicles sold in the US that contain 100+ onboard computers.",
      "term": "NHTSA Cybersecurity Guidelines",
      "length": 298
    },
    {
      "footnote_id": "fn-ddos-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 168,
      "definition": "**DDoS Attacks**: Distributed Denial-of-Service attacks overwhelm targets with traffic from multiple sources, first demonstrated in 1999. Modern DDoS attacks can exceed 2.3 Tbps (terabits per second), enough to take down entire internet infrastructures and costing businesses $2.3 million per incident on average. These devices, often deployed with factory-default usernames and passwords, were easily compromised by the Mirai malware and enlisted into a large-scale attack network.",
      "term": "DDoS Attacks",
      "length": 482
    },
    {
      "footnote_id": "fn-mirai-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 170,
      "definition": "**Mirai Botnet Scale**: At its peak, Mirai controlled over 600,000 infected IoT devices, generating 623 Gbps DDoS attacks that took down major internet services including Twitter, Netflix, and Reddit for hours. The attack revealed that IoT devices with default credentials (admin/admin, root/12345) could be weaponized at unprecedented scale.",
      "term": "Mirai Botnet Scale",
      "length": 342
    },
    {
      "footnote_id": "fn-arm-processors",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 192,
      "definition": "**ARM Processors**: Originally developed in 1985 for personal computers, ARM processors now power 95% of smartphones and 70% of IoT devices. Their low power consumption (0.1-2 watts vs 15-150 watts for x86) makes them ideal for edge ML, but their simplified architectures often omit advanced security features. As these devices take on more responsibility for local data processing and real-time decision-making, they become attractive targets for remote compromise.",
      "term": "ARM Processors",
      "length": 466
    },
    {
      "footnote_id": "fn-data-poisoning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 224,
      "definition": "**Data Poisoning**: Attack technique where adversaries inject malicious data during training, first formalized in 2012 [@biggio2012poisoning]. Studies show that poisoning just 0.1% of training data can reduce model accuracy by 10-50%, making it a highly efficient attack vector against ML systems.",
      "term": "Data Poisoning",
      "length": 297
    },
    {
      "footnote_id": "fn-adversarial-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 226,
      "definition": "**Adversarial Examples**: Inputs crafted to deceive ML models, discovered by Szegedy et al. [@szegedy2014intriguing]. These attacks can fool state-of-the-art image classifiers with perturbations invisible to humans (changing <0.01% of pixel values), affecting 99%+ of deep learning models.",
      "term": "Adversarial Examples",
      "length": 289
    },
    {
      "footnote_id": "fn-phishing-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 287,
      "definition": "**AI-Generated Phishing**: Large language models can generate convincing phishing emails with 99%+ grammatical accuracy, compared to 19% for traditional phishing. Security firms report a 1,265% increase in AI-generated phishing attacks since 2022, with some campaigns achieving 30%+ success rates. This dual-use potential necessitates a broader security perspective\u2014one that considers models not only as assets to defend but also as possible instruments of attack.",
      "term": "AI-Generated Phishing",
      "length": 464
    },
    {
      "footnote_id": "fn-ml-apis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 295,
      "definition": "**ML APIs**: Application Programming Interfaces for machine learning, popularized by Google's Prediction API (2010). Today's ML APIs handle billions of requests daily, with major providers processing billions of tokens monthly, creating vast attack surfaces for model extraction.",
      "term": "ML APIs",
      "length": 279
    },
    {
      "footnote_id": "fn-model-repositories",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 297,
      "definition": "**Model Repositories**: Centralized platforms for sharing ML models, led by Hugging Face (2016) which hosts 500,000+ models. While democratizing AI access, these repositories have become targets for supply chain attacks, with researchers finding malicious models in 5% of popular repositories.",
      "term": "Model Repositories",
      "length": 293
    },
    {
      "footnote_id": "fn-model-serialization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 299,
      "definition": "**Model Serialization**: Process of converting trained models into portable formats like ONNX (2017), TensorFlow SavedModel (2016), or PyTorch's .pth files. Insecure serialization can expose model weights and enable arbitrary code execution, affecting 80%+ of deployed ML systems.",
      "term": "Model Serialization",
      "length": 280
    },
    {
      "footnote_id": "fn-model-inversion-attack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 305,
      "definition": "**Model Inversion Attack**: First demonstrated in 2015 against face recognition systems, researchers reconstructed recognizable faces from neural network outputs using only confidence scores. The attack revealed that models trained on 40 individuals could leak identifiable facial features, proving that \"black-box\" API access isn't sufficient privacy protection.",
      "term": "Model Inversion Attack",
      "length": 363
    },
    {
      "footnote_id": "fn-netflix-deanonymization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 309,
      "definition": "**Netflix Deanonymization**: In 2008, researchers re-identified Netflix users by correlating the \"anonymous\" Prize dataset with public IMDb ratings. Using as few as 8 movie ratings with dates, they identified 99% of users, leading Netflix to cancel a second competition and highlighting the futility of naive anonymization.",
      "term": "Netflix Deanonymization",
      "length": 323
    },
    {
      "footnote_id": "fn-ml-side-channel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 380,
      "definition": "**Side-Channel Attacks on ML**: First demonstrated against neural networks in 2018, researchers showed that power consumption patterns during inference could reveal model architecture, layer sizes, and even distinguish between different model families. This extended traditional cryptographic side-channel attacks into the ML domain, creating new vulnerabilities for edge AI devices.",
      "term": "Side-Channel Attacks on ML",
      "length": 383
    },
    {
      "footnote_id": "fn-model-distillation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 388,
      "definition": "**Model Distillation**: Knowledge transfer technique developed by Hinton et al. [@hinton2015distilling] where a smaller \"student\" model learns from a larger \"teacher\" model. While designed for model compression, attackers exploit this to create stolen models with 95%+ accuracy using only 1% of the original training data.",
      "term": "Model Distillation",
      "length": 322
    },
    {
      "footnote_id": "fn-crowdsourcing-risks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 437,
      "definition": "**Crowdsourcing Risks**: Platforms like Amazon Mechanical Turk (2005) and Prolific democratized data labeling but introduced poisoning risks. Studies show 15-30% of crowdsourced labels contain errors or bias, with coordinated attacks capable of poisoning entire datasets at costs under $1,000. Even in more controlled settings, poisoning may occur through compromised data storage, insider manipulation, or insecure data transfer processes.",
      "term": "Crowdsourcing Risks",
      "length": 440
    },
    {
      "footnote_id": "fn-backdoor-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 458,
      "definition": "**Backdoor Attacks**: Hidden triggers embedded in ML models during training, first demonstrated in 2017. These attacks achieve 99%+ success rates while maintaining normal accuracy, with triggers as subtle as single-pixel modifications. BadNets, the seminal backdoor attack, affected 100% of tested models.",
      "term": "Backdoor Attacks",
      "length": 305
    },
    {
      "footnote_id": "fn-perspective-api",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 462,
      "definition": "**Perspective API**: Google's toxicity detection model launched in 2017, now processing 500+ million comments daily across platforms like The New York Times and Wikipedia. Despite sophisticated training, the API demonstrates how even billion-parameter models remain vulnerable to targeted poisoning attacks.",
      "term": "Perspective API",
      "length": 307
    },
    {
      "footnote_id": "fn-perspective-vulnerability",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 464,
      "definition": "After retraining, the poisoned model exhibited a significantly higher false negative rate, allowing offensive language to bypass filters. This case illustrates how poisoned data can exploit feedback loops in systems that rely on user-generated input, leading to reduced effectiveness over time and creating long-term vulnerabilities in content moderation pipelines.",
      "term": null,
      "length": 365
    },
    {
      "footnote_id": "fn-gans-adversarial",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 515,
      "definition": "**GANs for Adversarial Attacks**: Generative Adversarial Networks, invented by Ian Goodfellow in 2014 [@goodfellow2020generative], were quickly adapted for attacking other models. GAN-based adversarial generators can create perturbations that fool target models with 95%+ success rates while remaining imperceptible to humans. This iterative process allows the attacker to generate sophisticated and diverse adversarial examples.",
      "term": "GANs for Adversarial Attacks",
      "length": 429
    },
    {
      "footnote_id": "fn-transfer-learning-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 519,
      "definition": "**Transfer Learning Attacks**: Pre-trained models like ImageNet classifiers (2012) accelerated AI development but created new attack vectors. Adversarial perturbations targeting shared feature extractors can simultaneously compromise multiple downstream tasks, amplifying attack impact by 10-100x compared to single-model attacks. Headless attacks, for example, manipulate the feature extractor without requiring access to the classification head or training data [@ahmed2020headless]. This exposes a important vulnerability in systems that rely on pre-trained models.",
      "term": "Transfer Learning Attacks",
      "length": 568
    },
    {
      "footnote_id": "fn-meltdown-spectre-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 636,
      "definition": "**Meltdown/Spectre Impact**: Disclosed in January 2018, these vulnerabilities affected virtually every processor made since 1995\u2014billions of devices. The disclosure triggered emergency patches across all major operating systems, causing 5-30% performance degradation in some workloads, and led to a core rethinking of processor security.",
      "term": "Meltdown/Spectre Impact",
      "length": 337
    },
    {
      "footnote_id": "fn-speculative-execution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 640,
      "definition": "**Speculative Execution**: Introduced in the Intel Pentium Pro (1995), this technique executes instructions before confirming they're needed, improving performance by 10-25%. However, it creates a 20+ year attack window where speculated operations leak data through cache timing, affecting ML accelerators that rely on similar optimizations.",
      "term": "Speculative Execution",
      "length": 341
    },
    {
      "footnote_id": "fn-hipaa-violations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 650,
      "definition": "**HIPAA Violations**: Since enforcement began in 2003, HIPAA has generated over $130 million in fines, with individual penalties reaching $16 million. The largest healthcare data breach affected 78.8 million patients at Anthem Inc. in 2015, highlighting the massive scale of exposure when ML systems handling medical data are compromised.",
      "term": "HIPAA Violations",
      "length": 338
    },
    {
      "footnote_id": "fn-aes-standard",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 708,
      "definition": "**Advanced Encryption Standard (AES)**: Adopted by NIST in 2001 as the US government encryption standard, AES replaced DES after 24 years. Despite being mathematically secure with 2^128 possible keys for AES-128, physical implementations remain vulnerable to side-channel attacks that can extract keys in minutes. Techniques such as Differential Power Analysis (DPA), Differential Electromagnetic Analysis (DEMA), and Correlation Power Analysis (CPA) exploit these physical signals to recover secret keys.",
      "term": "Advanced Encryption Standard (AES)",
      "length": 505
    },
    {
      "footnote_id": "fn-iot-vulnerabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 746,
      "definition": "**IoT Device Vulnerabilities**: Studies reveal 70-80% of IoT devices contain serious security flaws, with baby monitors among the worst offenders. Security firm Rapid7 found that popular baby monitor brands exposed unencrypted video streams, affecting millions of households globally.",
      "term": "IoT Device Vulnerabilities",
      "length": 284
    },
    {
      "footnote_id": "fn-medical-device-security",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 748,
      "definition": "**Medical Device Security**: FDA reports show 53% of medical devices contain known vulnerabilities, with pacemakers and insulin pumps most at risk. The average medical device contains 6.2 vulnerabilities, some dating back over a decade, affecting 2.4 billion medical devices worldwide.",
      "term": "Medical Device Security",
      "length": 285
    },
    {
      "footnote_id": "fn-debug-ports",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 752,
      "definition": "**Debug Port Vulnerabilities**: Hardware debug interfaces like JTAG (1990) and SWD (2006) are essential for development but often left accessible in production. Security researchers estimate that 60-70% of embedded devices ship with unsecured debug ports, creating backdoors for attackers. In the automotive domain, unsecured OBD-II diagnostic ports have allowed attackers to manipulate braking and steering functions in connected vehicles, as demonstrated in the well-known Jeep Cherokee hack.",
      "term": "Debug Port Vulnerabilities",
      "length": 494
    },
    {
      "footnote_id": "fn-cybersecurity-regulations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 772,
      "definition": "**Cybersecurity Regulations**: Global cybersecurity compliance costs exceed $150 billion annually, with frameworks like SOC 2, ISO 27001, PCI DSS, and sector-specific rules governing ML systems. Financial services face additional requirements under regulations like SOX, while healthcare must comply with HIPAA, creating complex multi-regulatory environments.",
      "term": "Cybersecurity Regulations",
      "length": 359
    },
    {
      "footnote_id": "fn-dp-sgd-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 922,
      "definition": "**DP-SGD Industry Adoption**: Apple was the first major company to deploy differential privacy at scale in 2016, protecting 1+ billion users' data in iOS. Their implementation adds noise to emoji usage, Safari crashes, and QuickType suggestions, balancing privacy (\u03b5=4-16) with utility for improving user experience across their ecosystem.",
      "term": "DP-SGD Industry Adoption",
      "length": 339
    },
    {
      "footnote_id": "fn-privacy-utility-tension",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 934,
      "definition": "**Privacy-Utility Tension**: This core tradeoff was formalized by Dwork and McSherry, who proved that perfect privacy (infinite noise) yields no utility, while perfect utility (no noise) provides no privacy. The \"privacy budget\" concept emerged from this insight\u2014you can only spend privacy once, making every query a strategic decision.",
      "term": "Privacy-Utility Tension",
      "length": 336
    },
    {
      "footnote_id": "fn-he-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 947,
      "definition": "**Homomorphic Encryption Breakthrough**: Considered the \"holy grail\" of cryptography since the 1970s, fully homomorphic encryption remained theoretical until Craig Gentry's 2009 PhD thesis. His breakthrough was realizing that \"noisy\" ciphertexts could support unlimited operations if periodically \"refreshed,\" solving a decades-old puzzle that allows computation on encrypted data.",
      "term": "Homomorphic Encryption Breakthrough",
      "length": 381
    },
    {
      "footnote_id": "fn-smpc-overhead",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 956,
      "definition": "**SMPC Performance**: Secure multi-party computation typically incurs 1000-10,000x computational overhead compared to plaintext operations. A simple neural network inference that takes milliseconds on GPU requires hours using SMPC, limiting practical applications to small models and offline scenarios.",
      "term": "SMPC Performance",
      "length": 302
    },
    {
      "footnote_id": "fn-synthetic-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 962,
      "definition": "**Synthetic Data Growth**: The synthetic data market grew from $110 million in 2019 to $1.1 billion in 2023, driven by privacy regulations and data scarcity. Companies like Uber use synthetic trip data to protect user privacy while maintaining ML model performance, with some synthetic datasets achieving 95%+ statistical fidelity. This approach reduces the risk of direct reidentification, but does not offer formal privacy guarantees unless combined with DP constraints during generation.",
      "term": "Synthetic Data Growth",
      "length": 490
    },
    {
      "footnote_id": "fn-model-watermarking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1004,
      "definition": "**Model Watermarking**: Technique for proving model ownership developed in 2017, analogous to digital image watermarks. Modern watermarking can embed signatures in less than 0.01% of model parameters while maintaining 99%+ accuracy, helping prove IP theft in courts where billions of dollars in AI assets are at stake. These watermarks can be used to detect and prove misuse of stolen models in downstream deployments. Watermarking strategies must be carefully designed to remain robust to model compression, fine-tuning, and format conversion.",
      "term": "Model Watermarking",
      "length": 544
    },
    {
      "footnote_id": "fn-oauth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1024,
      "definition": "**OAuth Protocol**: Open Authorization standard developed in 2006, now used by 3+ billion users across Google, Facebook, and Microsoft services. OAuth 2.0 (2012) enables secure API access without exposing user credentials, processing trillions of authentication requests annually for ML API access.",
      "term": "OAuth Protocol",
      "length": 298
    },
    {
      "footnote_id": "fn-mutual-tls",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1026,
      "definition": "**Mutual TLS (mTLS)**: Enhanced Transport Layer Security where both client and server authenticate each other using certificates, introduced in 1999. mTLS provides 99.9%+ secure communication but increases latency by 15-30ms, making it suitable for high-security ML API endpoints requiring end-to-end authentication.",
      "term": "Mutual TLS (mTLS)",
      "length": 316
    },
    {
      "footnote_id": "fn-api-keys",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1028,
      "definition": "**API Keys**: Simple authentication tokens first popularized by Google Maps API (2005), now ubiquitous in ML services. While convenient, API keys in URL parameters or headers can be logged or exposed, with studies showing 10-15% of GitHub repositories accidentally contain leaked API keys worth millions in compute credits.",
      "term": "API Keys",
      "length": 323
    },
    {
      "footnote_id": "fn-rbac",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1030,
      "definition": "**Role-Based Access Control (RBAC)**: Access control model developed by NIST in the 1990s, now mandatory for government systems. RBAC reduces security administration overhead by 90%+ compared to individual permissions, with modern ML platforms supporting thousands of roles governing model access, data permissions, and compute resources. This key authenticates the client and allows the backend to enforce usage policies, monitor for abuse, and log access patterns. Secure implementations retrieve API keys from environment variables rather than hardcoding them into source code, preventing credential exposure in version control systems or application logs. Such key-based access control mechanisms are simple to implement but require careful key management and monitoring to prevent misuse, unauthorized access, or model extraction. Additional security measures in production deployments typically include model integrity verification through SHA-256 hash checking, rate limiting to prevent abuse, input validation for size and format constraints, and comprehensive logging for security event tracking.",
      "term": "Role-Based Access Control (RBAC)",
      "length": 1105
    },
    {
      "footnote_id": "fn-ci-cd-security",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1034,
      "definition": "**CI/CD Security**: Continuous Integration/Continuous Deployment pipelines, popularized by Netflix and Amazon, now deploy code 1000+ times per day. However, 60% of organizations report CI/CD security incidents, with supply chain attacks like SolarWinds (2020) affecting 18,000+ customers, highlighting the critical need for pipeline security in ML deployments. Verifying model signatures and maintaining audit trails helps ensure that only authorized models are deployed into production.",
      "term": "CI/CD Security",
      "length": 487
    },
    {
      "footnote_id": "fn-attention-maps",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1069,
      "definition": "**Attention Maps**: Visualization technique for understanding transformer model focus, introduced with the attention mechanism in 2015. Attention maps reveal which input tokens influence outputs most strongly, helping detect potential bias or manipulation in models processing 175+ billion parameters like GPT-3.",
      "term": "Attention Maps",
      "length": 312
    },
    {
      "footnote_id": "fn-healthcare-ml-compliance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1095,
      "definition": "**Healthcare ML Compliance**: FDA has approved 500+ AI-based medical devices since 2016, requiring strict validation under 21 CFR Part 820 quality systems. Healthcare ML systems must demonstrate safety, efficacy, and bias mitigation, with some approvals taking 2-5 years and costing $50+ million in clinical trials.",
      "term": "Healthcare ML Compliance",
      "length": 315
    },
    {
      "footnote_id": "fn-trustzone-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1157,
      "definition": "**ARM TrustZone**: Introduced in 2004, TrustZone now ships in 95% of ARM processors, protecting over 5 billion mobile devices. Despite its ubiquity, many devices underutilize TrustZone\u2014studies show only 20-30% of Android devices implement meaningful secure world applications beyond basic key storage.",
      "term": "ARM TrustZone",
      "length": 301
    },
    {
      "footnote_id": "fn-intel-sgx-limits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1159,
      "definition": "**Intel SGX Constraints**: SGX enclaves are limited to 128MB of protected memory (EPC), with cache misses causing 100x performance penalties. For ML workloads, a ResNet-50 requires 102MB for weights alone, consuming 80% of SGX EPC before any intermediate activations. Inference latency increases from 5ms to 150ms when model exceeds EPC capacity. This makes SGX unsuitable for large ML models but effective for protecting cryptographic keys and small inference models under 10MB.",
      "term": "Intel SGX Constraints",
      "length": 479
    },
    {
      "footnote_id": "fn-hsm-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1370,
      "definition": "**HSM Performance**: Enterprise HSMs can perform 10,000+ RSA-2048 operations per second but cost $20,000-$100,000+ per unit. In contrast, software-only cryptography on GPUs achieves 100,000+ operations/second at $1,000+ hardware cost, but without the tamper-resistance and regulatory compliance that HSMs provide.",
      "term": "HSM Performance",
      "length": 313
    },
    {
      "footnote_id": "fn-hsm-certification",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1392,
      "definition": "**HSM Certification**: Hardware Security Module certification under FIPS 140-2 or Common Criteria can take 12-24 months and cost $500,000-$2 million. However, many regulated industries require these certifications, with banking, government, and healthcare sectors mandating Level 3+ certified HSMs for cryptographic operations.",
      "term": "HSM Certification",
      "length": 327
    },
    {
      "footnote_id": "fn-fips-140",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1394,
      "definition": "**FIPS 140-2 Standard**: Federal Information Processing Standard for cryptographic modules, established in 2001 with four security levels. Level 4 HSMs must survive physical attacks, operating at -40\u00b0C to +85\u00b0C with tamper detection that zeroizes keys within seconds, making them suitable for the most sensitive ML applications. Access to the HSM is typically restricted to a small set of authorized personnel, which can complicate development workflows and slow iteration cycles.",
      "term": "FIPS 140-2 Standard",
      "length": 480
    },
    {
      "footnote_id": "fn-puf-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1402,
      "definition": "**PUF Market Growth**: The PUF market is projected to reach $320 million by 2025, driven by IoT security needs. Major semiconductor companies including Intel, Xilinx, and Synopsis now offer PUF IP, with deployment in smart cards, automotive ECUs, and edge ML devices requiring device-unique authentication.",
      "term": "PUF Market Growth",
      "length": 306
    },
    {
      "footnote_id": "fn-high-stakes-domains",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 46,
      "definition": "**High-Stakes Domains**: Areas where automated decisions directly impact fundamental life outcomes: healthcare (treatment decisions), criminal justice (sentencing recommendations), employment (hiring algorithms), and finance (loan approvals). Estimates suggest that algorithmic decision-making affects over 2 billion people daily across these domains, with errors potentially causing irreversible harm to individuals' health, freedom, or economic prospects.",
      "term": "High-Stakes Domains",
      "length": 457
    },
    {
      "footnote_id": "fn-structural-inequities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 48,
      "definition": "**Structural Inequities**: Systematic patterns of advantage and disadvantage embedded in social institutions, policies, and practices. In ML, these manifest when models trained on historical data perpetuate past discrimination\u2014for example, Amazon's recruiting algorithm (discontinued in 2018) systematically downgraded resumes containing words like \"women's\" because it learned from male-dominated hiring patterns spanning 10 years.",
      "term": "Structural Inequities",
      "length": 432
    },
    {
      "footnote_id": "fn-protected-attributes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 72,
      "definition": "**Protected Attributes**: Characteristics legally protected from discrimination in most jurisdictions, typically including race, gender, age, religion, disability status, and sexual orientation. The specific list varies by country\u2014the EU's GDPR covers 9 categories, while the US Civil Rights Act covers 5. In ML systems, these attributes require special handling because their historical correlation with outcomes often reflects past discrimination rather than legitimate predictive relationships.",
      "term": "Protected Attributes",
      "length": 497
    },
    {
      "footnote_id": "fn-demographic-parity-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 74,
      "definition": "**Demographic Parity Origins**: This fairness criterion was first formalized by computer scientist Cynthia Dwork and colleagues in 2011, building on legal concepts from the 1971 Supreme Court case *Griggs v. Duke Power Co.*, which established that employment practices with disparate impact could violate civil rights law even without discriminatory intent. The mathematical formalization bridged legal theory with algorithmic practice.",
      "term": "Demographic Parity Origins",
      "length": 436
    },
    {
      "footnote_id": "fn-equalized-odds",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 76,
      "definition": "**Equalized Odds**: A fairness constraint requiring that a classifier have equal true positive rates and equal false positive rates across protected groups. Developed by Moritz Hardt and others at Google in 2016, it's stricter than demographic parity because it conditions on the true outcome. For example, a medical diagnosis system would need equal sensitivity (correctly identifying disease) and equal specificity (correctly identifying health) across racial groups.",
      "term": "Equalized Odds",
      "length": 469
    },
    {
      "footnote_id": "fn-post-hoc-explanations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 80,
      "definition": "**Post-Hoc Explanations**: Interpretability methods applied after model training to understand decisions, including LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations). LIME, developed at the University of Washington in 2016, explains individual predictions by learning local surrogate models. SHAP, introduced by researchers at the University of Washington in 2017, provides theoretically grounded feature attribution based on game theory, now used by major tech companies for model interpretation.",
      "term": "Post-Hoc Explanations",
      "length": 543
    },
    {
      "footnote_id": "fn-value-alignment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 88,
      "definition": "**Value Alignment**: A fundamental challenge in AI safety, first formally articulated by Stuart Russell in 2015 and Nick Bostrom in 2014. The problem: how to ensure AI systems optimize for human values when those values are complex, context-dependent, and often conflicting. Notable failures include Facebook's 2016 \"Year in Review\" feature that created painful reminders for users who experienced loss, and YouTube's recommendation algorithm optimizing for \"engagement\" leading to promotion of extreme content.",
      "term": "Value Alignment",
      "length": 511
    },
    {
      "footnote_id": "fn-human-in-the-loop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 92,
      "definition": "**Human-in-the-Loop (HITL)**: A design pattern where humans actively participate in model training or decision-making, rather than being replaced by automation. Examples include content moderation (major platforms employ thousands of content reviewers), medical diagnosis (radiologists reviewing AI-flagged scans), and autonomous vehicles (safety drivers ready to intervene). Research shows HITL systems can reduce error rates by 50-80% compared to fully automated systems, though they introduce new challenges around human-machine coordination and trust.",
      "term": "Human-in-the-Loop (HITL)",
      "length": 555
    },
    {
      "footnote_id": "fn-differential-privacy-table",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 120,
      "definition": "**Differential Privacy**: A mathematical framework guaranteeing that removing or adding a single individual's data has minimal statistical impact on query results. Developed by Cynthia Dwork in 2006, it provides formal privacy guarantees by adding calibrated noise to computations. Apple implemented it across 1+ billion iOS devices in 2016, and Google uses it in Chrome for collecting usage statistics while protecting individual browsing patterns.",
      "term": "Differential Privacy",
      "length": 449
    },
    {
      "footnote_id": "fn-dp-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 122,
      "definition": "**DP-SGD (Differentially Private Stochastic Gradient Descent)**: A training algorithm that ensures privacy by clipping gradients and adding noise during optimization. Introduced by Google researchers in 2016, it enables training on sensitive data while limiting information leakage about individual examples. The technique reduces model accuracy by 2-5% while increasing training time by 15-30% and memory usage by 10-20% due to gradient clipping and noise computation overhead. Used in production by Google, Apple, and Microsoft for training on user data.",
      "term": "DP-SGD (Differentially Private Stochastic Gradient Descent)",
      "length": 556
    },
    {
      "footnote_id": "fn-compas-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 152,
      "definition": "**COMPAS Algorithm Controversy**: A 2016 ProPublica investigation revealed that COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) incorrectly flagged Black defendants as future criminals at nearly twice the rate of white defendants (45% vs 24%), while white defendants were mislabeled as low-risk more often than Black defendants (48% vs 28%). The algorithm was used in sentencing decisions across multiple states despite these documented disparities.",
      "term": "COMPAS Algorithm Controversy",
      "length": 479
    },
    {
      "footnote_id": "fn-local-explanations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 156,
      "definition": "**Local Explanations**: Interpretability methods that explain individual predictions, such as \"this loan was denied because the applicant's debt-to-income ratio (65%) exceeded the threshold (40%).\" Popular techniques include LIME and SHAP, which identify which input features most influenced a specific decision. These explanations help users understand and potentially contest individual outcomes, crucial for regulatory compliance and user trust.",
      "term": "Local Explanations",
      "length": 448
    },
    {
      "footnote_id": "fn-global-explanations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 158,
      "definition": "**Global Explanations**: Methods that describe a model's overall behavior patterns across all inputs, such as \"this model primarily relies on credit score (40% importance), income (25%), and payment history (20%) for loan decisions.\" Techniques include feature importance rankings, decision trees as surrogate models, and partial dependence plots. Global explanations help developers debug model behavior and auditors assess system-wide fairness.",
      "term": "Global Explanations",
      "length": 446
    },
    {
      "footnote_id": "fn-feature-engineering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 160,
      "definition": "**Feature Engineering**: The process of transforming raw data into input variables that machine learning algorithms can effectively use. Examples include converting categorical variables to numerical representations, creating interaction terms, and normalizing scales. Poor feature engineering can embed bias\u2014for example, using ZIP code as a feature may indirectly discriminate based on race due to residential segregation patterns.",
      "term": "Feature Engineering",
      "length": 432
    },
    {
      "footnote_id": "fn-gdpr-article-22",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 164,
      "definition": "**GDPR Article 22**: Known as the \"right to explanation,\" this provision affects an estimated 500 million EU citizens and has inspired similar legislation worldwide. Since GDPR's 2018 implementation, regulators have issued over \u20ac5.88 billion in fines as of January 2025, with many cases involving algorithmic decision-making. The regulation's global influence extends beyond Europe\u2014over 120 countries now have privacy laws modeled on GDPR principles.",
      "term": "GDPR Article 22",
      "length": 450
    },
    {
      "footnote_id": "fn-systemic-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 174,
      "definition": "**Systemic Bias**: Prejudice embedded in social systems and institutions that creates unequal outcomes for different groups. In ML, this manifests when historical data reflects past discrimination\u2014for example, if past hiring data shows men being promoted more often, a model may learn to favor male candidates. Research shows that without intervention, ML systems can amplify existing biases because they optimize for patterns in historical data that may reflect past discrimination.",
      "term": "Systemic Bias",
      "length": 483
    },
    {
      "footnote_id": "fn-healthcare-algorithm-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 178,
      "definition": "**Healthcare Algorithm Scale**: This Optum algorithm affected approximately 200 million Americans annually, determining access to high-risk care management programs. The bias reduced Black patients' enrollment by 50%\u2014if corrected, the number of Black patients identified for extra care would increase from 17.7% to 46.5%, highlighting how algorithmic decisions can perpetuate healthcare disparities at massive scale.",
      "term": "Healthcare Algorithm Scale",
      "length": 416
    },
    {
      "footnote_id": "fn-fairness-impossibility",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 217,
      "definition": "**Fairness Impossibility Theorems**: Mathematical proofs showing that multiple fairness criteria cannot be simultaneously satisfied except in trivial cases. Jon Kleinberg and others proved in 2016 that calibration, equalized odds, and demographic parity are mutually exclusive for any classifier where base rates differ between groups. This means practitioners must choose which type of fairness to prioritize, making fairness fundamentally a value-laden engineering decision rather than a purely technical optimization problem.",
      "term": "Fairness Impossibility Theorems",
      "length": 528
    },
    {
      "footnote_id": "fn-datacenter-environmental-justice",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 225,
      "definition": "**Datacenter Environmental Justice**: Research by the Environmental Justice Foundation shows that 68% of major cloud computing facilities in the U.S. are located within 10 miles of low-income communities or communities of color. These areas experience increased air pollution from backup generators, higher local temperatures from cooling systems, and strained local electrical grids. Meanwhile, high-speed internet access required for advanced AI services remains limited in many of these same communities, creating a computational equity gap where communities bear environmental costs without receiving proportional benefits.",
      "term": "Datacenter Environmental Justice",
      "length": 627
    },
    {
      "footnote_id": "fn-model-memorization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 237,
      "definition": "**Model Memorization**: The phenomenon where ML models inadvertently store training data verbatim, allowing extraction through carefully crafted queries. Nicholas Carlini and others demonstrated in 2021, with further quantification in 2023, that GPT-2 could reproduce entire email addresses, phone numbers, and personal information from training data. Studies suggest large language models like ChatGPT memorize roughly 1% of their training data, raising concerns about privacy violations when models are trained on personal information without explicit consent.",
      "term": "Model Memorization",
      "length": 562
    },
    {
      "footnote_id": "fn-membership-inference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 295,
      "definition": "**Membership Inference Attacks**: Privacy attacks that determine whether a specific individual's data was used to train a model by analyzing the model's behavior on that individual's data. First demonstrated by Reza Shokri and others in 2017, these attacks exploit the fact that models tend to be more confident on training data. They pose serious privacy risks\u2014for example, determining if someone's medical record was used to train a disease prediction model reveals sensitive health information.",
      "term": "Membership Inference Attacks",
      "length": 497
    },
    {
      "footnote_id": "fn-adversarial-inputs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 309,
      "definition": "**Adversarial Inputs**: Maliciously crafted inputs designed to fool machine learning models by adding imperceptible perturbations that cause misclassification. First demonstrated by Szegedy et al. in 2013, these attacks reveal fundamental vulnerabilities in deep neural networks and have significant implications for safety-critical applications like autonomous vehicles and medical diagnosis.",
      "term": "Adversarial Inputs",
      "length": 393
    },
    {
      "footnote_id": "fn-model-cards",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 329,
      "definition": "**Model Cards**: Standardized documentation for machine learning models, introduced by Google researchers in 2018. Similar to nutrition labels for food, they provide essential information about a model's intended use, performance across different groups, limitations, and ethical considerations. Companies like Google, Facebook, and IBM now use model cards for deployed systems. They help practitioners understand model behavior and enable auditors to assess fairness and safety.",
      "term": "Model Cards",
      "length": 479
    },
    {
      "footnote_id": "fn-datasheets",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 331,
      "definition": "**Datasheets for Datasets**: Standardized documentation for datasets, proposed by researchers at Microsoft and University of Washington in 2018. Modeled after electronics datasheets, they document dataset creation, composition, intended uses, and potential biases. Major datasets like ImageNet and CIFAR-10 now include datasheets. They help practitioners understand dataset limitations and assess suitability for their specific applications, reducing the risk of inappropriate usage.",
      "term": "Datasheets for Datasets",
      "length": 483
    },
    {
      "footnote_id": "fn-digital-infrastructure-divide",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 349,
      "definition": "**Digital Infrastructure Divide**: The Federal Communications Commission estimates that 39% of rural Americans lack access to high-speed broadband compared to only 2% in urban areas. For AI services requiring real-time responsiveness, users in underserved areas experience 200-500ms additional latency, making interactive explainability features and real-time bias monitoring infeasible. This infrastructure disparity means that responsible AI features are often unavailable to the communities most vulnerable to algorithmic harm, creating an inverse relationship between need and access to ethical AI protections.",
      "term": "Digital Infrastructure Divide",
      "length": 614
    },
    {
      "footnote_id": "fn-federated-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 373,
      "definition": "**Federated Learning**: A machine learning approach where models are trained across multiple decentralized devices or servers without centralizing the data. Developed by Google in 2016 for improving Gboard predictions while keeping typing data on devices. Now used by Apple for Siri improvements and by hospitals for medical research without sharing patient data. While privacy-preserving, federated learning complicates fairness assessment since no single entity can observe the complete demographic distribution across all participants.",
      "term": "Federated Learning",
      "length": 538
    },
    {
      "footnote_id": "fn-energy-privacy-tradeoff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 453,
      "definition": "**Energy-Privacy Tradeoffs**: Research by Stanford and MIT demonstrates that privacy-preserving techniques like differential privacy and secure multi-party computation can increase computational energy requirements by 20-60% depending on implementation. In federated learning scenarios, this translates to 15-30% faster battery drain on mobile devices. For users with older devices, limited battery life, or concerns about electricity costs, these energy requirements can effectively exclude them from privacy-protected AI services, creating a system where privacy becomes contingent on economic resources.",
      "term": "Energy-Privacy Tradeoffs",
      "length": 606
    },
    {
      "footnote_id": "fn-saliency-maps",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 489,
      "definition": "**Saliency Maps**: Visual explanations highlighting which parts of an input (like pixels in an image) most influenced a model's decision. Originally developed for computer vision in 2013, they use gradients to compute feature importance. Unlike LIME or SHAP, saliency maps require only a single backward pass, making them computationally efficient for edge devices. However, they can be noisy and may highlight irrelevant features, requiring careful interpretation.",
      "term": "Saliency Maps",
      "length": 465
    },
    {
      "footnote_id": "fn-fairlearn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 517,
      "definition": "**Fairlearn**: Microsoft's open-source toolkit for assessing and improving ML model fairness, first released in 2019. It provides metrics for measuring fairness across demographic groups and algorithms for bias mitigation. Used by organizations like Uber, Pinterest, and Ernst & Young to audit models before deployment. The toolkit supports both fairness assessment (identifying disparities) and fairness intervention (reducing disparities through techniques like reweighting and post-processing).",
      "term": "Fairlearn",
      "length": 497
    },
    {
      "footnote_id": "fn-differential-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 812,
      "definition": "**Differential Privacy**: Introduced by Cynthia Dwork in 2006, differential privacy revolutionized privacy-preserving computation by providing mathematical guarantees rather than heuristic protections. Apple was among the first major companies to deploy differential privacy at scale in 2016, using it to collect iOS usage statistics from over 1 billion devices while preserving individual privacy. The technique adds calibrated noise to computations, ensuring that no single person's data significantly affects the output.",
      "term": "Differential Privacy",
      "length": 523
    },
    {
      "footnote_id": "fn-ai-safety",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1324,
      "definition": "**AI Safety**: A research field focused on ensuring advanced AI systems remain beneficial and controllable. Originated from concerns raised by researchers like Stuart Russell and Nick Bostrom around 2010, it addresses both near-term risks (bias, privacy violations) and long-term risks (misaligned superintelligent systems). Major organizations like OpenAI, Anthropic, and DeepMind now invest significantly in safety research, while companies like Tesla have faced real-world safety challenges with autonomous driving systems.",
      "term": "AI Safety",
      "length": 526
    },
    {
      "footnote_id": "fn-proxy-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1326,
      "definition": "**Proxy Metrics**: Measurable indicators used as substitutes for the true objective when the real goal is difficult to quantify directly. Common examples include using click-through rates as a proxy for user satisfaction, or test scores as a proxy for educational quality. The danger arises from Goodhart's Law: \"When a measure becomes a target, it ceases to be a good measure\"\u2014systems optimize for the proxy rather than the underlying goal.",
      "term": "Proxy Metrics",
      "length": 441
    },
    {
      "footnote_id": "fn-ctr-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1330,
      "definition": "**Click-Through Rate (CTR) Optimization**: The practice of maximizing the percentage of users who click on content or ads. While seemingly logical, CTR optimization can incentivize sensationalism and clickbait. YouTube's 2012-2017 algorithm optimized for CTR, leading to promotion of conspiracy theories and extreme content because they generated more clicks. The platform shifted to optimizing \"watch time\" in 2017 to address this misalignment between clicks and user satisfaction.",
      "term": "Click-Through Rate (CTR) Optimization",
      "length": 482
    },
    {
      "footnote_id": "fn-reward-hacking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1332,
      "definition": "**Reward Hacking**: When an AI system finds unexpected ways to maximize its reward function that violate the designer's intentions. Classic examples include a Tetris-playing AI that learned to pause the game indefinitely to avoid losing (maximizing score by avoiding failure), and cleaning robots that learned to knock over objects to create messes they could then clean up. This phenomenon highlights the difficulty of specifying objectives that capture human intentions.",
      "term": "Reward Hacking",
      "length": 472
    },
    {
      "footnote_id": "fn-safety-critical",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 47,
      "definition": "**Safety-Critical Applications**: Systems where failure could result in loss of life, significant property damage, or environmental harm. Examples include nuclear power plants, aircraft control systems, and medical devices, domains where ML deployment requires the highest reliability standards.",
      "term": "Safety-Critical Applications",
      "length": 295
    },
    {
      "footnote_id": "fn-smart-cities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 51,
      "definition": "**Smart Cities**: Urban areas that use IoT devices, data analytics, and AI to optimize infrastructure, energy usage, and public services. With urban populations expected to reach 68% of the global population by 2050 [@un2018world], smart city initiatives will manage increasingly large data volumes, contributing to the estimated 2.5 quintillion bytes of data generated globally each day [@ibm2020data].",
      "term": "Smart Cities",
      "length": 403
    },
    {
      "footnote_id": "fn-transient-vs-permanent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 64,
      "definition": "**Transient vs Permanent Faults**: Transient faults are temporary disruptions (lasting microseconds to seconds) often caused by cosmic rays or electromagnetic interference, while permanent faults cause lasting damage requiring component replacement. Transient faults are 1000x more common than permanent faults in modern systems [@baumann2005soft].",
      "term": "Transient vs Permanent Faults",
      "length": 348
    },
    {
      "footnote_id": "fn-systemic-vulnerabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 70,
      "definition": "**Systemic Vulnerabilities**: Weaknesses that affect entire system architectures rather than individual components. Unlike isolated bugs, these can cascade across multiple layers, potentially compromising thousands of interconnected services simultaneously.",
      "term": "Systemic Vulnerabilities",
      "length": 257
    },
    {
      "footnote_id": "fn-hardening-strategies",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 74,
      "definition": "**Hardening Strategies**: Techniques to increase system resilience against faults and attacks, including redundancy, input validation, and fail-safe mechanisms. Edge systems often use selective hardening, protecting only critical components due to resource constraints.",
      "term": "Hardening Strategies",
      "length": 269
    },
    {
      "footnote_id": "fn-alexa-voice-service",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 123,
      "definition": "**Alexa Voice Service (AVS)**: Amazon's cloud-based voice AI platform that enables third-party manufacturers to integrate Alexa into their devices. Used by over 100,000 devices across 4,500 brands, processing billions of voice interactions annually.",
      "term": "Alexa Voice Service (AVS)",
      "length": 249
    },
    {
      "footnote_id": "fn-failsafe-mechanisms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 125,
      "definition": "**Failsafe Mechanisms**: Systems designed to automatically shift to a safe state when a fault occurs. Examples include circuit breakers that prevent cascading failures and graceful degradation that maintains core functionality when components fail.",
      "term": "Failsafe Mechanisms",
      "length": 248
    },
    {
      "footnote_id": "fn-silent-data-corruption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 129,
      "definition": "**Silent Data Corruption (SDC)**: Hardware or software errors that corrupt data without triggering error detection mechanisms. Studies show SDC affects 1 in every 1,000-10,000 computations in large-scale systems [@dixit2021silent], making it a major reliability concern.",
      "term": "Silent Data Corruption (SDC)",
      "length": 270
    },
    {
      "footnote_id": "fn-ai-hypercomputers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 250,
      "definition": "**AI Hypercomputers**: Massive computing systems specifically designed for AI workloads, featuring thousands of specialized processors (TPUs/GPUs) interconnected with high-bandwidth networks. Google's latest systems contain over 100,000 accelerators working in parallel.",
      "term": "AI Hypercomputers",
      "length": 270
    },
    {
      "footnote_id": "fn-edge-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 258,
      "definition": "**Edge Computing**: Processing data near its source rather than in centralized cloud servers, reducing latency from ~100ms to <10ms. Critical for autonomous vehicles where millisecond delays can mean the difference between collision avoidance and catastrophic failure.",
      "term": "Edge Computing",
      "length": 268
    },
    {
      "footnote_id": "fn-autopilot",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 264,
      "definition": "**Autopilot**: Tesla's driver assistance system that provides semi-autonomous capabilities like steering, braking, and acceleration while requiring active driver supervision.",
      "term": "Autopilot",
      "length": 174
    },
    {
      "footnote_id": "fn-embedded-systems",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 270,
      "definition": "**Embedded Systems**: Computer systems designed for specific control functions within larger systems, often with real-time constraints. Range from 8-bit microcontrollers with kilobytes of memory to complex systems-on-chip, typically operating for years without human intervention.",
      "term": "Embedded Systems",
      "length": 280
    },
    {
      "footnote_id": "fn-model-uncertainty",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 284,
      "definition": "**Model Uncertainty**: The inadequacy of a machine learning model to capture the full complexity of the underlying data-generating process.",
      "term": "Model Uncertainty",
      "length": 139
    },
    {
      "footnote_id": "fn-single-event-upsets",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 416,
      "definition": "**Single Event Upsets (SEUs)**: Radiation-induced bit flips in memory or logic caused by cosmic rays or alpha particles. Modern DRAM exhibits error rates of approximately 1 per 10^17 bits accessed under typical operating conditions [@baumann2005soft]. These rates occur roughly once per gigabit per month at sea level, increasing exponentially with altitude, with commercial aircraft experiencing 300x higher rates. For AI systems processing large datasets, these seemingly low error rates compound significantly - a 1TB model checkpoint experiences an expected 80 bit flips during a single full read operation, making error detection and correction essential for reliable ML training and inference.",
      "term": "Single Event Upsets (SEUs)",
      "length": 699
    },
    {
      "footnote_id": "fn-electromagnetic-interference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 418,
      "definition": "**Electromagnetic Interference (EMI)**: Disturbance caused by external electromagnetic sources that can disrupt electronic circuits. Common sources include cell phones, WiFi, and nearby switching power supplies, requiring careful shielding in sensitive systems.",
      "term": "Electromagnetic Interference (EMI)",
      "length": 261
    },
    {
      "footnote_id": "fn-crosstalk",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 420,
      "definition": "**Crosstalk**: Unwanted signal coupling between adjacent conductors due to parasitic capacitance and inductance. Becomes increasingly problematic as circuit densities increase, potentially causing timing violations and data corruption.",
      "term": "Crosstalk",
      "length": 235
    },
    {
      "footnote_id": "fn-glitches",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 492,
      "definition": "**Glitches**: Momentary deviation in voltage, current, or signal, often causing incorrect operation.",
      "term": "Glitches",
      "length": 100
    },
    {
      "footnote_id": "fn-combinationallogic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 494,
      "definition": "**Combinational logic**: Digital logic, wherein the output depends only on the current input states, not any past states.",
      "term": "Combinational logic",
      "length": 121
    },
    {
      "footnote_id": "fn-stochastic-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 510,
      "definition": "**Stochastic Computing**: A collection of techniques using random bits and logic operations to perform arithmetic and data processing, promising better fault tolerance.",
      "term": "Stochastic Computing",
      "length": 168
    },
    {
      "footnote_id": "fn-lookup-table",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 524,
      "definition": "**Lookup Table**: A data structure used to replace a runtime computation with a simpler array indexing operation.",
      "term": "Lookup Table",
      "length": 113
    },
    {
      "footnote_id": "fn-electromigration",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 539,
      "definition": "The movement of metal atoms in a conductor under the influence of an electric field.",
      "term": null,
      "length": 84
    },
    {
      "footnote_id": "fn-oxide-breakdown",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 541,
      "definition": "The failure of an oxide layer in a transistor due to excessive electric field stress.",
      "term": null,
      "length": 85
    },
    {
      "footnote_id": "fn-thermal-stress",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 543,
      "definition": "**Thermal Stress**: Degradation caused by repeated cycling through high and low temperatures. Modern AI accelerators commonly experience thermal throttling under sustained workloads, leading to performance degradation of 20-60% as processors reduce clock speeds to prevent overheating. This throttling directly impacts ML training times and inference throughput, making thermal management critical for maintaining consistent AI system performance in production environments.",
      "term": "Thermal Stress",
      "length": 474
    },
    {
      "footnote_id": "fn-error-correcting-codes",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 642,
      "definition": "**Error-Correcting Codes**: Methods used in data storage and transmission to detect and correct errors.",
      "term": "Error-Correcting Codes",
      "length": 103
    },
    {
      "footnote_id": "fn-checkpoint-restart",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 644,
      "definition": "**Checkpoint and Restart Mechanisms**: Techniques that periodically save a program's state so it can resume from the last saved state after a failure.",
      "term": "Checkpoint and Restart Mechanisms",
      "length": 150
    },
    {
      "footnote_id": "fn-scan-chains",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 708,
      "definition": "**Scan Chains**: Dedicated paths incorporated within a processor that grant access to internal registers and logic for testing.",
      "term": "Scan Chains",
      "length": 127
    },
    {
      "footnote_id": "fn-hamming1950error",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 819,
      "definition": "R. W. Hamming's seminal paper introduced error detection and correction codes, significantly advancing digital communication reliability.",
      "term": null,
      "length": 137
    },
    {
      "footnote_id": "fn-parity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 821,
      "definition": "In parity checks, an extra bit accounts for the total number of 1s in a data word, enabling basic error detection.",
      "term": null,
      "length": 114
    },
    {
      "footnote_id": "fn-dmr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 829,
      "definition": "**Double Modular Redundancy (DMR)**: A fault-tolerance process in which computations are duplicated to identify and correct errors.",
      "term": "Double Modular Redundancy (DMR)",
      "length": 131
    },
    {
      "footnote_id": "fn-tmr",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 831,
      "definition": "**Triple Modular Redundancy (TMR)**: A fault-tolerance process where three instances of a computation are performed to identify and correct errors.",
      "term": "Triple Modular Redundancy (TMR)",
      "length": 147
    },
    {
      "footnote_id": "fn-hot-spares",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 853,
      "definition": "**Hot Spares**: In a system redundancy design, these are the backup components kept ready to instantaneously replace failing components without disrupting the operation.",
      "term": "Hot Spares",
      "length": 169
    },
    {
      "footnote_id": "fn-gradient-based-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1443,
      "definition": "**Gradient-Based Attacks**: Adversarial techniques that use the model's gradients to craft perturbations. Discovered by Ian Goodfellow in 2014, these attacks revealed that neural networks are vulnerable to imperceptible input modifications, spurring an entire research field in adversarial machine learning.",
      "term": "Gradient-Based Attacks",
      "length": 307
    },
    {
      "footnote_id": "fn-fgsm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1445,
      "definition": "**Fast Gradient Sign Method (FGSM)**: The first practical adversarial attack method, proposed by Goodfellow et al. in 2014. Generates adversarial examples in a single step by moving in the direction of the gradient's sign, making it computationally efficient but often less effective than iterative methods.",
      "term": "Fast Gradient Sign Method (FGSM)",
      "length": 307
    },
    {
      "footnote_id": "fn-white-box-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1473,
      "definition": "**White-Box Attacks**: Adversarial attacks where the attacker has complete knowledge of the target model, including architecture, weights, and training data. More powerful than black-box attacks but less realistic in practice, as attackers rarely have full model access.",
      "term": "White-Box Attacks",
      "length": 270
    },
    {
      "footnote_id": "fn-carlini-wagner",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1479,
      "definition": "**Carlini and Wagner (C&W) Attack**: Developed in 2017, this sophisticated attack method finds minimal perturbations by solving an optimization problem with carefully designed loss functions. Often considered the strongest white-box attack, it successfully breaks many defensive mechanisms that stop simpler attacks.",
      "term": "Carlini and Wagner (C&W) Attack",
      "length": 316
    },
    {
      "footnote_id": "fn-transferability",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1491,
      "definition": "**Transferability**: A surprising property discovered in 2015 showing that adversarial examples often transfer between different neural networks. Success rates typically range from 30-70% across models, enabling practical black-box attacks without direct model access.",
      "term": "Transferability",
      "length": 268
    },
    {
      "footnote_id": "fn-data-poisoning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1577,
      "definition": "**Data Poisoning**: Attack method first formalized by Biggio et al. in 2012, where adversaries inject malicious samples into training data to compromise model behavior. Unlike adversarial examples that target inference, poisoning attacks the learning process itself, making them harder to detect and defend against.",
      "term": "Data Poisoning",
      "length": 315
    },
    {
      "footnote_id": "fn-backdoor-attacks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1595,
      "definition": "**Backdoor Attacks**: Introduced by Gu et al. in 2017, these attacks embed hidden triggers in training data that activate malicious behavior when specific patterns appear at inference time. Success rates can exceed 99% while maintaining normal accuracy on clean inputs, making them particularly dangerous.",
      "term": "Backdoor Attacks",
      "length": 305
    },
    {
      "footnote_id": "fn-dualusedilemma",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2125,
      "definition": "**Dual-use Dilemma**: In AI, the challenge of mitigating misuse of technology that has both positive and negative potential uses.",
      "term": "Dual-use Dilemma",
      "length": 129
    },
    {
      "footnote_id": "fn-huber-loss",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2780,
      "definition": "**Huber Loss**: A loss function used in robust regression that is less sensitive to outliers in data than squared error loss.",
      "term": "Huber Loss",
      "length": 125
    },
    {
      "footnote_id": "fn-regularization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2782,
      "definition": "**Regularization**: A method used in neural networks to prevent overfitting in models by adding a cost term to the loss function.",
      "term": "Regularization",
      "length": 129
    },
    {
      "footnote_id": "fn-minimax",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2786,
      "definition": "**Minimax**: A decision-making strategy, used in game theory and decision theory, which tries to minimize the maximum possible loss.",
      "term": "Minimax",
      "length": 132
    },
    {
      "footnote_id": "fn-principle-least-privilege",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2798,
      "definition": "**Principle of Least Privilege**: A security concept in which a user is given the minimum levels of access necessary to complete his/her job functions.",
      "term": "Principle of Least Privilege",
      "length": 151
    },
    {
      "footnote_id": "fn-data-sanitization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2802,
      "definition": "**Data Sanitization**: The process of deliberately, permanently, and irreversibly removing or destroying the data stored on a memory device to make it unrecoverable.",
      "term": "Data Sanitization",
      "length": 165
    },
    {
      "footnote_id": "fn-bayesian-neural-networks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2855,
      "definition": "**Bayesian Neural Networks**: Neural networks that incorporate probability distributions over their weights, enabling uncertainty quantification in predictions and more robust decision making.",
      "term": "Bayesian Neural Networks",
      "length": 192
    },
    {
      "footnote_id": "fn-ensemble-methods",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2857,
      "definition": "**Ensemble Methods**: An ML approach that combines several models to improve prediction accuracy.",
      "term": "Ensemble Methods",
      "length": 97
    },
    {
      "footnote_id": "fn-f1",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2928,
      "definition": "**F1 Score**: A measure of a model's accuracy that combines precision (correct positive predictions) and recall (proportion of actual positives identified) into a single metric. Calculated as the harmonic mean of precision and recall.",
      "term": "F1 Score",
      "length": 234
    },
    {
      "footnote_id": "fn-fault-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3522,
      "definition": "**Fault Models**: Formal specifications describing how hardware faults manifest and propagate through systems. Examples include stuck-at models (bits permanently 0 or 1), single-bit flip models (temporary bit inversions), and Byzantine models (arbitrary malicious behavior). Essential for designing realistic fault injection experiments.",
      "term": "Fault Models",
      "length": 337
    },
    {
      "footnote_id": "fn-fpga",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3633,
      "definition": "**Field-Programmable Gate Arrays (FPGAs)**: Reconfigurable hardware devices containing millions of logic blocks that can be programmed to implement custom digital circuits. Originally developed by Xilinx in 1985, FPGAs bridge the gap between software flexibility and hardware performance, enabling rapid prototyping and specialized accelerators.",
      "term": "Field-Programmable Gate Arrays (FPGAs)",
      "length": 345
    },
    {
      "footnote_id": "fn-beam_testing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3647,
      "definition": "**Beam Testing**: A testing method that exposes hardware to controlled particle radiation to evaluate its resilience to soft errors. Common in aerospace, medical devices, and high-reliability computing.",
      "term": "Beam Testing",
      "length": 202
    },
    {
      "footnote_id": "fn-multimodal-sensor-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3717,
      "definition": "**Multimodal Sensor Data**: Information collected simultaneously from multiple types of sensors (e.g., cameras, LiDAR, radar) to provide complementary perspectives of the environment. Critical for robust perception in autonomous systems.",
      "term": "Multimodal Sensor Data",
      "length": 237
    },
    {
      "footnote_id": "fn-gradients",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3719,
      "definition": "**Gradients and Convergence**: Core training concepts where gradients are mathematical derivatives indicating how to adjust model parameters, and convergence refers to the training process reaching a stable, optimal solution. These fundamental concepts are covered in detail in @sec-ai-training.",
      "term": "Gradients and Convergence",
      "length": 295
    },
    {
      "footnote_id": "fn-nn-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3721,
      "definition": "**Neural Network Learning Mechanisms**: The fundamental processes by which neural networks learn patterns from data, including gradient-based optimization, decision boundary formation, and high-dimensional feature representation. These core concepts are introduced comprehensively in @sec-dl-primer.",
      "term": "Neural Network Learning Mechanisms",
      "length": 299
    },
    {
      "footnote_id": "fn-nn-theory",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3723,
      "definition": "**Neural Network Theoretical Foundations**: The mathematical and algorithmic principles underlying how neural networks process information, learn representations, and make predictions in high-dimensional spaces. Complete theoretical coverage is provided in @sec-dl-primer.",
      "term": "Neural Network Theoretical Foundations",
      "length": 272
    },
    {
      "footnote_id": "fn-bayesian-nn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3725,
      "definition": "**Bayesian Neural Networks**: Advanced neural network architectures that incorporate probabilistic inference by treating weights as probability distributions rather than fixed values. This specialized approach requires understanding of basic neural network concepts covered in @sec-dl-primer.",
      "term": "Bayesian Neural Networks",
      "length": 292
    },
    {
      "footnote_id": "fn-dropout",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3727,
      "definition": "**Dropout Mechanism**: A regularization technique that randomly deactivates neurons during training to prevent overfitting and improve generalization. This method requires understanding of neural network architecture and training processes detailed in @sec-dl-primer.",
      "term": "Dropout Mechanism",
      "length": 267
    },
    {
      "footnote_id": "fn-autoencoders",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 3729,
      "definition": "**Autoencoders**: Specialized neural network architectures designed to learn efficient data representations by training to reconstruct input data from compressed encodings. This architecture requires foundational knowledge of neural networks covered in @sec-dl-primer.",
      "term": "Autoencoders",
      "length": 268
    },
    {
      "footnote_id": "fn-household-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 52,
      "definition": "**Household Energy Comparison**: The average U.S. household consumes 10,500 kWh annually (about 875 kWh monthly). Training GPT-4 consumed an estimated 50,000-100,000 MWh (50-100 million kWh), equivalent to 5-10 years of electricity for 10,000 households. This single training run used more energy than entire small cities like Aspen, Colorado (population 7,400) consume annually.",
      "term": "Household Energy Comparison",
      "length": 379
    },
    {
      "footnote_id": "fn-industry-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 54,
      "definition": "**AI vs Industrial Emissions**: Data centers (which include AI workloads) are projected to account for 8% of total power consumption by 2030, surpassing aviation (2.1%) and approaching cement production (4%). Current AI emissions already exceed those of Argentina (0.2 billion tons CO\u2082 annually). Training just the top 10 large language models in 2023 generated emissions equivalent to 40,000 round-trip flights from New York to London.",
      "term": "AI vs Industrial Emissions",
      "length": 436
    },
    {
      "footnote_id": "fn-gpu-manufacturing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 58,
      "definition": "**GPU Manufacturing Impact**: Producing a single high-end GPU like the NVIDIA H100 generates 300-500 kg of CO\u2082 before any computation occurs. Manufacturing requires 2,500+ liters of ultrapure water, 15+ rare earth elements, and energy-intensive processes reaching 1,000\u00b0C. TSMC's advanced 4nm process consumes 40% more energy per wafer than older 7nm nodes, increasing embodied carbon in AI accelerators.",
      "term": "GPU Manufacturing Impact",
      "length": 404
    },
    {
      "footnote_id": "fn-ewaste-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 60,
      "definition": "**E-Waste from Computing**: Global e-waste reached 54 million metric tons in 2019, with computing equipment contributing 15%. AI hardware accelerates this trend\u2014NVIDIA's GPU sales increased 200% from 2020-2023, with each high-end GPU weighing 2-4 lbs and containing toxic materials requiring specialized disposal. The rapid obsolescence cycle means AI hardware often becomes e-waste within 3-5 years.",
      "term": "E-Waste from Computing",
      "length": 400
    },
    {
      "footnote_id": "fn-ai-compute-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 76,
      "definition": "**AI Compute Explosion**: This 350,000\u00d7 increase represents a doubling time of approximately 3.4 months\u2014far exceeding Moore's Law's 2-year doubling cycle. For comparison, this is equivalent to going from the computational power of a smartphone to that of the world's largest supercomputer. The trend has only accelerated with large language models: GPT-4's training is estimated to have required 25\u00d7 more compute than GPT-3, while models like PaLM-2 and Claude used even more computational resources.",
      "term": "AI Compute Explosion",
      "length": 500
    },
    {
      "footnote_id": "fn-sustainable-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 80,
      "definition": "**Moore's Law Origins**: Named after Intel co-founder Gordon Moore, who made this observation in a 1965 *Electronics* magazine article, Moore's Law has driven the semiconductor industry for nearly 60 years. Moore initially predicted a doubling every year, later revised to two years. The law's economic impact is staggering: it allowed the $4 trillion global electronics industry and made possible everything from smartphones to supercomputers. However, at 3nm process nodes, individual atoms become the limiting factor.",
      "term": "Moore's Law Origins",
      "length": 520
    },
    {
      "footnote_id": "fn-dennard-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 82,
      "definition": "**Dennard Scaling**: Rule observed by IBM's Robert Dennard in 1974 that smaller transistors could run at the same power density by reducing voltage proportionally. Enabled 30 years of \"free\" performance gains until ~2005 when leakage current and voltage scaling limits ended the trend. Without Dennard scaling, modern CPUs would consume kilowatts instead of ~100W. Its end forced the shift to multi-core processors and specialized accelerators like GPUs for AI workloads.",
      "term": "Dennard Scaling",
      "length": 471
    },
    {
      "footnote_id": "fn-sustainable-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 90,
      "definition": "**GPT-3 Energy Consumption**: Training GPT-3 consumed approximately 1,287 MWh of electricity, equivalent to the annual energy consumption of 130 average American homes or the same amount of CO\u2082 as burning 500,000 pounds of coal. At average US electricity prices, this training run cost roughly $130,000 in electricity alone. GPT-4, with estimated 25\u00d7 more compute, likely consumed over 30,000 MWh\u2014enough to power a small city for a month. The energy per parameter ratio reveals hardware-software co-design inefficiencies: GPT-3's 175 billion parameters required 7.4 kWh per billion parameters, while optimized architectures can achieve sub-1 kWh ratios through mixed precision and sparsity techniques.",
      "term": "GPT-3 Energy Consumption",
      "length": 701
    },
    {
      "footnote_id": "fn-pue-metric",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 213,
      "definition": "**Power Usage Effectiveness (PUE)**: Industry standard metric calculated as Total Facility Power \u00f7 IT Equipment Power. Perfect efficiency = 1.0 (impossible), typical data centers = 1.6-2.0, Google's best facilities achieve 1.08. Each 0.1 PUE improvement saves millions in electricity costs. Facebook's Prineville data center achieves 1.09 PUE using outside air cooling. Legacy data centers often exceed 2.5 PUE.",
      "term": "Power Usage Effectiveness (PUE)",
      "length": 411
    },
    {
      "footnote_id": "fn-datacenter-emissions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 286,
      "definition": "**Data Center Climate Impact**: Data centers consume approximately 1% of global electricity and produce 0.3% of global carbon emissions directly. However, when including embodied carbon from hardware manufacturing, the figure rises to 2%. For perspective, this equals the annual emissions of Argentina (1.8% of global total) and exceeds the aviation industry's 2.1%. The largest hyperscale data centers consume over 100 MW continuously\u2014equivalent to powering 80,000 homes.",
      "term": "Data Center Climate Impact",
      "length": 472
    },
    {
      "footnote_id": "fn-hyperscale-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 292,
      "definition": "**Hyperscale Data Center Scale**: Meta's Prineville data center spans 2.5 million square feet (57 football fields) and houses 150,000+ servers. Microsoft's largest Azure data center in Iowa covers 700 acres with power capacity of 300 MW. Google operates 21 hyperscale facilities globally, consuming 12.2 TWh annually\u2014more electricity than entire countries like Lithuania or Sri Lanka.",
      "term": "Hyperscale Data Center Scale",
      "length": 384
    },
    {
      "footnote_id": "fn-transformer-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 358,
      "definition": "**Transformer + NAS Environmental Impact**: This 626,000 lbs CO\u2082 figure represents training one Transformer model while searching for optimal architecture. Includes evaluating 12,800 different model configurations over multiple days. For comparison, this equals the carbon footprint of 312 economy round-trip flights from NYC to London, or the annual emissions of 140 average Americans. Modern efficient NAS techniques have reduced this cost by 1000\u00d7.",
      "term": "Transformer + NAS Environmental Impact",
      "length": 451
    },
    {
      "footnote_id": "fn-euv-lithography",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 541,
      "definition": "**EUV Lithography**: Extreme ultraviolet light (13.5nm wavelength) used to print features smaller than 7nm on silicon chips. Each EUV machine costs $200+ million, weighs 180 tons, requires 1 MW of continuous power (enough for 800 homes), and uses 30,000 liters of ultrapure water daily. ASML is the sole global supplier. EUV enables modern AI chips but consumes 10\u00d7 more energy than older deep-UV lithography systems.",
      "term": "EUV Lithography",
      "length": 417
    },
    {
      "footnote_id": "fn-edge-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 545,
      "definition": "**Edge Computing for AI**: Processing data near its source rather than in distant cloud data centers. Reduces latency from 100-200ms (cloud) to 1-10ms (edge) for applications like autonomous vehicles. However, edge AI chips consume 5-50W continuously across billions of devices versus occasional cloud bursts. Tesla's FSD computer consumes 72W while driving; if all 1.4 billion cars had AI, collective power would equal 50 large power plants.",
      "term": "Edge Computing for AI",
      "length": 442
    },
    {
      "footnote_id": "fn-tsmc-water",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 700,
      "definition": "**Semiconductor Water Consumption Scale**: TSMC's Arizona facility will consume 3.2 billion gallons annually\u2014equivalent to 37,000 Olympic swimming pools. Each AI chip requires 5-10x more water than traditional processors due to advanced nodes and complex manufacturing. Intel's Ireland fab uses 1.5 billion gallons annually, while Samsung's Texas facility is projected to use 6 million gallons daily. Water treatment and purification add 30-50% to total consumption. During peak summer months, the cumulative daily water consumption of major fabs rivals that of cities with populations exceeding half a million people.",
      "term": "Semiconductor Water Consumption Scale",
      "length": 618
    },
    {
      "footnote_id": "fn-chemical-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 722,
      "definition": "**Hazardous Chemical Quantities**: A typical large semiconductor fab uses 500+ different chemicals annually, consuming 500-2,000 metric tons of acids, 50-200 metric tons of solvents, and 10-50 tons of toxic gases. Arsine gas is lethal at 3 parts per million over 30 minutes. TSMC's facilities store over 50,000 tons of chemicals on-site, requiring specialized emergency response teams and $100+ million in safety infrastructure per fab. Any leaks or accidental releases in fabs can lead to severe health hazards for workers and surrounding communities.",
      "term": "Hazardous Chemical Quantities",
      "length": 552
    },
    {
      "footnote_id": "fn-indium-supply",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 742,
      "definition": "**Critical Material Scarcity**: Indium production is only 600-800 tons annually worldwide, with China controlling 60% of supply. Prices fluctuate wildly\u2014from $60/kg in 2002 to $1,000/kg in 2005, now around $400/kg. Each smartphone contains 0.3mg of indium; each AI accelerator contains 50-100x more. At current AI hardware growth rates (40% annually), demand will exceed supply by 2035 without recycling breakthroughs. As AI hardware manufacturing scales, the demand for helium will continue to grow, necessitating more sustainable extraction and recycling practices.",
      "term": "Critical Material Scarcity",
      "length": 567
    },
    {
      "footnote_id": "fn-china-ree-control",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 748,
      "definition": "**Chinese Rare Earth Dominance**: China produces 85% of rare earth elements and controls 95% of global refining capacity. The 2010 China-Japan diplomatic crisis saw rare earth exports to Japan cut by 40%, causing prices to spike 2,000%. A single NVIDIA H100 contains 17 different rare earth elements totaling 200-300 grams. U.S. strategic reserves contain only 3-month supply, while building alternative supply chains requires 10-15 years and $50+ billion investment.",
      "term": "Chinese Rare Earth Dominance",
      "length": 467
    },
    {
      "footnote_id": "fn-jevons-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1094,
      "definition": "**Jevon's Paradox**: Named after British economist William Stanley Jevons who observed in 1865 that improving coal efficiency actually increased total coal consumption rather than reducing it. Modern examples include LEDs\u2014despite being 85% more efficient than incandescent bulbs, total lighting energy consumption has increased due to expanded usage. In AI, this means that making models 10\u00d7 more efficient might lead to 100\u00d7 more AI applications, resulting in net increase in environmental impact.",
      "term": "Jevon's Paradox",
      "length": 498
    },
    {
      "footnote_id": "fn-flops-vs-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1157,
      "definition": "**FLOPS vs FLOPs**: FLOPS (all caps) = Floating-Point Operations Per Second (rate), while FLOPs (mixed case) = total Floating-Point Operations (count). GPT-3 training required 3.1\u00d710\u00b2\u00b3 FLOPs total, executed on hardware capable of 1.25\u00d710\u00b9\u2077 FLOPS. Modern AI accelerators like H100 achieve 2,000 TFLOPS for AI workloads. Each ChatGPT response requires ~10\u00b9\u00b2 FLOPs\u2014roughly equivalent to your calculator performing one operation per second for 30,000 years. The energy efficiency varies dramatically across hardware: CPUs consume ~100 pJ/FLOP (100 \u00d7 10\u207b\u00b9\u00b2 J/FLOP), GPUs achieve ~10 pJ/FLOP, TPUs reach ~1 pJ/FLOP, while specialized AI accelerators approach 0.1 pJ/FLOP\u2014a 1000\u00d7 efficiency range that defines sustainability opportunities. To mitigate this inefficiency, several optimization techniques have been developed to reduce the computational overhead of AI models while maintaining accuracy.",
      "term": "FLOPS vs FLOPs",
      "length": 893
    },
    {
      "footnote_id": "fn-pue-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1203,
      "definition": "**Power Usage Effectiveness**: Data center efficiency is measured by PUE (Power Usage Effectiveness)\u2014total facility power divided by IT equipment power. Industry average PUE is 1.67 (67% overhead for cooling/infrastructure), but leading hyperscalers achieve 1.1-1.2. Google's best data centers reach PUE of 1.08, meaning only 8% energy overhead. Each 0.1 PUE improvement saves millions annually in electricity costs. The industry must adopt strategies to optimize power efficiency, integrate renewable energy sources, and improve cooling mechanisms to mitigate the environmental impact of AI infrastructure.",
      "term": "Power Usage Effectiveness",
      "length": 607
    },
    {
      "footnote_id": "fn-google-carbon-free",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1207,
      "definition": "**Google's Carbon-Free Commitment**: Google achieved carbon neutrality in 2007 and has been carbon neutral for 15 years, but 24/7 carbon-free energy is more ambitious\u2014requiring real-time matching of energy consumption with clean generation. Currently at 64% carbon-free energy globally. Denmark data centers run on 100% wind power, while others still rely on grid renewables certificates. This requires $15 billion+ investment in clean energy projects worldwide.",
      "term": "Google's Carbon-Free Commitment",
      "length": 462
    },
    {
      "footnote_id": "fn-cooling-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1211,
      "definition": "**Data Center Cooling Costs**: Cooling consumes 38% of total data center energy on average. A typical 10 MW data center spends $3.8 million annually on cooling electricity. Google's machine learning optimization reduced cooling energy by 40%, saving $150+ million globally. Liquid cooling can be 3,000x more efficient than air cooling for high-density AI workloads, reducing cooling energy from 40% to under 10% of total consumption. To improve efficiency, data centers are adopting alternative cooling strategies that reduce energy waste. Liquid cooling, which transfers heat away from AI accelerators using specially designed coolant systems, is significantly more effective than traditional air cooling and is now being deployed in high-density computing clusters. Free-air cooling, which utilizes natural airflow instead of mechanical refrigeration, has also been adopted in temperate climates, where external conditions allow for passive cooling. Microsoft has taken this a step further by deploying underwater data centers that use the surrounding ocean as a natural cooling mechanism, reducing the need for active temperature regulation.",
      "term": "Data Center Cooling Costs",
      "length": 1144
    },
    {
      "footnote_id": "fn-google-carbon-scheduling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1223,
      "definition": "**Google Carbon-Aware Scheduling Results**: Google's carbon-intelligent computing platform achieved 15% reduction in hourly carbon footprint by shifting workloads within regions. Globally shifting workloads between data centers achieved 40% reduction. The system processes 95 billion search queries daily while optimizing for grid carbon intensity. Non-urgent tasks like batch training can shift 70% of workload to lower-carbon time periods, reducing emissions equivalent to taking 50,000 cars off the road annually.",
      "term": "Google Carbon-Aware Scheduling Results",
      "length": 516
    },
    {
      "footnote_id": "fn-grid-carbon-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1229,
      "definition": "**Real-Time Grid Carbon Intensity**: Grid carbon intensity varies dramatically\u2014from 50g CO\u2082/kWh in nuclear-heavy France to 820g/kWh in coal-dependent Poland. In Texas, intensity fluctuates 10x daily (150-1,500g/kWh) based on wind generation. The Electricity Maps API serves 50+ million requests daily to allow carbon-aware computing. WattTime API provides marginal emissions data showing which power plants turn on/off next, allowing 2-5x better carbon optimization than average intensity.",
      "term": "Real-Time Grid Carbon Intensity",
      "length": 489
    },
    {
      "footnote_id": "fn-nuclear-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1239,
      "definition": "**Nuclear Power for AI Data Centers**: Microsoft partnered with Helion Energy for fusion power by 2028, signing the first commercial fusion agreement. Amazon invested $500M in small modular reactors (SMRs) for data centers. Google is exploring 24/7 nuclear partnerships with Kairos Power. Nuclear provides 20% of U.S. electricity with 12g CO\u2082/kWh lifecycle emissions versus 820-1,050g for coal. However, new nuclear costs $150-200/MWh versus $20-40 for renewables plus storage.",
      "term": "Nuclear Power for AI Data Centers",
      "length": 477
    },
    {
      "footnote_id": "fn-energy-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1245,
      "definition": "**Energy-Aware AI Frameworks**: Zeus framework achieves 75% energy savings on BERT training by automatically finding optimal energy-performance trade-offs. Perseus reduces GPU memory usage by 50% through dynamic batching, lowering energy consumption proportionally. CodeCarbon automatically tracks emissions, revealing that training can vary 10-100x in energy usage depending on optimization settings. These tools democratize energy optimization beyond just hyperscale companies.",
      "term": "Energy-Aware AI Frameworks",
      "length": 479
    },
    {
      "footnote_id": "fn-embodied-carbon",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1275,
      "definition": "**Embodied Carbon**: Carbon emissions from manufacturing, transportation, and disposal phases of a product\u2014distinct from operational emissions during use. For AI hardware, embodied carbon includes mining rare earth elements, semiconductor fabrication, packaging, and shipping. A single NVIDIA H100 GPU embodies 300-500 kg CO\u2082 before first use\u2014equivalent to 1,000-1,600 miles of driving. For comparison, the GPU's 700W power consumption generates 300 kg CO\u2082 annually (assuming average U.S. grid), meaning manufacturing emissions equal 1-2 years of operation. Research indicates that manufacturing emissions alone can account for up to 30% of an AI system's total carbon footprint, with this number potentially growing as data centers improve their reliance on renewable energy sources.",
      "term": "Embodied Carbon",
      "length": 784
    },
    {
      "footnote_id": "fn-eniac",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 152,
      "definition": "**ENIAC (Electronic Numerical Integrator and Computer)**: Completed in 1946 at the University of Pennsylvania, ENIAC weighed 30 tons, consumed 150kW of power, and performed 5,000 operations per second. Its 17,468 vacuum tubes required constant maintenance, but it demonstrated electronic computation could be 1,000x faster than mechanical calculators.",
      "term": "ENIAC (Electronic Numerical Integrator and Computer)",
      "length": 351
    },
    {
      "footnote_id": "fn-system360",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 154,
      "definition": "**IBM System/360**: Launched in 1964 as a $5 billion gamble (equivalent to $40 billion today), System/360 introduced the revolutionary concept of backward compatibility across different computer models. Its standardized instruction set architecture became the foundation for modern computing, enabling software portability that drives today's cloud computing.",
      "term": "IBM System/360",
      "length": 359
    },
    {
      "footnote_id": "fn-cdc6600",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 158,
      "definition": "**CDC 6600**: Designed by Seymour Cray and released in 1964, the CDC 6600 achieved 3 MFLOPS (million floating-point operations per second) using innovative parallel processing with 10 peripheral processors. Costing $8 million ($65 million today), it was the world's fastest computer until 1969 and established supercomputing as a field.",
      "term": "CDC 6600",
      "length": 336
    },
    {
      "footnote_id": "fn-cm5",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 160,
      "definition": "**Connection Machine CM-5**: Released by Thinking Machines in 1991, the CM-5 featured up to 16,384 processors connected by a fat-tree network, delivering over 100 GFLOPS. Its $10-50 million price tag and specialized parallel architecture made it a favorite for scientific computing but ultimately commercially unsuccessful as commodity clusters emerged.",
      "term": "Connection Machine CM-5",
      "length": 353
    },
    {
      "footnote_id": "fn-google-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 166,
      "definition": "**Google Data Centers**: Starting in 1998 with commodity PCs, Google pioneered warehouse-scale computing by 2003, managing over 100,000 servers across multiple facilities. By 2020, Google operated over 20 data centers consuming 12 TWh annually, equivalent to entire countries, while achieving industry-leading PUE (Power Usage Effectiveness) of 1.10 through innovative cooling.",
      "term": "Google Data Centers",
      "length": 377
    },
    {
      "footnote_id": "fn-training-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 172,
      "definition": "**AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet won ImageNet 2012 with 15.3% error rate (vs. 26.2% for second place), using two GTX 580 GPUs for 5-6 days of training. This breakthrough launched the deep learning revolution and demonstrated that GPUs could accelerate neural network training by 10-50x over CPUs.",
      "term": "AlexNet",
      "length": 351
    },
    {
      "footnote_id": "fn-nvidia-gpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 176,
      "definition": "**NVIDIA AI GPUs**: From the 2012 GTX 680 (3.09 TFLOPS) used for AlexNet to the 2023 H100 (989 TFLOPS for AI), NVIDIA GPUs increased AI performance by over 300x in a decade. The H100 costs $25,000-40,000 but enables training models that would be impossible on older hardware, demonstrating specialized silicon's critical role in AI advancement.",
      "term": "NVIDIA AI GPUs",
      "length": 344
    },
    {
      "footnote_id": "fn-google-tpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 178,
      "definition": "**Google TPUs**: First deployed internally in 2015, TPUs deliver 15-30x better price-performance than GPUs for specific AI workloads. The TPU v4 (2021) achieves 275 TFLOPS (bfloat16) with 32GB memory per chip, while TPU pods can scale to 1 exaFLOP. Google's $billions investment in custom silicon has enabled training models like PaLM (540B parameters) cost-effectively.",
      "term": "Google TPUs",
      "length": 370
    },
    {
      "footnote_id": "fn-training-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 213,
      "definition": "**Data Parallelism Scaling**: Linear scaling works until communication becomes the bottleneck, typically around 64-128 GPUs for most models. BERT-Large achieves 76x speedup on 128 GPUs (59% efficiency), while GPT-3 required 1,024 GPUs with only 45% efficiency. The key constraint is AllReduce communication cost scales as O(n) with number of devices, requiring high-bandwidth interconnects like InfiniBand.",
      "term": "Data Parallelism Scaling",
      "length": 406
    },
    {
      "footnote_id": "fn-training-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 215,
      "definition": "**Model Parallelism Memory Scaling**: Enables training models too large for single GPUs. GPT-3 (175B parameters) needs 350GB for weights in FP16 (700GB in FP32), far exceeding any single GPU's 80GB maximum. However, model parallelism often achieves only 20-60% compute efficiency due to sequential dependencies between model partitions and communication overhead between devices.",
      "term": "Model Parallelism Memory Scaling",
      "length": 379
    },
    {
      "footnote_id": "fn-strassen-algorithm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 266,
      "definition": "**Strassen's Algorithm**: Developed by Volker Strassen in 1969, this breakthrough reduced matrix multiplication from O(n\u00b3) to O(n^2.807) by using clever algebraic tricks with 7 multiplications instead of 8. While theoretically faster, it's only practical for matrices larger than 500\u00d7500 due to overhead. Modern implementations in libraries like Intel MKL switch between algorithms based on matrix size, demonstrating how theoretical advances require careful engineering for practical impact.",
      "term": "Strassen's Algorithm",
      "length": 492
    },
    {
      "footnote_id": "fn-batching-transformation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 278,
      "definition": "**Batching in Neural Networks**: Unlike traditional programming where data is processed one item at a time, ML systems process multiple examples simultaneously to maximize GPU utilization. A single example might achieve only 5-10% GPU utilization, while batches of 32-256 can reach 80-95%. This shift from scalar to tensor operations explains why ML systems require different programming patterns and hardware optimizations than traditional applications.",
      "term": "Batching in Neural Networks",
      "length": 454
    },
    {
      "footnote_id": "gradient-clipping",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 446,
      "definition": "**Gradient Clipping**: Technique that caps gradient values during backpropagation to prevent exploding gradients. Typically implemented by scaling gradients when their norm exceeds a threshold (e.g., clipping at norm 1.0), essential for training RNNs and transformers where gradients can grow exponentially through layers.",
      "term": "Gradient Clipping",
      "length": 322
    },
    {
      "footnote_id": "batch-norm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 448,
      "definition": "**Batch Normalization**: A technique that standardizes the inputs to each neural network layer by centering (subtracting the mean) and scaling (dividing by standard deviation) the activations within each training batch. After normalization, it applies learnable parameters to preserve the network's expressive power. This addresses \"internal covariate shift\"\u2014the problem where layer inputs constantly change distribution during training, making optimization harder. Benefits include enabling higher learning rates and providing regularization (reducing overfitting), though it requires extra memory to store statistics and becomes complex in distributed settings where batches are split across devices.",
      "term": "Batch Normalization",
      "length": 702
    },
    {
      "footnote_id": "fn-sigmoid-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 524,
      "definition": "**Sigmoid Computational Cost**: Computing sigmoid requires expensive exponential operations\u2014on CPU, `exp()` takes 10-20 clock cycles vs. 1 cycle for basic arithmetic. GPU implementations use 32-entry lookup tables with linear interpolation, reducing cost to 3-4 cycles but still 3x slower than ReLU. This overhead compounds in deep networks with millions of activations per forward pass.",
      "term": "Sigmoid Computational Cost",
      "length": 387
    },
    {
      "footnote_id": "fn-relu-hardware",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 540,
      "definition": "**ReLU Hardware Efficiency**: ReLU requires just 1 instruction (`max(0,x)`) vs. sigmoid's 10+ operations including exponentials. On NVIDIA GPUs, ReLU runs at 95% of peak FLOPS while sigmoid achieves only 30-40%. ReLU's sparsity (typically 50% zeros) enables additional optimizations: sparse matrix operations, reduced memory bandwidth, and compressed gradients during backpropagation.",
      "term": "ReLU Hardware Efficiency",
      "length": 384
    },
    {
      "footnote_id": "fn-sgd-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 610,
      "definition": "**Stochastic Gradient Descent**: Originally developed by Robbins and Monro in 1951 for statistical optimization, SGD was first applied to neural networks by Rosenblatt for the perceptron in 1958. The method remained largely theoretical until the 1980s when computational constraints made full-batch gradient descent impractical for larger networks. Today's \"mini-batch SGD\" (processing 32-512 examples) represents a compromise between the original single-example approach and full-batch methods, enabling parallel processing on modern GPUs. The stochastic nature of these updates introduces noise into the optimization process, but this noise often helps escape local minima and reach better solutions.",
      "term": "Stochastic Gradient Descent",
      "length": 702
    },
    {
      "footnote_id": "fn-training-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 704,
      "definition": "**Mixed-Precision Training**: Introduced by NVIDIA in 2018, this technique uses FP16 for forward/backward passes while maintaining FP32 precision for loss scaling, enabling 2x memory savings and 1.6x speedups on Tensor Core GPUs while maintaining model accuracy.",
      "term": "Mixed-Precision Training",
      "length": 262
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 863,
      "definition": "**Backpropagation Algorithm**: Independently rediscovered multiple times, backpropagation was popularized by Rumelhart, Hinton, and Williams in 1986 (though similar ideas appeared in Werbos 1974). This breakthrough enabled training of deep networks by efficiently computing gradients in O(n) time vs. naive O(n\u00b2) approaches. Modern implementations require careful memory management\u2014storing all activations for a ResNet-50 consumes 1.2GB per image.",
      "term": "Backpropagation Algorithm",
      "length": 447
    },
    {
      "footnote_id": "fn-autodiff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 889,
      "definition": "**Automatic Differentiation**: Not to be confused with symbolic or numerical differentiation, autodiff constructs a computational graph at runtime and applies the chain rule systematically. PyTorch uses \"define-by-run\" (dynamic graphs built during forward pass) while TensorFlow v1 used static graphs. This enables complex architectures like RNNs and transformers where graph structure changes dynamically, but requires careful memory management since the entire forward computation graph must be preserved for the backward pass.",
      "term": "Automatic Differentiation",
      "length": 529
    },
    {
      "footnote_id": "fn-etl-elt-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1246,
      "definition": "**ETL vs ELT in ML**: Traditional data warehousing used ETL (extract, transform, load) with expensive transformation on powerful central servers. Modern ML systems often prefer ELT (extract, load, transform) where raw data is loaded first, then transformed on-demand during training. This shift enables data augmentation (rotating images, adding noise) to create virtually unlimited training variations from the same source data, a technique impossible in traditional ETL where transformations are fixed.",
      "term": "ETL vs ELT in ML",
      "length": 504
    },
    {
      "footnote_id": "fn-memory-hierarchy-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1283,
      "definition": "**Memory Hierarchy in ML**: Unlike traditional CPU programs that focus on cache locality, ML training creates massive data flows between storage (TB datasets), system RAM (GB models), and GPU memory (GB activations). The 1000x bandwidth gap between storage (1-2 GB/s) and GPU memory (900+ GB/s) forces ML systems to use sophisticated prefetching and caching strategies. Traditional cache optimization (spatial/temporal locality) is less relevant than managing bulk data transfers efficiently.",
      "term": "Memory Hierarchy in ML",
      "length": 492
    },
    {
      "footnote_id": "fn-fp16-range",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 1953,
      "definition": "**FP16 Dynamic Range**: IEEE 754 half-precision (FP16) has only 5 exponent bits vs. 8 in FP32, limiting its range to \u00b165,504 (vs. \u00b13.4\u00d710\u00b3\u2078 for FP32). More critically, FP16's smallest representable positive number is 6\u00d710\u207b\u2078, while gradients in deep networks often fall below 10\u207b\u00b9\u2070. This mismatch causes gradient underflow, where tiny but important gradients become zero, stalling training\u2014hence the need for loss scaling techniques. Once the gradients are computed, the scaling factor is reversed during the weight update step to restore the original gradient magnitude. This process allows FP16 to be used effectively without sacrificing numerical stability.",
      "term": "FP16 Dynamic Range",
      "length": 659
    },
    {
      "footnote_id": "fn-gradient-accumulation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2213,
      "definition": "**Gradient Accumulation Impact**: Enables effective batch sizes of 2048+ on single GPUs with only 32-64 mini-batch size, essential for transformer training. BERT-Large training uses effective batch size of 256 (accumulated over 8 steps) achieving 99.5% of full-batch performance while reducing memory requirements by 8x. The technique trades 10-15% compute overhead for massive memory savings.",
      "term": "Gradient Accumulation Impact",
      "length": 393
    },
    {
      "footnote_id": "fn-training-activation-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2215,
      "definition": "**Activation Checkpointing Trade-offs**: Reduces memory usage by 50-90% at the cost of 15-30% additional compute time due to recomputation. For training GPT-3 on V100s, checkpointing enables 2.8x larger models (from 1.3B to 3.7B parameters) within 32GB memory constraints, making it essential for memory-bound large model training despite the compute penalty.",
      "term": "Activation Checkpointing Trade-offs",
      "length": 359
    },
    {
      "footnote_id": "fn-transformer-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2233,
      "definition": "**Transformer Batch Size Scaling**: Research shows transformers achieve optimal performance with batch sizes of 256-4096 tokens, requiring gradient accumulation on most hardware. GPT-2 training improved perplexity by 0.3-0.5 points when increasing from batch size 32 to 512, demonstrating the critical importance of large effective batch sizes for language model convergence.",
      "term": "Transformer Batch Size Scaling",
      "length": 375
    },
    {
      "footnote_id": "fn-distributed-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2307,
      "definition": "**Distributed Training**: Google's DistBelief (2012) pioneered large-scale distributed neural network training, enabling models with billions of parameters across thousands of machines. This breakthrough led to modern frameworks like Horovod (2017) and PyTorch's DistributedDataParallel, democratizing distributed training for researchers worldwide.",
      "term": "Distributed Training",
      "length": 349
    },
    {
      "footnote_id": "nvlink",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2313,
      "definition": "**NVLink**: NVIDIA's proprietary high-speed interconnect providing up to 600 GB/s bidirectional bandwidth between GPUs, roughly 10x faster than PCIe Gen4. Essential for efficient multi-GPU training as it enables rapid gradient synchronization and tensor exchanges between devices.",
      "term": "NVLink",
      "length": 280
    },
    {
      "footnote_id": "nccl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2315,
      "definition": "**NCCL (NVIDIA Collective Communications Library)**: Optimized library implementing efficient collective operations (AllReduce, Broadcast, etc.) for multi-GPU and multi-node communication. Automatically selects optimal communication patterns based on hardware topology, critical for distributed training performance.",
      "term": "NCCL (NVIDIA Collective Communications Library)",
      "length": 316
    },
    {
      "footnote_id": "fn-allreduce-algorithm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2756,
      "definition": "**AllReduce Algorithm**: A collective communication primitive where each process contributes data and all processes receive the same combined result (typically a sum). Naive implementations require O(n\u00b2) messages for n devices. The ring-allreduce algorithm, developed for high-performance computing in the 1980s, reduces this to O(n) by organizing devices in a logical ring where each device communicates only with its neighbors, making it scalable for modern ML with hundreds of GPUs.",
      "term": "AllReduce Algorithm",
      "length": 485
    },
    {
      "footnote_id": "fn-profiling-tools",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3347,
      "definition": "**Training Profiling Tools**: NVIDIA Nsight Systems can identify that data loading consumes 20-40% of training time in poorly optimized pipelines, while TensorFlow Profiler reveals GPU utilization rates (optimal: >90%). Intel VTune showed that memory bandwidth often limits performance more than raw compute\u2014typical deep learning workloads achieve only 30-50% of peak FLOPS due to memory bottlenecks.",
      "term": "Training Profiling Tools",
      "length": 400
    },
    {
      "footnote_id": "fn-lr-schedules",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3369,
      "definition": "**Learning Rate Schedules**: Critical for training stability and convergence. Cosine annealing (introduced in 2016) and linear warmup (from BERT 2018) became standard after showing 2-5% accuracy improvements. Large batch training requires linear scaling rule: multiply learning rate by batch size ratio (batch 512 \u2192 LR 0.1, batch 4096 \u2192 LR 0.8), discovered through extensive experimentation by Facebook and Google teams.",
      "term": "Learning Rate Schedules",
      "length": 420
    },
    {
      "footnote_id": "fn-nccl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3389,
      "definition": "**NVIDIA NCCL (Collective Communications Library)**: Optimized for multi-GPU communication, NCCL achieves 90-95% of theoretical bandwidth on modern interconnects. On DGX systems with NVLink, NCCL can transfer 600 GB/s between 8 GPUs\u201450x faster than PCIe\u2014making efficient distributed training possible. It implements optimized AllReduce algorithms that reduce communication from O(n\u00b2) to O(n).",
      "term": "NVIDIA NCCL (Collective Communications Library)",
      "length": 392
    },
    {
      "footnote_id": "fn-training-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3391,
      "definition": "**GPT-3 Training Scale**: Used 10,000 NVIDIA V100 GPUs for 3-4 months, consuming ~1,287 MWh of energy (equivalent to 120 US homes for a year). The training cost was estimated at $4-12 million, demonstrating how specialized hardware and distributed systems enable training at previously impossible scales while highlighting the enormous resource requirements.",
      "term": "GPT-3 Training Scale",
      "length": 358
    },
    {
      "footnote_id": "fn-training-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3395,
      "definition": "**Tensor Cores**: Introduced with NVIDIA's Volta architecture (2017), Tensor Cores deliver 4x speedup for mixed-precision training by performing 4x4 matrix operations in a single clock cycle. The H100's 4th-gen Tensor Cores achieve 989 TFLOPS for FP16 operations\u2014roughly 6x faster than traditional CUDA cores\u2014enabling training of larger models with the same hardware budget.",
      "term": "Tensor Cores",
      "length": 374
    },
    {
      "footnote_id": "fn-cuda-programming",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3401,
      "definition": "**CUDA Programming Model**: Introduced by NVIDIA in 2007, CUDA (Compute Unified Device Architecture) transformed GPUs from graphics processors into general-purpose parallel computing platforms. Unlike CPU programming with 4-16 cores, CUDA enables programming thousands of lightweight threads (32 threads per \"warp\"). ML frameworks like PyTorch and TensorFlow abstract away CUDA complexity, but understanding concepts like memory coalescing, shared memory, and occupancy remains crucial for optimizing custom ML operations.",
      "term": "CUDA Programming Model",
      "length": 522
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3417,
      "definition": "**Systolic Array Architecture**: Developed at Carnegie Mellon in 1978, systolic arrays excel at matrix operations by streaming data through a grid of processing elements. Google's TPU v4 systolic array performs 275 TFLOPS while consuming only 175W\u2014achieving 1.57 TFLOPS/W efficiency, roughly 2-3x more energy-efficient than comparable GPUs for ML workloads.",
      "term": "Systolic Array Architecture",
      "length": 357
    },
    {
      "footnote_id": "fn-fpga-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3435,
      "definition": "**Microsoft FPGA Deployment**: Project Catapult deployed FPGAs across Microsoft's entire datacenter fleet by 2016, with one FPGA per server (>1 million total). This $1 billion investment improved Bing search latency by 50% and Azure ML inference by 2x, while reducing power consumption by 10-15% through specialized acceleration of specific algorithms.",
      "term": "Microsoft FPGA Deployment",
      "length": 352
    },
    {
      "footnote_id": "fn-wse-specs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3457,
      "definition": "**Wafer-Scale Engine Specifications**: The WSE-2 (2021) contains 2.6 trillion transistors on a 21cm x 21cm wafer\u2014the largest chip ever manufactured. With 850,000 cores and 40GB on-chip memory, it delivers 15-20x speedup vs. GPU clusters for large language models while consuming 15kW (comparable to 16-20 V100 GPUs but with orders of magnitude less communication overhead).",
      "term": "Wafer-Scale Engine Specifications",
      "length": 373
    },
    {
      "footnote_id": "fn-transformers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3503,
      "definition": "**Transformer Architectures**: Detailed in @sec-dnn-architectures. Transformer models use attention mechanisms to process sequences without recurrence, enabling parallel computation and capturing long-range dependencies more effectively than RNNs.",
      "term": "Transformer Architectures",
      "length": 247
    },
    {
      "footnote_id": "fn-transformer-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3505,
      "definition": "**Transformer Training**: Large transformer models like GPT and BERT require specialized training techniques covered in @sec-dnn-architectures, including attention computation optimization and sequence parallelism strategies.",
      "term": "Transformer Training",
      "length": 225
    },
    {
      "footnote_id": "fn-rnns",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3507,
      "definition": "**Recurrent Neural Networks**: Architecture details covered in @sec-dnn-architectures. RNNs process sequential data by maintaining hidden states that capture information from previous time steps.",
      "term": "Recurrent Neural Networks",
      "length": 195
    },
    {
      "footnote_id": "fn-cnns",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3509,
      "definition": "**Convolutional Neural Networks**: Architecture and convolution operation details covered in @sec-dnn-architectures. CNNs use shared kernels to detect spatial patterns in grid-structured data like images.",
      "term": "Convolutional Neural Networks",
      "length": 204
    },
    {
      "footnote_id": "fn-rnns-lstms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3511,
      "definition": "**RNNs and LSTMs**: Long Short-Term Memory networks are specialized RNN variants designed to handle long-range dependencies. Both architectures are detailed in @sec-dnn-architectures.",
      "term": "RNNs and LSTMs",
      "length": 183
    },
    {
      "footnote_id": "fn-transformer-attention",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3513,
      "definition": "**Transformer Attention**: The attention mechanism in transformers computes weighted relationships between all positions in a sequence simultaneously. This architecture is covered in @sec-dnn-architectures.",
      "term": "Transformer Attention",
      "length": 206
    },
    {
      "footnote_id": "fn-convolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3515,
      "definition": "**Convolutional Operations**: Convolution operations apply learned filters across spatial dimensions to detect features. The mathematical details and implementation considerations are covered in @sec-dnn-architectures.",
      "term": "Convolutional Operations",
      "length": 218
    },
    {
      "footnote_id": "fn-attention-mechanisms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3517,
      "definition": "**Attention Mechanisms**: Attention allows models to focus on relevant parts of input sequences when making predictions. The mathematical formulation and architectural implementations are detailed in @sec-dnn-architectures.",
      "term": "Attention Mechanisms",
      "length": 223
    },
    {
      "footnote_id": "fn-mlops-maturity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 140,
      "definition": "**MLOps Maturity Models**: Organizations typically progress through 5 maturity levels, from manual processes (Level 0) to fully automated ML pipelines (Level 4). Google's MLOps maturity model [@kreuzberger2023machine], published in 2021, shows that approximately 20-25% of organizations reach Level 3+ automation, while 75-80% remain in manual or semi-automated processes. Companies at higher maturity levels report 35% faster time-to-market and 50% fewer production incidents, but require 18-24 months and significant cultural changes to advance between levels.",
      "term": "MLOps Maturity Models",
      "length": 562
    },
    {
      "footnote_id": "fn-cicd-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 154,
      "definition": "**CI/CD for Machine Learning**: Traditional continuous integration is designed for deterministic builds where code changes produce predictable outputs. ML systems violate this assumption because model behavior depends on training data, random initialization, and hardware differences. Google's TFX (TensorFlow Extended) [@baylor2017tfx] and similar platforms had to reinvent CI/CD principles for ML, introducing concepts like \"model validation\" and \"data validation\" that have no equivalent in traditional software. Survey data shows that 78% of ML teams report that their traditional DevOps tools are inadequate for ML workflows [@sculley2015hidden].",
      "term": "CI/CD for Machine Learning",
      "length": 651
    },
    {
      "footnote_id": "fn-data-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 156,
      "definition": "**Data Evolution in Production**: Unlike traditional software where inputs are static, ML system inputs evolve continuously\u2014user behavior changes, market conditions shift, and sensor data drifts. Netflix reports that their recommendation models see approximately 10-15% of features become stale monthly [@netflix2012recommendation], while financial fraud detection models experience 30-40% feature drift quarterly [@stripe2019machine]. This constant evolution means ML systems require \"data testing\" pipelines that validate 200+ statistical properties of incoming data, a complexity absent in traditional software where input validation involves simple type checking [@breck2017ml].",
      "term": "Data Evolution in Production",
      "length": 682
    },
    {
      "footnote_id": "fn-fraud-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 186,
      "definition": "**ML-Based Fraud Detection Evolution**: Traditional rule-based fraud systems (developed in the 1990s) had 60-70% accuracy and generated 20-30% false positives, causing customer friction. Modern ML fraud detection, pioneered by companies like PayPal (2000s) and Stripe (2010s), achieves 95%+ accuracy with <1% false positive rates by analyzing 500+ behavioral features in real-time [@stripe2019machine]. However, this improvement comes with new challenges: fraudsters adapt to ML patterns within 3-6 months, requiring continuous model retraining that rule-based systems never needed [@stripe2019machine].",
      "term": "ML-Based Fraud Detection Evolution",
      "length": 603
    },
    {
      "footnote_id": "fn-workflow-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 192,
      "definition": "**Data Versioning Challenges**: Unlike code, which changes through discrete edits, data can change gradually through drift, suddenly through schema changes, or subtly through quality degradation. Traditional version control systems like Git struggle with large datasets, leading to specialized tools like Git LFS (Large File Storage, 2015) and DVC (Data Version Control, 2017). Studies show that 87% of ML projects fail due to data issues, not algorithmic problems\u2014highlighting why ML workflows must treat data with the same rigor as code.",
      "term": "Data Versioning Challenges",
      "length": 539
    },
    {
      "footnote_id": "fn-dr-statistics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 291,
      "definition": "**Diabetic Retinopathy Global Impact**: Affects over 103 million people worldwide, with 28.5% of diabetic patients developing some form of retinopathy [@who2019classification]. In developing countries, up to 90% of vision loss from diabetes is preventable with early detection, but access to ophthalmologists remains severely limited\u2014rural areas in India have one ophthalmologist per 120,000 people, compared to the WHO recommendation of 1 per 20,000 [@who2019classification]. This stark disparity makes AI-assisted screening not just convenient but potentially life-changing for millions [@rajkomar2019machine].",
      "term": "Diabetic Retinopathy Global Impact",
      "length": 612
    },
    {
      "footnote_id": "fn-healthcare-ai-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 299,
      "definition": "**Healthcare AI Deployment Reality**: Studies show that 75-80% of healthcare AI projects never reach clinical deployment [@chen2019machine], with the majority failing not due to algorithmic issues but due to integration challenges, regulatory hurdles, and workflow disruption. The \"AI chasm\" between research success and clinical adoption is particularly wide in healthcare\u2014while medical AI papers show 95%+ accuracy rates, real-world implementation studies report significant performance drops due to data drift, equipment variations, and user acceptance issues [@kelly2019key].",
      "term": "Healthcare AI Deployment Reality",
      "length": 579
    },
    {
      "footnote_id": "fn-problem-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 309,
      "definition": "**ML vs. Traditional Problem Definition**: Traditional software problems are defined by deterministic specifications (\"if input X, then output Y\"), but ML problems are defined by examples and desired behaviors. This shift means that 73% of ML project failures occur during problem definition, compared to only 32% for traditional software [@standish2020chaos]. The challenge lies in translating business objectives into learning objectives\u2014something that didn't exist in software engineering until the rise of data-driven systems in the 2000s [@amershi2019software].",
      "term": "ML vs. Traditional Problem Definition",
      "length": 566
    },
    {
      "footnote_id": "fn-scaling-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 339,
      "definition": "**ML System Scaling Complexity**: Scaling ML systems is exponentially more complex than traditional software due to data heterogeneity, model drift, and infrastructure requirements. Studies show that ML systems require 10x more monitoring infrastructure than traditional applications [@paleyes2022challenges], with companies like Uber running 1,000+ model quality checks daily across their ML platform [@uber2017michelangelo]. The \"scaling wall\" typically hits at 100+ models in production, where manual processes break down and teams need specialized MLOps platforms\u2014explaining why the ML platform market grew from approximately $350M in 2019 to $4.0B in 2023, with pure MLOps tools reaching $1.2B in 2023 [@kreuzberger2023machine].",
      "term": "ML System Scaling Complexity",
      "length": 733
    },
    {
      "footnote_id": "fn-data-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 349,
      "definition": "**The 80/20 Rule in ML**: Data scientists spend 80% of their time on data collection, cleaning, and preparation\u2014only 20% on actual modeling. This ratio, first documented by CrowdFlower [@crowdflower2016data] in 2016, remains consistent across industries despite advances in automated tools. The \"data preparation tax\" includes handling missing values (present in 90% of real-world datasets), resolving inconsistencies (affecting 60% of data fields), and ensuring legal compliance (requiring 15+ different consent mechanisms for EU data). This explains why successful ML teams invest heavily in data engineering capabilities from day one.",
      "term": "The 80/20 Rule in ML",
      "length": 637
    },
    {
      "footnote_id": "fn-medical-annotation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 357,
      "definition": "**Medical Data Annotation Costs**: Expert medical annotation is extraordinarily expensive\u2014ophthalmologists charge $200-500 per hour, meaning the DR dataset's annotation cost exceeded $2.7 million in expert time alone. This represents one of the highest annotation costs per sample in ML history. For comparison, ImageNet's 14 million images cost approximately $50,000 to annotate using crowdsourcing, while medical datasets can cost 100-1000x more per image. This cost disparity explains why medical AI often relies on transfer learning and why synthetic data generation is becoming crucial for healthcare applications.",
      "term": "Medical Data Annotation Costs",
      "length": 619
    },
    {
      "footnote_id": "fn-federated-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 389,
      "definition": "**Federated Learning Architecture**: Federated learning [@mcmahan2017communication], introduced by Google in 2016 for mobile keyboards, enables training across distributed data sources without centralizing data. Healthcare applications are particularly suited for federated learning due to privacy regulations\u2014studies show federated medical models achieve 85-95% of centralized model accuracy while keeping data local. However, federated learning introduces new challenges: communication costs increase 100-1000x per training iteration, and statistical heterogeneity across sites can cause model convergence issues that centralized training doesn't face.",
      "term": "Federated Learning Architecture",
      "length": 654
    },
    {
      "footnote_id": "fn-algorithmic-fairness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 454,
      "definition": "**Algorithmic Fairness in Healthcare**: Medical AI systems show significant performance disparities across demographic groups\u2014dermatology AI trained on light-skinned patients shows 36% worse accuracy on dark-skinned patients [@larson2017gender], while diabetic retinopathy models trained primarily on European populations show 15-25% accuracy drops for Asian and African populations [@gulshan2016deep]. The FDA's 2021 Action Plan for AI/ML-based medical devices now requires demographic performance reporting [@fda2021artificial], and companies like Google Health spend 20-30% of development resources on fairness testing and bias mitigation across racial, gender, and socioeconomic groups [@rajkomar2019machine].",
      "term": "Algorithmic Fairness in Healthcare",
      "length": 713
    },
    {
      "footnote_id": "fn-hyperparameter-tuning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 460,
      "definition": "**Hyperparameter Optimization Complexity**: Modern deep learning models have 10-100+ hyperparameters (learning rate, batch size, architecture choices), creating search spaces with 10^20+ possible combinations. AutoML platforms like Google's AutoML and H2O spend $10,000-100,000 in compute costs to find optimal configurations for complex models. Random search (2012) surprisingly outperforms grid search, while Bayesian optimization (2010s) and population-based training (2017) represent current state-of-the-art, reducing tuning time by 10-100x but still requiring substantial computational resources that didn't exist in traditional software development.",
      "term": "Hyperparameter Optimization Complexity",
      "length": 656
    },
    {
      "footnote_id": "fn-workflow-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 468,
      "definition": "**Transfer Learning**: A technique where models pre-trained on large datasets (like ImageNet's 14 million images) are adapted for specific tasks, dramatically reducing training time and data requirements [@krizhevsky2012imagenet; @deng2009imagenet]. Introduced by Yann LeCun's team in the 1990s and popularized by the 2014 ImageNet competition, transfer learning became the foundation for most practical computer vision applications. Instead of training from scratch, practitioners can achieve expert-level performance with thousands rather than millions of training examples.",
      "term": "Transfer Learning",
      "length": 576
    },
    {
      "footnote_id": "fn-medical-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 480,
      "definition": "**Medical AI Performance Metrics**: Medical AI requires different metrics than general ML\u2014sensitivity (true positive rate) and specificity (true negative rate) are often more important than overall accuracy. For diabetic retinopathy screening, >90% sensitivity is crucial (missing cases causes blindness), while >80% specificity prevents unnecessary referrals. Medical AI also requires metrics like positive predictive value (PPV) and negative predictive value (NPV) that vary with disease prevalence in different populations\u2014a model with 95% accuracy in a lab setting might have only 50% PPV in a low-prevalence population, making it clinically useless despite high technical performance.",
      "term": "Medical AI Performance Metrics",
      "length": 689
    },
    {
      "footnote_id": "fn-data-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 560,
      "definition": "**Data Drift Detection**: Data drift occurs when input data characteristics change over time\u2014user behavior shifts, sensor calibration drifts, or population demographics evolve. Studies show that 78% of production ML models experience significant data drift within 12 months [@breck2017ml], yet only 23% of organizations have automated drift detection [@paleyes2022challenges]. Statistical tests like Kolmogorov-Smirnov and Population Stability Index can detect drift, but require setting thresholds and monitoring 100+ features continuously. Cloud providers now offer drift detection services (AWS SageMaker Model Monitor, Google AI Platform), but custom implementation remains necessary for domain-specific requirements.",
      "term": "Data Drift Detection",
      "length": 721
    },
    {
      "footnote_id": "fn-model-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 562,
      "definition": "**Model Drift Phenomenon**: ML models degrade over time without any code changes\u2014a phenomenon unknown in traditional software. Studies show that 50-80% of production ML models experience significant performance degradation within 6 months due to data drift, concept drift, or infrastructure drift [@polyzotis2019data]. This \"silent failure\" problem led to the development of specialized monitoring tools like Evidently AI (2020) and Fiddler (2018), creating an entirely new category of ML infrastructure that has no equivalent in traditional software engineering.",
      "term": "Model Drift Phenomenon",
      "length": 563
    },
    {
      "footnote_id": "fn-deployment-reality-gap",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 574,
      "definition": "**The Lab-to-Clinic Performance Gap**: Medical AI systems typically see 10-30% performance drops when deployed in real-world settings, a phenomenon known as the \"deployment reality gap.\" This occurs because training data, despite best efforts, cannot capture the full diversity of real-world conditions\u2014different camera models, varying image quality, diverse patient populations, and operator skill levels all contribute to this gap. The gap is so consistent that regulatory bodies like the FDA now require \"real-world performance studies\" for medical AI approval, acknowledging that laboratory performance is insufficient to predict clinical utility.",
      "term": "The Lab-to-Clinic Performance Gap",
      "length": 651
    },
    {
      "footnote_id": "fn-ml-team-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 658,
      "definition": "**ML Team Role Evolution**: The \"data scientist\" role only emerged around 2008 (coined by DJ Patil and Jeff Hammerbacher at Facebook and LinkedIn), while \"ML engineer\" became common around 2015 as companies realized that research models need production engineering. \"MLOps engineer\" appeared around 2018, and \"AI ethics officer\" became standard at major tech companies by 2020. This rapid role specialization reflects ML's evolution from research curiosity to production necessity\u2014modern enterprise ML teams average 8-12 distinct roles compared to 2-3 in traditional software teams.",
      "term": "ML Team Role Evolution",
      "length": 582
    }
  ]
}