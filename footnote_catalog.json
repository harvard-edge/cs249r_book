{
  "total_files": 65,
  "total_references": 765,
  "total_definitions": 383,
  "patterns": {
    "total_definitions": 383,
    "with_bold_terms": 383,
    "average_length": 383,
    "common_prefixes": {
      "fn": 383
    },
    "terms_used": [
      "tpu origins",
      "parameters",
      "tinyml",
      "data center climate impact",
      "billion-parameter models",
      "federated learning birth",
      "mnist",
      "batch processing evolution",
      "energy-aware ai frameworks",
      "latency-critical applications",
      "model parallelism memory scaling",
      "endpoint device constraints",
      "ml autoscaling at scale",
      "cosmic ray impact",
      "tensorflow lite",
      "3dmark",
      "hardware lottery",
      "sgd (stochastic gradient descent)",
      "google tpus",
      "mechanical turk origins",
      "microsoft farmbeats",
      "tensor processing unit (tpu)",
      "lapack (linear algebra package)",
      "hdfs origins",
      "medical imaging ai revolution",
      "feature store scale",
      "system design trade-offs",
      "activation checkpointing",
      "service level agreements (slas)",
      "data parallelism scaling",
      "data-centric ai",
      "adversarial training overhead",
      "ml reproducibility crisis",
      "microsoft fpga deployment",
      "memory optimization",
      "self-supervised learning",
      "tensorrt optimization",
      "imagenet",
      "hyperscale data centers",
      "sigmoid computational cost",
      "tpu (tensor processing unit)",
      "hipaa violations",
      "a11 bionic breakthrough",
      "lstm origins",
      "tensor cores",
      "crisp-dm (cross-industry standard process for data mining)",
      "automatic differentiation",
      "multi-agent system",
      "data drift",
      "mosquito species detection",
      "model parallelism",
      "learning rate schedules",
      "label flipping attack impact",
      "energy efficiency in tinyml",
      "distilbert",
      "tinytl memory breakthrough",
      "plantvillage nuru real-world impact",
      "stochastic computing resilience",
      "tinyml model compression",
      "ml apis",
      "structured pruning",
      "mobilemark",
      "mlops emergence",
      "power usage effectiveness",
      "cifar-10",
      "learning rate",
      "computer engineering",
      "tinyml market reality",
      "activation checkpointing trade-offs",
      "microcontroller power budget reality",
      "active learning",
      "neural processing unit (npu)",
      "mirai botnet scale",
      "on-device training constraints",
      "memory scale comparison",
      "real-time grid carbon intensity",
      "healthcare ai deployment reality",
      "lottery ticket hypothesis",
      "columnar format revolution",
      "data as code",
      "transformer",
      "c&w attack effectiveness",
      "hyperscale data center scale",
      "dram bit flip rates",
      "support vector machines (svms)",
      "cloud infrastructure evolution",
      "jupyter notebooks",
      "von neumann architecture",
      "flops",
      "data centers",
      "parameter scaling",
      "e-waste from computing",
      "curriculum learning",
      "memory hierarchy challenge",
      "cudnn",
      "etl evolution",
      "batch size effects",
      "privacy regulation timeline",
      "transfer learning",
      "arduino edge computing reality",
      "smpc performance",
      "onnx deployment",
      "coin-cell batteries",
      "parameter-efficient fine-tuning",
      "scaling laws",
      "arm cortex architecture spectrum",
      "model extraction threat",
      "gpt-3 energy consumption",
      "sdg global impact",
      "ai vs industrial emissions",
      "differential privacy",
      "ml vs. traditional problem definition",
      "computational photography",
      "google's carbon-free commitment",
      "voice recognition evolution",
      "hazardous chemical quantities",
      "defensive distillation effectiveness",
      "bert compression",
      "tvm (tensor virtual machine)",
      "adversarial patch success",
      "mobile system-on-chip",
      "ensemble methods",
      "ibm watson health",
      "prometheus at scale",
      "activation function",
      "sequential neural networks",
      "online learning vulnerability",
      "speculative execution",
      "youtube recommendation impact",
      "2014-2016 ebola outbreak",
      "fab water usage vs cities",
      "application-specific integrated circuit (asic)",
      "jax",
      "data center cooling costs",
      "data augmentation",
      "medical data annotation costs",
      "covid-19 ml impact",
      "mini-batch gpu optimization",
      "the lab-to-clinic performance gap",
      "spec cpu",
      "quantization-aware training",
      "fault injection tool ecosystem",
      "iot device growth",
      "stuck-at faults in ml",
      "ultra-long battery life",
      "data lake origins",
      "edge latency advantage",
      "alexnet's gpu revolution",
      "adam (adaptive moment estimation)",
      "concept drift challenge",
      "kubernetes origins",
      "operator fusion",
      "spec power",
      "data poisoning detection challenge",
      "tensor core breakthrough",
      "gdpr's ml impact",
      "backdoor attack effectiveness",
      "risc-v for ai",
      "intel 8087 impact",
      "eniac (electronic numerical integrator and computer)",
      "ibm system/360",
      "gdpr",
      "alexnet",
      "dartmouth conference (1956)",
      "dp-sgd industry adoption",
      "social good resource paradox",
      "model drift detection",
      "\"hey siri\" technical reality",
      "fdiv bug economic impact",
      "relu hardware efficiency",
      "overfitting",
      "single vs multi-bit fault impact",
      "jenkins origins",
      "hardware vs software injection",
      "hsm performance",
      "stop sign attack precision",
      "model quantization",
      "model drift phenomenon",
      "jevon's paradox",
      "adversarial examples discovery",
      "production alert thresholds",
      "blas (basic linear algebra subprograms)",
      "convolutional neural network (cnn)",
      "backpropagation algorithm",
      "ci/cd for machine learning",
      "mnist dataset",
      "shap in production",
      "tensorflow serving",
      "puf market growth",
      "gradient descent",
      "model inversion attack",
      "xla (accelerated linear algebra)",
      "gpus for deep learning",
      "medical ai privacy complexity",
      "mathematical convolution",
      "mobile device constraints",
      "feature store evolution",
      "brain energy efficiency",
      "rmsprop (root mean square propagation)",
      "microcontrollers",
      "training-serving skew impact",
      "systems integration philosophy",
      "systems thinking in ai",
      "ml hardware cost spectrum",
      "black box",
      "training profiling tools",
      "pgd attack strength",
      "transformer batch size scaling",
      "kserve (formerly kfserving)",
      "cray-1 vector legacy",
      "core ml",
      "tensor operations",
      "mixed-precision training",
      "artificial neurons",
      "wireless communication reality",
      "mobilenet innovation",
      "uci machine learning repository",
      "simd evolution",
      "gdpr article 22",
      "mlperf",
      "pytorch",
      "intel sgx constraints",
      "edge computing",
      "fbnet",
      "sparse energy savings",
      "gpt-4",
      "perplexity",
      "compas algorithm controversy",
      "mobile power constraints",
      "pruning",
      "moore's law",
      "nuclear power for ai data centers",
      "wafer-scale engine specifications",
      "smallholder farmers global impact",
      "adversarial transferability",
      "distributed training",
      "green500",
      "edge processor",
      "resnet revolution",
      "principal component analysis (pca)",
      "ai compute explosion",
      "data parallelism",
      "eliza",
      "tinyml device scale",
      "nas evaluation metrics",
      "efficientnet pruning",
      "generative ai breakthrough",
      "yann lecun and cnns",
      "efficientnet",
      "cassava disease impact",
      "viola-jones algorithm",
      "project natick underwater data centers",
      "diabetic retinopathy global impact",
      "knowledge distillation",
      "quantization",
      "cascade of classifiers",
      "resnet",
      "backpropagation (historical context)",
      "zero-day etymology",
      "lora technology",
      "global fishing watch impact",
      "paradigm shift",
      "industrial iot",
      "depthwise separable convolutions",
      "model scaling explosion",
      "household energy comparison",
      "inference attack",
      "autoregressive",
      "gpu manufacturing impact",
      "synapses",
      "squeezenet",
      "nvidia nccl (collective communications library)",
      "systolic array architecture",
      "vision transformers (vits)",
      "nvidia triton inference server",
      "stuxnet discovery",
      "bert",
      "cdc 6600",
      "large-scale training challenges",
      "cloud inference latency",
      "critical material scarcity",
      "foundation models",
      "demographic parity origins",
      "mobilenet",
      "data poisoning emergence",
      "gpt-3",
      "cough analysis technology",
      "cloudsuite",
      "energy efficiency metrics",
      "github actions for ml",
      "tensor processing units",
      "computational graphs",
      "pay-as-you-go pricing",
      "stochastic gradient descent (sgd)",
      "chinese rare earth dominance",
      "dhrystone",
      "mlops business impact",
      "energy star",
      "netflix deanonymization",
      "nlp computational demands",
      "edge computing origins",
      "electromigration in ai hardware",
      "brittleness in ai systems",
      "data versioning challenges",
      "apple neural engine evolution",
      "tokens",
      "ml security threat taxonomy",
      "basic linear algebra subprograms (blas)",
      "stm32f4 microcontroller reality",
      "data quality reality",
      "value alignment problem",
      "semiconductor water consumption scale",
      "tensorflow",
      "silent data corruption challenge",
      "ml team role evolution",
      "hardware-aware nas",
      "experiment tracking evolution",
      "healthcare algorithm scale",
      "esp32 edge computing",
      "safety-critical computing evolution",
      "google data centers",
      "fault injection framework evolution",
      "esp32 capabilities",
      "universal approximation theorem",
      "apple's neural engine strategy",
      "fgsm breakthrough",
      "google tensor soc architecture",
      "single event upset rates",
      "technical debt origins",
      "\"attention is all you need\"",
      "dvc creation story",
      "nvidia ai gpus",
      "federated learning scale",
      "systolic array renaissance",
      "moore's law origins",
      "devops origins",
      "arm trustzone",
      "theano",
      "linpack",
      "backpropagation",
      "imagenet revolution",
      "iot hubs",
      "mobile storage evolution",
      "connection machine cm-5",
      "gpt-3 training scale",
      "google carbon-aware scheduling results",
      "zillow ibuying failure",
      "service level objectives (slos)",
      "kubeflow production usage",
      "gradient accumulation impact",
      "whetstone",
      "meltdown/spectre impact",
      "perceptron",
      "ai's sdg impact potential",
      "cloud ml training economics"
    ]
  },
  "duplicates": {
    "duplicate_ids": {
      "fn-imagenet": [
        "benchmarking",
        "introduction",
        "efficient_ai"
      ],
      "fn-alexnet": [
        "benchmarking",
        "introduction",
        "training",
        "efficient_ai"
      ],
      "fn-resnet": [
        "benchmarking",
        "efficient_ai"
      ],
      "fn-tpu": [
        "dl_primer",
        "efficient_ai",
        "benchmarking",
        "frameworks",
        "ml_systems",
        "introduction"
      ],
      "fn-gpt3": [
        "benchmarking",
        "efficient_ai"
      ],
      "fn-mixed-precision": [
        "benchmarking",
        "training"
      ],
      "fn-data-versioning": [
        "workflow",
        "data_engineering"
      ],
      "fn-backpropagation": [
        "dnn_architectures",
        "training",
        "dl_primer"
      ],
      "fn-flops": [
        "dl_primer",
        "efficient_ai"
      ],
      "fn-blas": [
        "dnn_architectures",
        "frameworks"
      ],
      "fn-model-parallelism": [
        "training",
        "efficient_ai"
      ],
      "fn-sgd": [
        "training",
        "efficient_ai"
      ],
      "fn-pruning": [
        "optimizations",
        "efficient_ai"
      ],
      "fn-quantization": [
        "ml_systems",
        "efficient_ai"
      ],
      "fn-moores-law": [
        "sustainable_ai",
        "efficient_ai"
      ],
      "fn-data-parallelism": [
        "training",
        "efficient_ai"
      ],
      "fn-transfer-learning": [
        "introduction",
        "workflow",
        "efficient_ai"
      ],
      "fn-foundation-models": [
        "introduction",
        "efficient_ai"
      ],
      "fn-tensor-cores": [
        "hw_acceleration",
        "training"
      ],
      "fn-neural-engine": [
        "ondevice_learning",
        "hw_acceleration"
      ],
      "fn-activation-checkpointing": [
        "optimizations",
        "training"
      ]
    },
    "duplicate_terms": {
      "imagenet": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-imagenet"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-imagenet"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-imagenet"
        }
      ],
      "alexnet": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-alexnet"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-alexnet"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-alexnet"
        },
        {
          "file": "training",
          "footnote_id": "fn-alexnet"
        }
      ],
      "resnet": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-resnet"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-resnet"
        }
      ],
      "tensor processing unit (tpu)": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-tpu"
        },
        {
          "file": "dl_primer",
          "footnote_id": "fn-tpu"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-tpu"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-tpu"
        },
        {
          "file": "ml_systems",
          "footnote_id": "fn-tpu"
        }
      ],
      "gpt-3": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-gpt3"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-gpt3"
        }
      ],
      "mixed-precision training": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-mixed-precision"
        },
        {
          "file": "training",
          "footnote_id": "fn-mixed-precision"
        }
      ],
      "data parallelism": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-data-parallel"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-data-parallelism"
        }
      ],
      "model parallelism": [
        {
          "file": "benchmarking",
          "footnote_id": "fn-model-parallel"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-model-parallelism"
        }
      ],
      "data versioning challenges": [
        {
          "file": "data_engineering",
          "footnote_id": "fn-data-versioning"
        },
        {
          "file": "workflow",
          "footnote_id": "fn-data-versioning"
        }
      ],
      "perceptron": [
        {
          "file": "dl_primer",
          "footnote_id": "fn-perceptron"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-early"
        }
      ],
      "backpropagation": [
        {
          "file": "dl_primer",
          "footnote_id": "fn-backpropagation"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-backprop"
        }
      ],
      "flops": [
        {
          "file": "dl_primer",
          "footnote_id": "fn-flops"
        },
        {
          "file": "efficient_ai",
          "footnote_id": "fn-flops"
        }
      ],
      "backpropagation algorithm": [
        {
          "file": "dnn_architectures",
          "footnote_id": "fn-backpropagation"
        },
        {
          "file": "training",
          "footnote_id": "fn-backpropagation"
        }
      ],
      "pruning": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-pruning"
        },
        {
          "file": "optimizations",
          "footnote_id": "fn-pruning"
        }
      ],
      "knowledge distillation": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-knowledge-distillation"
        },
        {
          "file": "optimizations",
          "footnote_id": "fn-knowledge-distill"
        }
      ],
      "moore's law": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-moores-law"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-mooreslaw"
        }
      ],
      "transfer learning": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-transfer-learning"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-transfer-learning"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-transfer"
        },
        {
          "file": "workflow",
          "footnote_id": "fn-transfer-learning"
        }
      ],
      "foundation models": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-foundation-models"
        },
        {
          "file": "introduction",
          "footnote_id": "fn-foundation-models"
        }
      ],
      "tinyml": [
        {
          "file": "efficient_ai",
          "footnote_id": "fn-tinyml"
        },
        {
          "file": "ml_systems",
          "footnote_id": "fn-tinyml-origin"
        }
      ],
      "adversarial examples discovery": [
        {
          "file": "responsible_ai",
          "footnote_id": "fn-adversarial-examples"
        },
        {
          "file": "robust_ai",
          "footnote_id": "fn-adversarial-discovery"
        }
      ]
    },
    "undefined_references": [],
    "unused_definitions": []
  },
  "by_chapter": [
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "chapter_name": "ai_for_good",
      "total_references": 26,
      "total_definitions": 13,
      "footnote_ids": [
        "fn-cassava-impact",
        "fn-cough-detection",
        "fn-ebola-outbreak",
        "fn-esp32-constraints",
        "fn-farmbeats",
        "fn-global-fishing",
        "fn-lora-technology",
        "fn-mosquito-detection",
        "fn-plantvillage-nuru",
        "fn-resource-paradox",
        "fn-sdg-adoption",
        "fn-sdg-ai-potential",
        "fn-smallholder-farmers"
      ],
      "terms_defined": [
        "2014-2016 Ebola Outbreak",
        "AI's SDG Impact Potential",
        "Cassava Disease Impact",
        "Cough Analysis Technology",
        "ESP32 Capabilities",
        "Global Fishing Watch Impact",
        "LoRa Technology",
        "Microsoft FarmBeats",
        "Mosquito Species Detection",
        "PlantVillage Nuru Real-World Impact",
        "SDG Global Impact",
        "Smallholder Farmers Global Impact",
        "Social Good Resource Paradox"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "chapter_name": "benchmarking",
      "total_references": 48,
      "total_definitions": 24,
      "footnote_ids": [
        "fn-3dmark",
        "fn-alexnet",
        "fn-asic",
        "fn-bert",
        "fn-cloudsuite",
        "fn-cudnn",
        "fn-data-parallel",
        "fn-dhrystone",
        "fn-energy-star",
        "fn-gpt3",
        "fn-green500",
        "fn-hardware-lottery",
        "fn-imagenet",
        "fn-linpack",
        "fn-mixed-precision",
        "fn-mlperf",
        "fn-mobilemark",
        "fn-model-parallel",
        "fn-resnet",
        "fn-spec",
        "fn-spec-power",
        "fn-tensor-ops",
        "fn-tpu",
        "fn-whetstone"
      ],
      "terms_defined": [
        "3DMark",
        "AlexNet",
        "Application-Specific Integrated Circuit (ASIC)",
        "BERT",
        "CloudSuite",
        "Data Parallelism",
        "Dhrystone",
        "ENERGY STAR",
        "GPT-3",
        "Green500",
        "Hardware Lottery",
        "ImageNet",
        "LINPACK",
        "MLPerf",
        "Mixed-Precision Training",
        "MobileMark",
        "Model Parallelism",
        "ResNet",
        "SPEC CPU",
        "SPEC Power",
        "Tensor Operations",
        "Tensor Processing Unit (TPU)",
        "Whetstone",
        "cuDNN"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "chapter_name": "conclusion",
      "total_references": 6,
      "total_definitions": 3,
      "footnote_ids": [
        "fn-data-new-code",
        "fn-generative-ai-timeline",
        "fn-systems-integration"
      ],
      "terms_defined": [
        "Data as Code",
        "Generative AI Breakthrough",
        "Systems Integration Philosophy"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "chapter_name": "data_engineering",
      "total_references": 28,
      "total_definitions": 14,
      "footnote_ids": [
        "fn-batch-processing",
        "fn-columnar-formats",
        "fn-concept-drift",
        "fn-data-lake-origins",
        "fn-data-quality-stats",
        "fn-data-versioning",
        "fn-etl-history",
        "fn-feature-stores",
        "fn-hdfs-origins",
        "fn-mechanical-turk",
        "fn-medical-imaging",
        "fn-model-scaling",
        "fn-privacy-regulations",
        "fn-watson-health"
      ],
      "terms_defined": [
        "Batch Processing Evolution",
        "Columnar Format Revolution",
        "Concept Drift Challenge",
        "Data Lake Origins",
        "Data Quality Reality",
        "Data Versioning Challenges",
        "ETL Evolution",
        "Feature Store Evolution",
        "HDFS Origins",
        "IBM Watson Health",
        "Mechanical Turk Origins",
        "Medical Imaging AI Revolution",
        "Model Scaling Explosion",
        "Privacy Regulation Timeline"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "chapter_name": "dl_primer",
      "total_references": 18,
      "total_definitions": 9,
      "footnote_ids": [
        "fn-backpropagation",
        "fn-brain-efficiency",
        "fn-flops",
        "fn-gradient-descent",
        "fn-learning-rate",
        "fn-overfitting",
        "fn-perceptron",
        "fn-synapses",
        "fn-tpu"
      ],
      "terms_defined": [
        "Backpropagation",
        "Brain Energy Efficiency",
        "FLOPS",
        "Gradient Descent",
        "Learning Rate",
        "Overfitting",
        "Perceptron",
        "Synapses",
        "Tensor Processing Unit (TPU)"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "chapter_name": "dnn_architectures",
      "total_references": 26,
      "total_definitions": 13,
      "footnote_ids": [
        "fn-attention-is-all-you-need",
        "fn-backpropagation",
        "fn-blas",
        "fn-convolution-origin",
        "fn-imagenet-breakthrough",
        "fn-lecun-cnn",
        "fn-lstm-invention",
        "fn-mnist-dataset",
        "fn-parameter-scaling",
        "fn-resnet-breakthrough",
        "fn-tpu-development",
        "fn-uat",
        "fn-vision-transformers"
      ],
      "terms_defined": [
        "\"Attention is All You Need\"",
        "Backpropagation Algorithm",
        "Basic Linear Algebra Subprograms (BLAS)",
        "ImageNet Revolution",
        "LSTM Origins",
        "MNIST Dataset",
        "Mathematical Convolution",
        "Parameter Scaling",
        "ResNet Revolution",
        "Tensor Processing Units",
        "Universal Approximation Theorem",
        "Vision Transformers (ViTs)",
        "Yann LeCun and CNNs"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "chapter_name": "efficient_ai",
      "total_references": 76,
      "total_definitions": 38,
      "footnote_ids": [
        "fn-active-learning",
        "fn-alexnet",
        "fn-autoregressive",
        "fn-cifar10",
        "fn-curriculum-learning",
        "fn-data-augmentation",
        "fn-data-centric-ai",
        "fn-data-parallelism",
        "fn-efficientnet",
        "fn-ensemble",
        "fn-flops",
        "fn-foundation-models",
        "fn-gpt3",
        "fn-gpt4",
        "fn-gpu-deep-learning",
        "fn-imagenet",
        "fn-knowledge-distillation",
        "fn-mnist",
        "fn-mobilenet",
        "fn-model-parallelism",
        "fn-moores-law",
        "fn-param-efficient",
        "fn-pca",
        "fn-perplexity",
        "fn-pruning",
        "fn-quantization",
        "fn-resnet",
        "fn-scaling-laws",
        "fn-self-supervised",
        "fn-sgd",
        "fn-squeezenet",
        "fn-svms",
        "fn-tinyml",
        "fn-tokens",
        "fn-tpu",
        "fn-transfer-learning",
        "fn-transformer",
        "fn-uci"
      ],
      "terms_defined": [
        "Active Learning",
        "AlexNet",
        "Autoregressive",
        "CIFAR-10",
        "Curriculum Learning",
        "Data Augmentation",
        "Data Parallelism",
        "Data-Centric AI",
        "EfficientNet",
        "Ensemble Methods",
        "FLOPs",
        "Foundation Models",
        "GPT-3",
        "GPT-4",
        "GPUs for Deep Learning",
        "ImageNet",
        "Knowledge Distillation",
        "MNIST",
        "MobileNet",
        "Model Parallelism",
        "Moore's Law",
        "Parameter-Efficient Fine-tuning",
        "Perplexity",
        "Principal Component Analysis (PCA)",
        "Pruning",
        "Quantization",
        "ResNet",
        "Scaling Laws",
        "Self-Supervised Learning",
        "SqueezeNet",
        "Stochastic Gradient Descent (SGD)",
        "Support Vector Machines (SVMs)",
        "Tensor Processing Unit (TPU)",
        "TinyML",
        "Tokens",
        "Transfer Learning",
        "Transformer",
        "UCI Machine Learning Repository"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "chapter_name": "frameworks",
      "total_references": 18,
      "total_definitions": 9,
      "footnote_ids": [
        "fn-auto-diff",
        "fn-blas",
        "fn-comp-graphs",
        "fn-jax",
        "fn-lapack",
        "fn-pytorch",
        "fn-tensorflow",
        "fn-theano",
        "fn-tpu"
      ],
      "terms_defined": [
        "Automatic Differentiation",
        "BLAS (Basic Linear Algebra Subprograms)",
        "Computational Graphs",
        "JAX",
        "LAPACK (Linear Algebra Package)",
        "PyTorch",
        "TPU (Tensor Processing Unit)",
        "TensorFlow",
        "Theano"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "chapter_name": "hw_acceleration",
      "total_references": 22,
      "total_definitions": 11,
      "footnote_ids": [
        "fn-alexnet-gpu",
        "fn-cray-vector",
        "fn-intel-8087",
        "fn-memory-hierarchy",
        "fn-neural-engine",
        "fn-risc-v-ai",
        "fn-simd-evolution",
        "fn-systolic-origin",
        "fn-tensor-cores",
        "fn-tpu-origin",
        "fn-von-neumann"
      ],
      "terms_defined": [
        "AlexNet's GPU Revolution",
        "Apple's Neural Engine Strategy",
        "Cray-1 Vector Legacy",
        "Intel 8087 Impact",
        "Memory Hierarchy Challenge",
        "RISC-V for AI",
        "SIMD Evolution",
        "Systolic Array Renaissance",
        "TPU Origins",
        "Tensor Core Breakthrough",
        "Von Neumann Architecture"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "chapter_name": "introduction",
      "total_references": 57,
      "total_definitions": 28,
      "footnote_ids": [
        "fn-activation",
        "fn-alexnet",
        "fn-backprop",
        "fn-backprop-history",
        "fn-black-box",
        "fn-brittleness",
        "fn-cascade",
        "fn-cnn",
        "fn-computer-engineering",
        "fn-dartmouth-conference",
        "fn-drift",
        "fn-early",
        "fn-edge",
        "fn-eliza",
        "fn-foundation-models",
        "fn-imagenet",
        "fn-inference",
        "fn-mas",
        "fn-mooreslaw",
        "fn-neurons",
        "fn-paradigm-shift",
        "fn-parameters",
        "fn-rnn",
        "fn-tpu",
        "fn-training-challenges",
        "fn-transfer",
        "fn-transfer-learning",
        "fn-viola-jones"
      ],
      "terms_defined": [
        "Activation Function",
        "AlexNet",
        "Artificial Neurons",
        "Backpropagation",
        "Backpropagation (Historical Context)",
        "Black Box",
        "Brittleness in AI Systems",
        "Cascade of Classifiers",
        "Computer Engineering",
        "Convolutional Neural Network (CNN)",
        "Dartmouth Conference (1956)",
        "Data Drift",
        "ELIZA",
        "Edge Processor",
        "Foundation Models",
        "ImageNet",
        "Inference Attack",
        "Large-Scale Training Challenges",
        "Moore's Law",
        "Multi-Agent System",
        "Paradigm Shift",
        "Parameters",
        "Perceptron",
        "Sequential Neural Networks",
        "Tensor Processing Unit (TPU)",
        "Transfer Learning",
        "Transfer Learning",
        "Viola-Jones Algorithm"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "chapter_name": "ml_systems",
      "total_references": 74,
      "total_definitions": 37,
      "footnote_ids": [
        "fn-battery-life",
        "fn-billion-parameters",
        "fn-cloud-evolution",
        "fn-coin-cell",
        "fn-computational-photography",
        "fn-coreml",
        "fn-cost-spectrum",
        "fn-data-centers",
        "fn-design-tradeoffs",
        "fn-device-size",
        "fn-edge-computing",
        "fn-edge-latency",
        "fn-endpoint-constraints",
        "fn-energy-efficiency",
        "fn-hyperscale",
        "fn-industrial-iot",
        "fn-inference-latency",
        "fn-iot-growth",
        "fn-iot-hubs",
        "fn-latency-critical",
        "fn-memory-comparison",
        "fn-microcontrollers",
        "fn-ml-apis",
        "fn-mobile-constraints",
        "fn-mobile-power",
        "fn-mobile-soc",
        "fn-mobile-storage",
        "fn-model-compression",
        "fn-nlp-compute",
        "fn-npu",
        "fn-on-device-training",
        "fn-paas-pricing",
        "fn-quantization",
        "fn-tflite",
        "fn-tinyml-origin",
        "fn-tpu",
        "fn-voice-recognition"
      ],
      "terms_defined": [
        "Billion-Parameter Models",
        "Cloud Inference Latency",
        "Cloud Infrastructure Evolution",
        "Coin-Cell Batteries",
        "Computational Photography",
        "Core ML",
        "Data Centers",
        "Edge Computing",
        "Edge Latency Advantage",
        "Endpoint Device Constraints",
        "Energy Efficiency in TinyML",
        "Hyperscale Data Centers",
        "Industrial IoT",
        "IoT Device Growth",
        "IoT Hubs",
        "Latency-Critical Applications",
        "ML APIs",
        "ML Hardware Cost Spectrum",
        "Memory Scale Comparison",
        "Microcontrollers",
        "Mobile Device Constraints",
        "Mobile Power Constraints",
        "Mobile Storage Evolution",
        "Mobile System-on-Chip",
        "Model Quantization",
        "NLP Computational Demands",
        "Neural Processing Unit (NPU)",
        "On-Device Training Constraints",
        "Pay-as-You-Go Pricing",
        "System Design Trade-offs",
        "Tensor Processing Unit (TPU)",
        "TensorFlow Lite",
        "TinyML",
        "TinyML Device Scale",
        "TinyML Model Compression",
        "Ultra-Long Battery Life",
        "Voice Recognition Evolution"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "chapter_name": "ondevice_learning",
      "total_references": 34,
      "total_definitions": 17,
      "footnote_ids": [
        "fn-a11-bionic-breakthrough",
        "fn-arduino-constraints",
        "fn-arm-cortex-spectrum",
        "fn-depthwise-separable",
        "fn-edge-computing-origin",
        "fn-esp32-capabilities",
        "fn-federated-birth",
        "fn-gdpr-impact",
        "fn-hey-siri-constraints",
        "fn-microcontroller-power",
        "fn-mobilenet-innovation",
        "fn-neural-engine",
        "fn-stm32-constraints",
        "fn-tensor-soc",
        "fn-tinyml-scale",
        "fn-tinytl-efficiency",
        "fn-wireless-constraints"
      ],
      "terms_defined": [
        "\"Hey Siri\" Technical Reality",
        "A11 Bionic Breakthrough",
        "ARM Cortex Architecture Spectrum",
        "Apple Neural Engine Evolution",
        "Arduino Edge Computing Reality",
        "Depthwise Separable Convolutions",
        "ESP32 Edge Computing",
        "Edge Computing Origins",
        "Federated Learning Birth",
        "GDPR's ML Impact",
        "Google Tensor SoC Architecture",
        "Microcontroller Power Budget Reality",
        "MobileNet Innovation",
        "STM32F4 Microcontroller Reality",
        "TinyML Market Reality",
        "TinyTL Memory Breakthrough",
        "Wireless Communication Reality"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "chapter_name": "ops",
      "total_references": 52,
      "total_definitions": 26,
      "footnote_ids": [
        "fn-alerting-thresholds",
        "fn-cloud-ml-costs",
        "fn-covid-impact",
        "fn-devops-origins",
        "fn-drift-detection",
        "fn-dvc-story",
        "fn-feature-store-scale",
        "fn-github-actions-ml",
        "fn-jenkins-history",
        "fn-kserve-scaling",
        "fn-kubeflow-scale",
        "fn-kubernetes-birth",
        "fn-ml-autoscaling",
        "fn-mlops-business-impact",
        "fn-mlops-emergence",
        "fn-prometheus-scale",
        "fn-reproducibility-crisis",
        "fn-shap-adoption",
        "fn-sla-examples",
        "fn-slo-reality",
        "fn-tech-debt-origin",
        "fn-tensorflow-serving",
        "fn-training-serving-skew",
        "fn-triton-performance",
        "fn-youtube-engagement",
        "fn-zillow-losses"
      ],
      "terms_defined": [
        "COVID-19 ML Impact",
        "Cloud ML Training Economics",
        "DVC Creation Story",
        "DevOps Origins",
        "Feature Store Scale",
        "GitHub Actions for ML",
        "Jenkins Origins",
        "KServe (formerly KFServing)",
        "Kubeflow Production Usage",
        "Kubernetes Origins",
        "ML Autoscaling at Scale",
        "ML Reproducibility Crisis",
        "MLOps Business Impact",
        "MLOps Emergence",
        "Model Drift Detection",
        "NVIDIA Triton Inference Server",
        "Production Alert Thresholds",
        "Prometheus at Scale",
        "SHAP in Production",
        "Service Level Agreements (SLAs)",
        "Service Level Objectives (SLOs)",
        "Technical Debt Origins",
        "TensorFlow Serving",
        "Training-Serving Skew Impact",
        "YouTube Recommendation Impact",
        "Zillow iBuying Failure"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "chapter_name": "optimizations",
      "total_references": 42,
      "total_definitions": 21,
      "footnote_ids": [
        "fn-activation-checkpointing",
        "fn-batch-size-effects",
        "fn-bert-compression",
        "fn-distilbert-metrics",
        "fn-efficientnet-pruning",
        "fn-energy-efficiency-metrics",
        "fn-fbnet-nas",
        "fn-hardware-aware-nas",
        "fn-knowledge-distill",
        "fn-lottery-ticket",
        "fn-memory-optimization",
        "fn-nas-evaluation-metrics",
        "fn-onnx-deployment",
        "fn-operator-fusion",
        "fn-pruning",
        "fn-qat-performance",
        "fn-sparse-energy-savings",
        "fn-structured-pruning",
        "fn-tensorrt-optimization",
        "fn-tvm-compiler",
        "fn-xla-compiler"
      ],
      "terms_defined": [
        "Activation Checkpointing",
        "BERT Compression",
        "Batch Size Effects",
        "DistilBERT",
        "EfficientNet Pruning",
        "Energy Efficiency Metrics",
        "FBNet",
        "Hardware-Aware NAS",
        "Knowledge Distillation",
        "Lottery Ticket Hypothesis",
        "Memory Optimization",
        "NAS Evaluation Metrics",
        "ONNX Deployment",
        "Operator Fusion",
        "Pruning",
        "Quantization-Aware Training",
        "Sparse Energy Savings",
        "Structured Pruning",
        "TVM (Tensor Virtual Machine)",
        "TensorRT Optimization",
        "XLA (Accelerated Linear Algebra)"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "chapter_name": "privacy_security",
      "total_references": 34,
      "total_definitions": 17,
      "footnote_ids": [
        "fn-dp-sgd-adoption",
        "fn-federated-learning-scale",
        "fn-gdpr-penalties",
        "fn-hipaa-violations",
        "fn-hsm-performance",
        "fn-intel-sgx-limits",
        "fn-meltdown-spectre-impact",
        "fn-mirai-scale",
        "fn-model-extraction-2016",
        "fn-model-inversion-attack",
        "fn-netflix-deanonymization",
        "fn-puf-adoption",
        "fn-smpc-overhead",
        "fn-speculative-execution",
        "fn-stuxnet-discovery",
        "fn-trustzone-adoption",
        "fn-zero-day-term"
      ],
      "terms_defined": [
        "ARM TrustZone",
        "DP-SGD Industry Adoption",
        "Federated Learning Scale",
        "GDPR",
        "HIPAA Violations",
        "HSM Performance",
        "Intel SGX Constraints",
        "Meltdown/Spectre Impact",
        "Mirai Botnet Scale",
        "Model Extraction Threat",
        "Model Inversion Attack",
        "Netflix Deanonymization",
        "PUF Market Growth",
        "SMPC Performance",
        "Speculative Execution",
        "Stuxnet Discovery",
        "Zero-Day Etymology"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "chapter_name": "responsible_ai",
      "total_references": 14,
      "total_definitions": 7,
      "footnote_ids": [
        "fn-adversarial-examples",
        "fn-compas-bias",
        "fn-demographic-parity-origin",
        "fn-differential-privacy",
        "fn-gdpr-article-22",
        "fn-healthcare-algorithm-bias",
        "fn-value-alignment"
      ],
      "terms_defined": [
        "Adversarial Examples Discovery",
        "COMPAS Algorithm Controversy",
        "Demographic Parity Origins",
        "Differential Privacy",
        "GDPR Article 22",
        "Healthcare Algorithm Scale",
        "Value Alignment Problem"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "chapter_name": "robust_ai",
      "total_references": 55,
      "total_definitions": 28,
      "footnote_ids": [
        "fn-adversarial-discovery",
        "fn-adversarial-training-cost",
        "fn-backdoor-success-rate",
        "fn-bit-flip-dram",
        "fn-cosmic-ray-rate",
        "fn-cw-attack-power",
        "fn-data-poisoning-tay",
        "fn-defensive-distillation",
        "fn-electromigration-ai",
        "fn-fdiv-cost",
        "fn-fgsm-breakthrough",
        "fn-fidelity-tool-adoption",
        "fn-hardware-injection-accuracy",
        "fn-label-flip-impact",
        "fn-ml-security-threats",
        "fn-modern-fault-frameworks",
        "fn-online-poisoning-vulnerability",
        "fn-patch-effectiveness",
        "fn-pgd-benchmark",
        "fn-poisoning-detection-difficulty",
        "fn-safety-critical-evolution",
        "fn-sdc-scale",
        "fn-seu-space-rate",
        "fn-single-vs-multi-bit",
        "fn-stochastic-computing",
        "fn-stop-sign-attack",
        "fn-stuck-at-ml-impact",
        "fn-transferability-rates"
      ],
      "terms_defined": [
        "Adversarial Examples Discovery",
        "Adversarial Patch Success",
        "Adversarial Training Overhead",
        "Adversarial Transferability",
        "Backdoor Attack Effectiveness",
        "C&W Attack Effectiveness",
        "Cosmic Ray Impact",
        "DRAM Bit Flip Rates",
        "Data Poisoning Detection Challenge",
        "Data Poisoning Emergence",
        "Defensive Distillation Effectiveness",
        "Electromigration in AI Hardware",
        "FDIV Bug Economic Impact",
        "FGSM Breakthrough",
        "Fault Injection Framework Evolution",
        "Fault Injection Tool Ecosystem",
        "Hardware vs Software Injection",
        "Label Flipping Attack Impact",
        "ML Security Threat Taxonomy",
        "Online Learning Vulnerability",
        "PGD Attack Strength",
        "Safety-Critical Computing Evolution",
        "Silent Data Corruption Challenge",
        "Single Event Upset Rates",
        "Single vs Multi-bit Fault Impact",
        "Stochastic Computing Resilience",
        "Stop Sign Attack Precision",
        "Stuck-at Faults in ML"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "chapter_name": "sustainable_ai",
      "total_references": 45,
      "total_definitions": 23,
      "footnote_ids": [
        "fn-ai-compute-growth",
        "fn-chemical-scale",
        "fn-china-ree-control",
        "fn-cooling-energy",
        "fn-datacenter-emissions",
        "fn-energy-frameworks",
        "fn-ewaste-scale",
        "fn-fab-vs-cities",
        "fn-google-carbon-free",
        "fn-google-carbon-scheduling",
        "fn-gpt3-energy",
        "fn-gpu-manufacturing",
        "fn-grid-carbon-data",
        "fn-household-energy",
        "fn-hyperscale-size",
        "fn-indium-supply",
        "fn-industry-comparison",
        "fn-jevons-paradox",
        "fn-moores-law",
        "fn-nuclear-ai",
        "fn-pue-efficiency",
        "fn-tsmc-water",
        "fn-underwater-dc"
      ],
      "terms_defined": [
        "AI Compute Explosion",
        "AI vs Industrial Emissions",
        "Chinese Rare Earth Dominance",
        "Critical Material Scarcity",
        "Data Center Climate Impact",
        "Data Center Cooling Costs",
        "E-Waste from Computing",
        "Energy-Aware AI Frameworks",
        "Fab Water Usage vs Cities",
        "GPT-3 Energy Consumption",
        "GPU Manufacturing Impact",
        "Google Carbon-Aware Scheduling Results",
        "Google's Carbon-Free Commitment",
        "Hazardous Chemical Quantities",
        "Household Energy Comparison",
        "Hyperscale Data Center Scale",
        "Jevon's Paradox",
        "Moore's Law Origins",
        "Nuclear Power for AI Data Centers",
        "Power Usage Effectiveness",
        "Project Natick Underwater Data Centers",
        "Real-Time Grid Carbon Intensity",
        "Semiconductor Water Consumption Scale"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "chapter_name": "training",
      "total_references": 60,
      "total_definitions": 30,
      "footnote_ids": [
        "fn-activation-checkpointing",
        "fn-adam",
        "fn-alexnet",
        "fn-backpropagation",
        "fn-cdc6600",
        "fn-cm5",
        "fn-data-parallelism",
        "fn-distributed-training",
        "fn-eniac",
        "fn-fpga-datacenter",
        "fn-google-datacenter",
        "fn-google-tpus",
        "fn-gpt3-training",
        "fn-gradient-accumulation",
        "fn-lr-schedules",
        "fn-minibatch-gpu",
        "fn-mixed-precision",
        "fn-model-parallelism",
        "fn-nccl",
        "fn-nvidia-gpus",
        "fn-profiling-tools",
        "fn-relu-hardware",
        "fn-rmsprop",
        "fn-sgd",
        "fn-sigmoid-cost",
        "fn-system360",
        "fn-systolic-array",
        "fn-tensor-cores",
        "fn-transformer-scaling",
        "fn-wse-specs"
      ],
      "terms_defined": [
        "Activation Checkpointing Trade-offs",
        "Adam (Adaptive Moment Estimation)",
        "AlexNet",
        "Backpropagation Algorithm",
        "CDC 6600",
        "Connection Machine CM-5",
        "Data Parallelism Scaling",
        "Distributed Training",
        "ENIAC (Electronic Numerical Integrator and Computer)",
        "GPT-3 Training Scale",
        "Google Data Centers",
        "Google TPUs",
        "Gradient Accumulation Impact",
        "IBM System/360",
        "Learning Rate Schedules",
        "Microsoft FPGA Deployment",
        "Mini-batch GPU Optimization",
        "Mixed-Precision Training",
        "Model Parallelism Memory Scaling",
        "NVIDIA AI GPUs",
        "NVIDIA NCCL (Collective Communications Library)",
        "RMSprop (Root Mean Square Propagation)",
        "ReLU Hardware Efficiency",
        "SGD (Stochastic Gradient Descent)",
        "Sigmoid Computational Cost",
        "Systolic Array Architecture",
        "Tensor Cores",
        "Training Profiling Tools",
        "Transformer Batch Size Scaling",
        "Wafer-Scale Engine Specifications"
      ]
    },
    {
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "chapter_name": "workflow",
      "total_references": 30,
      "total_definitions": 15,
      "footnote_ids": [
        "fn-cicd-ml",
        "fn-crisp-dm",
        "fn-data-versioning",
        "fn-deployment-reality-gap",
        "fn-dr-statistics",
        "fn-healthcare-ai-challenges",
        "fn-jupyter",
        "fn-medical-annotation",
        "fn-medical-privacy",
        "fn-ml-team-evolution",
        "fn-mlflow",
        "fn-model-drift",
        "fn-problem-definition",
        "fn-systems-thinking",
        "fn-transfer-learning"
      ],
      "terms_defined": [
        "CI/CD for Machine Learning",
        "CRISP-DM (Cross-Industry Standard Process for Data Mining)",
        "Data Versioning Challenges",
        "Diabetic Retinopathy Global Impact",
        "Experiment Tracking Evolution",
        "Healthcare AI Deployment Reality",
        "Jupyter Notebooks",
        "ML Team Role Evolution",
        "ML vs. Traditional Problem Definition",
        "Medical AI Privacy Complexity",
        "Medical Data Annotation Costs",
        "Model Drift Phenomenon",
        "Systems Thinking in AI",
        "The Lab-to-Clinic Performance Gap",
        "Transfer Learning"
      ]
    }
  ],
  "all_references": [
    {
      "footnote_id": "fn-ebola-outbreak",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 53,
      "context": "...ons and coordinated responses could have dramatically altered outcomes. The 2014-2016 Ebola outbreak[^fn-ebola-outbreak] in West Africa, for instance, highlighted the catastrophic consequences of delayed detection and re...",
      "full_line": "History provides sobering examples of where timely interventions and coordinated responses could have dramatically altered outcomes. The 2014-2016 Ebola outbreak[^fn-ebola-outbreak] in West Africa, for instance, highlighted the catastrophic consequences of delayed detection and response systems ([WHO](https://www.who.int/emergencies/situations/ebola-outbreak-2014-2016-West-Africa)). Similarly, the 2011 famine in Somalia, despite being forecasted months in advance, caused immense suffering due to inadequate mechanisms to mobilize and allocate resources effectively ([ReliefWeb](https://reliefweb.int/report/somalia/somalia-famine-2011-2012)). In the aftermath of the 2010 Haiti earthquake, the lack of rapid and reliable damage assessment significantly hampered efforts to direct aid where it was most needed ([USGS](https://www.usgs.gov/natural-hazards/earthquake-hazards/science/2010-haiti-earthquake-overview?qt-science_center_objects=0#qt-science_center_objects))."
    },
    {
      "footnote_id": "fn-ebola-outbreak",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 55,
      "context": "[^fn-ebola-outbreak]: **2014-2016 Ebola Outbreak**: This outbreak killed 11,323 people across six countries, with over 2...",
      "full_line": "[^fn-ebola-outbreak]: **2014-2016 Ebola Outbreak**: This outbreak killed 11,323 people across six countries, with over 28,600 cases reported. The delayed international response\u2014WHO declared a Public Health Emergency only after 5 months\u2014demonstrated how early AI-powered disease surveillance could have saved thousands of lives. The economic cost exceeded $53 billion, highlighting the need for rapid detection systems that mobile health technologies now provide."
    },
    {
      "footnote_id": "fn-smallholder-farmers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 57,
      "context": "...griculture, a sector critical to global food security, faces parallel struggles. Smallholder farmers[^fn-smallholder-farmers], responsible for producing much of the world's food, make crucial decisions with limited informatio...",
      "full_line": "Today, similar challenges persist across diverse domains, particularly in resource-constrained environments. In healthcare, remote and underserved communities often experience preventable health crises due to the absence of timely access to medical expertise. A lack of diagnostic tools and specialists means that treatable conditions can escalate into life-threatening situations, creating unnecessary suffering and loss of life. Agriculture, a sector critical to global food security, faces parallel struggles. Smallholder farmers[^fn-smallholder-farmers], responsible for producing much of the world's food, make crucial decisions with limited information."
    },
    {
      "footnote_id": "fn-smallholder-farmers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 59,
      "context": "[^fn-smallholder-farmers]: **Smallholder Farmers Global Impact**: These farmers operate plots smaller than 2 hectares but pro...",
      "full_line": "[^fn-smallholder-farmers]: **Smallholder Farmers Global Impact**: These farmers operate plots smaller than 2 hectares but produce 35% of global food supply, feeding 2 billion people directly. In sub-Saharan Africa, they comprise 80% of farms yet receive only 2% of agricultural credit. Climate change threatens their $2.6 trillion annual production value, making AI-powered agricultural support systems critical for global food security and poverty reduction. Increasingly erratic weather patterns, pest outbreaks, and soil degradation compound their difficulties, often resulting in reduced yields and heightened food insecurity, particularly in vulnerable regions. These challenges demonstrate how systemic barriers and resource constraints perpetuate inequities and undermine resilience."
    },
    {
      "footnote_id": "fn-cassava-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 77,
      "context": "...armers in disconnected regions where access to agricultural advisors is limited [@ramcharan2017deep][^fn-cassava-impact].",
      "full_line": "In Sub-Saharan Africa, cassava farmers have long battled diseases that devastate crops and livelihoods. Now, with the help of mobile ML-powered smartphone apps, as shown in @fig-plantvillage, they can snap a photo of a leaf and receive instant feedback on potential diseases. This early detection system has reportedly reduced cassava losses from 40% to just 5%, offering hope to farmers in disconnected regions where access to agricultural advisors is limited [@ramcharan2017deep][^fn-cassava-impact]."
    },
    {
      "footnote_id": "fn-cassava-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 79,
      "context": "[^fn-cassava-impact]: **Cassava Disease Impact**: Cassava feeds 800 million people globally and is a critical food secur...",
      "full_line": "[^fn-cassava-impact]: **Cassava Disease Impact**: Cassava feeds 800 million people globally and is a critical food security crop in Africa. Cassava mosaic disease (CMD) and cassava brown streak disease (CBSD) can destroy entire harvests, affecting millions of smallholder farmers. The PlantVillage Nuru app has been downloaded by over 500,000 farmers across Kenya, Tanzania, and Uganda, demonstrating how mobile ML can scale agricultural expertise to underserved communities without internet connectivity."
    },
    {
      "footnote_id": "fn-farmbeats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 83,
      "context": "...Microsoft's [FarmBeats](https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/)[^fn-farmbeats] is pioneering the integration of IoT sensors, drones, and Cloud ML to create actionable insights fo...",
      "full_line": "On a global scale, Microsoft's [FarmBeats](https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/)[^fn-farmbeats] is pioneering the integration of IoT sensors, drones, and Cloud ML to create actionable insights for farmers. By leveraging weather forecasts, soil conditions, and crop health data, the platform allows farmers to optimize inputs like water and fertilizer, reducing waste and improving yields. Together, these innovations illustrate how AI technologies are bringing precision agriculture to life, addressing food security, sustainability, and climate resilience."
    },
    {
      "footnote_id": "fn-farmbeats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 85,
      "context": "[^fn-farmbeats]: **Microsoft FarmBeats**: Launched in 2017, FarmBeats has been deployed across 25,000+ farms worldw...",
      "full_line": "[^fn-farmbeats]: **Microsoft FarmBeats**: Launched in 2017, FarmBeats has been deployed across 25,000+ farms worldwide, helping farmers reduce water usage by 30% and increase crop yields by 15-20%. The platform processes data from 50+ sensor types and can predict crop health issues 2-3 weeks before visible symptoms appear, demonstrating how Cloud ML scales agricultural expertise to underserved farming communities."
    },
    {
      "footnote_id": "fn-cough-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 89,
      "context": ".../www.samayhealth.com/) uses embedded machine learning to analyze cough patterns and detect pneumonia[^fn-cough-detection]. Designed for remote areas, the device operates independently of internet connectivity and is power...",
      "full_line": "For millions in underserved communities, access to healthcare often means long waits and travel to distant clinics. Tiny ML is changing that by enabling diagnostics to occur at the patient's side. For example, a low-cost wearable developed by [Respira x Colabs](https://www.samayhealth.com/) uses embedded machine learning to analyze cough patterns and detect pneumonia[^fn-cough-detection]. Designed for remote areas, the device operates independently of internet connectivity and is powered by a simple microcontroller, making life-saving diagnostics accessible to those who need it most."
    },
    {
      "footnote_id": "fn-cough-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 91,
      "context": "[^fn-cough-detection]: **Cough Analysis Technology**: Pneumonia kills over 800,000 children under 5 annually, with most d...",
      "full_line": "[^fn-cough-detection]: **Cough Analysis Technology**: Pneumonia kills over 800,000 children under 5 annually, with most deaths occurring in resource-poor settings lacking access to chest X-rays. Cough analysis using TinyML can achieve 90%+ accuracy in pneumonia detection by analyzing acoustic features like cough duration, frequency, and spectral characteristics. The entire model runs on a microcontroller costing less than $10, democratizing diagnostic capabilities."
    },
    {
      "footnote_id": "fn-mosquito-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 93,
      "context": "...ow-cost devices that use machine learning to identify mosquito species by their wingbeat frequencies[^fn-mosquito-detection] [@altayeb2022classifying]. This technology enables real-time monitoring of malaria-carrying mosquit...",
      "full_line": "Tiny ML's potential extends to tackling global health issues like vector-borne diseases that are spread by mosquitoes. Researchers have developed low-cost devices that use machine learning to identify mosquito species by their wingbeat frequencies[^fn-mosquito-detection] [@altayeb2022classifying]. This technology enables real-time monitoring of malaria-carrying mosquitoes. It offers a scalable solution for malaria control in high-risk regions."
    },
    {
      "footnote_id": "fn-mosquito-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 95,
      "context": "[^fn-mosquito-detection]: **Mosquito Species Detection**: Malaria affects 241 million people annually, causing 627,000 death...",
      "full_line": "[^fn-mosquito-detection]: **Mosquito Species Detection**: Malaria affects 241 million people annually, causing 627,000 deaths primarily in sub-Saharan Africa. TinyML-powered mosquito detection devices achieve 95% accuracy in species identification using just acoustic signatures, costing under $50 versus traditional morphological identification requiring $5,000+ microscopy equipment. These devices can monitor 24/7 and detect Anopheles mosquitoes (malaria vectors) versus Culex (nuisance only), enabling targeted intervention strategies."
    },
    {
      "footnote_id": "fn-global-fishing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 119,
      "context": "...r illegal fishing activities. Platforms like [Global Fishing Watch](https://globalfishingwatch.org/)[^fn-global-fishing] analyze satellite data to detect anomalies, helping governments enforce regulations and protect mar...",
      "full_line": "At a global scale, Cloud ML is being used to monitor illegal fishing activities. Platforms like [Global Fishing Watch](https://globalfishingwatch.org/)[^fn-global-fishing] analyze satellite data to detect anomalies, helping governments enforce regulations and protect marine ecosystems."
    },
    {
      "footnote_id": "fn-global-fishing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 121,
      "context": "[^fn-global-fishing]: **Global Fishing Watch Impact**: Since 2016, this platform has tracked over 70,000 vessels globall...",
      "full_line": "[^fn-global-fishing]: **Global Fishing Watch Impact**: Since 2016, this platform has tracked over 70,000 vessels globally, processing 22+ million AIS (Automatic Identification System) data points daily. The system has helped identify $1.5 billion worth of illegal fishing activities and supported enforcement actions that recovered 180+ seized vessels. By making fishing activity transparent, the platform has contributed to 20% reductions in illegal fishing in monitored regions."
    },
    {
      "footnote_id": "fn-sdg-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 137,
      "context": "The SDGs shown in @fig-sdg are a global agenda adopted in 2015[^fn-sdg-adoption]. These 17 interconnected goals form a blueprint for addressing the world's most pressing challenges...",
      "full_line": "The SDGs shown in @fig-sdg are a global agenda adopted in 2015[^fn-sdg-adoption]. These 17 interconnected goals form a blueprint for addressing the world's most pressing challenges by 2030[^fn-sdg-ai-potential]. They range from eliminating poverty and hunger to ensuring quality education, from promoting gender equality to taking climate action."
    },
    {
      "footnote_id": "fn-sdg-ai-potential",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 137,
      "context": "...17 interconnected goals form a blueprint for addressing the world's most pressing challenges by 2030[^fn-sdg-ai-potential]. They range from eliminating poverty and hunger to ensuring quality education, from promoting gende...",
      "full_line": "The SDGs shown in @fig-sdg are a global agenda adopted in 2015[^fn-sdg-adoption]. These 17 interconnected goals form a blueprint for addressing the world's most pressing challenges by 2030[^fn-sdg-ai-potential]. They range from eliminating poverty and hunger to ensuring quality education, from promoting gender equality to taking climate action."
    },
    {
      "footnote_id": "fn-sdg-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 139,
      "context": "[^fn-sdg-adoption]: **SDG Global Impact**: Adopted by all 193 UN Member States, the SDGs represent the most ambitious...",
      "full_line": "[^fn-sdg-adoption]: **SDG Global Impact**: Adopted by all 193 UN Member States, the SDGs represent the most ambitious global agenda in history, covering 169 specific targets with a $5-7 trillion annual funding gap. The goals build on the success of the Millennium Development Goals (2000-2015), which helped lift 1 billion people out of extreme poverty. Unlike their predecessors, the SDGs apply universally to all countries, recognizing that sustainable development requires global cooperation."
    },
    {
      "footnote_id": "fn-sdg-ai-potential",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 141,
      "context": "[^fn-sdg-ai-potential]: **AI's SDG Impact Potential**: McKinsey estimates AI could accelerate achievement of 134 of the 16...",
      "full_line": "[^fn-sdg-ai-potential]: **AI's SDG Impact Potential**: McKinsey estimates AI could accelerate achievement of 134 of the 169 SDG targets, potentially contributing $13 trillion to global economic output by 2030. However, 97% of AI research focuses on SDG 9 (Industry/Innovation) while only 1% addresses basic needs like water, food, and health. This maldistribution means AI systems for social good require deliberate design to address the most critical human needs rather than commercial applications."
    },
    {
      "footnote_id": "fn-resource-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 190,
      "context": "Deploying machine learning systems in social impact contexts reveals a fundamental resource paradox[^fn-resource-paradox] that shapes every aspect of system design. While areas with the greatest needs could benefit most f...",
      "full_line": "Deploying machine learning systems in social impact contexts reveals a fundamental resource paradox[^fn-resource-paradox] that shapes every aspect of system design. While areas with the greatest needs could benefit most from machine learning capabilities, they often lack the basic infrastructure required for traditional deployments."
    },
    {
      "footnote_id": "fn-resource-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 192,
      "context": "[^fn-resource-paradox]: **Social Good Resource Paradox**: Resource-constrained environments need the most help but have th...",
      "full_line": "[^fn-resource-paradox]: **Social Good Resource Paradox**: Resource-constrained environments need the most help but have the least infrastructure to deploy solutions. For example, rural sub-Saharan Africa has 60% of global arable land but only 4% of worldwide internet connectivity. This paradox forces engineers to achieve 90%+ model compression (from 50MB to 500KB) while maintaining effectiveness, a challenge absent in commercial deployments with abundant resources."
    },
    {
      "footnote_id": "fn-lora-technology",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 196,
      "context": "...Gbps). Rural deployments must instead rely on low-power wide-area network technologies such as LoRa[^fn-lora-technology] or NB-IoT, which achieve multi-kilometer range coverage with minimal power consumption.",
      "full_line": "Network infrastructure limitations further constrain system design. Urban environments offer high-bandwidth options like fiber (100+ Mbps) and 5G networks (1-10 Gbps). Rural deployments must instead rely on low-power wide-area network technologies such as LoRa[^fn-lora-technology] or NB-IoT, which achieve multi-kilometer range coverage with minimal power consumption."
    },
    {
      "footnote_id": "fn-lora-technology",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 198,
      "context": "[^fn-lora-technology]: **LoRa Technology**: Long Range (LoRa) enables IoT devices to communicate over 15+ kilometers with...",
      "full_line": "[^fn-lora-technology]: **LoRa Technology**: Long Range (LoRa) enables IoT devices to communicate over 15+ kilometers with battery life exceeding 10 years. Operating in unlicensed spectrum bands, LoRa networks cost $1-5 per device annually versus $15-50 for cellular. This makes LoRa ideal for agricultural sensors monitoring soil moisture across vast farms or environmental sensors in remote conservation areas. Over 140 countries have deployed LoRaWAN networks, connecting 200+ million devices worldwide for social good applications."
    },
    {
      "footnote_id": "fn-esp32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 216,
      "context": "...of devices, cost and power constraints often mandate the use of microcontroller units like the ESP32[^fn-esp32-constraints], a widely used microcontroller unit from Espressif Systems, with its 240 MHz processor and mere 520...",
      "full_line": "Production deployments reveal stark resource limitations. When scaling to thousands of devices, cost and power constraints often mandate the use of microcontroller units like the ESP32[^fn-esp32-constraints], a widely used microcontroller unit from Espressif Systems, with its 240 MHz processor and mere 520 KB of RAM. This dramatic reduction in computational resources demands fundamental changes in system architecture. Models must be redesigned, optimization techniques such as quantization and pruning applied, and inference strategies reconsidered."
    },
    {
      "footnote_id": "fn-esp32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 218,
      "context": "[^fn-esp32-constraints]: **ESP32 Capabilities**: Despite its constraints, the ESP32 costs only $2-5, consumes 30-150mA duri...",
      "full_line": "[^fn-esp32-constraints]: **ESP32 Capabilities**: Despite its constraints, the ESP32 costs only $2-5, consumes 30-150mA during operation, and includes Wi-Fi, Bluetooth, and various sensors. This makes it ideal for IoT deployments in social impact applications. For comparison, a smartphone processor is 100\u00d7 more powerful but costs 50\u00d7 more. The ESP32's limitations\u2014RAM smaller than a single Instagram photo\u2014force engineers to develop ingenious optimization techniques that often benefit all platforms."
    },
    {
      "footnote_id": "fn-plantvillage-nuru",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 407,
      "context": "PlantVillage Nuru[^fn-plantvillage-nuru] operates with a baseline model optimized for resource-constrained environments. The system employs...",
      "full_line": "PlantVillage Nuru[^fn-plantvillage-nuru] operates with a baseline model optimized for resource-constrained environments. The system employs quantized convolutional neural networks (typically 2-5 MB in size) running on entry-level smartphones, capable of processing images at 1-2 frames per second while consuming less than 100 mW of power. These on-device models achieve 85-90% accuracy in identifying common crop diseases, providing essential diagnostic capabilities without requiring network connectivity."
    },
    {
      "footnote_id": "fn-plantvillage-nuru",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 409,
      "context": "[^fn-plantvillage-nuru]: **PlantVillage Nuru Real-World Impact**: Deployed across 500,000+ farmers in East Africa since 201...",
      "full_line": "[^fn-plantvillage-nuru]: **PlantVillage Nuru Real-World Impact**: Deployed across 500,000+ farmers in East Africa since 2019, Nuru has helped identify crop diseases affecting $2.6 billion worth of annual cassava production. The app works on $30 smartphones offline, processing 2.1 million crop images annually. Field studies show 73% reduction in crop losses and 40% increase in farmer incomes where the system is actively used, demonstrating how progressive enhancement patterns scale impact in resource-constrained environments."
    },
    {
      "footnote_id": "fn-whetstone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 67,
      "context": "...computational tasks. The [Whetstone benchmark](https://en.wikipedia.org/wiki/Whetstone_(benchmark))[^fn-whetstone], introduced in 1964 to measure floating-point arithmetic performance, became a definitive standard...",
      "full_line": "The evolution of benchmarks in computing illustrates how systematic performance measurement has shaped technological progress. During the 1960s and 1970s, when mainframe computers dominated the computing landscape, performance benchmarks focused primarily on fundamental computational tasks. The [Whetstone benchmark](https://en.wikipedia.org/wiki/Whetstone_(benchmark))[^fn-whetstone], introduced in 1964 to measure floating-point arithmetic performance, became a definitive standard that demonstrated how systematic testing could drive improvements in computer architecture [@curnow1976synthetic]."
    },
    {
      "footnote_id": "fn-whetstone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 69,
      "context": "[^fn-whetstone]: **Whetstone**: Named after the Whetstone ALGOL compiler, this 1964 benchmark measured floating-poi...",
      "full_line": "[^fn-whetstone]: **Whetstone**: Named after the Whetstone ALGOL compiler, this 1964 benchmark measured floating-point arithmetic performance in KIPS (thousands of instructions per second). It became the first widely-adopted standardized performance test, revealing that IBM System/360 processors could vary by 10x in floating-point performance despite similar architectures."
    },
    {
      "footnote_id": "fn-linpack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 71,
      "context": "The introduction of the [LINPACK benchmark](https://en.wikipedia.org/wiki/LINPACK_benchmark)[^fn-linpack] in 1979 expanded the focus of performance evaluation, offering a means to assess how efficiently sy...",
      "full_line": "The introduction of the [LINPACK benchmark](https://en.wikipedia.org/wiki/LINPACK_benchmark)[^fn-linpack] in 1979 expanded the focus of performance evaluation, offering a means to assess how efficiently systems solved linear equations. As computing shifted toward personal computers in the 1980s, the need for standardized performance measurement grew. The [Dhrystone benchmark](https://en.wikipedia.org/wiki/Dhrystone)[^fn-dhrystone], introduced in 1984, provided one of the first integer-based benchmarks, complementing floating-point evaluations [@Weicker1984]."
    },
    {
      "footnote_id": "fn-dhrystone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 71,
      "context": "...zed performance measurement grew. The [Dhrystone benchmark](https://en.wikipedia.org/wiki/Dhrystone)[^fn-dhrystone], introduced in 1984, provided one of the first integer-based benchmarks, complementing floating-poi...",
      "full_line": "The introduction of the [LINPACK benchmark](https://en.wikipedia.org/wiki/LINPACK_benchmark)[^fn-linpack] in 1979 expanded the focus of performance evaluation, offering a means to assess how efficiently systems solved linear equations. As computing shifted toward personal computers in the 1980s, the need for standardized performance measurement grew. The [Dhrystone benchmark](https://en.wikipedia.org/wiki/Dhrystone)[^fn-dhrystone], introduced in 1984, provided one of the first integer-based benchmarks, complementing floating-point evaluations [@Weicker1984]."
    },
    {
      "footnote_id": "fn-linpack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 73,
      "context": "[^fn-linpack]: **LINPACK**: Developed at Argonne National Laboratory to solve dense systems of linear equations,...",
      "full_line": "[^fn-linpack]: **LINPACK**: Developed at Argonne National Laboratory to solve dense systems of linear equations, LINPACK became famous for the Top500 supercomputer rankings starting in 1993. Modern systems achieve over 1 exaflop (10^18 operations/second) compared to the original 1979 benchmarks measuring mere megaflops."
    },
    {
      "footnote_id": "fn-dhrystone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 75,
      "context": "[^fn-dhrystone]: **Dhrystone**: Created by Reinhold Weicker to complement Whetstone's floating-point focus, Dhrysto...",
      "full_line": "[^fn-dhrystone]: **Dhrystone**: Created by Reinhold Weicker to complement Whetstone's floating-point focus, Dhrystone measures integer and string operations in DMIPS (Dhrystone MIPS). Unlike synthetic floating-point tests, it aimed to reflect \"typical\" programming constructs, though it became vulnerable to compiler optimizations that could artificially inflate scores."
    },
    {
      "footnote_id": "fn-spec",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 77,
      "context": "...rameworks that emphasized real-world workloads. The [SPEC CPU benchmarks](https://www.spec.org/cpu/)[^fn-spec], introduced in 1989 by the [System Performance Evaluation Cooperative (SPEC)](https://www.spec.org/...",
      "full_line": "The late 1980s and early 1990s saw the emergence of systematic benchmarking frameworks that emphasized real-world workloads. The [SPEC CPU benchmarks](https://www.spec.org/cpu/)[^fn-spec], introduced in 1989 by the [System Performance Evaluation Cooperative (SPEC)](https://www.spec.org/), fundamentally changed hardware evaluation by shifting the focus from synthetic tests to a standardized suite designed to measure performance using practical computing workloads. This approach enabled manufacturers to optimize their systems for real applications, accelerating advances in processor design and software optimization."
    },
    {
      "footnote_id": "fn-spec",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 79,
      "context": "[^fn-spec]: **SPEC CPU**: Founded by major vendors including Sun, IBM, and DEC to combat \"benchmark wars\" with...",
      "full_line": "[^fn-spec]: **SPEC CPU**: Founded by major vendors including Sun, IBM, and DEC to combat \"benchmark wars\" with misleading metrics, SPEC CPU introduced standardized real-world applications like compression, compilers, and scientific computing. SPEC2017 includes 43 benchmarks representing actual workloads, with results reported as geometric means to prevent gaming\u2014a 5000x improvement over early synthetic benchmarks."
    },
    {
      "footnote_id": "fn-3dmark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 81,
      "context": "...y 2000s presented new benchmarking challenges. The introduction of [3DMark](https://www.3dmark.com/)[^fn-3dmark] in 1998 established an industry standard for evaluating graphics performance, shaping the developme...",
      "full_line": "The increasing demand for graphics-intensive applications and mobile computing in the 1990s and early 2000s presented new benchmarking challenges. The introduction of [3DMark](https://www.3dmark.com/)[^fn-3dmark] in 1998 established an industry standard for evaluating graphics performance, shaping the development of programmable shaders and modern GPU architectures. Mobile computing introduced an additional constraint, namely, power efficiency, necessitating benchmarks that assessed both computational performance and energy consumption. The release of [MobileMark](https://bapco.com/products/mobilemark-2014/)[^fn-mobilemark] by [BAPCo](https://bapco.com/) provided a means to evaluate power efficiency in laptops and mobile devices, influencing the development of energy-efficient architectures such as [ARM](https://www.arm.com/)."
    },
    {
      "footnote_id": "fn-mobilemark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 81,
      "context": "...nce and energy consumption. The release of [MobileMark](https://bapco.com/products/mobilemark-2014/)[^fn-mobilemark] by [BAPCo](https://bapco.com/) provided a means to evaluate power efficiency in laptops and mobile...",
      "full_line": "The increasing demand for graphics-intensive applications and mobile computing in the 1990s and early 2000s presented new benchmarking challenges. The introduction of [3DMark](https://www.3dmark.com/)[^fn-3dmark] in 1998 established an industry standard for evaluating graphics performance, shaping the development of programmable shaders and modern GPU architectures. Mobile computing introduced an additional constraint, namely, power efficiency, necessitating benchmarks that assessed both computational performance and energy consumption. The release of [MobileMark](https://bapco.com/products/mobilemark-2014/)[^fn-mobilemark] by [BAPCo](https://bapco.com/) provided a means to evaluate power efficiency in laptops and mobile devices, influencing the development of energy-efficient architectures such as [ARM](https://www.arm.com/)."
    },
    {
      "footnote_id": "fn-3dmark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 83,
      "context": "[^fn-3dmark]: **3DMark**: Created by Finnish company Futuremark (now UL), this graphics benchmark drove GPU inno...",
      "full_line": "[^fn-3dmark]: **3DMark**: Created by Finnish company Futuremark (now UL), this graphics benchmark drove GPU innovation by testing real-time 3D rendering capabilities. Early versions measured triangle throughput and texture fill rates; modern 3DMark tests ray tracing and DLSS performance, with scores ranging from mobile devices (1,000 points) to high-end gaming PCs (30,000+ points)."
    },
    {
      "footnote_id": "fn-mobilemark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 85,
      "context": "[^fn-mobilemark]: **MobileMark**: Developed by the Business Applications Performance Corporation, this benchmark sim...",
      "full_line": "[^fn-mobilemark]: **MobileMark**: Developed by the Business Applications Performance Corporation, this benchmark simulates real laptop usage patterns including web browsing, video playback, and productivity tasks. Unlike peak performance tests, MobileMark measures battery life under realistic workloads, typically showing 6-12 hour endurance for modern laptops versus 15-30 minutes for synthetic stress tests."
    },
    {
      "footnote_id": "fn-cloudsuite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 87,
      "context": "...and cost-effectiveness [@ranganathan2024twenty]. Benchmarks like [CloudSuite](http://cloudsuite.ch/)[^fn-cloudsuite] have become critical for evaluating cloud infrastructure, measuring how well systems handle distrib...",
      "full_line": "The focus of benchmarking in the past decade has shifted toward cloud computing, big data, and artificial intelligence. Cloud service providers such as Amazon Web Services and Google Cloud optimize their platforms based on performance, scalability, and cost-effectiveness [@ranganathan2024twenty]. Benchmarks like [CloudSuite](http://cloudsuite.ch/)[^fn-cloudsuite] have become critical for evaluating cloud infrastructure, measuring how well systems handle distributed workloads."
    },
    {
      "footnote_id": "fn-cloudsuite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 89,
      "context": "[^fn-cloudsuite]: **CloudSuite**: Developed at EPFL to address the gap between traditional benchmarks and modern dat...",
      "full_line": "[^fn-cloudsuite]: **CloudSuite**: Developed at EPFL to address the gap between traditional benchmarks and modern datacenter workloads, CloudSuite includes realistic applications like web search, data analytics, and media streaming. Unlike synthetic benchmarks, it measures end-to-end performance including network latency, storage I/O, and memory bandwidth\u2014revealing that cloud applications are often memory-bound rather than CPU-bound. Machine learning has introduced another dimension of performance evaluation. The introduction of [MLPerf](https://mlcommons.org/)[^fn-mlperf] in 2018 established a widely accepted standard for measuring machine learning training and inference efficiency across different hardware architectures."
    },
    {
      "footnote_id": "fn-mlperf",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 89,
      "context": "...ed another dimension of performance evaluation. The introduction of [MLPerf](https://mlcommons.org/)[^fn-mlperf] in 2018 established a widely accepted standard for measuring machine learning training and inferenc...",
      "full_line": "[^fn-cloudsuite]: **CloudSuite**: Developed at EPFL to address the gap between traditional benchmarks and modern datacenter workloads, CloudSuite includes realistic applications like web search, data analytics, and media streaming. Unlike synthetic benchmarks, it measures end-to-end performance including network latency, storage I/O, and memory bandwidth\u2014revealing that cloud applications are often memory-bound rather than CPU-bound. Machine learning has introduced another dimension of performance evaluation. The introduction of [MLPerf](https://mlcommons.org/)[^fn-mlperf] in 2018 established a widely accepted standard for measuring machine learning training and inference efficiency across different hardware architectures."
    },
    {
      "footnote_id": "fn-mlperf",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 91,
      "context": "[^fn-mlperf]: **MLPerf**: Founded by Stanford, Harvard, Google, and other leading institutions, MLPerf created t...",
      "full_line": "[^fn-mlperf]: **MLPerf**: Founded by Stanford, Harvard, Google, and other leading institutions, MLPerf created the first industry-standard ML benchmarking suite, revealing surprising performance differences across hardware and establishing objective metrics for evaluating AI accelerators in training and inference tasks."
    },
    {
      "footnote_id": "fn-spec-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 99,
      "context": "...l standardized benchmarks that quantify energy efficiency. [SPEC Power](https://www.spec.org/power/)[^fn-spec-power] provides a widely accepted methodology for measuring server efficiency across varying workload leve...",
      "full_line": "The industry has responded with several standardized benchmarks that quantify energy efficiency. [SPEC Power](https://www.spec.org/power/)[^fn-spec-power] provides a widely accepted methodology for measuring server efficiency across varying workload levels, allowing for direct comparisons of power-performance trade-offs. The [Green500](https://top500.org/lists/green500/)[^fn-green500] ranking applies similar principles to high-performance computing, ranking the world's most powerful supercomputers based on their energy efficiency rather than their raw performance. The [ENERGY STAR](https://www.energystar.gov/products/computers)[^fn-energy-star] certification program has also established foundational energy standards that have shaped the design of consumer and enterprise computing systems."
    },
    {
      "footnote_id": "fn-green500",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 99,
      "context": "...rect comparisons of power-performance trade-offs. The [Green500](https://top500.org/lists/green500/)[^fn-green500] ranking applies similar principles to high-performance computing, ranking the world's most powerful...",
      "full_line": "The industry has responded with several standardized benchmarks that quantify energy efficiency. [SPEC Power](https://www.spec.org/power/)[^fn-spec-power] provides a widely accepted methodology for measuring server efficiency across varying workload levels, allowing for direct comparisons of power-performance trade-offs. The [Green500](https://top500.org/lists/green500/)[^fn-green500] ranking applies similar principles to high-performance computing, ranking the world's most powerful supercomputers based on their energy efficiency rather than their raw performance. The [ENERGY STAR](https://www.energystar.gov/products/computers)[^fn-energy-star] certification program has also established foundational energy standards that have shaped the design of consumer and enterprise computing systems."
    },
    {
      "footnote_id": "fn-energy-star",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 99,
      "context": "...rather than their raw performance. The [ENERGY STAR](https://www.energystar.gov/products/computers)[^fn-energy-star] certification program has also established foundational energy standards that have shaped the desig...",
      "full_line": "The industry has responded with several standardized benchmarks that quantify energy efficiency. [SPEC Power](https://www.spec.org/power/)[^fn-spec-power] provides a widely accepted methodology for measuring server efficiency across varying workload levels, allowing for direct comparisons of power-performance trade-offs. The [Green500](https://top500.org/lists/green500/)[^fn-green500] ranking applies similar principles to high-performance computing, ranking the world's most powerful supercomputers based on their energy efficiency rather than their raw performance. The [ENERGY STAR](https://www.energystar.gov/products/computers)[^fn-energy-star] certification program has also established foundational energy standards that have shaped the design of consumer and enterprise computing systems."
    },
    {
      "footnote_id": "fn-spec-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 101,
      "context": "[^fn-spec-power]: **SPEC Power**: Introduced in 2007 to address the growing importance of energy efficiency in serve...",
      "full_line": "[^fn-spec-power]: **SPEC Power**: Introduced in 2007 to address the growing importance of energy efficiency in server design, SPEC Power measures performance per watt across 10 different load levels from 10% to 100%. Results show that modern servers achieve 8-12 SPECpower_ssj2008 scores per watt, compared to 1-3 for systems from the mid-2000s\u2014a 4x efficiency improvement."
    },
    {
      "footnote_id": "fn-green500",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 103,
      "context": "[^fn-green500]: **Green500**: Started in 2007 as a counterpart to the Top500 supercomputer list, Green500 ranks sy...",
      "full_line": "[^fn-green500]: **Green500**: Started in 2007 as a counterpart to the Top500 supercomputer list, Green500 ranks systems by FLOPS per watt rather than raw performance. The most efficient systems achieve over 60 gigaFLOPS per watt compared to less than 1 gigaFLOPS/watt for early 2000s supercomputers\u2014demonstrating the dramatic improvements in computational efficiency."
    },
    {
      "footnote_id": "fn-energy-star",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 105,
      "context": "[^fn-energy-star]: **ENERGY STAR**: Launched by the EPA in 1992, this voluntary program has prevented over 4 billion...",
      "full_line": "[^fn-energy-star]: **ENERGY STAR**: Launched by the EPA in 1992, this voluntary program has prevented over 4 billion tons of greenhouse gas emissions and saved consumers $450 billion on energy bills. Computing equipment must meet strict efficiency requirements: ENERGY STAR computers typically consume 30-65% less energy than standard models during operation and sleep modes."
    },
    {
      "footnote_id": "fn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 131,
      "context": "...methodologies tailored to each domain's unique challenges. Algorithmic benchmarks, such as ImageNet[^fn-imagenet] [@deng2009imagenet], establish these evaluation frameworks, providing a consistent basis for compar...",
      "full_line": "AI algorithms must balance multiple interconnected performance objectives, including accuracy, speed, resource efficiency, and generalization capability. As machine learning applications span diverse domains, including computer vision, natural language processing, speech recognition, and reinforcement learning, evaluating these objectives requires standardized methodologies tailored to each domain's unique challenges. Algorithmic benchmarks, such as ImageNet[^fn-imagenet] [@deng2009imagenet], establish these evaluation frameworks, providing a consistent basis for comparing different machine learning approaches."
    },
    {
      "footnote_id": "fn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 133,
      "context": "[^fn-imagenet]: **ImageNet**: Created by Fei-Fei Li at Stanford starting in 2007, this dataset contains 14 million...",
      "full_line": "[^fn-imagenet]: **ImageNet**: Created by Fei-Fei Li at Stanford starting in 2007, this dataset contains 14 million images across 20,000 categories, with 1.2 million images used for the annual classification challenge (ILSVRC). ImageNet's impact is profound: it sparked the deep learning revolution when AlexNet achieved 15.3% top-5 error in 2012, compared to 25.8% for traditional methods\u2014the largest single-year improvement in computer vision history."
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 143,
      "context": "...task over the years. Starting from the baseline models in 2010 and 2011, the introduction of AlexNet[^fn-alexnet] in 2012 marked a substantial improvement, reducing the error rate from 25.8% to 16.4%. Subsequent m...",
      "full_line": "For instance, the graph in @fig-imagenet-challenge illustrates the significant reduction in error rates on the [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://www.image-net.org/challenges/LSVRC/) classification task over the years. Starting from the baseline models in 2010 and 2011, the introduction of AlexNet[^fn-alexnet] in 2012 marked a substantial improvement, reducing the error rate from 25.8% to 16.4%. Subsequent models like ZFNet, VGGNet, GoogleNet, and ResNet[^fn-resnet] continued this trend, with ResNet achieving a remarkable error rate of 3.57% by 2015. This progression highlights how algorithmic benchmarks not only measure current capabilities but also drive continuous advancements in AI performance."
    },
    {
      "footnote_id": "fn-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 143,
      "context": "...cing the error rate from 25.8% to 16.4%. Subsequent models like ZFNet, VGGNet, GoogleNet, and ResNet[^fn-resnet] continued this trend, with ResNet achieving a remarkable error rate of 3.57% by 2015. This progress...",
      "full_line": "For instance, the graph in @fig-imagenet-challenge illustrates the significant reduction in error rates on the [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://www.image-net.org/challenges/LSVRC/) classification task over the years. Starting from the baseline models in 2010 and 2011, the introduction of AlexNet[^fn-alexnet] in 2012 marked a substantial improvement, reducing the error rate from 25.8% to 16.4%. Subsequent models like ZFNet, VGGNet, GoogleNet, and ResNet[^fn-resnet] continued this trend, with ResNet achieving a remarkable error rate of 3.57% by 2015. This progression highlights how algorithmic benchmarks not only measure current capabilities but also drive continuous advancements in AI performance."
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 145,
      "context": "[^fn-alexnet]: **AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton at the University o...",
      "full_line": "[^fn-alexnet]: **AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton at the University of Toronto, this 8-layer convolutional neural network revolutionized computer vision in 2012. With 60 million parameters trained on two GTX 580 GPUs, AlexNet introduced key innovations like ReLU activations, dropout regularization, and data augmentation\u2014techniques now standard in deep learning."
    },
    {
      "footnote_id": "fn-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 147,
      "context": "[^fn-resnet]: **ResNet**: Microsoft's Residual Networks, introduced in 2015 by Kaiming He and colleagues, solved...",
      "full_line": "[^fn-resnet]: **ResNet**: Microsoft's Residual Networks, introduced in 2015 by Kaiming He and colleagues, solved the vanishing gradient problem with skip connections, enabling networks with 152+ layers. ResNet-50 became the de facto standard for transfer learning, while ResNet-152 achieved superhuman performance on ImageNet with 3.57% top-5 error\u2014better than the estimated 5% human error rate."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 180,
      "context": "...encompassing general-purpose CPUs, graphics processing units (GPUs), tensor processing units (TPUs)[^fn-tpu], and application-specific integrated circuits (ASICs)[^fn-asic], fundamentally determines the speed...",
      "full_line": "AI computations, particularly in machine learning, place extraordinary demands on computational resources. The underlying hardware infrastructure, encompassing general-purpose CPUs, graphics processing units (GPUs), tensor processing units (TPUs)[^fn-tpu], and application-specific integrated circuits (ASICs)[^fn-asic], fundamentally determines the speed, efficiency, and scalability of AI solutions. System benchmarks establish standardized methodologies for evaluating hardware performance across diverse AI workloads, measuring critical metrics including computational throughput, memory bandwidth, power efficiency, and scaling characteristics [@reddi2020mlperf; @mattson2020mlperf]."
    },
    {
      "footnote_id": "fn-asic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 180,
      "context": "...GPUs), tensor processing units (TPUs)[^fn-tpu], and application-specific integrated circuits (ASICs)[^fn-asic], fundamentally determines the speed, efficiency, and scalability of AI solutions. System benchmarks...",
      "full_line": "AI computations, particularly in machine learning, place extraordinary demands on computational resources. The underlying hardware infrastructure, encompassing general-purpose CPUs, graphics processing units (GPUs), tensor processing units (TPUs)[^fn-tpu], and application-specific integrated circuits (ASICs)[^fn-asic], fundamentally determines the speed, efficiency, and scalability of AI solutions. System benchmarks establish standardized methodologies for evaluating hardware performance across diverse AI workloads, measuring critical metrics including computational throughput, memory bandwidth, power efficiency, and scaling characteristics [@reddi2020mlperf; @mattson2020mlperf]."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 182,
      "context": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for neural network wo...",
      "full_line": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for neural network workloads, first deployed secretly in 2015 and announced in 2016. The first-generation TPU achieved 15-30x better performance per watt than contemporary GPUs for inference, while TPU v4 pods deliver 1.1 exaFLOPS of computing power\u2014demonstrating the potential of specialized AI hardware."
    },
    {
      "footnote_id": "fn-asic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 184,
      "context": "[^fn-asic]: **Application-Specific Integrated Circuit (ASIC)**: Custom chips designed for specific computation...",
      "full_line": "[^fn-asic]: **Application-Specific Integrated Circuit (ASIC)**: Custom chips designed for specific computational tasks, offering superior performance and energy efficiency compared to general-purpose processors. AI ASICs like Google's TPUs, Tesla's FSD chips, and Bitcoin mining ASICs can achieve 100-1000x better efficiency than CPUs for their target applications, but lack the flexibility for other workloads."
    },
    {
      "footnote_id": "fn-bert",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 347,
      "context": "...comparable domains. In natural language processing applications, transformer-based models like BERT[^fn-bert] have emerged as standard benchmarks for comparative analysis.",
      "full_line": "Baseline models serve as the reference points for evaluating novel approaches. These span from basic implementations, including linear regression for continuous predictions and logistic regression for classification tasks, to advanced architectures with proven success in comparable domains. In natural language processing applications, transformer-based models like BERT[^fn-bert] have emerged as standard benchmarks for comparative analysis."
    },
    {
      "footnote_id": "fn-bert",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 349,
      "context": "[^fn-bert]: **BERT**: Bidirectional Encoder Representations from Transformers, introduced by Google in 2018, r...",
      "full_line": "[^fn-bert]: **BERT**: Bidirectional Encoder Representations from Transformers, introduced by Google in 2018, revolutionized natural language processing by pre-training on vast text corpora using masked language modeling. BERT-Large contains 340 million parameters and achieved state-of-the-art results on 11 NLP tasks, establishing the foundation for modern language models like GPT and ChatGPT."
    },
    {
      "footnote_id": "fn-tensor-ops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 548,
      "context": "A key area of micro-benchmarking focuses on tensor operations[^fn-tensor-ops], which are the computational foundation of deep learning. Libraries like [cuDNN](https://developer....",
      "full_line": "A key area of micro-benchmarking focuses on tensor operations[^fn-tensor-ops], which are the computational foundation of deep learning. Libraries like [cuDNN](https://developer.nvidia.com/cudnn)[^fn-cudnn] by NVIDIA provide benchmarks for measuring fundamental computations such as convolutions and matrix multiplications across different hardware configurations. These measurements help developers understand how their hardware handles the core mathematical operations that dominate ML workloads."
    },
    {
      "footnote_id": "fn-cudnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 548,
      "context": "...omputational foundation of deep learning. Libraries like [cuDNN](https://developer.nvidia.com/cudnn)[^fn-cudnn] by NVIDIA provide benchmarks for measuring fundamental computations such as convolutions and matrix...",
      "full_line": "A key area of micro-benchmarking focuses on tensor operations[^fn-tensor-ops], which are the computational foundation of deep learning. Libraries like [cuDNN](https://developer.nvidia.com/cudnn)[^fn-cudnn] by NVIDIA provide benchmarks for measuring fundamental computations such as convolutions and matrix multiplications across different hardware configurations. These measurements help developers understand how their hardware handles the core mathematical operations that dominate ML workloads."
    },
    {
      "footnote_id": "fn-tensor-ops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 550,
      "context": "[^fn-tensor-ops]: **Tensor Operations**: Multi-dimensional array computations that form the backbone of neural netwo...",
      "full_line": "[^fn-tensor-ops]: **Tensor Operations**: Multi-dimensional array computations that form the backbone of neural networks, including matrix multiplication (GEMM), convolution, and element-wise operations. Modern AI accelerators optimize these primitives: NVIDIA's Tensor Cores can achieve 312 TFLOPS for mixed-precision matrix multiplications, compared to 15 TFLOPS for traditional FP32 computations\u2014a 20x speedup."
    },
    {
      "footnote_id": "fn-cudnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 552,
      "context": "[^fn-cudnn]: **cuDNN**: CUDA Deep Neural Network library, NVIDIA's GPU-accelerated library of primitives for de...",
      "full_line": "[^fn-cudnn]: **cuDNN**: CUDA Deep Neural Network library, NVIDIA's GPU-accelerated library of primitives for deep neural networks. Released in 2014, cuDNN provides highly optimized implementations for convolutions, pooling, normalization, and activation layers, delivering up to 10x performance improvements over naive implementations and becoming the de facto standard for GPU-accelerated deep learning."
    },
    {
      "footnote_id": "fn-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 612,
      "context": "For instance, large-scale models like [OpenAI's GPT-3](https://arxiv.org/abs/2005.14165)[^fn-gpt3] [@brown2020language], which consists of 175 billion parameters trained on 45 terabytes of data, hig...",
      "full_line": "For instance, large-scale models like [OpenAI's GPT-3](https://arxiv.org/abs/2005.14165)[^fn-gpt3] [@brown2020language], which consists of 175 billion parameters trained on 45 terabytes of data, highlight the immense computational demands of training. Benchmarks enable systematic evaluation of the underlying systems to ensure that hardware and software configurations can meet these demands efficiently."
    },
    {
      "footnote_id": "fn-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 614,
      "context": "[^fn-gpt3]: **GPT-3**: OpenAI's 2020 language model with 175 billion parameters, trained on 300 billion tokens...",
      "full_line": "[^fn-gpt3]: **GPT-3**: OpenAI's 2020 language model with 175 billion parameters, trained on 300 billion tokens using 10,000 NVIDIA V100 GPUs for several months at an estimated cost of $4.6 million. GPT-3 demonstrated emergent abilities like few-shot learning and in-context reasoning, establishing the paradigm of scaling laws where larger models consistently outperform smaller ones across diverse language tasks."
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 840,
      "context": "...ne learning frameworks implement various low-level optimizations, including mixed-precision training[^fn-mixed-precision], memory-efficient data loading, and distributed training strategies, that can significantly impact...",
      "full_line": "Beyond hardware selection, training benchmarks also inform software optimizations. Machine learning frameworks implement various low-level optimizations, including mixed-precision training[^fn-mixed-precision], memory-efficient data loading, and distributed training strategies, that can significantly impact system performance. Benchmarks help quantify the impact of these optimizations, ensuring that training systems are configured for maximum efficiency."
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 842,
      "context": "[^fn-mixed-precision]: **Mixed-Precision Training**: A training technique that uses both 16-bit (FP16) and 32-bit (FP32)...",
      "full_line": "[^fn-mixed-precision]: **Mixed-Precision Training**: A training technique that uses both 16-bit (FP16) and 32-bit (FP32) floating-point representations to accelerate training while maintaining model accuracy. Introduced by NVIDIA in 2017, mixed precision can achieve 1.5-2x speedups on modern GPUs with Tensor Cores while reducing memory usage by ~40%, enabling larger batch sizes and faster convergence for large models."
    },
    {
      "footnote_id": "fn-data-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 897,
      "context": "Parallelism in training is categorized into data parallelism[^fn-data-parallel], model parallelism[^fn-model-parallel], and pipeline parallelism, each presenting distinct challeng...",
      "full_line": "Parallelism in training is categorized into data parallelism[^fn-data-parallel], model parallelism[^fn-model-parallel], and pipeline parallelism, each presenting distinct challenges. Data parallelism, the most commonly used strategy, involves splitting the training dataset across multiple compute nodes. The efficiency of this approach depends on synchronization mechanisms and gradient communication overhead. In contrast, model parallelism partitions the neural network itself, requiring efficient coordination between processors. Benchmarks evaluate how well a system manages these parallelism strategies without degrading accuracy convergence."
    },
    {
      "footnote_id": "fn-model-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 897,
      "context": "Parallelism in training is categorized into data parallelism[^fn-data-parallel], model parallelism[^fn-model-parallel], and pipeline parallelism, each presenting distinct challenges. Data parallelism, the most commonly...",
      "full_line": "Parallelism in training is categorized into data parallelism[^fn-data-parallel], model parallelism[^fn-model-parallel], and pipeline parallelism, each presenting distinct challenges. Data parallelism, the most commonly used strategy, involves splitting the training dataset across multiple compute nodes. The efficiency of this approach depends on synchronization mechanisms and gradient communication overhead. In contrast, model parallelism partitions the neural network itself, requiring efficient coordination between processors. Benchmarks evaluate how well a system manages these parallelism strategies without degrading accuracy convergence."
    },
    {
      "footnote_id": "fn-data-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 899,
      "context": "[^fn-data-parallel]: **Data Parallelism**: The most common distributed training strategy where each GPU processes a dif...",
      "full_line": "[^fn-data-parallel]: **Data Parallelism**: The most common distributed training strategy where each GPU processes a different subset of the training batch, then synchronizes gradients across all nodes. Modern implementations use techniques like gradient accumulation and all-reduce operations to achieve near-linear scaling up to hundreds of GPUs, though communication overhead typically limits efficiency beyond 1000+ GPUs."
    },
    {
      "footnote_id": "fn-model-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 901,
      "context": "[^fn-model-parallel]: **Model Parallelism**: A distributed training approach where different parts of the neural network...",
      "full_line": "[^fn-model-parallel]: **Model Parallelism**: A distributed training approach where different parts of the neural network are placed on different GPUs, essential for models too large to fit in a single GPU's memory. GPT-3's 175B parameters required model parallelism across multiple nodes, as even high-memory GPUs can only hold ~40B parameters in mixed precision."
    },
    {
      "footnote_id": "fn-hardware-lottery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1900,
      "context": "A critical issue in benchmarking is what has been described as the hardware lottery[^fn-hardware-lottery], a concept introduced by [@hooker2021hardware]. The success of a machine learning model is often di...",
      "full_line": "A critical issue in benchmarking is what has been described as the hardware lottery[^fn-hardware-lottery], a concept introduced by [@hooker2021hardware]. The success of a machine learning model is often dictated not only by its architecture and training data but also by how well it aligns with the underlying hardware used for inference. Some models perform exceptionally well, not because they are inherently better, but because they are optimized for the parallel processing capabilities of GPUs or TPUs. Meanwhile, other promising architectures may be overlooked because they do not map efficiently to dominant hardware platforms."
    },
    {
      "footnote_id": "fn-hardware-lottery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1902,
      "context": "[^fn-hardware-lottery]: **Hardware Lottery**: The phenomenon where algorithmic progress is heavily influenced by which app...",
      "full_line": "[^fn-hardware-lottery]: **Hardware Lottery**: The phenomenon where algorithmic progress is heavily influenced by which approaches happen to align well with available hardware. For example, the Transformer architecture succeeded partly because its matrix multiplication operations perfectly match GPU capabilities, while equally valid architectures like graph neural networks remain underexplored due to poor GPU mapping. This suggests some \"breakthrough\" algorithms may simply be hardware-compatible rather than fundamentally superior."
    },
    {
      "footnote_id": "fn-systems-integration",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 21,
      "context": "To draw an analogy, consider the process of building a car[^fn-systems-integration]. While many resources are available on the various components of a car, such as the engine, transmi...",
      "full_line": "To draw an analogy, consider the process of building a car[^fn-systems-integration]. While many resources are available on the various components of a car, such as the engine, transmission, and suspension, there is often less understanding about how to assemble these components into a functional vehicle. Just as a car requires a well-designed and properly integrated system to operate efficiently and reliably, ML models also require a robust and carefully constructed system to deliver their full potential. Moreover, there is a lot of nuance in building ML systems, given their specific use case. For example, a Formula 1 race car must be assembled differently from an everyday Prius consumer car."
    },
    {
      "footnote_id": "fn-systems-integration",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 23,
      "context": "[^fn-systems-integration]: **Systems Integration Philosophy**: This concept traces back to Bell Labs in the 1940s, where engi...",
      "full_line": "[^fn-systems-integration]: **Systems Integration Philosophy**: This concept traces back to Bell Labs in the 1940s, where engineers first recognized that complex systems require dedicated integration expertise beyond component knowledge. The Apollo program epitomized this with dedicated \"systems integration\" roles\u2014NASA estimated that 60% of the program's complexity came from integration rather than individual components. Today, this principle drives everything from automotive manufacturing to ML systems engineering."
    },
    {
      "footnote_id": "fn-data-new-code",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 35,
      "context": "...have emphasized is that data is the foundation upon which ML systems are built. Data is the new code[^fn-data-new-code] that programs deep neural networks, making data engineering the first and most critical stage of an...",
      "full_line": "One of the key principles we have emphasized is that data is the foundation upon which ML systems are built. Data is the new code[^fn-data-new-code] that programs deep neural networks, making data engineering the first and most critical stage of any ML pipeline. That is why we began our exploration by diving into the basics of data engineering, recognizing that quality, diversity, and ethical sourcing are key to building robust and reliable machine learning models."
    },
    {
      "footnote_id": "fn-data-new-code",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 37,
      "context": "[^fn-data-new-code]: **Data as Code**: This concept emerged from the observation that traditional software is explicitl...",
      "full_line": "[^fn-data-new-code]: **Data as Code**: This concept emerged from the observation that traditional software is explicitly programmed with rules and logic, while neural networks are \"programmed\" implicitly through training data. Andrej Karpathy, former Tesla AI director, popularized this phrase by noting that in deep learning, \"data is 10,000x more important than code.\" Unlike traditional software where bugs are in code, ML system bugs often manifest through training data quality, distribution, or labeling issues."
    },
    {
      "footnote_id": "fn-generative-ai-timeline",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 141,
      "context": "In recent years, generative AI has taken the field by storm[^fn-generative-ai-timeline], demonstrating remarkable capabilities in creating realistic images, videos, and text. However, the...",
      "full_line": "In recent years, generative AI has taken the field by storm[^fn-generative-ai-timeline], demonstrating remarkable capabilities in creating realistic images, videos, and text. However, the rise of generative AI also brings new challenges for ML systems. Unlike traditional ML systems, generative models often demand more computational resources and pose challenges in terms of scalability and efficiency. Furthermore, evaluating and benchmarking generative models presents difficulties, as traditional metrics used for classification tasks may not be directly applicable. Developing robust evaluation frameworks for generative models is an active area of research, and something we hope to write about soon!"
    },
    {
      "footnote_id": "fn-generative-ai-timeline",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 143,
      "context": "[^fn-generative-ai-timeline]: **Generative AI Breakthrough**: While generative models existed for decades, the \"storm\" began wit...",
      "full_line": "[^fn-generative-ai-timeline]: **Generative AI Breakthrough**: While generative models existed for decades, the \"storm\" began with DALL-E (January 2021), accelerated with GPT-3's public API (2021), and exploded with ChatGPT's release (November 2022), which gained 100 million users in 2 months\u2014the fastest-growing consumer app in history. This sudden accessibility transformed generative AI from a research curiosity to a mainstream technology, triggering massive investments, policy discussions, and societal debates about AI's future."
    },
    {
      "footnote_id": "fn-watson-health",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 50,
      "context": "...to model failures, costly rebuilding, or even project termination. The failures of IBM Watson Health[^fn-watson-health] in 2019, where flawed data resulted in unsafe and incorrect cancer treatment recommendations [@stri...",
      "full_line": "The concept of \"Data Cascades,\" introduced by @sambasivan2021everyone, highlights the systemic failures that can arise when data quality issues are left unaddressed. Errors originating during data collection or processing stages can compound over time, creating cascading effects that lead to model failures, costly rebuilding, or even project termination. The failures of IBM Watson Health[^fn-watson-health] in 2019, where flawed data resulted in unsafe and incorrect cancer treatment recommendations [@strickland2019ibm], show the real-world consequences of neglecting data quality and its associated engineering requirements."
    },
    {
      "footnote_id": "fn-data-quality-stats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 52,
      "context": "...majority of their time, up to 60% as shown in @fig-ds-time, is spent on cleaning and organizing data[^fn-data-quality-stats]. This statistic highlights the critical need to prioritize data-related challenges early in the pip...",
      "full_line": "It is therefore unsurprising that data scientists spend the majority of their time, up to 60% as shown in @fig-ds-time, is spent on cleaning and organizing data[^fn-data-quality-stats]. This statistic highlights the critical need to prioritize data-related challenges early in the pipeline to avoid downstream issues and ensure the effectiveness of machine learning systems."
    },
    {
      "footnote_id": "fn-watson-health",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 54,
      "context": "[^fn-watson-health]: **IBM Watson Health**: IBM's ambitious AI health initiative, launched with $4 billion in investmen...",
      "full_line": "[^fn-watson-health]: **IBM Watson Health**: IBM's ambitious AI health initiative, launched with $4 billion in investment, was ultimately shut down in 2022 after years of promised breakthroughs failed to materialize due to fundamental data quality issues and over-hyped capabilities."
    },
    {
      "footnote_id": "fn-data-quality-stats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 56,
      "context": "[^fn-data-quality-stats]: **Data Quality Reality**: The famous \"garbage in, garbage out\" principle was first coined by IBM c...",
      "full_line": "[^fn-data-quality-stats]: **Data Quality Reality**: The famous \"garbage in, garbage out\" principle was first coined by IBM computer programmer George Fuechsel in the 1960s, describing how flawed input data produces nonsense output\u2014a principle that remains critically relevant in modern ML systems."
    },
    {
      "footnote_id": "fn-privacy-regulations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 841,
      "context": "...pr.eu/) and [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html)[^fn-privacy-regulations] limit the sharing of sensitive patient information. Synthetic data generation enables the creation...",
      "full_line": "In addition to expanding datasets, synthetic data addresses critical ethical and privacy concerns. Unlike real-world data, synthetic data attempts to not tie back to specific individuals or entities. This makes it especially useful in sensitive domains such as finance, healthcare, or human resources, where data confidentiality is paramount. The ability to preserve statistical properties while removing identifying information allows researchers to maintain high ethical standards without compromising the quality of their models. In healthcare, privacy regulations such as [GDPR](https://gdpr.eu/) and [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html)[^fn-privacy-regulations] limit the sharing of sensitive patient information. Synthetic data generation enables the creation of realistic yet anonymized datasets that can be used for training diagnostic models without compromising patient privacy."
    },
    {
      "footnote_id": "fn-privacy-regulations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 843,
      "context": "[^fn-privacy-regulations]: **Privacy Regulation Timeline**: GDPR (2016, effective 2018) imposed \u20ac20 million fines for violati...",
      "full_line": "[^fn-privacy-regulations]: **Privacy Regulation Timeline**: GDPR (2016, effective 2018) imposed \u20ac20 million fines for violations, followed by California's CCPA (2018), and now dozens of similar laws globally. These regulations fundamentally transformed how ML systems handle personal data, making privacy-by-design essential for any AI system."
    },
    {
      "footnote_id": "fn-mechanical-turk",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 857,
      "context": "...e word samples across different demographics and environments. Platforms like Amazon Mechanical Turk[^fn-mechanical-turk] can engage contributors to record wake words in various accents, speaking styles, and background co...",
      "full_line": "Crowdsourcing becomes valuable for collecting specific wake word samples across different demographics and environments. Platforms like Amazon Mechanical Turk[^fn-mechanical-turk] can engage contributors to record wake words in various accents, speaking styles, and background conditions. This approach is particularly useful for gathering data for underrepresented languages or specific acoustic environments."
    },
    {
      "footnote_id": "fn-mechanical-turk",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 859,
      "context": "[^fn-mechanical-turk]: **Mechanical Turk Origins**: Named after the 18th-century chess-playing \"automaton\" (actually a hu...",
      "full_line": "[^fn-mechanical-turk]: **Mechanical Turk Origins**: Named after the 18th-century chess-playing \"automaton\" (actually a human chess master hidden inside), Amazon's MTurk (2005) pioneered human-in-the-loop AI by enabling distributed human computation at scale\u2014ironically reversing the original Turk's deception of AI capabilities."
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 873,
      "context": "Batch ingestion[^fn-batch-processing] involves collecting data in groups or batches over a specified period before processing. This metho...",
      "full_line": "Batch ingestion[^fn-batch-processing] involves collecting data in groups or batches over a specified period before processing. This method is appropriate when real-time data processing is not critical and data can be processed at scheduled intervals. It's also useful for loading large volumes of historical data. For example, a retail company might use batch ingestion to process daily sales data overnight, updating their ML models for inventory prediction each morning [@akidau2015dataflow]."
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 875,
      "context": "[^fn-batch-processing]: **Batch Processing Evolution**: Batch processing dates back to IBM mainframes in the 1950s but was...",
      "full_line": "[^fn-batch-processing]: **Batch Processing Evolution**: Batch processing dates back to IBM mainframes in the 1950s but was revolutionized by Google's MapReduce (2004), which enabled distributed batch processing across thousands of machines. This paradigm shift made \"big data\" analytics economically feasible for the first time."
    },
    {
      "footnote_id": "fn-etl-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1082,
      "context": "ETL[^fn-etl-history] is a well-established paradigm in which data is first gathered from a source, then transformed to m...",
      "full_line": "ETL[^fn-etl-history] is a well-established paradigm in which data is first gathered from a source, then transformed to match the target schema or model, and finally loaded into a data warehouse or other repository. This approach typically results in data being stored in a ready-to-query format, which can be advantageous for ML systems that require consistent, pre-processed data. For instance, an ML system predicting customer churn might use ETL to standardize and aggregate customer interaction data from multiple sources before loading it into a format suitable for model training [@inmon2005building]."
    },
    {
      "footnote_id": "fn-etl-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1084,
      "context": "[^fn-etl-history]: **ETL Evolution**: ETL emerged in the 1970s with early data warehouses but was revolutionized by A...",
      "full_line": "[^fn-etl-history]: **ETL Evolution**: ETL emerged in the 1970s with early data warehouses but was revolutionized by Apache Spark in 2009, which enabled distributed data processing at unprecedented scale. Spark's in-memory computing made ETL pipelines 100x faster than traditional MapReduce approaches."
    },
    {
      "footnote_id": "fn-medical-imaging",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1354,
      "context": "...ults but also raises considerable system design challenges. For instance, in medical imaging systems[^fn-medical-imaging], experienced radiologists offer essential annotations. Such systems necessitate specialized interfa...",
      "full_line": "Manual labeling by experts is the primary approach in many annotation pipelines. This method produces high-quality results but also raises considerable system design challenges. For instance, in medical imaging systems[^fn-medical-imaging], experienced radiologists offer essential annotations. Such systems necessitate specialized interfaces for accurate labeling, secure data access controls to protect patient privacy, and reliable version control mechanisms to monitor annotation revisions."
    },
    {
      "footnote_id": "fn-medical-imaging",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1356,
      "context": "[^fn-medical-imaging]: **Medical Imaging AI Revolution**: The 2012 AlexNet breakthrough began with ImageNet's 14 million...",
      "full_line": "[^fn-medical-imaging]: **Medical Imaging AI Revolution**: The 2012 AlexNet breakthrough began with ImageNet's 14 million labeled images, but medical AI required specialized datasets. The NIH Clinical Center released 112,120 chest X-ray images in 2017, becoming one of the largest public medical imaging datasets for ML research. Despite the dependable outcomes of expert labeling, the scarcity and high expenses of specialists render it challenging to implement on a large scale for extensive datasets."
    },
    {
      "footnote_id": "fn-concept-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1510,
      "context": "...evant as the underlying distribution of data changes over time. This concept, known as concept drift[^fn-concept-drift], necessitates ongoing labeling efforts and periodic re-evaluation of existing labels.",
      "full_line": "The dynamic nature of real-world data presents another limitation. Labels that are accurate at the time of annotation may become outdated or irrelevant as the underlying distribution of data changes over time. This concept, known as concept drift[^fn-concept-drift], necessitates ongoing labeling efforts and periodic re-evaluation of existing labels."
    },
    {
      "footnote_id": "fn-concept-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1512,
      "context": "[^fn-concept-drift]: **Concept Drift Challenge**: First formalized by Schlimmer & Granger (1986), concept drift became...",
      "full_line": "[^fn-concept-drift]: **Concept Drift Challenge**: First formalized by Schlimmer & Granger (1986), concept drift became critical in ML systems deployment. Amazon's recommendation algorithms must continuously adapt as user preferences shift, while spam detection systems face adversarial concept drift as spammers evolve their tactics."
    },
    {
      "footnote_id": "fn-data-lake-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1570,
      "context": "Data lakes[^fn-data-lake-origins] address this gap by storing structured, semi-structured, and unstructured data in its native format...",
      "full_line": "Data lakes[^fn-data-lake-origins] address this gap by storing structured, semi-structured, and unstructured data in its native format, deferring schema definitions until the point of reading or analysis (sometimes called _schema-on-read_). As @tbl-storage shows, data lakes can handle large volumes of diverse data types. This approach grants data scientists tremendous latitude when dealing with experimental use cases or novel data types. However, data lakes also demand careful cataloging and metadata management. Without sufficient governance, these expansive repositories risk devolving into unsearchable, disorganized silos."
    },
    {
      "footnote_id": "fn-data-lake-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1572,
      "context": "[^fn-data-lake-origins]: **Data Lake Origins**: The term \"data lake\" was coined by Pentaho CTO James Dixon in 2010 to contr...",
      "full_line": "[^fn-data-lake-origins]: **Data Lake Origins**: The term \"data lake\" was coined by Pentaho CTO James Dixon in 2010 to contrast with data warehouses, comparing them to \"a data swamp\" if poorly managed. The concept emerged from Hadoop's ability to store vast amounts of unstructured data cheaply."
    },
    {
      "footnote_id": "fn-model-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1580,
      "context": "...pecially deep learning models, can have millions or even billions of parameters. For instance, GPT-3[^fn-model-scaling], a large language model, has 175 billion parameters, requiring approximately 350 GB of storage just...",
      "full_line": "One of the primary challenges in ML storage is handling large model weights. Modern ML models, especially deep learning models, can have millions or even billions of parameters. For instance, GPT-3[^fn-model-scaling], a large language model, has 175 billion parameters, requiring approximately 350 GB of storage just for the model weights [@brown2020language]. Storage systems need to be capable of handling these large, often dense, numerical arrays efficiently, both in terms of storage capacity and access speed."
    },
    {
      "footnote_id": "fn-model-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1582,
      "context": "[^fn-model-scaling]: **Model Scaling Explosion**: From AlexNet's 60 million parameters (2012) to GPT-3's 175 billion (2...",
      "full_line": "[^fn-model-scaling]: **Model Scaling Explosion**: From AlexNet's 60 million parameters (2012) to GPT-3's 175 billion (2020), model size grew 3,000x in 8 years. GPT-4's rumored 1.7 trillion parameters would require 3.5 TB of storage\u2014equivalent to 1,000 DVDs worth of model weights alone. This requirement goes beyond traditional data storage and enters the realm of high-performance computing storage solutions."
    },
    {
      "footnote_id": "fn-columnar-formats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1604,
      "context": "...choice of file format can significantly impact both throughput and latency. Columnar storage formats[^fn-columnar-formats] such as Parquet or ORC are particularly well-suited for ML workloads. These formats allow for effic...",
      "full_line": "The choice of file format can significantly impact both throughput and latency. Columnar storage formats[^fn-columnar-formats] such as Parquet or ORC are particularly well-suited for ML workloads. These formats allow for efficient retrieval of specific features without reading entire records, substantially reducing I/O operations and speeding up data loading for model training and inference. For example, when training a model that only requires a subset of features from a large dataset, columnar formats can reduce data read times by an order of magnitude compared to row-based formats."
    },
    {
      "footnote_id": "fn-columnar-formats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1606,
      "context": "[^fn-columnar-formats]: **Columnar Format Revolution**: Columnar storage was pioneered by C-Store in 2005, leading to Parq...",
      "full_line": "[^fn-columnar-formats]: **Columnar Format Revolution**: Columnar storage was pioneered by C-Store in 2005, leading to Parquet (developed at Twitter in 2013) and ORC (optimized row columnar, created at Hortonworks). These formats revolutionized analytics by enabling 10-100x faster queries for ML feature extraction."
    },
    {
      "footnote_id": "fn-hdfs-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1612,
      "context": "...h as [HDFS (Hadoop Distributed File System)](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)[^fn-hdfs-origins] or cloud-based object stores like [Amazon S3](https://aws.amazon.com/s3/), distribute data across m...",
      "full_line": "To handle the scale of data in modern ML systems, distributed storage architectures are often employed. These systems, such as [HDFS (Hadoop Distributed File System)](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)[^fn-hdfs-origins] or cloud-based object stores like [Amazon S3](https://aws.amazon.com/s3/), distribute data across multiple machines or data centers. This approach not only provides scalability but also enables parallel data access, which can substantially improve read performance for large-scale data processing tasks common in ML workflows."
    },
    {
      "footnote_id": "fn-hdfs-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1614,
      "context": "[^fn-hdfs-origins]: **HDFS Origins**: HDFS was inspired by Google's MapReduce paper (2004) and created at Yahoo! in 20...",
      "full_line": "[^fn-hdfs-origins]: **HDFS Origins**: HDFS was inspired by Google's MapReduce paper (2004) and created at Yahoo! in 2006 to handle web-scale data. It enabled the \"big data\" revolution by making petabyte-scale storage affordable using commodity hardware instead of expensive specialized systems."
    },
    {
      "footnote_id": "fn-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1628,
      "context": "...ol systems like Git excel at tracking code changes, they fall short when dealing with large datasets[^fn-data-versioning]. This gap has led to the emergence of specialized tools like [DVC (Data Version Control)](https://d...",
      "full_line": "One of the key challenges at this stage is managing the versions of datasets used in experiments. While traditional version control systems like Git excel at tracking code changes, they fall short when dealing with large datasets[^fn-data-versioning]. This gap has led to the emergence of specialized tools like [DVC (Data Version Control)](https://dvc.org/), which enable data scientists to efficiently track dataset changes, revert to previous versions, and share large files without duplication. These tools ensure that teams can maintain reproducibility and transparency throughout the iterative development process."
    },
    {
      "footnote_id": "fn-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1630,
      "context": "[^fn-data-versioning]: **Data Versioning Challenges**: Git's inability to handle large binary files efficiently led to th...",
      "full_line": "[^fn-data-versioning]: **Data Versioning Challenges**: Git's inability to handle large binary files efficiently led to the \"GitHub is not a CDN\" problem. DVC, created in 2017, solved this by treating data like code using content-addressable storage, enabling Git-like workflows for terabyte-scale datasets."
    },
    {
      "footnote_id": "fn-feature-stores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1664,
      "context": "Feature stores[^fn-feature-stores] are a centralized repository that stores and serves pre-computed features for machine learning mode...",
      "full_line": "Feature stores[^fn-feature-stores] are a centralized repository that stores and serves pre-computed features for machine learning models, ensuring consistency between training and inference workflows. They have emerged as a critical component in the ML infrastructure stack, addressing the unique challenges of managing and serving features for machine learning models. They act as a central repository for storing, managing, and serving machine learning features, bridging the gap between data engineering and machine learning operations."
    },
    {
      "footnote_id": "fn-feature-stores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1666,
      "context": "[^fn-feature-stores]: **Feature Store Evolution**: Feature stores were pioneered by Uber's Michelangelo platform in 2017...",
      "full_line": "[^fn-feature-stores]: **Feature Store Evolution**: Feature stores were pioneered by Uber's Michelangelo platform in 2017 to solve feature consistency issues at scale. The concept gained widespread adoption after Airbnb open-sourced their Zipline feature store, leading to modern solutions like Feast and Tecton."
    },
    {
      "footnote_id": "fn-brain-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 301,
      "context": "...this with incredible energy efficiency. The human brain operates on approximately 20 watts of power[^fn-brain-efficiency], about the same as a low-power light bulb, while performing complex cognitive tasks that would requ...",
      "full_line": "Perhaps most remarkably, biological systems achieve all this with incredible energy efficiency. The human brain operates on approximately 20 watts of power[^fn-brain-efficiency], about the same as a low-power light bulb, while performing complex cognitive tasks that would require orders of magnitude more power in current artificial systems. This efficiency hasn't just impressed researchers; it has become a crucial goal in the development of AI hardware and algorithms."
    },
    {
      "footnote_id": "fn-brain-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 303,
      "context": "[^fn-brain-efficiency]: **Brain Energy Efficiency**: The human brain contains approximately 86 billion neurons and perform...",
      "full_line": "[^fn-brain-efficiency]: **Brain Energy Efficiency**: The human brain contains approximately 86 billion neurons and performs roughly 10^16 operations per second on just 20 watts\u2014equivalent to running a single LED light bulb. In contrast, training GPT-3 consumed about 1,287 megawatt-hours of electricity. This stark efficiency gap drives research into neuromorphic computing and inspired the development of specialized AI chips designed to mimic brain-like processing."
    },
    {
      "footnote_id": "fn-synapses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 315,
      "context": "...receivers for incoming signals from other neurons. The connections between neurons occur at synapses[^fn-synapses], which modulate the strength of the transmitted signals. Finally, a long, slender projection called...",
      "full_line": "A biological neuron consists of several key components. The central part is the cell body, or soma, which contains the nucleus and performs the cell's basic life processes. Extending from the soma are branch-like structures called dendrites, which act as receivers for incoming signals from other neurons. The connections between neurons occur at synapses[^fn-synapses], which modulate the strength of the transmitted signals. Finally, a long, slender projection called the axon conducts electrical impulses away from the cell body to other neurons."
    },
    {
      "footnote_id": "fn-synapses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 317,
      "context": "[^fn-synapses]: **Synapses**: From the Greek word \"synaptein\" meaning \"to clasp together,\" synapses are the connec...",
      "full_line": "[^fn-synapses]: **Synapses**: From the Greek word \"synaptein\" meaning \"to clasp together,\" synapses are the connection points between neurons where chemical or electrical signals are transmitted. A typical neuron has 1,000-10,000 synaptic connections, and the human brain contains roughly 100 trillion synapses. The strength of synaptic connections can change through experience, forming the biological basis of learning and memory\u2014a principle directly mimicked by adjustable weights in artificial neural networks."
    },
    {
      "footnote_id": "fn-perceptron",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 425,
      "context": "...gan with early artificial neural networks in the 1950s, marked by the introduction of the Perceptron[^fn-perceptron]. While groundbreaking in concept, these early systems were severely limited by the computational ca...",
      "full_line": "We can now better appreciate how the field of deep learning evolved to meet these challenges through advances in hardware and algorithms. This journey began with early artificial neural networks in the 1950s, marked by the introduction of the Perceptron[^fn-perceptron]. While groundbreaking in concept, these early systems were severely limited by the computational capabilities of their era\u2014primarily mainframe computers that lacked both the processing power and memory capacity needed for complex networks."
    },
    {
      "footnote_id": "fn-perceptron",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 427,
      "context": "[^fn-perceptron]: **Perceptron**: Invented by Frank Rosenblatt in 1957 at Cornell, the perceptron was the first arti...",
      "full_line": "[^fn-perceptron]: **Perceptron**: Invented by Frank Rosenblatt in 1957 at Cornell, the perceptron was the first artificial neural network capable of learning. The New York Times famously reported it would be \"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\" While overly optimistic, this breakthrough laid the foundation for all modern neural networks."
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 429,
      "context": "...0s [@rumelhart1986learning], which we will learn about later, represented a theoretical breakthrough[^fn-backpropagation] and provided a systematic way to train multi-layer networks. However, the computational demands of...",
      "full_line": "The development of backpropagation algorithms in the 1980s [@rumelhart1986learning], which we will learn about later, represented a theoretical breakthrough[^fn-backpropagation] and provided a systematic way to train multi-layer networks. However, the computational demands of this algorithm far exceeded available hardware capabilities. Training even modest networks could take weeks, making experimentation and practical applications challenging. This mismatch between algorithmic requirements and hardware capabilities contributed to a period of reduced interest in neural networks."
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 431,
      "context": "[^fn-backpropagation]: **Backpropagation**: Published by Rumelhart, Hinton, and Williams in 1986, backpropagation solved...",
      "full_line": "[^fn-backpropagation]: **Backpropagation**: Published by Rumelhart, Hinton, and Williams in 1986, backpropagation solved the \"credit assignment problem\"\u2014how to determine which weights in a multi-layer network were responsible for errors. This algorithm, based on the mathematical chain rule, enabled training of deep networks and directly led to the modern AI revolution. Interestingly, a similar algorithm was discovered by Paul Werbos in 1974 but went largely unnoticed."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 435,
      "context": "...s: computational capabilities measured in the number of Floating Point Operations per Second (FLOPS)[^fn-flops] initially followed a $1.4\\times$ improvement pattern from 1952 to 2010, then accelerated to a 3.4-m...",
      "full_line": "The term \"deep learning\" gained prominence in the 2010s, coinciding with significant advances in computational power and data accessibility. The field has since experienced exponential growth, as illustrated in @fig-trends. The graph reveals two remarkable trends: computational capabilities measured in the number of Floating Point Operations per Second (FLOPS)[^fn-flops] initially followed a $1.4\\times$ improvement pattern from 1952 to 2010, then accelerated to a 3.4-month doubling cycle from 2012 to 2022. Perhaps more striking is the emergence of large-scale models between 2015 and 2022 (not explicitly shown or easily seen in the figure), which scaled 2 to 3 orders of magnitude faster than the general trend, following an aggressive 10-month doubling cycle."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 437,
      "context": "[^fn-flops]: **FLOPS**: Floating Point Operations Per Second measures computational throughput by counting math...",
      "full_line": "[^fn-flops]: **FLOPS**: Floating Point Operations Per Second measures computational throughput by counting mathematical operations like addition, subtraction, multiplication, and division of decimal numbers. Modern supercomputers achieve exascale performance (10^18 FLOPS), while neural network training requires petascale to exascale compute. For perspective, training GPT-3 required approximately 3.14 \u00d7 10^23 FLOPS\u2014more computation than was available to the entire world before 1960."
    },
    {
      "footnote_id": "fn-overfitting",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 474,
      "context": "...orks and controlling learning rates made training more stable. Techniques for preventing overfitting[^fn-overfitting] allowed models to generalize better to new data. Most importantly, researchers discovered that neur...",
      "full_line": "Algorithmic innovations made it possible to harness this data effectively. New methods for initializing networks and controlling learning rates made training more stable. Techniques for preventing overfitting[^fn-overfitting] allowed models to generalize better to new data. Most importantly, researchers discovered that neural network performance scaled predictably with model size, computation, and data quantity, leading to increasingly ambitious architectures."
    },
    {
      "footnote_id": "fn-overfitting",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 476,
      "context": "[^fn-overfitting]: **Overfitting**: When a model memorizes training examples instead of learning generalizable patter...",
      "full_line": "[^fn-overfitting]: **Overfitting**: When a model memorizes training examples instead of learning generalizable patterns\u2014like a student who memorizes answers instead of understanding concepts. The model performs perfectly on training data but fails on new examples. Common signs include training accuracy continuing to improve while validation accuracy plateaus or decreases. Think of it as becoming an \"expert\" on a practice test who panics when facing slightly different questions on the real exam."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 478,
      "context": "...capabilities needed for efficient neural network computation. Specialized AI accelerators like TPUs[^fn-tpu] [@jouppi2017datacenter] pushed performance further. High-bandwidth memory systems and fast intercon...",
      "full_line": "Computing infrastructure evolved to meet these growing demands. On the hardware side, graphics processing units (GPUs) provided the parallel processing capabilities needed for efficient neural network computation. Specialized AI accelerators like TPUs[^fn-tpu] [@jouppi2017datacenter] pushed performance further. High-bandwidth memory systems and fast interconnects addressed data movement challenges. Equally important were software advances\u2014frameworks and libraries that made it easier to build and train networks, distributed computing systems that enabled training at scale, and tools for optimizing model deployment."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 480,
      "context": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for tensor operati...",
      "full_line": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for tensor operations, the mathematical building blocks of neural networks. First deployed internally in 2015, TPUs can perform matrix multiplications up to 30\u00d7 faster than contemporary GPUs while using less power. The name reflects their optimization for tensor operations\u2014multi-dimensional arrays that represent data flowing through neural networks. Google has since made TPUs available through cloud services, democratizing access to this specialized AI hardware."
    },
    {
      "footnote_id": "fn-gradient-descent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1604,
      "context": "...ess adjusts the network's weights to improve its predictions. Using a method called gradient descent[^fn-gradient-descent], the network calculates how much each weight contributes to the error and updates it to reduce the...",
      "full_line": "The optimization process adjusts the network's weights to improve its predictions. Using a method called gradient descent[^fn-gradient-descent], the network calculates how much each weight contributes to the error and updates it to reduce the loss. This process is repeated over many iterations, gradually refining the network's ability to make accurate predictions."
    },
    {
      "footnote_id": "fn-gradient-descent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1606,
      "context": "[^fn-gradient-descent]: **Gradient Descent**: Think of gradient descent as finding the bottom of a valley while blindfolde...",
      "full_line": "[^fn-gradient-descent]: **Gradient Descent**: Think of gradient descent as finding the bottom of a valley while blindfolded\u2014you feel the slope under your feet and take steps downhill. Mathematically, the gradient points in the direction of steepest increase, so we move in the opposite direction to minimize our loss function. The name comes from the Latin \"gradus\" (step) and was first formalized by Cauchy in 1847 for solving systems of equations, though the modern machine learning version was developed much later."
    },
    {
      "footnote_id": "fn-learning-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1614,
      "context": "...scent will modify the weights to better distinguish between these digits. The learning rate $\\alpha$[^fn-learning-rate] controls how large these adjustments are\u2014too large, and the network might overshoot optimal values;...",
      "full_line": "For our MNIST example, this means adjusting weights to improve digit classification accuracy. If the network frequently confuses \"7\"s with \"1\"s, gradient descent will modify the weights to better distinguish between these digits. The learning rate $\\alpha$[^fn-learning-rate] controls how large these adjustments are\u2014too large, and the network might overshoot optimal values; too small, and training will progress very slowly."
    },
    {
      "footnote_id": "fn-learning-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1616,
      "context": "[^fn-learning-rate]: **Learning Rate**: Often called the most important hyperparameter in deep learning, the learning r...",
      "full_line": "[^fn-learning-rate]: **Learning Rate**: Often called the most important hyperparameter in deep learning, the learning rate determines the step size in optimization. Think of it like the gas pedal on a car\u2014too much acceleration and you'll crash past your destination, too little and you'll never get there. Typical values range from 0.1 to 0.0001, and getting this right can mean the difference between a model that learns in hours versus one that never converges."
    },
    {
      "footnote_id": "fn-uat",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 55,
      "context": "...Their computational power was established theoretically by the Universal Approximation Theorem (UAT)[^fn-uat] [@cybenko1989approximation; @hornik1989multilayer], which states that a sufficiently large MLP with...",
      "full_line": "Multi-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes fundamental computational patterns that appear throughout deep learning systems. Their computational power was established theoretically by the Universal Approximation Theorem (UAT)[^fn-uat] [@cybenko1989approximation; @hornik1989multilayer], which states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain, given suitable weights and biases."
    },
    {
      "footnote_id": "fn-uat",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 57,
      "context": "[^fn-uat]: **Universal Approximation Theorem**: Proven independently by Cybenko (1989) and Hornik (1989), thi...",
      "full_line": "[^fn-uat]: **Universal Approximation Theorem**: Proven independently by Cybenko (1989) and Hornik (1989), this fundamental result showed that neural networks could theoretically learn any function\u2014a discovery that reinvigorated interest in neural networks after the \"AI Winter\" of the 1980s and laid mathematical foundations for modern deep learning."
    },
    {
      "footnote_id": "fn-mnist-dataset",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 59,
      "context": "When applied to the MNIST handwritten digit recognition challenge[^fn-mnist-dataset], an MLP reveals its computational power by transforming a complex $28\\times 28$ pixel image into a...",
      "full_line": "When applied to the MNIST handwritten digit recognition challenge[^fn-mnist-dataset], an MLP reveals its computational power by transforming a complex $28\\times 28$ pixel image into a precise digit classification."
    },
    {
      "footnote_id": "fn-mnist-dataset",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 61,
      "context": "[^fn-mnist-dataset]: **MNIST Dataset**: Created by Yann LeCun in 1998 from NIST's database of handwritten digits, MNIST...",
      "full_line": "[^fn-mnist-dataset]: **MNIST Dataset**: Created by Yann LeCun in 1998 from NIST's database of handwritten digits, MNIST's 60,000 training images became the \"fruit fly\" of machine learning research. Despite achieving 99.7% accuracy being considered solved, MNIST remains invaluable for education because its simplicity lets students focus on architectural concepts without getting lost in data complexity. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits."
    },
    {
      "footnote_id": "fn-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 283,
      "context": "...r weights). While actual implementations use sophisticated optimizations through libraries like BLAS[^fn-blas] or cuBLAS, these fundamental patterns drive key system design decisions.",
      "full_line": "In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use sophisticated optimizations through libraries like BLAS[^fn-blas] or cuBLAS, these fundamental patterns drive key system design decisions."
    },
    {
      "footnote_id": "fn-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 285,
      "context": "[^fn-blas]: **Basic Linear Algebra Subprograms (BLAS)**: Developed in the 1970s as a standard for basic vector...",
      "full_line": "[^fn-blas]: **Basic Linear Algebra Subprograms (BLAS)**: Developed in the 1970s as a standard for basic vector and matrix operations, BLAS became the foundation for virtually all scientific computing. Modern implementations like Intel MKL and OpenBLAS achieve performance within 90% of theoretical peak, making them essential for neural network efficiency."
    },
    {
      "footnote_id": "fn-imagenet-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 533,
      "context": "...ation in an image by applying learnable filters across the input, enabling robust object recognition[^fn-imagenet-breakthrough]. These filters detect local features, and their repeated application across the image creates trans...",
      "full_line": "**Spatial Feature Extraction**: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition[^fn-imagenet-breakthrough]. These filters detect local features, and their repeated application across the image creates translation invariance\u2014the ability to recognize a pattern regardless of its position."
    },
    {
      "footnote_id": "fn-imagenet-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 535,
      "context": "[^fn-imagenet-breakthrough]: **ImageNet Revolution**: AlexNet's dramatic victory in the 2012 ImageNet challenge (reducing error...",
      "full_line": "[^fn-imagenet-breakthrough]: **ImageNet Revolution**: AlexNet's dramatic victory in the 2012 ImageNet challenge (reducing error from 26% to 16%) sparked the deep learning renaissance. ImageNet's 14 million labeled images across 20,000 categories provided the scale needed to train deep CNNs, proving that \"big data + big compute + big models\" could achieve superhuman performance."
    },
    {
      "footnote_id": "fn-lecun-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 538,
      "context": "This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun[^fn-lecun-cnn] and @lecun1989backpropagation.",
      "full_line": "This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun[^fn-lecun-cnn] and @lecun1989backpropagation."
    },
    {
      "footnote_id": "fn-lecun-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 540,
      "context": "[^fn-lecun-cnn]: **Yann LeCun and CNNs**: LeCun's 1989 LeNet architecture was inspired by Hubel and Wiesel's discov...",
      "full_line": "[^fn-lecun-cnn]: **Yann LeCun and CNNs**: LeCun's 1989 LeNet architecture was inspired by Hubel and Wiesel's discovery of simple and complex cells in cat visual cortex (1962). LeNet-5 achieved 99.2% accuracy on MNIST in 1998 and was deployed by banks to read millions of checks daily\u2014one of the first large-scale commercial applications of neural networks. As illustrated in @fig-cnn-spatial-processing, CNNs address spatial pattern processing through a fundamentally different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position\u2014a process known as convolution[^fn-convolution-origin]."
    },
    {
      "footnote_id": "fn-convolution-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 540,
      "context": "...ss the input space, applying the same set of weights at each position\u2014a process known as convolution[^fn-convolution-origin].",
      "full_line": "[^fn-lecun-cnn]: **Yann LeCun and CNNs**: LeCun's 1989 LeNet architecture was inspired by Hubel and Wiesel's discovery of simple and complex cells in cat visual cortex (1962). LeNet-5 achieved 99.2% accuracy on MNIST in 1998 and was deployed by banks to read millions of checks daily\u2014one of the first large-scale commercial applications of neural networks. As illustrated in @fig-cnn-spatial-processing, CNNs address spatial pattern processing through a fundamentally different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position\u2014a process known as convolution[^fn-convolution-origin]."
    },
    {
      "footnote_id": "fn-convolution-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 542,
      "context": "[^fn-convolution-origin]: **Mathematical Convolution**: The convolution operation dates to Euler (1760s) and was formalized...",
      "full_line": "[^fn-convolution-origin]: **Mathematical Convolution**: The convolution operation dates to Euler (1760s) and was formalized by mathematicians like Cauchy and Poisson in the 1800s for solving differential equations [@dominguez2015history]. CNNs adapted this 200-year-old mathematical concept to create translation-invariant feature detectors, proving that classical mathematics often provides the foundation for modern breakthroughs."
    },
    {
      "footnote_id": "fn-attention-is-all-you-need",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1327,
      "context": "Transformers, introduced in the landmark 'Attention is All You Need' paper[^fn-attention-is-all-you-need] by @vaswani2017attention, represent a significant evolution in the application of attention mechani...",
      "full_line": "Transformers, introduced in the landmark 'Attention is All You Need' paper[^fn-attention-is-all-you-need] by @vaswani2017attention, represent a significant evolution in the application of attention mechanisms, introducing the concept of self-attention to create a powerful architecture for dynamic pattern processing."
    },
    {
      "footnote_id": "fn-attention-is-all-you-need",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1329,
      "context": "[^fn-attention-is-all-you-need]: **\"Attention is All You Need\"**: This 2017 paper by Google researchers eliminated recurrence entir...",
      "full_line": "[^fn-attention-is-all-you-need]: **\"Attention is All You Need\"**: This 2017 paper by Google researchers eliminated recurrence entirely, showing that attention mechanisms alone could achieve state-of-the-art results. The title itself became a rallying cry, and within 5 years, transformer-based models achieved breakthrough performance in language (GPT, BERT), vision (ViT), and beyond. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself."
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1520,
      "context": "Perhaps most importantly, the development of MLPs established the backpropagation algorithm[^fn-backpropagation], which to this day remains the cornerstone of neural network training. This key contribution has en...",
      "full_line": "Perhaps most importantly, the development of MLPs established the backpropagation algorithm[^fn-backpropagation], which to this day remains the cornerstone of neural network training. This key contribution has enabled the training of deep architectures and influenced how later architectures would be designed to maintain gradient flow."
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1522,
      "context": "[^fn-backpropagation]: **Backpropagation Algorithm**: While the chain rule was known since the 1600s, Rumelhart, Hinton,...",
      "full_line": "[^fn-backpropagation]: **Backpropagation Algorithm**: While the chain rule was known since the 1600s, Rumelhart, Hinton, and Williams (1986) showed how to efficiently apply it to train multi-layer networks. This \"learning by error propagation\" algorithm made deep networks practical and remains virtually unchanged in modern systems\u2014a testament to its fundamental importance."
    },
    {
      "footnote_id": "fn-resnet-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1532,
      "context": "Perhaps even more influential was the introduction of skip connections through ResNets[^fn-resnet-breakthrough] [@he2016deep]. Originally they were designed to help train very deep CNNs, skip connections have be...",
      "full_line": "Perhaps even more influential was the introduction of skip connections through ResNets[^fn-resnet-breakthrough] [@he2016deep]. Originally they were designed to help train very deep CNNs, skip connections have become a fundamental building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs."
    },
    {
      "footnote_id": "fn-resnet-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1534,
      "context": "[^fn-resnet-breakthrough]: **ResNet Revolution**: ResNet (2016) solved the \"degradation problem\" where deeper networks perfor...",
      "full_line": "[^fn-resnet-breakthrough]: **ResNet Revolution**: ResNet (2016) solved the \"degradation problem\" where deeper networks performed worse than shallow ones. The key insight: adding identity shortcuts ($\\mathcal{F}(\\mathbf{x}) + \\mathbf{x}$) let networks learn residual mappings instead of full transformations, enabling training of 1000+ layer networks and winning ImageNet 2015."
    },
    {
      "footnote_id": "fn-lstm-invention",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1544,
      "context": "The development of LSTMs[^fn-lstm-invention] and GRUs brought sophisticated gating mechanisms to neural networks [@hochreiter1997long; @cho2014p...",
      "full_line": "The development of LSTMs[^fn-lstm-invention] and GRUs brought sophisticated gating mechanisms to neural networks [@hochreiter1997long; @cho2014properties]. These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design."
    },
    {
      "footnote_id": "fn-lstm-invention",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1546,
      "context": "[^fn-lstm-invention]: **LSTM Origins**: Sepp Hochreiter and J\u00fcrgen Schmidhuber invented LSTMs in 1997 to solve the \"vani...",
      "full_line": "[^fn-lstm-invention]: **LSTM Origins**: Sepp Hochreiter and J\u00fcrgen Schmidhuber invented LSTMs in 1997 to solve the \"vanishing gradient problem\" that plagued RNNs. Their gating mechanism was inspired by biological neurons' ability to selectively retain information\u2014a breakthrough that enabled sequence modeling and paved the way for modern language models."
    },
    {
      "footnote_id": "fn-vision-transformers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1612,
      "context": "...language models follow this pattern of recombining fundamental building blocks. Vision Transformers[^fn-vision-transformers] adapt the Transformer architecture to images while maintaining its essential components [@dosovitsk...",
      "full_line": "Even recent innovations in vision and language models follow this pattern of recombining fundamental building blocks. Vision Transformers[^fn-vision-transformers] adapt the Transformer architecture to images while maintaining its essential components [@dosovitskiy2021image]. Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution [@brown2020language]."
    },
    {
      "footnote_id": "fn-vision-transformers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1614,
      "context": "[^fn-vision-transformers]: **Vision Transformers (ViTs)**: Google's 2021 breakthrough showed that pure transformers could mat...",
      "full_line": "[^fn-vision-transformers]: **Vision Transformers (ViTs)**: Google's 2021 breakthrough showed that pure transformers could match CNN performance on ImageNet by treating image patches as \"words.\" ViTs split a $224\\times 224$ image into $16\\times 16$ patches (196 \"tokens\"), proving that attention mechanisms could replace convolutional inductive biases with sufficient data."
    },
    {
      "footnote_id": "fn-parameter-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2009,
      "context": "...interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters[^fn-parameter-scaling]), efficient data movement becomes increasingly critical, leading to innovations like near-memory pr...",
      "full_line": "The evolution from CNNs to Transformers has increased reliance on gather and reduction operations, driving hardware innovations like more flexible interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters[^fn-parameter-scaling]), efficient data movement becomes increasingly critical, leading to innovations like near-memory processing and sophisticated data flow optimizations."
    },
    {
      "footnote_id": "fn-parameter-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2011,
      "context": "[^fn-parameter-scaling]: **Parameter Scaling**: The leap from AlexNet's 60 million parameters (2012) to GPT-3's 175 billion...",
      "full_line": "[^fn-parameter-scaling]: **Parameter Scaling**: The leap from AlexNet's 60 million parameters (2012) to GPT-3's 175 billion parameters (2020) represents a 3,000x increase in just 8 years. Modern models like GPT-4 may exceed 1 trillion parameters, requiring specialized distributed computing infrastructure and consuming megawatts of power during training."
    },
    {
      "footnote_id": "fn-tpu-development",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2017,
      "context": "...tions and convolutions in deep learning has led to the development of tensor processing units (TPUs)[^fn-tpu-development] and tensor cores in GPUs, which are specifically designed to perform these operations efficiently.",
      "full_line": "One of the most significant impacts of these primitives on system design is the push towards specialized hardware. The prevalence of matrix multiplications and convolutions in deep learning has led to the development of tensor processing units (TPUs)[^fn-tpu-development] and tensor cores in GPUs, which are specifically designed to perform these operations efficiently."
    },
    {
      "footnote_id": "fn-tpu-development",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2019,
      "context": "[^fn-tpu-development]: **Tensor Processing Units**: Google's TPUs emerged from their need to run neural networks on billi...",
      "full_line": "[^fn-tpu-development]: **Tensor Processing Units**: Google's TPUs emerged from their need to run neural networks on billions of searches daily. First deployed secretly in 2015, TPUs achieve 15-30x better performance per watt than GPUs for inference. The TPU's $128\\times 128$ systolic array performs 65,536 multiply-accumulate operations per clock cycle, revolutionizing AI hardware design. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks."
    },
    {
      "footnote_id": "fn-scaling-laws",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 54,
      "context": "To address these concerns, researchers have developed scaling laws[^fn-scaling-laws]\u2014empirical relationships that quantify the correlation between model performance and training resour...",
      "full_line": "To address these concerns, researchers have developed scaling laws[^fn-scaling-laws]\u2014empirical relationships that quantify the correlation between model performance and training resources. These laws provide a formal framework for analyzing the trade-offs inherent in scaling and elucidate the increasing importance of efficiency as systems expand in size and complexity."
    },
    {
      "footnote_id": "fn-scaling-laws",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 56,
      "context": "[^fn-scaling-laws]: **Scaling Laws**: OpenAI established that language model performance follows predictable power-law...",
      "full_line": "[^fn-scaling-laws]: **Scaling Laws**: OpenAI established that language model performance follows predictable power-law relationships with model size, dataset size, and compute budget - fundamentally changing how researchers approach model design and resource allocation."
    },
    {
      "footnote_id": "fn-tokens",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 74,
      "context": "...training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve correspond...",
      "full_line": "Empirical studies of large language models (LLMs) further elucidate the interplay between these factors, parameters, data, and computational resources, under fixed resource constraints. As illustrated in @fig-compute-optimal, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 74,
      "context": "...oFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most effic...",
      "full_line": "Empirical studies of large language models (LLMs) further elucidate the interplay between these factors, parameters, data, and computational resources, under fixed resource constraints. As illustrated in @fig-compute-optimal, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models."
    },
    {
      "footnote_id": "fn-transformer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 74,
      "context": "...corresponds to a constant number of floating-point operations (FLOPs[^fn-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computati...",
      "full_line": "Empirical studies of large language models (LLMs) further elucidate the interplay between these factors, parameters, data, and computational resources, under fixed resource constraints. As illustrated in @fig-compute-optimal, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models."
    },
    {
      "footnote_id": "fn-autoregressive",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 74,
      "context": "...signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and t...",
      "full_line": "Empirical studies of large language models (LLMs) further elucidate the interplay between these factors, parameters, data, and computational resources, under fixed resource constraints. As illustrated in @fig-compute-optimal, for a given computational budget in language model training, there exists an optimal allocation between model size and dataset size (measured in tokens[^fn-tokens]) that minimizes training loss. The left panel depicts 'IsoFLOP curves,' where each curve corresponds to a constant number of floating-point operations (FLOPs[^fn-flops]) during transformer[^fn-transformer] training. Each valley in these curves signifies the most efficient model size for a given computational level when training autoregressive[^fn-autoregressive] language models. The center and right panels demonstrate how the optimal number of parameters and tokens scales predictably with increasing computational budgets in language model training, highlighting the necessity for coordinated scaling to maximize resource utilization in large language models."
    },
    {
      "footnote_id": "fn-tokens",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 76,
      "context": "[^fn-tokens]: **Tokens**: Individual units of text that language models process, typically words or subword piec...",
      "full_line": "[^fn-tokens]: **Tokens**: Individual units of text that language models process, typically words or subword pieces. GPT-3 was trained on 300 billion tokens, while modern models like PaLM use over 780 billion tokens\u2014requiring massive text corpora equivalent to thousands of books."
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 78,
      "context": "[^fn-flops]: **FLOPs**: Floating-Point Operations Per Second, the standard measure of computational throughput....",
      "full_line": "[^fn-flops]: **FLOPs**: Floating-Point Operations Per Second, the standard measure of computational throughput. Training GPT-3 required ~3.14 \u00d7 10\u00b2\u00b3 FLOPs, equivalent to running a modern gaming PC continuously for over 350 years."
    },
    {
      "footnote_id": "fn-transformer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 80,
      "context": "[^fn-transformer]: **Transformer**: Neural network architecture introduced by Vaswani et al. in 2017, revolutionizing...",
      "full_line": "[^fn-transformer]: **Transformer**: Neural network architecture introduced by Vaswani et al. in 2017, revolutionizing natural language processing through attention mechanisms. Forms the foundation of models like GPT, BERT, and T5, enabling parallel processing unlike sequential RNNs."
    },
    {
      "footnote_id": "fn-autoregressive",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 82,
      "context": "[^fn-autoregressive]: **Autoregressive**: Models that generate sequences by predicting the next element based on previou...",
      "full_line": "[^fn-autoregressive]: **Autoregressive**: Models that generate sequences by predicting the next element based on previous elements. GPT models are autoregressive, generating text one token at a time, unlike BERT which processes entire sequences simultaneously."
    },
    {
      "footnote_id": "fn-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 86,
      "context": "...upplied. Similarly, language models exhibit analogous patterns, with studies of models such as GPT-3[^fn-gpt3] demonstrating that performance scales predictably with both model parameters and training data volu...",
      "full_line": "For instance, in computer vision tasks, doubling the size of neural networks typically yields consistent accuracy gains, provided that proportional increases in training data are supplied. Similarly, language models exhibit analogous patterns, with studies of models such as GPT-3[^fn-gpt3] demonstrating that performance scales predictably with both model parameters and training data volume."
    },
    {
      "footnote_id": "fn-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 88,
      "context": "[^fn-gpt3]: **GPT-3**: OpenAI's 175-billion parameter language model released in 2020, costing an estimated $4...",
      "full_line": "[^fn-gpt3]: **GPT-3**: OpenAI's 175-billion parameter language model released in 2020, costing an estimated $4.6 million to train and consuming ~1,287 MWh of electricity. Its training data included 45TB of text from the internet, books, and other sources."
    },
    {
      "footnote_id": "fn-perplexity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 98,
      "context": "...across diverse domains have corroborated that performance metrics, including accuracy or perplexity[^fn-perplexity], exhibit smooth and monotonic improvements when models are scaled along these dimensions.",
      "full_line": "Scaling laws delineate the relationship between the performance of machine learning models and the augmentation of resources, including model size, dataset size, and computational budget. These relationships are typically expressed as power-law functions, which demonstrate that model loss decreases predictably with increased resource allocation. Empirical investigations across diverse domains have corroborated that performance metrics, including accuracy or perplexity[^fn-perplexity], exhibit smooth and monotonic improvements when models are scaled along these dimensions."
    },
    {
      "footnote_id": "fn-perplexity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 160,
      "context": "[^fn-perplexity]: **Perplexity**: A measurement of how well a language model predicts text, calculated as 2^(cross-e...",
      "full_line": "[^fn-perplexity]: **Perplexity**: A measurement of how well a language model predicts text, calculated as 2^(cross-entropy loss). GPT-3 achieved ~20 perplexity on WebText, meaning on average it's as confused as if choosing randomly among 20 equally-likely next words."
    },
    {
      "footnote_id": "fn-svms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 551,
      "context": "...constraints, particularly in terms of parallelization. Early algorithms like decision trees and SVMs[^fn-svms] were primarily optimized for single-machine performance, with parallel implementations limited main...",
      "full_line": "During the early decades of machine learning, algorithmic efficiency was closely tied to computational constraints, particularly in terms of parallelization. Early algorithms like decision trees and SVMs[^fn-svms] were primarily optimized for single-machine performance, with parallel implementations limited mainly to ensemble methods[^fn-ensemble] where multiple models could be trained independently on different data batches."
    },
    {
      "footnote_id": "fn-ensemble",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 551,
      "context": "...zed for single-machine performance, with parallel implementations limited mainly to ensemble methods[^fn-ensemble] where multiple models could be trained independently on different data batches.",
      "full_line": "During the early decades of machine learning, algorithmic efficiency was closely tied to computational constraints, particularly in terms of parallelization. Early algorithms like decision trees and SVMs[^fn-svms] were primarily optimized for single-machine performance, with parallel implementations limited mainly to ensemble methods[^fn-ensemble] where multiple models could be trained independently on different data batches."
    },
    {
      "footnote_id": "fn-svms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 553,
      "context": "[^fn-svms]: **Support Vector Machines (SVMs)**: Machine learning algorithm developed by Vapnik in the 1990s, u...",
      "full_line": "[^fn-svms]: **Support Vector Machines (SVMs)**: Machine learning algorithm developed by Vapnik in the 1990s, using the \"kernel trick\" to find optimal decision boundaries. Popular before deep learning, SVMs won most machine learning competitions until ~2010 when deep neural networks gained prominence."
    },
    {
      "footnote_id": "fn-ensemble",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 555,
      "context": "[^fn-ensemble]: **Ensemble Methods**: Techniques combining multiple models to improve performance, like Random For...",
      "full_line": "[^fn-ensemble]: **Ensemble Methods**: Techniques combining multiple models to improve performance, like Random Forest (2001) and Gradient Boosting (1999). Won many Kaggle competitions before deep learning, with XGBoost becoming the go-to method for structured data prediction."
    },
    {
      "footnote_id": "fn-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 557,
      "context": "...acity of the time. Unlike earlier algorithms, neural networks showed potential for model parallelism[^fn-model-parallelism], the ability to distribute model components across multiple processors, though this advantage would...",
      "full_line": "Neural networks also began to emerge during this period, but they were constrained by the limited computational capacity of the time. Unlike earlier algorithms, neural networks showed potential for model parallelism[^fn-model-parallelism], the ability to distribute model components across multiple processors, though this advantage wouldn't be fully realized until the deep learning era. This led to careful optimizations in their design, such as limiting the number of layers or neurons to keep computations manageable. Efficiency was achieved not only through model simplicity but also through innovations in optimization techniques, such as the adoption of stochastic gradient descent[^fn-sgd], which made training more practical for the hardware available."
    },
    {
      "footnote_id": "fn-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 557,
      "context": "...through innovations in optimization techniques, such as the adoption of stochastic gradient descent[^fn-sgd], which made training more practical for the hardware available.",
      "full_line": "Neural networks also began to emerge during this period, but they were constrained by the limited computational capacity of the time. Unlike earlier algorithms, neural networks showed potential for model parallelism[^fn-model-parallelism], the ability to distribute model components across multiple processors, though this advantage wouldn't be fully realized until the deep learning era. This led to careful optimizations in their design, such as limiting the number of layers or neurons to keep computations manageable. Efficiency was achieved not only through model simplicity but also through innovations in optimization techniques, such as the adoption of stochastic gradient descent[^fn-sgd], which made training more practical for the hardware available."
    },
    {
      "footnote_id": "fn-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 559,
      "context": "[^fn-model-parallelism]: **Model Parallelism**: Distributing model components across multiple processors, contrasting with...",
      "full_line": "[^fn-model-parallelism]: **Model Parallelism**: Distributing model components across multiple processors, contrasting with data parallelism. Modern transformer models like GPT-3 require model parallelism due to their 175B parameters exceeding single GPU memory (~24GB for A100)."
    },
    {
      "footnote_id": "fn-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 561,
      "context": "[^fn-sgd]: **Stochastic Gradient Descent (SGD)**: Optimization algorithm using random data samples, introduce...",
      "full_line": "[^fn-sgd]: **Stochastic Gradient Descent (SGD)**: Optimization algorithm using random data samples, introduced by Robbins and Monro (1951). Made neural network training practical by reducing memory requirements from full-batch to single-sample updates, enabling learning on larger datasets."
    },
    {
      "footnote_id": "fn-gpu-deep-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 567,
      "context": "...mitations, now benefited from advancements in computational power, particularly the adoption of GPUs[^fn-gpu-deep-learning] [@Krizhevsky_Sutskever_Hinton_2012]. This capability allowed researchers to train larger, more comp...",
      "full_line": "The introduction of deep learning in the early 2010s marked a turning point for algorithmic efficiency. Neural networks, which had previously been constrained by hardware limitations, now benefited from advancements in computational power, particularly the adoption of GPUs[^fn-gpu-deep-learning] [@Krizhevsky_Sutskever_Hinton_2012]. This capability allowed researchers to train larger, more complex models, leading to breakthroughs in tasks such as image recognition, natural language processing, and speech synthesis."
    },
    {
      "footnote_id": "fn-gpu-deep-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 569,
      "context": "[^fn-gpu-deep-learning]: **GPUs for Deep Learning**: Graphics Processing Units, originally designed for video games, proved...",
      "full_line": "[^fn-gpu-deep-learning]: **GPUs for Deep Learning**: Graphics Processing Units, originally designed for video games, proved ideal for neural network training due to parallel matrix operations. AlexNet's 2012 breakthrough used two GTX 580 GPUs, achieving 15.3% ImageNet error rate\u2014a revolutionary improvement."
    },
    {
      "footnote_id": "fn-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 571,
      "context": "...duce model size and computational requirements without sacrificing accuracy. Techniques like pruning[^fn-pruning] involved removing redundant connections within neural networks, reducing both parameters and comput...",
      "full_line": "However, the growing size and complexity of these models introduced new challenges. Larger models required significant computational resources and memory, making them difficult to deploy in practical applications. To address these challenges, researchers developed techniques to reduce model size and computational requirements without sacrificing accuracy. Techniques like pruning[^fn-pruning] involved removing redundant connections within neural networks, reducing both parameters and computational overhead [@LeCun_Denker_Solla_1990]. Quantization[^fn-quantization] focused on lowering the precision of numerical representations, enabling models to run faster and with less memory [@Jacob_et_al_2018]. Knowledge distillation[^fn-knowledge-distillation] techniques allowed large models to guide the training of smaller, more efficient models, achieving comparable performance with reduced complexity [@Hinton_Vinyals_2015]."
    },
    {
      "footnote_id": "fn-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 571,
      "context": "...tworks, reducing both parameters and computational overhead [@LeCun_Denker_Solla_1990]. Quantization[^fn-quantization] focused on lowering the precision of numerical representations, enabling models to run faster and w...",
      "full_line": "However, the growing size and complexity of these models introduced new challenges. Larger models required significant computational resources and memory, making them difficult to deploy in practical applications. To address these challenges, researchers developed techniques to reduce model size and computational requirements without sacrificing accuracy. Techniques like pruning[^fn-pruning] involved removing redundant connections within neural networks, reducing both parameters and computational overhead [@LeCun_Denker_Solla_1990]. Quantization[^fn-quantization] focused on lowering the precision of numerical representations, enabling models to run faster and with less memory [@Jacob_et_al_2018]. Knowledge distillation[^fn-knowledge-distillation] techniques allowed large models to guide the training of smaller, more efficient models, achieving comparable performance with reduced complexity [@Hinton_Vinyals_2015]."
    },
    {
      "footnote_id": "fn-knowledge-distillation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 571,
      "context": "...ions, enabling models to run faster and with less memory [@Jacob_et_al_2018]. Knowledge distillation[^fn-knowledge-distillation] techniques allowed large models to guide the training of smaller, more efficient models, achieving...",
      "full_line": "However, the growing size and complexity of these models introduced new challenges. Larger models required significant computational resources and memory, making them difficult to deploy in practical applications. To address these challenges, researchers developed techniques to reduce model size and computational requirements without sacrificing accuracy. Techniques like pruning[^fn-pruning] involved removing redundant connections within neural networks, reducing both parameters and computational overhead [@LeCun_Denker_Solla_1990]. Quantization[^fn-quantization] focused on lowering the precision of numerical representations, enabling models to run faster and with less memory [@Jacob_et_al_2018]. Knowledge distillation[^fn-knowledge-distillation] techniques allowed large models to guide the training of smaller, more efficient models, achieving comparable performance with reduced complexity [@Hinton_Vinyals_2015]."
    },
    {
      "footnote_id": "fn-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 573,
      "context": "[^fn-pruning]: **Pruning**: Removing unimportant neural network connections to reduce model size. Han et al. (201...",
      "full_line": "[^fn-pruning]: **Pruning**: Removing unimportant neural network connections to reduce model size. Han et al. (2015) achieved 9\u00d7 compression on AlexNet without accuracy loss, removing 89% of parameters while maintaining ImageNet performance."
    },
    {
      "footnote_id": "fn-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 575,
      "context": "[^fn-quantization]: **Quantization**: Reducing numerical precision from 32-bit floats to 8-bit integers or lower. Can...",
      "full_line": "[^fn-quantization]: **Quantization**: Reducing numerical precision from 32-bit floats to 8-bit integers or lower. Can achieve 4\u00d7 memory reduction and speed improvements while maintaining ~99% of original accuracy in many models."
    },
    {
      "footnote_id": "fn-knowledge-distillation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 577,
      "context": "[^fn-knowledge-distillation]: **Knowledge Distillation**: Training small \"student\" models to mimic large \"teacher\" models. Disti...",
      "full_line": "[^fn-knowledge-distillation]: **Knowledge Distillation**: Training small \"student\" models to mimic large \"teacher\" models. DistilBERT achieved 97% of BERT's performance with 40% fewer parameters and 60% faster inference speed."
    },
    {
      "footnote_id": "fn-mobilenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 579,
      "context": "...me, new architectures specifically designed for efficiency began to emerge. Models such as MobileNet[^fn-mobilenet] [@Howard_et_al_2017], EfficientNet[^fn-efficientnet] [@Tan_Le_2019], and SqueezeNet[^fn-squeezenet]...",
      "full_line": "At the same time, new architectures specifically designed for efficiency began to emerge. Models such as MobileNet[^fn-mobilenet] [@Howard_et_al_2017], EfficientNet[^fn-efficientnet] [@Tan_Le_2019], and SqueezeNet[^fn-squeezenet] [@Iandola_et_al_2016] demonstrated that compact designs could deliver high performance, enabling their deployment on devices with limited computational power, such as smartphones and IoT devices."
    },
    {
      "footnote_id": "fn-efficientnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 579,
      "context": "...ficiency began to emerge. Models such as MobileNet[^fn-mobilenet] [@Howard_et_al_2017], EfficientNet[^fn-efficientnet] [@Tan_Le_2019], and SqueezeNet[^fn-squeezenet] [@Iandola_et_al_2016] demonstrated that compact desi...",
      "full_line": "At the same time, new architectures specifically designed for efficiency began to emerge. Models such as MobileNet[^fn-mobilenet] [@Howard_et_al_2017], EfficientNet[^fn-efficientnet] [@Tan_Le_2019], and SqueezeNet[^fn-squeezenet] [@Iandola_et_al_2016] demonstrated that compact designs could deliver high performance, enabling their deployment on devices with limited computational power, such as smartphones and IoT devices."
    },
    {
      "footnote_id": "fn-squeezenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 579,
      "context": "...t[^fn-mobilenet] [@Howard_et_al_2017], EfficientNet[^fn-efficientnet] [@Tan_Le_2019], and SqueezeNet[^fn-squeezenet] [@Iandola_et_al_2016] demonstrated that compact designs could deliver high performance, enabling th...",
      "full_line": "At the same time, new architectures specifically designed for efficiency began to emerge. Models such as MobileNet[^fn-mobilenet] [@Howard_et_al_2017], EfficientNet[^fn-efficientnet] [@Tan_Le_2019], and SqueezeNet[^fn-squeezenet] [@Iandola_et_al_2016] demonstrated that compact designs could deliver high performance, enabling their deployment on devices with limited computational power, such as smartphones and IoT devices."
    },
    {
      "footnote_id": "fn-mobilenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 583,
      "context": "[^fn-mobilenet]: **MobileNet**: Efficient neural network architecture using depthwise separable convolutions, achie...",
      "full_line": "[^fn-mobilenet]: **MobileNet**: Efficient neural network architecture using depthwise separable convolutions, achieving ~50\u00d7 fewer parameters than traditional models. MobileNet-v1 has only 4.2M parameters vs. VGG-16's 138M, enabling deployment on smartphones with <100MB memory."
    },
    {
      "footnote_id": "fn-efficientnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 585,
      "context": "[^fn-efficientnet]: **EfficientNet**: Architecture achieving state-of-the-art accuracy with 10\u00d7 fewer parameters than...",
      "full_line": "[^fn-efficientnet]: **EfficientNet**: Architecture achieving state-of-the-art accuracy with 10\u00d7 fewer parameters than previous models. EfficientNet-B7 achieves 84.3% ImageNet top-1 accuracy with 66M parameters, compared to ResNet-152's 60M parameters achieving 78.3%."
    },
    {
      "footnote_id": "fn-squeezenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 587,
      "context": "[^fn-squeezenet]: **SqueezeNet**: Compact CNN architecture achieving AlexNet-level accuracy with 50\u00d7 fewer parameter...",
      "full_line": "[^fn-squeezenet]: **SqueezeNet**: Compact CNN architecture achieving AlexNet-level accuracy with 50\u00d7 fewer parameters (1.25M vs. 60M). Demonstrated that clever architecture design can dramatically reduce model size without sacrificing performance."
    },
    {
      "footnote_id": "fn-gpt4",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 589,
      "context": "...challenges require balancing performance with resource efficiency, particularly as models like GPT-4[^fn-gpt4] and beyond are applied to increasingly diverse tasks and environments. One emerging approach involv...",
      "full_line": "As machine learning systems continue to grow in scale and complexity, the focus on algorithmic efficiency has expanded to address sustainability and scalability. Today's challenges require balancing performance with resource efficiency, particularly as models like GPT-4[^fn-gpt4] and beyond are applied to increasingly diverse tasks and environments. One emerging approach involves parameter reduction, where only the most critical parameters of a model are retained, significantly reducing computational and memory demands. Hardware-aware design has also become a priority, as researchers optimize models to take full advantage of specific accelerators, such as GPUs, TPUs[^fn-tpu], and edge processors. Another important trend is parameter-efficient fine-tuning[^fn-param-efficient], where large pre-trained models can be adapted to new tasks by updating only a small subset of parameters. Techniques like parameter-efficient fine-tuning and prompt-tuning exemplify this approach, allowing systems to achieve task-specific performance while maintaining the efficiency advantages of smaller models."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 589,
      "context": "..., as researchers optimize models to take full advantage of specific accelerators, such as GPUs, TPUs[^fn-tpu], and edge processors. Another important trend is parameter-efficient fine-tuning[^fn-param-efficien...",
      "full_line": "As machine learning systems continue to grow in scale and complexity, the focus on algorithmic efficiency has expanded to address sustainability and scalability. Today's challenges require balancing performance with resource efficiency, particularly as models like GPT-4[^fn-gpt4] and beyond are applied to increasingly diverse tasks and environments. One emerging approach involves parameter reduction, where only the most critical parameters of a model are retained, significantly reducing computational and memory demands. Hardware-aware design has also become a priority, as researchers optimize models to take full advantage of specific accelerators, such as GPUs, TPUs[^fn-tpu], and edge processors. Another important trend is parameter-efficient fine-tuning[^fn-param-efficient], where large pre-trained models can be adapted to new tasks by updating only a small subset of parameters. Techniques like parameter-efficient fine-tuning and prompt-tuning exemplify this approach, allowing systems to achieve task-specific performance while maintaining the efficiency advantages of smaller models."
    },
    {
      "footnote_id": "fn-param-efficient",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 589,
      "context": "...GPUs, TPUs[^fn-tpu], and edge processors. Another important trend is parameter-efficient fine-tuning[^fn-param-efficient], where large pre-trained models can be adapted to new tasks by updating only a small subset of para...",
      "full_line": "As machine learning systems continue to grow in scale and complexity, the focus on algorithmic efficiency has expanded to address sustainability and scalability. Today's challenges require balancing performance with resource efficiency, particularly as models like GPT-4[^fn-gpt4] and beyond are applied to increasingly diverse tasks and environments. One emerging approach involves parameter reduction, where only the most critical parameters of a model are retained, significantly reducing computational and memory demands. Hardware-aware design has also become a priority, as researchers optimize models to take full advantage of specific accelerators, such as GPUs, TPUs[^fn-tpu], and edge processors. Another important trend is parameter-efficient fine-tuning[^fn-param-efficient], where large pre-trained models can be adapted to new tasks by updating only a small subset of parameters. Techniques like parameter-efficient fine-tuning and prompt-tuning exemplify this approach, allowing systems to achieve task-specific performance while maintaining the efficiency advantages of smaller models."
    },
    {
      "footnote_id": "fn-gpt4",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 591,
      "context": "[^fn-gpt4]: **GPT-4**: OpenAI's most advanced language model as of 2023, reportedly using a mixture-of-experts...",
      "full_line": "[^fn-gpt4]: **GPT-4**: OpenAI's most advanced language model as of 2023, reportedly using a mixture-of-experts architecture with ~1.8 trillion parameters. Training cost estimated at >$100 million, highlighting the extreme resource requirements of frontier models."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 593,
      "context": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for machine learni...",
      "full_line": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for machine learning workloads, first announced in 2016. TPU v4 delivers 275 teraFLOPs with 90% lower energy per operation compared to general-purpose processors."
    },
    {
      "footnote_id": "fn-param-efficient",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 595,
      "context": "[^fn-param-efficient]: **Parameter-Efficient Fine-tuning**: Methods like LoRA and Adapters that update <1% of model param...",
      "full_line": "[^fn-param-efficient]: **Parameter-Efficient Fine-tuning**: Methods like LoRA and Adapters that update <1% of model parameters while achieving full fine-tuning performance. Reduces memory requirements from gigabytes to megabytes for large model adaptation."
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 715,
      "context": "...go-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-alexnet]-level performance on ImageNet[^fn-imagenet] classification had decreased by $44\\times$ compared to...",
      "full_line": "Notably, as @fig-algo-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-alexnet]-level performance on ImageNet[^fn-imagenet] classification had decreased by $44\\times$ compared to 2012. This improvement, which halved every 16 months, outpaced the hardware efficiency gains of Moore's Law[^fn-moores-law]. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongside hardware innovations [@Hernandez_et_al_2020]."
    },
    {
      "footnote_id": "fn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 715,
      "context": "...urces needed to train a neural network to achieve AlexNet[^fn-alexnet]-level performance on ImageNet[^fn-imagenet] classification had decreased by $44\\times$ compared to 2012. This improvement, which halved every 1...",
      "full_line": "Notably, as @fig-algo-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-alexnet]-level performance on ImageNet[^fn-imagenet] classification had decreased by $44\\times$ compared to 2012. This improvement, which halved every 16 months, outpaced the hardware efficiency gains of Moore's Law[^fn-moores-law]. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongside hardware innovations [@Hernandez_et_al_2020]."
    },
    {
      "footnote_id": "fn-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 715,
      "context": "...his improvement, which halved every 16 months, outpaced the hardware efficiency gains of Moore's Law[^fn-moores-law]. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongs...",
      "full_line": "Notably, as @fig-algo-efficiency shows, the computational resources needed to train a neural network to achieve AlexNet[^fn-alexnet]-level performance on ImageNet[^fn-imagenet] classification had decreased by $44\\times$ compared to 2012. This improvement, which halved every 16 months, outpaced the hardware efficiency gains of Moore's Law[^fn-moores-law]. Such rapid progress demonstrates the role of algorithmic advancements in driving efficiency alongside hardware innovations [@Hernandez_et_al_2020]."
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 717,
      "context": "[^fn-alexnet]: **AlexNet**: Groundbreaking CNN by Krizhevsky, Sutskever, and Hinton (2012) that won ImageNet with...",
      "full_line": "[^fn-alexnet]: **AlexNet**: Groundbreaking CNN by Krizhevsky, Sutskever, and Hinton (2012) that won ImageNet with 15.3% error rate, nearly halving the previous best of 26.2%. Used 60M parameters, two GPUs, and launched the deep learning revolution."
    },
    {
      "footnote_id": "fn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 719,
      "context": "[^fn-imagenet]: **ImageNet**: Large-scale visual recognition dataset with 14+ million images across 20,000+ catego...",
      "full_line": "[^fn-imagenet]: **ImageNet**: Large-scale visual recognition dataset with 14+ million images across 20,000+ categories. The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) drove computer vision breakthroughs from 2010-2017."
    },
    {
      "footnote_id": "fn-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 721,
      "context": "[^fn-moores-law]: **Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles...",
      "full_line": "[^fn-moores-law]: **Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every ~2 years. Hardware improvements follow ~2x every 18-24 months, while AI algorithmic efficiency improved 44x in 7 years (2012-2019)."
    },
    {
      "footnote_id": "fn-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 725,
      "context": "[^fn-resnet]: **ResNet**: Residual Network architecture by He et al. (2015) enabling training of very deep netwo...",
      "full_line": "[^fn-resnet]: **ResNet**: Residual Network architecture by He et al. (2015) enabling training of very deep networks (152+ layers) through skip connections. Won ImageNet 2015 with 3.6% error rate, surpassing human-level performance for the first time."
    },
    {
      "footnote_id": "fn-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 739,
      "context": "...2010s brought a seismic shift in the landscape of compute efficiency. Models like AlexNet and ResNet[^fn-resnet] showed the potential of neural networks, but their computational demands quickly surpassed the capa...",
      "full_line": "The introduction of deep learning in the early 2010s brought a seismic shift in the landscape of compute efficiency. Models like AlexNet and ResNet[^fn-resnet] showed the potential of neural networks, but their computational demands quickly surpassed the capabilities of traditional CPUs. As shown in @fig-comp_efficiency, this marked the beginning of an era of exponential growth in compute usage. OpenAI's analysis reveals that the amount of compute used in AI training has increased 300,000 times since 2012, doubling approximately every 3.4 months\u2014a rate far exceeding Moore's Law [@Amodei_et_al_2018]."
    },
    {
      "footnote_id": "fn-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 992,
      "context": "...itting workloads across multiple machines. Techniques such as model parallelism and data parallelism[^fn-data-parallelism] allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to max...",
      "full_line": "Another focus is on distributed systems, where compute efficiency is achieved by splitting workloads across multiple machines. Techniques such as model parallelism and data parallelism[^fn-data-parallelism] allow large-scale models to be trained more efficiently, leveraging clusters of GPUs or TPUs to maximize throughput. These methods reduce training times while minimizing the idle time of hardware resources."
    },
    {
      "footnote_id": "fn-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 994,
      "context": "[^fn-data-parallelism]: **Data Parallelism**: Training method where the same model runs on multiple processors with differ...",
      "full_line": "[^fn-data-parallelism]: **Data Parallelism**: Training method where the same model runs on multiple processors with different data batches. GPT-3 training used data parallelism across thousands of GPUs, processing multiple text sequences simultaneously to achieve massive scale."
    },
    {
      "footnote_id": "fn-uci",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1010,
      "context": "...avily on curated datasets, such as [UCI's Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci], which provided clean, well-structured data for experimentation. Feature selection and dimensionali...",
      "full_line": "In the early days of machine learning, data efficiency was not a significant focus, largely because datasets were relatively small and manageable. The challenge during this period was often acquiring enough labeled data to train models effectively. Researchers relied heavily on curated datasets, such as [UCI's Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci], which provided clean, well-structured data for experimentation. Feature selection and dimensionality reduction techniques, such as principal component analysis (PCA)[^fn-pca], were common methods for ensuring that models extracted the most valuable information from limited data."
    },
    {
      "footnote_id": "fn-pca",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1010,
      "context": "...eature selection and dimensionality reduction techniques, such as principal component analysis (PCA)[^fn-pca], were common methods for ensuring that models extracted the most valuable information from limited...",
      "full_line": "In the early days of machine learning, data efficiency was not a significant focus, largely because datasets were relatively small and manageable. The challenge during this period was often acquiring enough labeled data to train models effectively. Researchers relied heavily on curated datasets, such as [UCI's Machine Learning Repository](https://archive.ics.uci.edu/)[^fn-uci], which provided clean, well-structured data for experimentation. Feature selection and dimensionality reduction techniques, such as principal component analysis (PCA)[^fn-pca], were common methods for ensuring that models extracted the most valuable information from limited data."
    },
    {
      "footnote_id": "fn-uci",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1012,
      "context": "[^fn-uci]: **UCI Machine Learning Repository**: Established in 1987 by the University of California, Irvine,...",
      "full_line": "[^fn-uci]: **UCI Machine Learning Repository**: Established in 1987 by the University of California, Irvine, one of the most widely-used resources for machine learning datasets. Contains over 600 datasets and has been cited in thousands of research papers, serving as a cornerstone for early ML research."
    },
    {
      "footnote_id": "fn-pca",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1014,
      "context": "[^fn-pca]: **Principal Component Analysis (PCA)**: Dimensionality reduction technique invented by Karl Pearso...",
      "full_line": "[^fn-pca]: **Principal Component Analysis (PCA)**: Dimensionality reduction technique invented by Karl Pearson in 1901, identifies the most important directions of variation in data. Reduces computational complexity while preserving 90%+ of data variance in many applications."
    },
    {
      "footnote_id": "fn-mnist",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1016,
      "context": "...nd data cleaning. Algorithms were designed to work well with relatively small datasets such as MNIST[^fn-mnist] [@deng2012mnist], Caltech 101 [@FeiFei2004LearningGV] and CIFAR-10[^fn-cifar10] [@Krizhevsky09learn...",
      "full_line": "During this era, data efficiency was achieved through careful preprocessing and data cleaning. Algorithms were designed to work well with relatively small datasets such as MNIST[^fn-mnist] [@deng2012mnist], Caltech 101 [@FeiFei2004LearningGV] and CIFAR-10[^fn-cifar10] [@Krizhevsky09learningmultiple], and computational limitations reinforced the need for data parsimony. These constraints shaped the development of techniques that maximized performance with minimal data, ensuring that every data point contributed meaningfully to the learning process."
    },
    {
      "footnote_id": "fn-cifar10",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1016,
      "context": "...datasets such as MNIST[^fn-mnist] [@deng2012mnist], Caltech 101 [@FeiFei2004LearningGV] and CIFAR-10[^fn-cifar10] [@Krizhevsky09learningmultiple], and computational limitations reinforced the need for data parsimo...",
      "full_line": "During this era, data efficiency was achieved through careful preprocessing and data cleaning. Algorithms were designed to work well with relatively small datasets such as MNIST[^fn-mnist] [@deng2012mnist], Caltech 101 [@FeiFei2004LearningGV] and CIFAR-10[^fn-cifar10] [@Krizhevsky09learningmultiple], and computational limitations reinforced the need for data parsimony. These constraints shaped the development of techniques that maximized performance with minimal data, ensuring that every data point contributed meaningfully to the learning process."
    },
    {
      "footnote_id": "fn-mnist",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1022,
      "context": "[^fn-mnist]: **MNIST**: Modified National Institute of Standards and Technology database of handwritten digits,...",
      "full_line": "[^fn-mnist]: **MNIST**: Modified National Institute of Standards and Technology database of handwritten digits, containing 70,000 28\u00d728 pixel images. Created in 1998, became the \"Hello World\" of computer vision, though modern models achieve >99% accuracy."
    },
    {
      "footnote_id": "fn-cifar10",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1024,
      "context": "[^fn-cifar10]: **CIFAR-10**: Canadian Institute for Advanced Research dataset with 60,000 32\u00d732 color images acro...",
      "full_line": "[^fn-cifar10]: **CIFAR-10**: Canadian Institute for Advanced Research dataset with 60,000 32\u00d732 color images across 10 classes. Released in 2009, remains a standard benchmark despite its small image size by modern standards."
    },
    {
      "footnote_id": "fn-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1026,
      "context": "...veloped techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning[^fn-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specifi...",
      "full_line": "However, this reliance on large datasets introduced significant inefficiencies. Data collection became a costly and time-consuming endeavor, requiring vast amounts of labeled data for supervised learning tasks. To address these challenges, researchers developed techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning[^fn-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specific data [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creating new variations of existing samples. Additionally, active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort while maintaining performance [@Settles_2009]."
    },
    {
      "footnote_id": "fn-data-augmentation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1026,
      "context": "...er datasets, reducing the need for task-specific data [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creatin...",
      "full_line": "However, this reliance on large datasets introduced significant inefficiencies. Data collection became a costly and time-consuming endeavor, requiring vast amounts of labeled data for supervised learning tasks. To address these challenges, researchers developed techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning[^fn-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specific data [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creating new variations of existing samples. Additionally, active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort while maintaining performance [@Settles_2009]."
    },
    {
      "footnote_id": "fn-active-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1026,
      "context": "...ally expanded datasets by creating new variations of existing samples. Additionally, active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort...",
      "full_line": "However, this reliance on large datasets introduced significant inefficiencies. Data collection became a costly and time-consuming endeavor, requiring vast amounts of labeled data for supervised learning tasks. To address these challenges, researchers developed techniques to enhance data efficiency, even as datasets continued to grow. Transfer learning[^fn-transfer-learning] allowed pre-trained models to be fine-tuned on smaller datasets, reducing the need for task-specific data [@yosinski2014transferable]. Data augmentation[^fn-data-augmentation] techniques, such as image rotations or text paraphrasing, artificially expanded datasets by creating new variations of existing samples. Additionally, active learning[^fn-active-learning] prioritized labeling only the most informative data points, minimizing the overall labeling effort while maintaining performance [@Settles_2009]."
    },
    {
      "footnote_id": "fn-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1034,
      "context": "[^fn-transfer-learning]: **Transfer Learning**: Technique where models pre-trained on large datasets are fine-tuned for spe...",
      "full_line": "[^fn-transfer-learning]: **Transfer Learning**: Technique where models pre-trained on large datasets are fine-tuned for specific tasks. ImageNet pre-trained models can achieve high accuracy on new vision tasks with <1000 labeled examples vs. millions needed from scratch."
    },
    {
      "footnote_id": "fn-data-augmentation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1036,
      "context": "[^fn-data-augmentation]: **Data Augmentation**: Artificially expanding datasets through transformations like rotations, cro...",
      "full_line": "[^fn-data-augmentation]: **Data Augmentation**: Artificially expanding datasets through transformations like rotations, crops, or noise. Can improve model performance by 5-15% and reduce overfitting, especially valuable when labeled data is scarce."
    },
    {
      "footnote_id": "fn-active-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1038,
      "context": "[^fn-active-learning]: **Active Learning**: Iteratively selecting the most informative samples for labeling to maximize l...",
      "full_line": "[^fn-active-learning]: **Active Learning**: Iteratively selecting the most informative samples for labeling to maximize learning efficiency. Can achieve target performance with 50-90% less labeled data compared to random sampling strategies."
    },
    {
      "footnote_id": "fn-data-centric-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1040,
      "context": "Data-centric AI[^fn-data-centric-ai] has emerged as a key paradigm, emphasizing the importance of data quality over quantity. This appro...",
      "full_line": "Data-centric AI[^fn-data-centric-ai] has emerged as a key paradigm, emphasizing the importance of data quality over quantity. This approach focuses on enhancing data preprocessing, removing redundancy, and improving labeling efficiency. Research has shown that careful curation and filtering of datasets can achieve comparable or superior model performance while using only a fraction of the original data volume. For instance, systematic analyses of web-scale datasets demonstrate that targeted filtering techniques can maintain model capabilities while significantly reducing training data requirements [@penedo2024fineweb]."
    },
    {
      "footnote_id": "fn-data-centric-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1042,
      "context": "[^fn-data-centric-ai]: **Data-Centric AI**: Paradigm shift from model-centric to data-centric development, popularized by...",
      "full_line": "[^fn-data-centric-ai]: **Data-Centric AI**: Paradigm shift from model-centric to data-centric development, popularized by Andrew Ng in 2021. Focuses on systematically improving data quality rather than just model architecture, often yielding greater performance gains."
    },
    {
      "footnote_id": "fn-self-supervised",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1044,
      "context": "...techniques have emerged to support this transition toward data efficiency. Self-supervised learning[^fn-self-supervised] enables models to learn meaningful representations from unlabeled data, reducing the dependency on...",
      "full_line": "Several techniques have emerged to support this transition toward data efficiency. Self-supervised learning[^fn-self-supervised] enables models to learn meaningful representations from unlabeled data, reducing the dependency on expensive human-labeled datasets. Active learning strategies selectively identify the most informative examples for labeling, while curriculum learning[^fn-curriculum-learning] structures the training process to progress from simple to complex examples, improving learning efficiency. These approaches work together to minimize data requirements while maintaining model performance."
    },
    {
      "footnote_id": "fn-curriculum-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1044,
      "context": "...trategies selectively identify the most informative examples for labeling, while curriculum learning[^fn-curriculum-learning] structures the training process to progress from simple to complex examples, improving learning eff...",
      "full_line": "Several techniques have emerged to support this transition toward data efficiency. Self-supervised learning[^fn-self-supervised] enables models to learn meaningful representations from unlabeled data, reducing the dependency on expensive human-labeled datasets. Active learning strategies selectively identify the most informative examples for labeling, while curriculum learning[^fn-curriculum-learning] structures the training process to progress from simple to complex examples, improving learning efficiency. These approaches work together to minimize data requirements while maintaining model performance."
    },
    {
      "footnote_id": "fn-self-supervised",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1046,
      "context": "[^fn-self-supervised]: **Self-Supervised Learning**: Training method where models create their own labels from input data...",
      "full_line": "[^fn-self-supervised]: **Self-Supervised Learning**: Training method where models create their own labels from input data structure, like predicting masked words in BERT or next frames in videos. Enables learning from billions of unlabeled examples, revolutionizing NLP and computer vision."
    },
    {
      "footnote_id": "fn-curriculum-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1048,
      "context": "[^fn-curriculum-learning]: **Curriculum Learning**: Training strategy where models learn from easy examples before progressin...",
      "full_line": "[^fn-curriculum-learning]: **Curriculum Learning**: Training strategy where models learn from easy examples before progressing to harder ones, mimicking human education. Can improve convergence speed by 25-50% and final model performance across various domains."
    },
    {
      "footnote_id": "fn-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1050,
      "context": "The importance of data efficiency is particularly evident in foundation models[^fn-foundation-models]. As these models grow in scale and capability, they are approaching the limits of available high-qu...",
      "full_line": "The importance of data efficiency is particularly evident in foundation models[^fn-foundation-models]. As these models grow in scale and capability, they are approaching the limits of available high-quality training data, especially for language tasks [@fig-running-out-of-human-data]. This scarcity drives innovation in data processing and curation techniques, pushing the field to develop more sophisticated approaches to data efficiency."
    },
    {
      "footnote_id": "fn-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1054,
      "context": "[^fn-foundation-models]: **Foundation Models**: Large-scale, general-purpose AI models trained on broad data that can be ad...",
      "full_line": "[^fn-foundation-models]: **Foundation Models**: Large-scale, general-purpose AI models trained on broad data that can be adapted for many tasks. Term coined by Stanford HAI in 2021, includes models like GPT-3, BERT, and DALL-E with billions of parameters."
    },
    {
      "footnote_id": "fn-tinyml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1056,
      "context": "Evidence for the impact of data quality appears across different scales of deployment. In Tiny ML[^fn-tinyml] applications, datasets like Wake Vision demonstrate how model performance critically depends on car...",
      "full_line": "Evidence for the impact of data quality appears across different scales of deployment. In Tiny ML[^fn-tinyml] applications, datasets like Wake Vision demonstrate how model performance critically depends on careful data curation [@banbury2024wakevisiontailoreddataset]. At larger scales, research on language models trained on web-scale datasets shows that intelligent filtering and selection strategies can significantly improve performance on downstream tasks [@penedo2024fineweb]."
    },
    {
      "footnote_id": "fn-tinyml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1058,
      "context": "[^fn-tinyml]: **TinyML**: Machine learning on microcontrollers and edge devices with <1KB-1MB memory and <1mW po...",
      "full_line": "[^fn-tinyml]: **TinyML**: Machine learning on microcontrollers and edge devices with <1KB-1MB memory and <1mW power consumption. Enables AI in IoT devices, wearables, and sensors where traditional ML deployment is impossible due to resource constraints."
    },
    {
      "footnote_id": "fn-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 116,
      "context": "...-vector multiplications. The Basic Linear Algebra Subprograms ([BLAS](https://www.netlib.org/blas/))[^fn-blas], developed in 1979, provided these essential matrix operations that would become the computational...",
      "full_line": "The foundation for modern ML frameworks begins at the most fundamental level of computation: matrix operations. Machine learning computations are primarily matrix-matrix and matrix-vector multiplications. The Basic Linear Algebra Subprograms ([BLAS](https://www.netlib.org/blas/))[^fn-blas], developed in 1979, provided these essential matrix operations that would become the computational backbone of machine learning [@kung1979systolic]. These low-level operations, when combined and executed efficiently, enable the complex calculations required for training neural networks and other ML models."
    },
    {
      "footnote_id": "fn-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 118,
      "context": "[^fn-blas]: **BLAS (Basic Linear Algebra Subprograms)**: Originally developed at Argonne National Laboratory,...",
      "full_line": "[^fn-blas]: **BLAS (Basic Linear Algebra Subprograms)**: Originally developed at Argonne National Laboratory, BLAS became the de facto standard for linear algebra operations, with Level 1 (vector-vector), Level 2 (matrix-vector), and Level 3 (matrix-matrix) operations that still underpin every modern ML framework."
    },
    {
      "footnote_id": "fn-lapack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 120,
      "context": "Building upon BLAS, the Linear Algebra Package ([LAPACK](https://www.netlib.org/lapack/))[^fn-lapack] emerged in 1992, extending these capabilities with more sophisticated linear algebra operations suc...",
      "full_line": "Building upon BLAS, the Linear Algebra Package ([LAPACK](https://www.netlib.org/lapack/))[^fn-lapack] emerged in 1992, extending these capabilities with more sophisticated linear algebra operations such as matrix decompositions, eigenvalue problems, and linear system solutions. This layered approach of building increasingly complex operations from fundamental matrix computations became a defining characteristic of ML frameworks."
    },
    {
      "footnote_id": "fn-lapack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 122,
      "context": "[^fn-lapack]: **LAPACK (Linear Algebra Package)**: Succeeded LINPACK and EISPACK, introducing block algorithms t...",
      "full_line": "[^fn-lapack]: **LAPACK (Linear Algebra Package)**: Succeeded LINPACK and EISPACK, introducing block algorithms that dramatically improved cache efficiency and parallel execution - innovations that became essential as datasets grew from megabytes to terabytes."
    },
    {
      "footnote_id": "fn-theano",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 134,
      "context": "[Theano](https://github.com/Theano/Theano)[^fn-theano], which appeared in 2007, was a major advancement, which was developed at the Montreal Institute for...",
      "full_line": "[Theano](https://github.com/Theano/Theano)[^fn-theano], which appeared in 2007, was a major advancement, which was developed at the Montreal Institute for Learning Algorithms, MILA, Theano introduced two revolutionary concepts: computational graphs[^fn-comp-graphs] and GPU acceleration [@al2016theano]. Computational graphs represented mathematical operations as directed graphs, with matrix operations as nodes and data flowing between them. This graph-based approach allowed for automatic differentiation and optimization of the underlying matrix operations. More importantly, it enabled the framework to automatically route these operations to GPU hardware, dramatically accelerating matrix computations."
    },
    {
      "footnote_id": "fn-comp-graphs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 134,
      "context": "...te for Learning Algorithms, MILA, Theano introduced two revolutionary concepts: computational graphs[^fn-comp-graphs] and GPU acceleration [@al2016theano]. Computational graphs represented mathematical operations as d...",
      "full_line": "[Theano](https://github.com/Theano/Theano)[^fn-theano], which appeared in 2007, was a major advancement, which was developed at the Montreal Institute for Learning Algorithms, MILA, Theano introduced two revolutionary concepts: computational graphs[^fn-comp-graphs] and GPU acceleration [@al2016theano]. Computational graphs represented mathematical operations as directed graphs, with matrix operations as nodes and data flowing between them. This graph-based approach allowed for automatic differentiation and optimization of the underlying matrix operations. More importantly, it enabled the framework to automatically route these operations to GPU hardware, dramatically accelerating matrix computations."
    },
    {
      "footnote_id": "fn-theano",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 136,
      "context": "[^fn-theano]: **Theano**: Named after the ancient Greek mathematician Theano of Croton, this framework pioneered...",
      "full_line": "[^fn-theano]: **Theano**: Named after the ancient Greek mathematician Theano of Croton, this framework pioneered the concept of symbolic mathematical expressions in Python, laying the groundwork for every modern deep learning framework."
    },
    {
      "footnote_id": "fn-comp-graphs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 138,
      "context": "[^fn-comp-graphs]: **Computational Graphs**: First formalized in automatic differentiation literature by Wengert (196...",
      "full_line": "[^fn-comp-graphs]: **Computational Graphs**: First formalized in automatic differentiation literature by Wengert (1964), this representation became the backbone of modern ML frameworks, enabling both forward and reverse-mode differentiation at unprecedented scale."
    },
    {
      "footnote_id": "fn-tensorflow",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 150,
      "context": "Google's [TensorFlow](https://www.tensorflow.org/)[^fn-tensorflow], introduced in 2015, revolutionized the field by treating matrix operations as part of a distribute...",
      "full_line": "Google's [TensorFlow](https://www.tensorflow.org/)[^fn-tensorflow], introduced in 2015, revolutionized the field by treating matrix operations as part of a distributed computing problem [@dean2012large]. It represented all computations, from individual matrix multiplications to entire neural networks, as a static computational graph that could be split across multiple devices. This approach enabled training of unprecedented model sizes by distributing matrix operations across clusters of computers and specialized hardware. TensorFlow's static graph approach, while initially constraining, allowed for aggressive optimization of matrix operations through techniques like kernel fusion (combining multiple operations into a single kernel for efficiency) and memory planning (pre-allocating memory for operations)."
    },
    {
      "footnote_id": "fn-tensorflow",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 152,
      "context": "[^fn-tensorflow]: **TensorFlow**: Named after tensor operations flowing through computational graphs, this framework...",
      "full_line": "[^fn-tensorflow]: **TensorFlow**: Named after tensor operations flowing through computational graphs, this framework democratized distributed machine learning by open-sourcing Google's internal DistBelief system, instantly giving researchers access to infrastructure that previously required massive corporate resources."
    },
    {
      "footnote_id": "fn-pytorch",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 156,
      "context": "Facebook's [PyTorch](https://pytorch.org/)[^fn-pytorch], also launched in 2016, took a radically different approach to handling matrix computations. Instea...",
      "full_line": "Facebook's [PyTorch](https://pytorch.org/)[^fn-pytorch], also launched in 2016, took a radically different approach to handling matrix computations. Instead of static graphs, PyTorch introduced dynamic computational graphs that could be modified on the fly [@paszke2019pytorch]. This dynamic approach, while potentially sacrificing some optimization opportunities, made it much easier for researchers to debug and understand the flow of matrix operations in their models. PyTorch's success demonstrated that the ability to introspect and modify computations dynamically was as important as raw performance for many applications."
    },
    {
      "footnote_id": "fn-pytorch",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 158,
      "context": "[^fn-pytorch]: **PyTorch**: Inspired by the original Torch framework from NYU, PyTorch brought \"define-by-run\" se...",
      "full_line": "[^fn-pytorch]: **PyTorch**: Inspired by the original Torch framework from NYU, PyTorch brought \"define-by-run\" semantics to Python, enabling researchers to modify models during execution - a breakthrough that accelerated research by making debugging as simple as using a standard Python debugger."
    },
    {
      "footnote_id": "fn-jax",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 164,
      "context": "Google's [JAX](https://github.com/google/jax)[^fn-jax], introduced in 2018, brought functional programming principles to deep learning computations, enabl...",
      "full_line": "Google's [JAX](https://github.com/google/jax)[^fn-jax], introduced in 2018, brought functional programming principles to deep learning computations, enabling new patterns of model development [@jax2018github]. [FastAI](https://www.fast.ai/) built upon PyTorch to package common deep learning patterns into reusable components, making advanced techniques more accessible to practitioners [@howard2020fastai]. These higher-level frameworks demonstrated how abstraction could simplify development while maintaining the performance benefits of their underlying implementations."
    },
    {
      "footnote_id": "fn-jax",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 166,
      "context": "[^fn-jax]: **JAX**: Stands for \"Just After eXecution\" and combines NumPy's API with functional programming tr...",
      "full_line": "[^fn-jax]: **JAX**: Stands for \"Just After eXecution\" and combines NumPy's API with functional programming transforms (jit, grad, vmap, pmap), enabling researchers to write concise code that automatically scales to TPUs and GPU clusters while maintaining NumPy compatibility."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 172,
      "context": "...utionized framework design. [Google's Tensor Processing Units (TPUs)](https://cloud.google.com/tpu/)[^fn-tpu], first deployed in 2016, were purpose-built for tensor operations, the fundamental building blocks...",
      "full_line": "The development of hardware-specific accelerators further revolutionized framework design. [Google's Tensor Processing Units (TPUs)](https://cloud.google.com/tpu/)[^fn-tpu], first deployed in 2016, were purpose-built for tensor operations, the fundamental building blocks of deep learning computations. TPUs introduced systolic array architectures, which are particularly efficient for matrix multiplication and convolution operations. This hardware architecture prompted frameworks like TensorFlow to develop specialized compilation strategies that could map high-level operations directly to TPU instructions, bypassing traditional CPU-oriented optimizations."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 174,
      "context": "[^fn-tpu]: **TPU (Tensor Processing Unit)**: Google's custom ASIC achieved 15-30x better performance-per-watt...",
      "full_line": "[^fn-tpu]: **TPU (Tensor Processing Unit)**: Google's custom ASIC achieved 15-30x better performance-per-watt than contemporary GPUs and CPUs for neural networks, fundamentally proving that domain-specific architectures could outperform general-purpose processors for ML workloads."
    },
    {
      "footnote_id": "fn-auto-diff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 584,
      "context": "...g this to a neural network with millions of operations. This is where automatic differentiation (AD)[^fn-auto-diff] becomes essential.",
      "full_line": "Even in this basic example, computing derivatives manually would require careful application of calculus rules - the product rule, the chain rule, and derivatives of trigonometric functions. Now imagine scaling this to a neural network with millions of operations. This is where automatic differentiation (AD)[^fn-auto-diff] becomes essential."
    },
    {
      "footnote_id": "fn-auto-diff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 586,
      "context": "[^fn-auto-diff]: **Automatic Differentiation**: Invented by Robert Edwin Wengert in 1964, this technique achieves m...",
      "full_line": "[^fn-auto-diff]: **Automatic Differentiation**: Invented by Robert Edwin Wengert in 1964, this technique achieves machine precision derivatives by applying the chain rule at the elementary operation level, making neural network training computationally feasible for networks with millions of parameters."
    },
    {
      "footnote_id": "fn-von-neumann",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 51,
      "context": "...complexity, hardware acceleration has evolved to keep pace. The shift from von Neumann architectures[^fn-von-neumann] to specialized accelerators reflects a broader trend in computing: reducing the cost of data moveme...",
      "full_line": "As ML models have grown in size and complexity, hardware acceleration has evolved to keep pace. The shift from von Neumann architectures[^fn-von-neumann] to specialized accelerators reflects a broader trend in computing: reducing the cost of data movement, increasing parallelism, and tailoring hardware to domain-specific workloads. Moving data across memory hierarchies often consumes more energy than computation itself, making efficient memory organization and computation placement critical to overall system performance."
    },
    {
      "footnote_id": "fn-von-neumann",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 53,
      "context": "[^fn-von-neumann]: **Von Neumann Architecture**: Proposed by John von Neumann in 1945, this architecture separates pr...",
      "full_line": "[^fn-von-neumann]: **Von Neumann Architecture**: Proposed by John von Neumann in 1945, this architecture separates program storage from data storage but forces all data to flow through a single bus between CPU and memory. In AI workloads, this \"von Neumann bottleneck\" becomes critical\u2014moving a 1GB model from memory consumes 100-1000x more energy than the actual computation, driving the need for specialized architectures that bring computation closer to data."
    },
    {
      "footnote_id": "fn-memory-hierarchy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 55,
      "context": "...key operations like matrix multiplications and activation functions, the role of memory hierarchies[^fn-memory-hierarchy] in data movement, and techniques for mapping neural networks to hardware. The discussion extends to...",
      "full_line": "This chapter explores AI acceleration from a systems perspective, examining how computational models, hardware optimizations, and software frameworks interact to enable efficient execution. It covers key operations like matrix multiplications and activation functions, the role of memory hierarchies[^fn-memory-hierarchy] in data movement, and techniques for mapping neural networks to hardware. The discussion extends to compilers, scheduling strategies, and runtime optimizations, highlighting their impact on performance. Finally, it addresses the challenges of scaling AI systems from single-chip accelerators to multi-chip and distributed architectures, integrating real-world examples to illustrate effective AI acceleration."
    },
    {
      "footnote_id": "fn-memory-hierarchy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 57,
      "context": "[^fn-memory-hierarchy]: **Memory Hierarchy Challenge**: Traditional memory hierarchy (L1: ~1ns, L2: ~10ns, RAM: ~100ns, SS...",
      "full_line": "[^fn-memory-hierarchy]: **Memory Hierarchy Challenge**: Traditional memory hierarchy (L1: ~1ns, L2: ~10ns, RAM: ~100ns, SSD: ~100\u03bcs) breaks down for AI workloads. A single transformer layer might need 1GB+ of weights, but L1 cache is only 32KB. Google's TPU addresses this with 128MB of on-chip memory running at 900 GB/s bandwidth\u2014600x faster than typical RAM\u2014because keeping weights on-chip dramatically reduces both latency and energy consumption."
    },
    {
      "footnote_id": "fn-tpu-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 63,
      "context": "...tial context for understanding how modern ML accelerators like GPUs with tensor cores, Google's TPUs[^fn-tpu-origin], and Apple's Neural Engine came to be. These technologies now power widely deployed applications su...",
      "full_line": "This evolution is not just of academic interest\u2014it provides essential context for understanding how modern ML accelerators like GPUs with tensor cores, Google's TPUs[^fn-tpu-origin], and Apple's Neural Engine came to be. These technologies now power widely deployed applications such as real-time language translation, image recognition, and personalized recommendations. The architectural strategies enabling such capabilities are deeply rooted in decades of hardware specialization."
    },
    {
      "footnote_id": "fn-tpu-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 65,
      "context": "[^fn-tpu-origin]: **TPU Origins**: Google secretly developed the Tensor Processing Unit (TPU) starting in 2013 when...",
      "full_line": "[^fn-tpu-origin]: **TPU Origins**: Google secretly developed the Tensor Processing Unit (TPU) starting in 2013 when they realized CPUs couldn't handle the computational demands of their neural networks. The TPUv1, deployed in 2015, delivered 15-30x better performance per watt than contemporary GPUs for inference. By 2017, Google's TPUs were processing over 100 petaops per second across their datacenters, fundamentally changing how the industry approached AI hardware."
    },
    {
      "footnote_id": "fn-intel-8087",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 75,
      "context": "One of the earliest examples of hardware specialization was the Intel 8087 mathematics coprocessor[^fn-intel-8087], introduced in 1980. This floating-point unit (FPU) was designed to offload arithmetic-intensive co...",
      "full_line": "One of the earliest examples of hardware specialization was the Intel 8087 mathematics coprocessor[^fn-intel-8087], introduced in 1980. This floating-point unit (FPU) was designed to offload arithmetic-intensive computations from the main CPU, dramatically improving performance for scientific and engineering applications. The 8087 demonstrated unprecedented efficiency, achieving performance gains of up to 100\u00d7 for floating-point operations compared to software-based implementations on general-purpose processors [@fisher_8087_1981]. This milestone established a fundamental principle in computer architecture: carefully designed hardware specialization could provide order-of-magnitude improvements for well-defined, computationally intensive tasks."
    },
    {
      "footnote_id": "fn-intel-8087",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 77,
      "context": "[^fn-intel-8087]: **Intel 8087 Impact**: The 8087 coprocessor cost $750 (about $2,800 today) but transformed scienti...",
      "full_line": "[^fn-intel-8087]: **Intel 8087 Impact**: The 8087 coprocessor cost $750 (about $2,800 today) but transformed scientific computing\u2014CAD workstations that took hours for complex calculations could complete them in minutes. This success created the entire coprocessor market and established the economic model for specialized hardware that persists today: charge premium prices for dramatic performance improvements in specific domains."
    },
    {
      "footnote_id": "fn-alexnet-gpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 171,
      "context": "...ics pipelines directly enabled its later adoption for training deep neural networks, such as AlexNet[^fn-alexnet-gpu] in 2012, which famously ran on consumer-grade NVIDIA GPUs. DSP innovations in low-power signal proc...",
      "full_line": "This period of expanding specialization demonstrated that hardware acceleration strategies could successfully address diverse computational requirements. The GPU's success in parallelizing 3D graphics pipelines directly enabled its later adoption for training deep neural networks, such as AlexNet[^fn-alexnet-gpu] in 2012, which famously ran on consumer-grade NVIDIA GPUs. DSP innovations in low-power signal processing helped pave the way for real-time inference on edge devices, such as voice assistants and wearables. These domains not only informed ML hardware designs but also proved that accelerators could be deployed across both cloud and embedded contexts\u2014a lesson that continues to shape today's AI ecosystem."
    },
    {
      "footnote_id": "fn-alexnet-gpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 173,
      "context": "[^fn-alexnet-gpu]: **AlexNet's GPU Revolution**: AlexNet's breakthrough wasn't just algorithmic\u2014it proved GPUs could...",
      "full_line": "[^fn-alexnet-gpu]: **AlexNet's GPU Revolution**: AlexNet's breakthrough wasn't just algorithmic\u2014it proved GPUs could train deep networks 10x faster than CPUs. The team split the 8-layer network across two NVIDIA GTX 580s (512 cores each), reducing training time from weeks to days. This success triggered the \"deep learning gold rush\" and established NVIDIA as the default AI hardware company, with GPU sales for data centers growing from $200 million to $47 billion by 2024."
    },
    {
      "footnote_id": "fn-risc-v-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 332,
      "context": "...by operating on multiple data elements simultaneously. As shown in @lst-riscv_vector_mac, the RISC-V[^fn-risc-v-ai] assembly code demonstrates modern vector processing.",
      "full_line": "Vector processing units transform this execution pattern by operating on multiple data elements simultaneously. As shown in @lst-riscv_vector_mac, the RISC-V[^fn-risc-v-ai] assembly code demonstrates modern vector processing."
    },
    {
      "footnote_id": "fn-risc-v-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 334,
      "context": "[^fn-risc-v-ai]: **RISC-V for AI**: RISC-V, the open-source instruction set architecture from UC Berkeley (2010), i...",
      "full_line": "[^fn-risc-v-ai]: **RISC-V for AI**: RISC-V, the open-source instruction set architecture from UC Berkeley (2010), is becoming crucial for AI accelerators because it's freely customizable. Companies like SiFive and Google have created RISC-V chips with custom AI extensions. Unlike proprietary architectures, RISC-V allows hardware designers to add specialized ML instructions without licensing fees, potentially democratizing AI hardware development beyond the current duopoly of x86 and ARM."
    },
    {
      "footnote_id": "fn-cray-vector",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 376,
      "context": "...where large arrays of data required efficient parallel processing. Early systems such as the Cray-1[^fn-cray-vector], one of the first commercially successful supercomputers, introduced dedicated vector units to perf...",
      "full_line": "The principles underlying vector operations have long played a central role in high-performance computing. In the 1970s and 1980s, vector processors emerged as a critical architectural solution for scientific computing, weather modeling, and physics simulations, where large arrays of data required efficient parallel processing. Early systems such as the Cray-1[^fn-cray-vector], one of the first commercially successful supercomputers, introduced dedicated vector units to perform arithmetic operations on entire data vectors in a single instruction. This approach dramatically improved computational throughput compared to traditional scalar execution [@jordan1982guide]."
    },
    {
      "footnote_id": "fn-cray-vector",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 378,
      "context": "[^fn-cray-vector]: **Cray-1 Vector Legacy**: The Cray-1 (1975) cost $8.8 million ($50 million today) but could perfor...",
      "full_line": "[^fn-cray-vector]: **Cray-1 Vector Legacy**: The Cray-1 (1975) cost $8.8 million ($50 million today) but could perform 160 million floating-point operations per second\u20141000x faster than typical computers. Its 64-element vector registers and pipelined vector units established the architectural template that modern AI accelerators still follow: process many data elements simultaneously with specialized hardware pipelines."
    },
    {
      "footnote_id": "fn-simd-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 609,
      "context": "Single Instruction Multiple Data (SIMD)[^fn-simd-evolution] execution applies identical operations to multiple data elements in parallel, minimizing instructio...",
      "full_line": "Single Instruction Multiple Data (SIMD)[^fn-simd-evolution] execution applies identical operations to multiple data elements in parallel, minimizing instruction overhead while maximizing data throughput. This execution model is widely used to accelerate workloads with regular, independent data parallelism, such as neural network computations. The ARM Scalable Vector Extension (SVE) provides a representative example of how modern architectures implement SIMD operations efficiently, as illustrated in @lst-arm_sve_vector."
    },
    {
      "footnote_id": "fn-simd-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 611,
      "context": "[^fn-simd-evolution]: **SIMD Evolution**: SIMD originated in Flynn's 1966 taxonomy for scientific computing, but neural...",
      "full_line": "[^fn-simd-evolution]: **SIMD Evolution**: SIMD originated in Flynn's 1966 taxonomy for scientific computing, but neural networks transformed it from a niche HPC concept to mainstream necessity. Modern CPUs have 512-bit SIMD units (AVX-512), but AI pushed development of SIMT (Single Instruction, Multiple Thread) where thousands of lightweight threads execute in parallel\u2014GPU architectures now coordinate 65,536+ threads simultaneously, impossible with traditional SIMD."
    },
    {
      "footnote_id": "fn-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 651,
      "context": "Tensor cores[^fn-tensor-cores], implemented in architectures such as NVIDIA's Ampere GPUs, provide an example of this approach. Th...",
      "full_line": "Tensor cores[^fn-tensor-cores], implemented in architectures such as NVIDIA's Ampere GPUs, provide an example of this approach. They expose matrix computation capabilities through specialized instructions, such as the tensor core operation shown in @lst-tensor_core_op on the NVIDIA A100 GPU."
    },
    {
      "footnote_id": "fn-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 653,
      "context": "[^fn-tensor-cores]: **Tensor Core Breakthrough**: NVIDIA introduced tensor cores in the V100 (2017) to accelerate the...",
      "full_line": "[^fn-tensor-cores]: **Tensor Core Breakthrough**: NVIDIA introduced tensor cores in the V100 (2017) to accelerate the 4x4 matrix operations common in neural networks. The A100's third-generation tensor cores achieve 312 TFLOPS for FP16 matrix math\u201420x faster than traditional CUDA cores. This single innovation enabled training of models like GPT-3 that would have been impossible with conventional hardware, fundamentally changing the scale of AI research."
    },
    {
      "footnote_id": "fn-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 669,
      "context": "...nits arranged in systolic arrays to maximize sustained training throughput. Apple's M1 neural engine[^fn-neural-engine] integrates smaller matrix processors optimized for mobile inference workloads, while Intel's Sapphi...",
      "full_line": "Tensor processing unit architectures differ based on design priorities. NVIDIA's Ampere architecture incorporates tensor cores optimized for general-purpose deep learning acceleration. Google's TPUv4 utilizes large-scale matrix units arranged in systolic arrays to maximize sustained training throughput. Apple's M1 neural engine[^fn-neural-engine] integrates smaller matrix processors optimized for mobile inference workloads, while Intel's Sapphire Rapids architecture introduces AMX tiles designed for high-performance datacenter applications."
    },
    {
      "footnote_id": "fn-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 671,
      "context": "[^fn-neural-engine]: **Apple's Neural Engine Strategy**: Apple introduced the Neural Engine in 2017's A11 chip to enabl...",
      "full_line": "[^fn-neural-engine]: **Apple's Neural Engine Strategy**: Apple introduced the Neural Engine in 2017's A11 chip to enable on-device ML without draining battery life. The M1's 16-core Neural Engine delivers 11.8 TOPS while consuming just 20 watts\u2014enabling real-time features like live text recognition and voice processing without cloud connectivity. This \"privacy through hardware\" approach influenced the entire industry to prioritize edge AI capabilities."
    },
    {
      "footnote_id": "fn-systolic-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 691,
      "context": "The concept of systolic arrays was first introduced by Kung and Leiserson[^fn-systolic-origin], who formalized their use in parallel computing architectures for efficient matrix operations [@Kun...",
      "full_line": "The concept of systolic arrays was first introduced by Kung and Leiserson[^fn-systolic-origin], who formalized their use in parallel computing architectures for efficient matrix operations [@Kung1982]. Unlike general-purpose execution units, systolic arrays exploit spatial and temporal locality by reusing operands as they propagate through the grid. Google's Tensor Processing Unit (TPU) exemplifies this architectural approach. In the TPUv4, a $128\\times128$ systolic array of multiply-accumulate units processes matrix operations by streaming data through the array in a pipelined manner, as shown in @fig-systolic-array."
    },
    {
      "footnote_id": "fn-systolic-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 693,
      "context": "[^fn-systolic-origin]: **Systolic Array Renaissance**: H.T. Kung and Charles Leiserson introduced systolic arrays at CMU...",
      "full_line": "[^fn-systolic-origin]: **Systolic Array Renaissance**: H.T. Kung and Charles Leiserson introduced systolic arrays at CMU in 1979 for VLSI signal processing, but the concept languished for decades due to programming complexity. Google's 2016 TPU resurrection proved these \"heartbeat\" architectures could deliver massive efficiency gains for neural networks\u2014the TPUv1's 256x256 systolic array achieved 92 TOPS while consuming just 40 watts, making systolic arrays the dominant AI architecture today."
    },
    {
      "footnote_id": "fn-paradigm-shift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 54,
      "context": "...approach to artificial intelligence resulted from extensive research and fundamental paradigm shifts[^fn-paradigm-shift] in the field. The progression of artificial intelligence encompasses both theoretical advances in u...",
      "full_line": "The emergence of machine learning as a viable scientific discipline approach to artificial intelligence resulted from extensive research and fundamental paradigm shifts[^fn-paradigm-shift] in the field. The progression of artificial intelligence encompasses both theoretical advances in understanding intelligence and practical developments in implementation methodologies. This development mirrors the evolution of other scientific and engineering disciplines\u2014from mechanical engineering's advancement from basic force principles to contemporary robotics, to electrical engineering's progression from fundamental electromagnetic theory to modern power and communication networks. Analysis of this historical trajectory reveals both the technological innovations leading to current machine learning approaches and the emergence of advanced learning approaches that inform contemporary AI system development."
    },
    {
      "footnote_id": "fn-paradigm-shift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 56,
      "context": "[^fn-paradigm-shift]: **Paradigm Shift**: A term coined by philosopher Thomas Kuhn in 1962 to describe fundamental chang...",
      "full_line": "[^fn-paradigm-shift]: **Paradigm Shift**: A term coined by philosopher Thomas Kuhn in 1962 to describe fundamental changes in scientific approach\u2014like the shift from Newtonian to Einstein's physics. In AI, key paradigm shifts include moving from symbolic reasoning to statistical learning (1990s), and from shallow to deep learning (2010s). Each shift required researchers to abandon established methods and embrace radically different approaches to understanding intelligence."
    },
    {
      "footnote_id": "fn-early",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 60,
      "context": "...eline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt, an early computational learning algorithm. Imagine walking into a comp...",
      "full_line": "The evolution of AI, depicted in the timeline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt, an early computational learning algorithm. Imagine walking into a computer lab in 1965. You'd find room-sized mainframes running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, while groundbreaking for their time, were a far cry from today's machine learning systems that can detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garry Kasparov in 1997. More recent advancements include the introduction of OpenAI's GPT-3 in 2020 and GPT-4 in 2023, demonstrating the dramatic evolution and increasing complexity of AI systems over the decades."
    },
    {
      "footnote_id": "fn-eliza",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 60,
      "context": "...or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garr...",
      "full_line": "The evolution of AI, depicted in the timeline shown in @fig-ai-timeline, highlights key milestones such as the development of the perceptron[^fn-early] in 1957 by Frank Rosenblatt, an early computational learning algorithm. Imagine walking into a computer lab in 1965. You'd find room-sized mainframes running programs that could prove basic mathematical theorems or play simple games like tic-tac-toe. These early artificial intelligence systems, while groundbreaking for their time, were a far cry from today's machine learning systems that can detect cancer in medical images or understand human speech. The timeline shows the progression from early innovations like the ELIZA[^fn-eliza] chatbot in 1966, to significant breakthroughs such as IBM's Deep Blue defeating chess champion Garry Kasparov in 1997. More recent advancements include the introduction of OpenAI's GPT-3 in 2020 and GPT-4 in 2023, demonstrating the dramatic evolution and increasing complexity of AI systems over the decades."
    },
    {
      "footnote_id": "fn-eliza",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 62,
      "context": "[^fn-eliza]: **ELIZA**: Created by MIT's Joseph Weizenbaum in 1966, ELIZA was one of the first chatbots that co...",
      "full_line": "[^fn-eliza]: **ELIZA**: Created by MIT's Joseph Weizenbaum in 1966, ELIZA was one of the first chatbots that could simulate human conversation by pattern matching and substitution\u2014ironically, Weizenbaum was horrified when people began forming emotional attachments to his simple program, leading him to become a critic of AI."
    },
    {
      "footnote_id": "fn-early",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 243,
      "context": "[^fn-early]: **Perceptron**: One of the first computational learning algorithms\u2014a system that could learn to cl...",
      "full_line": "[^fn-early]: **Perceptron**: One of the first computational learning algorithms\u2014a system that could learn to classify patterns by making yes/no decisions based on inputs."
    },
    {
      "footnote_id": "fn-dartmouth-conference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 249,
      "context": "The story of machine learning begins at the historic Dartmouth Conference[^fn-dartmouth-conference] in 1956, where pioneers like John McCarthy, Marvin Minsky, and Claude Shannon first coined the term...",
      "full_line": "The story of machine learning begins at the historic Dartmouth Conference[^fn-dartmouth-conference] in 1956, where pioneers like John McCarthy, Marvin Minsky, and Claude Shannon first coined the term \"artificial intelligence.\" Their approach was based on a compelling idea: intelligence could be reduced to symbol manipulation. Consider Daniel Bobrow's STUDENT system from 1964, one of the first AI programs that could solve algebra word problems. It was one of the first AI programs to demonstrate natural language understanding by converting English text into algebraic equations, marking an important milestone in symbolic AI."
    },
    {
      "footnote_id": "fn-dartmouth-conference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 251,
      "context": "[^fn-dartmouth-conference]: **Dartmouth Conference (1956)**: The legendary 8-week workshop at Dartmouth College where AI was o...",
      "full_line": "[^fn-dartmouth-conference]: **Dartmouth Conference (1956)**: The legendary 8-week workshop at Dartmouth College where AI was officially born. Organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, it was the first time researchers gathered specifically to discuss \"artificial intelligence\"\u2014a term McCarthy coined for the proposal. The ambitious goal was to make machines \"simulate every aspect of learning or any other feature of intelligence.\" Though overly optimistic, this gathering launched AI as a formal research field."
    },
    {
      "footnote_id": "fn-brittleness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 269,
      "context": "...der, using synonyms, or natural speech patterns, would cause the STUDENT to fail. This \"brittleness\"[^fn-brittleness] meant that while these solutions could appear intelligent when handling very specific cases they we...",
      "full_line": "Early AI like STUDENT suffered from a fundamental limitation: they could only handle inputs that exactly matched their pre-programmed patterns and rules. Imagine a language translator that only works when sentences follow perfect grammatical structure; even slight variations, such as changing word order, using synonyms, or natural speech patterns, would cause the STUDENT to fail. This \"brittleness\"[^fn-brittleness] meant that while these solutions could appear intelligent when handling very specific cases they were designed for, they would break down completely when faced with even minor variations or real-world complexity. This limitation wasn't just a technical inconvenience\u2014it revealed a deeper problem with rule-based approaches to AI: they couldn't genuinely understand or generalize from their programming, they could only match and manipulate text patterns exactly as specified in their programming."
    },
    {
      "footnote_id": "fn-brittleness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 271,
      "context": "[^fn-brittleness]: **Brittleness in AI Systems**: The tendency of rule-based systems to fail completely when encounte...",
      "full_line": "[^fn-brittleness]: **Brittleness in AI Systems**: The tendency of rule-based systems to fail completely when encountering inputs that fall outside their programmed scenarios, no matter how similar those inputs might be to what they were designed to handle. This contrasts with human intelligence, which can adapt and make reasonable guesses even in unfamiliar situations. The brittleness problem drove researchers toward machine learning approaches that could generalize from examples rather than relying on exhaustive rule sets."
    },
    {
      "footnote_id": "fn-mooreslaw",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 309,
      "context": "...evolution meant massive amounts of data were suddenly available to train the algorithms. Moore's Law[^fn-mooreslaw] delivered the computational power needed to process this data effectively. And researchers develope...",
      "full_line": "The 1990s marked a radical transformation in artificial intelligence as the field moved away from hand-coded rules toward statistical learning approaches. This wasn't a simple choice\u2014it was driven by three converging factors that made statistical methods both possible and powerful. The digital revolution meant massive amounts of data were suddenly available to train the algorithms. Moore's Law[^fn-mooreslaw] delivered the computational power needed to process this data effectively. And researchers developed new algorithms like Support Vector Machines and improved neural networks that could actually learn patterns from this data rather than following pre-programmed rules. This combination fundamentally changed how we built AI: instead of trying to encode human knowledge directly, we could now let machines discover patterns automatically from examples, leading to more robust and adaptable AI."
    },
    {
      "footnote_id": "fn-mooreslaw",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 311,
      "context": "[^fn-mooreslaw]: **Moore's Law**: The observation made by Intel co-founder Gordon Moore in 1965 that the number of...",
      "full_line": "[^fn-mooreslaw]: **Moore's Law**: The observation made by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years, while the cost halves. This exponential growth in computing power has been a key driver of advances in machine learning, though the pace has begun to slow in recent years."
    },
    {
      "footnote_id": "fn-viola-jones",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 375,
      "context": "Take the example of face detection, where the Viola-Jones algorithm[^fn-viola-jones] (2001) achieved real-time performance using simple rectangular features and a cascade of classifier...",
      "full_line": "Take the example of face detection, where the Viola-Jones algorithm[^fn-viola-jones] (2001) achieved real-time performance using simple rectangular features and a cascade of classifiers[^fn-cascade]. This algorithm powered digital camera face detection for nearly a decade."
    },
    {
      "footnote_id": "fn-cascade",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 375,
      "context": "...(2001) achieved real-time performance using simple rectangular features and a cascade of classifiers[^fn-cascade]. This algorithm powered digital camera face detection for nearly a decade.",
      "full_line": "Take the example of face detection, where the Viola-Jones algorithm[^fn-viola-jones] (2001) achieved real-time performance using simple rectangular features and a cascade of classifiers[^fn-cascade]. This algorithm powered digital camera face detection for nearly a decade."
    },
    {
      "footnote_id": "fn-viola-jones",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 377,
      "context": "[^fn-viola-jones]: **Viola-Jones Algorithm**: A groundbreaking computer vision algorithm that could detect faces in r...",
      "full_line": "[^fn-viola-jones]: **Viola-Jones Algorithm**: A groundbreaking computer vision algorithm that could detect faces in real-time by using simple rectangular patterns (like comparing the brightness of eye regions versus cheek regions) and making decisions in stages, filtering out non-faces quickly and spending more computation only on promising candidates."
    },
    {
      "footnote_id": "fn-cascade",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 379,
      "context": "[^fn-cascade]: **Cascade of Classifiers**: A multi-stage decision system where each stage acts as a filter, quick...",
      "full_line": "[^fn-cascade]: **Cascade of Classifiers**: A multi-stage decision system where each stage acts as a filter, quickly rejecting obvious non-matches and passing promising candidates to the next, more sophisticated stage\u2014similar to how security screening works at airports with multiple checkpoints of increasing thoroughness."
    },
    {
      "footnote_id": "fn-neurons",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 383,
      "context": "...inspired by the human brain's architecture. Deep learning is built from layers of artificial neurons[^fn-neurons], where each layer learns to transform its input data into increasingly abstract representations. Im...",
      "full_line": "While Support Vector Machines excelled at finding complex boundaries between categories using mathematical transformations, deep learning took a radically different approach inspired by the human brain's architecture. Deep learning is built from layers of artificial neurons[^fn-neurons], where each layer learns to transform its input data into increasingly abstract representations. Imagine processing an image of a cat: the first layer might learn to detect simple edges and contrasts, the next layer combines these into basic shapes and textures, another layer might recognize whiskers and pointy ears, and the final layers assemble these features into the concept of \"cat.\""
    },
    {
      "footnote_id": "fn-neurons",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 387,
      "context": "[^fn-neurons]: **Artificial Neurons**: Basic computational units in neural networks that mimic biological neurons...",
      "full_line": "[^fn-neurons]: **Artificial Neurons**: Basic computational units in neural networks that mimic biological neurons, taking multiple inputs, applying weights and biases, and producing an output signal through an activation function[^fn-activation]."
    },
    {
      "footnote_id": "fn-activation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 387,
      "context": "...e inputs, applying weights and biases, and producing an output signal through an activation function[^fn-activation].",
      "full_line": "[^fn-neurons]: **Artificial Neurons**: Basic computational units in neural networks that mimic biological neurons, taking multiple inputs, applying weights and biases, and producing an output signal through an activation function[^fn-activation]."
    },
    {
      "footnote_id": "fn-activation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 389,
      "context": "[^fn-activation]: **Activation Function**: A mathematical function that determines whether a neuron should be \"activ...",
      "full_line": "[^fn-activation]: **Activation Function**: A mathematical function that determines whether a neuron should be \"activated\" (send a signal) based on its inputs, similar to how biological neurons fire only when they receive sufficient stimulation from other neurons."
    },
    {
      "footnote_id": "fn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 391,
      "context": "...a deep neural network called AlexNet, shown in @fig-alexnet, achieved a breakthrough in the ImageNet[^fn-imagenet] competition that would transform the field of machine learning. The challenge was formidable: corre...",
      "full_line": "In 2012, a deep neural network called AlexNet, shown in @fig-alexnet, achieved a breakthrough in the ImageNet[^fn-imagenet] competition that would transform the field of machine learning. The challenge was formidable: correctly classify 1.2 million high-resolution images into 1,000 different categories. While previous approaches struggled with error rates above 25%, AlexNet[^fn-alexnet] achieved a 15.3% top-5 error rate, dramatically outperforming all existing methods."
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 391,
      "context": "...1,000 different categories. While previous approaches struggled with error rates above 25%, AlexNet[^fn-alexnet] achieved a 15.3% top-5 error rate, dramatically outperforming all existing methods.",
      "full_line": "In 2012, a deep neural network called AlexNet, shown in @fig-alexnet, achieved a breakthrough in the ImageNet[^fn-imagenet] competition that would transform the field of machine learning. The challenge was formidable: correctly classify 1.2 million high-resolution images into 1,000 different categories. While previous approaches struggled with error rates above 25%, AlexNet[^fn-alexnet] achieved a 15.3% top-5 error rate, dramatically outperforming all existing methods."
    },
    {
      "footnote_id": "fn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 393,
      "context": "[^fn-imagenet]: **ImageNet**: A massive visual database containing over 14 million labeled images across 20,000+ c...",
      "full_line": "[^fn-imagenet]: **ImageNet**: A massive visual database containing over 14 million labeled images across 20,000+ categories, created by Stanford's Fei-Fei Li starting in 2009. The annual ImageNet challenge became the Olympics of computer vision, driving breakthrough after breakthrough in image recognition until neural networks became so good they essentially solved the competition."
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 395,
      "context": "[^fn-alexnet]: **AlexNet**: A breakthrough deep neural network from 2012 that won the ImageNet competition by a l...",
      "full_line": "[^fn-alexnet]: **AlexNet**: A breakthrough deep neural network from 2012 that won the ImageNet competition by a large margin and helped spark the deep learning revolution. Named after Alex Krizhevsky, it proved that neural networks could outperform traditional computer vision methods when given enough data and computing power."
    },
    {
      "footnote_id": "fn-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 821,
      "context": "...works thousands of times larger than AlexNet. These massive models, often called \"foundation models\"[^fn-foundation-models], took deep learning to new heights.",
      "full_line": "From this foundation, deep learning entered an era of unprecedented scale. By the late 2010s, companies like Google, Facebook, and OpenAI were training neural networks thousands of times larger than AlexNet. These massive models, often called \"foundation models\"[^fn-foundation-models], took deep learning to new heights."
    },
    {
      "footnote_id": "fn-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 823,
      "context": "[^fn-foundation-models]: **Foundation Models**: Large-scale AI models trained on broad datasets that serve as the \"foundati...",
      "full_line": "[^fn-foundation-models]: **Foundation Models**: Large-scale AI models trained on broad datasets that serve as the \"foundation\" for many different applications through fine-tuning\u2014like GPT for language tasks or CLIP for vision tasks. The term was coined by Stanford's AI researchers in 2021 to capture how these models became the basis for building more specific AI systems."
    },
    {
      "footnote_id": "fn-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 825,
      "context": "GPT-3, released in 2020, contained 175 billion parameters\u2014nearly 3,000 times larger than AlexNet[^fn-parameters]---imagine a student that could read through all of Wikipedia multiple times and learn patterns from...",
      "full_line": "GPT-3, released in 2020, contained 175 billion parameters\u2014nearly 3,000 times larger than AlexNet[^fn-parameters]---imagine a student that could read through all of Wikipedia multiple times and learn patterns from every article. These models showed remarkable abilities: writing human-like text, engaging in conversation, generating images from descriptions, and even writing computer code. The key insight was simple but powerful: as we made neural networks bigger and fed them more data, they became capable of solving increasingly complex tasks. However, this scale brought unprecedented systems challenges[^fn-training-challenges]: how do you efficiently train models that require thousands of GPUs working in parallel? How do you store and serve models that are hundreds of gigabytes in size? How do you handle the massive datasets needed for training?"
    },
    {
      "footnote_id": "fn-training-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 825,
      "context": "...of solving increasingly complex tasks. However, this scale brought unprecedented systems challenges[^fn-training-challenges]: how do you efficiently train models that require thousands of GPUs working in parallel? How do you...",
      "full_line": "GPT-3, released in 2020, contained 175 billion parameters\u2014nearly 3,000 times larger than AlexNet[^fn-parameters]---imagine a student that could read through all of Wikipedia multiple times and learn patterns from every article. These models showed remarkable abilities: writing human-like text, engaging in conversation, generating images from descriptions, and even writing computer code. The key insight was simple but powerful: as we made neural networks bigger and fed them more data, they became capable of solving increasingly complex tasks. However, this scale brought unprecedented systems challenges[^fn-training-challenges]: how do you efficiently train models that require thousands of GPUs working in parallel? How do you store and serve models that are hundreds of gigabytes in size? How do you handle the massive datasets needed for training?"
    },
    {
      "footnote_id": "fn-training-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 827,
      "context": "[^fn-training-challenges]: **Large-Scale Training Challenges**: Training foundation models requires solving complex systems e...",
      "full_line": "[^fn-training-challenges]: **Large-Scale Training Challenges**: Training foundation models requires solving complex systems engineering problems including distributed computing coordination, memory management across thousands of machines, fault tolerance for multi-week training runs, and efficient data pipeline design. These challenges span hardware optimization, software engineering, and algorithmic innovation, requiring deep expertise in parallel computing and distributed systems."
    },
    {
      "footnote_id": "fn-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 829,
      "context": "[^fn-parameters]: **Parameters**: The adjustable values within a neural network that are modified during training, s...",
      "full_line": "[^fn-parameters]: **Parameters**: The adjustable values within a neural network that are modified during training, similar to how the brain's neural connections grow stronger as you learn a new skill. Having more parameters generally means that the model can learn more complex patterns."
    },
    {
      "footnote_id": "fn-backprop-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 831,
      "context": "...80s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation[^fn-backprop-history] in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated it...",
      "full_line": "The deep learning revolution of 2012 didn't emerge from nowhere, as it was founded on neural network research dating back to the 1950s. The story begins with Frank Rosenblatt's Perceptron in 1957, which captured the imagination of researchers by showing how a simple artificial neuron could learn to classify patterns. While it could only handle linearly separable problems, a limitation that was dramatically highlighted by Minsky and Papert's 1969 book, \"Perceptrons,\" it introduced the fundamental concept of trainable neural networks. The 1980s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation[^fn-backprop-history] in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated its practical application in recognizing handwritten digits using specialized neural networks designed for image processing[^fn-cnn]."
    },
    {
      "footnote_id": "fn-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 831,
      "context": "...on in recognizing handwritten digits using specialized neural networks designed for image processing[^fn-cnn].",
      "full_line": "The deep learning revolution of 2012 didn't emerge from nowhere, as it was founded on neural network research dating back to the 1950s. The story begins with Frank Rosenblatt's Perceptron in 1957, which captured the imagination of researchers by showing how a simple artificial neuron could learn to classify patterns. While it could only handle linearly separable problems, a limitation that was dramatically highlighted by Minsky and Papert's 1969 book, \"Perceptrons,\" it introduced the fundamental concept of trainable neural networks. The 1980s brought more important breakthroughs: Rumelhart, Hinton, and Williams introduced backpropagation[^fn-backprop-history] in 1986, providing a systematic way to train multi-layer networks, while Yann LeCun demonstrated its practical application in recognizing handwritten digits using specialized neural networks designed for image processing[^fn-cnn]."
    },
    {
      "footnote_id": "fn-backprop-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 833,
      "context": "[^fn-backprop-history]: **Backpropagation (Historical Context)**: A mathematical technique that allows neural networks to...",
      "full_line": "[^fn-backprop-history]: **Backpropagation (Historical Context)**: A mathematical technique that allows neural networks to learn by calculating how much each component contributed to errors and adjusting accordingly\u2014like a coach analyzing a team's mistakes and giving each player specific feedback to improve their performance."
    },
    {
      "footnote_id": "fn-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 835,
      "context": "[^fn-cnn]: **Convolutional Neural Network (CNN)**: A type of neural network specially designed for processing...",
      "full_line": "[^fn-cnn]: **Convolutional Neural Network (CNN)**: A type of neural network specially designed for processing images, inspired by how the human visual system works. The \"convolutional\" part refers to how it scans images in small chunks, similar to how our eyes focus on different parts of a scene."
    },
    {
      "footnote_id": "fn-computer-engineering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 849,
      "context": "...that period, as computing systems grew more complex, a new discipline emerged: Computer Engineering[^fn-computer-engineering]. This field bridged the gap between Electrical Engineering's hardware expertise and Computer Scienc...",
      "full_line": "This shift mirrors the evolution of computer science and engineering in the late 1960s and early 1970s. During that period, as computing systems grew more complex, a new discipline emerged: Computer Engineering[^fn-computer-engineering]. This field bridged the gap between Electrical Engineering's hardware expertise and Computer Science's focus on algorithms and software. Computer Engineering arose because the challenges of designing and building complex computing systems required an integrated approach that neither discipline could fully address on its own."
    },
    {
      "footnote_id": "fn-computer-engineering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 851,
      "context": "[^fn-computer-engineering]: **Computer Engineering**: This discipline emerged in the late 1960s when IBM System/360 and other...",
      "full_line": "[^fn-computer-engineering]: **Computer Engineering**: This discipline emerged in the late 1960s when IBM System/360 and other complex computing systems required expertise that spanned both hardware and software. Before Computer Engineering, electrical engineers focused on circuits while computer scientists worked on algorithms, but no one specialized in the integration challenges. Today's Computer Engineering programs, established at schools like Case Western Reserve and Stanford in the 1970s, combine hardware design, software systems, and computer architecture\u2014laying the groundwork for what ML Systems Engineering is becoming today."
    },
    {
      "footnote_id": "fn-mas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1124,
      "context": "...take actions, learn from outcomes, and adapt their behavior accordingly through multi-agent systems[^fn-mas] and advanced planning algorithms. These autonomous agents can plan, reason, and execute complex tas...",
      "full_line": "The rise of agentic systems marks a profound shift from traditional reactive ML systems that simply made predictions based on input data. Modern applications can now take actions, learn from outcomes, and adapt their behavior accordingly through multi-agent systems[^fn-mas] and advanced planning algorithms. These autonomous agents can plan, reason, and execute complex tasks, introducing new requirements for decision-making frameworks and safety constraints."
    },
    {
      "footnote_id": "fn-mas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1126,
      "context": "[^fn-mas]: **Multi-Agent System**: A computational system where multiple intelligent agents interact within a...",
      "full_line": "[^fn-mas]: **Multi-Agent System**: A computational system where multiple intelligent agents interact within an environment, each pursuing their own objectives while potentially cooperating or competing with other agents."
    },
    {
      "footnote_id": "fn-edge",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1136,
      "context": "...rators are emerging across the spectrum\u2014from powerful data center chips to efficient edge processors[^fn-edge] to tiny neural processing units in mobile devices. This heterogeneous computing landscape enables d...",
      "full_line": "At the infrastructure level, new hardware is reshaping deployment possibilities. Specialized AI accelerators are emerging across the spectrum\u2014from powerful data center chips to efficient edge processors[^fn-edge] to tiny neural processing units in mobile devices. This heterogeneous computing landscape enables dynamic model distribution across tiers based on computing capabilities and conditions, blurring traditional boundaries between cloud, edge, and embedded systems."
    },
    {
      "footnote_id": "fn-edge",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1138,
      "context": "[^fn-edge]: **Edge Processor**: A specialized computing device designed to perform AI computations close to wh...",
      "full_line": "[^fn-edge]: **Edge Processor**: A specialized computing device designed to perform AI computations close to where data is generated, optimized for low latency and energy efficiency rather than raw computing power."
    },
    {
      "footnote_id": "fn-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1158,
      "context": "...f operating with limited computational resources. Machine learning methods such as transfer learning[^fn-transfer-learning] allow models to learn on data-rich farms to be adapted for use in areas with limited historical dat...",
      "full_line": "FarmBeats uses a variety of ML algorithms tailored to agricultural applications. For soil moisture prediction, it uses temporal neural networks that can capture the complex dynamics of water movement in soil. Image analysis algorithms process drone imagery to detect crop stress, pest infestations, and yield estimates. These models must be robust to noisy data and capable of operating with limited computational resources. Machine learning methods such as transfer learning[^fn-transfer-learning] allow models to learn on data-rich farms to be adapted for use in areas with limited historical data."
    },
    {
      "footnote_id": "fn-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1160,
      "context": "[^fn-transfer-learning]: **Transfer Learning**: A machine learning technique where a model developed for one task is reused...",
      "full_line": "[^fn-transfer-learning]: **Transfer Learning**: A machine learning technique where a model developed for one task is reused as the starting point for a model on a related task, significantly reducing the amount of training data and computation required\u2014particularly valuable in domains like agriculture where labeled data may be scarce."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1192,
      "context": "...res massive parallel computing resources, leveraging clusters of GPUs or specialized AI chips (TPUs)[^fn-tpu] in a distributed computing environment. DeepMind utilized Google's cloud infrastructure, with the f...",
      "full_line": "The computational demands of AlphaFold epitomize the challenges of large-scale scientific ML systems. Training the model requires massive parallel computing resources, leveraging clusters of GPUs or specialized AI chips (TPUs)[^fn-tpu] in a distributed computing environment. DeepMind utilized Google's cloud infrastructure, with the final version of AlphaFold trained on 128 TPUv3 cores for several weeks."
    },
    {
      "footnote_id": "fn-rnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1208,
      "context": "...the behavior of other road users, leverage specialized neural networks designed for sequential data[^fn-rnn] to understand temporal patterns in road user behavior. Waymo has developed custom ML models like Ve...",
      "full_line": "Waymo's ML stack represents a sophisticated ensemble of algorithms tailored to the multifaceted challenge of autonomous driving. The perception system employs specialized neural networks to process visual data for object detection and tracking. Prediction models, needed for anticipating the behavior of other road users, leverage specialized neural networks designed for sequential data[^fn-rnn] to understand temporal patterns in road user behavior. Waymo has developed custom ML models like VectorNet for predicting vehicle trajectories. The planning and decision-making systems may incorporate learning-from-experience techniques to navigate complex traffic scenarios."
    },
    {
      "footnote_id": "fn-rnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1210,
      "context": "[^fn-rnn]: **Sequential Neural Networks**: Neural network architectures designed to process data that occurs...",
      "full_line": "[^fn-rnn]: **Sequential Neural Networks**: Neural network architectures designed to process data that occurs in sequences over time, such as predicting where a pedestrian will move next based on their previous movements. These networks maintain a form of \"memory\" of previous inputs to inform current decisions."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1214,
      "context": "...ions in real-time, often leveraging specialized hardware like GPUs or tensor processing units (TPUs)[^fn-tpu]. This edge computing is complemented by extensive use of cloud infrastructure, leveraging the power...",
      "full_line": "The computing infrastructure supporting Waymo's autonomous vehicles epitomizes the challenges of deploying ML systems across the full spectrum from edge to cloud. Each vehicle is equipped with a custom-designed compute platform capable of processing sensor data and making decisions in real-time, often leveraging specialized hardware like GPUs or tensor processing units (TPUs)[^fn-tpu]. This edge computing is complemented by extensive use of cloud infrastructure, leveraging the power of Google's data centers for training models, running large-scale simulations, and performing fleet-wide learning. The connectivity between these tiers is critical, with vehicles requiring reliable, high-bandwidth communication for real-time updates and data uploading. Waymo's infrastructure must be designed for robustness and fault tolerance, ensuring safe operation even in the face of hardware failures or network disruptions. The scale of Waymo's operation presents significant challenges in data management, model deployment, and system monitoring across a geographically distributed fleet of vehicles."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1216,
      "context": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: A specialized AI accelerator chip designed by Google specificall...",
      "full_line": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: A specialized AI accelerator chip designed by Google specifically for neural network machine learning, particularly efficient at matrix operations common in deep learning workloads."
    },
    {
      "footnote_id": "fn-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1232,
      "context": "Another critical challenge is how data changes over time. This phenomenon, known as \"data drift\"[^fn-drift], occurs when the patterns in new data begin to differ from the patterns the system originally learn...",
      "full_line": "Another critical challenge is how data changes over time. This phenomenon, known as \"data drift\"[^fn-drift], occurs when the patterns in new data begin to differ from the patterns the system originally learned from. For example, many predictive models struggled during the COVID-19 pandemic because consumer behavior changed so dramatically that historical patterns became less relevant. ML systems need ways to detect when this happens and adapt accordingly."
    },
    {
      "footnote_id": "fn-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1234,
      "context": "[^fn-drift]: **Data Drift**: The gradual change in the statistical properties of the target variable (what the...",
      "full_line": "[^fn-drift]: **Data Drift**: The gradual change in the statistical properties of the target variable (what the model is trying to predict) over time, which can degrade model performance if not properly monitored and addressed."
    },
    {
      "footnote_id": "fn-backprop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1238,
      "context": "...hundreds of billions of parameters that need to be optimized through systematic learning algorithms[^fn-backprop]. This complexity creates practical challenges: these models require enormous computing power to tra...",
      "full_line": "Creating and maintaining the ML models themselves presents another set of challenges. Modern ML models, particularly in deep learning, can be extremely complex. Consider a language model like GPT-3, which has hundreds of billions of parameters that need to be optimized through systematic learning algorithms[^fn-backprop]. This complexity creates practical challenges: these models require enormous computing power to train and run, making it difficult to deploy them in situations with limited resources, like on mobile phones or IoT devices."
    },
    {
      "footnote_id": "fn-backprop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1240,
      "context": "[^fn-backprop]: **Backpropagation**: The primary algorithm used to train neural networks, which calculates how eac...",
      "full_line": "[^fn-backprop]: **Backpropagation**: The primary algorithm used to train neural networks, which calculates how each parameter in the network should be adjusted to minimize prediction errors by propagating error gradients backward through the network layers."
    },
    {
      "footnote_id": "fn-transfer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1242,
      "context": "...rite explicit instructions, ML models learn from examples through techniques like knowledge transfer[^fn-transfer]. This learning process involves many choices: How should we structure the model? How long should we...",
      "full_line": "Training these models effectively is itself a significant challenge. Unlike traditional programming where we write explicit instructions, ML models learn from examples through techniques like knowledge transfer[^fn-transfer]. This learning process involves many choices: How should we structure the model? How long should we train it? How can we tell if it's learning the right things? Making these decisions often requires both technical expertise and considerable trial and error."
    },
    {
      "footnote_id": "fn-transfer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1244,
      "context": "[^fn-transfer]: **Transfer Learning**: A machine learning method where a model developed for one task is reused as...",
      "full_line": "[^fn-transfer]: **Transfer Learning**: A machine learning method where a model developed for one task is reused as the starting point for a model on a second task, significantly reducing the amount of training data and computation required."
    },
    {
      "footnote_id": "fn-black-box",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1260,
      "context": "...ion is transparency. Many modern ML models, particularly deep learning models, work as \"black boxes\"[^fn-black-box]---while they can make predictions, it's often difficult to understand how they arrived at their dec...",
      "full_line": "Another important consideration is transparency. Many modern ML models, particularly deep learning models, work as \"black boxes\"[^fn-black-box]---while they can make predictions, it's often difficult to understand how they arrived at their decisions."
    },
    {
      "footnote_id": "fn-black-box",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1262,
      "context": "[^fn-black-box]: **Black Box**: A system where you can observe the inputs and outputs but cannot see or understand...",
      "full_line": "[^fn-black-box]: **Black Box**: A system where you can observe the inputs and outputs but cannot see or understand the internal workings\u2014like how a radio receives signals and produces sound without most users understanding the electronics inside. In AI, this opacity becomes problematic when the system makes important decisions affecting people's lives."
    },
    {
      "footnote_id": "fn-inference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1266,
      "context": "...re that models don't inadvertently memorize and reveal private information through inference attacks[^fn-inference]? These challenges aren't merely technical problems to be solved, but ongoing considerations that sh...",
      "full_line": "Privacy is also a major concern. ML systems often need large amounts of data to work effectively, but this data might contain sensitive personal information. How do we balance the need for data with the need to protect individual privacy? How do we ensure that models don't inadvertently memorize and reveal private information through inference attacks[^fn-inference]? These challenges aren't merely technical problems to be solved, but ongoing considerations that shape how we approach ML system design and deployment."
    },
    {
      "footnote_id": "fn-inference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1268,
      "context": "[^fn-inference]: **Inference Attack**: A technique where an adversary attempts to extract sensitive information abo...",
      "full_line": "[^fn-inference]: **Inference Attack**: A technique where an adversary attempts to extract sensitive information about the training data by making careful queries to a trained model, exploiting patterns the model may have inadvertently memorized during training."
    },
    {
      "footnote_id": "fn-data-centers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 39,
      "context": "...one end, we have cloud ML, which leverages powerful centralized computing resources in data centers[^fn-data-centers] for complex, data-intensive tasks. Moving along the spectrum, we encounter edge ML[^fn-edge-computi...",
      "full_line": "Modern machine learning systems span a spectrum of deployment options, each with distinct characteristics and use cases based on available computing resources. At one end, we have cloud ML, which leverages powerful centralized computing resources in data centers[^fn-data-centers] for complex, data-intensive tasks. Moving along the spectrum, we encounter edge ML[^fn-edge-computing], which brings computation closer to where data is generated for reduced latency and improved privacy. Mobile ML further extends these capabilities to smartphones and tablets, while at the far end, we find Tiny ML[^fn-tinyml-origin], which enables machine learning on extremely low-power devices with severe memory and processing constraints."
    },
    {
      "footnote_id": "fn-edge-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 39,
      "context": "...^fn-data-centers] for complex, data-intensive tasks. Moving along the spectrum, we encounter edge ML[^fn-edge-computing], which brings computation closer to where data is generated for reduced latency and improved privac...",
      "full_line": "Modern machine learning systems span a spectrum of deployment options, each with distinct characteristics and use cases based on available computing resources. At one end, we have cloud ML, which leverages powerful centralized computing resources in data centers[^fn-data-centers] for complex, data-intensive tasks. Moving along the spectrum, we encounter edge ML[^fn-edge-computing], which brings computation closer to where data is generated for reduced latency and improved privacy. Mobile ML further extends these capabilities to smartphones and tablets, while at the far end, we find Tiny ML[^fn-tinyml-origin], which enables machine learning on extremely low-power devices with severe memory and processing constraints."
    },
    {
      "footnote_id": "fn-tinyml-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 39,
      "context": "...further extends these capabilities to smartphones and tablets, while at the far end, we find Tiny ML[^fn-tinyml-origin], which enables machine learning on extremely low-power devices with severe memory and processing co...",
      "full_line": "Modern machine learning systems span a spectrum of deployment options, each with distinct characteristics and use cases based on available computing resources. At one end, we have cloud ML, which leverages powerful centralized computing resources in data centers[^fn-data-centers] for complex, data-intensive tasks. Moving along the spectrum, we encounter edge ML[^fn-edge-computing], which brings computation closer to where data is generated for reduced latency and improved privacy. Mobile ML further extends these capabilities to smartphones and tablets, while at the far end, we find Tiny ML[^fn-tinyml-origin], which enables machine learning on extremely low-power devices with severe memory and processing constraints."
    },
    {
      "footnote_id": "fn-design-tradeoffs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 41,
      "context": "This deployment spectrum represents a fundamental trade-off in system design[^fn-design-tradeoffs]. Cloud deployments offer maximum computational power and storage but require network connectivity a...",
      "full_line": "This deployment spectrum represents a fundamental trade-off in system design[^fn-design-tradeoffs]. Cloud deployments offer maximum computational power and storage but require network connectivity and may have latency concerns. Edge deployments reduce latency and keep data local but have intermediate resource constraints (tens to hundreds of watts, gigabytes of memory). Mobile deployments must balance capability with battery life and thermal limits[^fn-mobile-power]. TinyML pushes the boundaries of what's possible with minimal resources, often running on devices with just kilobytes of memory. Understanding these trade-offs is essential for choosing the right deployment approach for each application."
    },
    {
      "footnote_id": "fn-mobile-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 41,
      "context": "...igabytes of memory). Mobile deployments must balance capability with battery life and thermal limits[^fn-mobile-power]. TinyML pushes the boundaries of what's possible with minimal resources, often running on devices w...",
      "full_line": "This deployment spectrum represents a fundamental trade-off in system design[^fn-design-tradeoffs]. Cloud deployments offer maximum computational power and storage but require network connectivity and may have latency concerns. Edge deployments reduce latency and keep data local but have intermediate resource constraints (tens to hundreds of watts, gigabytes of memory). Mobile deployments must balance capability with battery life and thermal limits[^fn-mobile-power]. TinyML pushes the boundaries of what's possible with minimal resources, often running on devices with just kilobytes of memory. Understanding these trade-offs is essential for choosing the right deployment approach for each application."
    },
    {
      "footnote_id": "fn-data-centers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 43,
      "context": "[^fn-data-centers]: **Data Centers**: Modern hyperscale data centers can house hundreds of thousands of servers and co...",
      "full_line": "[^fn-data-centers]: **Data Centers**: Modern hyperscale data centers can house hundreds of thousands of servers and consume 20-50 megawatts of power\u2014equivalent to a small city. Google's data centers alone process over 189,000 searches per second globally as of 2025."
    },
    {
      "footnote_id": "fn-edge-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 45,
      "context": "[^fn-edge-computing]: **Edge Computing**: The term \"edge\" originates from network topology diagrams where the \"edge\" rep...",
      "full_line": "[^fn-edge-computing]: **Edge Computing**: The term \"edge\" originates from network topology diagrams where the \"edge\" represents the boundary between the core network and end devices. This paradigm emerged in the 1990s with content delivery networks (CDNs) but gained prominence with IoT proliferation after 2010."
    },
    {
      "footnote_id": "fn-tinyml-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 47,
      "context": "[^fn-tinyml-origin]: **TinyML**: Coined at Harvard in 2019, TinyML targets devices with <1mW power consumption and <1MB...",
      "full_line": "[^fn-tinyml-origin]: **TinyML**: Coined at Harvard in 2019, TinyML targets devices with <1mW power consumption and <1MB memory. The field emerged from the realization that 99% of sensor data is discarded due to transmission costs\u2014processing locally saves both energy and bandwidth."
    },
    {
      "footnote_id": "fn-design-tradeoffs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 49,
      "context": "[^fn-design-tradeoffs]: **System Design Trade-offs**: These fundamental tensions in computing were formalized by David Pat...",
      "full_line": "[^fn-design-tradeoffs]: **System Design Trade-offs**: These fundamental tensions in computing were formalized by David Patterson and John Hennessy in their seminal work on computer architecture, earning them the 2017 Turing Award for establishing the principles that govern modern processor design."
    },
    {
      "footnote_id": "fn-mobile-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 51,
      "context": "[^fn-mobile-power]: **Mobile Power Constraints**: Modern smartphones contain 3000-4000mAh batteries (~15Wh) but ML inf...",
      "full_line": "[^fn-mobile-power]: **Mobile Power Constraints**: Modern smartphones contain 3000-4000mAh batteries (~15Wh) but ML inference can consume 1-5W, reducing battery life significantly. Apple's Neural Engine and Google's Tensor chips were specifically designed to perform AI tasks at <1W power consumption."
    },
    {
      "footnote_id": "fn-cost-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 228,
      "context": "...es illustrate the vast range of computational resources, power requirements, and cost considerations[^fn-cost-spectrum] across the ML systems spectrum. As we explore each paradigm in detail, you can refer back to these...",
      "full_line": "To better understand the dramatic differences between these ML deployment options, @tbl-representative-systems provides examples of representative hardware platforms for each category. These examples illustrate the vast range of computational resources, power requirements, and cost considerations[^fn-cost-spectrum] across the ML systems spectrum. As we explore each paradigm in detail, you can refer back to these concrete examples to better understand the practical implications of each approach."
    },
    {
      "footnote_id": "fn-billion-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 259,
      "context": "...for model development, training, and deployment. Cloud ML handles models with billions of parameters[^fn-billion-parameters] trained on petabytes of data, though network delays may introduce latencies of 100-500 ms for onlin...",
      "full_line": "**Cloud ML**: Machine learning began predominantly in the cloud, where powerful, scalable data center servers train and run large ML models. Cloud ML leverages vast computational resources and storage capacities, enabling development of complex models trained on massive datasets. Cloud systems excel at tasks requiring extensive processing power and distributed training, making them ideal for applications where real time responsiveness isn't critical. Popular platforms like AWS SageMaker, Google Cloud AI, and Azure ML offer flexible, scalable solutions for model development, training, and deployment. Cloud ML handles models with billions of parameters[^fn-billion-parameters] trained on petabytes of data, though network delays may introduce latencies of 100-500 ms for online inference[^fn-inference-latency] (acceptable for batch processing but problematic for real-time applications)."
    },
    {
      "footnote_id": "fn-inference-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 259,
      "context": "...petabytes of data, though network delays may introduce latencies of 100-500 ms for online inference[^fn-inference-latency] (acceptable for batch processing but problematic for real-time applications).",
      "full_line": "**Cloud ML**: Machine learning began predominantly in the cloud, where powerful, scalable data center servers train and run large ML models. Cloud ML leverages vast computational resources and storage capacities, enabling development of complex models trained on massive datasets. Cloud systems excel at tasks requiring extensive processing power and distributed training, making them ideal for applications where real time responsiveness isn't critical. Popular platforms like AWS SageMaker, Google Cloud AI, and Azure ML offer flexible, scalable solutions for model development, training, and deployment. Cloud ML handles models with billions of parameters[^fn-billion-parameters] trained on petabytes of data, though network delays may introduce latencies of 100-500 ms for online inference[^fn-inference-latency] (acceptable for batch processing but problematic for real-time applications)."
    },
    {
      "footnote_id": "fn-edge-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 261,
      "context": "...l gateways, smart cameras, autonomous vehicles, and IoT hubs. Edge ML reduces latency to under 50 ms[^fn-edge-latency], enhances privacy by keeping data local, and operates with intermittent cloud connectivity. Edge sy...",
      "full_line": "**Edge ML**: Growing demand for real time, low latency processing drove the emergence of Edge ML. Edge computing brings inference capabilities closer to data sources through deployment on industrial gateways, smart cameras, autonomous vehicles, and IoT hubs. Edge ML reduces latency to under 50 ms[^fn-edge-latency], enhances privacy by keeping data local, and operates with intermittent cloud connectivity. Edge systems prove particularly valuable for applications requiring quick responses or handling sensitive data in industrial and enterprise settings. Frameworks like NVIDIA Jetson and Google's Edge TPU enable powerful ML capabilities on edge devices, playing crucial roles in IoT ecosystems by enabling real-time decision making and reducing bandwidth usage through local data processing."
    },
    {
      "footnote_id": "fn-mobile-storage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 263,
      "context": "...ally and operates offline, though it must balance model performance with device resource constraints[^fn-mobile-storage] (typically 4 to 8 GB RAM, 100 to 200 GB storage).",
      "full_line": "**Mobile ML**: Building on edge computing concepts, Mobile ML leverages the computational capabilities of smartphones and tablets. Mobile systems enable personalized, responsive applications while reducing reliance on constant network connectivity. Mobile ML balances the power of edge computing with the ubiquity of personal devices, utilizing onboard sensors (cameras, GPS, accelerometers) for unique ML applications. Frameworks like TensorFlow Lite and Core ML (optimized mobile ML frameworks) enable developers to deploy optimized models on mobile devices, achieving inference times under 30 ms for common tasks. Mobile ML enhances privacy by keeping personal data locally and operates offline, though it must balance model performance with device resource constraints[^fn-mobile-storage] (typically 4 to 8 GB RAM, 100 to 200 GB storage)."
    },
    {
      "footnote_id": "fn-memory-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 265,
      "context": "...are critical factors. Tiny ML devices typically operate with less than 1 MB of RAM and flash memory[^fn-memory-comparison], consuming only milliwatts of power to enable battery life of months or years[^fn-battery-life]. Ap...",
      "full_line": "**Tiny ML**: The latest development in this progression, Tiny ML enables ML models to run on extremely resource-constrained microcontrollers and small embedded systems. Tiny ML performs local inference without relying on connectivity to cloud, edge, or mobile device processing power. Tiny systems prove crucial for applications where size, power consumption, and cost are critical factors. Tiny ML devices typically operate with less than 1 MB of RAM and flash memory[^fn-memory-comparison], consuming only milliwatts of power to enable battery life of months or years[^fn-battery-life]. Applications include wake word detection, gesture recognition, and predictive maintenance in industrial settings. Platforms like Arduino Nano 33 BLE Sense and STM32 microcontrollers, coupled with frameworks like TensorFlow Lite for Microcontrollers, enable ML on these tiny devices. However, Tiny ML requires significant model optimization and precision reduction techniques to fit within severe constraints."
    },
    {
      "footnote_id": "fn-battery-life",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 265,
      "context": "...^fn-memory-comparison], consuming only milliwatts of power to enable battery life of months or years[^fn-battery-life]. Applications include wake word detection, gesture recognition, and predictive maintenance in indus...",
      "full_line": "**Tiny ML**: The latest development in this progression, Tiny ML enables ML models to run on extremely resource-constrained microcontrollers and small embedded systems. Tiny ML performs local inference without relying on connectivity to cloud, edge, or mobile device processing power. Tiny systems prove crucial for applications where size, power consumption, and cost are critical factors. Tiny ML devices typically operate with less than 1 MB of RAM and flash memory[^fn-memory-comparison], consuming only milliwatts of power to enable battery life of months or years[^fn-battery-life]. Applications include wake word detection, gesture recognition, and predictive maintenance in industrial settings. Platforms like Arduino Nano 33 BLE Sense and STM32 microcontrollers, coupled with frameworks like TensorFlow Lite for Microcontrollers, enable ML on these tiny devices. However, Tiny ML requires significant model optimization and precision reduction techniques to fit within severe constraints."
    },
    {
      "footnote_id": "fn-cost-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 276,
      "context": "[^fn-cost-spectrum]: **ML Hardware Cost Spectrum**: The cost range spans 6 orders of magnitude\u2014from $10 ESP32-CAM modul...",
      "full_line": "[^fn-cost-spectrum]: **ML Hardware Cost Spectrum**: The cost range spans 6 orders of magnitude\u2014from $10 ESP32-CAM modules to $200K+ DGX A100 systems. This 20,000x cost difference reflects proportional differences in computational capability, enabling deployment across vastly different economic contexts and use cases."
    },
    {
      "footnote_id": "fn-billion-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 278,
      "context": "[^fn-billion-parameters]: **Billion-Parameter Models**: GPT-3 has 175 billion parameters requiring 350GB of memory just to s...",
      "full_line": "[^fn-billion-parameters]: **Billion-Parameter Models**: GPT-3 has 175 billion parameters requiring 350GB of memory just to store weights. GPT-4 is estimated at 1.8 trillion parameters. For comparison, the human brain has approximately 86 billion neurons with 100 trillion synaptic connections\u2014suggesting AI models are approaching biological complexity."
    },
    {
      "footnote_id": "fn-inference-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 280,
      "context": "[^fn-inference-latency]: **Cloud Inference Latency**: Network latency includes propagation delay (speed of light limits), r...",
      "full_line": "[^fn-inference-latency]: **Cloud Inference Latency**: Network latency includes propagation delay (speed of light limits), routing delays, and processing time. Round-trip from California to Virginia takes minimum 80ms just for light travel. Adding internet routing, DNS lookup, and server processing typically results in 100-500ms total latency."
    },
    {
      "footnote_id": "fn-edge-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 282,
      "context": "[^fn-edge-latency]: **Edge Latency Advantage**: Edge processing eliminates network round-trips, achieving <10ms respon...",
      "full_line": "[^fn-edge-latency]: **Edge Latency Advantage**: Edge processing eliminates network round-trips, achieving <10ms response times for local inference. Industrial robots require <1ms control loops, autonomous vehicles need <10ms emergency responses\u2014both impossible with cloud processing but achievable with edge deployment."
    },
    {
      "footnote_id": "fn-mobile-storage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 284,
      "context": "[^fn-mobile-storage]: **Mobile Storage Evolution**: iPhone storage grew from 4GB (2007) to 1TB (2023)\u2014a 250x increase in...",
      "full_line": "[^fn-mobile-storage]: **Mobile Storage Evolution**: iPhone storage grew from 4GB (2007) to 1TB (2023)\u2014a 250x increase in 16 years. However, ML models grew even faster: ResNet-50 (25MB, 2015) to modern language models (>1GB compressed), creating ongoing storage pressure despite hardware improvements."
    },
    {
      "footnote_id": "fn-memory-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 286,
      "context": "[^fn-memory-comparison]: **Memory Scale Comparison**: TinyML devices operate with 256KB-2MB memory versus smartphones with...",
      "full_line": "[^fn-memory-comparison]: **Memory Scale Comparison**: TinyML devices operate with 256KB-2MB memory versus smartphones with 8-12GB (40,000x difference) and cloud servers with 1TB+ (4,000,000x difference). Yet TinyML can still perform useful inference through aggressive model compression and quantization techniques."
    },
    {
      "footnote_id": "fn-battery-life",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 288,
      "context": "[^fn-battery-life]: **Ultra-Long Battery Life**: TinyML enables 10+ year deployments on single batteries through duty...",
      "full_line": "[^fn-battery-life]: **Ultra-Long Battery Life**: TinyML enables 10+ year deployments on single batteries through duty cycling\u2014devices sleep 99.9% of the time, wake periodically for inference, then return to sleep. Average power consumption drops to 10-100 microwatts, making decade-long operation feasible on coin-cell batteries."
    },
    {
      "footnote_id": "fn-cloud-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 375,
      "context": "...modern machine learning often require the scalability and power of centralized cloud infrastructures[^fn-cloud-evolution]. Cloud Machine Learning (Cloud ML) handles tasks such as large scale data processing, collaborative...",
      "full_line": "The vast computational demands of modern machine learning often require the scalability and power of centralized cloud infrastructures[^fn-cloud-evolution]. Cloud Machine Learning (Cloud ML) handles tasks such as large scale data processing, collaborative model development, and advanced analytics. Cloud data centers leverage distributed architectures, offering specialized resources to train complex models and support diverse applications, from recommendation systems to natural language processing[^fn-nlp-compute]."
    },
    {
      "footnote_id": "fn-nlp-compute",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 375,
      "context": "...models and support diverse applications, from recommendation systems to natural language processing[^fn-nlp-compute].",
      "full_line": "The vast computational demands of modern machine learning often require the scalability and power of centralized cloud infrastructures[^fn-cloud-evolution]. Cloud Machine Learning (Cloud ML) handles tasks such as large scale data processing, collaborative model development, and advanced analytics. Cloud data centers leverage distributed architectures, offering specialized resources to train complex models and support diverse applications, from recommendation systems to natural language processing[^fn-nlp-compute]."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 452,
      "context": "...nfrastructure. @fig-cloudml-example illustrates this concept with an example from Google's Cloud TPU[^fn-tpu] data center. Cloud service providers offer a **virtual platform** consisting of high-capacity serve...",
      "full_line": "Cloud ML's defining characteristic is its centralized infrastructure. @fig-cloudml-example illustrates this concept with an example from Google's Cloud TPU[^fn-tpu] data center. Cloud service providers offer a **virtual platform** consisting of high-capacity servers, expansive storage solutions, and robust networking architectures housed in globally distributed data centers[^fn-hyperscale]. These centralized facilities can reach massive scale, housing rows upon rows of specialized hardware. Centralized infrastructure enables pooling and efficient management of computational resources, simplifying machine learning project scaling."
    },
    {
      "footnote_id": "fn-hyperscale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 452,
      "context": "...e storage solutions, and robust networking architectures housed in globally distributed data centers[^fn-hyperscale]. These centralized facilities can reach massive scale, housing rows upon rows of specialized hardwa...",
      "full_line": "Cloud ML's defining characteristic is its centralized infrastructure. @fig-cloudml-example illustrates this concept with an example from Google's Cloud TPU[^fn-tpu] data center. Cloud service providers offer a **virtual platform** consisting of high-capacity servers, expansive storage solutions, and robust networking architectures housed in globally distributed data centers[^fn-hyperscale]. These centralized facilities can reach massive scale, housing rows upon rows of specialized hardware. Centralized infrastructure enables pooling and efficient management of computational resources, simplifying machine learning project scaling."
    },
    {
      "footnote_id": "fn-ml-apis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 464,
      "context": "...ent and accessibility. Once trained and validated, machine learning models deploy through cloud APIs[^fn-ml-apis] and services, becoming accessible to users worldwide. Cloud deployment enables seamless integration...",
      "full_line": "Cloud ML also offers exceptional flexibility in deployment and accessibility. Once trained and validated, machine learning models deploy through cloud APIs[^fn-ml-apis] and services, becoming accessible to users worldwide. Cloud deployment enables seamless integration of ML capabilities into applications across mobile, web, and IoT platforms, regardless of end user computational resources."
    },
    {
      "footnote_id": "fn-paas-pricing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 468,
      "context": "By leveraging the pay-as-you-go pricing model[^fn-paas-pricing] offered by cloud service providers, Cloud ML allows organizations to avoid the upfront capital expe...",
      "full_line": "By leveraging the pay-as-you-go pricing model[^fn-paas-pricing] offered by cloud service providers, Cloud ML allows organizations to avoid the upfront capital expenditure associated with building and maintaining dedicated ML infrastructure. The ability to scale resources up during intensive training periods and down during lower demand ensures cost-effectiveness and financial flexibility in managing machine learning projects."
    },
    {
      "footnote_id": "fn-cloud-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 470,
      "context": "[^fn-cloud-evolution]: **Cloud Infrastructure Evolution**: Cloud computing for ML emerged from Amazon's decision in 2002...",
      "full_line": "[^fn-cloud-evolution]: **Cloud Infrastructure Evolution**: Cloud computing for ML emerged from Amazon's decision in 2002 to treat their internal infrastructure as a service. AWS launched in 2006, followed by Google Cloud (2008) and Azure (2010). By 2024, global cloud infrastructure spending exceeded $250 billion annually."
    },
    {
      "footnote_id": "fn-nlp-compute",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 472,
      "context": "[^fn-nlp-compute]: **NLP Computational Demands**: Modern language models like GPT-3 required 3,640 petaflop-days of c...",
      "full_line": "[^fn-nlp-compute]: **NLP Computational Demands**: Modern language models like GPT-3 required 3,640 petaflop-days of compute for training\u2014equivalent to running 1,000 NVIDIA V100 GPUs continuously for 355 days. This computational scale drove the need for massive cloud infrastructure."
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 474,
      "context": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for tensor operations...",
      "full_line": "[^fn-tpu]: **Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for tensor operations, first used internally in 2015 for neural network inference. A single TPU v4 Pod contains 4,096 chips and delivers over 1 exaflop of compute power\u2014more than most supercomputers."
    },
    {
      "footnote_id": "fn-hyperscale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 476,
      "context": "[^fn-hyperscale]: **Hyperscale Data Centers**: These facilities contain 5,000+ servers and cover 10,000+ square feet...",
      "full_line": "[^fn-hyperscale]: **Hyperscale Data Centers**: These facilities contain 5,000+ servers and cover 10,000+ square feet. Microsoft's data centers span over 200 locations globally, with some individual facilities consuming enough electricity to power 80,000 homes."
    },
    {
      "footnote_id": "fn-ml-apis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 478,
      "context": "[^fn-ml-apis]: **ML APIs**: Application Programming Interfaces that democratized AI by providing pre-trained mode...",
      "full_line": "[^fn-ml-apis]: **ML APIs**: Application Programming Interfaces that democratized AI by providing pre-trained models as web services. Google's Vision API launched in 2016, processing over 1 billion images monthly within two years\u2014enabling developers to add AI capabilities without ML expertise."
    },
    {
      "footnote_id": "fn-paas-pricing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 480,
      "context": "[^fn-paas-pricing]: **Pay-as-You-Go Pricing**: Revolutionary model where users pay only for actual compute time used,...",
      "full_line": "[^fn-paas-pricing]: **Pay-as-You-Go Pricing**: Revolutionary model where users pay only for actual compute time used, measured in GPU-hours or inference requests. Training a model might cost $50-500 on demand versus $50,000-500,000 to purchase equivalent hardware."
    },
    {
      "footnote_id": "fn-industrial-iot",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 532,
      "context": "...his paradigm is critical for time-sensitive applications, such as autonomous systems, industrial IoT[^fn-industrial-iot], and smart infrastructure, where minimizing latency and preserving data privacy are paramount. Edge...",
      "full_line": "As machine learning applications grow, so does the need for faster, localized decision making. Edge Machine Learning (Edge ML) shifts computation away from centralized servers, processing data closer to its source. This paradigm is critical for time-sensitive applications, such as autonomous systems, industrial IoT[^fn-industrial-iot], and smart infrastructure, where minimizing latency and preserving data privacy are paramount. Edge devices, like gateways and IoT hubs[^fn-iot-hubs], enable these systems to function efficiently while reducing dependence on cloud infrastructures."
    },
    {
      "footnote_id": "fn-iot-hubs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 532,
      "context": "...nimizing latency and preserving data privacy are paramount. Edge devices, like gateways and IoT hubs[^fn-iot-hubs], enable these systems to function efficiently while reducing dependence on cloud infrastructures.",
      "full_line": "As machine learning applications grow, so does the need for faster, localized decision making. Edge Machine Learning (Edge ML) shifts computation away from centralized servers, processing data closer to its source. This paradigm is critical for time-sensitive applications, such as autonomous systems, industrial IoT[^fn-industrial-iot], and smart infrastructure, where minimizing latency and preserving data privacy are paramount. Edge devices, like gateways and IoT hubs[^fn-iot-hubs], enable these systems to function efficiently while reducing dependence on cloud infrastructures."
    },
    {
      "footnote_id": "fn-iot-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 607,
      "context": "...ding data to remote servers, devices like smartphones, tablets, and Internet of Things (IoT) devices[^fn-iot-growth] process data locally. The figure showcases various examples of these edge devices, including wearab...",
      "full_line": "Edge ML processes data in a decentralized fashion, as illustrated in @fig-edgeml-example. Instead of sending data to remote servers, devices like smartphones, tablets, and Internet of Things (IoT) devices[^fn-iot-growth] process data locally. The figure showcases various examples of these edge devices, including wearables, industrial sensors, and smart home appliances. This local processing allows devices to make quick decisions based on the data they collect without relying heavily on a central server's resources."
    },
    {
      "footnote_id": "fn-latency-critical",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 615,
      "context": "...ML's main advantages is the significant latency reduction compared to Cloud ML. This reduced latency[^fn-latency-critical] can be a critical benefit in situations where milliseconds count, such as in autonomous vehicles, w...",
      "full_line": "One of Edge ML's main advantages is the significant latency reduction compared to Cloud ML. This reduced latency[^fn-latency-critical] can be a critical benefit in situations where milliseconds count, such as in autonomous vehicles, where quick decision making can mean the difference between safety and an accident."
    },
    {
      "footnote_id": "fn-endpoint-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 623,
      "context": "...concerns is the limited computational resources compared to cloud-based solutions. Endpoint devices[^fn-endpoint-constraints] typically have significantly less processing power and storage capacity than cloud servers, limitin...",
      "full_line": "However, Edge ML has its challenges. One of the main concerns is the limited computational resources compared to cloud-based solutions. Endpoint devices[^fn-endpoint-constraints] typically have significantly less processing power and storage capacity than cloud servers, limiting the complexity of the machine learning models that can be deployed."
    },
    {
      "footnote_id": "fn-industrial-iot",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 625,
      "context": "[^fn-industrial-iot]: **Industrial IoT**: Manufacturing generates over 1 exabyte of data annually, but less than 1% is a...",
      "full_line": "[^fn-industrial-iot]: **Industrial IoT**: Manufacturing generates over 1 exabyte of data annually, but less than 1% is analyzed due to connectivity constraints. Edge ML enables real-time analysis, with predictive maintenance alone saving manufacturers $630 billion globally by 2025."
    },
    {
      "footnote_id": "fn-iot-hubs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 627,
      "context": "[^fn-iot-hubs]: **IoT Hubs**: Central connection points that aggregate data from multiple sensors before cloud tra...",
      "full_line": "[^fn-iot-hubs]: **IoT Hubs**: Central connection points that aggregate data from multiple sensors before cloud transmission. A typical smart building might have 1 hub managing 100-1000 IoT sensors, reducing cloud traffic by 90% while enabling local decision-making."
    },
    {
      "footnote_id": "fn-iot-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 629,
      "context": "[^fn-iot-growth]: **IoT Device Growth**: From 8.4 billion connected devices in 2017 to a projected 25.4 billion by 2...",
      "full_line": "[^fn-iot-growth]: **IoT Device Growth**: From 8.4 billion connected devices in 2017 to a projected 25.4 billion by 2030. Each device generates 2.5 quintillion bytes of data daily, making edge processing essential for bandwidth management."
    },
    {
      "footnote_id": "fn-latency-critical",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 631,
      "context": "[^fn-latency-critical]: **Latency-Critical Applications**: Autonomous vehicles require <10ms response times for emergency...",
      "full_line": "[^fn-latency-critical]: **Latency-Critical Applications**: Autonomous vehicles require <10ms response times for emergency braking decisions. Industrial robotics needs <1ms for precision control. Cloud round-trip latency typically ranges from 50-200ms, making edge processing essential for safety-critical applications."
    },
    {
      "footnote_id": "fn-endpoint-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 633,
      "context": "[^fn-endpoint-constraints]: **Endpoint Device Constraints**: Typical edge devices have 1-8GB RAM and 2-32GB storage, versus cl...",
      "full_line": "[^fn-endpoint-constraints]: **Endpoint Device Constraints**: Typical edge devices have 1-8GB RAM and 2-32GB storage, versus cloud servers with 128-1024GB RAM and petabytes of storage. Processing power differs by 10-100x, necessitating specialized model compression techniques."
    },
    {
      "footnote_id": "fn-voice-recognition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 653,
      "context": "...lized capabilities. Mobile Machine Learning (Mobile ML) supports applications like voice recognition[^fn-voice-recognition], computational photography[^fn-computational-photography], and health monitoring, all while maintai...",
      "full_line": "Machine learning is increasingly being integrated into portable devices like smartphones and tablets, empowering users with real-time, personalized capabilities. Mobile Machine Learning (Mobile ML) supports applications like voice recognition[^fn-voice-recognition], computational photography[^fn-computational-photography], and health monitoring, all while maintaining data privacy through on-device computation. These battery-powered devices are optimized for responsiveness and can operate offline, making them indispensable in everyday consumer technologies."
    },
    {
      "footnote_id": "fn-computational-photography",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 653,
      "context": "...e ML) supports applications like voice recognition[^fn-voice-recognition], computational photography[^fn-computational-photography], and health monitoring, all while maintaining data privacy through on-device computation. These bat...",
      "full_line": "Machine learning is increasingly being integrated into portable devices like smartphones and tablets, empowering users with real-time, personalized capabilities. Mobile Machine Learning (Mobile ML) supports applications like voice recognition[^fn-voice-recognition], computational photography[^fn-computational-photography], and health monitoring, all while maintaining data privacy through on-device computation. These battery-powered devices are optimized for responsiveness and can operate offline, making them indispensable in everyday consumer technologies."
    },
    {
      "footnote_id": "fn-mobile-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 733,
      "context": "Mobile ML utilizes the processing power of mobile devices' System-on-Chip (SoC) architectures[^fn-mobile-soc], including specialized Neural Processing Units (NPUs[^fn-npu], dedicated chips for AI calculations)...",
      "full_line": "Mobile ML utilizes the processing power of mobile devices' System-on-Chip (SoC) architectures[^fn-mobile-soc], including specialized Neural Processing Units (NPUs[^fn-npu], dedicated chips for AI calculations) and AI accelerators. This enables efficient execution of ML models directly on the device, allowing for real-time processing of data from device sensors like cameras, microphones, and motion sensors without constant cloud connectivity."
    },
    {
      "footnote_id": "fn-npu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 733,
      "context": "...tem-on-Chip (SoC) architectures[^fn-mobile-soc], including specialized Neural Processing Units (NPUs[^fn-npu], dedicated chips for AI calculations) and AI accelerators. This enables efficient execution of ML m...",
      "full_line": "Mobile ML utilizes the processing power of mobile devices' System-on-Chip (SoC) architectures[^fn-mobile-soc], including specialized Neural Processing Units (NPUs[^fn-npu], dedicated chips for AI calculations) and AI accelerators. This enables efficient execution of ML models directly on the device, allowing for real-time processing of data from device sensors like cameras, microphones, and motion sensors without constant cloud connectivity."
    },
    {
      "footnote_id": "fn-tflite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 735,
      "context": "...pecialized frameworks and tools designed specifically for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mob...",
      "full_line": "Mobile ML is supported by specialized frameworks and tools designed specifically for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mobile hardware and provide efficient model compression and quantization[^fn-quantization] techniques to ensure smooth performance within mobile resource constraints."
    },
    {
      "footnote_id": "fn-coreml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 735,
      "context": "...cifically for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mobile hardware and provide efficient model com...",
      "full_line": "Mobile ML is supported by specialized frameworks and tools designed specifically for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mobile hardware and provide efficient model compression and quantization[^fn-quantization] techniques to ensure smooth performance within mobile resource constraints."
    },
    {
      "footnote_id": "fn-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 735,
      "context": "...rameworks are optimized for mobile hardware and provide efficient model compression and quantization[^fn-quantization] techniques to ensure smooth performance within mobile resource constraints.",
      "full_line": "Mobile ML is supported by specialized frameworks and tools designed specifically for mobile deployment, such as TensorFlow Lite[^fn-tflite] for Android devices and Core ML[^fn-coreml] for iOS devices. These frameworks are optimized for mobile hardware and provide efficient model compression and quantization[^fn-quantization] techniques to ensure smooth performance within mobile resource constraints."
    },
    {
      "footnote_id": "fn-mobile-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 747,
      "context": "...compared to cloud servers. Mobile ML must operate within limited RAM, storage, and processing power[^fn-mobile-constraints], requiring careful optimization of models and efficient resource management.",
      "full_line": "Despite modern mobile devices being powerful, they still face resource constraints compared to cloud servers. Mobile ML must operate within limited RAM, storage, and processing power[^fn-mobile-constraints], requiring careful optimization of models and efficient resource management."
    },
    {
      "footnote_id": "fn-voice-recognition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 749,
      "context": "[^fn-voice-recognition]: **Voice Recognition Evolution**: Apple's Siri (2011) required cloud processing with 200-500ms late...",
      "full_line": "[^fn-voice-recognition]: **Voice Recognition Evolution**: Apple's Siri (2011) required cloud processing with 200-500ms latency. By 2017, on-device processing reduced latency to <50ms while improving privacy. Modern smartphones process 16kHz audio at 20-30ms latency using specialized neural engines."
    },
    {
      "footnote_id": "fn-computational-photography",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 751,
      "context": "[^fn-computational-photography]: **Computational Photography**: Combines multiple exposures and ML algorithms to enhance image qual...",
      "full_line": "[^fn-computational-photography]: **Computational Photography**: Combines multiple exposures and ML algorithms to enhance image quality. Google's Night Sight captures 15 frames in 6 seconds, using ML to align and merge them. Portrait mode uses depth estimation ML models to create professional-looking bokeh effects in real-time."
    },
    {
      "footnote_id": "fn-mobile-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 753,
      "context": "[^fn-mobile-soc]: **Mobile System-on-Chip**: Modern flagship SoCs integrate CPU, GPU, NPU, and memory controllers on...",
      "full_line": "[^fn-mobile-soc]: **Mobile System-on-Chip**: Modern flagship SoCs integrate CPU, GPU, NPU, and memory controllers on a single chip. Apple's A17 Pro contains 19 billion transistors in a 3nm process, while Snapdragon 8 Gen 3 delivers significant AI performance improvements over its predecessor."
    },
    {
      "footnote_id": "fn-npu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 755,
      "context": "[^fn-npu]: **Neural Processing Unit (NPU)**: Specialized processors optimized for neural network operations....",
      "full_line": "[^fn-npu]: **Neural Processing Unit (NPU)**: Specialized processors optimized for neural network operations. Apple's Neural Engine (introduced in A11, 2017) performs 600 billion operations per second. Qualcomm's Hexagon NPU in flagship chips delivers up to 75 TOPS while consuming <1W."
    },
    {
      "footnote_id": "fn-tflite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 757,
      "context": "[^fn-tflite]: **TensorFlow Lite**: Google's mobile ML framework launched in 2017, designed to run models <100MB...",
      "full_line": "[^fn-tflite]: **TensorFlow Lite**: Google's mobile ML framework launched in 2017, designed to run models <100MB with <100ms inference time. Supports quantization to reduce model size by 75% while maintaining 95% accuracy. Used in over 4 billion devices worldwide."
    },
    {
      "footnote_id": "fn-coreml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 759,
      "context": "[^fn-coreml]: **Core ML**: Apple's framework introduced in iOS 11 (2017), optimized for on-device inference. Sup...",
      "full_line": "[^fn-coreml]: **Core ML**: Apple's framework introduced in iOS 11 (2017), optimized for on-device inference. Supports models from 1KB to 1GB, with automatic optimization for Apple Silicon. Enables features like Live Text, which processes text in real-time using on-device OCR models."
    },
    {
      "footnote_id": "fn-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 761,
      "context": "[^fn-quantization]: **Model Quantization**: Reduces model precision from 32-bit to 8-bit integers, cutting model size...",
      "full_line": "[^fn-quantization]: **Model Quantization**: Reduces model precision from 32-bit to 8-bit integers, cutting model size by 75% and speeding inference by 2-4x. INT8 quantization maintains >99% of original accuracy for most models while enabling deployment on resource-constrained devices."
    },
    {
      "footnote_id": "fn-mobile-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 763,
      "context": "[^fn-mobile-constraints]: **Mobile Device Constraints**: Flagship phones typically have 8-12GB RAM and 256-512GB storage, ve...",
      "full_line": "[^fn-mobile-constraints]: **Mobile Device Constraints**: Flagship phones typically have 8-12GB RAM and 256-512GB storage, versus cloud servers with 128-1024GB RAM and unlimited storage. Mobile processors operate at 15-25W peak power compared to server CPUs at 200-400W."
    },
    {
      "footnote_id": "fn-microcontrollers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 783,
      "context": "Tiny Machine Learning (Tiny ML) brings intelligence to the smallest devices, from microcontrollers[^fn-microcontrollers] to embedded sensors, enabling real-time computation in resource-constrained environments. These sys...",
      "full_line": "Tiny Machine Learning (Tiny ML) brings intelligence to the smallest devices, from microcontrollers[^fn-microcontrollers] to embedded sensors, enabling real-time computation in resource-constrained environments. These systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition. Tiny ML devices are optimized for energy efficiency[^fn-energy-efficiency], often running for months or years on limited power sources, such as coin-cell batteries[^fn-coin-cell], while delivering actionable insights in remote or disconnected environments."
    },
    {
      "footnote_id": "fn-energy-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 783,
      "context": "...ntal monitoring, and simple gesture recognition. Tiny ML devices are optimized for energy efficiency[^fn-energy-efficiency], often running for months or years on limited power sources, such as coin-cell batteries[^fn-coin-c...",
      "full_line": "Tiny Machine Learning (Tiny ML) brings intelligence to the smallest devices, from microcontrollers[^fn-microcontrollers] to embedded sensors, enabling real-time computation in resource-constrained environments. These systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition. Tiny ML devices are optimized for energy efficiency[^fn-energy-efficiency], often running for months or years on limited power sources, such as coin-cell batteries[^fn-coin-cell], while delivering actionable insights in remote or disconnected environments."
    },
    {
      "footnote_id": "fn-coin-cell",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 783,
      "context": "...efficiency], often running for months or years on limited power sources, such as coin-cell batteries[^fn-coin-cell], while delivering actionable insights in remote or disconnected environments.",
      "full_line": "Tiny Machine Learning (Tiny ML) brings intelligence to the smallest devices, from microcontrollers[^fn-microcontrollers] to embedded sensors, enabling real-time computation in resource-constrained environments. These systems power applications such as predictive maintenance, environmental monitoring, and simple gesture recognition. Tiny ML devices are optimized for energy efficiency[^fn-energy-efficiency], often running for months or years on limited power sources, such as coin-cell batteries[^fn-coin-cell], while delivering actionable insights in remote or disconnected environments."
    },
    {
      "footnote_id": "fn-on-device-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 860,
      "context": "...ice machine learning. This means that machine learning models are deployed and trained on the device[^fn-on-device-training], eliminating the need for external servers or cloud infrastructures. This allows Tiny ML to enable...",
      "full_line": "In Tiny ML, the focus, much like in Mobile ML, is on on-device machine learning. This means that machine learning models are deployed and trained on the device[^fn-on-device-training], eliminating the need for external servers or cloud infrastructures. This allows Tiny ML to enable intelligent decision making right where the data is generated, making real time insights and actions possible, even in settings where connectivity is limited or unavailable."
    },
    {
      "footnote_id": "fn-device-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 862,
      "context": "...ices can typically fit in the palm of your hand or, in some cases, are even as small as a fingernail[^fn-device-size]. Tiny ML meets the need for efficiency through specialized algorithms and models designed to delive...",
      "full_line": "Tiny ML excels in low-power and resource-constrained settings. These environments require highly optimized solutions that function within the available resources. @fig-TinyML-example showcases an example Tiny ML device kit, illustrating the compact nature of these systems. These devices can typically fit in the palm of your hand or, in some cases, are even as small as a fingernail[^fn-device-size]. Tiny ML meets the need for efficiency through specialized algorithms and models designed to deliver decent performance while consuming minimal energy, thus ensuring extended operational periods, even in battery-powered devices like those shown."
    },
    {
      "footnote_id": "fn-model-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 880,
      "context": "A central challenge in Tiny ML is model optimization and compression[^fn-model-compression]. Creating machine learning models that can operate effectively within the limited memory and comput...",
      "full_line": "A central challenge in Tiny ML is model optimization and compression[^fn-model-compression]. Creating machine learning models that can operate effectively within the limited memory and computational power of microcontrollers requires innovative approaches to model design. Developers often face the challenge of striking a delicate balance and optimizing models to maintain effectiveness while fitting within stringent resource constraints."
    },
    {
      "footnote_id": "fn-microcontrollers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 882,
      "context": "[^fn-microcontrollers]: **Microcontrollers**: Single-chip computers with integrated CPU, memory, and peripherals, typicall...",
      "full_line": "[^fn-microcontrollers]: **Microcontrollers**: Single-chip computers with integrated CPU, memory, and peripherals, typically operating at 1-100MHz with 32KB-2MB RAM. Arduino Uno uses an ATmega328P with 32KB flash and 2KB RAM, while ESP32 provides WiFi capability with 520KB RAM\u2014still thousands of times less than a smartphone."
    },
    {
      "footnote_id": "fn-energy-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 884,
      "context": "[^fn-energy-efficiency]: **Energy Efficiency in TinyML**: Ultra-low power consumption enables deployment in remote location...",
      "full_line": "[^fn-energy-efficiency]: **Energy Efficiency in TinyML**: Ultra-low power consumption enables deployment in remote locations. Modern ARM Cortex-M0+ microcontrollers consume <1\u00b5W in sleep mode and 100-300\u00b5W/MHz when active. Efficient ML inference can run for years on a single coin-cell battery."
    },
    {
      "footnote_id": "fn-coin-cell",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 886,
      "context": "[^fn-coin-cell]: **Coin-Cell Batteries**: Small, round batteries (CR2032 being most common) providing 200-250mAh at...",
      "full_line": "[^fn-coin-cell]: **Coin-Cell Batteries**: Small, round batteries (CR2032 being most common) providing 200-250mAh at 3V. When powering TinyML devices at 10-50mW average consumption, these batteries can operate devices for 1-5 years, enabling \"deploy-and-forget\" IoT applications."
    },
    {
      "footnote_id": "fn-on-device-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 888,
      "context": "[^fn-on-device-training]: **On-Device Training Constraints**: Unlike mobile devices, microcontrollers rarely support full mo...",
      "full_line": "[^fn-on-device-training]: **On-Device Training Constraints**: Unlike mobile devices, microcontrollers rarely support full model training due to memory limitations. Instead, they use techniques like transfer learning, where a pre-trained model is fine-tuned with minimal on-device adaptation, or federated learning aggregation."
    },
    {
      "footnote_id": "fn-device-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 890,
      "context": "[^fn-device-size]: **TinyML Device Scale**: The smallest ML-capable devices measure just 5x5mm (Syntiant NDP chips)....",
      "full_line": "[^fn-device-size]: **TinyML Device Scale**: The smallest ML-capable devices measure just 5x5mm (Syntiant NDP chips). Google's Coral Dev Board Mini measures 40x48mm but includes WiFi and full Linux capability. The extreme miniaturization enables integration into previously \"dumb\" objects like smart dust sensors."
    },
    {
      "footnote_id": "fn-model-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 892,
      "context": "[^fn-model-compression]: **TinyML Model Compression**: Techniques include pruning (removing 90%+ of neural network connecti...",
      "full_line": "[^fn-model-compression]: **TinyML Model Compression**: Techniques include pruning (removing 90%+ of neural network connections), quantization to 8-bit or even 1-bit precision, and knowledge distillation. A typical smartphone model of 50MB might compress to 250KB for microcontroller deployment while retaining 95% accuracy."
    },
    {
      "footnote_id": "fn-a11-bionic-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 42,
      "context": "...ocess of training or adapting machine learning models directly on the device where they are deployed[^fn-a11-bionic-breakthrough]. : **On-Device Learning Evolution**: While edge inference emerged in the 2000s with mobile GPUs, on...",
      "full_line": "On-device learning refers to the process of training or adapting machine learning models directly on the device where they are deployed[^fn-a11-bionic-breakthrough]. : **On-Device Learning Evolution**: While edge inference emerged in the 2000s with mobile GPUs, on-device training didn't become feasible until Apple's A11 Bionic chip (2017) and Google's Pixel Visual Core (2017) provided sufficient TOPS for gradient computation. The shift from \"smart inference\" to \"smart training\" marked a fundamental paradigm change in mobile AI."
    },
    {
      "footnote_id": "fn-a11-bionic-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 44,
      "context": "[^fn-a11-bionic-breakthrough]: **A11 Bionic Breakthrough**: Apple's A11 Bionic (2017) was the first mobile chip with sufficient c...",
      "full_line": "[^fn-a11-bionic-breakthrough]: **A11 Bionic Breakthrough**: Apple's A11 Bionic (2017) was the first mobile chip with sufficient computational power for on-device training, delivering 0.6 TOPS compared to the previous A10's 0.2 TOPS. This 3x improvement, combined with 2.39 billion transistors and a dual-core Neural Engine, enabled gradient computation for the first time on mobile devices. Google's Pixel Visual Core achieved similar capabilities with 8 custom Image Processing Units optimized for machine learning workloads."
    },
    {
      "footnote_id": "fn-edge-computing-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 50,
      "context": "...on, lifelong adaptation, and secure execution_, expanding the frontier of intelligent edge computing[^fn-edge-computing-origin].",
      "full_line": "**On-Device Learning** is the _local adaptation or training_ of machine learning models directly on deployed hardware devices, without reliance on continuous connectivity to centralized servers. It enables _personalization, privacy preservation, and autonomous operation_ by leveraging user-specific data collected in situ. On-device learning systems must operate under _tight constraints on compute, memory, energy, and data availability_, requiring specialized methods for model optimization, training efficiency, and data representation. As on-device learning matures, it increasingly incorporates _federated collaboration, lifelong adaptation, and secure execution_, expanding the frontier of intelligent edge computing[^fn-edge-computing-origin]."
    },
    {
      "footnote_id": "fn-edge-computing-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 54,
      "context": "[^fn-edge-computing-origin]: **Edge Computing Origins**: The term \"edge computing\" was coined by Akamai in the late 1990s for c...",
      "full_line": "[^fn-edge-computing-origin]: **Edge Computing Origins**: The term \"edge computing\" was coined by Akamai in the late 1990s for content delivery networks, but the modern concept emerged from Cisco's \"fog computing\" initiative in 2012. The realization that data was growing faster than network bandwidth could handle led to the \"bring compute to the data\" philosophy that defines edge AI today."
    },
    {
      "footnote_id": "fn-federated-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 58,
      "context": "Particular emphasis is placed on distributed and collaborative methods, such as federated learning[^fn-federated-birth], which enable decentralized training without direct data sharing. The chapter concludes with an ana...",
      "full_line": "Particular emphasis is placed on distributed and collaborative methods, such as federated learning[^fn-federated-birth], which enable decentralized training without direct data sharing. The chapter concludes with an analysis of outstanding challenges, including issues related to reliability, system validation, and the heterogeneity of deployment environments."
    },
    {
      "footnote_id": "fn-federated-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 74,
      "context": "[^fn-federated-birth]: **Federated Learning Birth**: Google's Brendan McMahan coined \"federated learning\" in 2016, but th...",
      "full_line": "[^fn-federated-birth]: **Federated Learning Birth**: Google's Brendan McMahan coined \"federated learning\" in 2016, but the concept emerged from their Gboard team's frustration with keyboard personalization. They realized they needed user-specific data to improve predictions, but couldn't collect keystrokes due to privacy concerns. This led to the \"train where the data lives\" philosophy that defined federated learning."
    },
    {
      "footnote_id": "fn-gdpr-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 76,
      "context": "...rating within privacy-preserving boundaries\u2014potentially aiding adherence to regulations such as GDPR[^fn-gdpr-impact], HIPAA, or region-specific data sovereignty laws.",
      "full_line": "Privacy is another critical factor. Many applications involve sensitive or regulated data, including biometric measurements, typed input, location traces, or health information. Transmitting such data to the cloud introduces privacy risks and compliance burdens. Local learning mitigates these concerns by keeping raw data on the device and operating within privacy-preserving boundaries\u2014potentially aiding adherence to regulations such as GDPR[^fn-gdpr-impact], HIPAA, or region-specific data sovereignty laws."
    },
    {
      "footnote_id": "fn-gdpr-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 92,
      "context": "[^fn-gdpr-impact]: **GDPR's ML Impact**: When GDPR took effect in May 2018, it essentially made centralized ML traini...",
      "full_line": "[^fn-gdpr-impact]: **GDPR's ML Impact**: When GDPR took effect in May 2018, it essentially made centralized ML training illegal for personal data without explicit consent. The \"right to be forgotten\" also meant models trained on personal data could be legally required to \"unlearn\" specific users\u2014technically impossible with traditional training. This drove massive investment in privacy-preserving ML techniques."
    },
    {
      "footnote_id": "fn-arduino-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 535,
      "context": "...far exceeds the memory available on embedded microcontrollers such as the Arduino Nano 33 BLE Sense[^fn-arduino-constraints], which provides only 256 KB of SRAM and 1 MB of flash storage. In such platforms, even a single lay...",
      "full_line": "For example, the MobileNetV2 architecture, commonly used in mobile vision tasks, requires approximately 14 MB of storage in its standard configuration. While this is feasible for modern smartphones, it far exceeds the memory available on embedded microcontrollers such as the Arduino Nano 33 BLE Sense[^fn-arduino-constraints], which provides only 256 KB of SRAM and 1 MB of flash storage. In such platforms, even a single layer of a typical convolutional neural network may exceed available RAM during training due to the need to store intermediate feature maps."
    },
    {
      "footnote_id": "fn-arduino-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 537,
      "context": "[^fn-arduino-constraints]: **Arduino Edge Computing Reality**: The Arduino Nano 33 BLE Sense represents typical microcontroll...",
      "full_line": "[^fn-arduino-constraints]: **Arduino Edge Computing Reality**: The Arduino Nano 33 BLE Sense represents typical microcontroller constraints\u2014256KB SRAM is roughly 65,000 times smaller than a modern smartphone's 16GB RAM. To put this in perspective, storing just one 224\u00d7224\u00d73 RGB image (150KB) would consume 60% of available memory. Training requires 3-5x more memory for gradients and activations, making even tiny models challenging. The 1MB flash storage can hold only the smallest quantized models, forcing designers to use 8-bit or even 4-bit representations."
    },
    {
      "footnote_id": "fn-mobilenet-innovation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 543,
      "context": "...e adaptation due to their size and complexity. Instead, lightweight architectures such as MobileNets[^fn-mobilenet-innovation], SqueezeNet, and EfficientNet have been developed specifically for resource-constrained environment...",
      "full_line": "The model architecture itself must also be designed with on-device learning in mind. Many conventional architectures, such as transformers or large convolutional networks, are not well-suited for on-device adaptation due to their size and complexity. Instead, lightweight architectures such as MobileNets[^fn-mobilenet-innovation], SqueezeNet, and EfficientNet have been developed specifically for resource-constrained environments. These models use techniques such as depthwise separable convolutions[^fn-depthwise-separable], bottleneck layers, and quantization to reduce memory and compute requirements while maintaining performance."
    },
    {
      "footnote_id": "fn-depthwise-separable",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 543,
      "context": "...ource-constrained environments. These models use techniques such as depthwise separable convolutions[^fn-depthwise-separable], bottleneck layers, and quantization to reduce memory and compute requirements while maintaining pe...",
      "full_line": "The model architecture itself must also be designed with on-device learning in mind. Many conventional architectures, such as transformers or large convolutional networks, are not well-suited for on-device adaptation due to their size and complexity. Instead, lightweight architectures such as MobileNets[^fn-mobilenet-innovation], SqueezeNet, and EfficientNet have been developed specifically for resource-constrained environments. These models use techniques such as depthwise separable convolutions[^fn-depthwise-separable], bottleneck layers, and quantization to reduce memory and compute requirements while maintaining performance."
    },
    {
      "footnote_id": "fn-mobilenet-innovation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 545,
      "context": "[^fn-mobilenet-innovation]: **MobileNet Innovation**: Google's MobileNet family revolutionized mobile AI by achieving 10-20x p...",
      "full_line": "[^fn-mobilenet-innovation]: **MobileNet Innovation**: Google's MobileNet family revolutionized mobile AI by achieving 10-20x parameter reduction compared to traditional CNNs. MobileNetV1 (2017) used depthwise separable convolutions to reduce FLOPs by 8-9x, while MobileNetV2 (2018) added inverted residuals and linear bottlenecks. The breakthrough enabled real-time inference on smartphones: MobileNetV2 runs ImageNet classification in ~75ms on a Pixel phone versus 1.8 seconds for ResNet-50."
    },
    {
      "footnote_id": "fn-depthwise-separable",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 547,
      "context": "[^fn-depthwise-separable]: **Depthwise Separable Convolutions**: This technique decomposes standard convolution into two oper...",
      "full_line": "[^fn-depthwise-separable]: **Depthwise Separable Convolutions**: This technique decomposes standard convolution into two operations: depthwise convolution (applies single filter per input channel) and pointwise convolution (1\u00d71 conv to combine channels). For a 3\u00d73 conv with 512 input/output channels, standard convolution requires 2.4M parameters while depthwise separable needs only 13.8K\u2014a 174x reduction. The computational savings are similarly dramatic, making real-time inference possible on mobile CPUs."
    },
    {
      "footnote_id": "fn-stm32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 569,
      "context": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and la...",
      "full_line": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-point operations [@lai2020tinyml]. These constraints preclude the use of conventional deep learning libraries and require models to be carefully designed for integer arithmetic and minimal runtime memory allocation. In such cases, even small models require tailored techniques, including quantization-aware training and selective parameter updates, to execute training loops without exceeding memory or power budgets. For example, the STM32F4 microcontroller can run a simple linear regression model with a few hundred parameters, but training even a small convolutional neural network would exceed its memory capacity. In these environments, training is often limited to simple algorithms such as stochastic gradient descent (SGD) or $k$-means clustering, which can be implemented using integer arithmetic and minimal memory overhead."
    },
    {
      "footnote_id": "fn-esp32-capabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 569,
      "context": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-...",
      "full_line": "On the embedded end of the spectrum, devices such as the STM32F4[^fn-stm32-constraints] or ESP32[^fn-esp32-capabilities] microcontrollers offer only a few hundred kilobytes of SRAM and lack hardware support for floating-point operations [@lai2020tinyml]. These constraints preclude the use of conventional deep learning libraries and require models to be carefully designed for integer arithmetic and minimal runtime memory allocation. In such cases, even small models require tailored techniques, including quantization-aware training and selective parameter updates, to execute training loops without exceeding memory or power budgets. For example, the STM32F4 microcontroller can run a simple linear regression model with a few hundred parameters, but training even a small convolutional neural network would exceed its memory capacity. In these environments, training is often limited to simple algorithms such as stochastic gradient descent (SGD) or $k$-means clustering, which can be implemented using integer arithmetic and minimal memory overhead."
    },
    {
      "footnote_id": "fn-stm32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 571,
      "context": "[^fn-stm32-constraints]: **STM32F4 Microcontroller Reality**: The STM32F4 represents the harsh reality of embedded computin...",
      "full_line": "[^fn-stm32-constraints]: **STM32F4 Microcontroller Reality**: The STM32F4 represents the harsh reality of embedded computing\u2014192KB SRAM (roughly the size of a small JPEG image) and 1MB flash storage, running at 168MHz without floating-point hardware acceleration. Integer arithmetic is 10-100x slower than dedicated floating-point units found in mobile chips. Power consumption is ~100mW during active processing, requiring careful duty-cycling to preserve battery life. These constraints make even simple neural networks challenging\u2014a 10-neuron hidden layer requires ~40KB for weights alone in FP32."
    },
    {
      "footnote_id": "fn-esp32-capabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 573,
      "context": "[^fn-esp32-capabilities]: **ESP32 Edge Computing**: The ESP32 provides 520KB SRAM and dual-core processing at 240MHz, making...",
      "full_line": "[^fn-esp32-capabilities]: **ESP32 Edge Computing**: The ESP32 provides 520KB SRAM and dual-core processing at 240MHz, making it more capable than STM32F4 but still severely constrained. Its key advantage is built-in WiFi and Bluetooth for federated learning scenarios. However, the lack of hardware floating-point support means all ML operations must use integer quantization. Real-world deployments show 8-bit quantized models can achieve 95% of FP32 accuracy while fitting in ~50KB memory, enabling basic on-device training for simple tasks like sensor anomaly detection."
    },
    {
      "footnote_id": "fn-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 575,
      "context": "In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedic...",
      "full_line": "In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedicated AI accelerators and optimized support for 8-bit or mixed-precision matrix operations. These platforms can support more complex training routines, including full backpropagation over compact models, though they still fall short of the computational throughput and memory bandwidth available in centralized data centers. For instance, training a lightweight transformer on a smartphone is feasible but must be tightly bounded in both time and energy consumption to avoid degrading the user experience."
    },
    {
      "footnote_id": "fn-tensor-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 575,
      "context": "...re, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedicated AI accelerators and optimized supp...",
      "full_line": "In contrast, mobile-class hardware, including the Qualcomm Snapdragon, Apple Neural Engine[^fn-neural-engine], and Google Tensor SoC[^fn-tensor-soc], provides significantly more compute power, often with dedicated AI accelerators and optimized support for 8-bit or mixed-precision matrix operations. These platforms can support more complex training routines, including full backpropagation over compact models, though they still fall short of the computational throughput and memory bandwidth available in centralized data centers. For instance, training a lightweight transformer on a smartphone is feasible but must be tightly bounded in both time and energy consumption to avoid degrading the user experience."
    },
    {
      "footnote_id": "fn-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 577,
      "context": "[^fn-neural-engine]: **Apple Neural Engine Evolution**: Apple's Neural Engine has evolved dramatically since the A11 Bi...",
      "full_line": "[^fn-neural-engine]: **Apple Neural Engine Evolution**: Apple's Neural Engine has evolved dramatically since the A11 Bionic. The A17 Pro (2023) features a 16-core Neural Engine delivering 35 TOPS\u2014roughly equivalent to an NVIDIA GTX 1080 Ti. This represents a 58x improvement over the original A11. The Neural Engine specializes in matrix operations with dedicated 8-bit and 16-bit arithmetic units, enabling efficient on-device training. Real-world performance: fine-tuning a MobileNet classifier takes ~2 seconds versus 45 seconds on CPU alone, while consuming only ~500mW additional power."
    },
    {
      "footnote_id": "fn-tensor-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 579,
      "context": "[^fn-tensor-soc]: **Google Tensor SoC Architecture**: Google's Tensor chips (starting with Pixel 6 in 2021) feature...",
      "full_line": "[^fn-tensor-soc]: **Google Tensor SoC Architecture**: Google's Tensor chips (starting with Pixel 6 in 2021) feature a custom TPU v1-derived Edge TPU delivering ~5.5 TOPS for ML workloads. Unlike Apple's Neural Engine, Tensor optimizes for Google's specific models (speech recognition, computational photography). The TPU can perform 8-bit integer operations at 600 TOPS while consuming only 2W, making it highly efficient for federated learning scenarios where devices train locally on speech or image data."
    },
    {
      "footnote_id": "fn-tinytl-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 637,
      "context": "...el using TinyTL can reduce the number of updated parameters from over 3 million to fewer than 50,000[^fn-tinytl-efficiency]. Combined with quantization, this enables local adaptation on devices with only a few hundred kilob...",
      "full_line": "These design choices allow TinyTL to reduce training memory usage by more than 10\u00d7. For instance, adapting a MobileNetV2 model using TinyTL can reduce the number of updated parameters from over 3 million to fewer than 50,000[^fn-tinytl-efficiency]. Combined with quantization, this enables local adaptation on devices with only a few hundred kilobytes of memory\u2014making on-device learning truly feasible in constrained environments."
    },
    {
      "footnote_id": "fn-tinytl-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 639,
      "context": "[^fn-tinytl-efficiency]: **TinyTL Memory Breakthrough**: TinyTL's 60x parameter reduction (3M to 50K) translates to dramati...",
      "full_line": "[^fn-tinytl-efficiency]: **TinyTL Memory Breakthrough**: TinyTL's 60x parameter reduction (3M to 50K) translates to dramatic memory savings. In FP32, MobileNetV2 requires ~12MB for weights plus ~8MB for activation caching during training\u2014exceeding most microcontroller capabilities. TinyTL reduces this to ~200KB weights plus ~400KB activations, fitting comfortably within a 1MB memory budget. Real deployments on STM32H7 achieve 85% of full fine-tuning accuracy while using 15x less memory and completing updates in ~30 seconds versus 8 minutes for full training."
    },
    {
      "footnote_id": "fn-hey-siri-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 816,
      "context": "...@warden2018speech]. These models are used to detect fixed phrases, including phrases like \"Hey Siri\"[^fn-hey-siri-constraints] or \"OK Google\", with low latency and high reliability. A typical KWS model consists of a pretrained...",
      "full_line": "Keyword spotting (KWS) systems offer a concrete example of few-shot adaptation in a real-world, on-device deployment [@warden2018speech]. These models are used to detect fixed phrases, including phrases like \"Hey Siri\"[^fn-hey-siri-constraints] or \"OK Google\", with low latency and high reliability. A typical KWS model consists of a pretrained acoustic encoder (e.g., a small convolutional or recurrent network that transforms input audio into an embedding space) followed by a lightweight classifier. In commercial systems, the encoder is trained centrally using thousands of hours of labeled speech across multiple languages and speakers. However, supporting custom wake words (e.g., \"Hey Jarvis\") or adapting to underrepresented accents and dialects is often infeasible via centralized training due to data scarcity and privacy concerns."
    },
    {
      "footnote_id": "fn-hey-siri-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 818,
      "context": "[^fn-hey-siri-constraints]: **\"Hey Siri\" Technical Reality**: Apple's \"Hey Siri\" system operates under extreme constraints\u2014det...",
      "full_line": "[^fn-hey-siri-constraints]: **\"Hey Siri\" Technical Reality**: Apple's \"Hey Siri\" system operates under extreme constraints\u2014detection must complete within 100ms to feel responsive, while consuming less than 1mW power when listening continuously. The always-on processor monitors audio using a 192KB model running at ~0.5 TOPS. False positive rate must be under 0.001% (less than once per day) while maintaining >95% true positive rate across accents, background noise, and speaking styles. The system processes 16kHz audio in 200ms windows, extracting Mel-frequency features for classification."
    },
    {
      "footnote_id": "fn-tinyml-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 871,
      "context": "In TinyML applications[^fn-tinyml-scale], experience replay has been applied to problems such as gesture recognition, where devices must con...",
      "full_line": "In TinyML applications[^fn-tinyml-scale], experience replay has been applied to problems such as gesture recognition, where devices must continuously improve predictions while observing a small number of events per day. Instead of training directly on the streaming data, the device stores representative feature vectors from recent gestures and uses them to finetune classification boundaries periodically. Similarly, in on-device keyword spotting, replaying past utterances can improve wake-word detection accuracy without the need to transmit audio data off-device."
    },
    {
      "footnote_id": "fn-tinyml-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 873,
      "context": "[^fn-tinyml-scale]: **TinyML Market Reality**: The TinyML market reached $2.4 billion in 2023 and is projected to grow...",
      "full_line": "[^fn-tinyml-scale]: **TinyML Market Reality**: The TinyML market reached $2.4 billion in 2023 and is projected to grow to $23.3 billion by 2030. Over 100 billion microcontrollers ship annually, but fewer than 1% currently support on-device learning due to memory and power constraints. Successful TinyML deployments typically consume <1mW power, use <256KB memory, and cost under $1 per chip. Applications include predictive maintenance (vibration sensors), health monitoring (heart rate variability), and smart agriculture (soil moisture prediction)."
    },
    {
      "footnote_id": "fn-wireless-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1231,
      "context": "...energy budgets\u2014particularly for mobile or embedded devices operating over constrained wireless links[^fn-wireless-constraints]. To address this, a range of techniques have been developed to reduce communication overhead while...",
      "full_line": "One of the principal bottlenecks in federated learning systems is the cost of communication between edge clients and the central server. Transmitting full model weights or gradients after every training round can quickly overwhelm bandwidth and energy budgets\u2014particularly for mobile or embedded devices operating over constrained wireless links[^fn-wireless-constraints]. To address this, a range of techniques have been developed to reduce communication overhead while preserving learning efficacy."
    },
    {
      "footnote_id": "fn-wireless-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1233,
      "context": "[^fn-wireless-constraints]: **Wireless Communication Reality**: Mobile devices face severe bandwidth and energy constraints fo...",
      "full_line": "[^fn-wireless-constraints]: **Wireless Communication Reality**: Mobile devices face severe bandwidth and energy constraints for federated learning. LTE uploads average 5-10 Mbps versus 50+ Mbps downloads, creating asymmetric bottlenecks. Transmitting a 50MB model update consumes ~100mAh battery (2-3% of typical capacity) and takes 40-80 seconds. WiFi improves throughput but isn't always available. Low-power devices using LoRaWAN or NB-IoT face even harsher limits\u2014LoRaWAN maxes at 50kbps with 1% duty cycle restrictions, making frequent updates impractical without aggressive compression."
    },
    {
      "footnote_id": "fn-arm-cortex-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1381,
      "context": "...devices differ in terms of memory capacity, processor architecture (e.g., ARM Cortex-M vs. A-series)[^fn-arm-cortex-spectrum], instruction set support (e.g., availability of SIMD or floating-point units), and the presence or...",
      "full_line": "At the hardware level, devices differ in terms of memory capacity, processor architecture (e.g., ARM Cortex-M vs. A-series)[^fn-arm-cortex-spectrum], instruction set support (e.g., availability of SIMD or floating-point units), and the presence or absence of AI accelerators. Some clients may possess powerful NPUs capable of running small training loops, while others may rely solely on low-frequency CPUs with minimal RAM. These differences affect the feasible size of models, the choice of training algorithm, and the frequency of updates."
    },
    {
      "footnote_id": "fn-arm-cortex-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1383,
      "context": "[^fn-arm-cortex-spectrum]: **ARM Cortex Architecture Spectrum**: The ARM Cortex family spans 6 orders of magnitude in capabil...",
      "full_line": "[^fn-arm-cortex-spectrum]: **ARM Cortex Architecture Spectrum**: The ARM Cortex family spans 6 orders of magnitude in capabilities. Cortex-M0+ (IoT sensors) runs at 48MHz with 32KB RAM and no floating-point, consuming ~10\u00b5W. Cortex-M7 (embedded systems) reaches 400MHz with 1MB RAM and single-precision FPU, consuming ~100mW. Cortex-A78 (smartphones) delivers 3GHz performance with multi-core processing, NEON SIMD, and advanced branch prediction, consuming 1-5W. This diversity means federated learning must adapt algorithms dynamically\u2014quantized inference on M0+, lightweight training on M7, and full backpropagation on A78."
    },
    {
      "footnote_id": "fn-microcontroller-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1429,
      "context": "...rming a single epoch of adaptation on a microcontroller-class device can consume several millijoules[^fn-microcontroller-power]\u2014an appreciable fraction of the energy budget for a duty-cycled system operating on harvested power....",
      "full_line": "Energy consumption compounds this problem. Adaptation typically involves sustained computation over multiple input samples, which taxes battery-powered systems and may lead to rapid energy depletion. For instance, performing a single epoch of adaptation on a microcontroller-class device can consume several millijoules[^fn-microcontroller-power]\u2014an appreciable fraction of the energy budget for a duty-cycled system operating on harvested power. This necessitates careful scheduling, such that learning occurs only during idle periods, when energy reserves are high and user latency constraints are relaxed."
    },
    {
      "footnote_id": "fn-microcontroller-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1431,
      "context": "[^fn-microcontroller-power]: **Microcontroller Power Budget Reality**: A typical microcontroller consuming 100mW during trainin...",
      "full_line": "[^fn-microcontroller-power]: **Microcontroller Power Budget Reality**: A typical microcontroller consuming 100mW during training exhausts 3.6 joules per hour, equivalent to a 1000mAh battery in 2.8 hours. Energy harvesting systems collect only 10-100mW continuously (solar panels in indoor light), making sustained training impossible. Real deployments use duty cycling: train for 10 seconds every hour, consuming ~1 joule total. This constrains training to 100-1000 gradient steps maximum, requiring extremely efficient algorithms and careful energy budgeting between sensing, computation, and communication."
    },
    {
      "footnote_id": "fn-mlops-emergence",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 37,
      "context": "Machine Learning Operations (MLOps)[^fn-mlops-emergence] is a systematic discipline that integrates machine learning, data science, and software engineering...",
      "full_line": "Machine Learning Operations (MLOps)[^fn-mlops-emergence] is a systematic discipline that integrates machine learning, data science, and software engineering practices to automate and streamline the end-to-end ML lifecycle. This lifecycle encompasses data preparation, model training, evaluation, deployment, monitoring, and ongoing maintenance. The goal of MLOps is to ensure that ML models are developed, deployed, and operated reliably, efficiently, and at scale."
    },
    {
      "footnote_id": "fn-mlops-emergence",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 41,
      "context": "[^fn-mlops-emergence]: **MLOps Emergence**: The term \"MLOps\" was first coined around 2018 by D. Sculley and colleagues at...",
      "full_line": "[^fn-mlops-emergence]: **MLOps Emergence**: The term \"MLOps\" was first coined around 2018 by D. Sculley and colleagues at Google, who published the influential paper \"Hidden Technical Debt in Machine Learning Systems.\" The discipline emerged as organizations like Netflix, Uber, and Airbnb faced the \"last mile\" problem\u201490% of ML models never made it to production due to operational challenges."
    },
    {
      "footnote_id": "fn-devops-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 43,
      "context": "...collaboration across traditionally siloed roles, including data scientists, ML engineers, and DevOps[^fn-devops-origins] professionals, by defining interfaces and responsibilities. MLOps also supports continuous integrat...",
      "full_line": "By establishing standard protocols, tools, and workflows, MLOps enables models developed during experimentation to transition seamlessly into production. It promotes collaboration across traditionally siloed roles, including data scientists, ML engineers, and DevOps[^fn-devops-origins] professionals, by defining interfaces and responsibilities. MLOps also supports continuous integration and delivery for ML, allowing teams to retrain, validate, and redeploy models frequently in response to new data or system conditions."
    },
    {
      "footnote_id": "fn-mlops-business-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 49,
      "context": "...LOps to increase team productivity, reduce time-to-market, and improve the reliability of ML systems[^fn-mlops-business-impact]. The adoption of MLOps not only enhances model performance and robustness but also enables a sustai...",
      "full_line": "Organizations across sectors are adopting MLOps to increase team productivity, reduce time-to-market, and improve the reliability of ML systems[^fn-mlops-business-impact]. The adoption of MLOps not only enhances model performance and robustness but also enables a sustainable approach to managing ML systems at scale."
    },
    {
      "footnote_id": "fn-mlops-business-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 51,
      "context": "[^fn-mlops-business-impact]: **MLOps Business Impact**: Companies implementing mature MLOps practices report 3-10x faster model...",
      "full_line": "[^fn-mlops-business-impact]: **MLOps Business Impact**: Companies implementing mature MLOps practices report 3-10x faster model deployment (from months to weeks), 50-80% reduction in model debugging time, and 20-40% improvement in model reliability. Gartner estimates MLOps can reduce operational ML costs by 30-50% while increasing model success rates from 20% to 85%."
    },
    {
      "footnote_id": "fn-devops-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 61,
      "context": "[^fn-devops-origins]: **DevOps Origins**: The \"wall of confusion\" between development and operations teams was so notori...",
      "full_line": "[^fn-devops-origins]: **DevOps Origins**: The \"wall of confusion\" between development and operations teams was so notorious that Patrick Debois called his 2009 conference \"DevOpsDays\" specifically to bridge this gap. The movement emerged from the frustrations of the \"throw it over the wall\" mentality where developers built software in isolation from operations teams who had to deploy and maintain it."
    },
    {
      "footnote_id": "fn-jenkins-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 67,
      "context": "Tools such as [Jenkins](https://www.jenkins.io/)[^fn-jenkins-history], [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/)[^fn-kubernetes-birth]...",
      "full_line": "Tools such as [Jenkins](https://www.jenkins.io/)[^fn-jenkins-history], [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/)[^fn-kubernetes-birth] became foundational to implementing continuous integration and continuous delivery (CI/CD) practices."
    },
    {
      "footnote_id": "fn-kubernetes-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 67,
      "context": ".../)[^fn-jenkins-history], [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/)[^fn-kubernetes-birth] became foundational to implementing continuous integration and continuous delivery (CI/CD) practice...",
      "full_line": "Tools such as [Jenkins](https://www.jenkins.io/)[^fn-jenkins-history], [Docker](https://www.docker.com/), and [Kubernetes](https://kubernetes.io/)[^fn-kubernetes-birth] became foundational to implementing continuous integration and continuous delivery (CI/CD) practices."
    },
    {
      "footnote_id": "fn-jenkins-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 69,
      "context": "[^fn-jenkins-history]: **Jenkins Origins**: Originally called \"Hudson,\" Jenkins was created by Kohsuke Kawaguchi at Sun M...",
      "full_line": "[^fn-jenkins-history]: **Jenkins Origins**: Originally called \"Hudson,\" Jenkins was created by Kohsuke Kawaguchi at Sun Microsystems in 2004 to automate his own tedious testing processes. The name change to \"Jenkins\" came in 2011 after a trademark dispute, named after the devoted butler from P.G. Wodehouse's stories."
    },
    {
      "footnote_id": "fn-kubernetes-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 71,
      "context": "[^fn-kubernetes-birth]: **Kubernetes Origins**: Greek for \"helmsman,\" Kubernetes emerged from Google's internal Borg syste...",
      "full_line": "[^fn-kubernetes-birth]: **Kubernetes Origins**: Greek for \"helmsman,\" Kubernetes emerged from Google's internal Borg system that managed billions of containers across their data centers. Google open-sourced it in 2014, realizing that their competitive advantage wasn't the orchestration system itself, but how they used it to run services at planetary scale."
    },
    {
      "footnote_id": "fn-reproducibility-crisis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 87,
      "context": "Equally critical is reproducibility[^fn-reproducibility-crisis]\u2014ML workflows often lack standardized mechanisms to track code, datasets, configurations, and enviro...",
      "full_line": "Equally critical is reproducibility[^fn-reproducibility-crisis]\u2014ML workflows often lack standardized mechanisms to track code, datasets, configurations, and environments, making it difficult to reproduce past experiments [@schelter2018automating]. The lack of explainability in complex models has further driven demand for tools that increase model transparency and interpretability, particularly in regulated domains."
    },
    {
      "footnote_id": "fn-reproducibility-crisis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 129,
      "context": "[^fn-reproducibility-crisis]: **ML Reproducibility Crisis**: A 2019 study found that only 54% of AI research papers could be rep...",
      "full_line": "[^fn-reproducibility-crisis]: **ML Reproducibility Crisis**: A 2019 study found that only 54% of AI research papers could be reproduced, compared to 70% in other scientific fields. This crisis led to initiatives like Papers with Code and the requirement for code submission at major ML conferences."
    },
    {
      "footnote_id": "fn-dvc-story",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 133,
      "context": "MLOps introduces specialized practices such as [data versioning](https://dvc.org/)[^fn-dvc-story], [model versioning](https://dvc.org/), and [model monitoring](https://www.fiddler.ai/) that go beyo...",
      "full_line": "MLOps introduces specialized practices such as [data versioning](https://dvc.org/)[^fn-dvc-story], [model versioning](https://dvc.org/), and [model monitoring](https://www.fiddler.ai/) that go beyond the scope of DevOps. It emphasizes scalable experimentation, reproducibility, governance, and responsiveness to evolving data conditions. @tbl-mlops summarizes key similarities and differences between DevOps and MLOps:"
    },
    {
      "footnote_id": "fn-dvc-story",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 157,
      "context": "[^fn-dvc-story]: **DVC Creation Story**: Data Version Control was born from the frustration of Dmitry Petrov, who s...",
      "full_line": "[^fn-dvc-story]: **DVC Creation Story**: Data Version Control was born from the frustration of Dmitry Petrov, who spent weeks trying to reproduce an experiment only to discover the training data had been quietly updated. He created DVC in 2017 to bring Git-like versioning to data science, solving what he called \"the biggest unsolved problem in machine learning.\""
    },
    {
      "footnote_id": "fn-feature-store-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 262,
      "context": "Feature stores[^fn-feature-store-scale] provide an abstraction layer between data engineering and machine learning. Their primary purpose i...",
      "full_line": "Feature stores[^fn-feature-store-scale] provide an abstraction layer between data engineering and machine learning. Their primary purpose is to enable consistent, reliable access to engineered features across training and inference workflows. In conventional pipelines, feature engineering logic may be duplicated, manually reimplemented, or diverge across environments. This introduces risks of training-serving skew[^fn-training-serving-skew], data leakage, and model drift."
    },
    {
      "footnote_id": "fn-training-serving-skew",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 262,
      "context": "...nually reimplemented, or diverge across environments. This introduces risks of training-serving skew[^fn-training-serving-skew], data leakage, and model drift.",
      "full_line": "Feature stores[^fn-feature-store-scale] provide an abstraction layer between data engineering and machine learning. Their primary purpose is to enable consistent, reliable access to engineered features across training and inference workflows. In conventional pipelines, feature engineering logic may be duplicated, manually reimplemented, or diverge across environments. This introduces risks of training-serving skew[^fn-training-serving-skew], data leakage, and model drift."
    },
    {
      "footnote_id": "fn-feature-store-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 264,
      "context": "[^fn-feature-store-scale]: **Feature Store Scale**: Uber's Michelangelo feature store serves 10+ million features per second...",
      "full_line": "[^fn-feature-store-scale]: **Feature Store Scale**: Uber's Michelangelo feature store serves 10+ million features per second with P99 latency under 10ms, storing 200+ petabytes of feature data. Airbnb's feature store supports 1,000+ ML models with automated feature validation preventing 85% of potential training-serving skew issues."
    },
    {
      "footnote_id": "fn-training-serving-skew",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 266,
      "context": "[^fn-training-serving-skew]: **Training-Serving Skew Impact**: Studies show training-serving skew causes 5-15% accuracy degrada...",
      "full_line": "[^fn-training-serving-skew]: **Training-Serving Skew Impact**: Studies show training-serving skew causes 5-15% accuracy degradation in production models. Google reported that fixing skew issues improved ad click prediction accuracy by 8%, translating to millions in additional revenue annually."
    },
    {
      "footnote_id": "fn-github-actions-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 300,
      "context": "...s.io/), [CircleCI](https://circleci.com/), and [GitHub Actions](https://github.com/features/actions)[^fn-github-actions-ml] are commonly used to manage version control events and execution logic. These tools are frequently...",
      "full_line": "A wide range of tools is available for implementing ML-focused CI/CD workflows. General-purpose CI/CD orchestrators such as [Jenkins](https://www.jenkins.io/), [CircleCI](https://circleci.com/), and [GitHub Actions](https://github.com/features/actions)[^fn-github-actions-ml] are commonly used to manage version control events and execution logic. These tools are frequently integrated with domain-specific platforms such as [Kubeflow](https://www.kubeflow.org/)[^fn-kubeflow-scale], [Metaflow](https://metaflow.org/), and [Prefect](https://www.prefect.io/), which offer higher-level abstractions for managing ML tasks and workflows."
    },
    {
      "footnote_id": "fn-kubeflow-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 300,
      "context": "...e frequently integrated with domain-specific platforms such as [Kubeflow](https://www.kubeflow.org/)[^fn-kubeflow-scale], [Metaflow](https://metaflow.org/), and [Prefect](https://www.prefect.io/), which offer higher-leve...",
      "full_line": "A wide range of tools is available for implementing ML-focused CI/CD workflows. General-purpose CI/CD orchestrators such as [Jenkins](https://www.jenkins.io/), [CircleCI](https://circleci.com/), and [GitHub Actions](https://github.com/features/actions)[^fn-github-actions-ml] are commonly used to manage version control events and execution logic. These tools are frequently integrated with domain-specific platforms such as [Kubeflow](https://www.kubeflow.org/)[^fn-kubeflow-scale], [Metaflow](https://metaflow.org/), and [Prefect](https://www.prefect.io/), which offer higher-level abstractions for managing ML tasks and workflows."
    },
    {
      "footnote_id": "fn-github-actions-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 302,
      "context": "[^fn-github-actions-ml]: **GitHub Actions for ML**: Over 60% of ML teams now use GitHub Actions for CI/CD, with typical ML...",
      "full_line": "[^fn-github-actions-ml]: **GitHub Actions for ML**: Over 60% of ML teams now use GitHub Actions for CI/CD, with typical ML pipelines taking 15-45 minutes to run (vs. 2-5 minutes for traditional software). Netflix runs 10,000+ ML pipeline executions weekly through GitHub Actions, with 95% success rate on first run."
    },
    {
      "footnote_id": "fn-kubeflow-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 304,
      "context": "[^fn-kubeflow-scale]: **Kubeflow Production Usage**: Google's internal Kubeflow deployment runs 500,000+ ML jobs monthly...",
      "full_line": "[^fn-kubeflow-scale]: **Kubeflow Production Usage**: Google's internal Kubeflow deployment runs 500,000+ ML jobs monthly across 50+ clusters, with automatic resource scaling reducing training costs by 40%. Companies like Spotify use Kubeflow to orchestrate 1,000+ concurrent training jobs with fault tolerance."
    },
    {
      "footnote_id": "fn-cloud-ml-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 528,
      "context": "...at provision high-performance computing resources, which include GPU and TPU accelerators, on demand[^fn-cloud-ml-costs]. Depending on the platform, teams may construct their own training workflows or rely on fully manag...",
      "full_line": "The increasing availability of cloud-based infrastructure has further expanded the reach of model training. Cloud providers offer managed services that provision high-performance computing resources, which include GPU and TPU accelerators, on demand[^fn-cloud-ml-costs]. Depending on the platform, teams may construct their own training workflows or rely on fully managed services such as [Vertex AI Fine Tuning](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models), which support automated adaptation of foundation models to new tasks. Nonetheless, hardware availability, regional access restrictions, and cost constraints remain important considerations when designing cloud-based training systems."
    },
    {
      "footnote_id": "fn-cloud-ml-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 530,
      "context": "[^fn-cloud-ml-costs]: **Cloud ML Training Economics**: Training GPT-3 costs approximately $4.6 million on AWS, while fin...",
      "full_line": "[^fn-cloud-ml-costs]: **Cloud ML Training Economics**: Training GPT-3 costs approximately $4.6 million on AWS, while fine-tuning typically costs $100-$10,000. Google's TPU v4 pods can reduce training costs by 2-5x compared to equivalent GPU clusters, with some organizations reporting 60-80% cost savings through spot instances and preemptible training."
    },
    {
      "footnote_id": "fn-tensorflow-serving",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 648,
      "context": "...works have emerged. Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[^fn-tensorflow-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-...",
      "full_line": "To address these challenges, production-grade serving frameworks have emerged. Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[^fn-tensorflow-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized mechanisms for deploying, versioning, and scaling machine learning models across heterogeneous infrastructure. These frameworks abstract many of the lower-level concerns, allowing teams to focus on system behavior, integration, and performance targets."
    },
    {
      "footnote_id": "fn-triton-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 648,
      "context": "...low-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized me...",
      "full_line": "To address these challenges, production-grade serving frameworks have emerged. Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[^fn-tensorflow-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized mechanisms for deploying, versioning, and scaling machine learning models across heterogeneous infrastructure. These frameworks abstract many of the lower-level concerns, allowing teams to focus on system behavior, integration, and performance targets."
    },
    {
      "footnote_id": "fn-kserve-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 648,
      "context": "...on-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized mechanisms for deploying, versioning, and scaling machine learning models acro...",
      "full_line": "To address these challenges, production-grade serving frameworks have emerged. Tools such as [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)[^fn-tensorflow-serving], [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server)[^fn-triton-performance], and [KServe](https://kserve.github.io/website/latest/)[^fn-kserve-scaling] provide standardized mechanisms for deploying, versioning, and scaling machine learning models across heterogeneous infrastructure. These frameworks abstract many of the lower-level concerns, allowing teams to focus on system behavior, integration, and performance targets."
    },
    {
      "footnote_id": "fn-tensorflow-serving",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 650,
      "context": "[^fn-tensorflow-serving]: **TensorFlow Serving**: Google's production-grade ML serving system handles over 100,000 queries p...",
      "full_line": "[^fn-tensorflow-serving]: **TensorFlow Serving**: Google's production-grade ML serving system handles over 100,000 queries per second per machine with <10ms latency for most models. Originally built to serve YouTube's recommendation system, processing over 1 billion hours of video watched daily."
    },
    {
      "footnote_id": "fn-triton-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 652,
      "context": "[^fn-triton-performance]: **NVIDIA Triton Inference Server**: Can achieve up to 40,000 inferences per second on a single A10...",
      "full_line": "[^fn-triton-performance]: **NVIDIA Triton Inference Server**: Can achieve up to 40,000 inferences per second on a single A100 GPU for BERT models, with dynamic batching reducing latency by up to 10x compared to naive serving approaches. Supports concurrent execution of up to 100 different model types."
    },
    {
      "footnote_id": "fn-kserve-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 654,
      "context": "[^fn-kserve-scaling]: **KServe (formerly KFServing)**: Kubernetes-native serving framework that can autoscale from zero...",
      "full_line": "[^fn-kserve-scaling]: **KServe (formerly KFServing)**: Kubernetes-native serving framework that can autoscale from zero to thousands of replicas in under 30 seconds. Used by companies like Bloomberg to serve over 10,000 models simultaneously with 99.9% uptime SLA."
    },
    {
      "footnote_id": "fn-sla-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 662,
      "context": "...roughput. Serving systems are therefore constructed to meet specific Service Level Agreements (SLAs)[^fn-sla-examples] and Service Level Objectives (SLOs)[^fn-slo-reality], which quantify acceptable performance boundar...",
      "full_line": "Each of these approaches introduces different constraints in terms of availability, responsiveness, and throughput. Serving systems are therefore constructed to meet specific Service Level Agreements (SLAs)[^fn-sla-examples] and Service Level Objectives (SLOs)[^fn-slo-reality], which quantify acceptable performance boundaries along dimensions such as latency, error rates, and uptime. Achieving these goals requires a range of optimizations in request handling, scheduling, and resource allocation."
    },
    {
      "footnote_id": "fn-slo-reality",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 662,
      "context": "...meet specific Service Level Agreements (SLAs)[^fn-sla-examples] and Service Level Objectives (SLOs)[^fn-slo-reality], which quantify acceptable performance boundaries along dimensions such as latency, error rates, an...",
      "full_line": "Each of these approaches introduces different constraints in terms of availability, responsiveness, and throughput. Serving systems are therefore constructed to meet specific Service Level Agreements (SLAs)[^fn-sla-examples] and Service Level Objectives (SLOs)[^fn-slo-reality], which quantify acceptable performance boundaries along dimensions such as latency, error rates, and uptime. Achieving these goals requires a range of optimizations in request handling, scheduling, and resource allocation."
    },
    {
      "footnote_id": "fn-sla-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 664,
      "context": "[^fn-sla-examples]: **Service Level Agreements (SLAs)**: Production ML systems typically target 99.9% uptime (8.77 hou...",
      "full_line": "[^fn-sla-examples]: **Service Level Agreements (SLAs)**: Production ML systems typically target 99.9% uptime (8.77 hours downtime/year) for critical services, with penalties of 10-25% monthly service credits for each 0.1% below target. Google's Cloud AI Platform promises 99.95% uptime with automatic failover in <30 seconds."
    },
    {
      "footnote_id": "fn-slo-reality",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 666,
      "context": "[^fn-slo-reality]: **Service Level Objectives (SLOs)**: Real-world ML serving SLOs often specify P95 latency <100ms f...",
      "full_line": "[^fn-slo-reality]: **Service Level Objectives (SLOs)**: Real-world ML serving SLOs often specify P95 latency <100ms for online inference, P99 <500ms, and error rates <0.1%. Netflix's recommendation system maintains P99 latency under 150ms while serving 200+ million users, processing 3+ billion hours of content monthly."
    },
    {
      "footnote_id": "fn-ml-autoscaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 715,
      "context": "...perparameter tuning and surges in prediction traffic, teams rely on cloud elasticity and autoscaling[^fn-ml-autoscaling]. Cloud platforms support on-demand provisioning and horizontal scaling of infrastructure resources....",
      "full_line": "To handle changes in workload intensity, including spikes during hyperparameter tuning and surges in prediction traffic, teams rely on cloud elasticity and autoscaling[^fn-ml-autoscaling]. Cloud platforms support on-demand provisioning and horizontal scaling of infrastructure resources. [Autoscaling mechanisms](https://aws.amazon.com/autoscaling/) automatically adjust compute capacity based on usage metrics, enabling teams to optimize for both performance and cost-efficiency."
    },
    {
      "footnote_id": "fn-ml-autoscaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 717,
      "context": "[^fn-ml-autoscaling]: **ML Autoscaling at Scale**: Kubernetes-based ML serving can scale from 1 to 1,000+ replicas in un...",
      "full_line": "[^fn-ml-autoscaling]: **ML Autoscaling at Scale**: Kubernetes-based ML serving can scale from 1 to 1,000+ replicas in under 60 seconds. Uber's ML platform automatically scales 2,000+ models daily, reducing infrastructure costs by 35-50% through intelligent resource allocation and cold-start optimization achieving 99.95% availability."
    },
    {
      "footnote_id": "fn-drift-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 731,
      "context": "One of the primary risks in production ML systems is model drift[^fn-drift-detection]\u2014a gradual decline in model performance as the input data distribution or the relationship between i...",
      "full_line": "One of the primary risks in production ML systems is model drift[^fn-drift-detection]\u2014a gradual decline in model performance as the input data distribution or the relationship between inputs and outputs changes. Drift manifests in two main forms:"
    },
    {
      "footnote_id": "fn-drift-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 733,
      "context": "[^fn-drift-detection]: **Model Drift Detection**: Production systems typically trigger alerts when accuracy drops >5% ove...",
      "full_line": "[^fn-drift-detection]: **Model Drift Detection**: Production systems typically trigger alerts when accuracy drops >5% over 24 hours or >10% over a week. Advanced systems like those at Spotify detect drift within 2-4 hours using statistical tests, with 85% of drift incidents caught before user impact."
    },
    {
      "footnote_id": "fn-covid-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 735,
      "context": "- Concept drift[^fn-covid-impact] occurs when the underlying relationship between features and targets evolves. For example, during t...",
      "full_line": "- Concept drift[^fn-covid-impact] occurs when the underlying relationship between features and targets evolves. For example, during the COVID-19 pandemic, purchasing behavior shifted dramatically, invalidating many previously accurate recommendation models."
    },
    {
      "footnote_id": "fn-covid-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 737,
      "context": "[^fn-covid-impact]: **COVID-19 ML Impact**: E-commerce recommendation systems saw accuracy drops of 15-40% within week...",
      "full_line": "[^fn-covid-impact]: **COVID-19 ML Impact**: E-commerce recommendation systems saw accuracy drops of 15-40% within weeks of lockdowns beginning in March 2020. Amazon reported having to retrain over 1,000 models, while Netflix saw a 25% increase in viewing time that broke their capacity planning models."
    },
    {
      "footnote_id": "fn-prometheus-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 740,
      "context": "...ant and responsive under varying load conditions. Tools such as [Prometheus](https://prometheus.io/)[^fn-prometheus-scale], [Grafana](https://grafana.com/), and [Elastic](https://www.elastic.co/) are widely used to collect...",
      "full_line": "In addition to model-level monitoring, infrastructure-level monitoring tracks indicators such as CPU and GPU utilization, memory and disk consumption, network latency, and service availability. These signals help ensure that the system remains performant and responsive under varying load conditions. Tools such as [Prometheus](https://prometheus.io/)[^fn-prometheus-scale], [Grafana](https://grafana.com/), and [Elastic](https://www.elastic.co/) are widely used to collect, aggregate, and visualize operational metrics. These tools often integrate into dashboards that offer real-time and historical views of system behavior."
    },
    {
      "footnote_id": "fn-prometheus-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 742,
      "context": "[^fn-prometheus-scale]: **Prometheus at Scale**: Can ingest 1+ million samples per second per instance, with some deployme...",
      "full_line": "[^fn-prometheus-scale]: **Prometheus at Scale**: Can ingest 1+ million samples per second per instance, with some deployments monitoring 100,000+ machines. DigitalOcean's Prometheus setup stores 2+ years of metrics data across 40,000+ time series, with query response times under 100ms for 95% of requests."
    },
    {
      "footnote_id": "fn-alerting-thresholds",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 744,
      "context": "...tive alerting mechanisms are configured to notify teams when anomalies or threshold violations occur[^fn-alerting-thresholds]. For example, a sustained drop in model accuracy may trigger an alert to investigate potential drif...",
      "full_line": "Proactive alerting mechanisms are configured to notify teams when anomalies or threshold violations occur[^fn-alerting-thresholds]. For example, a sustained drop in model accuracy may trigger an alert to investigate potential drift, prompting retraining with updated data. Similarly, infrastructure alerts can signal memory saturation or degraded network performance, allowing engineers to take corrective action before failures propagate."
    },
    {
      "footnote_id": "fn-alerting-thresholds",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 746,
      "context": "[^fn-alerting-thresholds]: **Production Alert Thresholds**: Typical ML production alerts fire when GPU memory >90%, CPU >85%...",
      "full_line": "[^fn-alerting-thresholds]: **Production Alert Thresholds**: Typical ML production alerts fire when GPU memory >90%, CPU >85% for >5 minutes, P99 latency >2x normal for >10 minutes, or error rates >1% for >60 seconds. High-frequency trading firms use microsecond-level alerts, while batch processing systems may use hour-long windows."
    },
    {
      "footnote_id": "fn-shap-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 758,
      "context": "...nsparency and explainability. For example, methods such as [SHAP](https://github.com/slundberg/shap)[^fn-shap-adoption] and [LIME](https://github.com/marcotcr/lime) offer post hoc explanations of model predictions by id...",
      "full_line": "Governance begins during the model development phase, where teams implement techniques to increase transparency and explainability. For example, methods such as [SHAP](https://github.com/slundberg/shap)[^fn-shap-adoption] and [LIME](https://github.com/marcotcr/lime) offer post hoc explanations of model predictions by identifying which input features were most influential in a particular decision. These techniques allow auditors, developers, and non-technical stakeholders to better understand how and why a model behaves the way it does."
    },
    {
      "footnote_id": "fn-shap-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 760,
      "context": "[^fn-shap-adoption]: **SHAP in Production**: SHAP explanations add 10-500ms latency per prediction depending on model c...",
      "full_line": "[^fn-shap-adoption]: **SHAP in Production**: SHAP explanations add 10-500ms latency per prediction depending on model complexity, making them costly for real-time serving. However, 40% of enterprise ML teams now use SHAP in production, with Microsoft reporting SHAP helped identify $2M in potential bias-related legal exposure in their hiring models."
    },
    {
      "footnote_id": "fn-tech-debt-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 790,
      "context": "Originally proposed in software engineering in the 1990s[^fn-tech-debt-origin], the technical debt metaphor compares shortcuts in implementation to financial debt: it may enable...",
      "full_line": "Originally proposed in software engineering in the 1990s[^fn-tech-debt-origin], the technical debt metaphor compares shortcuts in implementation to financial debt: it may enable short-term velocity, but requires ongoing interest payments in the form of maintenance, refactoring, and systemic risk. While some debt is strategic and manageable, uncontrolled technical debt can inhibit flexibility, slow iteration, and introduce brittleness into production systems."
    },
    {
      "footnote_id": "fn-tech-debt-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 792,
      "context": "[^fn-tech-debt-origin]: **Technical Debt Origins**: Ward Cunningham coined the term in 1992, comparing rushed coding decis...",
      "full_line": "[^fn-tech-debt-origin]: **Technical Debt Origins**: Ward Cunningham coined the term in 1992, comparing rushed coding decisions to financial debt: \"A little debt speeds development so long as it is paid back promptly with a rewrite.\" He later regretted the metaphor became an excuse for bad code rather than a tool for communicating tradeoffs."
    },
    {
      "footnote_id": "fn-youtube-engagement",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1025,
      "context": "...s recommendation engine has faced repeated criticism for promoting sensational or polarizing content[^fn-youtube-engagement]. A large part of this stems from feedback loop debt: recommendations influence user behavior, which...",
      "full_line": "YouTube's recommendation engine has faced repeated criticism for promoting sensational or polarizing content[^fn-youtube-engagement]. A large part of this stems from feedback loop debt: recommendations influence user behavior, which in turn becomes training data. Over time, this led to unintended content amplification. Mitigating this required substantial architectural overhauls, including cohort-based evaluation, delayed labeling, and more explicit disentanglement between engagement metrics and ranking logic."
    },
    {
      "footnote_id": "fn-youtube-engagement",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1027,
      "context": "[^fn-youtube-engagement]: **YouTube Recommendation Impact**: The recommendation system drives 70% of watch time on the platf...",
      "full_line": "[^fn-youtube-engagement]: **YouTube Recommendation Impact**: The recommendation system drives 70% of watch time on the platform (1+ billion hours daily), but algorithmic changes in 2016 increased average session time by 50% while inadvertently promoting conspiracy content. Fixing these feedback loops required 2+ years of engineering work and new evaluation frameworks."
    },
    {
      "footnote_id": "fn-zillow-losses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1031,
      "context": "...'s home valuation model (Zestimate) faced significant correction cascades during its iBuying venture[^fn-zillow-losses]. When initial valuation errors propagated into purchasing decisions, retroactive corrections trigge...",
      "full_line": "Zillow's home valuation model (Zestimate) faced significant correction cascades during its iBuying venture[^fn-zillow-losses]. When initial valuation errors propagated into purchasing decisions, retroactive corrections triggered systemic instability that required data revalidation, model redesign, and eventually a full system rollback. The company shut down the iBuying arm in 2021, citing model unpredictability and data feedback effects as core challenges."
    },
    {
      "footnote_id": "fn-zillow-losses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1033,
      "context": "[^fn-zillow-losses]: **Zillow iBuying Failure**: Zillow lost $881 million in a single quarter (Q3 2021) due to ML model...",
      "full_line": "[^fn-zillow-losses]: **Zillow iBuying Failure**: Zillow lost $881 million in a single quarter (Q3 2021) due to ML model failures, with the Zestimate algorithm overvaluing homes by an average of 5-7%. The company laid off 2,000+ employees and took a $569 million inventory write-down when shutting down Zillow Offers."
    },
    {
      "footnote_id": "fn-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 54,
      "context": "...eter complexity while preserving inferential capabilities are introduced. Techniques such as pruning[^fn-pruning] and knowledge distillation[^fn-knowledge-distill] are examined for their ability to compress and re...",
      "full_line": "This chapter explores the principles of model optimization from a systems perspective. @fig-3-sections illustrates the three distinct layers of the optimization stack discussed in the chapter. At the highest level, methodologies aimed at reducing model parameter complexity while preserving inferential capabilities are introduced. Techniques such as pruning[^fn-pruning] and knowledge distillation[^fn-knowledge-distill] are examined for their ability to compress and refine models, thereby enhancing model quality and improving system runtime performance."
    },
    {
      "footnote_id": "fn-knowledge-distill",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 54,
      "context": "...tial capabilities are introduced. Techniques such as pruning[^fn-pruning] and knowledge distillation[^fn-knowledge-distill] are examined for their ability to compress and refine models, thereby enhancing model quality and i...",
      "full_line": "This chapter explores the principles of model optimization from a systems perspective. @fig-3-sections illustrates the three distinct layers of the optimization stack discussed in the chapter. At the highest level, methodologies aimed at reducing model parameter complexity while preserving inferential capabilities are introduced. Techniques such as pruning[^fn-pruning] and knowledge distillation[^fn-knowledge-distill] are examined for their ability to compress and refine models, thereby enhancing model quality and improving system runtime performance."
    },
    {
      "footnote_id": "fn-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 56,
      "context": "[^fn-pruning]: **Pruning**: The optimal brain damage algorithm by Yann LeCun (1990) pioneered removing unnecessar...",
      "full_line": "[^fn-pruning]: **Pruning**: The optimal brain damage algorithm by Yann LeCun (1990) pioneered removing unnecessary neural network connections, inspiring modern magnitude-based and structured pruning techniques that can reduce model sizes by 90% with minimal accuracy loss."
    },
    {
      "footnote_id": "fn-knowledge-distill",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 58,
      "context": "[^fn-knowledge-distill]: **Knowledge Distillation**: Geoffrey Hinton et al. (2015) introduced this technique where a smalle...",
      "full_line": "[^fn-knowledge-distill]: **Knowledge Distillation**: Geoffrey Hinton et al. (2015) introduced this technique where a smaller \"student\" network learns from a larger \"teacher\" network's soft outputs, enabling compact models that retain much of the original's performance while being orders of magnitude more efficient."
    },
    {
      "footnote_id": "fn-operator-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 157,
      "context": "...ive pruning and quantization, while a latency-sensitive application may benefit from operator fusion[^fn-operator-fusion] and hardware-aware scheduling.",
      "full_line": "The choice of optimizations is driven by system constraints, which define the practical limitations within which models must operate. A machine learning model deployed in a data center has different constraints from one running on a mobile device or an embedded system. Computational cost, memory usage, inference latency, and energy efficiency all influence which optimizations are most appropriate for a given scenario. A model that is too large for a resource-constrained device may require aggressive pruning and quantization, while a latency-sensitive application may benefit from operator fusion[^fn-operator-fusion] and hardware-aware scheduling."
    },
    {
      "footnote_id": "fn-structured-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 678,
      "context": "While unstructured pruning removes individual weights from a neural network, structured pruning[^fn-structured-pruning] eliminates entire computational units, such as neurons, filters, channels, or layers. This approach...",
      "full_line": "While unstructured pruning removes individual weights from a neural network, structured pruning[^fn-structured-pruning] eliminates entire computational units, such as neurons, filters, channels, or layers. This approach is particularly beneficial for hardware efficiency, as it produces smaller dense models that can be directly mapped to modern machine learning accelerators. Unlike unstructured pruning, which results in sparse weight matrices that require specialized execution kernels to exploit computational benefits, structured pruning leads to more efficient inference on general-purpose hardware by reducing the overall size of the network architecture."
    },
    {
      "footnote_id": "fn-structured-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 682,
      "context": "[^fn-structured-pruning]: **Structured Pruning**: Filter pruning in ResNet-34 achieves 50% FLOP reduction with only 1.0% acc...",
      "full_line": "[^fn-structured-pruning]: **Structured Pruning**: Filter pruning in ResNet-34 achieves 50% FLOP reduction with only 1.0% accuracy loss on CIFAR-10. Channel pruning in MobileNetV2 reduces parameters by 73% while maintaining 96.5% of original accuracy, enabling 3.2x faster inference on ARM processors."
    },
    {
      "footnote_id": "fn-lottery-ticket",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1306,
      "context": "This perspective leads to the Lottery Ticket Hypothesis[^fn-lottery-ticket] (LTH), which challenges conventional pruning workflows by proposing that within large neural networ...",
      "full_line": "This perspective leads to the Lottery Ticket Hypothesis[^fn-lottery-ticket] (LTH), which challenges conventional pruning workflows by proposing that within large neural networks, there exist small, well-initialized subnetworks, referred to as 'winning tickets', that can achieve comparable accuracy to the full model when trained in isolation. Rather than viewing pruning as just a post-training compression step, LTH suggests it can serve as a discovery mechanism to identify these efficient subnetworks early in training."
    },
    {
      "footnote_id": "fn-lottery-ticket",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1308,
      "context": "[^fn-lottery-ticket]: **Lottery Ticket Hypothesis**: Frankle & Carbin (2018) demonstrated that ResNet-18 subnetworks at...",
      "full_line": "[^fn-lottery-ticket]: **Lottery Ticket Hypothesis**: Frankle & Carbin (2018) demonstrated that ResNet-18 subnetworks at 10-20% original size achieve 93.2% accuracy vs. 94.1% for full model on CIFAR-10. BERT-base winning tickets retain 97% performance with 90% fewer parameters, requiring 5-8x less training time to converge."
    },
    {
      "footnote_id": "fn-onnx-deployment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1569,
      "context": "ONNX[^fn-onnx-deployment], an open standard for model representation, does not implement pruning directly but provides export...",
      "full_line": "ONNX[^fn-onnx-deployment], an open standard for model representation, does not implement pruning directly but provides export and compatibility support for pruned models from PyTorch and TensorFlow. Since ONNX is designed to be hardware-agnostic, it allows models that have undergone pruning in different frameworks to be optimized for inference engines such as TensorRT[^fn-tensorrt-optimization], OpenVINO, and EdgeTPU. These inference engines can further leverage structured and dynamic pruning for execution efficiency, particularly on specialized hardware accelerators."
    },
    {
      "footnote_id": "fn-tensorrt-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1569,
      "context": "...ave undergone pruning in different frameworks to be optimized for inference engines such as TensorRT[^fn-tensorrt-optimization], OpenVINO, and EdgeTPU. These inference engines can further leverage structured and dynamic pruning...",
      "full_line": "ONNX[^fn-onnx-deployment], an open standard for model representation, does not implement pruning directly but provides export and compatibility support for pruned models from PyTorch and TensorFlow. Since ONNX is designed to be hardware-agnostic, it allows models that have undergone pruning in different frameworks to be optimized for inference engines such as TensorRT[^fn-tensorrt-optimization], OpenVINO, and EdgeTPU. These inference engines can further leverage structured and dynamic pruning for execution efficiency, particularly on specialized hardware accelerators."
    },
    {
      "footnote_id": "fn-bert-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1597,
      "context": "...has been pruned to reduce inference latency while preserving accuracy [@howard2017mobilenets]. BERT[^fn-bert-compression], a widely used transformer model for natural language processing, has undergone structured pruning...",
      "full_line": "Understanding these trade-offs is essential when deploying pruned models in real-world settings. Several high-profile models have successfully integrated pruning to optimize performance. MobileNet, a lightweight convolutional neural network designed for mobile and embedded applications, has been pruned to reduce inference latency while preserving accuracy [@howard2017mobilenets]. BERT[^fn-bert-compression], a widely used transformer model for natural language processing, has undergone structured pruning of attention heads and intermediate layers to create efficient versions such as DistilBERT[^fn-distilbert-metrics] and TinyBERT, which retain much of the original performance while reducing computational overhead [@sanh2019distilbert]. In computer vision, EfficientNet[^fn-efficientnet-pruning] has been pruned to remove unnecessary filters, optimizing it for deployment in resource-constrained environments [@tan2019efficientnet]."
    },
    {
      "footnote_id": "fn-distilbert-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1597,
      "context": "...d pruning of attention heads and intermediate layers to create efficient versions such as DistilBERT[^fn-distilbert-metrics] and TinyBERT, which retain much of the original performance while reducing computational overhead [...",
      "full_line": "Understanding these trade-offs is essential when deploying pruned models in real-world settings. Several high-profile models have successfully integrated pruning to optimize performance. MobileNet, a lightweight convolutional neural network designed for mobile and embedded applications, has been pruned to reduce inference latency while preserving accuracy [@howard2017mobilenets]. BERT[^fn-bert-compression], a widely used transformer model for natural language processing, has undergone structured pruning of attention heads and intermediate layers to create efficient versions such as DistilBERT[^fn-distilbert-metrics] and TinyBERT, which retain much of the original performance while reducing computational overhead [@sanh2019distilbert]. In computer vision, EfficientNet[^fn-efficientnet-pruning] has been pruned to remove unnecessary filters, optimizing it for deployment in resource-constrained environments [@tan2019efficientnet]."
    },
    {
      "footnote_id": "fn-efficientnet-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1597,
      "context": "...rmance while reducing computational overhead [@sanh2019distilbert]. In computer vision, EfficientNet[^fn-efficientnet-pruning] has been pruned to remove unnecessary filters, optimizing it for deployment in resource-constrained...",
      "full_line": "Understanding these trade-offs is essential when deploying pruned models in real-world settings. Several high-profile models have successfully integrated pruning to optimize performance. MobileNet, a lightweight convolutional neural network designed for mobile and embedded applications, has been pruned to reduce inference latency while preserving accuracy [@howard2017mobilenets]. BERT[^fn-bert-compression], a widely used transformer model for natural language processing, has undergone structured pruning of attention heads and intermediate layers to create efficient versions such as DistilBERT[^fn-distilbert-metrics] and TinyBERT, which retain much of the original performance while reducing computational overhead [@sanh2019distilbert]. In computer vision, EfficientNet[^fn-efficientnet-pruning] has been pruned to remove unnecessary filters, optimizing it for deployment in resource-constrained environments [@tan2019efficientnet]."
    },
    {
      "footnote_id": "fn-bert-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1599,
      "context": "[^fn-bert-compression]: **BERT Compression**: BERT-Base (110M params) can be compressed to 67M params (39% reduction) with...",
      "full_line": "[^fn-bert-compression]: **BERT Compression**: BERT-Base (110M params) can be compressed to 67M params (39% reduction) with only 1.2% GLUE score drop. Attention head pruning removes 144 of 192 heads with minimal impact, while layer pruning reduces 12 layers to 6 layers maintaining 97.8% performance."
    },
    {
      "footnote_id": "fn-distilbert-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1601,
      "context": "[^fn-distilbert-metrics]: **DistilBERT**: Achieves 97% of BERT-Base performance with 40% fewer parameters (66M vs. 110M) and...",
      "full_line": "[^fn-distilbert-metrics]: **DistilBERT**: Achieves 97% of BERT-Base performance with 40% fewer parameters (66M vs. 110M) and 60% faster inference. On SQuAD v1.1, DistilBERT scores 86.9 F1 vs. BERT's 88.5 F1, while reducing memory from 1.35GB to 0.54GB and latency from 85ms to 34ms."
    },
    {
      "footnote_id": "fn-efficientnet-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1603,
      "context": "[^fn-efficientnet-pruning]: **EfficientNet Pruning**: EfficientNet-B0 with 70% structured pruning maintains 75.8% ImageNet acc...",
      "full_line": "[^fn-efficientnet-pruning]: **EfficientNet Pruning**: EfficientNet-B0 with 70% structured pruning maintains 75.8% ImageNet accuracy (vs. 77.1% original) with 2.8x speedup on mobile devices. Channel pruning reduces FLOPs from 390M to 140M while keeping inference under 20ms on Pixel 4."
    },
    {
      "footnote_id": "fn-hardware-aware-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2284,
      "context": "NAS[^fn-hardware-aware-nas] addresses these limitations by automating model design. As illustrated in @fig-nas-flow, instead of...",
      "full_line": "NAS[^fn-hardware-aware-nas] addresses these limitations by automating model design. As illustrated in @fig-nas-flow, instead of manually tuning configurations, NAS systematically explores a large space of architectures to identify those that best balance accuracy, computational cost, memory efficiency, and inference latency. By framing model selection as a structured search problem, NAS reduces reliance on trial and error, allowing architectures to be discovered programmatically rather than heuristically [@zoph2017neural]."
    },
    {
      "footnote_id": "fn-nas-evaluation-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2362,
      "context": "...trics include computational complexity, memory consumption, inference latency, and energy efficiency[^fn-nas-evaluation-metrics]. Computational complexity, often measured in FLOPs, determines the overall resource demands of a mo...",
      "full_line": "The primary evaluation metrics include computational complexity, memory consumption, inference latency, and energy efficiency[^fn-nas-evaluation-metrics]. Computational complexity, often measured in FLOPs, determines the overall resource demands of a model. NAS favors architectures that achieve high accuracy while reducing unnecessary computations. Memory consumption, which includes both parameter count and activation storage, ensures that models fit within hardware constraints. For real-time applications, inference latency is a key factor, with NAS selecting architectures that minimize execution time on specific hardware platforms. Finally, some NAS implementations explicitly optimize for power consumption, ensuring that models are suitable for mobile and edge devices."
    },
    {
      "footnote_id": "fn-fbnet-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2364,
      "context": "For example, FBNet[^fn-fbnet-nas], a NAS-generated architecture optimized for mobile inference, incorporated latency constraints into...",
      "full_line": "For example, FBNet[^fn-fbnet-nas], a NAS-generated architecture optimized for mobile inference, incorporated latency constraints into the search process."
    },
    {
      "footnote_id": "fn-hardware-aware-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2366,
      "context": "[^fn-hardware-aware-nas]: **Hardware-Aware NAS**: MnasNet achieves 78.1% ImageNet accuracy with 315M FLOPs vs. MobileNetV2's...",
      "full_line": "[^fn-hardware-aware-nas]: **Hardware-Aware NAS**: MnasNet achieves 78.1% ImageNet accuracy with 315M FLOPs vs. MobileNetV2's 72.0% with 300M FLOPs. EfficientNet-B0 delivers 77.1% accuracy with 390M FLOPs, 23% better accuracy/FLOP ratio than ResNet-50, enabling 4.9x faster mobile inference."
    },
    {
      "footnote_id": "fn-nas-evaluation-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2368,
      "context": "[^fn-nas-evaluation-metrics]: **NAS Evaluation Metrics**: Multi-objective optimization considers accuracy (top-1/top-5), latency...",
      "full_line": "[^fn-nas-evaluation-metrics]: **NAS Evaluation Metrics**: Multi-objective optimization considers accuracy (top-1/top-5), latency (ms on target hardware), memory (MB activations + parameters), and energy (mJ per inference). Pareto-optimal architectures provide 15-40% better efficiency frontiers than manual designs."
    },
    {
      "footnote_id": "fn-fbnet-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2370,
      "context": "[^fn-fbnet-nas]: **FBNet**: Achieves 74.9% ImageNet accuracy with 375M FLOPs and 23ms latency on Samsung S8, 15% fa...",
      "full_line": "[^fn-fbnet-nas]: **FBNet**: Achieves 74.9% ImageNet accuracy with 375M FLOPs and 23ms latency on Samsung S8, 15% faster than MobileNetV2 with comparable accuracy. The latency-aware search uses device-specific lookup tables for actual hardware performance measurement. Instead of selecting the most accurate model, NAS identified architectures that provided the best balance between accuracy and inference speed [@wu2019fbnet]. Similarly, EfficientNet was discovered through NAS by jointly optimizing for accuracy and computational efficiency, resulting in a model that delivers state-of-the-art performance while reducing FLOPs compared to conventional architectures [@tan2019efficientnet]."
    },
    {
      "footnote_id": "fn-energy-efficiency-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2497,
      "context": "**Energy Costs**: Lower precision reduces computational energy[^fn-energy-efficiency-metrics], illustrating trade-offs in model accuracy. Machine learning systems can optimize efficiency by red...",
      "full_line": "**Energy Costs**: Lower precision reduces computational energy[^fn-energy-efficiency-metrics], illustrating trade-offs in model accuracy. Machine learning systems can optimize efficiency by reducing floating-point operations from 32-bit to 16-bit or even lower for significant savings. Source: IEEE spectrum."
    },
    {
      "footnote_id": "fn-onnx-deployment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2819,
      "context": "[^fn-onnx-deployment]: **ONNX Deployment**: ONNX Runtime achieves 1.3-2.9x speedup over TensorFlow and 1.1-1.7x over PyTo...",
      "full_line": "[^fn-onnx-deployment]: **ONNX Deployment**: ONNX Runtime achieves 1.3-2.9x speedup over TensorFlow and 1.1-1.7x over PyTorch across various models. ResNet-50 inference drops from 7.2ms to 2.8ms on CPU, while BERT-Base reduces from 45ms to 23ms with ONNX Runtime optimizations including graph fusion and memory pooling."
    },
    {
      "footnote_id": "fn-tensorrt-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2821,
      "context": "[^fn-tensorrt-optimization]: **TensorRT Optimization**: NVIDIA TensorRT delivers 5-40x speedup for inference on V100 GPUs. ResN...",
      "full_line": "[^fn-tensorrt-optimization]: **TensorRT Optimization**: NVIDIA TensorRT delivers 5-40x speedup for inference on V100 GPUs. ResNet-50 INT8 inference achieves 1.2ms vs. 4.8ms FP32, while BERT-Large drops from 10.4ms to 2.1ms. Layer fusion reduces kernel launches by 80%, memory bandwidth by 50%."
    },
    {
      "footnote_id": "fn-qat-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3417,
      "context": "A primary advantage of QAT[^fn-qat-performance] is its ability to maintain model accuracy, even under low-precision inference conditions. Incorpora...",
      "full_line": "A primary advantage of QAT[^fn-qat-performance] is its ability to maintain model accuracy, even under low-precision inference conditions. Incorporating quantization during training helps the model to compensate for precision loss, reducing the impact of rounding errors and numerical instability. This is critical for quantization-sensitive models commonly used in NLP, speech recognition, and high-resolution computer vision [@gholami2021survey]."
    },
    {
      "footnote_id": "fn-qat-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3419,
      "context": "[^fn-qat-performance]: **Quantization-Aware Training**: QAT enables INT8 inference with minimal accuracy loss - ResNet-50...",
      "full_line": "[^fn-qat-performance]: **Quantization-Aware Training**: QAT enables INT8 inference with minimal accuracy loss - ResNet-50 maintains 76.1% vs. 76.2% FP32 ImageNet accuracy, while MobileNetV2 achieves 71.8% vs. 72.0%. BERT-Base INT8 retains 99.1% of FP32 performance on GLUE, compared to 96.8% with post-training quantization alone."
    },
    {
      "footnote_id": "fn-memory-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3792,
      "context": "Memory optimization[^fn-memory-optimization] is a fundamental aspect of model efficiency, especially when deploying machine learning models on r...",
      "full_line": "Memory optimization[^fn-memory-optimization] is a fundamental aspect of model efficiency, especially when deploying machine learning models on resource-constrained hardware, such as mobile devices, embedded systems, and edge AI platforms. Inference-based models require memory to store activations, intermediate feature maps, and parameters. If these memory demands exceed the hardware's available resources, the model can experience performance bottlenecks, including increased inference latency and power inefficiencies due to frequent memory accesses. Efficient memory management is crucial to minimize these issues while maintaining accuracy and performance."
    },
    {
      "footnote_id": "fn-activation-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3803,
      "context": "Another useful technique is activation checkpointing[^fn-activation-checkpointing], which is especially beneficial during training. In a typical neural network, backpropagation requi...",
      "full_line": "Another useful technique is activation checkpointing[^fn-activation-checkpointing], which is especially beneficial during training. In a typical neural network, backpropagation requires storing all forward activations for the backward pass. This can lead to a significant memory overhead, especially for large models. Activation checkpointing reduces memory consumption by only storing a subset of activations and recomputing the remaining ones when needed. If an architecture requires storing $A_{\\text{total}}$ activations, the standard backpropagation method requires the full storage:"
    },
    {
      "footnote_id": "fn-sparse-energy-savings",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5479,
      "context": "...reduction in the number of computations and memory accesses directly translates into energy savings[^fn-sparse-energy-savings]. Sparse operations require fewer arithmetic operations and fewer memory fetches, leading to a decre...",
      "full_line": "The reduction in the number of computations and memory accesses directly translates into energy savings[^fn-sparse-energy-savings]. Sparse operations require fewer arithmetic operations and fewer memory fetches, leading to a decrease in the energy consumption required for both training and inference. This energy efficiency is particularly important for applications that run on edge devices, where power constraints are critical."
    },
    {
      "footnote_id": "fn-energy-efficiency-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5481,
      "context": "[^fn-energy-efficiency-metrics]: **Energy Efficiency Metrics**: INT8 quantization reduces energy consumption by 4-8x over FP32. Mob...",
      "full_line": "[^fn-energy-efficiency-metrics]: **Energy Efficiency Metrics**: INT8 quantization reduces energy consumption by 4-8x over FP32. MobileNetV2 INT8 consumes 47mJ vs. 312mJ FP32 per inference on Cortex-A75. ResNet-50 on TPU v4 achieves 0.9 TOPS/Watt vs. 0.3 TOPS/Watt on V100 GPU."
    },
    {
      "footnote_id": "fn-sparse-energy-savings",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5483,
      "context": "[^fn-sparse-energy-savings]: **Sparse Energy Savings**: 90% sparsity in BERT reduces training energy by 2.3x and inference ener...",
      "full_line": "[^fn-sparse-energy-savings]: **Sparse Energy Savings**: 90% sparsity in BERT reduces training energy by 2.3x and inference energy by 4.1x on V100. Structured 2:4 sparsity patterns deliver 1.6x energy savings on A100 GPUs while maintaining 99% of dense model accuracy. Hardware accelerators like TPUs and GPUs are optimized to handle these operations efficiently, making sparse networks not only faster but also more energy-efficient [@Cheng2022]."
    },
    {
      "footnote_id": "fn-batch-size-effects",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5615,
      "context": "...ys a crucial role in determining a model's performance. Parameters such as learning rate, batch size[^fn-batch-size-effects], weight decay, and activation functions must be carefully tuned for stability and efficiency.",
      "full_line": "Beyond architecture design, AutoML also focuses on hyperparameter optimization, which plays a crucial role in determining a model's performance. Parameters such as learning rate, batch size[^fn-batch-size-effects], weight decay, and activation functions must be carefully tuned for stability and efficiency."
    },
    {
      "footnote_id": "fn-memory-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5617,
      "context": "[^fn-memory-optimization]: **Memory Optimization**: DenseNet-121 reduces memory consumption by 50% compared to ResNet-50 thro...",
      "full_line": "[^fn-memory-optimization]: **Memory Optimization**: DenseNet-121 reduces memory consumption by 50% compared to ResNet-50 through feature reuse, requiring only 7.9MB vs. 15.3MB activation memory on ImageNet. MobileNetV3 achieves 73% memory reduction with depth-wise separable convolutions, enabling deployment on 2GB mobile devices."
    },
    {
      "footnote_id": "fn-activation-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5619,
      "context": "[^fn-activation-checkpointing]: **Activation Checkpointing**: Enables training 4x larger models on same hardware by trading comput...",
      "full_line": "[^fn-activation-checkpointing]: **Activation Checkpointing**: Enables training 4x larger models on same hardware by trading computation for memory. BERT-Large training memory drops from 16GB to 4GB with 20% computational overhead, while GPT-3 175B uses checkpointing to fit on 8x A100 GPUs instead of 32x."
    },
    {
      "footnote_id": "fn-batch-size-effects",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5621,
      "context": "[^fn-batch-size-effects]: **Batch Size Effects**: Large batches (512-2048) improve throughput by 2-4x but require gradient a...",
      "full_line": "[^fn-batch-size-effects]: **Batch Size Effects**: Large batches (512-2048) improve throughput by 2-4x but require gradient accumulation for memory constraints. Linear scaling rule maintains convergence: learning rate scales linearly with batch size, enabling ImageNet training in 1 hour with batch size 8192. Instead of relying on trial and error, AutoML frameworks employ systematic search strategies, including Bayesian optimization, evolutionary algorithms, and adaptive heuristics, to efficiently identify the best hyperparameter settings for a given model and dataset [@Bergstra2011]."
    },
    {
      "footnote_id": "fn-xla-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5749,
      "context": "For model representation optimizations like pruning, libraries such as TensorRT, XLA[^fn-xla-compiler], and OpenVINO provide sparsity-aware acceleration through optimized kernels that efficiently handle...",
      "full_line": "For model representation optimizations like pruning, libraries such as TensorRT, XLA[^fn-xla-compiler], and OpenVINO provide sparsity-aware acceleration through optimized kernels that efficiently handle sparse computations. TensorRT specifically supports structured sparsity patterns, allowing models trained with techniques like two-out-of-four structured pruning to run efficiently on NVIDIA GPUs. Similarly, TPUs leverage XLA's sparse matrix optimizations, while FPGAs enable custom sparse execution through frameworks like Vitis AI."
    },
    {
      "footnote_id": "fn-tvm-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5751,
      "context": "...constraints. For models discovered through neural architecture search (NAS), frameworks such as TVM[^fn-tvm-compiler] and TIMM provide compiler support to tune the architectures for various hardware backends.",
      "full_line": "Knowledge distillation benefits from hardware-aware optimizations that help compact student models achieve high inference efficiency. Libraries like TensorRT, OpenVINO, and SNPE optimize distilled models for low-power execution, often combining distillation with quantization or architectural restructuring to meet hardware constraints. For models discovered through neural architecture search (NAS), frameworks such as TVM[^fn-tvm-compiler] and TIMM provide compiler support to tune the architectures for various hardware backends."
    },
    {
      "footnote_id": "fn-xla-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5753,
      "context": "[^fn-xla-compiler]: **XLA (Accelerated Linear Algebra)**: Google's domain-specific compiler achieves 1.15-1.4x speedup...",
      "full_line": "[^fn-xla-compiler]: **XLA (Accelerated Linear Algebra)**: Google's domain-specific compiler achieves 1.15-1.4x speedup on ResNet-50 inference and 1.2-1.7x on BERT-Large training through operator fusion and memory optimization. XLA reduces HBM traffic by 25-40% through aggressive kernel fusion, delivering 15-30% energy savings on TPUs."
    },
    {
      "footnote_id": "fn-tvm-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5755,
      "context": "[^fn-tvm-compiler]: **TVM (Tensor Virtual Machine)**: Apache TVM's auto-tuning delivers 1.2-2.8x speedup over vendor l...",
      "full_line": "[^fn-tvm-compiler]: **TVM (Tensor Virtual Machine)**: Apache TVM's auto-tuning delivers 1.2-2.8x speedup over vendor libraries on ARM CPUs and 1.5-3.2x on mobile GPUs. TVM's graph-level optimizations reduce inference latency by 40-60% on edge devices through operator scheduling and memory planning."
    },
    {
      "footnote_id": "fn-operator-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5763,
      "context": "[^fn-operator-fusion]: **Operator Fusion**: Graph-level optimization that combines multiple operations into single kernel...",
      "full_line": "[^fn-operator-fusion]: **Operator Fusion**: Graph-level optimization that combines multiple operations into single kernels, reducing memory bandwidth by 30-50%. In ResNet-50, fusing Conv+BatchNorm+ReLU operations achieves 1.8x speedup on V100 GPUs, while BERT transformer blocks show 25% latency reduction through attention fusion. Dynamic computation approaches like early exit architectures and conditional computation are supported by custom execution runtimes that optimize control flow."
    },
    {
      "footnote_id": "fn-model-extraction-2016",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 44,
      "context": "...hallenges in ML systems continues to evolve. High-profile incidents such as model extraction attacks[^fn-model-extraction-2016], data leakage from generative models, and hardware-level vulnerabilities have underscored the need...",
      "full_line": "The landscape of security and privacy challenges in ML systems continues to evolve. High-profile incidents such as model extraction attacks[^fn-model-extraction-2016], data leakage from generative models, and hardware-level vulnerabilities have underscored the need for comprehensive and adaptive defenses."
    },
    {
      "footnote_id": "fn-model-extraction-2016",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 46,
      "context": "[^fn-model-extraction-2016]: **Model Extraction Threat**: The vulnerability was first demonstrated in 2016 when researchers sho...",
      "full_line": "[^fn-model-extraction-2016]: **Model Extraction Threat**: The vulnerability was first demonstrated in 2016 when researchers showed they could steal machine learning models through API queries alone. By systematically querying a model and analyzing responses, attackers could recreate proprietary models worth millions in R&D investment\u2014turning public APIs into inadvertent IP leakage channels."
    },
    {
      "footnote_id": "fn-gdpr-penalties",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 93,
      "context": "...tion     | Emphasized in cybersecurity standards       | Central to data protection laws (e.g., GDPR[^fn-gdpr-penalties])   |",
      "full_line": "| Relevance to Regulation     | Emphasized in cybersecurity standards       | Central to data protection laws (e.g., GDPR[^fn-gdpr-penalties])   |"
    },
    {
      "footnote_id": "fn-gdpr-penalties",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 98,
      "context": "[^fn-gdpr-penalties]: **GDPR**: The General Data Protection Regulation, enacted in 2018, imposes fines up to 4% of globa...",
      "full_line": "[^fn-gdpr-penalties]: **GDPR**: The General Data Protection Regulation, enacted in 2018, imposes fines up to 4% of global annual revenue or \u20ac20 million (whichever is higher) for privacy violations. Since implementation, it has generated over \u20ac1.6 billion in fines, with a single 2021 penalty against Amazon reaching \u20ac746 million\u2014demonstrating the regulation's significant financial impact on data-driven businesses."
    },
    {
      "footnote_id": "fn-stuxnet-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 116,
      "context": "...ps://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[^fn-stuxnet-discovery], which targeted industrial control systems used in Iran's Natanz nuclear facility [@farwell2011stux...",
      "full_line": "In 2010, security researchers discovered a highly sophisticated computer worm later named [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[^fn-stuxnet-discovery], which targeted industrial control systems used in Iran's Natanz nuclear facility [@farwell2011stuxnet]. Stuxnet exploited four previously unknown \"[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)\"[^fn-zero-day-term] vulnerabilities in Microsoft Windows, allowing it to spread undetected through both networked and isolated systems."
    },
    {
      "footnote_id": "fn-zero-day-term",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 116,
      "context": "...ploited four previously unknown \"[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)\"[^fn-zero-day-term] vulnerabilities in Microsoft Windows, allowing it to spread undetected through both networked and i...",
      "full_line": "In 2010, security researchers discovered a highly sophisticated computer worm later named [Stuxnet](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/200661/Cyber-Reports-2017-04.pdf)[^fn-stuxnet-discovery], which targeted industrial control systems used in Iran's Natanz nuclear facility [@farwell2011stuxnet]. Stuxnet exploited four previously unknown \"[zero-day](https://en.wikipedia.org/wiki/Zero-day_%28computing%29)\"[^fn-zero-day-term] vulnerabilities in Microsoft Windows, allowing it to spread undetected through both networked and isolated systems."
    },
    {
      "footnote_id": "fn-stuxnet-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 118,
      "context": "[^fn-stuxnet-discovery]: **Stuxnet Discovery**: The malware was first detected by VirusBokNok, a small Belarusian antivirus...",
      "full_line": "[^fn-stuxnet-discovery]: **Stuxnet Discovery**: The malware was first detected by VirusBokNok, a small Belarusian antivirus company, when their client computers began crashing unexpectedly. What seemed like a routine malware investigation turned into one of the most significant cybersecurity discoveries in history\u2014the first confirmed cyberweapon designed to cause physical destruction."
    },
    {
      "footnote_id": "fn-zero-day-term",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 120,
      "context": "[^fn-zero-day-term]: **Zero-Day Etymology**: The term originated in software piracy circles, referring to the \"zero day...",
      "full_line": "[^fn-zero-day-term]: **Zero-Day Etymology**: The term originated in software piracy circles, referring to the \"zero days\" since a program's release when pirated copies appeared. In security, it describes the \"zero days\" defenders have to patch a vulnerability before attackers exploit it\u2014representing the ultimate race between attack and defense."
    },
    {
      "footnote_id": "fn-mirai-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 148,
      "context": "In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[^fn-mirai-scale] emerged as one of the most disruptive distributed denial-of-service (DDoS) attacks in internet hist...",
      "full_line": "In 2016, the [Mirai botnet](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-attack/)[^fn-mirai-scale] emerged as one of the most disruptive distributed denial-of-service (DDoS) attacks in internet history [@antonakakis2017understanding]. The botnet infected thousands of networked devices, including digital cameras, DVRs, and other consumer electronics. These devices, often deployed with factory-default usernames and passwords, were easily compromised by the Mirai malware and enlisted into a large-scale attack network."
    },
    {
      "footnote_id": "fn-mirai-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 150,
      "context": "[^fn-mirai-scale]: **Mirai Botnet Scale**: At its peak, Mirai controlled over 600,000 infected IoT devices, generatin...",
      "full_line": "[^fn-mirai-scale]: **Mirai Botnet Scale**: At its peak, Mirai controlled over 600,000 infected IoT devices, generating 623 Gbps DDoS attacks that took down major internet services including Twitter, Netflix, and Reddit for hours. The attack revealed that IoT devices with default credentials (admin/admin, root/12345) could be weaponized at unprecedented scale."
    },
    {
      "footnote_id": "fn-model-inversion-attack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 269,
      "context": "...tics, and embedded trade secrets. This knowledge can also be used to conduct model inversion attacks[^fn-model-inversion-attack], where an attacker attempts to infer private details about the model's training data [@fredrikson20...",
      "full_line": "The consequences of model theft extend beyond economic loss. Stolen models can be used to extract sensitive information, replicate proprietary algorithms, or enable further attacks. For instance, a competitor who obtains a stolen recommendation model from an e-commerce platform might gain insights into customer behavior, business analytics, and embedded trade secrets. This knowledge can also be used to conduct model inversion attacks[^fn-model-inversion-attack], where an attacker attempts to infer private details about the model's training data [@fredrikson2015model]."
    },
    {
      "footnote_id": "fn-netflix-deanonymization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 271,
      "context": "...was trained. Similar vulnerabilities have been demonstrated in studies on the Netflix Prize dataset[^fn-netflix-deanonymization], where researchers were able to infer individual movie preferences from anonymized data [@narayanan...",
      "full_line": "In a model inversion attack, the adversary queries the model through a legitimate interface, such as a public API, and observes its outputs. By analyzing confidence scores or output probabilities, the attacker can optimize inputs to reconstruct data resembling the model's training set. For example, a facial recognition model used for secure access could be manipulated to reveal statistical properties of the employee photos on which it was trained. Similar vulnerabilities have been demonstrated in studies on the Netflix Prize dataset[^fn-netflix-deanonymization], where researchers were able to infer individual movie preferences from anonymized data [@narayanan2006break]."
    },
    {
      "footnote_id": "fn-model-inversion-attack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 273,
      "context": "[^fn-model-inversion-attack]: **Model Inversion Attack**: First demonstrated in 2015 against face recognition systems, researche...",
      "full_line": "[^fn-model-inversion-attack]: **Model Inversion Attack**: First demonstrated in 2015 against face recognition systems, researchers reconstructed recognizable faces from neural network outputs using only confidence scores. The attack revealed that models trained on 40 individuals could leak identifiable facial features, proving that \"black-box\" API access isn't sufficient privacy protection."
    },
    {
      "footnote_id": "fn-netflix-deanonymization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 275,
      "context": "[^fn-netflix-deanonymization]: **Netflix Deanonymization**: In 2008, researchers re-identified Netflix users by correlating the \"...",
      "full_line": "[^fn-netflix-deanonymization]: **Netflix Deanonymization**: In 2008, researchers re-identified Netflix users by correlating the \"anonymous\" Prize dataset with public IMDb ratings. Using as few as 8 movie ratings with dates, they identified 99% of users, leading Netflix to cancel a second competition and highlighting the futility of naive anonymization."
    },
    {
      "footnote_id": "fn-meltdown-spectre-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 584,
      "context": "...most notable examples came with the discovery of [Meltdown and Spectre](https://meltdownattack.com/)[^fn-meltdown-spectre-impact]\u2014two vulnerabilities in modern processors that allow malicious programs to bypass memory isolation a...",
      "full_line": "Hardware is not immune to the pervasive issue of design flaws or bugs. Attackers can exploit these vulnerabilities to access, manipulate, or extract sensitive data, breaching the confidentiality and integrity that users and services depend on. One of the most notable examples came with the discovery of [Meltdown and Spectre](https://meltdownattack.com/)[^fn-meltdown-spectre-impact]\u2014two vulnerabilities in modern processors that allow malicious programs to bypass memory isolation and read the data of other applications and the operating system [@Lipp2018meltdown; @Kocher2018spectre]."
    },
    {
      "footnote_id": "fn-speculative-execution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 586,
      "context": "These attacks exploit speculative execution[^fn-speculative-execution], a performance optimization in CPUs that executes instructions out of order before safety checks ar...",
      "full_line": "These attacks exploit speculative execution[^fn-speculative-execution], a performance optimization in CPUs that executes instructions out of order before safety checks are complete. While improving computational speed, this optimization inadvertently exposes sensitive data through microarchitectural side channels, such as CPU caches. The technical sophistication of these attacks highlights the difficulty of eliminating vulnerabilities even with extensive hardware validation."
    },
    {
      "footnote_id": "fn-meltdown-spectre-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 588,
      "context": "[^fn-meltdown-spectre-impact]: **Meltdown/Spectre Impact**: Disclosed in January 2018, these vulnerabilities affected virtually e...",
      "full_line": "[^fn-meltdown-spectre-impact]: **Meltdown/Spectre Impact**: Disclosed in January 2018, these vulnerabilities affected virtually every processor made since 1995\u2014over 20 billion devices. The disclosure triggered emergency patches across all major operating systems, causing 5-30% performance degradation in some workloads, and led to a fundamental rethinking of processor security."
    },
    {
      "footnote_id": "fn-speculative-execution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 590,
      "context": "[^fn-speculative-execution]: **Speculative Execution**: Introduced in the Intel Pentium Pro (1995), this technique executes ins...",
      "full_line": "[^fn-speculative-execution]: **Speculative Execution**: Introduced in the Intel Pentium Pro (1995), this technique executes instructions before confirming they're needed, improving performance by 10-25%. However, it creates a 20+ year attack window where speculated operations leak data through cache timing, affecting ML accelerators that rely on similar optimizations."
    },
    {
      "footnote_id": "fn-hipaa-violations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 598,
      "context": "...w.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html)[^fn-hipaa-violations], leading to significant legal and ethical consequences.",
      "full_line": "Such vulnerabilities are particularly concerning in privacy-sensitive domains like healthcare, where ML systems routinely handle patient data. A breach could violate privacy regulations such as the [Health Insurance Portability and Accountability Act (HIPAA)](https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html)[^fn-hipaa-violations], leading to significant legal and ethical consequences."
    },
    {
      "footnote_id": "fn-hipaa-violations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 600,
      "context": "[^fn-hipaa-violations]: **HIPAA Violations**: Since enforcement began in 2003, HIPAA has generated over $130 million in fi...",
      "full_line": "[^fn-hipaa-violations]: **HIPAA Violations**: Since enforcement began in 2003, HIPAA has generated over $130 million in fines, with individual penalties reaching $16 million. The largest healthcare data breach affected 78.8 million patients at Anthem Inc. in 2015, highlighting the massive scale of exposure when ML systems handling medical data are compromised."
    },
    {
      "footnote_id": "fn-dp-sgd-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 848,
      "context": "...sian mechanism. Training techniques like differentially private stochastic gradient descent (DP-SGD)[^fn-dp-sgd-adoption] integrate calibrated noise into gradient computations during each training step, ensuring that indi...",
      "full_line": "This bound ensures that the algorithm's behavior remains statistically indistinguishable regardless of whether any individual's data is present, thereby limiting the information that can be inferred about that individual. In practice, DP is implemented by adding calibrated noise to model updates or query responses, using mechanisms such as the Laplace or Gaussian mechanism. Training techniques like differentially private stochastic gradient descent (DP-SGD)[^fn-dp-sgd-adoption] integrate calibrated noise into gradient computations during each training step, ensuring that individual data points cannot be distinguished from the model's learned behavior."
    },
    {
      "footnote_id": "fn-dp-sgd-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 850,
      "context": "[^fn-dp-sgd-adoption]: **DP-SGD Industry Adoption**: Apple was the first major company to deploy differential privacy at...",
      "full_line": "[^fn-dp-sgd-adoption]: **DP-SGD Industry Adoption**: Apple was the first major company to deploy differential privacy at scale in 2016, protecting 1+ billion users' data in iOS. Their implementation adds noise to emoji usage, Safari crashes, and QuickType suggestions, balancing privacy (\u03b5=4-16) with utility for improving user experience across their ecosystem."
    },
    {
      "footnote_id": "fn-federated-learning-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 858,
      "context": "Complementary to DP, federated learning (FL)[^fn-federated-learning-scale] reduces privacy risks by restructuring the learning process itself. Rather than aggregating raw dat...",
      "full_line": "Complementary to DP, federated learning (FL)[^fn-federated-learning-scale] reduces privacy risks by restructuring the learning process itself. Rather than aggregating raw data at a central location, FL distributes the training across a set of client devices, each holding local data [@mcmahan2017communicationefficient]. Clients compute model updates locally and share only parameter deltas with a central server for aggregation:"
    },
    {
      "footnote_id": "fn-federated-learning-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 865,
      "context": "[^fn-federated-learning-scale]: **Federated Learning Scale**: Google's Gboard keyboard uses federated learning across 1+ billion d...",
      "full_line": "[^fn-federated-learning-scale]: **Federated Learning Scale**: Google's Gboard keyboard uses federated learning across 1+ billion devices to improve text predictions without sending keystrokes to servers. Each phone trains locally on typing patterns, sharing only model updates\u2014processing 4 billion characters daily while preserving privacy at unprecedented scale."
    },
    {
      "footnote_id": "fn-smpc-overhead",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 874,
      "context": "...HE remains high, making it more suitable for fixed-function models and low-latency batch tasks. SMPC[^fn-smpc-overhead], by contrast, distributes the computation across multiple parties such that no single party learns...",
      "full_line": "This property supports privacy-preserving computation in untrusted environments, such as cloud inference over sensitive health or financial records. However, the computational cost of HE remains high, making it more suitable for fixed-function models and low-latency batch tasks. SMPC[^fn-smpc-overhead], by contrast, distributes the computation across multiple parties such that no single party learns the complete input or output. This is particularly useful in joint training across institutions with strict data-use policies, such as hospitals or banks."
    },
    {
      "footnote_id": "fn-smpc-overhead",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 876,
      "context": "[^fn-smpc-overhead]: **SMPC Performance**: Secure multi-party computation typically incurs 1000-10,000x computational o...",
      "full_line": "[^fn-smpc-overhead]: **SMPC Performance**: Secure multi-party computation typically incurs 1000-10,000x computational overhead compared to plaintext operations. A simple neural network inference that takes milliseconds on GPU requires hours using SMPC, limiting practical applications to small models and offline scenarios."
    },
    {
      "footnote_id": "fn-trustzone-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1070,
      "context": "...ferent deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[^fn-trustzone-adoption] offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT...",
      "full_line": "Several commercial platforms provide TEE functionality tailored for different deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[^fn-trustzone-adoption] offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT applications. [Intel SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[^fn-intel-sgx-limits] implements enclave-based security for cloud and desktop systems, enabling secure computation even on untrusted infrastructure. [Qualcomm's Secure Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions) supports secure mobile transactions and user authentication. Apple's Secure Enclave remains a canonical example of a hardware-isolated security coprocessor for consumer devices."
    },
    {
      "footnote_id": "fn-intel-sgx-limits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1070,
      "context": "...(https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[^fn-intel-sgx-limits] implements enclave-based security for cloud and desktop systems, enabling secure computation even o...",
      "full_line": "Several commercial platforms provide TEE functionality tailored for different deployment contexts. [ARM TrustZone](https://www.arm.com/technologies/trustzone-for-cortex-m)[^fn-trustzone-adoption] offers secure and normal world execution on ARM-based systems and is widely used in mobile and IoT applications. [Intel SGX](https://www.intel.com/content/www/us/en/architecture-and-technology/software-guard-extensions.html)[^fn-intel-sgx-limits] implements enclave-based security for cloud and desktop systems, enabling secure computation even on untrusted infrastructure. [Qualcomm's Secure Execution Environment](https://www.qualcomm.com/products/features/mobile-security-solutions) supports secure mobile transactions and user authentication. Apple's Secure Enclave remains a canonical example of a hardware-isolated security coprocessor for consumer devices."
    },
    {
      "footnote_id": "fn-trustzone-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1072,
      "context": "[^fn-trustzone-adoption]: **ARM TrustZone**: Introduced in 2004, TrustZone now ships in 95% of ARM processors, protecting ov...",
      "full_line": "[^fn-trustzone-adoption]: **ARM TrustZone**: Introduced in 2004, TrustZone now ships in 95% of ARM processors, protecting over 5 billion mobile devices. Despite its ubiquity, many devices underutilize TrustZone\u2014studies show only 20-30% of Android devices implement meaningful secure world applications beyond basic key storage."
    },
    {
      "footnote_id": "fn-intel-sgx-limits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1074,
      "context": "[^fn-intel-sgx-limits]: **Intel SGX Constraints**: SGX enclaves are limited to 128MB of protected memory (EPC), with cache...",
      "full_line": "[^fn-intel-sgx-limits]: **Intel SGX Constraints**: SGX enclaves are limited to 128MB of protected memory (EPC), with cache misses causing 100x performance penalties. This makes SGX unsuitable for large ML models but effective for protecting cryptographic keys and small inference models under 10MB."
    },
    {
      "footnote_id": "fn-hsm-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1283,
      "context": "A Hardware Security Module (HSM)[^fn-hsm-performance] is a tamper-resistant physical device designed to perform cryptographic operations and securely man...",
      "full_line": "A Hardware Security Module (HSM)[^fn-hsm-performance] is a tamper-resistant physical device designed to perform cryptographic operations and securely manage digital keys. HSMs are widely used across security-critical industries such as finance, defense, and cloud infrastructure, and they are increasingly relevant for securing the machine learning pipeline\u2014particularly in deployments where key confidentiality, model integrity, and regulatory compliance are essential."
    },
    {
      "footnote_id": "fn-hsm-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1287,
      "context": "[^fn-hsm-performance]: **HSM Performance**: Enterprise HSMs can perform 10,000+ RSA-2048 operations per second but cost $...",
      "full_line": "[^fn-hsm-performance]: **HSM Performance**: Enterprise HSMs can perform 10,000+ RSA-2048 operations per second but cost $20,000-$100,000+ per unit. In contrast, software-only cryptography on GPUs achieves 100,000+ operations/second at $1,000+ hardware cost, but without the tamper-resistance and regulatory compliance that HSMs provide."
    },
    {
      "footnote_id": "fn-puf-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1311,
      "context": "Physical Unclonable Functions (PUFs)[^fn-puf-adoption] provide a hardware-intrinsic mechanism for cryptographic key generation and device authentication b...",
      "full_line": "Physical Unclonable Functions (PUFs)[^fn-puf-adoption] provide a hardware-intrinsic mechanism for cryptographic key generation and device authentication by leveraging physical randomness in semiconductor fabrication [@gassend2002silicon]. Unlike traditional keys stored in memory, a PUF generates secret values based on microscopic variations in a chip's physical properties\u2014variations that are inherent to manufacturing processes and difficult to clone or predict, even by the manufacturer."
    },
    {
      "footnote_id": "fn-puf-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1313,
      "context": "[^fn-puf-adoption]: **PUF Market Growth**: The PUF market is projected to reach $320 million by 2025, driven by IoT se...",
      "full_line": "[^fn-puf-adoption]: **PUF Market Growth**: The PUF market is projected to reach $320 million by 2025, driven by IoT security needs. Major semiconductor companies including Intel, Xilinx, and Synopsis now offer PUF IP, with deployment in smart cards, automotive ECUs, and edge ML devices requiring device-unique authentication."
    },
    {
      "footnote_id": "fn-demographic-parity-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 53,
      "context": "...y, justice, and structural bias. The two key statistical measures of fairness are demographic parity[^fn-demographic-parity-origin] and equalized odds. Demographic parity ensures equal outcomes across different demographic groups....",
      "full_line": "Fairness refers to the expectation that machine learning systems do not discriminate against individuals or groups on the basis of protected attributes such as race, gender, or socioeconomic status. This principle encompasses both statistical metrics and broader normative concerns about equity, justice, and structural bias. The two key statistical measures of fairness are demographic parity[^fn-demographic-parity-origin] and equalized odds. Demographic parity ensures equal outcomes across different demographic groups. For example, if a loan approval system maintains the same approval rate for all racial groups, it would satisfy demographic parity. The equalized odds criterion requires that equal outcomes be maintained for all groups at all decision thresholds. In practice, this means the true positive and false positive rates should be equal across protected groups. However, fairness extends beyond these statistical definitions to address deeper questions of equity, historical discrimination, and systemic bias in how machine learning systems impact different communities."
    },
    {
      "footnote_id": "fn-demographic-parity-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 55,
      "context": "[^fn-demographic-parity-origin]: **Demographic Parity Origins**: This fairness criterion was first formalized by computer scientist...",
      "full_line": "[^fn-demographic-parity-origin]: **Demographic Parity Origins**: This fairness criterion was first formalized by computer scientist Cynthia Dwork and colleagues in 2011, building on legal concepts from the 1971 Supreme Court case *Griggs v. Duke Power Co.*, which established that employment practices with disparate impact could violate civil rights law even without discriminatory intent. The mathematical formalization bridged legal theory with algorithmic practice."
    },
    {
      "footnote_id": "fn-compas-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 97,
      "context": "...px) algorithm, used in the United States to assess recidivism risk, was found to exhibit racial bias[^fn-compas-bias]. However, the proprietary nature of the system, combined with limited access to interpretability to...",
      "full_line": "Machine learning systems are frequently criticized for their lack of interpretability. In many cases, models operate as opaque \"black boxes,\" producing outputs that are difficult for users, developers, and regulators to understand or scrutinize. This opacity presents a significant barrier to trust, particularly in high-stakes domains such as criminal justice, healthcare, and finance, where accountability and the right to recourse are essential. For example, the [COMPAS](https://doc.wi.gov/Pages/AboutDOC/COMPAS.aspx) algorithm, used in the United States to assess recidivism risk, was found to exhibit racial bias[^fn-compas-bias]. However, the proprietary nature of the system, combined with limited access to interpretability tools, hindered efforts to investigate or address the issue."
    },
    {
      "footnote_id": "fn-compas-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 99,
      "context": "[^fn-compas-bias]: **COMPAS Algorithm Controversy**: A 2016 ProPublica investigation revealed that COMPAS (Correction...",
      "full_line": "[^fn-compas-bias]: **COMPAS Algorithm Controversy**: A 2016 ProPublica investigation revealed that COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) incorrectly flagged Black defendants as future criminals at nearly twice the rate of white defendants (45% vs 24%), while white defendants were mislabeled as low-risk more often than Black defendants (48% vs 28%). The algorithm was used in sentencing decisions across multiple states despite these documented disparities."
    },
    {
      "footnote_id": "fn-gdpr-article-22",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 103,
      "context": "...receive meaningful information about the logic of automated decisions that significantly affect them[^fn-gdpr-article-22]. Similar regulatory pressures are emerging in other domains, reinforcing the need to treat explaina...",
      "full_line": "These principles are not merely best practices; in many jurisdictions, they are legal obligations. For instance, the European Unions [General Data Protection Regulation (GDPR)](https://gdpr.eu/tag/gdpr/) requires that individuals receive meaningful information about the logic of automated decisions that significantly affect them[^fn-gdpr-article-22]. Similar regulatory pressures are emerging in other domains, reinforcing the need to treat explainability and transparency as core architectural requirements."
    },
    {
      "footnote_id": "fn-gdpr-article-22",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 105,
      "context": "[^fn-gdpr-article-22]: **GDPR Article 22**: Known as the \"right to explanation,\" this provision affects an estimated 500...",
      "full_line": "[^fn-gdpr-article-22]: **GDPR Article 22**: Known as the \"right to explanation,\" this provision affects an estimated 500 million EU citizens and has inspired similar legislation worldwide. Since GDPR's 2018 implementation, regulators have issued over \u20ac5.88 billion in fines as of January 2025, with many cases involving algorithmic decision-making. The regulation's global influence extends beyond Europe\u2014over 120 countries now have privacy laws modeled on GDPR principles."
    },
    {
      "footnote_id": "fn-healthcare-algorithm-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 115,
      "context": "...found to systematically underestimate the health needs of Black patients [@obermeyer2019dissecting][^fn-healthcare-algorithm-bias]. The model used healthcare expenditures as a proxy for health status, but due to longstanding dispa...",
      "full_line": "A widely studied example comes from the healthcare domain. An algorithm used to allocate care management resources in U.S. hospitals was found to systematically underestimate the health needs of Black patients [@obermeyer2019dissecting][^fn-healthcare-algorithm-bias]. The model used healthcare expenditures as a proxy for health status, but due to longstanding disparities in access and spending, Black patients were less likely to incur high costs. As a result, the model inferred that they were less sick, despite often having equal or greater medical need. This case illustrates how seemingly neutral design choices such as proxy variable selection can yield discriminatory outcomes when historical inequities are not properly accounted for."
    },
    {
      "footnote_id": "fn-healthcare-algorithm-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 117,
      "context": "[^fn-healthcare-algorithm-bias]: **Healthcare Algorithm Scale**: This Optum algorithm affected approximately 200 million Americans...",
      "full_line": "[^fn-healthcare-algorithm-bias]: **Healthcare Algorithm Scale**: This Optum algorithm affected approximately 200 million Americans annually, determining access to high-risk care management programs. The bias reduced Black patients' enrollment by 50%\u2014if corrected, the number of Black patients identified for extra care would increase from 17.7% to 46.5%, highlighting how algorithmic decisions can perpetuate healthcare disparities at massive scale."
    },
    {
      "footnote_id": "fn-adversarial-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 234,
      "context": "One illustrative failure mode arises from adversarial inputs[^fn-adversarial-examples]: carefully constructed perturbations that appear benign to humans but cause a model to output incor...",
      "full_line": "One illustrative failure mode arises from adversarial inputs[^fn-adversarial-examples]: carefully constructed perturbations that appear benign to humans but cause a model to output incorrect or harmful predictions [@szegedy2013intriguing]. Such vulnerabilities are not limited to image classification\u2014they have been observed across modalities including audio, text, and structured data, and they reveal the brittleness of learned representations in high-dimensional spaces. These behaviors highlight that robustness must be considered not only during training but as a global property of how systems interact with real-world complexity."
    },
    {
      "footnote_id": "fn-adversarial-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 236,
      "context": "[^fn-adversarial-examples]: **Adversarial Examples Discovery**: First discovered by Christian Szegedy and colleagues at Google...",
      "full_line": "[^fn-adversarial-examples]: **Adversarial Examples Discovery**: First discovered by Christian Szegedy and colleagues at Google in 2013, adversarial examples revealed that small, imperceptible changes to inputs could cause state-of-the-art neural networks to misclassify with high confidence. The original paper showed that adding noise with magnitude 0.007 (less than 1% of the input range) could cause a panda image to be classified as a gibbon with 99.3% confidence, fundamentally challenging assumptions about neural network reliability."
    },
    {
      "footnote_id": "fn-differential-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 481,
      "context": "...acy-preserving techniques have been developed. Among the most widely adopted is differential privacy[^fn-differential-privacy], which provides formal guarantees that the inclusion or exclusion of a single datapoint has a stati...",
      "full_line": "To mitigate such vulnerabilities, a range of privacy-preserving techniques have been developed. Among the most widely adopted is differential privacy[^fn-differential-privacy], which provides formal guarantees that the inclusion or exclusion of a single datapoint has a statistically bounded effect on the models output. Algorithms such as differentially private stochastic gradient descent (DP-SGD) enforce these guarantees by clipping gradients and injecting noise during training [@abadi2016deep]. When implemented correctly, these methods prevent the model from memorizing individual datapoints and reduce the risk of inference attacks."
    },
    {
      "footnote_id": "fn-differential-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 483,
      "context": "[^fn-differential-privacy]: **Differential Privacy**: Introduced by Cynthia Dwork in 2006, differential privacy revolutionized...",
      "full_line": "[^fn-differential-privacy]: **Differential Privacy**: Introduced by Cynthia Dwork in 2006, differential privacy revolutionized privacy-preserving computation by providing mathematical guarantees rather than heuristic protections. Apple was among the first major companies to deploy differential privacy at scale in 2016, using it to collect iOS usage statistics from over 1 billion devices while preserving individual privacy. The technique adds calibrated noise to computations, ensuring that no single person's data significantly affects the output."
    },
    {
      "footnote_id": "fn-value-alignment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1002,
      "context": "...@russell2021human]. Within the field of AI safety, a central focus is the problem of value alignment[^fn-value-alignment]: how to ensure that machine learning systems act in accordance with broad human intentions, rather...",
      "full_line": "As the capabilities of deep learning models have increasingly approached, and, in certain instances, exceeded, human performance, the concern that such systems may pursue unintended or undesirable goals has become more pressing [@russell2021human]. Within the field of AI safety, a central focus is the problem of value alignment[^fn-value-alignment]: how to ensure that machine learning systems act in accordance with broad human intentions, rather than optimizing misaligned proxies or exhibiting emergent behavior that undermines social goals. As Russell argues in Human-Compatible Artificial Intelligence, much of current AI research presumes that the objectives to be optimized are known and fixed, focusing instead on the effectiveness of optimization rather than the design of objectives themselves."
    },
    {
      "footnote_id": "fn-value-alignment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1004,
      "context": "[^fn-value-alignment]: **Value Alignment Problem**: First articulated by philosopher Nick Bostrom in 2012 and formalized...",
      "full_line": "[^fn-value-alignment]: **Value Alignment Problem**: First articulated by philosopher Nick Bostrom in 2012 and formalized by Stuart Russell in 2016, the value alignment problem asks how to ensure AI systems pursue goals compatible with human welfare. The challenge became prominent after observing that simple objectives often lead to unintended consequences\u2014famously illustrated by the \"paperclip maximizer\" thought experiment, where an AI optimizing paperclip production might ultimately convert all available matter into paperclips, including humans."
    },
    {
      "footnote_id": "fn-adversarial-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 24,
      "context": "From subtle hardware anomalies to sophisticated adversarial attacks[^fn-adversarial-discovery] and the unpredictable nature of real-world data, the potential for failure is ever-present.",
      "full_line": "From subtle hardware anomalies to sophisticated adversarial attacks[^fn-adversarial-discovery] and the unpredictable nature of real-world data, the potential for failure is ever-present."
    },
    {
      "footnote_id": "fn-adversarial-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 26,
      "context": "[^fn-adversarial-discovery]: **Adversarial Examples Discovery**: Christian Szegedy and colleagues at Google discovered adversar...",
      "full_line": "[^fn-adversarial-discovery]: **Adversarial Examples Discovery**: Christian Szegedy and colleagues at Google discovered adversarial examples almost by accident in 2013 while trying to understand what neural networks had learned. Their finding that \"imperceptibly small perturbations\" could fool state-of-the-art models sent shockwaves through the ML community and launched an entire field of adversarial machine learning research. This reality underscores the need to fundamentally rethink how machine learning systems are designed and deployed, placing robustness and trustworthiness at the forefront. Building resilient machine learning systems is not merely a technical objective; it is a foundational requirement for ensuring their safe and effective operation in dynamic and uncertain environments."
    },
    {
      "footnote_id": "fn-safety-critical-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 44,
      "context": "Looking ahead, as these systems become more complex and are deployed in safety-critical[^fn-safety-critical-evolution] applications, the need for robust and fault-tolerant designs becomes paramount.",
      "full_line": "Looking ahead, as these systems become more complex and are deployed in safety-critical[^fn-safety-critical-evolution] applications, the need for robust and fault-tolerant designs becomes paramount."
    },
    {
      "footnote_id": "fn-safety-critical-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 46,
      "context": "[^fn-safety-critical-evolution]: **Safety-Critical Computing Evolution**: The field evolved from nuclear reactor control systems in...",
      "full_line": "[^fn-safety-critical-evolution]: **Safety-Critical Computing Evolution**: The field evolved from nuclear reactor control systems in the 1950s to aviation in the 1970s (DO-178B standard), and now encompasses ML systems. The key insight is that in safety-critical domains, \"good enough\" performance isn't enough\u2014systems must be provably safe under all anticipated conditions."
    },
    {
      "footnote_id": "fn-data-poisoning-tay",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 48,
      "context": "...ng hardware and software faults, and malicious inputs such as adversarial attacks and data poisoning[^fn-data-poisoning-tay], and environmental shifts, can be severe, potentially resulting in loss of life, economic disruptio...",
      "full_line": "ML systems are expected to play critical roles in autonomous vehicles, smart cities, healthcare, and industrial automation. In these domains, the consequences of systemic failures, including hardware and software faults, and malicious inputs such as adversarial attacks and data poisoning[^fn-data-poisoning-tay], and environmental shifts, can be severe, potentially resulting in loss of life, economic disruption, or environmental harm."
    },
    {
      "footnote_id": "fn-data-poisoning-tay",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 50,
      "context": "[^fn-data-poisoning-tay]: **Data Poisoning Emergence**: The threat became prominent with Microsoft's Tay chatbot (2016), whi...",
      "full_line": "[^fn-data-poisoning-tay]: **Data Poisoning Emergence**: The threat became prominent with Microsoft's Tay chatbot (2016), which was \"corrupted\" by coordinated user inputs within 24 hours. This incident revealed how bad actors could systematically manipulate training data or model updates to create backdoors or degrade performance\u2014a vulnerability unique to learning systems."
    },
    {
      "footnote_id": "fn-ml-security-threats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 71,
      "context": "...nd sustained performance. Building such systems requires understanding the security threat landscape[^fn-ml-security-threats] unique to machine learning.",
      "full_line": "Regardless of deployment context, the essential characteristics of a robust ML system include fault tolerance, error resilience, and sustained performance. Building such systems requires understanding the security threat landscape[^fn-ml-security-threats] unique to machine learning."
    },
    {
      "footnote_id": "fn-ml-security-threats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 73,
      "context": "[^fn-ml-security-threats]: **ML Security Threat Taxonomy**: Unlike traditional software security, ML systems face unique thre...",
      "full_line": "[^fn-ml-security-threats]: **ML Security Threat Taxonomy**: Unlike traditional software security, ML systems face unique threats: training-time attacks (data poisoning, backdoors), inference-time attacks (adversarial examples, model inversion), and deployment-time attacks (model stealing, membership inference). Each category requires distinct defense mechanisms and represents billions in potential economic impact. By understanding and addressing these multifaceted challenges, it is possible to develop reliable ML systems capable of operating effectively in real-world environments."
    },
    {
      "footnote_id": "fn-sdc-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 91,
      "context": "[^fn-sdc-scale]: **Silent Data Corruption Challenge**: SDC became a major concern as data centers scaled to million...",
      "full_line": "[^fn-sdc-scale]: **Silent Data Corruption Challenge**: SDC became a major concern as data centers scaled to millions of servers. Unlike ECC memory errors that are detected, SDC errors pass all checks but contain wrong values. Google estimated that 1 in 1,700 servers experienced SDC per year\u2014seemingly rare until multiplied by planetary-scale infrastructure."
    },
    {
      "footnote_id": "fn-seu-space-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 262,
      "context": "Some of the common types of transient faults include Single Event Upsets (SEUs)[^fn-seu-space-rate] caused by ionizing radiation, voltage fluctuations [@reddi2013resilient] due to power supply noise...",
      "full_line": "Some of the common types of transient faults include Single Event Upsets (SEUs)[^fn-seu-space-rate] caused by ionizing radiation, voltage fluctuations [@reddi2013resilient] due to power supply noise or electromagnetic interference, Electromagnetic Interference (EMI) induced by external electromagnetic fields, Electrostatic Discharge (ESD) resulting from sudden static electricity flow, crosstalk caused by unintended signal coupling, ground bounce triggered by simultaneous switching of multiple outputs, timing violations due to signal timing constraint breaches, and soft errors in combinational logic affecting the output of logic circuits [@mukherjee2005soft]."
    },
    {
      "footnote_id": "fn-seu-space-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 264,
      "context": "[^fn-seu-space-rate]: **Single Event Upset Rates**: SEU rates vary dramatically with altitude and technology node. At gr...",
      "full_line": "[^fn-seu-space-rate]: **Single Event Upset Rates**: SEU rates vary dramatically with altitude and technology node. At ground level, 28nm processors experience ~1 SEU per device per 1000 hours, but this increases 100x in space environments. Modern AI accelerators with millions of parameters are particularly vulnerable due to their large memory footprints. Understanding these different types of transient faults is crucial for designing robust and resilient hardware systems that can mitigate their impact and ensure reliable operation."
    },
    {
      "footnote_id": "fn-cosmic-ray-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 324,
      "context": "...environmental factors represent one of the most significant sources of transient faults. Cosmic rays[^fn-cosmic-ray-rate]\u2014high-energy particles originating from outer space\u2014can strike sensitive areas of hardware, such as...",
      "full_line": "External environmental factors represent one of the most significant sources of transient faults. Cosmic rays[^fn-cosmic-ray-rate]\u2014high-energy particles originating from outer space\u2014can strike sensitive areas of hardware, such as memory cells or transistors, inducing charge disturbances that alter stored or transmitted data."
    },
    {
      "footnote_id": "fn-cosmic-ray-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 326,
      "context": "[^fn-cosmic-ray-rate]: **Cosmic Ray Impact**: At sea level, approximately 1000 cosmic ray particles pass through each squ...",
      "full_line": "[^fn-cosmic-ray-rate]: **Cosmic Ray Impact**: At sea level, approximately 1000 cosmic ray particles pass through each square meter of surface every minute. Modern 16nm chips experience roughly 1 soft error per billion device-hours of operation due to cosmic radiation\u2014a rate that increases dramatically at higher altitudes where atmospheric shielding is reduced. This is illustrated in @fig-transient-fault. [Electromagnetic interference (EMI)](https://www.trentonsystems.com/en-us/resource-hub/blog/what-is-electromagnetic-interference) from nearby devices represents another significant external cause, where electromagnetic fields couple with circuits and cause voltage spikes or glitches that temporarily disrupt normal operation. Electrostatic discharge (ESD) events, resulting from sudden static electricity flow, can also induce transient faults by creating temporary voltage surges that affect sensitive electronic components."
    },
    {
      "footnote_id": "fn-bit-flip-dram",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 340,
      "context": "A common example of a transient fault is a bit flip[^fn-bit-flip-dram] in the main memory. If an important data structure or critical instruction is stored in the affecte...",
      "full_line": "A common example of a transient fault is a bit flip[^fn-bit-flip-dram] in the main memory. If an important data structure or critical instruction is stored in the affected memory location, it can lead to incorrect computations or program misbehavior."
    },
    {
      "footnote_id": "fn-bit-flip-dram",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 342,
      "context": "[^fn-bit-flip-dram]: **DRAM Bit Flip Rates**: Modern DDR4 memory experiences approximately 1 uncorrectable bit flip per...",
      "full_line": "[^fn-bit-flip-dram]: **DRAM Bit Flip Rates**: Modern DDR4 memory experiences approximately 1 uncorrectable bit flip per 10^17 bit-hours. While ECC memory can correct single-bit errors, multi-bit errors occur roughly once per 10^14 bit-hours, making them a significant concern for large-scale AI training clusters with thousands of GPUs. For instance, a bit flip in the memory storing a loop counter can cause the loop to execute indefinitely or terminate prematurely. Transient faults in control registers or flag bits can alter the flow of program execution, leading to unexpected jumps or incorrect branch decisions. In communication systems, transient faults can corrupt transmitted data packets, resulting in retransmissions or data loss."
    },
    {
      "footnote_id": "fn-stochastic-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 352,
      "context": "...erging approaches such as [stochastic computing](https://en.wikipedia.org/wiki/Stochastic_computing)[^fn-stochastic-computing] (which represents values as probabilities encoded in bit streams) are being explored to enhance fau...",
      "full_line": "Transient faults can be amplified in resource-constrained environments like TinyML, where limited computational and memory resources exacerbate their impact. One prominent example is Binarized Neural Networks (BNNs) [@courbariaux2016binarized], which quantize network weights to just +1 or -1 values to achieve computational efficiency and faster inference times. While this binary representation is advantageous for resource-constrained systems, it also makes BNNs particularly fragile to bit-flip errors. For instance, prior work [@Aygun2021BSBNN] has shown that a two-hidden-layer BNN architecture for a simple task such as MNIST classification suffers performance degradation from 98% test accuracy to 70% when random bit-flipping soft errors are inserted through model weights with a 10% probability. To address these vulnerabilities, techniques like flip-aware training and emerging approaches such as [stochastic computing](https://en.wikipedia.org/wiki/Stochastic_computing)[^fn-stochastic-computing] (which represents values as probabilities encoded in bit streams) are being explored to enhance fault tolerance."
    },
    {
      "footnote_id": "fn-stochastic-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 354,
      "context": "[^fn-stochastic-computing]: **Stochastic Computing Resilience**: Stochastic computing represents numbers as probabilities in b...",
      "full_line": "[^fn-stochastic-computing]: **Stochastic Computing Resilience**: Stochastic computing represents numbers as probabilities in bit streams, making it inherently fault-tolerant. A single bit flip in a 1000-bit stream changes the represented value by only 0.1%, compared to potentially catastrophic errors in binary representations. This makes it promising for fault-tolerant AI inference in harsh environments."
    },
    {
      "footnote_id": "fn-fdiv-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 363,
      "context": "...example of a permanent fault is the [Intel FDIV bug](https://en.wikipedia.org/wiki/Pentium_FDIV_bug)[^fn-fdiv-cost], discovered in 1994.",
      "full_line": "One notable example of a permanent fault is the [Intel FDIV bug](https://en.wikipedia.org/wiki/Pentium_FDIV_bug)[^fn-fdiv-cost], discovered in 1994."
    },
    {
      "footnote_id": "fn-fdiv-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 365,
      "context": "[^fn-fdiv-cost]: **FDIV Bug Economic Impact**: Intel's Pentium FDIV bug cost the company $475 million in 1994 (equi...",
      "full_line": "[^fn-fdiv-cost]: **FDIV Bug Economic Impact**: Intel's Pentium FDIV bug cost the company $475 million in 1994 (equivalent to $900 million in 2024). The bug affected approximately 5 million processors and occurred in 1 out of every 9 billion division operations, demonstrating how rare but systematic errors can have massive financial consequences. This flaw affected the floating-point division (FDIV) units of certain Intel Pentium processors, causing incorrect results for specific division operations and leading to inaccurate calculations."
    },
    {
      "footnote_id": "fn-electromigration-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 380,
      "context": "...ts. In contrast, [wear-out mechanisms](https://semiengineering.com/what-causes-semiconductor-aging/)[^fn-electromigration-ai] occur over time due to prolonged use and operational stress. Phenomena like electromigration, oxide...",
      "full_line": "[Manufacturing defects](https://www.sciencedirect.com/science/article/pii/B9780128181058000206) are flaws introduced during the fabrication process, including improper etching, incorrect doping, or contamination. These defects may result in non-functional or partially functional components. In contrast, [wear-out mechanisms](https://semiengineering.com/what-causes-semiconductor-aging/)[^fn-electromigration-ai] occur over time due to prolonged use and operational stress. Phenomena like electromigration, oxide breakdown, and thermal stress degrade component integrity, eventually leading to permanent failure."
    },
    {
      "footnote_id": "fn-electromigration-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 382,
      "context": "[^fn-electromigration-ai]: **Electromigration in AI Hardware**: Electromigration\u2014the gradual movement of metal atoms due to h...",
      "full_line": "[^fn-electromigration-ai]: **Electromigration in AI Hardware**: Electromigration\u2014the gradual movement of metal atoms due to high current density\u2014is accelerated in AI accelerators that run at high utilization. NVIDIA GPUs running AI workloads can experience 10-year mean time to failure, compared to 25-30 years for typical computing workloads, due to sustained high power density."
    },
    {
      "footnote_id": "fn-stuck-at-ml-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 386,
      "context": "...ms, depending on their nature and location. A common example is the stuck-at fault [@seong2010safer][^fn-stuck-at-ml-impact], where a signal or memory cell becomes permanently fixed at either 0 or 1, regardless of the intend...",
      "full_line": "Permanent faults manifest through several mechanisms, depending on their nature and location. A common example is the stuck-at fault [@seong2010safer][^fn-stuck-at-ml-impact], where a signal or memory cell becomes permanently fixed at either 0 or 1, regardless of the intended input, as shown in @fig-stuck-fault."
    },
    {
      "footnote_id": "fn-stuck-at-ml-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 388,
      "context": "[^fn-stuck-at-ml-impact]: **Stuck-at Faults in ML**: In neural networks, stuck-at faults in weight memory can be devastating...",
      "full_line": "[^fn-stuck-at-ml-impact]: **Stuck-at Faults in ML**: In neural networks, stuck-at faults in weight memory can be devastating. A single stuck weight in a critical layer can reduce accuracy by 20-40%. However, research shows that neural networks have some natural resilience\u2014up to 10% of weights can be stuck without significant performance loss due to the distributed nature of learning. This type of fault can occur in logic gates, memory cells, or interconnects and typically results in incorrect computations or persistent data corruption."
    },
    {
      "footnote_id": "fn-fgsm-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1040,
      "context": "...[Fast Gradient Sign Method](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm) (FGSM)[^fn-fgsm-breakthrough] is a well-known technique in this category.",
      "full_line": "One prominent category of adversarial attacks is gradient-based attacks. These attacks leverage the gradients of the ML model's loss function to craft adversarial examples. The [Fast Gradient Sign Method](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm) (FGSM)[^fn-fgsm-breakthrough] is a well-known technique in this category."
    },
    {
      "footnote_id": "fn-fgsm-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1042,
      "context": "[^fn-fgsm-breakthrough]: **FGSM Breakthrough**: Ian Goodfellow invented FGSM in 2014 while working at Google, creating the...",
      "full_line": "[^fn-fgsm-breakthrough]: **FGSM Breakthrough**: Ian Goodfellow invented FGSM in 2014 while working at Google, creating the first practical algorithm for generating adversarial examples. His insight was elegantly simple: \"If you want to fool a neural network, just push the input in the direction that increases the loss most rapidly.\" This single-step attack became the foundation for an entire field of adversarial research. : **FGSM Breakthrough**: Ian Goodfellow invented FGSM in 2014 while working at Google, creating the first practical algorithm for generating adversarial examples. His insight was elegantly simple: \"If you want to fool a neural network, just push the input in the direction that increases the loss most rapidly.\" This single-step attack became the foundation for an entire field of adversarial research."
    },
    {
      "footnote_id": "fn-pgd-benchmark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1062,
      "context": "Another variant, the Projected Gradient Descent (PGD)[^fn-pgd-benchmark] attack, extends FGSM by iteratively applying the gradient update step, allowing for more refined an...",
      "full_line": "Another variant, the Projected Gradient Descent (PGD)[^fn-pgd-benchmark] attack, extends FGSM by iteratively applying the gradient update step, allowing for more refined and powerful adversarial examples."
    },
    {
      "footnote_id": "fn-pgd-benchmark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1064,
      "context": "[^fn-pgd-benchmark]: **PGD Attack Strength**: Introduced by Madry et al. in 2017, PGD became the gold standard for eval...",
      "full_line": "[^fn-pgd-benchmark]: **PGD Attack Strength**: Introduced by Madry et al. in 2017, PGD became the gold standard for evaluating adversarial robustness. On CIFAR-10, PGD can achieve 100% attack success rate against undefended models with perturbations as small as 8/255 pixel intensity\u2014imperceptible to humans but devastating to neural networks. PGD projects each perturbation step back into a constrained norm ball around the original input, ensuring that the adversarial example remains within a specified distortion limit. This makes PGD a stronger white-box attack and a benchmark for evaluating model robustness."
    },
    {
      "footnote_id": "fn-cw-attack-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1072,
      "context": "...late the generation of adversarial examples as an optimization problem. The Carlini and Wagner (C&W)[^fn-cw-attack-power] attack is a prominent example in this category.",
      "full_line": "These attacks formulate the generation of adversarial examples as an optimization problem. The Carlini and Wagner (C&W)[^fn-cw-attack-power] attack is a prominent example in this category."
    },
    {
      "footnote_id": "fn-cw-attack-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1074,
      "context": "[^fn-cw-attack-power]: **C&W Attack Effectiveness**: The C&W attack, published in 2017, broke 10 defense mechanisms that...",
      "full_line": "[^fn-cw-attack-power]: **C&W Attack Effectiveness**: The C&W attack, published in 2017, broke 10 defense mechanisms that were considered state-of-the-art at the time. It can find adversarial examples with L2 distortions 3x smaller than FGSM while maintaining 100% success rate, making it one of the most powerful optimization-based attacks. It finds the smallest perturbation that can cause misclassification while maintaining the perceptual similarity to the original input. The C&W attack employs an iterative optimization process to minimize the perturbation while maximizing the model's prediction error. It uses a customized loss function with a confidence term to generate more confident misclassifications."
    },
    {
      "footnote_id": "fn-transferability-rates",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1084,
      "context": "Transfer-based attacks exploit the transferability[^fn-transferability-rates] property of adversarial examples.",
      "full_line": "Transfer-based attacks exploit the transferability[^fn-transferability-rates] property of adversarial examples."
    },
    {
      "footnote_id": "fn-transferability-rates",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1086,
      "context": "[^fn-transferability-rates]: **Adversarial Transferability**: Research shows that adversarial examples transfer across models w...",
      "full_line": "[^fn-transferability-rates]: **Adversarial Transferability**: Research shows that adversarial examples transfer across models with 70-90% success rate when models share similar architectures, and 30-50% even across different architectures. This property enables black-box attacks where attackers can fool unknown models by crafting attacks on publicly available substitutes. Transferability refers to the phenomenon where adversarial examples crafted for one ML model can often fool other models, even if they have different architectures or were trained on different datasets. This enables attackers to generate adversarial examples using a surrogate model and then transfer them to the target model without requiring direct access to its parameters or gradients."
    },
    {
      "footnote_id": "fn-patch-effectiveness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1094,
      "context": "...or manipulations that can deceive ML models when captured by sensors or cameras. Adversarial patches[^fn-patch-effectiveness], for example, are small, carefully designed patterns that can be placed on objects to fool object d...",
      "full_line": "Physical-world attacks bring adversarial examples into the realm of real-world scenarios. These attacks involve creating physical objects or manipulations that can deceive ML models when captured by sensors or cameras. Adversarial patches[^fn-patch-effectiveness], for example, are small, carefully designed patterns that can be placed on objects to fool object detection or classification models."
    },
    {
      "footnote_id": "fn-patch-effectiveness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1096,
      "context": "[^fn-patch-effectiveness]: **Adversarial Patch Success**: Brown et al. (2017) demonstrated that adversarial patches covering...",
      "full_line": "[^fn-patch-effectiveness]: **Adversarial Patch Success**: Brown et al. (2017) demonstrated that adversarial patches covering just 2% of an image could fool state-of-the-art ImageNet classifiers 87.9% of the time. These patches remain effective when printed and photographed, bridging the gap between digital and physical-world attacks. These patches are designed to work under varying lighting conditions, viewing angles, and distances, making them robust in real-world environments."
    },
    {
      "footnote_id": "fn-adversarial-training-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1129,
      "context": "Defending against adversarial attacks requires a multifaceted approach. Adversarial training[^fn-adversarial-training-cost] is one common defense strategy in which models are trained on adversarial examples to improve robus...",
      "full_line": "Defending against adversarial attacks requires a multifaceted approach. Adversarial training[^fn-adversarial-training-cost] is one common defense strategy in which models are trained on adversarial examples to improve robustness."
    },
    {
      "footnote_id": "fn-adversarial-training-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1131,
      "context": "[^fn-adversarial-training-cost]: **Adversarial Training Overhead**: Standard adversarial training increases training time by 6-10x...",
      "full_line": "[^fn-adversarial-training-cost]: **Adversarial Training Overhead**: Standard adversarial training increases training time by 6-10x compared to normal training due to the need to generate adversarial examples for each batch. Despite this cost, it remains the most effective defense, improving robustness from <5% to 45-50% accuracy under PGD attacks on CIFAR-10. Exposing the model to adversarial examples during training teaches it to classify them correctly and become more resilient to attacks. Defensive distillation, input preprocessing, and ensemble methods are other techniques that can help mitigate the impact of adversarial attacks."
    },
    {
      "footnote_id": "fn-stop-sign-attack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1141,
      "context": "...ng result emerged. The models misclassified the stop signs as speed limit signs over 85% of the time[^fn-stop-sign-attack].",
      "full_line": "One striking example of the impact of adversarial attacks was demonstrated by researchers in 2017. They experimented with small black and white stickers on stop signs [@eykholt2018robust]. To the human eye, these stickers did not obscure the sign or prevent its interpretability. However, when images of the sticker-modified stop signs were fed into standard traffic sign classification ML models, a shocking result emerged. The models misclassified the stop signs as speed limit signs over 85% of the time[^fn-stop-sign-attack]."
    },
    {
      "footnote_id": "fn-stop-sign-attack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1143,
      "context": "[^fn-stop-sign-attack]: **Stop Sign Attack Precision**: The 2017 study by Eykholt et al. achieved 100% success rate under...",
      "full_line": "[^fn-stop-sign-attack]: **Stop Sign Attack Precision**: The 2017 study by Eykholt et al. achieved 100% success rate under laboratory conditions and 84.8% success rate in real-world conditions using just 4 small rectangular stickers. The attack worked across multiple viewing angles (up to 45 degrees) and distances (5-55 feet), demonstrating the robustness of physical adversarial attacks."
    },
    {
      "footnote_id": "fn-defensive-distillation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1155,
      "context": "...antly increase training time and computational requirements [@bai2021recent]. Defensive distillation[^fn-defensive-distillation] and input preprocessing also show promise as complementary approaches.",
      "full_line": "Defending against adversarial attacks often requires additional computational resources and can impact the overall system performance. Techniques like adversarial training, where models are trained on adversarial examples to improve robustness, can significantly increase training time and computational requirements [@bai2021recent]. Defensive distillation[^fn-defensive-distillation] and input preprocessing also show promise as complementary approaches."
    },
    {
      "footnote_id": "fn-defensive-distillation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1157,
      "context": "[^fn-defensive-distillation]: **Defensive Distillation Effectiveness**: Introduced by Papernot et al. (2016), defensive distilla...",
      "full_line": "[^fn-defensive-distillation]: **Defensive Distillation Effectiveness**: Introduced by Papernot et al. (2016), defensive distillation reduces adversarial attack success rate from 95% to 0.5% on MNIST against FGSM. However, it was later shown to be vulnerable to C&W attacks, highlighting the ongoing arms race between attacks and defenses. Runtime detection and mitigation mechanisms, such as input preprocessing [@addepalli2020towards] or prediction consistency checks, introduce latency and affect the real-time performance of ML systems."
    },
    {
      "footnote_id": "fn-poisoning-detection-difficulty",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1165,
      "context": "Data poisoning[^fn-poisoning-detection-difficulty] presents a critical challenge to the integrity and reliability of machine learning systems. By intr...",
      "full_line": "Data poisoning[^fn-poisoning-detection-difficulty] presents a critical challenge to the integrity and reliability of machine learning systems. By introducing carefully crafted malicious data into the training pipeline, adversaries can subtly manipulate model behavior in ways that are difficult to detect through standard validation procedures."
    },
    {
      "footnote_id": "fn-poisoning-detection-difficulty",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1167,
      "context": "[^fn-poisoning-detection-difficulty]: **Data Poisoning Detection Challenge**: Studies show that poisoning attacks with as little as 0.1%...",
      "full_line": "[^fn-poisoning-detection-difficulty]: **Data Poisoning Detection Challenge**: Studies show that poisoning attacks with as little as 0.1% corrupted training data can reduce model accuracy by 10-20% while remaining undetectable by standard statistical tests. The stealthiness comes from crafting poisoned samples that follow the same distribution as clean data. Unlike adversarial examples, which target models at inference time, poisoning attacks exploit upstream components of the system\u2014such as data collection, labeling, or ingestion. As ML systems are increasingly deployed in automated and high-stakes environments, understanding how poisoning occurs and how it propagates through the system is essential for developing effective defenses."
    },
    {
      "footnote_id": "fn-label-flip-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1185,
      "context": "...soning attacks have been identified in the literature [@oprea2022poisoning]. In availability attacks[^fn-label-flip-impact], a substantial portion of the training data is poisoned with the aim of degrading overall model per...",
      "full_line": "Four main categories of poisoning attacks have been identified in the literature [@oprea2022poisoning]. In availability attacks[^fn-label-flip-impact], a substantial portion of the training data is poisoned with the aim of degrading overall model performance. A classic example involves flipping labels\u2014for instance, systematically changing instances with true label $y = 1$ to $y = 0$ in a binary classification task."
    },
    {
      "footnote_id": "fn-label-flip-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1187,
      "context": "[^fn-label-flip-impact]: **Label Flipping Attack Impact**: Research shows that flipping just 40% of training labels can red...",
      "full_line": "[^fn-label-flip-impact]: **Label Flipping Attack Impact**: Research shows that flipping just 40% of training labels can reduce model accuracy from 92% to 60% on CIFAR-10. More sophisticated attacks can achieve similar degradation with only 10% label corruption by targeting specific classes or using gradient-based selection of samples to flip. These attacks render the model unreliable across a wide range of inputs, effectively making it unusable."
    },
    {
      "footnote_id": "fn-backdoor-success-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1191,
      "context": "Backdoor poisoning[^fn-backdoor-success-rate] introduces hidden triggers into training data\u2014subtle patterns or features that the model learns to...",
      "full_line": "Backdoor poisoning[^fn-backdoor-success-rate] introduces hidden triggers into training data\u2014subtle patterns or features that the model learns to associate with a particular output. When the trigger appears at inference time, the model is manipulated into producing a predetermined response."
    },
    {
      "footnote_id": "fn-backdoor-success-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1193,
      "context": "[^fn-backdoor-success-rate]: **Backdoor Attack Effectiveness**: BadNets, a seminal backdoor attack, achieves 99%+ attack succes...",
      "full_line": "[^fn-backdoor-success-rate]: **Backdoor Attack Effectiveness**: BadNets, a seminal backdoor attack, achieves 99%+ attack success rate (when trigger is present) while maintaining 95%+ accuracy on clean inputs. The attack succeeds with as few as 1% poisoned training samples, making it extremely stealthy and difficult to detect through standard validation. These attacks are often effective even if the trigger pattern is imperceptible to human observers."
    },
    {
      "footnote_id": "fn-online-poisoning-vulnerability",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1221,
      "context": "Online learning systems[^fn-online-poisoning-vulnerability] represent another unique attack surface. These systems continuously adapt to new data streams, maki...",
      "full_line": "Online learning systems[^fn-online-poisoning-vulnerability] represent another unique attack surface. These systems continuously adapt to new data streams, making them particularly susceptible to gradual poisoning. An attacker may introduce malicious samples incrementally, causing slow but steady shifts in model behavior."
    },
    {
      "footnote_id": "fn-online-poisoning-vulnerability",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1223,
      "context": "[^fn-online-poisoning-vulnerability]: **Online Learning Vulnerability**: Studies show that online learning systems can be compromised wi...",
      "full_line": "[^fn-online-poisoning-vulnerability]: **Online Learning Vulnerability**: Studies show that online learning systems can be compromised with as little as 1% poisoned data per update batch. The attack is particularly effective because the model has no opportunity to \"forget\" the poisoned patterns, leading to permanent degradation that compounds over time. This form of attack is illustrated in @fig-poisoning-attack-example."
    },
    {
      "footnote_id": "fn-modern-fault-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2311,
      "context": "...and assumptions are grounded in a different fault model entirely. Modern fault injection frameworks[^fn-modern-fault-frameworks] address this by supporting multiple fault models within a single tool.",
      "full_line": "The choice of fault or error model is central to robustness evaluation. For example, a system built to study single-bit transient faults [@sangchoolie2017one] will not offer meaningful insight into the effects of permanent multi-bit faults [@wilkening2014calculating], since its design and assumptions are grounded in a different fault model entirely. Modern fault injection frameworks[^fn-modern-fault-frameworks] address this by supporting multiple fault models within a single tool."
    },
    {
      "footnote_id": "fn-modern-fault-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2313,
      "context": "[^fn-modern-fault-frameworks]: **Fault Injection Framework Evolution**: Modern tools like PyTorchFI support 15+ fault models incl...",
      "full_line": "[^fn-modern-fault-frameworks]: **Fault Injection Framework Evolution**: Modern tools like PyTorchFI support 15+ fault models including bit-flips, stuck-at faults, and Byzantine failures. These frameworks can inject faults at multiple abstraction levels\u2014from hardware simulation (gem5) to Python tensor operations\u2014enabling comprehensive robustness evaluation across the entire stack."
    },
    {
      "footnote_id": "fn-single-vs-multi-bit",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2317,
      "context": "...rch has consistently demonstrated that single-bit faults cause more disruption than multi-bit faults[^fn-single-vs-multi-bit], whether examining hardware-level effects or software-visible impacts [@sangchoolie2017one; @papadi...",
      "full_line": "Interestingly, certain fault behavior patterns remain consistent regardless of abstraction level. For example, research has consistently demonstrated that single-bit faults cause more disruption than multi-bit faults[^fn-single-vs-multi-bit], whether examining hardware-level effects or software-visible impacts [@sangchoolie2017one; @papadimitriou2021demystifying]."
    },
    {
      "footnote_id": "fn-single-vs-multi-bit",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2319,
      "context": "[^fn-single-vs-multi-bit]: **Single vs Multi-bit Fault Impact**: Counter-intuitively, single-bit faults are often more danger...",
      "full_line": "[^fn-single-vs-multi-bit]: **Single vs Multi-bit Fault Impact**: Counter-intuitively, single-bit faults are often more dangerous than multi-bit faults in ML systems. Single-bit errors can flip sign bits, causing massive value changes (e.g., +127 to -128 in 8-bit integers), while multi-bit errors often create invalid encodings that are detected and filtered out by error-checking mechanisms. However, other important behaviors like error masking [@mohanram2003partial] may only be observable at lower abstraction levels. As illustrated in @fig-error-masking, this masking phenomenon can cause faults to be filtered out before they propagate to higher levels, meaning software-based tools may miss these effects entirely."
    },
    {
      "footnote_id": "fn-fidelity-tool-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2396,
      "context": "To address these discrepancies, tools like Fidelity [@he2020fidelity][^fn-fidelity-tool-adoption] have been developed to align fault models across abstraction layers.",
      "full_line": "To address these discrepancies, tools like Fidelity [@he2020fidelity][^fn-fidelity-tool-adoption] have been developed to align fault models across abstraction layers."
    },
    {
      "footnote_id": "fn-fidelity-tool-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2398,
      "context": "[^fn-fidelity-tool-adoption]: **Fault Injection Tool Ecosystem**: The Fidelity framework bridges hardware and software fault mod...",
      "full_line": "[^fn-fidelity-tool-adoption]: **Fault Injection Tool Ecosystem**: The Fidelity framework bridges hardware and software fault models with 95% correlation accuracy. Other popular tools include PyTorchFI (2,000+ GitHub stars), TensorFI for TensorFlow, and BinFI for binary-level injection. This ecosystem reflects the growing need for systematic fault tolerance evaluation in ML systems. By mapping software-observed fault behaviors to corresponding hardware-level patterns [@cheng2016clear], Fidelity offers a more accurate means of simulating hardware faults at the software level. While lower-level tools capture the true propagation of errors through a hardware system, they are generally slower and more complex. Software-level tools, such as those implemented in PyTorch or TensorFlow, are faster and easier to use for large-scale robustness testing, albeit with less precision."
    },
    {
      "footnote_id": "fn-hardware-injection-accuracy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2402,
      "context": "Hardware-based fault injection methods[^fn-hardware-injection-accuracy] allow researchers to directly introduce faults into physical systems and observe their effects on m...",
      "full_line": "Hardware-based fault injection methods[^fn-hardware-injection-accuracy] allow researchers to directly introduce faults into physical systems and observe their effects on machine learning (ML) models. These approaches are essential for validating assumptions made in software-level fault injection tools and for studying how real-world hardware faults influence system behavior."
    },
    {
      "footnote_id": "fn-hardware-injection-accuracy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2404,
      "context": "[^fn-hardware-injection-accuracy]: **Hardware vs Software Injection**: Hardware-based fault injection using techniques like laser fau...",
      "full_line": "[^fn-hardware-injection-accuracy]: **Hardware vs Software Injection**: Hardware-based fault injection using techniques like laser fault injection or electromagnetic pulse can achieve 99%+ accuracy in replicating real faults, compared to 70-85% accuracy for software simulation. However, hardware injection takes 100x longer and costs 1000x more, limiting its use to critical validation scenarios. While most error injection tools used in ML robustness research are software-based, because of their speed and scalability, hardware-based approaches remain critical for grounding higher-level error models. They are considered the most accurate means of studying the impact of faults on ML systems by manipulating the hardware directly to introduce errors."
    },
    {
      "footnote_id": "fn-ewaste-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 36,
      "context": "...beyond energy consumption, encompassing carbon emissions, resource extraction, and electronic waste[^fn-ewaste-scale]. As a result, it is imperative to examine AI systems through the lens of sustainability and assess...",
      "full_line": "Machine learning has become an essential driver of technological progress, powering advancements across industries and scientific domains. However, as AI models grow in complexity and scale, the computational demands required to train and deploy them have increased significantly, raising critical concerns about sustainability. The environmental impact of AI extends beyond energy consumption, encompassing carbon emissions, resource extraction, and electronic waste[^fn-ewaste-scale]. As a result, it is imperative to examine AI systems through the lens of sustainability and assess the trade-offs between performance and ecological responsibility."
    },
    {
      "footnote_id": "fn-ewaste-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 38,
      "context": "[^fn-ewaste-scale]: **E-Waste from Computing**: Global e-waste reached 54 million metric tons in 2019, with computing...",
      "full_line": "[^fn-ewaste-scale]: **E-Waste from Computing**: Global e-waste reached 54 million metric tons in 2019, with computing equipment contributing 15%. AI hardware accelerates this trend\u2014NVIDIA's GPU sales increased 200% from 2020-2023, with each high-end GPU weighing 2-4 lbs and containing toxic materials requiring specialized disposal. The rapid obsolescence cycle means AI hardware often becomes e-waste within 3-5 years."
    },
    {
      "footnote_id": "fn-household-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 40,
      "context": "...housands of megawatt-hours of electricity, equivalent to powering hundreds of households for a month[^fn-household-energy]. Much of this energy is supplied by data centers, which rely heavily on nonrenewable energy sources...",
      "full_line": "Developing large-scale AI models, such as state-of-the-art language and vision models, requires substantial computational power. Training a single large model can consume thousands of megawatt-hours of electricity, equivalent to powering hundreds of households for a month[^fn-household-energy]. Much of this energy is supplied by data centers, which rely heavily on nonrenewable energy sources, contributing to global carbon emissions."
    },
    {
      "footnote_id": "fn-household-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 42,
      "context": "[^fn-household-energy]: **Household Energy Comparison**: The average U.S. household consumes 10,500 kWh annually (about 87...",
      "full_line": "[^fn-household-energy]: **Household Energy Comparison**: The average U.S. household consumes 10,500 kWh annually (about 875 kWh monthly). Training GPT-4 consumed an estimated 50,000-100,000 MWh (50-100 million kWh), equivalent to 5-10 years of electricity for 10,000 households. This single training run used more energy than entire small cities like Aspen, Colorado (population 7,400) consume annually."
    },
    {
      "footnote_id": "fn-industry-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 44,
      "context": "[^fn-industry-comparison]: **AI vs Industrial Emissions**: Machine learning workloads are projected to produce 3.5% of global...",
      "full_line": "[^fn-industry-comparison]: **AI vs Industrial Emissions**: Machine learning workloads are projected to produce 3.5% of global carbon emissions by 2030, surpassing aviation (2.8%) and approaching cement production (4%). Current AI emissions already exceed those of Argentina (0.7 billion tons CO\u2082 annually). Training just the top 10 large language models in 2023 generated emissions equivalent to 40,000 round-trip flights from New York to London."
    },
    {
      "footnote_id": "fn-gpu-manufacturing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 46,
      "context": "...tilization. Training and inference workloads depend on specialized processors, such as GPUs and TPUs[^fn-gpu-manufacturing], which require rare earth metals whose extraction and processing generate significant pollution. Ad...",
      "full_line": "Beyond energy consumption, AI systems also impact the environment through hardware manufacturing and resource utilization. Training and inference workloads depend on specialized processors, such as GPUs and TPUs[^fn-gpu-manufacturing], which require rare earth metals whose extraction and processing generate significant pollution. Additionally, the growing demand for AI applications accelerates electronic waste production, as hardware rapidly becomes obsolete."
    },
    {
      "footnote_id": "fn-gpu-manufacturing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 48,
      "context": "[^fn-gpu-manufacturing]: **GPU Manufacturing Impact**: Producing a single high-end GPU like the NVIDIA H100 generates 300-5...",
      "full_line": "[^fn-gpu-manufacturing]: **GPU Manufacturing Impact**: Producing a single high-end GPU like the NVIDIA H100 generates 300-500 kg of CO\u2082 before any computation occurs. Manufacturing requires 2,500+ liters of ultrapure water, 15+ rare earth elements, and energy-intensive processes reaching 1,000\u00b0C. TSMC's advanced 4nm process consumes 40% more energy per wafer than older 7nm nodes, increasing embodied carbon in AI accelerators. Even small-scale AI systems, such as those deployed on edge devices, contribute to sustainability challenges, necessitating careful consideration of their lifecycle impact."
    },
    {
      "footnote_id": "fn-ai-compute-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 56,
      "context": "...ecedented rate, with compute requirements increasing 350,000\u00d7 from 2012 to 2019 [@schwartz2020green][^fn-ai-compute-growth]. This trend shows no signs of slowing down, as advancements in deep learning continue to prioritize...",
      "full_line": "The long-term sustainability of AI is increasingly challenged by the exponential growth of computational demands required to train and deploy machine learning models. Over the past decade, AI systems have scaled at an unprecedented rate, with compute requirements increasing 350,000\u00d7 from 2012 to 2019 [@schwartz2020green][^fn-ai-compute-growth]. This trend shows no signs of slowing down, as advancements in deep learning continue to prioritize larger models with more parameters, larger training datasets, and higher computational complexity. However, sustaining this trajectory poses significant sustainability challenges, particularly as the efficiency gains from hardware improvements fail to keep pace with the rising demands of AI workloads."
    },
    {
      "footnote_id": "fn-ai-compute-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 58,
      "context": "[^fn-ai-compute-growth]: **AI Compute Explosion**: This 350,000\u00d7 increase represents a doubling time of approximately 3.4 m...",
      "full_line": "[^fn-ai-compute-growth]: **AI Compute Explosion**: This 350,000\u00d7 increase represents a doubling time of approximately 3.4 months\u2014far exceeding Moore's Law's 2-year doubling cycle. For comparison, this is equivalent to going from the computational power of a smartphone to that of the world's largest supercomputer. The trend has only accelerated with large language models: GPT-4's training is estimated to have required 25\u00d7 more compute than GPT-3, while models like PaLM-2 and Claude used even more computational resources."
    },
    {
      "footnote_id": "fn-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 60,
      "context": "...storically, computational efficiency improved with advances in semiconductor technology. Moore's Law[^fn-moores-law], which predicted that the number of transistors on a chip would double approximately every two year...",
      "full_line": "Historically, computational efficiency improved with advances in semiconductor technology. Moore's Law[^fn-moores-law], which predicted that the number of transistors on a chip would double approximately every two years, led to continuous improvements in processing power and energy efficiency. However, Moore's Law is now reaching fundamental physical limits, making further transistor scaling increasingly difficult and costly. Dennard scaling, which once ensured that smaller transistors would operate at lower power levels, has also ended, leading to stagnation in energy efficiency improvements per transistor. As a result, while AI models continue to scale in size and capability, the hardware running these models is no longer improving at the same exponential rate. This growing divergence between computational demand and hardware efficiency creates an unsustainable trajectory in which AI consumes ever-increasing amounts of energy."
    },
    {
      "footnote_id": "fn-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 62,
      "context": "[^fn-moores-law]: **Moore's Law Origins**: Named after Intel co-founder Gordon Moore, who made this observation in a...",
      "full_line": "[^fn-moores-law]: **Moore's Law Origins**: Named after Intel co-founder Gordon Moore, who made this observation in a 1965 *Electronics* magazine article, Moore's Law has driven the semiconductor industry for nearly 60 years. Moore initially predicted a doubling every year, later revised to two years. The law's economic impact is staggering: it enabled the $4 trillion global electronics industry and made possible everything from smartphones to supercomputers. However, at 3nm process nodes, individual atoms become the limiting factor."
    },
    {
      "footnote_id": "fn-gpt3-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 64,
      "context": "...quivalent to the monthly energy consumption of 1,450 average U.S. households [@maslej2023artificial][^fn-gpt3-energy]. In recent years, these generative AI models have gained increasing popularity, leading to more mod...",
      "full_line": "The training of complex AI systems like large deep learning models demands startlingly high levels of computing power with profound energy implications. Consider OpenAI's state-of-the-art language model GPT-3 as a prime example. This system pushes the frontiers of text generation through algorithms trained on massive datasets, with training estimated to require 1,300 megawatt-hours (MWh) of electricity\u2014roughly equivalent to the monthly energy consumption of 1,450 average U.S. households [@maslej2023artificial][^fn-gpt3-energy]. In recent years, these generative AI models have gained increasing popularity, leading to more models being trained with ever-growing parameter counts."
    },
    {
      "footnote_id": "fn-gpt3-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 66,
      "context": "[^fn-gpt3-energy]: **GPT-3 Energy Consumption**: Training GPT-3 consumed approximately 1,300 MWh of electricity, equi...",
      "full_line": "[^fn-gpt3-energy]: **GPT-3 Energy Consumption**: Training GPT-3 consumed approximately 1,300 MWh of electricity, equivalent to the annual energy consumption of 130 average American homes or the same amount of CO\u2082 as burning 500,000 pounds of coal. At average US electricity prices, this training run cost roughly $130,000 in electricity alone. GPT-4, with estimated 25\u00d7 more compute, likely consumed over 30,000 MWh\u2014enough to power a small city for a month."
    },
    {
      "footnote_id": "fn-datacenter-emissions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 197,
      "context": "...lobal CO\u2082 emissions\u2014a figure that is closing in on the airline industry's footprint [@liu2020energy][^fn-datacenter-emissions]. The energy burden of AI is expected to grow exponentially due to three key factors: increasing dat...",
      "full_line": "Data centers play a central role in AI's energy demands, consuming vast amounts of electricity to power compute servers, storage, and cooling systems. Without access to renewable energy, these facilities rely heavily on nonrenewable sources such as coal and natural gas, contributing significantly to global carbon emissions. Current estimates suggest that data centers produce up to 2% of total global CO\u2082 emissions\u2014a figure that is closing in on the airline industry's footprint [@liu2020energy][^fn-datacenter-emissions]. The energy burden of AI is expected to grow exponentially due to three key factors: increasing data center capacity, rising AI training workloads, and surging inference demands [@patterson2022carbon]. Without intervention, these trends risk making AI's environmental footprint unsustainably large [@thompson2023compute]."
    },
    {
      "footnote_id": "fn-datacenter-emissions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 199,
      "context": "[^fn-datacenter-emissions]: **Data Center Climate Impact**: Data centers consume approximately 1% of global electricity and pr...",
      "full_line": "[^fn-datacenter-emissions]: **Data Center Climate Impact**: Data centers consume approximately 1% of global electricity and produce 0.3% of global carbon emissions directly. However, when including embodied carbon from hardware manufacturing, the figure rises to 2%. For perspective, this equals the annual emissions of Argentina (1.8% of global total) and exceeds the aviation industry's 2.1%. The largest hyperscale data centers consume over 100 MW continuously\u2014equivalent to powering 80,000 homes."
    },
    {
      "footnote_id": "fn-hyperscale-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 203,
      "context": "...ers spanning multiple football fields in size, housing hundreds of thousands of AI-optimized servers[^fn-hyperscale-size]. The training of large language models (LLMs) such as GPT-4 required over 25,000 Nvidia A100 GPUs r...",
      "full_line": "AI workloads are among the most compute-intensive operations in modern data centers. Companies such as Meta operate hyperscale data centers spanning multiple football fields in size, housing hundreds of thousands of AI-optimized servers[^fn-hyperscale-size]. The training of large language models (LLMs) such as GPT-4 required over 25,000 Nvidia A100 GPUs running continuously for 90 to 100 days [@semianalysisGPT4], consuming thousands of megawatt-hours (MWh) of electricity. These facilities rely on high-performance AI accelerators like NVIDIA DGX H100 units, each of which can draw up to 10.2 kW at peak power [@nvidiadgxH100]."
    },
    {
      "footnote_id": "fn-hyperscale-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 205,
      "context": "[^fn-hyperscale-size]: **Hyperscale Data Center Scale**: Meta's Prineville data center spans 2.5 million square feet (57...",
      "full_line": "[^fn-hyperscale-size]: **Hyperscale Data Center Scale**: Meta's Prineville data center spans 2.5 million square feet (57 football fields) and houses 150,000+ servers. Microsoft's largest Azure data center in Iowa covers 700 acres with power capacity of 300 MW. Google operates 21 hyperscale facilities globally, consuming 12.2 TWh annually\u2014more electricity than entire countries like Lithuania or Sri Lanka."
    },
    {
      "footnote_id": "fn-tsmc-water",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 599,
      "context": "...r example, TSMC's latest fab in Arizona is projected to consume 8.9 million gallons of water per day[^fn-tsmc-water], accounting for nearly 3% of the city's total water production. This demand places significant stra...",
      "full_line": "Semiconductor fabrication is an exceptionally water-intensive process, requiring vast quantities of ultrapure water  for cleaning, cooling, and chemical processing. The scale of water consumption in modern fabs is comparable to that of entire urban populations. For example, TSMC's latest fab in Arizona is projected to consume 8.9 million gallons of water per day[^fn-tsmc-water], accounting for nearly 3% of the city's total water production. This demand places significant strain on local water resources, particularly in water-scarce regions such as Taiwan, Arizona, and Singapore, where semiconductor manufacturing is concentrated. Semiconductor companies have recognized this challenge and are actively investing in recycling technologies and more efficient water management practices. STMicroelectronics, for example, recycles and reuses approximately 41% of its water, significantly reducing its environmental footprint. @fig-water_cycle illustrates the typical semiconductor fab water cycle, showing the stages from raw water intake to wastewater treatment and reuse."
    },
    {
      "footnote_id": "fn-tsmc-water",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 605,
      "context": "[^fn-tsmc-water]: **Semiconductor Water Consumption Scale**: TSMC's Arizona facility will consume 3.2 billion gallon...",
      "full_line": "[^fn-tsmc-water]: **Semiconductor Water Consumption Scale**: TSMC's Arizona facility will consume 3.2 billion gallons annually\u2014equivalent to 37,000 Olympic swimming pools. Each AI chip requires 5-10x more water than traditional processors due to advanced nodes and complex manufacturing. Intel's Ireland fab uses 1.5 billion gallons annually, while Samsung's Texas facility is projected to use 6 million gallons daily. Water treatment and purification add 30-50% to total consumption. During peak summer months, the cumulative daily water consumption of major fabs rivals that of cities with populations exceeding half a million people[^fn-fab-vs-cities]."
    },
    {
      "footnote_id": "fn-fab-vs-cities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 605,
      "context": "...ter consumption of major fabs rivals that of cities with populations exceeding half a million people[^fn-fab-vs-cities].",
      "full_line": "[^fn-tsmc-water]: **Semiconductor Water Consumption Scale**: TSMC's Arizona facility will consume 3.2 billion gallons annually\u2014equivalent to 37,000 Olympic swimming pools. Each AI chip requires 5-10x more water than traditional processors due to advanced nodes and complex manufacturing. Intel's Ireland fab uses 1.5 billion gallons annually, while Samsung's Texas facility is projected to use 6 million gallons daily. Water treatment and purification add 30-50% to total consumption. During peak summer months, the cumulative daily water consumption of major fabs rivals that of cities with populations exceeding half a million people[^fn-fab-vs-cities]."
    },
    {
      "footnote_id": "fn-fab-vs-cities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 609,
      "context": "[^fn-fab-vs-cities]: **Fab Water Usage vs Cities**: Taiwan's semiconductor industry consumes 150 million tons of water...",
      "full_line": "[^fn-fab-vs-cities]: **Fab Water Usage vs Cities**: Taiwan's semiconductor industry consumes 150 million tons of water annually\u20145% of the island's total usage, equivalent to cities like Boston (685,000 people). During Taiwan's 2021 drought, fabs were allocated water priority over farmers, highlighting resource conflicts. TSMC alone uses more water than cities like Tampa, Florida, leading to groundwater depletion rates of 1-2 meters annually in Hsinchu."
    },
    {
      "footnote_id": "fn-chemical-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 623,
      "context": "...immense, with thousands of metric tons of hazardous substances consumed annually [@kim2018chemical][^fn-chemical-scale].",
      "full_line": "Semiconductor fabrication is heavily reliant on highly hazardous chemicals, which play an essential role in processes such as etching, doping, and wafer cleaning. The manufacturing of AI hardware, including GPUs, TPUs, and other specialized accelerators, requires the use of strong acids, volatile solvents, and toxic gases, all of which pose significant health and environmental risks if not properly managed. The scale of chemical usage in fabs is immense, with thousands of metric tons of hazardous substances consumed annually [@kim2018chemical][^fn-chemical-scale]."
    },
    {
      "footnote_id": "fn-chemical-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 631,
      "context": "[^fn-chemical-scale]: **Hazardous Chemical Quantities**: A typical large semiconductor fab uses 500+ different chemicals...",
      "full_line": "[^fn-chemical-scale]: **Hazardous Chemical Quantities**: A typical large semiconductor fab uses 500+ different chemicals annually, consuming 500-2,000 metric tons of acids, 50-200 metric tons of solvents, and 10-50 tons of toxic gases. Arsine gas is lethal at 3 parts per million over 30 minutes. TSMC's facilities store over 50,000 tons of chemicals on-site, requiring specialized emergency response teams and $100+ million in safety infrastructure per fab. Any leaks or accidental releases in fabs can lead to severe health hazards for workers and surrounding communities."
    },
    {
      "footnote_id": "fn-indium-supply",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 643,
      "context": "...lies expected to last fewer than 15 years at the current rate of consumption [@davies2011endangered][^fn-indium-supply].",
      "full_line": "Although silicon forms the foundation of semiconductor devices, high-performance AI chips depend on rare elements such as gallium, indium, and arsenic, which are essential for high-speed, low-power electronic components [@chen2006gallium]. Gallium and indium, for example, are widely used in compound semiconductors, particularly for 5G communications, optoelectronics, and AI accelerators. The United States Geological Survey (USGS) has classified indium as a critical material, with global supplies expected to last fewer than 15 years at the current rate of consumption [@davies2011endangered][^fn-indium-supply]."
    },
    {
      "footnote_id": "fn-indium-supply",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 647,
      "context": "[^fn-indium-supply]: **Critical Material Scarcity**: Indium production is only 600-800 tons annually worldwide, with Ch...",
      "full_line": "[^fn-indium-supply]: **Critical Material Scarcity**: Indium production is only 600-800 tons annually worldwide, with China controlling 60% of supply. Prices fluctuate wildly\u2014from $60/kg in 2002 to $1,000/kg in 2005, now around $400/kg. Each smartphone contains 0.3mg of indium; each AI accelerator contains 50-100x more. At current AI hardware growth rates (40% annually), demand will exceed supply by 2035 without recycling breakthroughs. As AI hardware manufacturing scales, the demand for helium will continue to grow, necessitating more sustainable extraction and recycling practices."
    },
    {
      "footnote_id": "fn-china-ree-control",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 649,
      "context": "...lenges. China currently dominates over 90% of the world's rare earth element (REE) refining capacity[^fn-china-ree-control], including materials essential for AI chips, such as neodymium (for high-performance magnets in AI...",
      "full_line": "Beyond raw material availability, the geopolitical control of rare earth elements poses additional challenges. China currently dominates over 90% of the world's rare earth element (REE) refining capacity[^fn-china-ree-control], including materials essential for AI chips, such as neodymium (for high-performance magnets in AI accelerators) and yttrium (for high-temperature superconductors) [@jha2014rare]. This concentration of supply creates supply chain vulnerabilities, as trade restrictions or geopolitical tensions could severely impact AI hardware production."
    },
    {
      "footnote_id": "fn-china-ree-control",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 651,
      "context": "[^fn-china-ree-control]: **Chinese Rare Earth Dominance**: China produces 85% of rare earth elements and controls 95% of gl...",
      "full_line": "[^fn-china-ree-control]: **Chinese Rare Earth Dominance**: China produces 85% of rare earth elements and controls 95% of global refining capacity. The 2010 China-Japan diplomatic crisis saw rare earth exports to Japan cut by 40%, causing prices to spike 2,000%. A single NVIDIA H100 contains 17 different rare earth elements totaling 200-300 grams. U.S. strategic reserves contain only 3-month supply, while building alternative supply chains requires 10-15 years and $50+ billion investment."
    },
    {
      "footnote_id": "fn-jevons-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 989,
      "context": "...ental principle that must guide all efforts to mitigate AI's environmental impact is Jevon's Paradox[^fn-jevons-paradox]. This paradox, observed by William Stanley Jevons in the 19th century [@jevons1865coal], says that...",
      "full_line": "A fundamental principle that must guide all efforts to mitigate AI's environmental impact is Jevon's Paradox[^fn-jevons-paradox]. This paradox, observed by William Stanley Jevons in the 19th century [@jevons1865coal], says that improvements in technological efficiency can lead to an increase in overall consumption. In the context of AI, even as we develop more energy-efficient models and hardware, the increased accessibility and adoption of AI technologies could lead to a net increase in energy consumption and resource utilization. Therefore, we must approach mitigation strategies with a keen awareness of this potential rebound effect, ensuring that efficiency gains do not inadvertently drive greater consumption. This section explores key strategies for mitigating AI's environmental impact, beginning with sustainable AI development principles."
    },
    {
      "footnote_id": "fn-jevons-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 991,
      "context": "[^fn-jevons-paradox]: **Jevon's Paradox**: Named after British economist William Stanley Jevons who observed in 1865 tha...",
      "full_line": "[^fn-jevons-paradox]: **Jevon's Paradox**: Named after British economist William Stanley Jevons who observed in 1865 that improving coal efficiency actually increased total coal consumption rather than reducing it. Modern examples include LEDs\u2014despite being 85% more efficient than incandescent bulbs, total lighting energy consumption has increased due to expanded usage. In AI, this means that making models 10\u00d7 more efficient might lead to 100\u00d7 more AI applications, resulting in net increase in environmental impact."
    },
    {
      "footnote_id": "fn-pue-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1090,
      "context": "...can consume over 100 megawatts of power, a level comparable to the electricity usage of a small city[^fn-pue-efficiency]. Without intervention, the continued growth of AI workloads threatens to push the energy consumptio...",
      "full_line": "The increasing computational demands of AI have made data centers one of the largest consumers of electricity in the digital economy. Large-scale cloud data centers provide the infrastructure necessary for training and deploying machine learning models, but their energy consumption is substantial. A single hyperscale data center can consume over 100 megawatts of power, a level comparable to the electricity usage of a small city[^fn-pue-efficiency]. Without intervention, the continued growth of AI workloads threatens to push the energy consumption of data centers beyond sustainable levels."
    },
    {
      "footnote_id": "fn-pue-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1092,
      "context": "[^fn-pue-efficiency]: **Power Usage Effectiveness**: Data center efficiency is measured by PUE (Power Usage Effectivenes...",
      "full_line": "[^fn-pue-efficiency]: **Power Usage Effectiveness**: Data center efficiency is measured by PUE (Power Usage Effectiveness)\u2014total facility power divided by IT equipment power. Industry average PUE is 1.67 (67% overhead for cooling/infrastructure), but leading hyperscalers achieve 1.1-1.2. Google's best data centers reach PUE of 1.08, meaning only 8% energy overhead. Each 0.1 PUE improvement saves millions annually in electricity costs. The industry must adopt strategies to optimize power efficiency, integrate renewable energy sources, and improve cooling mechanisms to mitigate the environmental impact of AI infrastructure."
    },
    {
      "footnote_id": "fn-google-carbon-free",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1094,
      "context": ".... Google, for example, has set a goal to operate its data centers on 24/7 carbon-free energy by 2030[^fn-google-carbon-free], ensuring that every unit of electricity consumed is matched with renewable generation rather than...",
      "full_line": "One of the most promising approaches to reducing data center emissions is the transition to renewable energy. Major cloud providers, including Google, Microsoft, and Amazon Web Services, have committed to powering their data centers with renewable energy, but implementation challenges remain. Unlike fossil fuel plants, which provide consistent electricity output, renewable sources such as wind and solar are intermittent, with generation levels fluctuating throughout the day. To address this variability, AI infrastructure must incorporate energy storage solutions, such as large-scale battery deployments, and implement intelligent scheduling mechanisms that shift AI workloads to times when renewable energy availability is highest. Google, for example, has set a goal to operate its data centers on 24/7 carbon-free energy by 2030[^fn-google-carbon-free], ensuring that every unit of electricity consumed is matched with renewable generation rather than relying on carbon offsets alone."
    },
    {
      "footnote_id": "fn-google-carbon-free",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1096,
      "context": "[^fn-google-carbon-free]: **Google's Carbon-Free Commitment**: Google achieved carbon neutrality in 2007 and has been carbon...",
      "full_line": "[^fn-google-carbon-free]: **Google's Carbon-Free Commitment**: Google achieved carbon neutrality in 2007 and has been carbon neutral for 15 years, but 24/7 carbon-free energy is more ambitious\u2014requiring real-time matching of energy consumption with clean generation. Currently at 64% carbon-free energy globally. Denmark data centers run on 100% wind power, while others still rely on grid renewables certificates. This requires $15 billion+ investment in clean energy projects worldwide."
    },
    {
      "footnote_id": "fn-cooling-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1098,
      "context": "...gy footprint of data centers, often accounting for 30 to 40 percent of total electricity consumption[^fn-cooling-energy]. Traditional cooling methods rely on air conditioning units and mechanical chillers, both of which...",
      "full_line": "Cooling systems represent another major contributor to the energy footprint of data centers, often accounting for 30 to 40 percent of total electricity consumption[^fn-cooling-energy]. Traditional cooling methods rely on air conditioning units and mechanical chillers, both of which require significant power and water resources."
    },
    {
      "footnote_id": "fn-cooling-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1100,
      "context": "[^fn-cooling-energy]: **Data Center Cooling Costs**: Cooling consumes 38% of total data center energy on average. A typi...",
      "full_line": "[^fn-cooling-energy]: **Data Center Cooling Costs**: Cooling consumes 38% of total data center energy on average. A typical 10 MW data center spends $3.8 million annually on cooling electricity. Google's machine learning optimization reduced cooling energy by 40%, saving $150+ million globally. Liquid cooling can be 3,000x more efficient than air cooling for high-density AI workloads, reducing cooling energy from 40% to under 10% of total consumption. To improve efficiency, data centers are adopting alternative cooling strategies that reduce energy waste. Liquid cooling, which transfers heat away from AI accelerators using specially designed coolant systems, is significantly more effective than traditional air cooling and is now being deployed in high-density computing clusters. Free-air cooling, which utilizes natural airflow instead of mechanical refrigeration, has also been adopted in temperate climates, where external conditions allow for passive cooling. Microsoft has taken this a step further by deploying underwater data centers[^fn-underwater-dc] that use the surrounding ocean as a natural cooling mechanism, reducing the need for active temperature regulation."
    },
    {
      "footnote_id": "fn-underwater-dc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1100,
      "context": "...ow for passive cooling. Microsoft has taken this a step further by deploying underwater data centers[^fn-underwater-dc] that use the surrounding ocean as a natural cooling mechanism, reducing the need for active tempera...",
      "full_line": "[^fn-cooling-energy]: **Data Center Cooling Costs**: Cooling consumes 38% of total data center energy on average. A typical 10 MW data center spends $3.8 million annually on cooling electricity. Google's machine learning optimization reduced cooling energy by 40%, saving $150+ million globally. Liquid cooling can be 3,000x more efficient than air cooling for high-density AI workloads, reducing cooling energy from 40% to under 10% of total consumption. To improve efficiency, data centers are adopting alternative cooling strategies that reduce energy waste. Liquid cooling, which transfers heat away from AI accelerators using specially designed coolant systems, is significantly more effective than traditional air cooling and is now being deployed in high-density computing clusters. Free-air cooling, which utilizes natural airflow instead of mechanical refrigeration, has also been adopted in temperate climates, where external conditions allow for passive cooling. Microsoft has taken this a step further by deploying underwater data centers[^fn-underwater-dc] that use the surrounding ocean as a natural cooling mechanism, reducing the need for active temperature regulation."
    },
    {
      "footnote_id": "fn-underwater-dc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1102,
      "context": "[^fn-underwater-dc]: **Project Natick Underwater Data Centers**: Microsoft's underwater data center achieved 87.5% serv...",
      "full_line": "[^fn-underwater-dc]: **Project Natick Underwater Data Centers**: Microsoft's underwater data center achieved 87.5% server uptime compared to 96% on land, but operated 8x more reliably due to absence of corrosive oxygen and humidity. The sealed containers consume 1/8th the energy for cooling. However, recovery and maintenance require specialized vessels costing $500,000+ per retrieval. The project demonstrated feasibility but highlighted trade-offs between efficiency and operational complexity."
    },
    {
      "footnote_id": "fn-google-carbon-scheduling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1112,
      "context": "...bon-aware computing in its cloud infrastructure. In 2020, the company introduced a scheduling system[^fn-google-carbon-scheduling] that delays non-urgent AI tasks until times when renewable energy sources such as solar or wind pow...",
      "full_line": "Google has pioneered one of the most advanced implementations of carbon-aware computing in its cloud infrastructure. In 2020, the company introduced a scheduling system[^fn-google-carbon-scheduling] that delays non-urgent AI tasks until times when renewable energy sources such as solar or wind power are more abundant. This approach enables AI workloads to align with the natural variability of clean energy availability, reducing reliance on fossil fuels while maintaining high computational efficiency. Google has further extended this strategy by geographically distributing AI workloads, moving computations to data centers in regions where clean energy is more accessible. By shifting large-scale AI training jobs from fossil fuel-heavy grids to low-carbon power sources, the company has demonstrated that significant emissions reductions can be achieved through intelligent workload placement."
    },
    {
      "footnote_id": "fn-google-carbon-scheduling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1114,
      "context": "[^fn-google-carbon-scheduling]: **Google Carbon-Aware Scheduling Results**: Google's carbon-intelligent computing platform achieve...",
      "full_line": "[^fn-google-carbon-scheduling]: **Google Carbon-Aware Scheduling Results**: Google's carbon-intelligent computing platform achieved 15% reduction in hourly carbon footprint by shifting workloads within regions. Globally shifting workloads between data centers achieved 40% reduction. The system processes 95 billion search queries daily while optimizing for grid carbon intensity. Non-urgent tasks like batch training can shift 70% of workload to lower-carbon time periods, reducing emissions equivalent to taking 50,000 cars off the road annually."
    },
    {
      "footnote_id": "fn-grid-carbon-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1118,
      "context": "...r instance, the Electricity Maps API provides real-time CO\u2082 emissions data for power grids worldwide[^fn-grid-carbon-data], enabling AI infrastructure to adjust computational workloads based on carbon availability. As acce...",
      "full_line": "The effectiveness of carbon-aware AI scheduling depends on accurate real-time data about grid emissions. Electricity providers and sustainability organizations have begun publishing grid carbon intensity data through publicly available APIs, allowing AI systems to dynamically respond to changes in energy supply. For instance, the Electricity Maps API provides real-time CO\u2082 emissions data for power grids worldwide[^fn-grid-carbon-data], enabling AI infrastructure to adjust computational workloads based on carbon availability. As access to grid emissions data improves, carbon-aware computing will become a scalable and widely adoptable solution for reducing the environmental impact of AI operations."
    },
    {
      "footnote_id": "fn-grid-carbon-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1120,
      "context": "[^fn-grid-carbon-data]: **Real-Time Grid Carbon Intensity**: Grid carbon intensity varies dramatically\u2014from 50g CO\u2082/kWh in...",
      "full_line": "[^fn-grid-carbon-data]: **Real-Time Grid Carbon Intensity**: Grid carbon intensity varies dramatically\u2014from 50g CO\u2082/kWh in nuclear-heavy France to 820g/kWh in coal-dependent Poland. In Texas, intensity fluctuates 10x daily (150-1,500g/kWh) based on wind generation. The Electricity Maps API serves 50+ million requests daily to enable carbon-aware computing. WattTime API provides marginal emissions data showing which power plants turn on/off next, allowing 2-5x better carbon optimization than average intensity."
    },
    {
      "footnote_id": "fn-nuclear-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1128,
      "context": "...ity. Tech companies like Microsoft have shown interest in nuclear energy to power their data centers[^fn-nuclear-ai], as their more constant demand profile (compared to residential use) aligns well with nuclear gener...",
      "full_line": "To fully leverage carbon-aware scheduling for AI inference workloads, innovation in energy storage solutions is essential for consistent renewable energy use. The base energy load is currently met with nuclear energy\u2014a constant source that produces no direct carbon emissions but lacks the flexibility to accommodate renewable energy variability. Tech companies like Microsoft have shown interest in nuclear energy to power their data centers[^fn-nuclear-ai], as their more constant demand profile (compared to residential use) aligns well with nuclear generation characteristics."
    },
    {
      "footnote_id": "fn-nuclear-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1130,
      "context": "[^fn-nuclear-ai]: **Nuclear Power for AI Data Centers**: Microsoft partnered with Helion Energy for fusion power by...",
      "full_line": "[^fn-nuclear-ai]: **Nuclear Power for AI Data Centers**: Microsoft partnered with Helion Energy for fusion power by 2028, signing the first commercial fusion agreement. Amazon invested $500M in small modular reactors (SMRs) for data centers. Google is exploring 24/7 nuclear partnerships with Kairos Power. Nuclear provides 20% of U.S. electricity with 12g CO\u2082/kWh lifecycle emissions versus 820-1,050g for coal. However, new nuclear costs $150-200/MWh versus $20-40 for renewables plus storage."
    },
    {
      "footnote_id": "fn-energy-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1134,
      "context": "...ucial role. Energy-aware AI frameworks, such as Zeus [@jie2023zeus] and Perseus [@jaewon2023perseus][^fn-energy-frameworks], balance computational speed and power efficiency during both training and inference. These platfor...",
      "full_line": "Software frameworks specifically designed for energy efficiency also play a crucial role. Energy-aware AI frameworks, such as Zeus [@jie2023zeus] and Perseus [@jaewon2023perseus][^fn-energy-frameworks], balance computational speed and power efficiency during both training and inference. These platforms optimize model execution by analyzing trade-offs between speed and energy consumption, facilitating widespread adoption of energy-efficient AI strategies, particularly for inference operations that must run continuously at scale."
    },
    {
      "footnote_id": "fn-energy-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1136,
      "context": "[^fn-energy-frameworks]: **Energy-Aware AI Frameworks**: Zeus framework achieves 75% energy savings on BERT training by aut...",
      "full_line": "[^fn-energy-frameworks]: **Energy-Aware AI Frameworks**: Zeus framework achieves 75% energy savings on BERT training by automatically finding optimal energy-performance trade-offs. Perseus reduces GPU memory usage by 50% through dynamic batching, lowering energy consumption proportionally. CodeCarbon automatically tracks emissions, revealing that training can vary 10-100x in energy usage depending on optimization settings. These tools democratize energy optimization beyond just hyperscale companies."
    },
    {
      "footnote_id": "fn-eniac",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 147,
      "context": "Electronic computation began with the mainframe era. ENIAC[^fn-eniac] (1945) established the viability of electronic computation at scale, while the IBM System/360[^fn-s...",
      "full_line": "Electronic computation began with the mainframe era. ENIAC[^fn-eniac] (1945) established the viability of electronic computation at scale, while the IBM System/360[^fn-system360] (1964) introduced architectural principles of standardized instruction sets and memory hierarchies. These fundamental concepts laid the groundwork for all subsequent computing systems."
    },
    {
      "footnote_id": "fn-system360",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 147,
      "context": "...eniac] (1945) established the viability of electronic computation at scale, while the IBM System/360[^fn-system360] (1964) introduced architectural principles of standardized instruction sets and memory hierarchies....",
      "full_line": "Electronic computation began with the mainframe era. ENIAC[^fn-eniac] (1945) established the viability of electronic computation at scale, while the IBM System/360[^fn-system360] (1964) introduced architectural principles of standardized instruction sets and memory hierarchies. These fundamental concepts laid the groundwork for all subsequent computing systems."
    },
    {
      "footnote_id": "fn-cdc6600",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 149,
      "context": "...ton1965cdc] built upon these foundations while specializing for scientific computation. The CDC 6600[^fn-cdc6600] and later systems like the CM-5[^fn-cm5] [@thinking_machines_cm5] optimized for dense matrix operat...",
      "full_line": "High-performance computing (HPC) systems [@thornton1965cdc] built upon these foundations while specializing for scientific computation. The CDC 6600[^fn-cdc6600] and later systems like the CM-5[^fn-cm5] [@thinking_machines_cm5] optimized for dense matrix operations and floating-point calculations."
    },
    {
      "footnote_id": "fn-cm5",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 149,
      "context": "...e specializing for scientific computation. The CDC 6600[^fn-cdc6600] and later systems like the CM-5[^fn-cm5] [@thinking_machines_cm5] optimized for dense matrix operations and floating-point calculations.",
      "full_line": "High-performance computing (HPC) systems [@thornton1965cdc] built upon these foundations while specializing for scientific computation. The CDC 6600[^fn-cdc6600] and later systems like the CM-5[^fn-cm5] [@thinking_machines_cm5] optimized for dense matrix operations and floating-point calculations."
    },
    {
      "footnote_id": "fn-google-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 153,
      "context": "Warehouse-scale computing marked the next evolutionary step. Google's data center implementations[^fn-google-datacenter] [@barroso2003web] introduced new optimizations for internet-scale data processing. Unlike HPC syste...",
      "full_line": "Warehouse-scale computing marked the next evolutionary step. Google's data center implementations[^fn-google-datacenter] [@barroso2003web] introduced new optimizations for internet-scale data processing. Unlike HPC systems focused on tightly coupled scientific calculations, warehouse computing handled loosely coupled tasks with irregular data access patterns."
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 157,
      "context": "...tion emerged as the next frontier, building upon this accumulated architectural knowledge. AlexNet's[^fn-alexnet] [@krizhevsky2012imagenet] success in 2012 highlighted the need for further specialization. While pr...",
      "full_line": "Deep learning computation emerged as the next frontier, building upon this accumulated architectural knowledge. AlexNet's[^fn-alexnet] [@krizhevsky2012imagenet] success in 2012 highlighted the need for further specialization. While previous systems focused on either scientific calculations or independent data processing tasks, neural network training introduced new computational patterns. The training process required continuous updates to large sets of parameters, with complex data dependencies during model optimization. These workloads demanded new approaches to memory management and inter-device communication that neither HPC nor warehouse computing had fully addressed."
    },
    {
      "footnote_id": "fn-nvidia-gpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 159,
      "context": "...computing era, beginning in 2015, represents the latest step in this evolutionary chain. NVIDIA GPUs[^fn-nvidia-gpus] and Google TPUs[^fn-google-tpus] introduced hardware designs specifically optimized for neural netw...",
      "full_line": "The AI hypercomputing era, beginning in 2015, represents the latest step in this evolutionary chain. NVIDIA GPUs[^fn-nvidia-gpus] and Google TPUs[^fn-google-tpus] introduced hardware designs specifically optimized for neural network computations, moving beyond adaptations of existing architectures. These systems implemented new approaches to parallel processing, memory access, and device communication to handle the distinct patterns of model training. The resulting architectures balanced the numerical precision needs of scientific computing with the scale requirements of warehouse systems, while adding specialized support for the iterative nature of neural network optimization."
    },
    {
      "footnote_id": "fn-google-tpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 159,
      "context": "...represents the latest step in this evolutionary chain. NVIDIA GPUs[^fn-nvidia-gpus] and Google TPUs[^fn-google-tpus] introduced hardware designs specifically optimized for neural network computations, moving beyond a...",
      "full_line": "The AI hypercomputing era, beginning in 2015, represents the latest step in this evolutionary chain. NVIDIA GPUs[^fn-nvidia-gpus] and Google TPUs[^fn-google-tpus] introduced hardware designs specifically optimized for neural network computations, moving beyond adaptations of existing architectures. These systems implemented new approaches to parallel processing, memory access, and device communication to handle the distinct patterns of model training. The resulting architectures balanced the numerical precision needs of scientific computing with the scale requirements of warehouse systems, while adding specialized support for the iterative nature of neural network optimization."
    },
    {
      "footnote_id": "fn-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 192,
      "context": "...memory utilization for models containing billions of parameters. The management of data parallelism[^fn-data-parallelism], model parallelism[^fn-model-parallelism], and inter-device communication presents significant tech...",
      "full_line": "The emergence of transformer architectures and large-scale models has introduced new requirements for training systems. Contemporary implementations must efficiently process petabyte-scale datasets, orchestrate distributed training across multiple accelerators, and optimize memory utilization for models containing billions of parameters. The management of data parallelism[^fn-data-parallelism], model parallelism[^fn-model-parallelism], and inter-device communication presents significant technical challenges in modern training architectures."
    },
    {
      "footnote_id": "fn-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 192,
      "context": "...billions of parameters. The management of data parallelism[^fn-data-parallelism], model parallelism[^fn-model-parallelism], and inter-device communication presents significant technical challenges in modern training archit...",
      "full_line": "The emergence of transformer architectures and large-scale models has introduced new requirements for training systems. Contemporary implementations must efficiently process petabyte-scale datasets, orchestrate distributed training across multiple accelerators, and optimize memory utilization for models containing billions of parameters. The management of data parallelism[^fn-data-parallelism], model parallelism[^fn-model-parallelism], and inter-device communication presents significant technical challenges in modern training architectures."
    },
    {
      "footnote_id": "fn-sigmoid-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 487,
      "context": "...th software and hardware implementations. In software, this computation is expensive and inefficient[^fn-sigmoid-cost], particularly for deep networks or large datasets. Additionally, sigmoid suffers from vanishing gra...",
      "full_line": "The sigmoid function has smooth gradients and a bounded output in the range $(0, 1)$, making it useful in probabilistic settings. However, the computation of the sigmoid involves an exponential function, which becomes a key consideration in both software and hardware implementations. In software, this computation is expensive and inefficient[^fn-sigmoid-cost], particularly for deep networks or large datasets. Additionally, sigmoid suffers from vanishing gradients, especially for large input values, which can hinder the learning process in deep architectures. Its non-zero-centered output can also slow optimization, requiring more epochs to converge."
    },
    {
      "footnote_id": "fn-relu-hardware",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 501,
      "context": "...ion requires just a single comparison and conditional set, translating to minimal circuit complexity[^fn-relu-hardware]. Modern GPUs and TPUs can implement ReLU using a simple multiplexer that checks the input's sign bi...",
      "full_line": "The hardware implementation of ReLU showcases why it has become the dominant activation function in modern neural networks. Its simple $\\max(0,x)$ operation requires just a single comparison and conditional set, translating to minimal circuit complexity[^fn-relu-hardware]. Modern GPUs and TPUs can implement ReLU using a simple multiplexer that checks the input's sign bit, allowing for extremely efficient parallel processing. This hardware efficiency, combined with the sparsity it introduces, results in both reduced computation time and lower memory bandwidth requirements."
    },
    {
      "footnote_id": "fn-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 565,
      "context": "These system limitations motivated the development of more efficient optimization approaches. SGD[^fn-sgd] is a big shift in the optimization strategy. Rather than computing gradients over the entire datase...",
      "full_line": "These system limitations motivated the development of more efficient optimization approaches. SGD[^fn-sgd] is a big shift in the optimization strategy. Rather than computing gradients over the entire dataset, SGD estimates gradients using individual training examples:"
    },
    {
      "footnote_id": "fn-eniac",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 569,
      "context": "[^fn-eniac]: **ENIAC (Electronic Numerical Integrator and Computer)**: Completed in 1946 at the University of P...",
      "full_line": "[^fn-eniac]: **ENIAC (Electronic Numerical Integrator and Computer)**: Completed in 1946 at the University of Pennsylvania, ENIAC weighed 30 tons, consumed 150kW of power, and performed 5,000 operations per second. Its 17,468 vacuum tubes required constant maintenance, but it demonstrated electronic computation could be 1,000x faster than mechanical calculators."
    },
    {
      "footnote_id": "fn-system360",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 571,
      "context": "[^fn-system360]: **IBM System/360**: Launched in 1964 as a $5 billion gamble (equivalent to $40 billion today), Sys...",
      "full_line": "[^fn-system360]: **IBM System/360**: Launched in 1964 as a $5 billion gamble (equivalent to $40 billion today), System/360 introduced the revolutionary concept of backward compatibility across different computer models. Its standardized instruction set architecture became the foundation for modern computing, enabling software portability that drives today's cloud computing."
    },
    {
      "footnote_id": "fn-cdc6600",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 573,
      "context": "[^fn-cdc6600]: **CDC 6600**: Designed by Seymour Cray and released in 1964, the CDC 6600 achieved 3 MFLOPS (milli...",
      "full_line": "[^fn-cdc6600]: **CDC 6600**: Designed by Seymour Cray and released in 1964, the CDC 6600 achieved 3 MFLOPS (million floating-point operations per second) using innovative parallel processing with 10 peripheral processors. Costing $8 million ($65 million today), it was the world's fastest computer until 1969 and established supercomputing as a field."
    },
    {
      "footnote_id": "fn-cm5",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 575,
      "context": "[^fn-cm5]: **Connection Machine CM-5**: Released by Thinking Machines in 1991, the CM-5 featured up to 16,384...",
      "full_line": "[^fn-cm5]: **Connection Machine CM-5**: Released by Thinking Machines in 1991, the CM-5 featured up to 16,384 processors connected by a fat-tree network, delivering over 100 GFLOPS. Its $10-50 million price tag and specialized parallel architecture made it a favorite for scientific computing but ultimately commercially unsuccessful as commodity clusters emerged."
    },
    {
      "footnote_id": "fn-google-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 577,
      "context": "[^fn-google-datacenter]: **Google Data Centers**: Starting in 1998 with commodity PCs, Google pioneered warehouse-scale com...",
      "full_line": "[^fn-google-datacenter]: **Google Data Centers**: Starting in 1998 with commodity PCs, Google pioneered warehouse-scale computing by 2003, managing over 100,000 servers across multiple facilities. By 2020, Google operated over 20 data centers consuming 12 TWh annually\u2014equivalent to entire countries\u2014while achieving industry-leading PUE (Power Usage Effectiveness) of 1.10 through innovative cooling."
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 579,
      "context": "[^fn-alexnet]: **AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet won ImageN...",
      "full_line": "[^fn-alexnet]: **AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet won ImageNet 2012 with 15.3% error rate (vs. 26.2% for second place), using two GTX 580 GPUs for 5-6 days of training. This breakthrough launched the deep learning revolution and demonstrated that GPUs could accelerate neural network training by 10-50x over CPUs."
    },
    {
      "footnote_id": "fn-nvidia-gpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 581,
      "context": "[^fn-nvidia-gpus]: **NVIDIA AI GPUs**: From the 2012 GTX 680 (3.1 TFLOPS) used for AlexNet to the 2023 H100 (989 TFLO...",
      "full_line": "[^fn-nvidia-gpus]: **NVIDIA AI GPUs**: From the 2012 GTX 680 (3.1 TFLOPS) used for AlexNet to the 2023 H100 (989 TFLOPS for AI), NVIDIA GPUs increased AI performance by over 300x in a decade. The H100 costs $25,000-40,000 but enables training models that would be impossible on older hardware, demonstrating specialized silicon's critical role in AI advancement."
    },
    {
      "footnote_id": "fn-google-tpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 583,
      "context": "[^fn-google-tpus]: **Google TPUs**: First deployed internally in 2015, TPUs deliver 15-30x better price-performance t...",
      "full_line": "[^fn-google-tpus]: **Google TPUs**: First deployed internally in 2015, TPUs deliver 15-30x better price-performance than GPUs for specific AI workloads. The TPU v4 (2021) achieves 275 TFLOPS with 32GB memory, while TPU pods can scale to 1 exaFLOP. Google's $billions investment in custom silicon has enabled training models like PaLM (540B parameters) cost-effectively."
    },
    {
      "footnote_id": "fn-minibatch-gpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 585,
      "context": "[^fn-minibatch-gpu]: **Mini-batch GPU Optimization**: Modern GPUs achieve peak efficiency with batch sizes of 32-512 ex...",
      "full_line": "[^fn-minibatch-gpu]: **Mini-batch GPU Optimization**: Modern GPUs achieve peak efficiency with batch sizes of 32-512 examples. A Tesla V100 can process 32 ImageNet images in 15ms but would take 480ms processing them individually\u2014a 32x efficiency gain. Optimal batch size balances GPU utilization (targeting >90%) with memory constraints, typically requiring 8-16GB for batch sizes of 256-512 on modern vision models."
    },
    {
      "footnote_id": "fn-rmsprop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 587,
      "context": "[^fn-rmsprop]: **RMSprop (Root Mean Square Propagation)**: Developed by Geoffrey Hinton in 2012 for his Coursera...",
      "full_line": "[^fn-rmsprop]: **RMSprop (Root Mean Square Propagation)**: Developed by Geoffrey Hinton in 2012 for his Coursera course, RMSprop addresses AdaGrad's aggressive learning rate decay by using exponentially decaying averages. It typically achieves 20-40% faster convergence than SGD on deep networks, with the standard decay rate of 0.9 working well across most applications."
    },
    {
      "footnote_id": "fn-sigmoid-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 589,
      "context": "[^fn-sigmoid-cost]: **Sigmoid Computational Cost**: Computing sigmoid requires expensive exponential operations\u2014on CPU...",
      "full_line": "[^fn-sigmoid-cost]: **Sigmoid Computational Cost**: Computing sigmoid requires expensive exponential operations\u2014on CPU, `exp()` takes 10-20 clock cycles vs. 1 cycle for basic arithmetic. GPU implementations use 32-entry lookup tables with linear interpolation, reducing cost to 3-4 cycles but still 3x slower than ReLU. This overhead compounds in deep networks with millions of activations per forward pass."
    },
    {
      "footnote_id": "fn-relu-hardware",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 591,
      "context": "[^fn-relu-hardware]: **ReLU Hardware Efficiency**: ReLU requires just 1 instruction (`max(0,x)`) vs. sigmoid's 10+ oper...",
      "full_line": "[^fn-relu-hardware]: **ReLU Hardware Efficiency**: ReLU requires just 1 instruction (`max(0,x)`) vs. sigmoid's 10+ operations including exponentials. On NVIDIA GPUs, ReLU runs at 95% of peak FLOPS while sigmoid achieves only 30-40%. ReLU's sparsity (typically 50% zeros) enables additional optimizations: sparse matrix operations, reduced memory bandwidth, and compressed gradients during backpropagation."
    },
    {
      "footnote_id": "fn-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 593,
      "context": "[^fn-data-parallelism]: **Data Parallelism Scaling**: Linear scaling works until communication becomes the bottleneck\u2014typi...",
      "full_line": "[^fn-data-parallelism]: **Data Parallelism Scaling**: Linear scaling works until communication becomes the bottleneck\u2014typically around 64-128 GPUs for most models. BERT-Large achieves 76x speedup on 128 GPUs (59% efficiency), while GPT-3 required 1,024 GPUs with only 45% efficiency. The key constraint is AllReduce communication cost scales as O(n) with number of devices, requiring high-bandwidth interconnects like InfiniBand."
    },
    {
      "footnote_id": "fn-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 595,
      "context": "[^fn-model-parallelism]: **Model Parallelism Memory Scaling**: Enables training models too large for single GPUs\u2014GPT-3 (175...",
      "full_line": "[^fn-model-parallelism]: **Model Parallelism Memory Scaling**: Enables training models too large for single GPUs\u2014GPT-3 (175B parameters) needs 350GB just for weights in FP16, far exceeding any single GPU's 80GB maximum. However, model parallelism often achieves only 20-60% compute efficiency due to sequential dependencies between model partitions and communication overhead between devices."
    },
    {
      "footnote_id": "fn-nccl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 597,
      "context": "[^fn-nccl]: **NVIDIA NCCL (Collective Communications Library)**: Optimized for multi-GPU communication, NCCL a...",
      "full_line": "[^fn-nccl]: **NVIDIA NCCL (Collective Communications Library)**: Optimized for multi-GPU communication, NCCL achieves 90-95% of theoretical bandwidth on modern interconnects. On DGX systems with NVLink, NCCL can transfer 600 GB/s between 8 GPUs\u201450x faster than PCIe\u2014making efficient distributed training possible. It implements optimized AllReduce algorithms that reduce communication from O(n\u00b2) to O(n)."
    },
    {
      "footnote_id": "fn-gpt3-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 599,
      "context": "[^fn-gpt3-training]: **GPT-3 Training Scale**: Used 10,000 NVIDIA V100 GPUs for 3-4 months, consuming ~1,287 MWh of ene...",
      "full_line": "[^fn-gpt3-training]: **GPT-3 Training Scale**: Used 10,000 NVIDIA V100 GPUs for 3-4 months, consuming ~1,287 MWh of energy (equivalent to 120 US homes for a year). The training cost was estimated at $4-12 million, demonstrating how specialized hardware and distributed systems enable training at previously impossible scales while highlighting the enormous resource requirements."
    },
    {
      "footnote_id": "fn-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 601,
      "context": "[^fn-tensor-cores]: **Tensor Cores**: Introduced with NVIDIA's Volta architecture (2017), Tensor Cores deliver 4x spee...",
      "full_line": "[^fn-tensor-cores]: **Tensor Cores**: Introduced with NVIDIA's Volta architecture (2017), Tensor Cores deliver 4x speedup for mixed-precision training by performing 4x4 matrix operations in a single clock cycle. The H100's 4th-gen Tensor Cores achieve 989 TFLOPS for FP16 operations\u2014roughly 6x faster than traditional CUDA cores\u2014enabling training of larger models with the same hardware budget."
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 603,
      "context": "[^fn-systolic-array]: **Systolic Array Architecture**: Developed at Carnegie Mellon in 1978, systolic arrays excel at ma...",
      "full_line": "[^fn-systolic-array]: **Systolic Array Architecture**: Developed at Carnegie Mellon in 1978, systolic arrays excel at matrix operations by streaming data through a grid of processing elements. Google's TPU v4 systolic array performs 275 TFLOPS while consuming only 175W\u2014achieving 1.57 TFLOPS/W efficiency, roughly 2-3x more energy-efficient than comparable GPUs for ML workloads."
    },
    {
      "footnote_id": "fn-fpga-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 605,
      "context": "[^fn-fpga-datacenter]: **Microsoft FPGA Deployment**: Project Catapult deployed FPGAs across Microsoft's entire datacente...",
      "full_line": "[^fn-fpga-datacenter]: **Microsoft FPGA Deployment**: Project Catapult deployed FPGAs across Microsoft's entire datacenter fleet by 2016, with one FPGA per server (>1 million total). This $1 billion investment improved Bing search latency by 50% and Azure ML inference by 2x, while reducing power consumption by 10-15% through specialized acceleration of specific algorithms."
    },
    {
      "footnote_id": "fn-wse-specs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 607,
      "context": "[^fn-wse-specs]: **Wafer-Scale Engine Specifications**: The WSE-2 (2021) contains 2.6 trillion transistors on a 21c...",
      "full_line": "[^fn-wse-specs]: **Wafer-Scale Engine Specifications**: The WSE-2 (2021) contains 2.6 trillion transistors on a 21cm x 21cm wafer\u2014the largest chip ever manufactured. With 850,000 cores and 40GB on-chip memory, it delivers 15-20x speedup vs. GPU clusters for large language models while consuming 15kW (comparable to 16-20 V100 GPUs but with orders of magnitude less communication overhead)."
    },
    {
      "footnote_id": "fn-gradient-accumulation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 609,
      "context": "[^fn-gradient-accumulation]: **Gradient Accumulation Impact**: Enables effective batch sizes of 2048+ on single GPUs with only...",
      "full_line": "[^fn-gradient-accumulation]: **Gradient Accumulation Impact**: Enables effective batch sizes of 2048+ on single GPUs with only 32-64 mini-batch size, crucial for transformer training. BERT-Large training uses effective batch size of 256 (accumulated over 8 steps) achieving 99.5% of full-batch performance while reducing memory requirements by 8x. The technique trades 10-15% compute overhead for massive memory savings."
    },
    {
      "footnote_id": "fn-activation-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 611,
      "context": "[^fn-activation-checkpointing]: **Activation Checkpointing Trade-offs**: Reduces memory usage by 50-90% at the cost of 15-30% addi...",
      "full_line": "[^fn-activation-checkpointing]: **Activation Checkpointing Trade-offs**: Reduces memory usage by 50-90% at the cost of 15-30% additional compute time due to recomputation. For training GPT-3 on V100s, checkpointing enables 2.8x larger models (from 1.3B to 3.7B parameters) within 32GB memory constraints, making it essential for memory-bound large model training despite the compute penalty."
    },
    {
      "footnote_id": "fn-transformer-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 613,
      "context": "[^fn-transformer-scaling]: **Transformer Batch Size Scaling**: Research shows transformers achieve optimal performance with b...",
      "full_line": "[^fn-transformer-scaling]: **Transformer Batch Size Scaling**: Research shows transformers achieve optimal performance with batch sizes of 256-4096 tokens, requiring gradient accumulation on most hardware. GPT-2 training improved perplexity by 0.3-0.5 points when increasing from batch size 32 to 512, demonstrating the critical importance of large effective batch sizes for language model convergence."
    },
    {
      "footnote_id": "fn-profiling-tools",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 615,
      "context": "[^fn-profiling-tools]: **Training Profiling Tools**: NVIDIA Nsight Systems can identify that data loading consumes 20-40%...",
      "full_line": "[^fn-profiling-tools]: **Training Profiling Tools**: NVIDIA Nsight Systems can identify that data loading consumes 20-40% of training time in poorly optimized pipelines, while TensorFlow Profiler reveals GPU utilization rates (optimal: >90%). Intel VTune showed that memory bandwidth often limits performance more than raw compute\u2014typical deep learning workloads achieve only 30-50% of peak FLOPS due to memory bottlenecks."
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 617,
      "context": "[^fn-backpropagation]: **Backpropagation Algorithm**: Independently rediscovered multiple times, backpropagation was popu...",
      "full_line": "[^fn-backpropagation]: **Backpropagation Algorithm**: Independently rediscovered multiple times, backpropagation was popularized by Rumelhart, Hinton, and Williams in 1986 (though similar ideas appeared in Werbos 1974). This breakthrough enabled training of deep networks by efficiently computing gradients in O(n) time vs. naive O(n\u00b2) approaches. Modern implementations require careful memory management\u2014storing all activations for a ResNet-50 consumes 1.2GB per image."
    },
    {
      "footnote_id": "fn-lr-schedules",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 619,
      "context": "[^fn-lr-schedules]: **Learning Rate Schedules**: Critical for training stability and convergence. Cosine annealing (in...",
      "full_line": "[^fn-lr-schedules]: **Learning Rate Schedules**: Critical for training stability and convergence. Cosine annealing (introduced in 2016) and linear warmup (from BERT 2018) became standard after showing 2-5% accuracy improvements. Large batch training requires linear scaling rule: multiply learning rate by batch size ratio (batch 512 \u2192 LR 0.1, batch 4096 \u2192 LR 0.8), discovered through extensive experimentation by Facebook and Google teams."
    },
    {
      "footnote_id": "fn-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 621,
      "context": "[^fn-sgd]: **SGD (Stochastic Gradient Descent)**: First introduced by Herbert Robbins and Sutton Monro in 195...",
      "full_line": "[^fn-sgd]: **SGD (Stochastic Gradient Descent)**: First introduced by Herbert Robbins and Sutton Monro in 1951 for stochastic approximation, SGD became the foundation of modern ML training after Rumelhart et al.'s 1986 backpropagation paper, enabling neural networks to learn from individual examples rather than entire datasets."
    },
    {
      "footnote_id": "fn-minibatch-gpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 627,
      "context": "...ll batches of examples, enabling parallel computations that align well with modern GPU architectures[^fn-minibatch-gpu] [@dean2012large].",
      "full_line": "Mini-batch gradient descent emerges as a practical compromise between full-batch and stochastic methods. It computes gradients over small batches of examples, enabling parallel computations that align well with modern GPU architectures[^fn-minibatch-gpu] [@dean2012large]."
    },
    {
      "footnote_id": "fn-rmsprop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 653,
      "context": "RMSprop[^fn-rmsprop] modifies the basic gradient descent update by maintaining a moving average of squared gradients for...",
      "full_line": "RMSprop[^fn-rmsprop] modifies the basic gradient descent update by maintaining a moving average of squared gradients for each parameter:"
    },
    {
      "footnote_id": "fn-adam",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 664,
      "context": "Adam[^fn-adam] combines concepts from both momentum and RMSprop, maintaining two moving averages for each paramete...",
      "full_line": "Adam[^fn-adam] combines concepts from both momentum and RMSprop, maintaining two moving averages for each parameter:"
    },
    {
      "footnote_id": "fn-adam",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 673,
      "context": "[^fn-adam]: **Adam (Adaptive Moment Estimation)**: Introduced by Diederik Kingma and Jimmy Ba in 2014, Adam re...",
      "full_line": "[^fn-adam]: **Adam (Adaptive Moment Estimation)**: Introduced by Diederik Kingma and Jimmy Ba in 2014, Adam revolutionized deep learning training by combining the best of momentum and adaptive learning rates, becoming the default optimizer for most neural networks and enabling the training of increasingly complex models."
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 711,
      "context": "Mixed-precision training[^fn-mixed-precision] has been shown to achieve comparable accuracy while significantly reducing memory consumption and c...",
      "full_line": "Mixed-precision training[^fn-mixed-precision] has been shown to achieve comparable accuracy while significantly reducing memory consumption and computational overhead [@micikevicius2018mixed; @krishnamoorthi2018quantizing]."
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 713,
      "context": "[^fn-mixed-precision]: **Mixed-Precision Training**: Introduced by NVIDIA in 2018, this technique uses FP16 for forward/b...",
      "full_line": "[^fn-mixed-precision]: **Mixed-Precision Training**: Introduced by NVIDIA in 2018, this technique uses FP16 for forward/backward passes while maintaining FP32 precision for loss scaling, enabling 2x memory savings and 1.6x speedups on Tensor Core GPUs while maintaining model accuracy."
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 753,
      "context": "The backpropagation algorithm[^fn-backpropagation] computes gradients by systematically moving backward through a neural network's computational graph...",
      "full_line": "The backpropagation algorithm[^fn-backpropagation] computes gradients by systematically moving backward through a neural network's computational graph. While earlier discussions introduced backpropagation's mathematical principles, implementing this algorithm in training systems requires careful management of memory, computation, and data flow."
    },
    {
      "footnote_id": "fn-gradient-accumulation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2067,
      "context": "Gradient accumulation[^fn-gradient-accumulation] and activation checkpointing[^fn-activation-checkpointing] provide solutions to the memory limitati...",
      "full_line": "Gradient accumulation[^fn-gradient-accumulation] and activation checkpointing[^fn-activation-checkpointing] provide solutions to the memory limitations often encountered in training large-scale machine learning models. By optimizing how memory is used during training, these techniques enable the development and deployment of complex architectures, even on hardware with constrained resources."
    },
    {
      "footnote_id": "fn-activation-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2067,
      "context": "Gradient accumulation[^fn-gradient-accumulation] and activation checkpointing[^fn-activation-checkpointing] provide solutions to the memory limitations often encountered in training large-scale machine learn...",
      "full_line": "Gradient accumulation[^fn-gradient-accumulation] and activation checkpointing[^fn-activation-checkpointing] provide solutions to the memory limitations often encountered in training large-scale machine learning models. By optimizing how memory is used during training, these techniques enable the development and deployment of complex architectures, even on hardware with constrained resources."
    },
    {
      "footnote_id": "fn-transformer-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2083,
      "context": "...achieve stable convergence. For example, models like GPT, BERT, and other transformer architectures[^fn-transformer-scaling] often benefit from larger batch sizes due to their improved gradient estimates. However, these batc...",
      "full_line": "A common use case for gradient accumulation is in training models that require large batch sizes to achieve stable convergence. For example, models like GPT, BERT, and other transformer architectures[^fn-transformer-scaling] often benefit from larger batch sizes due to their improved gradient estimates. However, these batch sizes can quickly exceed the memory capacity of GPUs, especially when working with high-dimensional inputs or multiple GPUs. By accumulating gradients over multiple smaller micro-batches, gradient accumulation enables the use of effective large batch sizes without exceeding memory limits. This is particularly beneficial for tasks like language modeling, sequence-to-sequence learning, and image classification, where batch size significantly impacts training dynamics."
    },
    {
      "footnote_id": "fn-distributed-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2145,
      "context": "...ns like natural language processing, computer vision, and scientific computing. Distributed training[^fn-distributed-training] addresses this challenge by spreading the workload across multiple machines, which coordinate to tr...",
      "full_line": "Thus far, we have focused on ML training pipelines from a single-system perspective. However, training machine learning models often requires scaling beyond a single machine due to increasing model complexity and dataset sizes. The demand for computational power, memory, and storage can exceed the capacity of individual devices, especially in domains like natural language processing, computer vision, and scientific computing. Distributed training[^fn-distributed-training] addresses this challenge by spreading the workload across multiple machines, which coordinate to train a single model efficiently."
    },
    {
      "footnote_id": "fn-distributed-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2147,
      "context": "[^fn-distributed-training]: **Distributed Training**: Google's DistBelief (2012) pioneered large-scale distributed neural netw...",
      "full_line": "[^fn-distributed-training]: **Distributed Training**: Google's DistBelief (2012) pioneered large-scale distributed neural network training, enabling models with billions of parameters across thousands of machines. This breakthrough led to modern frameworks like Horovod (2017) and PyTorch's DistributedDataParallel, democratizing distributed training for researchers worldwide."
    },
    {
      "footnote_id": "fn-profiling-tools",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3057,
      "context": "One essential technique is profiling training workloads[^fn-profiling-tools]. Profiling involves collecting detailed metrics about the system's performance during training, suc...",
      "full_line": "One essential technique is profiling training workloads[^fn-profiling-tools]. Profiling involves collecting detailed metrics about the system's performance during training, such as computation times, memory usage, and communication overhead. These metrics help reveal inefficiencies, such as imbalanced resource usage or excessive time spent in specific stages of the training pipeline. Profiling tools such as NVIDIA Nsight Systems or TensorFlow Profiler can provide actionable insights, enabling developers to make informed adjustments to their training configurations."
    },
    {
      "footnote_id": "fn-lr-schedules",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3077,
      "context": "...er convergence or reduced generalization. Techniques like learning rate scaling and warmup schedules[^fn-lr-schedules] can help mitigate these issues, ensuring stable and effective training even with large batches.",
      "full_line": "One common scaling technique is batch size scaling. Increasing the batch size can reduce the number of synchronization steps required during training, as fewer updates are needed to process the same amount of data. However, larger batch sizes may introduce challenges, such as slower convergence or reduced generalization. Techniques like learning rate scaling and warmup schedules[^fn-lr-schedules] can help mitigate these issues, ensuring stable and effective training even with large batches."
    },
    {
      "footnote_id": "fn-nccl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3093,
      "context": "...odel parallelism. NVIDIA's ecosystem, including tools like [NCCL](https://developer.nvidia.com/nccl)[^fn-nccl] for multi-GPU communication, facilitates efficient parameter synchronization, a frequent challenge...",
      "full_line": "In distributed training systems, GPUs enable scalable strategies such as data parallelism and model parallelism. NVIDIA's ecosystem, including tools like [NCCL](https://developer.nvidia.com/nccl)[^fn-nccl] for multi-GPU communication, facilitates efficient parameter synchronization, a frequent challenge in large-scale setups. For example, in training large models like GPT-3[^fn-gpt3-training], GPUs were used in tandem with distributed frameworks to split computations across thousands of devices while addressing memory and compute scaling issues [@Brown2020]."
    },
    {
      "footnote_id": "fn-gpt3-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3093,
      "context": "...zation, a frequent challenge in large-scale setups. For example, in training large models like GPT-3[^fn-gpt3-training], GPUs were used in tandem with distributed frameworks to split computations across thousands of dev...",
      "full_line": "In distributed training systems, GPUs enable scalable strategies such as data parallelism and model parallelism. NVIDIA's ecosystem, including tools like [NCCL](https://developer.nvidia.com/nccl)[^fn-nccl] for multi-GPU communication, facilitates efficient parameter synchronization, a frequent challenge in large-scale setups. For example, in training large models like GPT-3[^fn-gpt3-training], GPUs were used in tandem with distributed frameworks to split computations across thousands of devices while addressing memory and compute scaling issues [@Brown2020]."
    },
    {
      "footnote_id": "fn-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3095,
      "context": "Hardware-specific features further enhance GPU performance. NVIDIA's tensor cores[^fn-tensor-cores], for instance, are optimized for mixed-precision training, which reduces memory usage while maintai...",
      "full_line": "Hardware-specific features further enhance GPU performance. NVIDIA's tensor cores[^fn-tensor-cores], for instance, are optimized for mixed-precision training, which reduces memory usage while maintaining numerical stability [@Micikevicius2018]. This directly addresses memory constraints, a common bottleneck in training massive models. Combined with software-level optimizations like fused kernels, GPUs deliver substantial speedups in both single-device and multi-device configurations."
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3113,
      "context": "...tasets are split across multiple devices. A key feature of TPUs is their systolic array architecture[^fn-systolic-array], which performs efficient matrix multiplications by streaming data through a network of processing...",
      "full_line": "Google developed TPUs with a primary goal: to accelerate machine learning workloads at scale while reducing the energy and infrastructure costs associated with traditional hardware. Their architecture is optimized for tasks that benefit from batch processing, making them particularly effective in distributed training systems where large datasets are split across multiple devices. A key feature of TPUs is their systolic array architecture[^fn-systolic-array], which performs efficient matrix multiplications by streaming data through a network of processing elements. This design minimizes data movement overhead, reducing latency and energy consumption\u2014critical factors for training large-scale models like transformers [@Jouppi2017]."
    },
    {
      "footnote_id": "fn-fpga-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3129,
      "context": "...eries or language translation services. By integrating FPGAs directly into their data center network[^fn-fpga-datacenter], Microsoft has achieved significant performance gains while minimizing power consumption.",
      "full_line": "Microsoft had been exploring the use of FPGAs for a while, as seen in @fig-inference-fpgas, with one prominent example being [Project Brainwave](https://www.microsoft.com/en-us/research/project/project-brainwave/). This initiative leverages FPGAs to accelerate machine learning workloads in the Azure cloud. Microsoft chose FPGAs for their ability to provide low-latency inference (not training) while maintaining high throughput. This approach is especially beneficial in scenarios where real-time predictions are critical, such as search engine queries or language translation services. By integrating FPGAs directly into their data center network[^fn-fpga-datacenter], Microsoft has achieved significant performance gains while minimizing power consumption."
    },
    {
      "footnote_id": "fn-wse-specs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 3149,
      "context": "...rchitecture enables Cerebras to pack 2.6 trillion transistors and 850,000 cores onto a single device[^fn-wse-specs]. These cores are connected via a high-bandwidth, low-latency interconnect, allowing data to move ac...",
      "full_line": "The Cerebras WSE is unlike traditional chips in that it is a single wafer-scale processor, spanning the entire silicon wafer rather than being cut into smaller chips. This architecture enables Cerebras to pack 2.6 trillion transistors and 850,000 cores onto a single device[^fn-wse-specs]. These cores are connected via a high-bandwidth, low-latency interconnect, allowing data to move across the chip without the bottlenecks associated with external communication between discrete GPUs or TPUs [@Feldman2020]."
    },
    {
      "footnote_id": "fn-mlflow",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 121,
      "context": "...sess reliability and effectiveness. Modern ML teams increasingly rely on experiment tracking systems[^fn-mlflow] to manage the complexity of iterative model development and comparison. The validated models move t...",
      "full_line": "The data then enters the preparation stage, where it is transformed into machine learning-ready datasets through processes such as splitting and versioning. These datasets are used in the model training stage, where machine learning algorithms are applied to create predictive models. The resulting models are rigorously tested in the model evaluation stage, where performance metrics, such as key performance indicators (KPIs), are computed to assess reliability and effectiveness. Modern ML teams increasingly rely on experiment tracking systems[^fn-mlflow] to manage the complexity of iterative model development and comparison. The validated models move to the ML system validation phase, where they are verified for deployment readiness. Once validated, these models are integrated into production systems during the ML system deployment stage, ensuring alignment with operational requirements. The final stage tracks the performance of deployed systems in real time, enabling continuous adaptation to new data and evolving conditions."
    },
    {
      "footnote_id": "fn-mlflow",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 123,
      "context": "[^fn-mlflow]: **Experiment Tracking Evolution**: MLflow, open-sourced by Databricks in 2018, was one of the firs...",
      "full_line": "[^fn-mlflow]: **Experiment Tracking Evolution**: MLflow, open-sourced by Databricks in 2018, was one of the first comprehensive experiment tracking platforms, addressing the \"ML experiment management crisis\" where data scientists were losing track of model versions and results. Similar platforms like Weights & Biases (2017) and Neptune (2019) emerged to solve what the industry calls \"the reproducibility crisis\"\u2014studies found that only 15% of ML papers could be reproduced by other researchers, largely due to poor experiment tracking."
    },
    {
      "footnote_id": "fn-cicd-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 139,
      "context": "...enges are compounded in production ML systems, where continuous integration and deployment practices[^fn-cicd-ml] must account for both code changes and data evolution.",
      "full_line": "A defining feature of this framework is its iterative and dynamic nature. Feedback loops, such as those derived from monitoring that guide data collection improvements or deployment adjustments, ensure that machine learning systems maintain effectiveness and relevance over time. This adaptability is critical for addressing challenges such as shifting data distributions, operational constraints, and evolving user requirements. These challenges are compounded in production ML systems, where continuous integration and deployment practices[^fn-cicd-ml] must account for both code changes and data evolution."
    },
    {
      "footnote_id": "fn-cicd-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 141,
      "context": "[^fn-cicd-ml]: **CI/CD for Machine Learning**: Traditional continuous integration assumes deterministic builds\u2014th...",
      "full_line": "[^fn-cicd-ml]: **CI/CD for Machine Learning**: Traditional continuous integration assumes deterministic builds\u2014the same code produces the same output. ML systems violate this assumption because model behavior depends on training data, random initialization, and hardware differences. Google's TFX (TensorFlow Extended) and similar platforms had to reinvent CI/CD principles for ML, introducing concepts like \"model validation\" and \"data validation\" that have no equivalent in traditional software. Survey data shows that 78% of ML teams report that their traditional DevOps tools are inadequate for ML workflows."
    },
    {
      "footnote_id": "fn-crisp-dm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 147,
      "context": "...i2019software]. This systematic approach builds upon decades of structured development methodologies[^fn-crisp-dm] that have evolved to address the unique challenges of data-driven systems.",
      "full_line": "The machine learning (ML) lifecycle is a structured, iterative process that guides the development, evaluation, and continual improvement of machine learning systems. Integrating ML into broader software engineering practices introduces unique challenges that necessitate systematic approaches to experimentation, evaluation, and adaptation over time [@amershi2019software]. This systematic approach builds upon decades of structured development methodologies[^fn-crisp-dm] that have evolved to address the unique challenges of data-driven systems."
    },
    {
      "footnote_id": "fn-crisp-dm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 149,
      "context": "[^fn-crisp-dm]: **CRISP-DM (Cross-Industry Standard Process for Data Mining)**: Developed in 1996 by a consortium...",
      "full_line": "[^fn-crisp-dm]: **CRISP-DM (Cross-Industry Standard Process for Data Mining)**: Developed in 1996 by a consortium of companies including IBM, SPSS, and Daimler, CRISP-DM was one of the first structured methodologies for data projects. Its six phases (Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment) laid the groundwork for modern ML lifecycles, though today's approaches emphasize continuous iteration and monitoring that wasn't central to the original framework."
    },
    {
      "footnote_id": "fn-jupyter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 157,
      "context": "...yment, and ongoing optimization. Modern practitioners often use interactive development environments[^fn-jupyter] that support this iterative, experimental approach to ML system development.",
      "full_line": "Rather than prescribing a fixed methodology, the ML lifecycle focuses on achieving specific objectives at each stage. This flexibility allows practitioners to adapt the process to the unique constraints and goals of individual projects. Typical stages include problem formulation, data acquisition and preprocessing, model development and training, evaluation, deployment, and ongoing optimization. Modern practitioners often use interactive development environments[^fn-jupyter] that support this iterative, experimental approach to ML system development."
    },
    {
      "footnote_id": "fn-jupyter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 159,
      "context": "[^fn-jupyter]: **Jupyter Notebooks**: Created by Fernando P\u00e9rez in 2001 as IPython, and later evolved into Projec...",
      "full_line": "[^fn-jupyter]: **Jupyter Notebooks**: Created by Fernando P\u00e9rez in 2001 as IPython, and later evolved into Project Jupyter in 2014. The name \"Jupyter\" comes from the core languages it supports: Julia, Python, and R. These notebooks revolutionized data science by allowing code, visualizations, and explanatory text in a single document, making the experimental nature of ML development more transparent and reproducible. Netflix estimates over 150,000 notebooks are created daily across the industry."
    },
    {
      "footnote_id": "fn-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 175,
      "context": "...in system design\u2014something traditional software engineering methodologies weren't designed to handle[^fn-data-versioning].",
      "full_line": "The key distinctions are summarized in @tbl-sw-ml-cycles below. These differences reflect the fundamental challenge of working with data as a first-class citizen in system design\u2014something traditional software engineering methodologies weren't designed to handle[^fn-data-versioning]."
    },
    {
      "footnote_id": "fn-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 177,
      "context": "[^fn-data-versioning]: **Data Versioning Challenges**: Unlike code, which changes through discrete edits, data can change...",
      "full_line": "[^fn-data-versioning]: **Data Versioning Challenges**: Unlike code, which changes through discrete edits, data can change gradually through drift, suddenly through schema changes, or subtly through quality degradation. Traditional version control systems like Git struggle with large datasets, leading to specialized tools like Git LFS (Large File Storage, 2015) and DVC (Data Version Control, 2017). Studies show that 87% of ML projects fail due to data issues, not algorithmic problems\u2014highlighting why ML workflows must treat data with the same rigor as code."
    },
    {
      "footnote_id": "fn-dr-statistics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 269,
      "context": "...of preventable blindness worldwide, can be detected through regular screening of retinal photographs[^fn-dr-statistics]. @fig-eye-dr illustrates examples of such images: (A) a healthy retina and (B) a retina with diabet...",
      "full_line": "Diabetic retinopathy, a leading cause of preventable blindness worldwide, can be detected through regular screening of retinal photographs[^fn-dr-statistics]. @fig-eye-dr illustrates examples of such images: (A) a healthy retina and (B) a retina with diabetic retinopathy, marked by hemorrhages (dark red spots). The goal is to train a model to detect the hemorrhages."
    },
    {
      "footnote_id": "fn-dr-statistics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 271,
      "context": "[^fn-dr-statistics]: **Diabetic Retinopathy Global Impact**: Affects over 103 million people worldwide, with 28.5% of d...",
      "full_line": "[^fn-dr-statistics]: **Diabetic Retinopathy Global Impact**: Affects over 103 million people worldwide, with 28.5% of diabetic patients developing some form of retinopathy. In developing countries, up to 90% of vision loss from diabetes is preventable with early detection, but access to ophthalmologists remains severely limited\u2014rural areas in India have one ophthalmologist per 120,000 people, compared to the WHO recommendation of 1 per 20,000. This stark disparity makes AI-assisted screening not just convenient but potentially life-changing for millions."
    },
    {
      "footnote_id": "fn-healthcare-ai-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 277,
      "context": "...ough deployment and maintenance. These deployment challenges reflect broader issues in healthcare AI[^fn-healthcare-ai-challenges] that affect most real-world medical ML applications.",
      "full_line": "The initial results in controlled settings were promising. The system achieved performance comparable to expert ophthalmologists in detecting DR from high-quality retinal photographs. Yet, when the team attempted to deploy the system in rural clinics across Thailand and India (based on Google's documented deployment experiences), they encountered a series of challenges that spanned the entire ML lifecycle, from data collection through deployment and maintenance. These deployment challenges reflect broader issues in healthcare AI[^fn-healthcare-ai-challenges] that affect most real-world medical ML applications."
    },
    {
      "footnote_id": "fn-healthcare-ai-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 279,
      "context": "[^fn-healthcare-ai-challenges]: **Healthcare AI Deployment Reality**: Studies show that 85% of healthcare AI projects never reach...",
      "full_line": "[^fn-healthcare-ai-challenges]: **Healthcare AI Deployment Reality**: Studies show that 85% of healthcare AI projects never reach clinical deployment, with the majority failing not due to algorithmic issues but due to integration challenges, regulatory hurdles, and workflow disruption. The \"AI chasm\" between research success and clinical adoption is particularly wide in healthcare\u2014while medical AI papers show 95%+ accuracy rates, real-world implementation studies report significant performance drops due to data drift, equipment variations, and user acceptance issues."
    },
    {
      "footnote_id": "fn-problem-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 287,
      "context": "...teams to consider how the system will learn from data while operating within real-world constraints[^fn-problem-definition]. This stage lays the foundation for all subsequent phases in the ML lifecycle.",
      "full_line": "The development of machine learning systems begins with a critical challenge that fundamentally differs from traditional software development: defining not just what the system should do, but how it should learn to do it. Unlike conventional software, where requirements directly translate into implementation rules, ML systems require teams to consider how the system will learn from data while operating within real-world constraints[^fn-problem-definition]. This stage lays the foundation for all subsequent phases in the ML lifecycle."
    },
    {
      "footnote_id": "fn-problem-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 289,
      "context": "[^fn-problem-definition]: **ML vs. Traditional Problem Definition**: Traditional software problems are defined by determinis...",
      "full_line": "[^fn-problem-definition]: **ML vs. Traditional Problem Definition**: Traditional software problems are defined by deterministic specifications (\"if input X, then output Y\"), but ML problems are defined by examples and desired behaviors. This fundamental shift means that 73% of ML project failures occur during problem definition, compared to only 32% for traditional software. The challenge lies in translating business objectives into learning objectives\u2014something that didn't exist in software engineering until the rise of data-driven systems in the 2000s."
    },
    {
      "footnote_id": "fn-medical-annotation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 333,
      "context": "...dus photographs evaluated by a panel of 54 ophthalmologists, with each image reviewed by 3-7 experts[^fn-medical-annotation]. This collaborative effort ensured high-quality labels that captured clinically relevant features l...",
      "full_line": "In the DR project, data collection involved a development dataset of 128,000 retinal fundus photographs evaluated by a panel of 54 ophthalmologists, with each image reviewed by 3-7 experts[^fn-medical-annotation]. This collaborative effort ensured high-quality labels that captured clinically relevant features like microaneurysms, hemorrhages, and hard exudates. Additionally, clinical validation datasets comprising 12,000 images provided an independent benchmark to test the model's robustness against real-world variability, illustrating the importance of rigorous and representative data collection. The scale and complexity of this effort highlight how domain expertise and interdisciplinary collaboration are critical to building datasets for high-stakes ML systems."
    },
    {
      "footnote_id": "fn-medical-annotation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 335,
      "context": "[^fn-medical-annotation]: **Medical Data Annotation Costs**: Expert medical annotation is extraordinarily expensive\u2014ophthalm...",
      "full_line": "[^fn-medical-annotation]: **Medical Data Annotation Costs**: Expert medical annotation is extraordinarily expensive\u2014ophthalmologists charge $200-500 per hour, meaning the DR dataset's annotation cost exceeded $2.7 million in expert time alone. This represents one of the highest annotation costs per sample in ML history. For comparison, ImageNet's 14 million images cost approximately $50,000 to annotate using crowdsourcing, while medical datasets can cost 100-1000x more per image. This cost disparity explains why medical AI often relies on transfer learning and why synthetic data generation is becoming crucial for healthcare applications."
    },
    {
      "footnote_id": "fn-medical-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 341,
      "context": "...ivacy regulations required secure data handling at every stage, from image capture to model training[^fn-medical-privacy]. Coordinating expert annotations also introduced logistical challenges, necessitating systems that...",
      "full_line": "These operational realities shaped the system architecture in significant ways. The volume and size of high-resolution images necessitated local storage and preprocessing capabilities at clinics, as centralizing all data collection was impractical due to unreliable internet access. Furthermore, patient privacy regulations required secure data handling at every stage, from image capture to model training[^fn-medical-privacy]. Coordinating expert annotations also introduced logistical challenges, necessitating systems that could bridge the physical distance between clinics and ophthalmologists while maintaining workflow efficiency."
    },
    {
      "footnote_id": "fn-medical-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 343,
      "context": "[^fn-medical-privacy]: **Medical AI Privacy Complexity**: Healthcare data crosses jurisdictional boundaries with differen...",
      "full_line": "[^fn-medical-privacy]: **Medical AI Privacy Complexity**: Healthcare data crosses jurisdictional boundaries with different privacy laws\u2014HIPAA in the US, GDPR in Europe, and various national regulations elsewhere. Medical AI systems must implement techniques like differential privacy, federated learning, and homomorphic encryption, adding 40-60% to development costs. The \"data cannot leave the country\" requirements in many regions have led to the rise of federated learning architectures, where models travel to data rather than data traveling to models\u2014a paradigm shift that Google's DR project helped establish in healthcare AI."
    },
    {
      "footnote_id": "fn-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 440,
      "context": "...lmologists (0.91). This outcome highlights the effectiveness of advanced machine learning approaches[^fn-transfer-learning] and the importance of interdisciplinary collaboration between data scientists and medical experts t...",
      "full_line": "For DR detection, the model needed to achieve expert-level accuracy while handling the high resolution and variability of retinal images. Using a machine learning model trained on their meticulously labeled dataset, the team achieved an F-score of 0.95, slightly exceeding the median score of the consulted ophthalmologists (0.91). This outcome highlights the effectiveness of advanced machine learning approaches[^fn-transfer-learning] and the importance of interdisciplinary collaboration between data scientists and medical experts to refine features and interpret model outputs."
    },
    {
      "footnote_id": "fn-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 442,
      "context": "[^fn-transfer-learning]: **Transfer Learning**: A technique where models pre-trained on large datasets (like ImageNet's 14...",
      "full_line": "[^fn-transfer-learning]: **Transfer Learning**: A technique where models pre-trained on large datasets (like ImageNet's 14 million images) are adapted for specific tasks, dramatically reducing training time and data requirements. Introduced by Yann LeCun's team in the 1990s and popularized by the 2014 ImageNet competition, transfer learning became the foundation for most practical computer vision applications. Instead of training from scratch, practitioners can achieve expert-level performance with thousands rather than millions of training examples."
    },
    {
      "footnote_id": "fn-model-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 572,
      "context": "...unt for shifts in data distributions, changing usage patterns, and evolving operational requirements[^fn-model-drift]. Monitoring provides the feedback necessary to adapt to these challenges, while maintenance ensures...",
      "full_line": "Monitoring and maintenance represent the ongoing, critical processes that ensure the continued effectiveness and reliability of deployed machine learning systems. Unlike traditional software, ML systems must account for shifts in data distributions, changing usage patterns, and evolving operational requirements[^fn-model-drift]. Monitoring provides the feedback necessary to adapt to these challenges, while maintenance ensures the system evolves to meet new needs."
    },
    {
      "footnote_id": "fn-model-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 574,
      "context": "[^fn-model-drift]: **Model Drift Phenomenon**: ML models degrade over time without any code changes\u2014a phenomenon unkn...",
      "full_line": "[^fn-model-drift]: **Model Drift Phenomenon**: ML models degrade over time without any code changes\u2014a phenomenon unknown in traditional software. Studies show that 70% of production ML models experience significant performance degradation within 6 months due to data drift, concept drift, or infrastructure drift. This \"silent failure\" problem led to the development of specialized monitoring tools like Evidently AI (2020) and Fiddler (2018), creating an entirely new category of ML infrastructure that has no equivalent in traditional software engineering."
    },
    {
      "footnote_id": "fn-deployment-reality-gap",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 584,
      "context": "...retinal conditions, demonstrating that even a well-trained model could face blind spots in practice[^fn-deployment-reality-gap]. These insights informed maintenance strategies, including targeted updates to address specific cha...",
      "full_line": "Initial deployment highlighted several areas where the system failed to meet real-world needs, such as decreased accuracy in clinics with outdated equipment or lower-quality images. Monitoring systems detected performance drops in specific subgroups, such as patients with less common retinal conditions, demonstrating that even a well-trained model could face blind spots in practice[^fn-deployment-reality-gap]. These insights informed maintenance strategies, including targeted updates to address specific challenges and expanded training datasets to cover edge cases."
    },
    {
      "footnote_id": "fn-deployment-reality-gap",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 586,
      "context": "[^fn-deployment-reality-gap]: **The Lab-to-Clinic Performance Gap**: Medical AI systems typically see 10-30% performance drops w...",
      "full_line": "[^fn-deployment-reality-gap]: **The Lab-to-Clinic Performance Gap**: Medical AI systems typically see 10-30% performance drops when deployed in real-world settings, a phenomenon known as the \"deployment reality gap.\" This occurs because training data, despite best efforts, cannot capture the full diversity of real-world conditions\u2014different camera models, varying image quality, diverse patient populations, and operator skill levels all contribute to this gap. The gap is so consistent that regulatory bodies like the FDA now require \"real-world performance studies\" for medical AI approval, acknowledging that laboratory performance is insufficient to predict clinical utility."
    },
    {
      "footnote_id": "fn-ml-team-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 646,
      "context": "...uit; it's a collaborative endeavor that thrives on the diverse expertise of a multidisciplinary team[^fn-ml-team-evolution]. Each role in this intricate dance brings unique skills and insights, supporting different phases o...",
      "full_line": "Building effective and resilient machine learning systems is far more than a solo pursuit; it's a collaborative endeavor that thrives on the diverse expertise of a multidisciplinary team[^fn-ml-team-evolution]. Each role in this intricate dance brings unique skills and insights, supporting different phases of the AI development process. Understanding who these players are, what they contribute, and how they interconnect is crucial to navigating the complexities of modern AI systems."
    },
    {
      "footnote_id": "fn-ml-team-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 648,
      "context": "[^fn-ml-team-evolution]: **ML Team Role Evolution**: The \"data scientist\" role only emerged around 2008 (coined by DJ Patil...",
      "full_line": "[^fn-ml-team-evolution]: **ML Team Role Evolution**: The \"data scientist\" role only emerged around 2008 (coined by DJ Patil and Jeff Hammerbacher at Facebook and LinkedIn), while \"ML engineer\" became common around 2015 as companies realized that research models need production engineering. \"MLOps engineer\" appeared around 2018, and \"AI ethics officer\" became standard at major tech companies by 2020. This rapid role specialization reflects ML's evolution from research curiosity to production necessity\u2014modern enterprise ML teams average 8-12 distinct roles compared to 2-3 in traditional software teams."
    },
    {
      "footnote_id": "fn-systems-thinking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 682,
      "context": "...terconnectedness underscores the importance of systems thinking in AI development across all sectors[^fn-systems-thinking]. Success in AI projects, regardless of domain, comes from understanding and managing the complex in...",
      "full_line": "This interconnectedness underscores the importance of systems thinking in AI development across all sectors[^fn-systems-thinking]. Success in AI projects, regardless of domain, comes from understanding and managing the complex interactions between stages, always considering the broader context in which the system will operate."
    },
    {
      "footnote_id": "fn-systems-thinking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 684,
      "context": "[^fn-systems-thinking]: **Systems Thinking in AI**: The concept of \"systems thinking\" originated with biologist Ludwig von...",
      "full_line": "[^fn-systems-thinking]: **Systems Thinking in AI**: The concept of \"systems thinking\" originated with biologist Ludwig von Bertalanffy in the 1940s and was later applied to engineering by Jay Forrester at MIT in the 1950s. Its application to AI became critical around 2018 when companies realized that optimizing individual ML models wasn't enough\u2014success required optimizing the entire ML pipeline. This shift explains why \"ML systems engineering\" emerged as a distinct discipline, separate from both traditional software engineering and machine learning research, with its own conferences (MLSys, first held in 2020) and academic programs."
    }
  ],
  "all_definitions": [
    {
      "footnote_id": "fn-ebola-outbreak",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 55,
      "definition": "**2014-2016 Ebola Outbreak**: This outbreak killed 11,323 people across six countries, with over 28,600 cases reported. The delayed international response\u2014WHO declared a Public Health Emergency only after 5 months\u2014demonstrated how early AI-powered disease surveillance could have saved thousands of lives. The economic cost exceeded $53 billion, highlighting the need for rapid detection systems that mobile health technologies now provide.",
      "term": "2014-2016 Ebola Outbreak",
      "length": 440
    },
    {
      "footnote_id": "fn-smallholder-farmers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 59,
      "definition": "**Smallholder Farmers Global Impact**: These farmers operate plots smaller than 2 hectares but produce 35% of global food supply, feeding 2 billion people directly. In sub-Saharan Africa, they comprise 80% of farms yet receive only 2% of agricultural credit. Climate change threatens their $2.6 trillion annual production value, making AI-powered agricultural support systems critical for global food security and poverty reduction. Increasingly erratic weather patterns, pest outbreaks, and soil degradation compound their difficulties, often resulting in reduced yields and heightened food insecurity, particularly in vulnerable regions. These challenges demonstrate how systemic barriers and resource constraints perpetuate inequities and undermine resilience.",
      "term": "Smallholder Farmers Global Impact",
      "length": 763
    },
    {
      "footnote_id": "fn-cassava-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 79,
      "definition": "**Cassava Disease Impact**: Cassava feeds 800 million people globally and is a critical food security crop in Africa. Cassava mosaic disease (CMD) and cassava brown streak disease (CBSD) can destroy entire harvests, affecting millions of smallholder farmers. The PlantVillage Nuru app has been downloaded by over 500,000 farmers across Kenya, Tanzania, and Uganda, demonstrating how mobile ML can scale agricultural expertise to underserved communities without internet connectivity.",
      "term": "Cassava Disease Impact",
      "length": 483
    },
    {
      "footnote_id": "fn-farmbeats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 85,
      "definition": "**Microsoft FarmBeats**: Launched in 2017, FarmBeats has been deployed across 25,000+ farms worldwide, helping farmers reduce water usage by 30% and increase crop yields by 15-20%. The platform processes data from 50+ sensor types and can predict crop health issues 2-3 weeks before visible symptoms appear, demonstrating how Cloud ML scales agricultural expertise to underserved farming communities.",
      "term": "Microsoft FarmBeats",
      "length": 400
    },
    {
      "footnote_id": "fn-cough-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 91,
      "definition": "**Cough Analysis Technology**: Pneumonia kills over 800,000 children under 5 annually, with most deaths occurring in resource-poor settings lacking access to chest X-rays. Cough analysis using TinyML can achieve 90%+ accuracy in pneumonia detection by analyzing acoustic features like cough duration, frequency, and spectral characteristics. The entire model runs on a microcontroller costing less than $10, democratizing diagnostic capabilities.",
      "term": "Cough Analysis Technology",
      "length": 446
    },
    {
      "footnote_id": "fn-mosquito-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 95,
      "definition": "**Mosquito Species Detection**: Malaria affects 241 million people annually, causing 627,000 deaths primarily in sub-Saharan Africa. TinyML-powered mosquito detection devices achieve 95% accuracy in species identification using just acoustic signatures, costing under $50 versus traditional morphological identification requiring $5,000+ microscopy equipment. These devices can monitor 24/7 and detect Anopheles mosquitoes (malaria vectors) versus Culex (nuisance only), enabling targeted intervention strategies.",
      "term": "Mosquito Species Detection",
      "length": 513
    },
    {
      "footnote_id": "fn-global-fishing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 121,
      "definition": "**Global Fishing Watch Impact**: Since 2016, this platform has tracked over 70,000 vessels globally, processing 22+ million AIS (Automatic Identification System) data points daily. The system has helped identify $1.5 billion worth of illegal fishing activities and supported enforcement actions that recovered 180+ seized vessels. By making fishing activity transparent, the platform has contributed to 20% reductions in illegal fishing in monitored regions.",
      "term": "Global Fishing Watch Impact",
      "length": 458
    },
    {
      "footnote_id": "fn-sdg-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 139,
      "definition": "**SDG Global Impact**: Adopted by all 193 UN Member States, the SDGs represent the most ambitious global agenda in history, covering 169 specific targets with a $5-7 trillion annual funding gap. The goals build on the success of the Millennium Development Goals (2000-2015), which helped lift 1 billion people out of extreme poverty. Unlike their predecessors, the SDGs apply universally to all countries, recognizing that sustainable development requires global cooperation.",
      "term": "SDG Global Impact",
      "length": 475
    },
    {
      "footnote_id": "fn-sdg-ai-potential",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 141,
      "definition": "**AI's SDG Impact Potential**: McKinsey estimates AI could accelerate achievement of 134 of the 169 SDG targets, potentially contributing $13 trillion to global economic output by 2030. However, 97% of AI research focuses on SDG 9 (Industry/Innovation) while only 1% addresses basic needs like water, food, and health. This maldistribution means AI systems for social good require deliberate design to address the most critical human needs rather than commercial applications.",
      "term": "AI's SDG Impact Potential",
      "length": 476
    },
    {
      "footnote_id": "fn-resource-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 192,
      "definition": "**Social Good Resource Paradox**: Resource-constrained environments need the most help but have the least infrastructure to deploy solutions. For example, rural sub-Saharan Africa has 60% of global arable land but only 4% of worldwide internet connectivity. This paradox forces engineers to achieve 90%+ model compression (from 50MB to 500KB) while maintaining effectiveness, a challenge absent in commercial deployments with abundant resources.",
      "term": "Social Good Resource Paradox",
      "length": 445
    },
    {
      "footnote_id": "fn-lora-technology",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 198,
      "definition": "**LoRa Technology**: Long Range (LoRa) enables IoT devices to communicate over 15+ kilometers with battery life exceeding 10 years. Operating in unlicensed spectrum bands, LoRa networks cost $1-5 per device annually versus $15-50 for cellular. This makes LoRa ideal for agricultural sensors monitoring soil moisture across vast farms or environmental sensors in remote conservation areas. Over 140 countries have deployed LoRaWAN networks, connecting 200+ million devices worldwide for social good applications.",
      "term": "LoRa Technology",
      "length": 511
    },
    {
      "footnote_id": "fn-esp32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 218,
      "definition": "**ESP32 Capabilities**: Despite its constraints, the ESP32 costs only $2-5, consumes 30-150mA during operation, and includes Wi-Fi, Bluetooth, and various sensors. This makes it ideal for IoT deployments in social impact applications. For comparison, a smartphone processor is 100\u00d7 more powerful but costs 50\u00d7 more. The ESP32's limitations\u2014RAM smaller than a single Instagram photo\u2014force engineers to develop ingenious optimization techniques that often benefit all platforms.",
      "term": "ESP32 Capabilities",
      "length": 476
    },
    {
      "footnote_id": "fn-plantvillage-nuru",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ai_for_good/ai_for_good.qmd",
      "line": 409,
      "definition": "**PlantVillage Nuru Real-World Impact**: Deployed across 500,000+ farmers in East Africa since 2019, Nuru has helped identify crop diseases affecting $2.6 billion worth of annual cassava production. The app works on $30 smartphones offline, processing 2.1 million crop images annually. Field studies show 73% reduction in crop losses and 40% increase in farmer incomes where the system is actively used, demonstrating how progressive enhancement patterns scale impact in resource-constrained environments.",
      "term": "PlantVillage Nuru Real-World Impact",
      "length": 505
    },
    {
      "footnote_id": "fn-whetstone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 69,
      "definition": "**Whetstone**: Named after the Whetstone ALGOL compiler, this 1964 benchmark measured floating-point arithmetic performance in KIPS (thousands of instructions per second). It became the first widely-adopted standardized performance test, revealing that IBM System/360 processors could vary by 10x in floating-point performance despite similar architectures.",
      "term": "Whetstone",
      "length": 357
    },
    {
      "footnote_id": "fn-linpack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 73,
      "definition": "**LINPACK**: Developed at Argonne National Laboratory to solve dense systems of linear equations, LINPACK became famous for the Top500 supercomputer rankings starting in 1993. Modern systems achieve over 1 exaflop (10^18 operations/second) compared to the original 1979 benchmarks measuring mere megaflops.",
      "term": "LINPACK",
      "length": 306
    },
    {
      "footnote_id": "fn-dhrystone",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 75,
      "definition": "**Dhrystone**: Created by Reinhold Weicker to complement Whetstone's floating-point focus, Dhrystone measures integer and string operations in DMIPS (Dhrystone MIPS). Unlike synthetic floating-point tests, it aimed to reflect \"typical\" programming constructs, though it became vulnerable to compiler optimizations that could artificially inflate scores.",
      "term": "Dhrystone",
      "length": 353
    },
    {
      "footnote_id": "fn-spec",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 79,
      "definition": "**SPEC CPU**: Founded by major vendors including Sun, IBM, and DEC to combat \"benchmark wars\" with misleading metrics, SPEC CPU introduced standardized real-world applications like compression, compilers, and scientific computing. SPEC2017 includes 43 benchmarks representing actual workloads, with results reported as geometric means to prevent gaming\u2014a 5000x improvement over early synthetic benchmarks.",
      "term": "SPEC CPU",
      "length": 405
    },
    {
      "footnote_id": "fn-3dmark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 83,
      "definition": "**3DMark**: Created by Finnish company Futuremark (now UL), this graphics benchmark drove GPU innovation by testing real-time 3D rendering capabilities. Early versions measured triangle throughput and texture fill rates; modern 3DMark tests ray tracing and DLSS performance, with scores ranging from mobile devices (1,000 points) to high-end gaming PCs (30,000+ points).",
      "term": "3DMark",
      "length": 370
    },
    {
      "footnote_id": "fn-mobilemark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 85,
      "definition": "**MobileMark**: Developed by the Business Applications Performance Corporation, this benchmark simulates real laptop usage patterns including web browsing, video playback, and productivity tasks. Unlike peak performance tests, MobileMark measures battery life under realistic workloads, typically showing 6-12 hour endurance for modern laptops versus 15-30 minutes for synthetic stress tests.",
      "term": "MobileMark",
      "length": 392
    },
    {
      "footnote_id": "fn-cloudsuite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 89,
      "definition": "**CloudSuite**: Developed at EPFL to address the gap between traditional benchmarks and modern datacenter workloads, CloudSuite includes realistic applications like web search, data analytics, and media streaming. Unlike synthetic benchmarks, it measures end-to-end performance including network latency, storage I/O, and memory bandwidth\u2014revealing that cloud applications are often memory-bound rather than CPU-bound. Machine learning has introduced another dimension of performance evaluation. The introduction of [MLPerf](https://mlcommons.org/)[^fn-mlperf] in 2018 established a widely accepted standard for measuring machine learning training and inference efficiency across different hardware architectures.",
      "term": "CloudSuite",
      "length": 713
    },
    {
      "footnote_id": "fn-mlperf",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 91,
      "definition": "**MLPerf**: Founded by Stanford, Harvard, Google, and other leading institutions, MLPerf created the first industry-standard ML benchmarking suite, revealing surprising performance differences across hardware and establishing objective metrics for evaluating AI accelerators in training and inference tasks.",
      "term": "MLPerf",
      "length": 307
    },
    {
      "footnote_id": "fn-spec-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 101,
      "definition": "**SPEC Power**: Introduced in 2007 to address the growing importance of energy efficiency in server design, SPEC Power measures performance per watt across 10 different load levels from 10% to 100%. Results show that modern servers achieve 8-12 SPECpower_ssj2008 scores per watt, compared to 1-3 for systems from the mid-2000s\u2014a 4x efficiency improvement.",
      "term": "SPEC Power",
      "length": 355
    },
    {
      "footnote_id": "fn-green500",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 103,
      "definition": "**Green500**: Started in 2007 as a counterpart to the Top500 supercomputer list, Green500 ranks systems by FLOPS per watt rather than raw performance. The most efficient systems achieve over 60 gigaFLOPS per watt compared to less than 1 gigaFLOPS/watt for early 2000s supercomputers\u2014demonstrating the dramatic improvements in computational efficiency.",
      "term": "Green500",
      "length": 351
    },
    {
      "footnote_id": "fn-energy-star",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 105,
      "definition": "**ENERGY STAR**: Launched by the EPA in 1992, this voluntary program has prevented over 4 billion tons of greenhouse gas emissions and saved consumers $450 billion on energy bills. Computing equipment must meet strict efficiency requirements: ENERGY STAR computers typically consume 30-65% less energy than standard models during operation and sleep modes.",
      "term": "ENERGY STAR",
      "length": 356
    },
    {
      "footnote_id": "fn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 133,
      "definition": "**ImageNet**: Created by Fei-Fei Li at Stanford starting in 2007, this dataset contains 14 million images across 20,000 categories, with 1.2 million images used for the annual classification challenge (ILSVRC). ImageNet's impact is profound: it sparked the deep learning revolution when AlexNet achieved 15.3% top-5 error in 2012, compared to 25.8% for traditional methods\u2014the largest single-year improvement in computer vision history.",
      "term": "ImageNet",
      "length": 436
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 145,
      "definition": "**AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton at the University of Toronto, this 8-layer convolutional neural network revolutionized computer vision in 2012. With 60 million parameters trained on two GTX 580 GPUs, AlexNet introduced key innovations like ReLU activations, dropout regularization, and data augmentation\u2014techniques now standard in deep learning.",
      "term": "AlexNet",
      "length": 392
    },
    {
      "footnote_id": "fn-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 147,
      "definition": "**ResNet**: Microsoft's Residual Networks, introduced in 2015 by Kaiming He and colleagues, solved the vanishing gradient problem with skip connections, enabling networks with 152+ layers. ResNet-50 became the de facto standard for transfer learning, while ResNet-152 achieved superhuman performance on ImageNet with 3.57% top-5 error\u2014better than the estimated 5% human error rate.",
      "term": "ResNet",
      "length": 381
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 182,
      "definition": "**Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for neural network workloads, first deployed secretly in 2015 and announced in 2016. The first-generation TPU achieved 15-30x better performance per watt than contemporary GPUs for inference, while TPU v4 pods deliver 1.1 exaFLOPS of computing power\u2014demonstrating the potential of specialized AI hardware.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 382
    },
    {
      "footnote_id": "fn-asic",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 184,
      "definition": "**Application-Specific Integrated Circuit (ASIC)**: Custom chips designed for specific computational tasks, offering superior performance and energy efficiency compared to general-purpose processors. AI ASICs like Google's TPUs, Tesla's FSD chips, and Bitcoin mining ASICs can achieve 100-1000x better efficiency than CPUs for their target applications, but lack the flexibility for other workloads.",
      "term": "Application-Specific Integrated Circuit (ASIC)",
      "length": 399
    },
    {
      "footnote_id": "fn-bert",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 349,
      "definition": "**BERT**: Bidirectional Encoder Representations from Transformers, introduced by Google in 2018, revolutionized natural language processing by pre-training on vast text corpora using masked language modeling. BERT-Large contains 340 million parameters and achieved state-of-the-art results on 11 NLP tasks, establishing the foundation for modern language models like GPT and ChatGPT.",
      "term": "BERT",
      "length": 383
    },
    {
      "footnote_id": "fn-tensor-ops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 550,
      "definition": "**Tensor Operations**: Multi-dimensional array computations that form the backbone of neural networks, including matrix multiplication (GEMM), convolution, and element-wise operations. Modern AI accelerators optimize these primitives: NVIDIA's Tensor Cores can achieve 312 TFLOPS for mixed-precision matrix multiplications, compared to 15 TFLOPS for traditional FP32 computations\u2014a 20x speedup.",
      "term": "Tensor Operations",
      "length": 394
    },
    {
      "footnote_id": "fn-cudnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 552,
      "definition": "**cuDNN**: CUDA Deep Neural Network library, NVIDIA's GPU-accelerated library of primitives for deep neural networks. Released in 2014, cuDNN provides highly optimized implementations for convolutions, pooling, normalization, and activation layers, delivering up to 10x performance improvements over naive implementations and becoming the de facto standard for GPU-accelerated deep learning.",
      "term": "cuDNN",
      "length": 391
    },
    {
      "footnote_id": "fn-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 614,
      "definition": "**GPT-3**: OpenAI's 2020 language model with 175 billion parameters, trained on 300 billion tokens using 10,000 NVIDIA V100 GPUs for several months at an estimated cost of $4.6 million. GPT-3 demonstrated emergent abilities like few-shot learning and in-context reasoning, establishing the paradigm of scaling laws where larger models consistently outperform smaller ones across diverse language tasks.",
      "term": "GPT-3",
      "length": 402
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 842,
      "definition": "**Mixed-Precision Training**: A training technique that uses both 16-bit (FP16) and 32-bit (FP32) floating-point representations to accelerate training while maintaining model accuracy. Introduced by NVIDIA in 2017, mixed precision can achieve 1.5-2x speedups on modern GPUs with Tensor Cores while reducing memory usage by ~40%, enabling larger batch sizes and faster convergence for large models.",
      "term": "Mixed-Precision Training",
      "length": 398
    },
    {
      "footnote_id": "fn-data-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 899,
      "definition": "**Data Parallelism**: The most common distributed training strategy where each GPU processes a different subset of the training batch, then synchronizes gradients across all nodes. Modern implementations use techniques like gradient accumulation and all-reduce operations to achieve near-linear scaling up to hundreds of GPUs, though communication overhead typically limits efficiency beyond 1000+ GPUs.",
      "term": "Data Parallelism",
      "length": 403
    },
    {
      "footnote_id": "fn-model-parallel",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 901,
      "definition": "**Model Parallelism**: A distributed training approach where different parts of the neural network are placed on different GPUs, essential for models too large to fit in a single GPU's memory. GPT-3's 175B parameters required model parallelism across multiple nodes, as even high-memory GPUs can only hold ~40B parameters in mixed precision.",
      "term": "Model Parallelism",
      "length": 341
    },
    {
      "footnote_id": "fn-hardware-lottery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/benchmarking/benchmarking.qmd",
      "line": 1902,
      "definition": "**Hardware Lottery**: The phenomenon where algorithmic progress is heavily influenced by which approaches happen to align well with available hardware. For example, the Transformer architecture succeeded partly because its matrix multiplication operations perfectly match GPU capabilities, while equally valid architectures like graph neural networks remain underexplored due to poor GPU mapping. This suggests some \"breakthrough\" algorithms may simply be hardware-compatible rather than fundamentally superior.",
      "term": "Hardware Lottery",
      "length": 511
    },
    {
      "footnote_id": "fn-systems-integration",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 23,
      "definition": "**Systems Integration Philosophy**: This concept traces back to Bell Labs in the 1940s, where engineers first recognized that complex systems require dedicated integration expertise beyond component knowledge. The Apollo program epitomized this with dedicated \"systems integration\" roles\u2014NASA estimated that 60% of the program's complexity came from integration rather than individual components. Today, this principle drives everything from automotive manufacturing to ML systems engineering.",
      "term": "Systems Integration Philosophy",
      "length": 493
    },
    {
      "footnote_id": "fn-data-new-code",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 37,
      "definition": "**Data as Code**: This concept emerged from the observation that traditional software is explicitly programmed with rules and logic, while neural networks are \"programmed\" implicitly through training data. Andrej Karpathy, former Tesla AI director, popularized this phrase by noting that in deep learning, \"data is 10,000x more important than code.\" Unlike traditional software where bugs are in code, ML system bugs often manifest through training data quality, distribution, or labeling issues.",
      "term": "Data as Code",
      "length": 496
    },
    {
      "footnote_id": "fn-generative-ai-timeline",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/conclusion/conclusion.qmd",
      "line": 143,
      "definition": "**Generative AI Breakthrough**: While generative models existed for decades, the \"storm\" began with DALL-E (January 2021), accelerated with GPT-3's public API (2021), and exploded with ChatGPT's release (November 2022), which gained 100 million users in 2 months\u2014the fastest-growing consumer app in history. This sudden accessibility transformed generative AI from a research curiosity to a mainstream technology, triggering massive investments, policy discussions, and societal debates about AI's future.",
      "term": "Generative AI Breakthrough",
      "length": 505
    },
    {
      "footnote_id": "fn-watson-health",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 54,
      "definition": "**IBM Watson Health**: IBM's ambitious AI health initiative, launched with $4 billion in investment, was ultimately shut down in 2022 after years of promised breakthroughs failed to materialize due to fundamental data quality issues and over-hyped capabilities.",
      "term": "IBM Watson Health",
      "length": 261
    },
    {
      "footnote_id": "fn-data-quality-stats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 56,
      "definition": "**Data Quality Reality**: The famous \"garbage in, garbage out\" principle was first coined by IBM computer programmer George Fuechsel in the 1960s, describing how flawed input data produces nonsense output\u2014a principle that remains critically relevant in modern ML systems.",
      "term": "Data Quality Reality",
      "length": 271
    },
    {
      "footnote_id": "fn-privacy-regulations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 843,
      "definition": "**Privacy Regulation Timeline**: GDPR (2016, effective 2018) imposed \u20ac20 million fines for violations, followed by California's CCPA (2018), and now dozens of similar laws globally. These regulations fundamentally transformed how ML systems handle personal data, making privacy-by-design essential for any AI system.",
      "term": "Privacy Regulation Timeline",
      "length": 316
    },
    {
      "footnote_id": "fn-mechanical-turk",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 859,
      "definition": "**Mechanical Turk Origins**: Named after the 18th-century chess-playing \"automaton\" (actually a human chess master hidden inside), Amazon's MTurk (2005) pioneered human-in-the-loop AI by enabling distributed human computation at scale\u2014ironically reversing the original Turk's deception of AI capabilities.",
      "term": "Mechanical Turk Origins",
      "length": 305
    },
    {
      "footnote_id": "fn-batch-processing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 875,
      "definition": "**Batch Processing Evolution**: Batch processing dates back to IBM mainframes in the 1950s but was revolutionized by Google's MapReduce (2004), which enabled distributed batch processing across thousands of machines. This paradigm shift made \"big data\" analytics economically feasible for the first time.",
      "term": "Batch Processing Evolution",
      "length": 304
    },
    {
      "footnote_id": "fn-etl-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1084,
      "definition": "**ETL Evolution**: ETL emerged in the 1970s with early data warehouses but was revolutionized by Apache Spark in 2009, which enabled distributed data processing at unprecedented scale. Spark's in-memory computing made ETL pipelines 100x faster than traditional MapReduce approaches.",
      "term": "ETL Evolution",
      "length": 282
    },
    {
      "footnote_id": "fn-medical-imaging",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1356,
      "definition": "**Medical Imaging AI Revolution**: The 2012 AlexNet breakthrough began with ImageNet's 14 million labeled images, but medical AI required specialized datasets. The NIH Clinical Center released 112,120 chest X-ray images in 2017, becoming one of the largest public medical imaging datasets for ML research. Despite the dependable outcomes of expert labeling, the scarcity and high expenses of specialists render it challenging to implement on a large scale for extensive datasets.",
      "term": "Medical Imaging AI Revolution",
      "length": 479
    },
    {
      "footnote_id": "fn-concept-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1512,
      "definition": "**Concept Drift Challenge**: First formalized by Schlimmer & Granger (1986), concept drift became critical in ML systems deployment. Amazon's recommendation algorithms must continuously adapt as user preferences shift, while spam detection systems face adversarial concept drift as spammers evolve their tactics.",
      "term": "Concept Drift Challenge",
      "length": 312
    },
    {
      "footnote_id": "fn-data-lake-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1572,
      "definition": "**Data Lake Origins**: The term \"data lake\" was coined by Pentaho CTO James Dixon in 2010 to contrast with data warehouses, comparing them to \"a data swamp\" if poorly managed. The concept emerged from Hadoop's ability to store vast amounts of unstructured data cheaply.",
      "term": "Data Lake Origins",
      "length": 269
    },
    {
      "footnote_id": "fn-model-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1582,
      "definition": "**Model Scaling Explosion**: From AlexNet's 60 million parameters (2012) to GPT-3's 175 billion (2020), model size grew 3,000x in 8 years. GPT-4's rumored 1.7 trillion parameters would require 3.5 TB of storage\u2014equivalent to 1,000 DVDs worth of model weights alone. This requirement goes beyond traditional data storage and enters the realm of high-performance computing storage solutions.",
      "term": "Model Scaling Explosion",
      "length": 389
    },
    {
      "footnote_id": "fn-columnar-formats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1606,
      "definition": "**Columnar Format Revolution**: Columnar storage was pioneered by C-Store in 2005, leading to Parquet (developed at Twitter in 2013) and ORC (optimized row columnar, created at Hortonworks). These formats revolutionized analytics by enabling 10-100x faster queries for ML feature extraction.",
      "term": "Columnar Format Revolution",
      "length": 291
    },
    {
      "footnote_id": "fn-hdfs-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1614,
      "definition": "**HDFS Origins**: HDFS was inspired by Google's MapReduce paper (2004) and created at Yahoo! in 2006 to handle web-scale data. It enabled the \"big data\" revolution by making petabyte-scale storage affordable using commodity hardware instead of expensive specialized systems.",
      "term": "HDFS Origins",
      "length": 274
    },
    {
      "footnote_id": "fn-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1630,
      "definition": "**Data Versioning Challenges**: Git's inability to handle large binary files efficiently led to the \"GitHub is not a CDN\" problem. DVC, created in 2017, solved this by treating data like code using content-addressable storage, enabling Git-like workflows for terabyte-scale datasets.",
      "term": "Data Versioning Challenges",
      "length": 283
    },
    {
      "footnote_id": "fn-feature-stores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/data_engineering/data_engineering.qmd",
      "line": 1666,
      "definition": "**Feature Store Evolution**: Feature stores were pioneered by Uber's Michelangelo platform in 2017 to solve feature consistency issues at scale. The concept gained widespread adoption after Airbnb open-sourced their Zipline feature store, leading to modern solutions like Feast and Tecton.",
      "term": "Feature Store Evolution",
      "length": 289
    },
    {
      "footnote_id": "fn-brain-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 303,
      "definition": "**Brain Energy Efficiency**: The human brain contains approximately 86 billion neurons and performs roughly 10^16 operations per second on just 20 watts\u2014equivalent to running a single LED light bulb. In contrast, training GPT-3 consumed about 1,287 megawatt-hours of electricity. This stark efficiency gap drives research into neuromorphic computing and inspired the development of specialized AI chips designed to mimic brain-like processing.",
      "term": "Brain Energy Efficiency",
      "length": 443
    },
    {
      "footnote_id": "fn-synapses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 317,
      "definition": "**Synapses**: From the Greek word \"synaptein\" meaning \"to clasp together,\" synapses are the connection points between neurons where chemical or electrical signals are transmitted. A typical neuron has 1,000-10,000 synaptic connections, and the human brain contains roughly 100 trillion synapses. The strength of synaptic connections can change through experience, forming the biological basis of learning and memory\u2014a principle directly mimicked by adjustable weights in artificial neural networks.",
      "term": "Synapses",
      "length": 498
    },
    {
      "footnote_id": "fn-perceptron",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 427,
      "definition": "**Perceptron**: Invented by Frank Rosenblatt in 1957 at Cornell, the perceptron was the first artificial neural network capable of learning. The New York Times famously reported it would be \"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\" While overly optimistic, this breakthrough laid the foundation for all modern neural networks.",
      "term": "Perceptron",
      "length": 439
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 431,
      "definition": "**Backpropagation**: Published by Rumelhart, Hinton, and Williams in 1986, backpropagation solved the \"credit assignment problem\"\u2014how to determine which weights in a multi-layer network were responsible for errors. This algorithm, based on the mathematical chain rule, enabled training of deep networks and directly led to the modern AI revolution. Interestingly, a similar algorithm was discovered by Paul Werbos in 1974 but went largely unnoticed.",
      "term": "Backpropagation",
      "length": 449
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 437,
      "definition": "**FLOPS**: Floating Point Operations Per Second measures computational throughput by counting mathematical operations like addition, subtraction, multiplication, and division of decimal numbers. Modern supercomputers achieve exascale performance (10^18 FLOPS), while neural network training requires petascale to exascale compute. For perspective, training GPT-3 required approximately 3.14 \u00d7 10^23 FLOPS\u2014more computation than was available to the entire world before 1960.",
      "term": "FLOPS",
      "length": 473
    },
    {
      "footnote_id": "fn-overfitting",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 476,
      "definition": "**Overfitting**: When a model memorizes training examples instead of learning generalizable patterns\u2014like a student who memorizes answers instead of understanding concepts. The model performs perfectly on training data but fails on new examples. Common signs include training accuracy continuing to improve while validation accuracy plateaus or decreases. Think of it as becoming an \"expert\" on a practice test who panics when facing slightly different questions on the real exam.",
      "term": "Overfitting",
      "length": 480
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 480,
      "definition": "**Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for tensor operations, the mathematical building blocks of neural networks. First deployed internally in 2015, TPUs can perform matrix multiplications up to 30\u00d7 faster than contemporary GPUs while using less power. The name reflects their optimization for tensor operations\u2014multi-dimensional arrays that represent data flowing through neural networks. Google has since made TPUs available through cloud services, democratizing access to this specialized AI hardware.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 546
    },
    {
      "footnote_id": "fn-gradient-descent",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1606,
      "definition": "**Gradient Descent**: Think of gradient descent as finding the bottom of a valley while blindfolded\u2014you feel the slope under your feet and take steps downhill. Mathematically, the gradient points in the direction of steepest increase, so we move in the opposite direction to minimize our loss function. The name comes from the Latin \"gradus\" (step) and was first formalized by Cauchy in 1847 for solving systems of equations, though the modern machine learning version was developed much later.",
      "term": "Gradient Descent",
      "length": 494
    },
    {
      "footnote_id": "fn-learning-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dl_primer/dl_primer.qmd",
      "line": 1616,
      "definition": "**Learning Rate**: Often called the most important hyperparameter in deep learning, the learning rate determines the step size in optimization. Think of it like the gas pedal on a car\u2014too much acceleration and you'll crash past your destination, too little and you'll never get there. Typical values range from 0.1 to 0.0001, and getting this right can mean the difference between a model that learns in hours versus one that never converges.",
      "term": "Learning Rate",
      "length": 442
    },
    {
      "footnote_id": "fn-uat",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 57,
      "definition": "**Universal Approximation Theorem**: Proven independently by Cybenko (1989) and Hornik (1989), this fundamental result showed that neural networks could theoretically learn any function\u2014a discovery that reinvigorated interest in neural networks after the \"AI Winter\" of the 1980s and laid mathematical foundations for modern deep learning.",
      "term": "Universal Approximation Theorem",
      "length": 339
    },
    {
      "footnote_id": "fn-mnist-dataset",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 61,
      "definition": "**MNIST Dataset**: Created by Yann LeCun in 1998 from NIST's database of handwritten digits, MNIST's 60,000 training images became the \"fruit fly\" of machine learning research. Despite achieving 99.7% accuracy being considered solved, MNIST remains invaluable for education because its simplicity lets students focus on architectural concepts without getting lost in data complexity. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits.",
      "term": "MNIST Dataset",
      "length": 685
    },
    {
      "footnote_id": "fn-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 285,
      "definition": "**Basic Linear Algebra Subprograms (BLAS)**: Developed in the 1970s as a standard for basic vector and matrix operations, BLAS became the foundation for virtually all scientific computing. Modern implementations like Intel MKL and OpenBLAS achieve performance within 90% of theoretical peak, making them essential for neural network efficiency.",
      "term": "Basic Linear Algebra Subprograms (BLAS)",
      "length": 344
    },
    {
      "footnote_id": "fn-imagenet-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 535,
      "definition": "**ImageNet Revolution**: AlexNet's dramatic victory in the 2012 ImageNet challenge (reducing error from 26% to 16%) sparked the deep learning renaissance. ImageNet's 14 million labeled images across 20,000 categories provided the scale needed to train deep CNNs, proving that \"big data + big compute + big models\" could achieve superhuman performance.",
      "term": "ImageNet Revolution",
      "length": 351
    },
    {
      "footnote_id": "fn-lecun-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 540,
      "definition": "**Yann LeCun and CNNs**: LeCun's 1989 LeNet architecture was inspired by Hubel and Wiesel's discovery of simple and complex cells in cat visual cortex (1962). LeNet-5 achieved 99.2% accuracy on MNIST in 1998 and was deployed by banks to read millions of checks daily\u2014one of the first large-scale commercial applications of neural networks. As illustrated in @fig-cnn-spatial-processing, CNNs address spatial pattern processing through a fundamentally different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position\u2014a process known as convolution[^fn-convolution-origin].",
      "term": "Yann LeCun and CNNs",
      "length": 826
    },
    {
      "footnote_id": "fn-convolution-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 542,
      "definition": "**Mathematical Convolution**: The convolution operation dates to Euler (1760s) and was formalized by mathematicians like Cauchy and Poisson in the 1800s for solving differential equations [@dominguez2015history]. CNNs adapted this 200-year-old mathematical concept to create translation-invariant feature detectors, proving that classical mathematics often provides the foundation for modern breakthroughs.",
      "term": "Mathematical Convolution",
      "length": 406
    },
    {
      "footnote_id": "fn-attention-is-all-you-need",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1329,
      "definition": "**\"Attention is All You Need\"**: This 2017 paper by Google researchers eliminated recurrence entirely, showing that attention mechanisms alone could achieve state-of-the-art results. The title itself became a rallying cry, and within 5 years, transformer-based models achieved breakthrough performance in language (GPT, BERT), vision (ViT), and beyond. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself.",
      "term": "\"Attention is All You Need\"",
      "length": 612
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1522,
      "definition": "**Backpropagation Algorithm**: While the chain rule was known since the 1600s, Rumelhart, Hinton, and Williams (1986) showed how to efficiently apply it to train multi-layer networks. This \"learning by error propagation\" algorithm made deep networks practical and remains virtually unchanged in modern systems\u2014a testament to its fundamental importance.",
      "term": "Backpropagation Algorithm",
      "length": 352
    },
    {
      "footnote_id": "fn-resnet-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1534,
      "definition": "**ResNet Revolution**: ResNet (2016) solved the \"degradation problem\" where deeper networks performed worse than shallow ones. The key insight: adding identity shortcuts ($\\mathcal{F}(\\mathbf{x}) + \\mathbf{x}$) let networks learn residual mappings instead of full transformations, enabling training of 1000+ layer networks and winning ImageNet 2015.",
      "term": "ResNet Revolution",
      "length": 349
    },
    {
      "footnote_id": "fn-lstm-invention",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1546,
      "definition": "**LSTM Origins**: Sepp Hochreiter and J\u00fcrgen Schmidhuber invented LSTMs in 1997 to solve the \"vanishing gradient problem\" that plagued RNNs. Their gating mechanism was inspired by biological neurons' ability to selectively retain information\u2014a breakthrough that enabled sequence modeling and paved the way for modern language models.",
      "term": "LSTM Origins",
      "length": 333
    },
    {
      "footnote_id": "fn-vision-transformers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 1614,
      "definition": "**Vision Transformers (ViTs)**: Google's 2021 breakthrough showed that pure transformers could match CNN performance on ImageNet by treating image patches as \"words.\" ViTs split a $224\\times 224$ image into $16\\times 16$ patches (196 \"tokens\"), proving that attention mechanisms could replace convolutional inductive biases with sufficient data.",
      "term": "Vision Transformers (ViTs)",
      "length": 345
    },
    {
      "footnote_id": "fn-parameter-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2011,
      "definition": "**Parameter Scaling**: The leap from AlexNet's 60 million parameters (2012) to GPT-3's 175 billion parameters (2020) represents a 3,000x increase in just 8 years. Modern models like GPT-4 may exceed 1 trillion parameters, requiring specialized distributed computing infrastructure and consuming megawatts of power during training.",
      "term": "Parameter Scaling",
      "length": 330
    },
    {
      "footnote_id": "fn-tpu-development",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/dnn_architectures/dnn_architectures.qmd",
      "line": 2019,
      "definition": "**Tensor Processing Units**: Google's TPUs emerged from their need to run neural networks on billions of searches daily. First deployed secretly in 2015, TPUs achieve 15-30x better performance per watt than GPUs for inference. The TPU's $128\\times 128$ systolic array performs 65,536 multiply-accumulate operations per clock cycle, revolutionizing AI hardware design. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks.",
      "term": "Tensor Processing Units",
      "length": 520
    },
    {
      "footnote_id": "fn-scaling-laws",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 56,
      "definition": "**Scaling Laws**: OpenAI established that language model performance follows predictable power-law relationships with model size, dataset size, and compute budget - fundamentally changing how researchers approach model design and resource allocation.",
      "term": "Scaling Laws",
      "length": 250
    },
    {
      "footnote_id": "fn-tokens",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 76,
      "definition": "**Tokens**: Individual units of text that language models process, typically words or subword pieces. GPT-3 was trained on 300 billion tokens, while modern models like PaLM use over 780 billion tokens\u2014requiring massive text corpora equivalent to thousands of books.",
      "term": "Tokens",
      "length": 265
    },
    {
      "footnote_id": "fn-flops",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 78,
      "definition": "**FLOPs**: Floating-Point Operations Per Second, the standard measure of computational throughput. Training GPT-3 required ~3.14 \u00d7 10\u00b2\u00b3 FLOPs, equivalent to running a modern gaming PC continuously for over 350 years.",
      "term": "FLOPs",
      "length": 216
    },
    {
      "footnote_id": "fn-transformer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 80,
      "definition": "**Transformer**: Neural network architecture introduced by Vaswani et al. in 2017, revolutionizing natural language processing through attention mechanisms. Forms the foundation of models like GPT, BERT, and T5, enabling parallel processing unlike sequential RNNs.",
      "term": "Transformer",
      "length": 264
    },
    {
      "footnote_id": "fn-autoregressive",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 82,
      "definition": "**Autoregressive**: Models that generate sequences by predicting the next element based on previous elements. GPT models are autoregressive, generating text one token at a time, unlike BERT which processes entire sequences simultaneously.",
      "term": "Autoregressive",
      "length": 238
    },
    {
      "footnote_id": "fn-gpt3",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 88,
      "definition": "**GPT-3**: OpenAI's 175-billion parameter language model released in 2020, costing an estimated $4.6 million to train and consuming ~1,287 MWh of electricity. Its training data included 45TB of text from the internet, books, and other sources.",
      "term": "GPT-3",
      "length": 243
    },
    {
      "footnote_id": "fn-perplexity",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 160,
      "definition": "**Perplexity**: A measurement of how well a language model predicts text, calculated as 2^(cross-entropy loss). GPT-3 achieved ~20 perplexity on WebText, meaning on average it's as confused as if choosing randomly among 20 equally-likely next words.",
      "term": "Perplexity",
      "length": 249
    },
    {
      "footnote_id": "fn-svms",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 553,
      "definition": "**Support Vector Machines (SVMs)**: Machine learning algorithm developed by Vapnik in the 1990s, using the \"kernel trick\" to find optimal decision boundaries. Popular before deep learning, SVMs won most machine learning competitions until ~2010 when deep neural networks gained prominence.",
      "term": "Support Vector Machines (SVMs)",
      "length": 289
    },
    {
      "footnote_id": "fn-ensemble",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 555,
      "definition": "**Ensemble Methods**: Techniques combining multiple models to improve performance, like Random Forest (2001) and Gradient Boosting (1999). Won many Kaggle competitions before deep learning, with XGBoost becoming the go-to method for structured data prediction.",
      "term": "Ensemble Methods",
      "length": 260
    },
    {
      "footnote_id": "fn-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 559,
      "definition": "**Model Parallelism**: Distributing model components across multiple processors, contrasting with data parallelism. Modern transformer models like GPT-3 require model parallelism due to their 175B parameters exceeding single GPU memory (~24GB for A100).",
      "term": "Model Parallelism",
      "length": 253
    },
    {
      "footnote_id": "fn-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 561,
      "definition": "**Stochastic Gradient Descent (SGD)**: Optimization algorithm using random data samples, introduced by Robbins and Monro (1951). Made neural network training practical by reducing memory requirements from full-batch to single-sample updates, enabling learning on larger datasets.",
      "term": "Stochastic Gradient Descent (SGD)",
      "length": 279
    },
    {
      "footnote_id": "fn-gpu-deep-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 569,
      "definition": "**GPUs for Deep Learning**: Graphics Processing Units, originally designed for video games, proved ideal for neural network training due to parallel matrix operations. AlexNet's 2012 breakthrough used two GTX 580 GPUs, achieving 15.3% ImageNet error rate\u2014a revolutionary improvement.",
      "term": "GPUs for Deep Learning",
      "length": 283
    },
    {
      "footnote_id": "fn-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 573,
      "definition": "**Pruning**: Removing unimportant neural network connections to reduce model size. Han et al. (2015) achieved 9\u00d7 compression on AlexNet without accuracy loss, removing 89% of parameters while maintaining ImageNet performance.",
      "term": "Pruning",
      "length": 225
    },
    {
      "footnote_id": "fn-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 575,
      "definition": "**Quantization**: Reducing numerical precision from 32-bit floats to 8-bit integers or lower. Can achieve 4\u00d7 memory reduction and speed improvements while maintaining ~99% of original accuracy in many models.",
      "term": "Quantization",
      "length": 208
    },
    {
      "footnote_id": "fn-knowledge-distillation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 577,
      "definition": "**Knowledge Distillation**: Training small \"student\" models to mimic large \"teacher\" models. DistilBERT achieved 97% of BERT's performance with 40% fewer parameters and 60% faster inference speed.",
      "term": "Knowledge Distillation",
      "length": 196
    },
    {
      "footnote_id": "fn-mobilenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 583,
      "definition": "**MobileNet**: Efficient neural network architecture using depthwise separable convolutions, achieving ~50\u00d7 fewer parameters than traditional models. MobileNet-v1 has only 4.2M parameters vs. VGG-16's 138M, enabling deployment on smartphones with <100MB memory.",
      "term": "MobileNet",
      "length": 261
    },
    {
      "footnote_id": "fn-efficientnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 585,
      "definition": "**EfficientNet**: Architecture achieving state-of-the-art accuracy with 10\u00d7 fewer parameters than previous models. EfficientNet-B7 achieves 84.3% ImageNet top-1 accuracy with 66M parameters, compared to ResNet-152's 60M parameters achieving 78.3%.",
      "term": "EfficientNet",
      "length": 247
    },
    {
      "footnote_id": "fn-squeezenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 587,
      "definition": "**SqueezeNet**: Compact CNN architecture achieving AlexNet-level accuracy with 50\u00d7 fewer parameters (1.25M vs. 60M). Demonstrated that clever architecture design can dramatically reduce model size without sacrificing performance.",
      "term": "SqueezeNet",
      "length": 229
    },
    {
      "footnote_id": "fn-gpt4",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 591,
      "definition": "**GPT-4**: OpenAI's most advanced language model as of 2023, reportedly using a mixture-of-experts architecture with ~1.8 trillion parameters. Training cost estimated at >$100 million, highlighting the extreme resource requirements of frontier models.",
      "term": "GPT-4",
      "length": 251
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 593,
      "definition": "**Tensor Processing Unit (TPU)**: Google's custom silicon designed specifically for machine learning workloads, first announced in 2016. TPU v4 delivers 275 teraFLOPs with 90% lower energy per operation compared to general-purpose processors.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 242
    },
    {
      "footnote_id": "fn-param-efficient",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 595,
      "definition": "**Parameter-Efficient Fine-tuning**: Methods like LoRA and Adapters that update <1% of model parameters while achieving full fine-tuning performance. Reduces memory requirements from gigabytes to megabytes for large model adaptation.",
      "term": "Parameter-Efficient Fine-tuning",
      "length": 233
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 717,
      "definition": "**AlexNet**: Groundbreaking CNN by Krizhevsky, Sutskever, and Hinton (2012) that won ImageNet with 15.3% error rate, nearly halving the previous best of 26.2%. Used 60M parameters, two GPUs, and launched the deep learning revolution.",
      "term": "AlexNet",
      "length": 233
    },
    {
      "footnote_id": "fn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 719,
      "definition": "**ImageNet**: Large-scale visual recognition dataset with 14+ million images across 20,000+ categories. The annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) drove computer vision breakthroughs from 2010-2017.",
      "term": "ImageNet",
      "length": 225
    },
    {
      "footnote_id": "fn-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 721,
      "definition": "**Moore's Law**: Intel co-founder Gordon Moore's 1965 observation that transistor density doubles every ~2 years. Hardware improvements follow ~2x every 18-24 months, while AI algorithmic efficiency improved 44x in 7 years (2012-2019).",
      "term": "Moore's Law",
      "length": 235
    },
    {
      "footnote_id": "fn-resnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 725,
      "definition": "**ResNet**: Residual Network architecture by He et al. (2015) enabling training of very deep networks (152+ layers) through skip connections. Won ImageNet 2015 with 3.6% error rate, surpassing human-level performance for the first time.",
      "term": "ResNet",
      "length": 236
    },
    {
      "footnote_id": "fn-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 994,
      "definition": "**Data Parallelism**: Training method where the same model runs on multiple processors with different data batches. GPT-3 training used data parallelism across thousands of GPUs, processing multiple text sequences simultaneously to achieve massive scale.",
      "term": "Data Parallelism",
      "length": 254
    },
    {
      "footnote_id": "fn-uci",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1012,
      "definition": "**UCI Machine Learning Repository**: Established in 1987 by the University of California, Irvine, one of the most widely-used resources for machine learning datasets. Contains over 600 datasets and has been cited in thousands of research papers, serving as a cornerstone for early ML research.",
      "term": "UCI Machine Learning Repository",
      "length": 293
    },
    {
      "footnote_id": "fn-pca",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1014,
      "definition": "**Principal Component Analysis (PCA)**: Dimensionality reduction technique invented by Karl Pearson in 1901, identifies the most important directions of variation in data. Reduces computational complexity while preserving 90%+ of data variance in many applications.",
      "term": "Principal Component Analysis (PCA)",
      "length": 265
    },
    {
      "footnote_id": "fn-mnist",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1022,
      "definition": "**MNIST**: Modified National Institute of Standards and Technology database of handwritten digits, containing 70,000 28\u00d728 pixel images. Created in 1998, became the \"Hello World\" of computer vision, though modern models achieve >99% accuracy.",
      "term": "MNIST",
      "length": 242
    },
    {
      "footnote_id": "fn-cifar10",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1024,
      "definition": "**CIFAR-10**: Canadian Institute for Advanced Research dataset with 60,000 32\u00d732 color images across 10 classes. Released in 2009, remains a standard benchmark despite its small image size by modern standards.",
      "term": "CIFAR-10",
      "length": 209
    },
    {
      "footnote_id": "fn-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1034,
      "definition": "**Transfer Learning**: Technique where models pre-trained on large datasets are fine-tuned for specific tasks. ImageNet pre-trained models can achieve high accuracy on new vision tasks with <1000 labeled examples vs. millions needed from scratch.",
      "term": "Transfer Learning",
      "length": 246
    },
    {
      "footnote_id": "fn-data-augmentation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1036,
      "definition": "**Data Augmentation**: Artificially expanding datasets through transformations like rotations, crops, or noise. Can improve model performance by 5-15% and reduce overfitting, especially valuable when labeled data is scarce.",
      "term": "Data Augmentation",
      "length": 223
    },
    {
      "footnote_id": "fn-active-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1038,
      "definition": "**Active Learning**: Iteratively selecting the most informative samples for labeling to maximize learning efficiency. Can achieve target performance with 50-90% less labeled data compared to random sampling strategies.",
      "term": "Active Learning",
      "length": 218
    },
    {
      "footnote_id": "fn-data-centric-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1042,
      "definition": "**Data-Centric AI**: Paradigm shift from model-centric to data-centric development, popularized by Andrew Ng in 2021. Focuses on systematically improving data quality rather than just model architecture, often yielding greater performance gains.",
      "term": "Data-Centric AI",
      "length": 245
    },
    {
      "footnote_id": "fn-self-supervised",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1046,
      "definition": "**Self-Supervised Learning**: Training method where models create their own labels from input data structure, like predicting masked words in BERT or next frames in videos. Enables learning from billions of unlabeled examples, revolutionizing NLP and computer vision.",
      "term": "Self-Supervised Learning",
      "length": 267
    },
    {
      "footnote_id": "fn-curriculum-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1048,
      "definition": "**Curriculum Learning**: Training strategy where models learn from easy examples before progressing to harder ones, mimicking human education. Can improve convergence speed by 25-50% and final model performance across various domains.",
      "term": "Curriculum Learning",
      "length": 234
    },
    {
      "footnote_id": "fn-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1054,
      "definition": "**Foundation Models**: Large-scale, general-purpose AI models trained on broad data that can be adapted for many tasks. Term coined by Stanford HAI in 2021, includes models like GPT-3, BERT, and DALL-E with billions of parameters.",
      "term": "Foundation Models",
      "length": 230
    },
    {
      "footnote_id": "fn-tinyml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/efficient_ai/efficient_ai.qmd",
      "line": 1058,
      "definition": "**TinyML**: Machine learning on microcontrollers and edge devices with <1KB-1MB memory and <1mW power consumption. Enables AI in IoT devices, wearables, and sensors where traditional ML deployment is impossible due to resource constraints.",
      "term": "TinyML",
      "length": 239
    },
    {
      "footnote_id": "fn-blas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 118,
      "definition": "**BLAS (Basic Linear Algebra Subprograms)**: Originally developed at Argonne National Laboratory, BLAS became the de facto standard for linear algebra operations, with Level 1 (vector-vector), Level 2 (matrix-vector), and Level 3 (matrix-matrix) operations that still underpin every modern ML framework.",
      "term": "BLAS (Basic Linear Algebra Subprograms)",
      "length": 303
    },
    {
      "footnote_id": "fn-lapack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 122,
      "definition": "**LAPACK (Linear Algebra Package)**: Succeeded LINPACK and EISPACK, introducing block algorithms that dramatically improved cache efficiency and parallel execution - innovations that became essential as datasets grew from megabytes to terabytes.",
      "term": "LAPACK (Linear Algebra Package)",
      "length": 245
    },
    {
      "footnote_id": "fn-theano",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 136,
      "definition": "**Theano**: Named after the ancient Greek mathematician Theano of Croton, this framework pioneered the concept of symbolic mathematical expressions in Python, laying the groundwork for every modern deep learning framework.",
      "term": "Theano",
      "length": 222
    },
    {
      "footnote_id": "fn-comp-graphs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 138,
      "definition": "**Computational Graphs**: First formalized in automatic differentiation literature by Wengert (1964), this representation became the backbone of modern ML frameworks, enabling both forward and reverse-mode differentiation at unprecedented scale.",
      "term": "Computational Graphs",
      "length": 245
    },
    {
      "footnote_id": "fn-tensorflow",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 152,
      "definition": "**TensorFlow**: Named after tensor operations flowing through computational graphs, this framework democratized distributed machine learning by open-sourcing Google's internal DistBelief system, instantly giving researchers access to infrastructure that previously required massive corporate resources.",
      "term": "TensorFlow",
      "length": 302
    },
    {
      "footnote_id": "fn-pytorch",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 158,
      "definition": "**PyTorch**: Inspired by the original Torch framework from NYU, PyTorch brought \"define-by-run\" semantics to Python, enabling researchers to modify models during execution - a breakthrough that accelerated research by making debugging as simple as using a standard Python debugger.",
      "term": "PyTorch",
      "length": 281
    },
    {
      "footnote_id": "fn-jax",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 166,
      "definition": "**JAX**: Stands for \"Just After eXecution\" and combines NumPy's API with functional programming transforms (jit, grad, vmap, pmap), enabling researchers to write concise code that automatically scales to TPUs and GPU clusters while maintaining NumPy compatibility.",
      "term": "JAX",
      "length": 264
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 174,
      "definition": "**TPU (Tensor Processing Unit)**: Google's custom ASIC achieved 15-30x better performance-per-watt than contemporary GPUs and CPUs for neural networks, fundamentally proving that domain-specific architectures could outperform general-purpose processors for ML workloads.",
      "term": "TPU (Tensor Processing Unit)",
      "length": 270
    },
    {
      "footnote_id": "fn-auto-diff",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/frameworks/frameworks.qmd",
      "line": 586,
      "definition": "**Automatic Differentiation**: Invented by Robert Edwin Wengert in 1964, this technique achieves machine precision derivatives by applying the chain rule at the elementary operation level, making neural network training computationally feasible for networks with millions of parameters.",
      "term": "Automatic Differentiation",
      "length": 286
    },
    {
      "footnote_id": "fn-von-neumann",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 53,
      "definition": "**Von Neumann Architecture**: Proposed by John von Neumann in 1945, this architecture separates program storage from data storage but forces all data to flow through a single bus between CPU and memory. In AI workloads, this \"von Neumann bottleneck\" becomes critical\u2014moving a 1GB model from memory consumes 100-1000x more energy than the actual computation, driving the need for specialized architectures that bring computation closer to data.",
      "term": "Von Neumann Architecture",
      "length": 443
    },
    {
      "footnote_id": "fn-memory-hierarchy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 57,
      "definition": "**Memory Hierarchy Challenge**: Traditional memory hierarchy (L1: ~1ns, L2: ~10ns, RAM: ~100ns, SSD: ~100\u03bcs) breaks down for AI workloads. A single transformer layer might need 1GB+ of weights, but L1 cache is only 32KB. Google's TPU addresses this with 128MB of on-chip memory running at 900 GB/s bandwidth\u2014600x faster than typical RAM\u2014because keeping weights on-chip dramatically reduces both latency and energy consumption.",
      "term": "Memory Hierarchy Challenge",
      "length": 426
    },
    {
      "footnote_id": "fn-tpu-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 65,
      "definition": "**TPU Origins**: Google secretly developed the Tensor Processing Unit (TPU) starting in 2013 when they realized CPUs couldn't handle the computational demands of their neural networks. The TPUv1, deployed in 2015, delivered 15-30x better performance per watt than contemporary GPUs for inference. By 2017, Google's TPUs were processing over 100 petaops per second across their datacenters, fundamentally changing how the industry approached AI hardware.",
      "term": "TPU Origins",
      "length": 453
    },
    {
      "footnote_id": "fn-intel-8087",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 77,
      "definition": "**Intel 8087 Impact**: The 8087 coprocessor cost $750 (about $2,800 today) but transformed scientific computing\u2014CAD workstations that took hours for complex calculations could complete them in minutes. This success created the entire coprocessor market and established the economic model for specialized hardware that persists today: charge premium prices for dramatic performance improvements in specific domains.",
      "term": "Intel 8087 Impact",
      "length": 414
    },
    {
      "footnote_id": "fn-alexnet-gpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 173,
      "definition": "**AlexNet's GPU Revolution**: AlexNet's breakthrough wasn't just algorithmic\u2014it proved GPUs could train deep networks 10x faster than CPUs. The team split the 8-layer network across two NVIDIA GTX 580s (512 cores each), reducing training time from weeks to days. This success triggered the \"deep learning gold rush\" and established NVIDIA as the default AI hardware company, with GPU sales for data centers growing from $200 million to $47 billion by 2024.",
      "term": "AlexNet's GPU Revolution",
      "length": 456
    },
    {
      "footnote_id": "fn-risc-v-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 334,
      "definition": "**RISC-V for AI**: RISC-V, the open-source instruction set architecture from UC Berkeley (2010), is becoming crucial for AI accelerators because it's freely customizable. Companies like SiFive and Google have created RISC-V chips with custom AI extensions. Unlike proprietary architectures, RISC-V allows hardware designers to add specialized ML instructions without licensing fees, potentially democratizing AI hardware development beyond the current duopoly of x86 and ARM.",
      "term": "RISC-V for AI",
      "length": 475
    },
    {
      "footnote_id": "fn-cray-vector",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 378,
      "definition": "**Cray-1 Vector Legacy**: The Cray-1 (1975) cost $8.8 million ($50 million today) but could perform 160 million floating-point operations per second\u20141000x faster than typical computers. Its 64-element vector registers and pipelined vector units established the architectural template that modern AI accelerators still follow: process many data elements simultaneously with specialized hardware pipelines.",
      "term": "Cray-1 Vector Legacy",
      "length": 404
    },
    {
      "footnote_id": "fn-simd-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 611,
      "definition": "**SIMD Evolution**: SIMD originated in Flynn's 1966 taxonomy for scientific computing, but neural networks transformed it from a niche HPC concept to mainstream necessity. Modern CPUs have 512-bit SIMD units (AVX-512), but AI pushed development of SIMT (Single Instruction, Multiple Thread) where thousands of lightweight threads execute in parallel\u2014GPU architectures now coordinate 65,536+ threads simultaneously, impossible with traditional SIMD.",
      "term": "SIMD Evolution",
      "length": 448
    },
    {
      "footnote_id": "fn-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 653,
      "definition": "**Tensor Core Breakthrough**: NVIDIA introduced tensor cores in the V100 (2017) to accelerate the 4x4 matrix operations common in neural networks. The A100's third-generation tensor cores achieve 312 TFLOPS for FP16 matrix math\u201420x faster than traditional CUDA cores. This single innovation enabled training of models like GPT-3 that would have been impossible with conventional hardware, fundamentally changing the scale of AI research.",
      "term": "Tensor Core Breakthrough",
      "length": 437
    },
    {
      "footnote_id": "fn-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 671,
      "definition": "**Apple's Neural Engine Strategy**: Apple introduced the Neural Engine in 2017's A11 chip to enable on-device ML without draining battery life. The M1's 16-core Neural Engine delivers 11.8 TOPS while consuming just 20 watts\u2014enabling real-time features like live text recognition and voice processing without cloud connectivity. This \"privacy through hardware\" approach influenced the entire industry to prioritize edge AI capabilities.",
      "term": "Apple's Neural Engine Strategy",
      "length": 435
    },
    {
      "footnote_id": "fn-systolic-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/hw_acceleration/hw_acceleration.qmd",
      "line": 693,
      "definition": "**Systolic Array Renaissance**: H.T. Kung and Charles Leiserson introduced systolic arrays at CMU in 1979 for VLSI signal processing, but the concept languished for decades due to programming complexity. Google's 2016 TPU resurrection proved these \"heartbeat\" architectures could deliver massive efficiency gains for neural networks\u2014the TPUv1's 256x256 systolic array achieved 92 TOPS while consuming just 40 watts, making systolic arrays the dominant AI architecture today.",
      "term": "Systolic Array Renaissance",
      "length": 474
    },
    {
      "footnote_id": "fn-paradigm-shift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 56,
      "definition": "**Paradigm Shift**: A term coined by philosopher Thomas Kuhn in 1962 to describe fundamental changes in scientific approach\u2014like the shift from Newtonian to Einstein's physics. In AI, key paradigm shifts include moving from symbolic reasoning to statistical learning (1990s), and from shallow to deep learning (2010s). Each shift required researchers to abandon established methods and embrace radically different approaches to understanding intelligence.",
      "term": "Paradigm Shift",
      "length": 455
    },
    {
      "footnote_id": "fn-eliza",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 62,
      "definition": "**ELIZA**: Created by MIT's Joseph Weizenbaum in 1966, ELIZA was one of the first chatbots that could simulate human conversation by pattern matching and substitution\u2014ironically, Weizenbaum was horrified when people began forming emotional attachments to his simple program, leading him to become a critic of AI.",
      "term": "ELIZA",
      "length": 312
    },
    {
      "footnote_id": "fn-early",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 243,
      "definition": "**Perceptron**: One of the first computational learning algorithms\u2014a system that could learn to classify patterns by making yes/no decisions based on inputs.",
      "term": "Perceptron",
      "length": 157
    },
    {
      "footnote_id": "fn-dartmouth-conference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 251,
      "definition": "**Dartmouth Conference (1956)**: The legendary 8-week workshop at Dartmouth College where AI was officially born. Organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, it was the first time researchers gathered specifically to discuss \"artificial intelligence\"\u2014a term McCarthy coined for the proposal. The ambitious goal was to make machines \"simulate every aspect of learning or any other feature of intelligence.\" Though overly optimistic, this gathering launched AI as a formal research field.",
      "term": "Dartmouth Conference (1956)",
      "length": 526
    },
    {
      "footnote_id": "fn-brittleness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 271,
      "definition": "**Brittleness in AI Systems**: The tendency of rule-based systems to fail completely when encountering inputs that fall outside their programmed scenarios, no matter how similar those inputs might be to what they were designed to handle. This contrasts with human intelligence, which can adapt and make reasonable guesses even in unfamiliar situations. The brittleness problem drove researchers toward machine learning approaches that could generalize from examples rather than relying on exhaustive rule sets.",
      "term": "Brittleness in AI Systems",
      "length": 510
    },
    {
      "footnote_id": "fn-mooreslaw",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 311,
      "definition": "**Moore's Law**: The observation made by Intel co-founder Gordon Moore in 1965 that the number of transistors on a microchip doubles approximately every two years, while the cost halves. This exponential growth in computing power has been a key driver of advances in machine learning, though the pace has begun to slow in recent years.",
      "term": "Moore's Law",
      "length": 335
    },
    {
      "footnote_id": "fn-viola-jones",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 377,
      "definition": "**Viola-Jones Algorithm**: A groundbreaking computer vision algorithm that could detect faces in real-time by using simple rectangular patterns (like comparing the brightness of eye regions versus cheek regions) and making decisions in stages, filtering out non-faces quickly and spending more computation only on promising candidates.",
      "term": "Viola-Jones Algorithm",
      "length": 335
    },
    {
      "footnote_id": "fn-cascade",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 379,
      "definition": "**Cascade of Classifiers**: A multi-stage decision system where each stage acts as a filter, quickly rejecting obvious non-matches and passing promising candidates to the next, more sophisticated stage\u2014similar to how security screening works at airports with multiple checkpoints of increasing thoroughness.",
      "term": "Cascade of Classifiers",
      "length": 307
    },
    {
      "footnote_id": "fn-neurons",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 387,
      "definition": "**Artificial Neurons**: Basic computational units in neural networks that mimic biological neurons, taking multiple inputs, applying weights and biases, and producing an output signal through an activation function[^fn-activation].",
      "term": "Artificial Neurons",
      "length": 231
    },
    {
      "footnote_id": "fn-activation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 389,
      "definition": "**Activation Function**: A mathematical function that determines whether a neuron should be \"activated\" (send a signal) based on its inputs, similar to how biological neurons fire only when they receive sufficient stimulation from other neurons.",
      "term": "Activation Function",
      "length": 245
    },
    {
      "footnote_id": "fn-imagenet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 393,
      "definition": "**ImageNet**: A massive visual database containing over 14 million labeled images across 20,000+ categories, created by Stanford's Fei-Fei Li starting in 2009. The annual ImageNet challenge became the Olympics of computer vision, driving breakthrough after breakthrough in image recognition until neural networks became so good they essentially solved the competition.",
      "term": "ImageNet",
      "length": 368
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 395,
      "definition": "**AlexNet**: A breakthrough deep neural network from 2012 that won the ImageNet competition by a large margin and helped spark the deep learning revolution. Named after Alex Krizhevsky, it proved that neural networks could outperform traditional computer vision methods when given enough data and computing power.",
      "term": "AlexNet",
      "length": 313
    },
    {
      "footnote_id": "fn-foundation-models",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 823,
      "definition": "**Foundation Models**: Large-scale AI models trained on broad datasets that serve as the \"foundation\" for many different applications through fine-tuning\u2014like GPT for language tasks or CLIP for vision tasks. The term was coined by Stanford's AI researchers in 2021 to capture how these models became the basis for building more specific AI systems.",
      "term": "Foundation Models",
      "length": 348
    },
    {
      "footnote_id": "fn-training-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 827,
      "definition": "**Large-Scale Training Challenges**: Training foundation models requires solving complex systems engineering problems including distributed computing coordination, memory management across thousands of machines, fault tolerance for multi-week training runs, and efficient data pipeline design. These challenges span hardware optimization, software engineering, and algorithmic innovation, requiring deep expertise in parallel computing and distributed systems.",
      "term": "Large-Scale Training Challenges",
      "length": 460
    },
    {
      "footnote_id": "fn-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 829,
      "definition": "**Parameters**: The adjustable values within a neural network that are modified during training, similar to how the brain's neural connections grow stronger as you learn a new skill. Having more parameters generally means that the model can learn more complex patterns.",
      "term": "Parameters",
      "length": 269
    },
    {
      "footnote_id": "fn-backprop-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 833,
      "definition": "**Backpropagation (Historical Context)**: A mathematical technique that allows neural networks to learn by calculating how much each component contributed to errors and adjusting accordingly\u2014like a coach analyzing a team's mistakes and giving each player specific feedback to improve their performance.",
      "term": "Backpropagation (Historical Context)",
      "length": 302
    },
    {
      "footnote_id": "fn-cnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 835,
      "definition": "**Convolutional Neural Network (CNN)**: A type of neural network specially designed for processing images, inspired by how the human visual system works. The \"convolutional\" part refers to how it scans images in small chunks, similar to how our eyes focus on different parts of a scene.",
      "term": "Convolutional Neural Network (CNN)",
      "length": 286
    },
    {
      "footnote_id": "fn-computer-engineering",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 851,
      "definition": "**Computer Engineering**: This discipline emerged in the late 1960s when IBM System/360 and other complex computing systems required expertise that spanned both hardware and software. Before Computer Engineering, electrical engineers focused on circuits while computer scientists worked on algorithms, but no one specialized in the integration challenges. Today's Computer Engineering programs, established at schools like Case Western Reserve and Stanford in the 1970s, combine hardware design, software systems, and computer architecture\u2014laying the groundwork for what ML Systems Engineering is becoming today.",
      "term": "Computer Engineering",
      "length": 612
    },
    {
      "footnote_id": "fn-mas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1126,
      "definition": "**Multi-Agent System**: A computational system where multiple intelligent agents interact within an environment, each pursuing their own objectives while potentially cooperating or competing with other agents.",
      "term": "Multi-Agent System",
      "length": 209
    },
    {
      "footnote_id": "fn-edge",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1138,
      "definition": "**Edge Processor**: A specialized computing device designed to perform AI computations close to where data is generated, optimized for low latency and energy efficiency rather than raw computing power.",
      "term": "Edge Processor",
      "length": 201
    },
    {
      "footnote_id": "fn-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1160,
      "definition": "**Transfer Learning**: A machine learning technique where a model developed for one task is reused as the starting point for a model on a related task, significantly reducing the amount of training data and computation required\u2014particularly valuable in domains like agriculture where labeled data may be scarce.",
      "term": "Transfer Learning",
      "length": 311
    },
    {
      "footnote_id": "fn-rnn",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1210,
      "definition": "**Sequential Neural Networks**: Neural network architectures designed to process data that occurs in sequences over time, such as predicting where a pedestrian will move next based on their previous movements. These networks maintain a form of \"memory\" of previous inputs to inform current decisions.",
      "term": "Sequential Neural Networks",
      "length": 300
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1216,
      "definition": "**Tensor Processing Unit (TPU)**: A specialized AI accelerator chip designed by Google specifically for neural network machine learning, particularly efficient at matrix operations common in deep learning workloads.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 215
    },
    {
      "footnote_id": "fn-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1234,
      "definition": "**Data Drift**: The gradual change in the statistical properties of the target variable (what the model is trying to predict) over time, which can degrade model performance if not properly monitored and addressed.",
      "term": "Data Drift",
      "length": 213
    },
    {
      "footnote_id": "fn-backprop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1240,
      "definition": "**Backpropagation**: The primary algorithm used to train neural networks, which calculates how each parameter in the network should be adjusted to minimize prediction errors by propagating error gradients backward through the network layers.",
      "term": "Backpropagation",
      "length": 241
    },
    {
      "footnote_id": "fn-transfer",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1244,
      "definition": "**Transfer Learning**: A machine learning method where a model developed for one task is reused as the starting point for a model on a second task, significantly reducing the amount of training data and computation required.",
      "term": "Transfer Learning",
      "length": 224
    },
    {
      "footnote_id": "fn-black-box",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1262,
      "definition": "**Black Box**: A system where you can observe the inputs and outputs but cannot see or understand the internal workings\u2014like how a radio receives signals and produces sound without most users understanding the electronics inside. In AI, this opacity becomes problematic when the system makes important decisions affecting people's lives.",
      "term": "Black Box",
      "length": 337
    },
    {
      "footnote_id": "fn-inference",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/introduction/introduction.qmd",
      "line": 1268,
      "definition": "**Inference Attack**: A technique where an adversary attempts to extract sensitive information about the training data by making careful queries to a trained model, exploiting patterns the model may have inadvertently memorized during training.",
      "term": "Inference Attack",
      "length": 244
    },
    {
      "footnote_id": "fn-data-centers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 43,
      "definition": "**Data Centers**: Modern hyperscale data centers can house hundreds of thousands of servers and consume 20-50 megawatts of power\u2014equivalent to a small city. Google's data centers alone process over 189,000 searches per second globally as of 2025.",
      "term": "Data Centers",
      "length": 246
    },
    {
      "footnote_id": "fn-edge-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 45,
      "definition": "**Edge Computing**: The term \"edge\" originates from network topology diagrams where the \"edge\" represents the boundary between the core network and end devices. This paradigm emerged in the 1990s with content delivery networks (CDNs) but gained prominence with IoT proliferation after 2010.",
      "term": "Edge Computing",
      "length": 290
    },
    {
      "footnote_id": "fn-tinyml-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 47,
      "definition": "**TinyML**: Coined at Harvard in 2019, TinyML targets devices with <1mW power consumption and <1MB memory. The field emerged from the realization that 99% of sensor data is discarded due to transmission costs\u2014processing locally saves both energy and bandwidth.",
      "term": "TinyML",
      "length": 260
    },
    {
      "footnote_id": "fn-design-tradeoffs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 49,
      "definition": "**System Design Trade-offs**: These fundamental tensions in computing were formalized by David Patterson and John Hennessy in their seminal work on computer architecture, earning them the 2017 Turing Award for establishing the principles that govern modern processor design.",
      "term": "System Design Trade-offs",
      "length": 274
    },
    {
      "footnote_id": "fn-mobile-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 51,
      "definition": "**Mobile Power Constraints**: Modern smartphones contain 3000-4000mAh batteries (~15Wh) but ML inference can consume 1-5W, reducing battery life significantly. Apple's Neural Engine and Google's Tensor chips were specifically designed to perform AI tasks at <1W power consumption.",
      "term": "Mobile Power Constraints",
      "length": 280
    },
    {
      "footnote_id": "fn-cost-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 276,
      "definition": "**ML Hardware Cost Spectrum**: The cost range spans 6 orders of magnitude\u2014from $10 ESP32-CAM modules to $200K+ DGX A100 systems. This 20,000x cost difference reflects proportional differences in computational capability, enabling deployment across vastly different economic contexts and use cases.",
      "term": "ML Hardware Cost Spectrum",
      "length": 297
    },
    {
      "footnote_id": "fn-billion-parameters",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 278,
      "definition": "**Billion-Parameter Models**: GPT-3 has 175 billion parameters requiring 350GB of memory just to store weights. GPT-4 is estimated at 1.8 trillion parameters. For comparison, the human brain has approximately 86 billion neurons with 100 trillion synaptic connections\u2014suggesting AI models are approaching biological complexity.",
      "term": "Billion-Parameter Models",
      "length": 326
    },
    {
      "footnote_id": "fn-inference-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 280,
      "definition": "**Cloud Inference Latency**: Network latency includes propagation delay (speed of light limits), routing delays, and processing time. Round-trip from California to Virginia takes minimum 80ms just for light travel. Adding internet routing, DNS lookup, and server processing typically results in 100-500ms total latency.",
      "term": "Cloud Inference Latency",
      "length": 319
    },
    {
      "footnote_id": "fn-edge-latency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 282,
      "definition": "**Edge Latency Advantage**: Edge processing eliminates network round-trips, achieving <10ms response times for local inference. Industrial robots require <1ms control loops, autonomous vehicles need <10ms emergency responses\u2014both impossible with cloud processing but achievable with edge deployment.",
      "term": "Edge Latency Advantage",
      "length": 299
    },
    {
      "footnote_id": "fn-mobile-storage",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 284,
      "definition": "**Mobile Storage Evolution**: iPhone storage grew from 4GB (2007) to 1TB (2023)\u2014a 250x increase in 16 years. However, ML models grew even faster: ResNet-50 (25MB, 2015) to modern language models (>1GB compressed), creating ongoing storage pressure despite hardware improvements.",
      "term": "Mobile Storage Evolution",
      "length": 278
    },
    {
      "footnote_id": "fn-memory-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 286,
      "definition": "**Memory Scale Comparison**: TinyML devices operate with 256KB-2MB memory versus smartphones with 8-12GB (40,000x difference) and cloud servers with 1TB+ (4,000,000x difference). Yet TinyML can still perform useful inference through aggressive model compression and quantization techniques.",
      "term": "Memory Scale Comparison",
      "length": 290
    },
    {
      "footnote_id": "fn-battery-life",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 288,
      "definition": "**Ultra-Long Battery Life**: TinyML enables 10+ year deployments on single batteries through duty cycling\u2014devices sleep 99.9% of the time, wake periodically for inference, then return to sleep. Average power consumption drops to 10-100 microwatts, making decade-long operation feasible on coin-cell batteries.",
      "term": "Ultra-Long Battery Life",
      "length": 309
    },
    {
      "footnote_id": "fn-cloud-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 470,
      "definition": "**Cloud Infrastructure Evolution**: Cloud computing for ML emerged from Amazon's decision in 2002 to treat their internal infrastructure as a service. AWS launched in 2006, followed by Google Cloud (2008) and Azure (2010). By 2024, global cloud infrastructure spending exceeded $250 billion annually.",
      "term": "Cloud Infrastructure Evolution",
      "length": 300
    },
    {
      "footnote_id": "fn-nlp-compute",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 472,
      "definition": "**NLP Computational Demands**: Modern language models like GPT-3 required 3,640 petaflop-days of compute for training\u2014equivalent to running 1,000 NVIDIA V100 GPUs continuously for 355 days. This computational scale drove the need for massive cloud infrastructure.",
      "term": "NLP Computational Demands",
      "length": 263
    },
    {
      "footnote_id": "fn-tpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 474,
      "definition": "**Tensor Processing Unit (TPU)**: Google's custom ASIC designed specifically for tensor operations, first used internally in 2015 for neural network inference. A single TPU v4 Pod contains 4,096 chips and delivers over 1 exaflop of compute power\u2014more than most supercomputers.",
      "term": "Tensor Processing Unit (TPU)",
      "length": 276
    },
    {
      "footnote_id": "fn-hyperscale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 476,
      "definition": "**Hyperscale Data Centers**: These facilities contain 5,000+ servers and cover 10,000+ square feet. Microsoft's data centers span over 200 locations globally, with some individual facilities consuming enough electricity to power 80,000 homes.",
      "term": "Hyperscale Data Centers",
      "length": 242
    },
    {
      "footnote_id": "fn-ml-apis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 478,
      "definition": "**ML APIs**: Application Programming Interfaces that democratized AI by providing pre-trained models as web services. Google's Vision API launched in 2016, processing over 1 billion images monthly within two years\u2014enabling developers to add AI capabilities without ML expertise.",
      "term": "ML APIs",
      "length": 278
    },
    {
      "footnote_id": "fn-paas-pricing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 480,
      "definition": "**Pay-as-You-Go Pricing**: Revolutionary model where users pay only for actual compute time used, measured in GPU-hours or inference requests. Training a model might cost $50-500 on demand versus $50,000-500,000 to purchase equivalent hardware.",
      "term": "Pay-as-You-Go Pricing",
      "length": 244
    },
    {
      "footnote_id": "fn-industrial-iot",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 625,
      "definition": "**Industrial IoT**: Manufacturing generates over 1 exabyte of data annually, but less than 1% is analyzed due to connectivity constraints. Edge ML enables real-time analysis, with predictive maintenance alone saving manufacturers $630 billion globally by 2025.",
      "term": "Industrial IoT",
      "length": 260
    },
    {
      "footnote_id": "fn-iot-hubs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 627,
      "definition": "**IoT Hubs**: Central connection points that aggregate data from multiple sensors before cloud transmission. A typical smart building might have 1 hub managing 100-1000 IoT sensors, reducing cloud traffic by 90% while enabling local decision-making.",
      "term": "IoT Hubs",
      "length": 249
    },
    {
      "footnote_id": "fn-iot-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 629,
      "definition": "**IoT Device Growth**: From 8.4 billion connected devices in 2017 to a projected 25.4 billion by 2030. Each device generates 2.5 quintillion bytes of data daily, making edge processing essential for bandwidth management.",
      "term": "IoT Device Growth",
      "length": 220
    },
    {
      "footnote_id": "fn-latency-critical",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 631,
      "definition": "**Latency-Critical Applications**: Autonomous vehicles require <10ms response times for emergency braking decisions. Industrial robotics needs <1ms for precision control. Cloud round-trip latency typically ranges from 50-200ms, making edge processing essential for safety-critical applications.",
      "term": "Latency-Critical Applications",
      "length": 294
    },
    {
      "footnote_id": "fn-endpoint-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 633,
      "definition": "**Endpoint Device Constraints**: Typical edge devices have 1-8GB RAM and 2-32GB storage, versus cloud servers with 128-1024GB RAM and petabytes of storage. Processing power differs by 10-100x, necessitating specialized model compression techniques.",
      "term": "Endpoint Device Constraints",
      "length": 248
    },
    {
      "footnote_id": "fn-voice-recognition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 749,
      "definition": "**Voice Recognition Evolution**: Apple's Siri (2011) required cloud processing with 200-500ms latency. By 2017, on-device processing reduced latency to <50ms while improving privacy. Modern smartphones process 16kHz audio at 20-30ms latency using specialized neural engines.",
      "term": "Voice Recognition Evolution",
      "length": 274
    },
    {
      "footnote_id": "fn-computational-photography",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 751,
      "definition": "**Computational Photography**: Combines multiple exposures and ML algorithms to enhance image quality. Google's Night Sight captures 15 frames in 6 seconds, using ML to align and merge them. Portrait mode uses depth estimation ML models to create professional-looking bokeh effects in real-time.",
      "term": "Computational Photography",
      "length": 295
    },
    {
      "footnote_id": "fn-mobile-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 753,
      "definition": "**Mobile System-on-Chip**: Modern flagship SoCs integrate CPU, GPU, NPU, and memory controllers on a single chip. Apple's A17 Pro contains 19 billion transistors in a 3nm process, while Snapdragon 8 Gen 3 delivers significant AI performance improvements over its predecessor.",
      "term": "Mobile System-on-Chip",
      "length": 275
    },
    {
      "footnote_id": "fn-npu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 755,
      "definition": "**Neural Processing Unit (NPU)**: Specialized processors optimized for neural network operations. Apple's Neural Engine (introduced in A11, 2017) performs 600 billion operations per second. Qualcomm's Hexagon NPU in flagship chips delivers up to 75 TOPS while consuming <1W.",
      "term": "Neural Processing Unit (NPU)",
      "length": 274
    },
    {
      "footnote_id": "fn-tflite",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 757,
      "definition": "**TensorFlow Lite**: Google's mobile ML framework launched in 2017, designed to run models <100MB with <100ms inference time. Supports quantization to reduce model size by 75% while maintaining 95% accuracy. Used in over 4 billion devices worldwide.",
      "term": "TensorFlow Lite",
      "length": 249
    },
    {
      "footnote_id": "fn-coreml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 759,
      "definition": "**Core ML**: Apple's framework introduced in iOS 11 (2017), optimized for on-device inference. Supports models from 1KB to 1GB, with automatic optimization for Apple Silicon. Enables features like Live Text, which processes text in real-time using on-device OCR models.",
      "term": "Core ML",
      "length": 269
    },
    {
      "footnote_id": "fn-quantization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 761,
      "definition": "**Model Quantization**: Reduces model precision from 32-bit to 8-bit integers, cutting model size by 75% and speeding inference by 2-4x. INT8 quantization maintains >99% of original accuracy for most models while enabling deployment on resource-constrained devices.",
      "term": "Model Quantization",
      "length": 265
    },
    {
      "footnote_id": "fn-mobile-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 763,
      "definition": "**Mobile Device Constraints**: Flagship phones typically have 8-12GB RAM and 256-512GB storage, versus cloud servers with 128-1024GB RAM and unlimited storage. Mobile processors operate at 15-25W peak power compared to server CPUs at 200-400W.",
      "term": "Mobile Device Constraints",
      "length": 243
    },
    {
      "footnote_id": "fn-microcontrollers",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 882,
      "definition": "**Microcontrollers**: Single-chip computers with integrated CPU, memory, and peripherals, typically operating at 1-100MHz with 32KB-2MB RAM. Arduino Uno uses an ATmega328P with 32KB flash and 2KB RAM, while ESP32 provides WiFi capability with 520KB RAM\u2014still thousands of times less than a smartphone.",
      "term": "Microcontrollers",
      "length": 301
    },
    {
      "footnote_id": "fn-energy-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 884,
      "definition": "**Energy Efficiency in TinyML**: Ultra-low power consumption enables deployment in remote locations. Modern ARM Cortex-M0+ microcontrollers consume <1\u00b5W in sleep mode and 100-300\u00b5W/MHz when active. Efficient ML inference can run for years on a single coin-cell battery.",
      "term": "Energy Efficiency in TinyML",
      "length": 269
    },
    {
      "footnote_id": "fn-coin-cell",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 886,
      "definition": "**Coin-Cell Batteries**: Small, round batteries (CR2032 being most common) providing 200-250mAh at 3V. When powering TinyML devices at 10-50mW average consumption, these batteries can operate devices for 1-5 years, enabling \"deploy-and-forget\" IoT applications.",
      "term": "Coin-Cell Batteries",
      "length": 261
    },
    {
      "footnote_id": "fn-on-device-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 888,
      "definition": "**On-Device Training Constraints**: Unlike mobile devices, microcontrollers rarely support full model training due to memory limitations. Instead, they use techniques like transfer learning, where a pre-trained model is fine-tuned with minimal on-device adaptation, or federated learning aggregation.",
      "term": "On-Device Training Constraints",
      "length": 300
    },
    {
      "footnote_id": "fn-device-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 890,
      "definition": "**TinyML Device Scale**: The smallest ML-capable devices measure just 5x5mm (Syntiant NDP chips). Google's Coral Dev Board Mini measures 40x48mm but includes WiFi and full Linux capability. The extreme miniaturization enables integration into previously \"dumb\" objects like smart dust sensors.",
      "term": "TinyML Device Scale",
      "length": 293
    },
    {
      "footnote_id": "fn-model-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ml_systems/ml_systems.qmd",
      "line": 892,
      "definition": "**TinyML Model Compression**: Techniques include pruning (removing 90%+ of neural network connections), quantization to 8-bit or even 1-bit precision, and knowledge distillation. A typical smartphone model of 50MB might compress to 250KB for microcontroller deployment while retaining 95% accuracy.",
      "term": "TinyML Model Compression",
      "length": 298
    },
    {
      "footnote_id": "fn-a11-bionic-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 44,
      "definition": "**A11 Bionic Breakthrough**: Apple's A11 Bionic (2017) was the first mobile chip with sufficient computational power for on-device training, delivering 0.6 TOPS compared to the previous A10's 0.2 TOPS. This 3x improvement, combined with 2.39 billion transistors and a dual-core Neural Engine, enabled gradient computation for the first time on mobile devices. Google's Pixel Visual Core achieved similar capabilities with 8 custom Image Processing Units optimized for machine learning workloads.",
      "term": "A11 Bionic Breakthrough",
      "length": 495
    },
    {
      "footnote_id": "fn-edge-computing-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 54,
      "definition": "**Edge Computing Origins**: The term \"edge computing\" was coined by Akamai in the late 1990s for content delivery networks, but the modern concept emerged from Cisco's \"fog computing\" initiative in 2012. The realization that data was growing faster than network bandwidth could handle led to the \"bring compute to the data\" philosophy that defines edge AI today.",
      "term": "Edge Computing Origins",
      "length": 362
    },
    {
      "footnote_id": "fn-federated-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 74,
      "definition": "**Federated Learning Birth**: Google's Brendan McMahan coined \"federated learning\" in 2016, but the concept emerged from their Gboard team's frustration with keyboard personalization. They realized they needed user-specific data to improve predictions, but couldn't collect keystrokes due to privacy concerns. This led to the \"train where the data lives\" philosophy that defined federated learning.",
      "term": "Federated Learning Birth",
      "length": 398
    },
    {
      "footnote_id": "fn-gdpr-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 92,
      "definition": "**GDPR's ML Impact**: When GDPR took effect in May 2018, it essentially made centralized ML training illegal for personal data without explicit consent. The \"right to be forgotten\" also meant models trained on personal data could be legally required to \"unlearn\" specific users\u2014technically impossible with traditional training. This drove massive investment in privacy-preserving ML techniques.",
      "term": "GDPR's ML Impact",
      "length": 394
    },
    {
      "footnote_id": "fn-arduino-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 537,
      "definition": "**Arduino Edge Computing Reality**: The Arduino Nano 33 BLE Sense represents typical microcontroller constraints\u2014256KB SRAM is roughly 65,000 times smaller than a modern smartphone's 16GB RAM. To put this in perspective, storing just one 224\u00d7224\u00d73 RGB image (150KB) would consume 60% of available memory. Training requires 3-5x more memory for gradients and activations, making even tiny models challenging. The 1MB flash storage can hold only the smallest quantized models, forcing designers to use 8-bit or even 4-bit representations.",
      "term": "Arduino Edge Computing Reality",
      "length": 536
    },
    {
      "footnote_id": "fn-mobilenet-innovation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 545,
      "definition": "**MobileNet Innovation**: Google's MobileNet family revolutionized mobile AI by achieving 10-20x parameter reduction compared to traditional CNNs. MobileNetV1 (2017) used depthwise separable convolutions to reduce FLOPs by 8-9x, while MobileNetV2 (2018) added inverted residuals and linear bottlenecks. The breakthrough enabled real-time inference on smartphones: MobileNetV2 runs ImageNet classification in ~75ms on a Pixel phone versus 1.8 seconds for ResNet-50.",
      "term": "MobileNet Innovation",
      "length": 464
    },
    {
      "footnote_id": "fn-depthwise-separable",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 547,
      "definition": "**Depthwise Separable Convolutions**: This technique decomposes standard convolution into two operations: depthwise convolution (applies single filter per input channel) and pointwise convolution (1\u00d71 conv to combine channels). For a 3\u00d73 conv with 512 input/output channels, standard convolution requires 2.4M parameters while depthwise separable needs only 13.8K\u2014a 174x reduction. The computational savings are similarly dramatic, making real-time inference possible on mobile CPUs.",
      "term": "Depthwise Separable Convolutions",
      "length": 483
    },
    {
      "footnote_id": "fn-stm32-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 571,
      "definition": "**STM32F4 Microcontroller Reality**: The STM32F4 represents the harsh reality of embedded computing\u2014192KB SRAM (roughly the size of a small JPEG image) and 1MB flash storage, running at 168MHz without floating-point hardware acceleration. Integer arithmetic is 10-100x slower than dedicated floating-point units found in mobile chips. Power consumption is ~100mW during active processing, requiring careful duty-cycling to preserve battery life. These constraints make even simple neural networks challenging\u2014a 10-neuron hidden layer requires ~40KB for weights alone in FP32.",
      "term": "STM32F4 Microcontroller Reality",
      "length": 575
    },
    {
      "footnote_id": "fn-esp32-capabilities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 573,
      "definition": "**ESP32 Edge Computing**: The ESP32 provides 520KB SRAM and dual-core processing at 240MHz, making it more capable than STM32F4 but still severely constrained. Its key advantage is built-in WiFi and Bluetooth for federated learning scenarios. However, the lack of hardware floating-point support means all ML operations must use integer quantization. Real-world deployments show 8-bit quantized models can achieve 95% of FP32 accuracy while fitting in ~50KB memory, enabling basic on-device training for simple tasks like sensor anomaly detection.",
      "term": "ESP32 Edge Computing",
      "length": 547
    },
    {
      "footnote_id": "fn-neural-engine",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 577,
      "definition": "**Apple Neural Engine Evolution**: Apple's Neural Engine has evolved dramatically since the A11 Bionic. The A17 Pro (2023) features a 16-core Neural Engine delivering 35 TOPS\u2014roughly equivalent to an NVIDIA GTX 1080 Ti. This represents a 58x improvement over the original A11. The Neural Engine specializes in matrix operations with dedicated 8-bit and 16-bit arithmetic units, enabling efficient on-device training. Real-world performance: fine-tuning a MobileNet classifier takes ~2 seconds versus 45 seconds on CPU alone, while consuming only ~500mW additional power.",
      "term": "Apple Neural Engine Evolution",
      "length": 570
    },
    {
      "footnote_id": "fn-tensor-soc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 579,
      "definition": "**Google Tensor SoC Architecture**: Google's Tensor chips (starting with Pixel 6 in 2021) feature a custom TPU v1-derived Edge TPU delivering ~5.5 TOPS for ML workloads. Unlike Apple's Neural Engine, Tensor optimizes for Google's specific models (speech recognition, computational photography). The TPU can perform 8-bit integer operations at 600 TOPS while consuming only 2W, making it highly efficient for federated learning scenarios where devices train locally on speech or image data.",
      "term": "Google Tensor SoC Architecture",
      "length": 489
    },
    {
      "footnote_id": "fn-tinytl-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 639,
      "definition": "**TinyTL Memory Breakthrough**: TinyTL's 60x parameter reduction (3M to 50K) translates to dramatic memory savings. In FP32, MobileNetV2 requires ~12MB for weights plus ~8MB for activation caching during training\u2014exceeding most microcontroller capabilities. TinyTL reduces this to ~200KB weights plus ~400KB activations, fitting comfortably within a 1MB memory budget. Real deployments on STM32H7 achieve 85% of full fine-tuning accuracy while using 15x less memory and completing updates in ~30 seconds versus 8 minutes for full training.",
      "term": "TinyTL Memory Breakthrough",
      "length": 539
    },
    {
      "footnote_id": "fn-hey-siri-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 818,
      "definition": "**\"Hey Siri\" Technical Reality**: Apple's \"Hey Siri\" system operates under extreme constraints\u2014detection must complete within 100ms to feel responsive, while consuming less than 1mW power when listening continuously. The always-on processor monitors audio using a 192KB model running at ~0.5 TOPS. False positive rate must be under 0.001% (less than once per day) while maintaining >95% true positive rate across accents, background noise, and speaking styles. The system processes 16kHz audio in 200ms windows, extracting Mel-frequency features for classification.",
      "term": "\"Hey Siri\" Technical Reality",
      "length": 565
    },
    {
      "footnote_id": "fn-tinyml-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 873,
      "definition": "**TinyML Market Reality**: The TinyML market reached $2.4 billion in 2023 and is projected to grow to $23.3 billion by 2030. Over 100 billion microcontrollers ship annually, but fewer than 1% currently support on-device learning due to memory and power constraints. Successful TinyML deployments typically consume <1mW power, use <256KB memory, and cost under $1 per chip. Applications include predictive maintenance (vibration sensors), health monitoring (heart rate variability), and smart agriculture (soil moisture prediction).",
      "term": "TinyML Market Reality",
      "length": 531
    },
    {
      "footnote_id": "fn-wireless-constraints",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1233,
      "definition": "**Wireless Communication Reality**: Mobile devices face severe bandwidth and energy constraints for federated learning. LTE uploads average 5-10 Mbps versus 50+ Mbps downloads, creating asymmetric bottlenecks. Transmitting a 50MB model update consumes ~100mAh battery (2-3% of typical capacity) and takes 40-80 seconds. WiFi improves throughput but isn't always available. Low-power devices using LoRaWAN or NB-IoT face even harsher limits\u2014LoRaWAN maxes at 50kbps with 1% duty cycle restrictions, making frequent updates impractical without aggressive compression.",
      "term": "Wireless Communication Reality",
      "length": 564
    },
    {
      "footnote_id": "fn-arm-cortex-spectrum",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1383,
      "definition": "**ARM Cortex Architecture Spectrum**: The ARM Cortex family spans 6 orders of magnitude in capabilities. Cortex-M0+ (IoT sensors) runs at 48MHz with 32KB RAM and no floating-point, consuming ~10\u00b5W. Cortex-M7 (embedded systems) reaches 400MHz with 1MB RAM and single-precision FPU, consuming ~100mW. Cortex-A78 (smartphones) delivers 3GHz performance with multi-core processing, NEON SIMD, and advanced branch prediction, consuming 1-5W. This diversity means federated learning must adapt algorithms dynamically\u2014quantized inference on M0+, lightweight training on M7, and full backpropagation on A78.",
      "term": "ARM Cortex Architecture Spectrum",
      "length": 599
    },
    {
      "footnote_id": "fn-microcontroller-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ondevice_learning/ondevice_learning.qmd",
      "line": 1431,
      "definition": "**Microcontroller Power Budget Reality**: A typical microcontroller consuming 100mW during training exhausts 3.6 joules per hour, equivalent to a 1000mAh battery in 2.8 hours. Energy harvesting systems collect only 10-100mW continuously (solar panels in indoor light), making sustained training impossible. Real deployments use duty cycling: train for 10 seconds every hour, consuming ~1 joule total. This constrains training to 100-1000 gradient steps maximum, requiring extremely efficient algorithms and careful energy budgeting between sensing, computation, and communication.",
      "term": "Microcontroller Power Budget Reality",
      "length": 580
    },
    {
      "footnote_id": "fn-mlops-emergence",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 41,
      "definition": "**MLOps Emergence**: The term \"MLOps\" was first coined around 2018 by D. Sculley and colleagues at Google, who published the influential paper \"Hidden Technical Debt in Machine Learning Systems.\" The discipline emerged as organizations like Netflix, Uber, and Airbnb faced the \"last mile\" problem\u201490% of ML models never made it to production due to operational challenges.",
      "term": "MLOps Emergence",
      "length": 372
    },
    {
      "footnote_id": "fn-mlops-business-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 51,
      "definition": "**MLOps Business Impact**: Companies implementing mature MLOps practices report 3-10x faster model deployment (from months to weeks), 50-80% reduction in model debugging time, and 20-40% improvement in model reliability. Gartner estimates MLOps can reduce operational ML costs by 30-50% while increasing model success rates from 20% to 85%.",
      "term": "MLOps Business Impact",
      "length": 340
    },
    {
      "footnote_id": "fn-devops-origins",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 61,
      "definition": "**DevOps Origins**: The \"wall of confusion\" between development and operations teams was so notorious that Patrick Debois called his 2009 conference \"DevOpsDays\" specifically to bridge this gap. The movement emerged from the frustrations of the \"throw it over the wall\" mentality where developers built software in isolation from operations teams who had to deploy and maintain it.",
      "term": "DevOps Origins",
      "length": 381
    },
    {
      "footnote_id": "fn-jenkins-history",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 69,
      "definition": "**Jenkins Origins**: Originally called \"Hudson,\" Jenkins was created by Kohsuke Kawaguchi at Sun Microsystems in 2004 to automate his own tedious testing processes. The name change to \"Jenkins\" came in 2011 after a trademark dispute, named after the devoted butler from P.G. Wodehouse's stories.",
      "term": "Jenkins Origins",
      "length": 295
    },
    {
      "footnote_id": "fn-kubernetes-birth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 71,
      "definition": "**Kubernetes Origins**: Greek for \"helmsman,\" Kubernetes emerged from Google's internal Borg system that managed billions of containers across their data centers. Google open-sourced it in 2014, realizing that their competitive advantage wasn't the orchestration system itself, but how they used it to run services at planetary scale.",
      "term": "Kubernetes Origins",
      "length": 334
    },
    {
      "footnote_id": "fn-reproducibility-crisis",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 129,
      "definition": "**ML Reproducibility Crisis**: A 2019 study found that only 54% of AI research papers could be reproduced, compared to 70% in other scientific fields. This crisis led to initiatives like Papers with Code and the requirement for code submission at major ML conferences.",
      "term": "ML Reproducibility Crisis",
      "length": 268
    },
    {
      "footnote_id": "fn-dvc-story",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 157,
      "definition": "**DVC Creation Story**: Data Version Control was born from the frustration of Dmitry Petrov, who spent weeks trying to reproduce an experiment only to discover the training data had been quietly updated. He created DVC in 2017 to bring Git-like versioning to data science, solving what he called \"the biggest unsolved problem in machine learning.\"",
      "term": "DVC Creation Story",
      "length": 347
    },
    {
      "footnote_id": "fn-feature-store-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 264,
      "definition": "**Feature Store Scale**: Uber's Michelangelo feature store serves 10+ million features per second with P99 latency under 10ms, storing 200+ petabytes of feature data. Airbnb's feature store supports 1,000+ ML models with automated feature validation preventing 85% of potential training-serving skew issues.",
      "term": "Feature Store Scale",
      "length": 307
    },
    {
      "footnote_id": "fn-training-serving-skew",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 266,
      "definition": "**Training-Serving Skew Impact**: Studies show training-serving skew causes 5-15% accuracy degradation in production models. Google reported that fixing skew issues improved ad click prediction accuracy by 8%, translating to millions in additional revenue annually.",
      "term": "Training-Serving Skew Impact",
      "length": 265
    },
    {
      "footnote_id": "fn-github-actions-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 302,
      "definition": "**GitHub Actions for ML**: Over 60% of ML teams now use GitHub Actions for CI/CD, with typical ML pipelines taking 15-45 minutes to run (vs. 2-5 minutes for traditional software). Netflix runs 10,000+ ML pipeline executions weekly through GitHub Actions, with 95% success rate on first run.",
      "term": "GitHub Actions for ML",
      "length": 290
    },
    {
      "footnote_id": "fn-kubeflow-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 304,
      "definition": "**Kubeflow Production Usage**: Google's internal Kubeflow deployment runs 500,000+ ML jobs monthly across 50+ clusters, with automatic resource scaling reducing training costs by 40%. Companies like Spotify use Kubeflow to orchestrate 1,000+ concurrent training jobs with fault tolerance.",
      "term": "Kubeflow Production Usage",
      "length": 288
    },
    {
      "footnote_id": "fn-cloud-ml-costs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 530,
      "definition": "**Cloud ML Training Economics**: Training GPT-3 costs approximately $4.6 million on AWS, while fine-tuning typically costs $100-$10,000. Google's TPU v4 pods can reduce training costs by 2-5x compared to equivalent GPU clusters, with some organizations reporting 60-80% cost savings through spot instances and preemptible training.",
      "term": "Cloud ML Training Economics",
      "length": 331
    },
    {
      "footnote_id": "fn-tensorflow-serving",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 650,
      "definition": "**TensorFlow Serving**: Google's production-grade ML serving system handles over 100,000 queries per second per machine with <10ms latency for most models. Originally built to serve YouTube's recommendation system, processing over 1 billion hours of video watched daily.",
      "term": "TensorFlow Serving",
      "length": 270
    },
    {
      "footnote_id": "fn-triton-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 652,
      "definition": "**NVIDIA Triton Inference Server**: Can achieve up to 40,000 inferences per second on a single A100 GPU for BERT models, with dynamic batching reducing latency by up to 10x compared to naive serving approaches. Supports concurrent execution of up to 100 different model types.",
      "term": "NVIDIA Triton Inference Server",
      "length": 276
    },
    {
      "footnote_id": "fn-kserve-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 654,
      "definition": "**KServe (formerly KFServing)**: Kubernetes-native serving framework that can autoscale from zero to thousands of replicas in under 30 seconds. Used by companies like Bloomberg to serve over 10,000 models simultaneously with 99.9% uptime SLA.",
      "term": "KServe (formerly KFServing)",
      "length": 242
    },
    {
      "footnote_id": "fn-sla-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 664,
      "definition": "**Service Level Agreements (SLAs)**: Production ML systems typically target 99.9% uptime (8.77 hours downtime/year) for critical services, with penalties of 10-25% monthly service credits for each 0.1% below target. Google's Cloud AI Platform promises 99.95% uptime with automatic failover in <30 seconds.",
      "term": "Service Level Agreements (SLAs)",
      "length": 305
    },
    {
      "footnote_id": "fn-slo-reality",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 666,
      "definition": "**Service Level Objectives (SLOs)**: Real-world ML serving SLOs often specify P95 latency <100ms for online inference, P99 <500ms, and error rates <0.1%. Netflix's recommendation system maintains P99 latency under 150ms while serving 200+ million users, processing 3+ billion hours of content monthly.",
      "term": "Service Level Objectives (SLOs)",
      "length": 301
    },
    {
      "footnote_id": "fn-ml-autoscaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 717,
      "definition": "**ML Autoscaling at Scale**: Kubernetes-based ML serving can scale from 1 to 1,000+ replicas in under 60 seconds. Uber's ML platform automatically scales 2,000+ models daily, reducing infrastructure costs by 35-50% through intelligent resource allocation and cold-start optimization achieving 99.95% availability.",
      "term": "ML Autoscaling at Scale",
      "length": 313
    },
    {
      "footnote_id": "fn-drift-detection",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 733,
      "definition": "**Model Drift Detection**: Production systems typically trigger alerts when accuracy drops >5% over 24 hours or >10% over a week. Advanced systems like those at Spotify detect drift within 2-4 hours using statistical tests, with 85% of drift incidents caught before user impact.",
      "term": "Model Drift Detection",
      "length": 278
    },
    {
      "footnote_id": "fn-covid-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 737,
      "definition": "**COVID-19 ML Impact**: E-commerce recommendation systems saw accuracy drops of 15-40% within weeks of lockdowns beginning in March 2020. Amazon reported having to retrain over 1,000 models, while Netflix saw a 25% increase in viewing time that broke their capacity planning models.",
      "term": "COVID-19 ML Impact",
      "length": 282
    },
    {
      "footnote_id": "fn-prometheus-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 742,
      "definition": "**Prometheus at Scale**: Can ingest 1+ million samples per second per instance, with some deployments monitoring 100,000+ machines. DigitalOcean's Prometheus setup stores 2+ years of metrics data across 40,000+ time series, with query response times under 100ms for 95% of requests.",
      "term": "Prometheus at Scale",
      "length": 282
    },
    {
      "footnote_id": "fn-alerting-thresholds",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 746,
      "definition": "**Production Alert Thresholds**: Typical ML production alerts fire when GPU memory >90%, CPU >85% for >5 minutes, P99 latency >2x normal for >10 minutes, or error rates >1% for >60 seconds. High-frequency trading firms use microsecond-level alerts, while batch processing systems may use hour-long windows.",
      "term": "Production Alert Thresholds",
      "length": 306
    },
    {
      "footnote_id": "fn-shap-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 760,
      "definition": "**SHAP in Production**: SHAP explanations add 10-500ms latency per prediction depending on model complexity, making them costly for real-time serving. However, 40% of enterprise ML teams now use SHAP in production, with Microsoft reporting SHAP helped identify $2M in potential bias-related legal exposure in their hiring models.",
      "term": "SHAP in Production",
      "length": 329
    },
    {
      "footnote_id": "fn-tech-debt-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 792,
      "definition": "**Technical Debt Origins**: Ward Cunningham coined the term in 1992, comparing rushed coding decisions to financial debt: \"A little debt speeds development so long as it is paid back promptly with a rewrite.\" He later regretted the metaphor became an excuse for bad code rather than a tool for communicating tradeoffs.",
      "term": "Technical Debt Origins",
      "length": 318
    },
    {
      "footnote_id": "fn-youtube-engagement",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1027,
      "definition": "**YouTube Recommendation Impact**: The recommendation system drives 70% of watch time on the platform (1+ billion hours daily), but algorithmic changes in 2016 increased average session time by 50% while inadvertently promoting conspiracy content. Fixing these feedback loops required 2+ years of engineering work and new evaluation frameworks.",
      "term": "YouTube Recommendation Impact",
      "length": 344
    },
    {
      "footnote_id": "fn-zillow-losses",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/ops/ops.qmd",
      "line": 1033,
      "definition": "**Zillow iBuying Failure**: Zillow lost $881 million in a single quarter (Q3 2021) due to ML model failures, with the Zestimate algorithm overvaluing homes by an average of 5-7%. The company laid off 2,000+ employees and took a $569 million inventory write-down when shutting down Zillow Offers.",
      "term": "Zillow iBuying Failure",
      "length": 295
    },
    {
      "footnote_id": "fn-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 56,
      "definition": "**Pruning**: The optimal brain damage algorithm by Yann LeCun (1990) pioneered removing unnecessary neural network connections, inspiring modern magnitude-based and structured pruning techniques that can reduce model sizes by 90% with minimal accuracy loss.",
      "term": "Pruning",
      "length": 257
    },
    {
      "footnote_id": "fn-knowledge-distill",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 58,
      "definition": "**Knowledge Distillation**: Geoffrey Hinton et al. (2015) introduced this technique where a smaller \"student\" network learns from a larger \"teacher\" network's soft outputs, enabling compact models that retain much of the original's performance while being orders of magnitude more efficient.",
      "term": "Knowledge Distillation",
      "length": 291
    },
    {
      "footnote_id": "fn-structured-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 682,
      "definition": "**Structured Pruning**: Filter pruning in ResNet-34 achieves 50% FLOP reduction with only 1.0% accuracy loss on CIFAR-10. Channel pruning in MobileNetV2 reduces parameters by 73% while maintaining 96.5% of original accuracy, enabling 3.2x faster inference on ARM processors.",
      "term": "Structured Pruning",
      "length": 274
    },
    {
      "footnote_id": "fn-lottery-ticket",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1308,
      "definition": "**Lottery Ticket Hypothesis**: Frankle & Carbin (2018) demonstrated that ResNet-18 subnetworks at 10-20% original size achieve 93.2% accuracy vs. 94.1% for full model on CIFAR-10. BERT-base winning tickets retain 97% performance with 90% fewer parameters, requiring 5-8x less training time to converge.",
      "term": "Lottery Ticket Hypothesis",
      "length": 302
    },
    {
      "footnote_id": "fn-bert-compression",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1599,
      "definition": "**BERT Compression**: BERT-Base (110M params) can be compressed to 67M params (39% reduction) with only 1.2% GLUE score drop. Attention head pruning removes 144 of 192 heads with minimal impact, while layer pruning reduces 12 layers to 6 layers maintaining 97.8% performance.",
      "term": "BERT Compression",
      "length": 275
    },
    {
      "footnote_id": "fn-distilbert-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1601,
      "definition": "**DistilBERT**: Achieves 97% of BERT-Base performance with 40% fewer parameters (66M vs. 110M) and 60% faster inference. On SQuAD v1.1, DistilBERT scores 86.9 F1 vs. BERT's 88.5 F1, while reducing memory from 1.35GB to 0.54GB and latency from 85ms to 34ms.",
      "term": "DistilBERT",
      "length": 256
    },
    {
      "footnote_id": "fn-efficientnet-pruning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 1603,
      "definition": "**EfficientNet Pruning**: EfficientNet-B0 with 70% structured pruning maintains 75.8% ImageNet accuracy (vs. 77.1% original) with 2.8x speedup on mobile devices. Channel pruning reduces FLOPs from 390M to 140M while keeping inference under 20ms on Pixel 4.",
      "term": "EfficientNet Pruning",
      "length": 256
    },
    {
      "footnote_id": "fn-hardware-aware-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2366,
      "definition": "**Hardware-Aware NAS**: MnasNet achieves 78.1% ImageNet accuracy with 315M FLOPs vs. MobileNetV2's 72.0% with 300M FLOPs. EfficientNet-B0 delivers 77.1% accuracy with 390M FLOPs, 23% better accuracy/FLOP ratio than ResNet-50, enabling 4.9x faster mobile inference.",
      "term": "Hardware-Aware NAS",
      "length": 264
    },
    {
      "footnote_id": "fn-nas-evaluation-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2368,
      "definition": "**NAS Evaluation Metrics**: Multi-objective optimization considers accuracy (top-1/top-5), latency (ms on target hardware), memory (MB activations + parameters), and energy (mJ per inference). Pareto-optimal architectures provide 15-40% better efficiency frontiers than manual designs.",
      "term": "NAS Evaluation Metrics",
      "length": 285
    },
    {
      "footnote_id": "fn-fbnet-nas",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2370,
      "definition": "**FBNet**: Achieves 74.9% ImageNet accuracy with 375M FLOPs and 23ms latency on Samsung S8, 15% faster than MobileNetV2 with comparable accuracy. The latency-aware search uses device-specific lookup tables for actual hardware performance measurement. Instead of selecting the most accurate model, NAS identified architectures that provided the best balance between accuracy and inference speed [@wu2019fbnet]. Similarly, EfficientNet was discovered through NAS by jointly optimizing for accuracy and computational efficiency, resulting in a model that delivers state-of-the-art performance while reducing FLOPs compared to conventional architectures [@tan2019efficientnet].",
      "term": "FBNet",
      "length": 673
    },
    {
      "footnote_id": "fn-onnx-deployment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2819,
      "definition": "**ONNX Deployment**: ONNX Runtime achieves 1.3-2.9x speedup over TensorFlow and 1.1-1.7x over PyTorch across various models. ResNet-50 inference drops from 7.2ms to 2.8ms on CPU, while BERT-Base reduces from 45ms to 23ms with ONNX Runtime optimizations including graph fusion and memory pooling.",
      "term": "ONNX Deployment",
      "length": 295
    },
    {
      "footnote_id": "fn-tensorrt-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 2821,
      "definition": "**TensorRT Optimization**: NVIDIA TensorRT delivers 5-40x speedup for inference on V100 GPUs. ResNet-50 INT8 inference achieves 1.2ms vs. 4.8ms FP32, while BERT-Large drops from 10.4ms to 2.1ms. Layer fusion reduces kernel launches by 80%, memory bandwidth by 50%.",
      "term": "TensorRT Optimization",
      "length": 264
    },
    {
      "footnote_id": "fn-qat-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 3419,
      "definition": "**Quantization-Aware Training**: QAT enables INT8 inference with minimal accuracy loss - ResNet-50 maintains 76.1% vs. 76.2% FP32 ImageNet accuracy, while MobileNetV2 achieves 71.8% vs. 72.0%. BERT-Base INT8 retains 99.1% of FP32 performance on GLUE, compared to 96.8% with post-training quantization alone.",
      "term": "Quantization-Aware Training",
      "length": 307
    },
    {
      "footnote_id": "fn-energy-efficiency-metrics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5481,
      "definition": "**Energy Efficiency Metrics**: INT8 quantization reduces energy consumption by 4-8x over FP32. MobileNetV2 INT8 consumes 47mJ vs. 312mJ FP32 per inference on Cortex-A75. ResNet-50 on TPU v4 achieves 0.9 TOPS/Watt vs. 0.3 TOPS/Watt on V100 GPU.",
      "term": "Energy Efficiency Metrics",
      "length": 243
    },
    {
      "footnote_id": "fn-sparse-energy-savings",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5483,
      "definition": "**Sparse Energy Savings**: 90% sparsity in BERT reduces training energy by 2.3x and inference energy by 4.1x on V100. Structured 2:4 sparsity patterns deliver 1.6x energy savings on A100 GPUs while maintaining 99% of dense model accuracy. Hardware accelerators like TPUs and GPUs are optimized to handle these operations efficiently, making sparse networks not only faster but also more energy-efficient [@Cheng2022].",
      "term": "Sparse Energy Savings",
      "length": 417
    },
    {
      "footnote_id": "fn-memory-optimization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5617,
      "definition": "**Memory Optimization**: DenseNet-121 reduces memory consumption by 50% compared to ResNet-50 through feature reuse, requiring only 7.9MB vs. 15.3MB activation memory on ImageNet. MobileNetV3 achieves 73% memory reduction with depth-wise separable convolutions, enabling deployment on 2GB mobile devices.",
      "term": "Memory Optimization",
      "length": 304
    },
    {
      "footnote_id": "fn-activation-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5619,
      "definition": "**Activation Checkpointing**: Enables training 4x larger models on same hardware by trading computation for memory. BERT-Large training memory drops from 16GB to 4GB with 20% computational overhead, while GPT-3 175B uses checkpointing to fit on 8x A100 GPUs instead of 32x.",
      "term": "Activation Checkpointing",
      "length": 273
    },
    {
      "footnote_id": "fn-batch-size-effects",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5621,
      "definition": "**Batch Size Effects**: Large batches (512-2048) improve throughput by 2-4x but require gradient accumulation for memory constraints. Linear scaling rule maintains convergence: learning rate scales linearly with batch size, enabling ImageNet training in 1 hour with batch size 8192. Instead of relying on trial and error, AutoML frameworks employ systematic search strategies, including Bayesian optimization, evolutionary algorithms, and adaptive heuristics, to efficiently identify the best hyperparameter settings for a given model and dataset [@Bergstra2011].",
      "term": "Batch Size Effects",
      "length": 563
    },
    {
      "footnote_id": "fn-xla-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5753,
      "definition": "**XLA (Accelerated Linear Algebra)**: Google's domain-specific compiler achieves 1.15-1.4x speedup on ResNet-50 inference and 1.2-1.7x on BERT-Large training through operator fusion and memory optimization. XLA reduces HBM traffic by 25-40% through aggressive kernel fusion, delivering 15-30% energy savings on TPUs.",
      "term": "XLA (Accelerated Linear Algebra)",
      "length": 316
    },
    {
      "footnote_id": "fn-tvm-compiler",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5755,
      "definition": "**TVM (Tensor Virtual Machine)**: Apache TVM's auto-tuning delivers 1.2-2.8x speedup over vendor libraries on ARM CPUs and 1.5-3.2x on mobile GPUs. TVM's graph-level optimizations reduce inference latency by 40-60% on edge devices through operator scheduling and memory planning.",
      "term": "TVM (Tensor Virtual Machine)",
      "length": 279
    },
    {
      "footnote_id": "fn-operator-fusion",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/optimizations/optimizations.qmd",
      "line": 5763,
      "definition": "**Operator Fusion**: Graph-level optimization that combines multiple operations into single kernels, reducing memory bandwidth by 30-50%. In ResNet-50, fusing Conv+BatchNorm+ReLU operations achieves 1.8x speedup on V100 GPUs, while BERT transformer blocks show 25% latency reduction through attention fusion. Dynamic computation approaches like early exit architectures and conditional computation are supported by custom execution runtimes that optimize control flow.",
      "term": "Operator Fusion",
      "length": 468
    },
    {
      "footnote_id": "fn-model-extraction-2016",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 46,
      "definition": "**Model Extraction Threat**: The vulnerability was first demonstrated in 2016 when researchers showed they could steal machine learning models through API queries alone. By systematically querying a model and analyzing responses, attackers could recreate proprietary models worth millions in R&D investment\u2014turning public APIs into inadvertent IP leakage channels.",
      "term": "Model Extraction Threat",
      "length": 364
    },
    {
      "footnote_id": "fn-gdpr-penalties",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 98,
      "definition": "**GDPR**: The General Data Protection Regulation, enacted in 2018, imposes fines up to 4% of global annual revenue or \u20ac20 million (whichever is higher) for privacy violations. Since implementation, it has generated over \u20ac1.6 billion in fines, with a single 2021 penalty against Amazon reaching \u20ac746 million\u2014demonstrating the regulation's significant financial impact on data-driven businesses.",
      "term": "GDPR",
      "length": 393
    },
    {
      "footnote_id": "fn-stuxnet-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 118,
      "definition": "**Stuxnet Discovery**: The malware was first detected by VirusBokNok, a small Belarusian antivirus company, when their client computers began crashing unexpectedly. What seemed like a routine malware investigation turned into one of the most significant cybersecurity discoveries in history\u2014the first confirmed cyberweapon designed to cause physical destruction.",
      "term": "Stuxnet Discovery",
      "length": 362
    },
    {
      "footnote_id": "fn-zero-day-term",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 120,
      "definition": "**Zero-Day Etymology**: The term originated in software piracy circles, referring to the \"zero days\" since a program's release when pirated copies appeared. In security, it describes the \"zero days\" defenders have to patch a vulnerability before attackers exploit it\u2014representing the ultimate race between attack and defense.",
      "term": "Zero-Day Etymology",
      "length": 325
    },
    {
      "footnote_id": "fn-mirai-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 150,
      "definition": "**Mirai Botnet Scale**: At its peak, Mirai controlled over 600,000 infected IoT devices, generating 623 Gbps DDoS attacks that took down major internet services including Twitter, Netflix, and Reddit for hours. The attack revealed that IoT devices with default credentials (admin/admin, root/12345) could be weaponized at unprecedented scale.",
      "term": "Mirai Botnet Scale",
      "length": 342
    },
    {
      "footnote_id": "fn-model-inversion-attack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 273,
      "definition": "**Model Inversion Attack**: First demonstrated in 2015 against face recognition systems, researchers reconstructed recognizable faces from neural network outputs using only confidence scores. The attack revealed that models trained on 40 individuals could leak identifiable facial features, proving that \"black-box\" API access isn't sufficient privacy protection.",
      "term": "Model Inversion Attack",
      "length": 363
    },
    {
      "footnote_id": "fn-netflix-deanonymization",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 275,
      "definition": "**Netflix Deanonymization**: In 2008, researchers re-identified Netflix users by correlating the \"anonymous\" Prize dataset with public IMDb ratings. Using as few as 8 movie ratings with dates, they identified 99% of users, leading Netflix to cancel a second competition and highlighting the futility of naive anonymization.",
      "term": "Netflix Deanonymization",
      "length": 323
    },
    {
      "footnote_id": "fn-meltdown-spectre-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 588,
      "definition": "**Meltdown/Spectre Impact**: Disclosed in January 2018, these vulnerabilities affected virtually every processor made since 1995\u2014over 20 billion devices. The disclosure triggered emergency patches across all major operating systems, causing 5-30% performance degradation in some workloads, and led to a fundamental rethinking of processor security.",
      "term": "Meltdown/Spectre Impact",
      "length": 348
    },
    {
      "footnote_id": "fn-speculative-execution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 590,
      "definition": "**Speculative Execution**: Introduced in the Intel Pentium Pro (1995), this technique executes instructions before confirming they're needed, improving performance by 10-25%. However, it creates a 20+ year attack window where speculated operations leak data through cache timing, affecting ML accelerators that rely on similar optimizations.",
      "term": "Speculative Execution",
      "length": 341
    },
    {
      "footnote_id": "fn-hipaa-violations",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 600,
      "definition": "**HIPAA Violations**: Since enforcement began in 2003, HIPAA has generated over $130 million in fines, with individual penalties reaching $16 million. The largest healthcare data breach affected 78.8 million patients at Anthem Inc. in 2015, highlighting the massive scale of exposure when ML systems handling medical data are compromised.",
      "term": "HIPAA Violations",
      "length": 338
    },
    {
      "footnote_id": "fn-dp-sgd-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 850,
      "definition": "**DP-SGD Industry Adoption**: Apple was the first major company to deploy differential privacy at scale in 2016, protecting 1+ billion users' data in iOS. Their implementation adds noise to emoji usage, Safari crashes, and QuickType suggestions, balancing privacy (\u03b5=4-16) with utility for improving user experience across their ecosystem.",
      "term": "DP-SGD Industry Adoption",
      "length": 339
    },
    {
      "footnote_id": "fn-federated-learning-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 865,
      "definition": "**Federated Learning Scale**: Google's Gboard keyboard uses federated learning across 1+ billion devices to improve text predictions without sending keystrokes to servers. Each phone trains locally on typing patterns, sharing only model updates\u2014processing 4 billion characters daily while preserving privacy at unprecedented scale.",
      "term": "Federated Learning Scale",
      "length": 331
    },
    {
      "footnote_id": "fn-smpc-overhead",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 876,
      "definition": "**SMPC Performance**: Secure multi-party computation typically incurs 1000-10,000x computational overhead compared to plaintext operations. A simple neural network inference that takes milliseconds on GPU requires hours using SMPC, limiting practical applications to small models and offline scenarios.",
      "term": "SMPC Performance",
      "length": 302
    },
    {
      "footnote_id": "fn-trustzone-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1072,
      "definition": "**ARM TrustZone**: Introduced in 2004, TrustZone now ships in 95% of ARM processors, protecting over 5 billion mobile devices. Despite its ubiquity, many devices underutilize TrustZone\u2014studies show only 20-30% of Android devices implement meaningful secure world applications beyond basic key storage.",
      "term": "ARM TrustZone",
      "length": 301
    },
    {
      "footnote_id": "fn-intel-sgx-limits",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1074,
      "definition": "**Intel SGX Constraints**: SGX enclaves are limited to 128MB of protected memory (EPC), with cache misses causing 100x performance penalties. This makes SGX unsuitable for large ML models but effective for protecting cryptographic keys and small inference models under 10MB.",
      "term": "Intel SGX Constraints",
      "length": 274
    },
    {
      "footnote_id": "fn-hsm-performance",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1287,
      "definition": "**HSM Performance**: Enterprise HSMs can perform 10,000+ RSA-2048 operations per second but cost $20,000-$100,000+ per unit. In contrast, software-only cryptography on GPUs achieves 100,000+ operations/second at $1,000+ hardware cost, but without the tamper-resistance and regulatory compliance that HSMs provide.",
      "term": "HSM Performance",
      "length": 313
    },
    {
      "footnote_id": "fn-puf-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/privacy_security/privacy_security.qmd",
      "line": 1313,
      "definition": "**PUF Market Growth**: The PUF market is projected to reach $320 million by 2025, driven by IoT security needs. Major semiconductor companies including Intel, Xilinx, and Synopsis now offer PUF IP, with deployment in smart cards, automotive ECUs, and edge ML devices requiring device-unique authentication.",
      "term": "PUF Market Growth",
      "length": 306
    },
    {
      "footnote_id": "fn-demographic-parity-origin",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 55,
      "definition": "**Demographic Parity Origins**: This fairness criterion was first formalized by computer scientist Cynthia Dwork and colleagues in 2011, building on legal concepts from the 1971 Supreme Court case *Griggs v. Duke Power Co.*, which established that employment practices with disparate impact could violate civil rights law even without discriminatory intent. The mathematical formalization bridged legal theory with algorithmic practice.",
      "term": "Demographic Parity Origins",
      "length": 436
    },
    {
      "footnote_id": "fn-compas-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 99,
      "definition": "**COMPAS Algorithm Controversy**: A 2016 ProPublica investigation revealed that COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) incorrectly flagged Black defendants as future criminals at nearly twice the rate of white defendants (45% vs 24%), while white defendants were mislabeled as low-risk more often than Black defendants (48% vs 28%). The algorithm was used in sentencing decisions across multiple states despite these documented disparities.",
      "term": "COMPAS Algorithm Controversy",
      "length": 479
    },
    {
      "footnote_id": "fn-gdpr-article-22",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 105,
      "definition": "**GDPR Article 22**: Known as the \"right to explanation,\" this provision affects an estimated 500 million EU citizens and has inspired similar legislation worldwide. Since GDPR's 2018 implementation, regulators have issued over \u20ac5.88 billion in fines as of January 2025, with many cases involving algorithmic decision-making. The regulation's global influence extends beyond Europe\u2014over 120 countries now have privacy laws modeled on GDPR principles.",
      "term": "GDPR Article 22",
      "length": 450
    },
    {
      "footnote_id": "fn-healthcare-algorithm-bias",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 117,
      "definition": "**Healthcare Algorithm Scale**: This Optum algorithm affected approximately 200 million Americans annually, determining access to high-risk care management programs. The bias reduced Black patients' enrollment by 50%\u2014if corrected, the number of Black patients identified for extra care would increase from 17.7% to 46.5%, highlighting how algorithmic decisions can perpetuate healthcare disparities at massive scale.",
      "term": "Healthcare Algorithm Scale",
      "length": 416
    },
    {
      "footnote_id": "fn-adversarial-examples",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 236,
      "definition": "**Adversarial Examples Discovery**: First discovered by Christian Szegedy and colleagues at Google in 2013, adversarial examples revealed that small, imperceptible changes to inputs could cause state-of-the-art neural networks to misclassify with high confidence. The original paper showed that adding noise with magnitude 0.007 (less than 1% of the input range) could cause a panda image to be classified as a gibbon with 99.3% confidence, fundamentally challenging assumptions about neural network reliability.",
      "term": "Adversarial Examples Discovery",
      "length": 512
    },
    {
      "footnote_id": "fn-differential-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 483,
      "definition": "**Differential Privacy**: Introduced by Cynthia Dwork in 2006, differential privacy revolutionized privacy-preserving computation by providing mathematical guarantees rather than heuristic protections. Apple was among the first major companies to deploy differential privacy at scale in 2016, using it to collect iOS usage statistics from over 1 billion devices while preserving individual privacy. The technique adds calibrated noise to computations, ensuring that no single person's data significantly affects the output.",
      "term": "Differential Privacy",
      "length": 523
    },
    {
      "footnote_id": "fn-value-alignment",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/responsible_ai/responsible_ai.qmd",
      "line": 1004,
      "definition": "**Value Alignment Problem**: First articulated by philosopher Nick Bostrom in 2012 and formalized by Stuart Russell in 2016, the value alignment problem asks how to ensure AI systems pursue goals compatible with human welfare. The challenge became prominent after observing that simple objectives often lead to unintended consequences\u2014famously illustrated by the \"paperclip maximizer\" thought experiment, where an AI optimizing paperclip production might ultimately convert all available matter into paperclips, including humans.",
      "term": "Value Alignment Problem",
      "length": 529
    },
    {
      "footnote_id": "fn-adversarial-discovery",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 26,
      "definition": "**Adversarial Examples Discovery**: Christian Szegedy and colleagues at Google discovered adversarial examples almost by accident in 2013 while trying to understand what neural networks had learned. Their finding that \"imperceptibly small perturbations\" could fool state-of-the-art models sent shockwaves through the ML community and launched an entire field of adversarial machine learning research. This reality underscores the need to fundamentally rethink how machine learning systems are designed and deployed, placing robustness and trustworthiness at the forefront. Building resilient machine learning systems is not merely a technical objective; it is a foundational requirement for ensuring their safe and effective operation in dynamic and uncertain environments.",
      "term": "Adversarial Examples Discovery",
      "length": 773
    },
    {
      "footnote_id": "fn-safety-critical-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 46,
      "definition": "**Safety-Critical Computing Evolution**: The field evolved from nuclear reactor control systems in the 1950s to aviation in the 1970s (DO-178B standard), and now encompasses ML systems. The key insight is that in safety-critical domains, \"good enough\" performance isn't enough\u2014systems must be provably safe under all anticipated conditions.",
      "term": "Safety-Critical Computing Evolution",
      "length": 340
    },
    {
      "footnote_id": "fn-data-poisoning-tay",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 50,
      "definition": "**Data Poisoning Emergence**: The threat became prominent with Microsoft's Tay chatbot (2016), which was \"corrupted\" by coordinated user inputs within 24 hours. This incident revealed how bad actors could systematically manipulate training data or model updates to create backdoors or degrade performance\u2014a vulnerability unique to learning systems.",
      "term": "Data Poisoning Emergence",
      "length": 348
    },
    {
      "footnote_id": "fn-ml-security-threats",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 73,
      "definition": "**ML Security Threat Taxonomy**: Unlike traditional software security, ML systems face unique threats: training-time attacks (data poisoning, backdoors), inference-time attacks (adversarial examples, model inversion), and deployment-time attacks (model stealing, membership inference). Each category requires distinct defense mechanisms and represents billions in potential economic impact. By understanding and addressing these multifaceted challenges, it is possible to develop reliable ML systems capable of operating effectively in real-world environments.",
      "term": "ML Security Threat Taxonomy",
      "length": 560
    },
    {
      "footnote_id": "fn-sdc-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 91,
      "definition": "**Silent Data Corruption Challenge**: SDC became a major concern as data centers scaled to millions of servers. Unlike ECC memory errors that are detected, SDC errors pass all checks but contain wrong values. Google estimated that 1 in 1,700 servers experienced SDC per year\u2014seemingly rare until multiplied by planetary-scale infrastructure.",
      "term": "Silent Data Corruption Challenge",
      "length": 341
    },
    {
      "footnote_id": "fn-seu-space-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 264,
      "definition": "**Single Event Upset Rates**: SEU rates vary dramatically with altitude and technology node. At ground level, 28nm processors experience ~1 SEU per device per 1000 hours, but this increases 100x in space environments. Modern AI accelerators with millions of parameters are particularly vulnerable due to their large memory footprints. Understanding these different types of transient faults is crucial for designing robust and resilient hardware systems that can mitigate their impact and ensure reliable operation.",
      "term": "Single Event Upset Rates",
      "length": 515
    },
    {
      "footnote_id": "fn-cosmic-ray-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 326,
      "definition": "**Cosmic Ray Impact**: At sea level, approximately 1000 cosmic ray particles pass through each square meter of surface every minute. Modern 16nm chips experience roughly 1 soft error per billion device-hours of operation due to cosmic radiation\u2014a rate that increases dramatically at higher altitudes where atmospheric shielding is reduced. This is illustrated in @fig-transient-fault. [Electromagnetic interference (EMI)](https://www.trentonsystems.com/en-us/resource-hub/blog/what-is-electromagnetic-interference) from nearby devices represents another significant external cause, where electromagnetic fields couple with circuits and cause voltage spikes or glitches that temporarily disrupt normal operation. Electrostatic discharge (ESD) events, resulting from sudden static electricity flow, can also induce transient faults by creating temporary voltage surges that affect sensitive electronic components.",
      "term": "Cosmic Ray Impact",
      "length": 911
    },
    {
      "footnote_id": "fn-bit-flip-dram",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 342,
      "definition": "**DRAM Bit Flip Rates**: Modern DDR4 memory experiences approximately 1 uncorrectable bit flip per 10^17 bit-hours. While ECC memory can correct single-bit errors, multi-bit errors occur roughly once per 10^14 bit-hours, making them a significant concern for large-scale AI training clusters with thousands of GPUs. For instance, a bit flip in the memory storing a loop counter can cause the loop to execute indefinitely or terminate prematurely. Transient faults in control registers or flag bits can alter the flow of program execution, leading to unexpected jumps or incorrect branch decisions. In communication systems, transient faults can corrupt transmitted data packets, resulting in retransmissions or data loss.",
      "term": "DRAM Bit Flip Rates",
      "length": 721
    },
    {
      "footnote_id": "fn-stochastic-computing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 354,
      "definition": "**Stochastic Computing Resilience**: Stochastic computing represents numbers as probabilities in bit streams, making it inherently fault-tolerant. A single bit flip in a 1000-bit stream changes the represented value by only 0.1%, compared to potentially catastrophic errors in binary representations. This makes it promising for fault-tolerant AI inference in harsh environments.",
      "term": "Stochastic Computing Resilience",
      "length": 379
    },
    {
      "footnote_id": "fn-fdiv-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 365,
      "definition": "**FDIV Bug Economic Impact**: Intel's Pentium FDIV bug cost the company $475 million in 1994 (equivalent to $900 million in 2024). The bug affected approximately 5 million processors and occurred in 1 out of every 9 billion division operations, demonstrating how rare but systematic errors can have massive financial consequences. This flaw affected the floating-point division (FDIV) units of certain Intel Pentium processors, causing incorrect results for specific division operations and leading to inaccurate calculations.",
      "term": "FDIV Bug Economic Impact",
      "length": 526
    },
    {
      "footnote_id": "fn-electromigration-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 382,
      "definition": "**Electromigration in AI Hardware**: Electromigration\u2014the gradual movement of metal atoms due to high current density\u2014is accelerated in AI accelerators that run at high utilization. NVIDIA GPUs running AI workloads can experience 10-year mean time to failure, compared to 25-30 years for typical computing workloads, due to sustained high power density.",
      "term": "Electromigration in AI Hardware",
      "length": 353
    },
    {
      "footnote_id": "fn-stuck-at-ml-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 388,
      "definition": "**Stuck-at Faults in ML**: In neural networks, stuck-at faults in weight memory can be devastating. A single stuck weight in a critical layer can reduce accuracy by 20-40%. However, research shows that neural networks have some natural resilience\u2014up to 10% of weights can be stuck without significant performance loss due to the distributed nature of learning. This type of fault can occur in logic gates, memory cells, or interconnects and typically results in incorrect computations or persistent data corruption.",
      "term": "Stuck-at Faults in ML",
      "length": 515
    },
    {
      "footnote_id": "fn-fgsm-breakthrough",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1042,
      "definition": "**FGSM Breakthrough**: Ian Goodfellow invented FGSM in 2014 while working at Google, creating the first practical algorithm for generating adversarial examples. His insight was elegantly simple: \"If you want to fool a neural network, just push the input in the direction that increases the loss most rapidly.\" This single-step attack became the foundation for an entire field of adversarial research. : **FGSM Breakthrough**: Ian Goodfellow invented FGSM in 2014 while working at Google, creating the first practical algorithm for generating adversarial examples. His insight was elegantly simple: \"If you want to fool a neural network, just push the input in the direction that increases the loss most rapidly.\" This single-step attack became the foundation for an entire field of adversarial research.",
      "term": "FGSM Breakthrough",
      "length": 803
    },
    {
      "footnote_id": "fn-pgd-benchmark",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1064,
      "definition": "**PGD Attack Strength**: Introduced by Madry et al. in 2017, PGD became the gold standard for evaluating adversarial robustness. On CIFAR-10, PGD can achieve 100% attack success rate against undefended models with perturbations as small as 8/255 pixel intensity\u2014imperceptible to humans but devastating to neural networks. PGD projects each perturbation step back into a constrained norm ball around the original input, ensuring that the adversarial example remains within a specified distortion limit. This makes PGD a stronger white-box attack and a benchmark for evaluating model robustness.",
      "term": "PGD Attack Strength",
      "length": 593
    },
    {
      "footnote_id": "fn-cw-attack-power",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1074,
      "definition": "**C&W Attack Effectiveness**: The C&W attack, published in 2017, broke 10 defense mechanisms that were considered state-of-the-art at the time. It can find adversarial examples with L2 distortions 3x smaller than FGSM while maintaining 100% success rate, making it one of the most powerful optimization-based attacks. It finds the smallest perturbation that can cause misclassification while maintaining the perceptual similarity to the original input. The C&W attack employs an iterative optimization process to minimize the perturbation while maximizing the model's prediction error. It uses a customized loss function with a confidence term to generate more confident misclassifications.",
      "term": "C&W Attack Effectiveness",
      "length": 690
    },
    {
      "footnote_id": "fn-transferability-rates",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1086,
      "definition": "**Adversarial Transferability**: Research shows that adversarial examples transfer across models with 70-90% success rate when models share similar architectures, and 30-50% even across different architectures. This property enables black-box attacks where attackers can fool unknown models by crafting attacks on publicly available substitutes. Transferability refers to the phenomenon where adversarial examples crafted for one ML model can often fool other models, even if they have different architectures or were trained on different datasets. This enables attackers to generate adversarial examples using a surrogate model and then transfer them to the target model without requiring direct access to its parameters or gradients.",
      "term": "Adversarial Transferability",
      "length": 735
    },
    {
      "footnote_id": "fn-patch-effectiveness",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1096,
      "definition": "**Adversarial Patch Success**: Brown et al. (2017) demonstrated that adversarial patches covering just 2% of an image could fool state-of-the-art ImageNet classifiers 87.9% of the time. These patches remain effective when printed and photographed, bridging the gap between digital and physical-world attacks. These patches are designed to work under varying lighting conditions, viewing angles, and distances, making them robust in real-world environments.",
      "term": "Adversarial Patch Success",
      "length": 456
    },
    {
      "footnote_id": "fn-adversarial-training-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1131,
      "definition": "**Adversarial Training Overhead**: Standard adversarial training increases training time by 6-10x compared to normal training due to the need to generate adversarial examples for each batch. Despite this cost, it remains the most effective defense, improving robustness from <5% to 45-50% accuracy under PGD attacks on CIFAR-10. Exposing the model to adversarial examples during training teaches it to classify them correctly and become more resilient to attacks. Defensive distillation, input preprocessing, and ensemble methods are other techniques that can help mitigate the impact of adversarial attacks.",
      "term": "Adversarial Training Overhead",
      "length": 608
    },
    {
      "footnote_id": "fn-stop-sign-attack",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1143,
      "definition": "**Stop Sign Attack Precision**: The 2017 study by Eykholt et al. achieved 100% success rate under laboratory conditions and 84.8% success rate in real-world conditions using just 4 small rectangular stickers. The attack worked across multiple viewing angles (up to 45 degrees) and distances (5-55 feet), demonstrating the robustness of physical adversarial attacks.",
      "term": "Stop Sign Attack Precision",
      "length": 365
    },
    {
      "footnote_id": "fn-defensive-distillation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1157,
      "definition": "**Defensive Distillation Effectiveness**: Introduced by Papernot et al. (2016), defensive distillation reduces adversarial attack success rate from 95% to 0.5% on MNIST against FGSM. However, it was later shown to be vulnerable to C&W attacks, highlighting the ongoing arms race between attacks and defenses. Runtime detection and mitigation mechanisms, such as input preprocessing [@addepalli2020towards] or prediction consistency checks, introduce latency and affect the real-time performance of ML systems.",
      "term": "Defensive Distillation Effectiveness",
      "length": 509
    },
    {
      "footnote_id": "fn-poisoning-detection-difficulty",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1167,
      "definition": "**Data Poisoning Detection Challenge**: Studies show that poisoning attacks with as little as 0.1% corrupted training data can reduce model accuracy by 10-20% while remaining undetectable by standard statistical tests. The stealthiness comes from crafting poisoned samples that follow the same distribution as clean data. Unlike adversarial examples, which target models at inference time, poisoning attacks exploit upstream components of the system\u2014such as data collection, labeling, or ingestion. As ML systems are increasingly deployed in automated and high-stakes environments, understanding how poisoning occurs and how it propagates through the system is essential for developing effective defenses.",
      "term": "Data Poisoning Detection Challenge",
      "length": 705
    },
    {
      "footnote_id": "fn-label-flip-impact",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1187,
      "definition": "**Label Flipping Attack Impact**: Research shows that flipping just 40% of training labels can reduce model accuracy from 92% to 60% on CIFAR-10. More sophisticated attacks can achieve similar degradation with only 10% label corruption by targeting specific classes or using gradient-based selection of samples to flip. These attacks render the model unreliable across a wide range of inputs, effectively making it unusable.",
      "term": "Label Flipping Attack Impact",
      "length": 424
    },
    {
      "footnote_id": "fn-backdoor-success-rate",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1193,
      "definition": "**Backdoor Attack Effectiveness**: BadNets, a seminal backdoor attack, achieves 99%+ attack success rate (when trigger is present) while maintaining 95%+ accuracy on clean inputs. The attack succeeds with as few as 1% poisoned training samples, making it extremely stealthy and difficult to detect through standard validation. These attacks are often effective even if the trigger pattern is imperceptible to human observers.",
      "term": "Backdoor Attack Effectiveness",
      "length": 425
    },
    {
      "footnote_id": "fn-online-poisoning-vulnerability",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 1223,
      "definition": "**Online Learning Vulnerability**: Studies show that online learning systems can be compromised with as little as 1% poisoned data per update batch. The attack is particularly effective because the model has no opportunity to \"forget\" the poisoned patterns, leading to permanent degradation that compounds over time. This form of attack is illustrated in @fig-poisoning-attack-example.",
      "term": "Online Learning Vulnerability",
      "length": 385
    },
    {
      "footnote_id": "fn-modern-fault-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2313,
      "definition": "**Fault Injection Framework Evolution**: Modern tools like PyTorchFI support 15+ fault models including bit-flips, stuck-at faults, and Byzantine failures. These frameworks can inject faults at multiple abstraction levels\u2014from hardware simulation (gem5) to Python tensor operations\u2014enabling comprehensive robustness evaluation across the entire stack.",
      "term": "Fault Injection Framework Evolution",
      "length": 351
    },
    {
      "footnote_id": "fn-single-vs-multi-bit",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2319,
      "definition": "**Single vs Multi-bit Fault Impact**: Counter-intuitively, single-bit faults are often more dangerous than multi-bit faults in ML systems. Single-bit errors can flip sign bits, causing massive value changes (e.g., +127 to -128 in 8-bit integers), while multi-bit errors often create invalid encodings that are detected and filtered out by error-checking mechanisms. However, other important behaviors like error masking [@mohanram2003partial] may only be observable at lower abstraction levels. As illustrated in @fig-error-masking, this masking phenomenon can cause faults to be filtered out before they propagate to higher levels, meaning software-based tools may miss these effects entirely.",
      "term": "Single vs Multi-bit Fault Impact",
      "length": 694
    },
    {
      "footnote_id": "fn-fidelity-tool-adoption",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2398,
      "definition": "**Fault Injection Tool Ecosystem**: The Fidelity framework bridges hardware and software fault models with 95% correlation accuracy. Other popular tools include PyTorchFI (2,000+ GitHub stars), TensorFI for TensorFlow, and BinFI for binary-level injection. This ecosystem reflects the growing need for systematic fault tolerance evaluation in ML systems. By mapping software-observed fault behaviors to corresponding hardware-level patterns [@cheng2016clear], Fidelity offers a more accurate means of simulating hardware faults at the software level. While lower-level tools capture the true propagation of errors through a hardware system, they are generally slower and more complex. Software-level tools, such as those implemented in PyTorch or TensorFlow, are faster and easier to use for large-scale robustness testing, albeit with less precision.",
      "term": "Fault Injection Tool Ecosystem",
      "length": 851
    },
    {
      "footnote_id": "fn-hardware-injection-accuracy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/robust_ai/robust_ai.qmd",
      "line": 2404,
      "definition": "**Hardware vs Software Injection**: Hardware-based fault injection using techniques like laser fault injection or electromagnetic pulse can achieve 99%+ accuracy in replicating real faults, compared to 70-85% accuracy for software simulation. However, hardware injection takes 100x longer and costs 1000x more, limiting its use to critical validation scenarios. While most error injection tools used in ML robustness research are software-based, because of their speed and scalability, hardware-based approaches remain critical for grounding higher-level error models. They are considered the most accurate means of studying the impact of faults on ML systems by manipulating the hardware directly to introduce errors.",
      "term": "Hardware vs Software Injection",
      "length": 718
    },
    {
      "footnote_id": "fn-ewaste-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 38,
      "definition": "**E-Waste from Computing**: Global e-waste reached 54 million metric tons in 2019, with computing equipment contributing 15%. AI hardware accelerates this trend\u2014NVIDIA's GPU sales increased 200% from 2020-2023, with each high-end GPU weighing 2-4 lbs and containing toxic materials requiring specialized disposal. The rapid obsolescence cycle means AI hardware often becomes e-waste within 3-5 years.",
      "term": "E-Waste from Computing",
      "length": 400
    },
    {
      "footnote_id": "fn-household-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 42,
      "definition": "**Household Energy Comparison**: The average U.S. household consumes 10,500 kWh annually (about 875 kWh monthly). Training GPT-4 consumed an estimated 50,000-100,000 MWh (50-100 million kWh), equivalent to 5-10 years of electricity for 10,000 households. This single training run used more energy than entire small cities like Aspen, Colorado (population 7,400) consume annually.",
      "term": "Household Energy Comparison",
      "length": 379
    },
    {
      "footnote_id": "fn-industry-comparison",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 44,
      "definition": "**AI vs Industrial Emissions**: Machine learning workloads are projected to produce 3.5% of global carbon emissions by 2030, surpassing aviation (2.8%) and approaching cement production (4%). Current AI emissions already exceed those of Argentina (0.7 billion tons CO\u2082 annually). Training just the top 10 large language models in 2023 generated emissions equivalent to 40,000 round-trip flights from New York to London.",
      "term": "AI vs Industrial Emissions",
      "length": 419
    },
    {
      "footnote_id": "fn-gpu-manufacturing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 48,
      "definition": "**GPU Manufacturing Impact**: Producing a single high-end GPU like the NVIDIA H100 generates 300-500 kg of CO\u2082 before any computation occurs. Manufacturing requires 2,500+ liters of ultrapure water, 15+ rare earth elements, and energy-intensive processes reaching 1,000\u00b0C. TSMC's advanced 4nm process consumes 40% more energy per wafer than older 7nm nodes, increasing embodied carbon in AI accelerators. Even small-scale AI systems, such as those deployed on edge devices, contribute to sustainability challenges, necessitating careful consideration of their lifecycle impact.",
      "term": "GPU Manufacturing Impact",
      "length": 577
    },
    {
      "footnote_id": "fn-ai-compute-growth",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 58,
      "definition": "**AI Compute Explosion**: This 350,000\u00d7 increase represents a doubling time of approximately 3.4 months\u2014far exceeding Moore's Law's 2-year doubling cycle. For comparison, this is equivalent to going from the computational power of a smartphone to that of the world's largest supercomputer. The trend has only accelerated with large language models: GPT-4's training is estimated to have required 25\u00d7 more compute than GPT-3, while models like PaLM-2 and Claude used even more computational resources.",
      "term": "AI Compute Explosion",
      "length": 500
    },
    {
      "footnote_id": "fn-moores-law",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 62,
      "definition": "**Moore's Law Origins**: Named after Intel co-founder Gordon Moore, who made this observation in a 1965 *Electronics* magazine article, Moore's Law has driven the semiconductor industry for nearly 60 years. Moore initially predicted a doubling every year, later revised to two years. The law's economic impact is staggering: it enabled the $4 trillion global electronics industry and made possible everything from smartphones to supercomputers. However, at 3nm process nodes, individual atoms become the limiting factor.",
      "term": "Moore's Law Origins",
      "length": 520
    },
    {
      "footnote_id": "fn-gpt3-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 66,
      "definition": "**GPT-3 Energy Consumption**: Training GPT-3 consumed approximately 1,300 MWh of electricity, equivalent to the annual energy consumption of 130 average American homes or the same amount of CO\u2082 as burning 500,000 pounds of coal. At average US electricity prices, this training run cost roughly $130,000 in electricity alone. GPT-4, with estimated 25\u00d7 more compute, likely consumed over 30,000 MWh\u2014enough to power a small city for a month.",
      "term": "GPT-3 Energy Consumption",
      "length": 438
    },
    {
      "footnote_id": "fn-datacenter-emissions",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 199,
      "definition": "**Data Center Climate Impact**: Data centers consume approximately 1% of global electricity and produce 0.3% of global carbon emissions directly. However, when including embodied carbon from hardware manufacturing, the figure rises to 2%. For perspective, this equals the annual emissions of Argentina (1.8% of global total) and exceeds the aviation industry's 2.1%. The largest hyperscale data centers consume over 100 MW continuously\u2014equivalent to powering 80,000 homes.",
      "term": "Data Center Climate Impact",
      "length": 472
    },
    {
      "footnote_id": "fn-hyperscale-size",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 205,
      "definition": "**Hyperscale Data Center Scale**: Meta's Prineville data center spans 2.5 million square feet (57 football fields) and houses 150,000+ servers. Microsoft's largest Azure data center in Iowa covers 700 acres with power capacity of 300 MW. Google operates 21 hyperscale facilities globally, consuming 12.2 TWh annually\u2014more electricity than entire countries like Lithuania or Sri Lanka.",
      "term": "Hyperscale Data Center Scale",
      "length": 384
    },
    {
      "footnote_id": "fn-tsmc-water",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 605,
      "definition": "**Semiconductor Water Consumption Scale**: TSMC's Arizona facility will consume 3.2 billion gallons annually\u2014equivalent to 37,000 Olympic swimming pools. Each AI chip requires 5-10x more water than traditional processors due to advanced nodes and complex manufacturing. Intel's Ireland fab uses 1.5 billion gallons annually, while Samsung's Texas facility is projected to use 6 million gallons daily. Water treatment and purification add 30-50% to total consumption. During peak summer months, the cumulative daily water consumption of major fabs rivals that of cities with populations exceeding half a million people[^fn-fab-vs-cities].",
      "term": "Semiconductor Water Consumption Scale",
      "length": 637
    },
    {
      "footnote_id": "fn-fab-vs-cities",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 609,
      "definition": "**Fab Water Usage vs Cities**: Taiwan's semiconductor industry consumes 150 million tons of water annually\u20145% of the island's total usage, equivalent to cities like Boston (685,000 people). During Taiwan's 2021 drought, fabs were allocated water priority over farmers, highlighting resource conflicts. TSMC alone uses more water than cities like Tampa, Florida, leading to groundwater depletion rates of 1-2 meters annually in Hsinchu.",
      "term": "Fab Water Usage vs Cities",
      "length": 435
    },
    {
      "footnote_id": "fn-chemical-scale",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 631,
      "definition": "**Hazardous Chemical Quantities**: A typical large semiconductor fab uses 500+ different chemicals annually, consuming 500-2,000 metric tons of acids, 50-200 metric tons of solvents, and 10-50 tons of toxic gases. Arsine gas is lethal at 3 parts per million over 30 minutes. TSMC's facilities store over 50,000 tons of chemicals on-site, requiring specialized emergency response teams and $100+ million in safety infrastructure per fab. Any leaks or accidental releases in fabs can lead to severe health hazards for workers and surrounding communities.",
      "term": "Hazardous Chemical Quantities",
      "length": 552
    },
    {
      "footnote_id": "fn-indium-supply",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 647,
      "definition": "**Critical Material Scarcity**: Indium production is only 600-800 tons annually worldwide, with China controlling 60% of supply. Prices fluctuate wildly\u2014from $60/kg in 2002 to $1,000/kg in 2005, now around $400/kg. Each smartphone contains 0.3mg of indium; each AI accelerator contains 50-100x more. At current AI hardware growth rates (40% annually), demand will exceed supply by 2035 without recycling breakthroughs. As AI hardware manufacturing scales, the demand for helium will continue to grow, necessitating more sustainable extraction and recycling practices.",
      "term": "Critical Material Scarcity",
      "length": 567
    },
    {
      "footnote_id": "fn-china-ree-control",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 651,
      "definition": "**Chinese Rare Earth Dominance**: China produces 85% of rare earth elements and controls 95% of global refining capacity. The 2010 China-Japan diplomatic crisis saw rare earth exports to Japan cut by 40%, causing prices to spike 2,000%. A single NVIDIA H100 contains 17 different rare earth elements totaling 200-300 grams. U.S. strategic reserves contain only 3-month supply, while building alternative supply chains requires 10-15 years and $50+ billion investment.",
      "term": "Chinese Rare Earth Dominance",
      "length": 467
    },
    {
      "footnote_id": "fn-jevons-paradox",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 991,
      "definition": "**Jevon's Paradox**: Named after British economist William Stanley Jevons who observed in 1865 that improving coal efficiency actually increased total coal consumption rather than reducing it. Modern examples include LEDs\u2014despite being 85% more efficient than incandescent bulbs, total lighting energy consumption has increased due to expanded usage. In AI, this means that making models 10\u00d7 more efficient might lead to 100\u00d7 more AI applications, resulting in net increase in environmental impact.",
      "term": "Jevon's Paradox",
      "length": 498
    },
    {
      "footnote_id": "fn-pue-efficiency",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1092,
      "definition": "**Power Usage Effectiveness**: Data center efficiency is measured by PUE (Power Usage Effectiveness)\u2014total facility power divided by IT equipment power. Industry average PUE is 1.67 (67% overhead for cooling/infrastructure), but leading hyperscalers achieve 1.1-1.2. Google's best data centers reach PUE of 1.08, meaning only 8% energy overhead. Each 0.1 PUE improvement saves millions annually in electricity costs. The industry must adopt strategies to optimize power efficiency, integrate renewable energy sources, and improve cooling mechanisms to mitigate the environmental impact of AI infrastructure.",
      "term": "Power Usage Effectiveness",
      "length": 607
    },
    {
      "footnote_id": "fn-google-carbon-free",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1096,
      "definition": "**Google's Carbon-Free Commitment**: Google achieved carbon neutrality in 2007 and has been carbon neutral for 15 years, but 24/7 carbon-free energy is more ambitious\u2014requiring real-time matching of energy consumption with clean generation. Currently at 64% carbon-free energy globally. Denmark data centers run on 100% wind power, while others still rely on grid renewables certificates. This requires $15 billion+ investment in clean energy projects worldwide.",
      "term": "Google's Carbon-Free Commitment",
      "length": 462
    },
    {
      "footnote_id": "fn-cooling-energy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1100,
      "definition": "**Data Center Cooling Costs**: Cooling consumes 38% of total data center energy on average. A typical 10 MW data center spends $3.8 million annually on cooling electricity. Google's machine learning optimization reduced cooling energy by 40%, saving $150+ million globally. Liquid cooling can be 3,000x more efficient than air cooling for high-density AI workloads, reducing cooling energy from 40% to under 10% of total consumption. To improve efficiency, data centers are adopting alternative cooling strategies that reduce energy waste. Liquid cooling, which transfers heat away from AI accelerators using specially designed coolant systems, is significantly more effective than traditional air cooling and is now being deployed in high-density computing clusters. Free-air cooling, which utilizes natural airflow instead of mechanical refrigeration, has also been adopted in temperate climates, where external conditions allow for passive cooling. Microsoft has taken this a step further by deploying underwater data centers[^fn-underwater-dc] that use the surrounding ocean as a natural cooling mechanism, reducing the need for active temperature regulation.",
      "term": "Data Center Cooling Costs",
      "length": 1163
    },
    {
      "footnote_id": "fn-underwater-dc",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1102,
      "definition": "**Project Natick Underwater Data Centers**: Microsoft's underwater data center achieved 87.5% server uptime compared to 96% on land, but operated 8x more reliably due to absence of corrosive oxygen and humidity. The sealed containers consume 1/8th the energy for cooling. However, recovery and maintenance require specialized vessels costing $500,000+ per retrieval. The project demonstrated feasibility but highlighted trade-offs between efficiency and operational complexity.",
      "term": "Project Natick Underwater Data Centers",
      "length": 477
    },
    {
      "footnote_id": "fn-google-carbon-scheduling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1114,
      "definition": "**Google Carbon-Aware Scheduling Results**: Google's carbon-intelligent computing platform achieved 15% reduction in hourly carbon footprint by shifting workloads within regions. Globally shifting workloads between data centers achieved 40% reduction. The system processes 95 billion search queries daily while optimizing for grid carbon intensity. Non-urgent tasks like batch training can shift 70% of workload to lower-carbon time periods, reducing emissions equivalent to taking 50,000 cars off the road annually.",
      "term": "Google Carbon-Aware Scheduling Results",
      "length": 516
    },
    {
      "footnote_id": "fn-grid-carbon-data",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1120,
      "definition": "**Real-Time Grid Carbon Intensity**: Grid carbon intensity varies dramatically\u2014from 50g CO\u2082/kWh in nuclear-heavy France to 820g/kWh in coal-dependent Poland. In Texas, intensity fluctuates 10x daily (150-1,500g/kWh) based on wind generation. The Electricity Maps API serves 50+ million requests daily to enable carbon-aware computing. WattTime API provides marginal emissions data showing which power plants turn on/off next, allowing 2-5x better carbon optimization than average intensity.",
      "term": "Real-Time Grid Carbon Intensity",
      "length": 490
    },
    {
      "footnote_id": "fn-nuclear-ai",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1130,
      "definition": "**Nuclear Power for AI Data Centers**: Microsoft partnered with Helion Energy for fusion power by 2028, signing the first commercial fusion agreement. Amazon invested $500M in small modular reactors (SMRs) for data centers. Google is exploring 24/7 nuclear partnerships with Kairos Power. Nuclear provides 20% of U.S. electricity with 12g CO\u2082/kWh lifecycle emissions versus 820-1,050g for coal. However, new nuclear costs $150-200/MWh versus $20-40 for renewables plus storage.",
      "term": "Nuclear Power for AI Data Centers",
      "length": 477
    },
    {
      "footnote_id": "fn-energy-frameworks",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/sustainable_ai/sustainable_ai.qmd",
      "line": 1136,
      "definition": "**Energy-Aware AI Frameworks**: Zeus framework achieves 75% energy savings on BERT training by automatically finding optimal energy-performance trade-offs. Perseus reduces GPU memory usage by 50% through dynamic batching, lowering energy consumption proportionally. CodeCarbon automatically tracks emissions, revealing that training can vary 10-100x in energy usage depending on optimization settings. These tools democratize energy optimization beyond just hyperscale companies.",
      "term": "Energy-Aware AI Frameworks",
      "length": 479
    },
    {
      "footnote_id": "fn-eniac",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 569,
      "definition": "**ENIAC (Electronic Numerical Integrator and Computer)**: Completed in 1946 at the University of Pennsylvania, ENIAC weighed 30 tons, consumed 150kW of power, and performed 5,000 operations per second. Its 17,468 vacuum tubes required constant maintenance, but it demonstrated electronic computation could be 1,000x faster than mechanical calculators.",
      "term": "ENIAC (Electronic Numerical Integrator and Computer)",
      "length": 351
    },
    {
      "footnote_id": "fn-system360",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 571,
      "definition": "**IBM System/360**: Launched in 1964 as a $5 billion gamble (equivalent to $40 billion today), System/360 introduced the revolutionary concept of backward compatibility across different computer models. Its standardized instruction set architecture became the foundation for modern computing, enabling software portability that drives today's cloud computing.",
      "term": "IBM System/360",
      "length": 359
    },
    {
      "footnote_id": "fn-cdc6600",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 573,
      "definition": "**CDC 6600**: Designed by Seymour Cray and released in 1964, the CDC 6600 achieved 3 MFLOPS (million floating-point operations per second) using innovative parallel processing with 10 peripheral processors. Costing $8 million ($65 million today), it was the world's fastest computer until 1969 and established supercomputing as a field.",
      "term": "CDC 6600",
      "length": 336
    },
    {
      "footnote_id": "fn-cm5",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 575,
      "definition": "**Connection Machine CM-5**: Released by Thinking Machines in 1991, the CM-5 featured up to 16,384 processors connected by a fat-tree network, delivering over 100 GFLOPS. Its $10-50 million price tag and specialized parallel architecture made it a favorite for scientific computing but ultimately commercially unsuccessful as commodity clusters emerged.",
      "term": "Connection Machine CM-5",
      "length": 353
    },
    {
      "footnote_id": "fn-google-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 577,
      "definition": "**Google Data Centers**: Starting in 1998 with commodity PCs, Google pioneered warehouse-scale computing by 2003, managing over 100,000 servers across multiple facilities. By 2020, Google operated over 20 data centers consuming 12 TWh annually\u2014equivalent to entire countries\u2014while achieving industry-leading PUE (Power Usage Effectiveness) of 1.10 through innovative cooling.",
      "term": "Google Data Centers",
      "length": 375
    },
    {
      "footnote_id": "fn-alexnet",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 579,
      "definition": "**AlexNet**: Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet won ImageNet 2012 with 15.3% error rate (vs. 26.2% for second place), using two GTX 580 GPUs for 5-6 days of training. This breakthrough launched the deep learning revolution and demonstrated that GPUs could accelerate neural network training by 10-50x over CPUs.",
      "term": "AlexNet",
      "length": 351
    },
    {
      "footnote_id": "fn-nvidia-gpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 581,
      "definition": "**NVIDIA AI GPUs**: From the 2012 GTX 680 (3.1 TFLOPS) used for AlexNet to the 2023 H100 (989 TFLOPS for AI), NVIDIA GPUs increased AI performance by over 300x in a decade. The H100 costs $25,000-40,000 but enables training models that would be impossible on older hardware, demonstrating specialized silicon's critical role in AI advancement.",
      "term": "NVIDIA AI GPUs",
      "length": 343
    },
    {
      "footnote_id": "fn-google-tpus",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 583,
      "definition": "**Google TPUs**: First deployed internally in 2015, TPUs deliver 15-30x better price-performance than GPUs for specific AI workloads. The TPU v4 (2021) achieves 275 TFLOPS with 32GB memory, while TPU pods can scale to 1 exaFLOP. Google's $billions investment in custom silicon has enabled training models like PaLM (540B parameters) cost-effectively.",
      "term": "Google TPUs",
      "length": 350
    },
    {
      "footnote_id": "fn-minibatch-gpu",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 585,
      "definition": "**Mini-batch GPU Optimization**: Modern GPUs achieve peak efficiency with batch sizes of 32-512 examples. A Tesla V100 can process 32 ImageNet images in 15ms but would take 480ms processing them individually\u2014a 32x efficiency gain. Optimal batch size balances GPU utilization (targeting >90%) with memory constraints, typically requiring 8-16GB for batch sizes of 256-512 on modern vision models.",
      "term": "Mini-batch GPU Optimization",
      "length": 395
    },
    {
      "footnote_id": "fn-rmsprop",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 587,
      "definition": "**RMSprop (Root Mean Square Propagation)**: Developed by Geoffrey Hinton in 2012 for his Coursera course, RMSprop addresses AdaGrad's aggressive learning rate decay by using exponentially decaying averages. It typically achieves 20-40% faster convergence than SGD on deep networks, with the standard decay rate of 0.9 working well across most applications.",
      "term": "RMSprop (Root Mean Square Propagation)",
      "length": 356
    },
    {
      "footnote_id": "fn-sigmoid-cost",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 589,
      "definition": "**Sigmoid Computational Cost**: Computing sigmoid requires expensive exponential operations\u2014on CPU, `exp()` takes 10-20 clock cycles vs. 1 cycle for basic arithmetic. GPU implementations use 32-entry lookup tables with linear interpolation, reducing cost to 3-4 cycles but still 3x slower than ReLU. This overhead compounds in deep networks with millions of activations per forward pass.",
      "term": "Sigmoid Computational Cost",
      "length": 387
    },
    {
      "footnote_id": "fn-relu-hardware",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 591,
      "definition": "**ReLU Hardware Efficiency**: ReLU requires just 1 instruction (`max(0,x)`) vs. sigmoid's 10+ operations including exponentials. On NVIDIA GPUs, ReLU runs at 95% of peak FLOPS while sigmoid achieves only 30-40%. ReLU's sparsity (typically 50% zeros) enables additional optimizations: sparse matrix operations, reduced memory bandwidth, and compressed gradients during backpropagation.",
      "term": "ReLU Hardware Efficiency",
      "length": 384
    },
    {
      "footnote_id": "fn-data-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 593,
      "definition": "**Data Parallelism Scaling**: Linear scaling works until communication becomes the bottleneck\u2014typically around 64-128 GPUs for most models. BERT-Large achieves 76x speedup on 128 GPUs (59% efficiency), while GPT-3 required 1,024 GPUs with only 45% efficiency. The key constraint is AllReduce communication cost scales as O(n) with number of devices, requiring high-bandwidth interconnects like InfiniBand.",
      "term": "Data Parallelism Scaling",
      "length": 405
    },
    {
      "footnote_id": "fn-model-parallelism",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 595,
      "definition": "**Model Parallelism Memory Scaling**: Enables training models too large for single GPUs\u2014GPT-3 (175B parameters) needs 350GB just for weights in FP16, far exceeding any single GPU's 80GB maximum. However, model parallelism often achieves only 20-60% compute efficiency due to sequential dependencies between model partitions and communication overhead between devices.",
      "term": "Model Parallelism Memory Scaling",
      "length": 367
    },
    {
      "footnote_id": "fn-nccl",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 597,
      "definition": "**NVIDIA NCCL (Collective Communications Library)**: Optimized for multi-GPU communication, NCCL achieves 90-95% of theoretical bandwidth on modern interconnects. On DGX systems with NVLink, NCCL can transfer 600 GB/s between 8 GPUs\u201450x faster than PCIe\u2014making efficient distributed training possible. It implements optimized AllReduce algorithms that reduce communication from O(n\u00b2) to O(n).",
      "term": "NVIDIA NCCL (Collective Communications Library)",
      "length": 392
    },
    {
      "footnote_id": "fn-gpt3-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 599,
      "definition": "**GPT-3 Training Scale**: Used 10,000 NVIDIA V100 GPUs for 3-4 months, consuming ~1,287 MWh of energy (equivalent to 120 US homes for a year). The training cost was estimated at $4-12 million, demonstrating how specialized hardware and distributed systems enable training at previously impossible scales while highlighting the enormous resource requirements.",
      "term": "GPT-3 Training Scale",
      "length": 358
    },
    {
      "footnote_id": "fn-tensor-cores",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 601,
      "definition": "**Tensor Cores**: Introduced with NVIDIA's Volta architecture (2017), Tensor Cores deliver 4x speedup for mixed-precision training by performing 4x4 matrix operations in a single clock cycle. The H100's 4th-gen Tensor Cores achieve 989 TFLOPS for FP16 operations\u2014roughly 6x faster than traditional CUDA cores\u2014enabling training of larger models with the same hardware budget.",
      "term": "Tensor Cores",
      "length": 374
    },
    {
      "footnote_id": "fn-systolic-array",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 603,
      "definition": "**Systolic Array Architecture**: Developed at Carnegie Mellon in 1978, systolic arrays excel at matrix operations by streaming data through a grid of processing elements. Google's TPU v4 systolic array performs 275 TFLOPS while consuming only 175W\u2014achieving 1.57 TFLOPS/W efficiency, roughly 2-3x more energy-efficient than comparable GPUs for ML workloads.",
      "term": "Systolic Array Architecture",
      "length": 357
    },
    {
      "footnote_id": "fn-fpga-datacenter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 605,
      "definition": "**Microsoft FPGA Deployment**: Project Catapult deployed FPGAs across Microsoft's entire datacenter fleet by 2016, with one FPGA per server (>1 million total). This $1 billion investment improved Bing search latency by 50% and Azure ML inference by 2x, while reducing power consumption by 10-15% through specialized acceleration of specific algorithms.",
      "term": "Microsoft FPGA Deployment",
      "length": 352
    },
    {
      "footnote_id": "fn-wse-specs",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 607,
      "definition": "**Wafer-Scale Engine Specifications**: The WSE-2 (2021) contains 2.6 trillion transistors on a 21cm x 21cm wafer\u2014the largest chip ever manufactured. With 850,000 cores and 40GB on-chip memory, it delivers 15-20x speedup vs. GPU clusters for large language models while consuming 15kW (comparable to 16-20 V100 GPUs but with orders of magnitude less communication overhead).",
      "term": "Wafer-Scale Engine Specifications",
      "length": 373
    },
    {
      "footnote_id": "fn-gradient-accumulation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 609,
      "definition": "**Gradient Accumulation Impact**: Enables effective batch sizes of 2048+ on single GPUs with only 32-64 mini-batch size, crucial for transformer training. BERT-Large training uses effective batch size of 256 (accumulated over 8 steps) achieving 99.5% of full-batch performance while reducing memory requirements by 8x. The technique trades 10-15% compute overhead for massive memory savings.",
      "term": "Gradient Accumulation Impact",
      "length": 391
    },
    {
      "footnote_id": "fn-activation-checkpointing",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 611,
      "definition": "**Activation Checkpointing Trade-offs**: Reduces memory usage by 50-90% at the cost of 15-30% additional compute time due to recomputation. For training GPT-3 on V100s, checkpointing enables 2.8x larger models (from 1.3B to 3.7B parameters) within 32GB memory constraints, making it essential for memory-bound large model training despite the compute penalty.",
      "term": "Activation Checkpointing Trade-offs",
      "length": 359
    },
    {
      "footnote_id": "fn-transformer-scaling",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 613,
      "definition": "**Transformer Batch Size Scaling**: Research shows transformers achieve optimal performance with batch sizes of 256-4096 tokens, requiring gradient accumulation on most hardware. GPT-2 training improved perplexity by 0.3-0.5 points when increasing from batch size 32 to 512, demonstrating the critical importance of large effective batch sizes for language model convergence.",
      "term": "Transformer Batch Size Scaling",
      "length": 375
    },
    {
      "footnote_id": "fn-profiling-tools",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 615,
      "definition": "**Training Profiling Tools**: NVIDIA Nsight Systems can identify that data loading consumes 20-40% of training time in poorly optimized pipelines, while TensorFlow Profiler reveals GPU utilization rates (optimal: >90%). Intel VTune showed that memory bandwidth often limits performance more than raw compute\u2014typical deep learning workloads achieve only 30-50% of peak FLOPS due to memory bottlenecks.",
      "term": "Training Profiling Tools",
      "length": 400
    },
    {
      "footnote_id": "fn-backpropagation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 617,
      "definition": "**Backpropagation Algorithm**: Independently rediscovered multiple times, backpropagation was popularized by Rumelhart, Hinton, and Williams in 1986 (though similar ideas appeared in Werbos 1974). This breakthrough enabled training of deep networks by efficiently computing gradients in O(n) time vs. naive O(n\u00b2) approaches. Modern implementations require careful memory management\u2014storing all activations for a ResNet-50 consumes 1.2GB per image.",
      "term": "Backpropagation Algorithm",
      "length": 447
    },
    {
      "footnote_id": "fn-lr-schedules",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 619,
      "definition": "**Learning Rate Schedules**: Critical for training stability and convergence. Cosine annealing (introduced in 2016) and linear warmup (from BERT 2018) became standard after showing 2-5% accuracy improvements. Large batch training requires linear scaling rule: multiply learning rate by batch size ratio (batch 512 \u2192 LR 0.1, batch 4096 \u2192 LR 0.8), discovered through extensive experimentation by Facebook and Google teams.",
      "term": "Learning Rate Schedules",
      "length": 420
    },
    {
      "footnote_id": "fn-sgd",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 621,
      "definition": "**SGD (Stochastic Gradient Descent)**: First introduced by Herbert Robbins and Sutton Monro in 1951 for stochastic approximation, SGD became the foundation of modern ML training after Rumelhart et al.'s 1986 backpropagation paper, enabling neural networks to learn from individual examples rather than entire datasets.",
      "term": "SGD (Stochastic Gradient Descent)",
      "length": 318
    },
    {
      "footnote_id": "fn-adam",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 673,
      "definition": "**Adam (Adaptive Moment Estimation)**: Introduced by Diederik Kingma and Jimmy Ba in 2014, Adam revolutionized deep learning training by combining the best of momentum and adaptive learning rates, becoming the default optimizer for most neural networks and enabling the training of increasingly complex models.",
      "term": "Adam (Adaptive Moment Estimation)",
      "length": 310
    },
    {
      "footnote_id": "fn-mixed-precision",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 713,
      "definition": "**Mixed-Precision Training**: Introduced by NVIDIA in 2018, this technique uses FP16 for forward/backward passes while maintaining FP32 precision for loss scaling, enabling 2x memory savings and 1.6x speedups on Tensor Core GPUs while maintaining model accuracy.",
      "term": "Mixed-Precision Training",
      "length": 262
    },
    {
      "footnote_id": "fn-distributed-training",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/training/training.qmd",
      "line": 2147,
      "definition": "**Distributed Training**: Google's DistBelief (2012) pioneered large-scale distributed neural network training, enabling models with billions of parameters across thousands of machines. This breakthrough led to modern frameworks like Horovod (2017) and PyTorch's DistributedDataParallel, democratizing distributed training for researchers worldwide.",
      "term": "Distributed Training",
      "length": 349
    },
    {
      "footnote_id": "fn-mlflow",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 123,
      "definition": "**Experiment Tracking Evolution**: MLflow, open-sourced by Databricks in 2018, was one of the first comprehensive experiment tracking platforms, addressing the \"ML experiment management crisis\" where data scientists were losing track of model versions and results. Similar platforms like Weights & Biases (2017) and Neptune (2019) emerged to solve what the industry calls \"the reproducibility crisis\"\u2014studies found that only 15% of ML papers could be reproduced by other researchers, largely due to poor experiment tracking.",
      "term": "Experiment Tracking Evolution",
      "length": 524
    },
    {
      "footnote_id": "fn-cicd-ml",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 141,
      "definition": "**CI/CD for Machine Learning**: Traditional continuous integration assumes deterministic builds\u2014the same code produces the same output. ML systems violate this assumption because model behavior depends on training data, random initialization, and hardware differences. Google's TFX (TensorFlow Extended) and similar platforms had to reinvent CI/CD principles for ML, introducing concepts like \"model validation\" and \"data validation\" that have no equivalent in traditional software. Survey data shows that 78% of ML teams report that their traditional DevOps tools are inadequate for ML workflows.",
      "term": "CI/CD for Machine Learning",
      "length": 597
    },
    {
      "footnote_id": "fn-crisp-dm",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 149,
      "definition": "**CRISP-DM (Cross-Industry Standard Process for Data Mining)**: Developed in 1996 by a consortium of companies including IBM, SPSS, and Daimler, CRISP-DM was one of the first structured methodologies for data projects. Its six phases (Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment) laid the groundwork for modern ML lifecycles, though today's approaches emphasize continuous iteration and monitoring that wasn't central to the original framework.",
      "term": "CRISP-DM (Cross-Industry Standard Process for Data Mining)",
      "length": 495
    },
    {
      "footnote_id": "fn-jupyter",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 159,
      "definition": "**Jupyter Notebooks**: Created by Fernando P\u00e9rez in 2001 as IPython, and later evolved into Project Jupyter in 2014. The name \"Jupyter\" comes from the core languages it supports: Julia, Python, and R. These notebooks revolutionized data science by allowing code, visualizations, and explanatory text in a single document, making the experimental nature of ML development more transparent and reproducible. Netflix estimates over 150,000 notebooks are created daily across the industry.",
      "term": "Jupyter Notebooks",
      "length": 485
    },
    {
      "footnote_id": "fn-data-versioning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 177,
      "definition": "**Data Versioning Challenges**: Unlike code, which changes through discrete edits, data can change gradually through drift, suddenly through schema changes, or subtly through quality degradation. Traditional version control systems like Git struggle with large datasets, leading to specialized tools like Git LFS (Large File Storage, 2015) and DVC (Data Version Control, 2017). Studies show that 87% of ML projects fail due to data issues, not algorithmic problems\u2014highlighting why ML workflows must treat data with the same rigor as code.",
      "term": "Data Versioning Challenges",
      "length": 539
    },
    {
      "footnote_id": "fn-dr-statistics",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 271,
      "definition": "**Diabetic Retinopathy Global Impact**: Affects over 103 million people worldwide, with 28.5% of diabetic patients developing some form of retinopathy. In developing countries, up to 90% of vision loss from diabetes is preventable with early detection, but access to ophthalmologists remains severely limited\u2014rural areas in India have one ophthalmologist per 120,000 people, compared to the WHO recommendation of 1 per 20,000. This stark disparity makes AI-assisted screening not just convenient but potentially life-changing for millions.",
      "term": "Diabetic Retinopathy Global Impact",
      "length": 539
    },
    {
      "footnote_id": "fn-healthcare-ai-challenges",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 279,
      "definition": "**Healthcare AI Deployment Reality**: Studies show that 85% of healthcare AI projects never reach clinical deployment, with the majority failing not due to algorithmic issues but due to integration challenges, regulatory hurdles, and workflow disruption. The \"AI chasm\" between research success and clinical adoption is particularly wide in healthcare\u2014while medical AI papers show 95%+ accuracy rates, real-world implementation studies report significant performance drops due to data drift, equipment variations, and user acceptance issues.",
      "term": "Healthcare AI Deployment Reality",
      "length": 541
    },
    {
      "footnote_id": "fn-problem-definition",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 289,
      "definition": "**ML vs. Traditional Problem Definition**: Traditional software problems are defined by deterministic specifications (\"if input X, then output Y\"), but ML problems are defined by examples and desired behaviors. This fundamental shift means that 73% of ML project failures occur during problem definition, compared to only 32% for traditional software. The challenge lies in translating business objectives into learning objectives\u2014something that didn't exist in software engineering until the rise of data-driven systems in the 2000s.",
      "term": "ML vs. Traditional Problem Definition",
      "length": 534
    },
    {
      "footnote_id": "fn-medical-annotation",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 335,
      "definition": "**Medical Data Annotation Costs**: Expert medical annotation is extraordinarily expensive\u2014ophthalmologists charge $200-500 per hour, meaning the DR dataset's annotation cost exceeded $2.7 million in expert time alone. This represents one of the highest annotation costs per sample in ML history. For comparison, ImageNet's 14 million images cost approximately $50,000 to annotate using crowdsourcing, while medical datasets can cost 100-1000x more per image. This cost disparity explains why medical AI often relies on transfer learning and why synthetic data generation is becoming crucial for healthcare applications.",
      "term": "Medical Data Annotation Costs",
      "length": 619
    },
    {
      "footnote_id": "fn-medical-privacy",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 343,
      "definition": "**Medical AI Privacy Complexity**: Healthcare data crosses jurisdictional boundaries with different privacy laws\u2014HIPAA in the US, GDPR in Europe, and various national regulations elsewhere. Medical AI systems must implement techniques like differential privacy, federated learning, and homomorphic encryption, adding 40-60% to development costs. The \"data cannot leave the country\" requirements in many regions have led to the rise of federated learning architectures, where models travel to data rather than data traveling to models\u2014a paradigm shift that Google's DR project helped establish in healthcare AI.",
      "term": "Medical AI Privacy Complexity",
      "length": 610
    },
    {
      "footnote_id": "fn-transfer-learning",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 442,
      "definition": "**Transfer Learning**: A technique where models pre-trained on large datasets (like ImageNet's 14 million images) are adapted for specific tasks, dramatically reducing training time and data requirements. Introduced by Yann LeCun's team in the 1990s and popularized by the 2014 ImageNet competition, transfer learning became the foundation for most practical computer vision applications. Instead of training from scratch, practitioners can achieve expert-level performance with thousands rather than millions of training examples.",
      "term": "Transfer Learning",
      "length": 531
    },
    {
      "footnote_id": "fn-model-drift",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 574,
      "definition": "**Model Drift Phenomenon**: ML models degrade over time without any code changes\u2014a phenomenon unknown in traditional software. Studies show that 70% of production ML models experience significant performance degradation within 6 months due to data drift, concept drift, or infrastructure drift. This \"silent failure\" problem led to the development of specialized monitoring tools like Evidently AI (2020) and Fiddler (2018), creating an entirely new category of ML infrastructure that has no equivalent in traditional software engineering.",
      "term": "Model Drift Phenomenon",
      "length": 539
    },
    {
      "footnote_id": "fn-deployment-reality-gap",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 586,
      "definition": "**The Lab-to-Clinic Performance Gap**: Medical AI systems typically see 10-30% performance drops when deployed in real-world settings, a phenomenon known as the \"deployment reality gap.\" This occurs because training data, despite best efforts, cannot capture the full diversity of real-world conditions\u2014different camera models, varying image quality, diverse patient populations, and operator skill levels all contribute to this gap. The gap is so consistent that regulatory bodies like the FDA now require \"real-world performance studies\" for medical AI approval, acknowledging that laboratory performance is insufficient to predict clinical utility.",
      "term": "The Lab-to-Clinic Performance Gap",
      "length": 651
    },
    {
      "footnote_id": "fn-ml-team-evolution",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 648,
      "definition": "**ML Team Role Evolution**: The \"data scientist\" role only emerged around 2008 (coined by DJ Patil and Jeff Hammerbacher at Facebook and LinkedIn), while \"ML engineer\" became common around 2015 as companies realized that research models need production engineering. \"MLOps engineer\" appeared around 2018, and \"AI ethics officer\" became standard at major tech companies by 2020. This rapid role specialization reflects ML's evolution from research curiosity to production necessity\u2014modern enterprise ML teams average 8-12 distinct roles compared to 2-3 in traditional software teams.",
      "term": "ML Team Role Evolution",
      "length": 582
    },
    {
      "footnote_id": "fn-systems-thinking",
      "file": "/Users/VJ/GitHub/MLSysBook/quarto/contents/core/workflow/workflow.qmd",
      "line": 684,
      "definition": "**Systems Thinking in AI**: The concept of \"systems thinking\" originated with biologist Ludwig von Bertalanffy in the 1940s and was later applied to engineering by Jay Forrester at MIT in the 1950s. Its application to AI became critical around 2018 when companies realized that optimizing individual ML models wasn't enough\u2014success required optimizing the entire ML pipeline. This shift explains why \"ML systems engineering\" emerged as a distinct discipline, separate from both traditional software engineering and machine learning research, with its own conferences (MLSys, first held in 2020) and academic programs.",
      "term": "Systems Thinking in AI",
      "length": 617
    }
  ]
}